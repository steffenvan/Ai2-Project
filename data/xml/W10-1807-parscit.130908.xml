<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.849593">
Influence of Pre-annotation on POS-tagged Corpus Development
</title>
<author confidence="0.869577">
Kar¨en Fort Benoit Sagot
</author>
<affiliation confidence="0.442757">
INIST CNRS / LIPN INRIA Paris-Rocquencourt / Paris 7
Nancy / Paris, France. Paris, France.
</affiliation>
<email confidence="0.974663">
karen.fort@inist.fr benoit.sagot@inria.fr
</email>
<sectionHeader confidence="0.993059" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962263157895">
This article details a series of carefully de-
signed experiments aiming at evaluating
the influence of automatic pre-annotation
on the manual part-of-speech annotation
of a corpus, both from the quality and the
time points of view, with a specific atten-
tion drawn to biases. For this purpose, we
manually annotated parts of the Penn Tree-
bank corpus (Marcus et al., 1993) under
various experimental setups, either from
scratch or using various pre-annotations.
These experiments confirm and detail the
gain in quality observed before (Marcus et
al., 1993; Dandapat et al., 2009; Rehbein
et al., 2009), while showing that biases do
appear and should be taken into account.
They finally demonstrate that even a not
so accurate tagger can help improving an-
notation speed.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999370625">
Training a machine-learning based part-of-speech
(POS) tagger implies manually tagging a signifi-
cant amount of text. The cost of this, in terms of
human effort, slows down the development of tag-
gers for under-resourced languages.
One usual way to improve this situation is to
automatically pre-annotate the corpus, so that the
work of the annotators is limited to the validation
of this pre-annotation. This method proved quite
efficient in a number of POS-annotated corpus de-
velopment projects (Marcus et al., 1993; Danda-
pat et al., 2009), allowing for a significant gain
not only in annotation time but also in consistency.
However, the influence of the pre-tagging quality
on the error rate in the resulting annotated corpus
and the bias introduced by the pre-annotation has
been little examined. This is what we propose to
do here, using different parts of the Penn Treebank
to train various instances of a POS tagger and ex-
periment on pre-annotation. Our goal is to assess
the impact of the quality (i.e., accuracy) of the
POS tagger used for pre-annotating and to com-
pare the use of pre-annotation with purely manual
tagging, while minimizing all kinds of biases. We
quantify the results in terms of error rate in the re-
sulting annotated corpus, manual annotation time
and inter-annotator agreement.
This article is organized as follows. In Sec-
tion 2, we mention some related work, while Sec-
tion 3 describes the experimental setup, followed
by a discussion on the obtained results (Section 4)
and a conclusion.
</bodyText>
<sectionHeader confidence="0.999861" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.957267">
2.1 Pre-annotation for POS Tagging
</subsectionHeader>
<bodyText confidence="0.999913956521739">
Very few manual annotation projects give details
about the campaign itself. One major exception is
the Penn Treebank project (Marcus et al., 1993),
that provided detailed information about the man-
ual annotation methodology, evaluation and cost.
Marcus et al. (1993) thus showed that manual tag-
ging took twice as long as correcting pre-tagged
text and resulted in twice the inter-annotator dis-
agreement rate, as well as an error rate (using a
gold-standard annotation) about 50% higher. The
pre-annotation was done using a tagger trained on
the Brown Corpus, which, due to errors introduced
by an automatic mapping of tags from the Brown
tagset to the Penn Treebank tagset, had an error
rate of 7–9%. However, they report neither the in-
fluence of the training of the annotators on the po-
tential biases in correction, nor that of the quality
of the tagger on the correction time and the ob-
tained quality.
Dandapat et al. (2009) went further and showed
that, for complex POS-tagging (for Hindi and
Bangla), pre-annotation of the corpus allows for
a gain in time, but not necessarily in consis-
</bodyText>
<page confidence="0.972004">
56
</page>
<note confidence="0.764216">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 56–63,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999983888888889">
tency, which depends largely on the pre-tagging
quality. They also noticed that untrained annota-
tors were more influenced by pre-annotation than
the trained ones, who showed “consistent perfor-
mance”. However, this very complete and inter-
esting experiment lacked a reference allowing for
an evaluation of the quality of the annotations. Be-
sides, it only took into account two types of pre-
tagging quality, high accuracy and low accuracy.
</bodyText>
<subsectionHeader confidence="0.9987135">
2.2 Pre-annotation in Other Annotation
Tasks
</subsectionHeader>
<bodyText confidence="0.999955765625">
Alex et al. (2008) led some experiments in the
biomedical domain, within the framework of a
“curation” task of protein-protein interaction. Cu-
ration consists in reading through electronic ver-
sion of papers and entering retrieved information
into a template. They showed that perfectly pre-
annotating the corpus leads to a reduction of more
than 1/3 in curation time, as well as a better recall
from the annotators. Less perfect pre-annotation
still leads to a gain in time, but less so (a little less
than 1/4th). They also tested the effect of higher
recall or precision of pre-annotation on one anno-
tator (curator), who rated recall more positively
than precision. However, as they notice, this result
can be explained by the curation style and should
be tested on more annotators.
Rehbein et al. (2009) led quite thorough ex-
periments on the subject, in the field of semantic
frame assignment annotation. They asked 6 an-
notators to annotate or correct frame assignment
using a task-specific annotation tool. Here again,
pre-annotation was done using only two types of
pre-tagging quality, state-of-the-art and enhanced.
The results of the experiments are a bit disappoint-
ing as they could not find a direct improvement of
annotation time using pre-annotation. The authors
reckon this might be at least partly due to “an inter-
action between time savings from pre-annotation
and time savings due to a training effect.” For
the same reason, they had to exclude some of the
annotation results for quality evaluation in order
to show that, in line with (Marcus et al., 1993),
quality pre-annotation helps increasing annotation
quality. They also found that noisy and low qual-
ity pre-annotation does not overall corrupt human
judgment.
On the other hand, Fort et al. (2009) claim that
pre-annotation introduces a bias in named entity
annotation, due to the preference given by anno-
tators to what is already annotated, thus prevent-
ing them from noticing entities that were not pre-
annotated. This particular type of bias should not
appear in POS-tagging, as all the elements are to
be annotated, but a pre-tagging could influence
the annotators, preventing them from asking them-
selves questions about a specific pre-annotation.
In a completely different field, Barque et
al. (2010) used a series of NLP tools, called
MACAON, to automatically identify the central
component and optional peripheral components of
dictionary definitions. This pre-processing gave
disappointing results as compared to entirely man-
ual annotation, as it did not allow for a significant
gain in time. The authors consider that the bad
results are due to the quality of the tool that they
wish to improve as they believe that “an automatic
segmentation of better quality would surely yield
some gains.”
Yet, the question remains: is there a quality
threshold for pre-annotation to be useful? and if
so, how can we evaluate it? We tried to answer
at least part of these questions for a quite simple
task for which data is available: POS-tagging in
English.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999992333333333">
The idea underlying our experiments is the follow-
ing. We split the Penn Treebank corpus (Marcus et
al., 1993) in a usual manner, namely we use Sec-
tions 2 to 21 to train various instances of a POS
tagger, and Section 23 to perform the actual ex-
periments. In order to measure the impact of the
POS tagger’s quality, we trained it on subcorpora
of increasing sizes, and pre-annotated Section 23
with these various POS taggers. Then, we man-
ually annotated parts of Section 23 under various
experimental setups, either from scratch or using
various pre-annotations, as explained below.
</bodyText>
<subsectionHeader confidence="0.999925">
3.1 Creating the Taggers
</subsectionHeader>
<bodyText confidence="0.999974571428571">
We used the MElt POS tagger (Denis and Sagot,
2009), a maximum-entropy based system that is
able to take into account both information ex-
tracted from a training corpus and information ex-
tracted from an external morphological lexicon.1
It has been shown to lead to a state-of-the-art POS
tagger for French. Trained on Sections 2 to 21
</bodyText>
<footnote confidence="0.861859666666667">
1MElt is freely available under LGPL license, on the web
page of its hosting project (http://gforge.inria.
fr/projects/lingwb/).
</footnote>
<page confidence="0.997595">
57
</page>
<bodyText confidence="0.998664411764706">
of the Penn Treebank (MEltALL
en ), and evaluated
on Section 23, MElt exhibits a 96.4% accuracy,
which is reasonably close to the state-of-the-art
(Spoustov´a et al. (2009) report 97.4%). Since it is
trained without any external lexicon, MEltALL
en is
very close to the original maximum-entropy based
tagger (Ratnaparkhi, 1996), which has indeed a
similar 96.6% accuracy.
We trained MElt on increasingly larger parts of
the POS-tagged Penn Treebank,2 thus creating dif-
ferent taggers with growing degrees of accuracy
(see table 1). We then POS-tagged the Section 23
with each of these taggers, thus obtaining for each
sentence in Section 23 a set of pre-annotations,
one from each tagger.
</bodyText>
<table confidence="0.998266882352941">
Tagger Nb train. sent. Nb tokens Acc. (%)
MElt10 10 189 66.5
en
MElt50 50 1,254 81.6
en
MElt100 100 2,774 86.7
en
MElt500 500 12,630 92.1
en
MElt1000 1,000 25,994 93.6
en
MElt5000 5,000 126,376 95.8
en
MElt10000 10,000 252,416 96.2
en
MEltALL 37,990 944,859 96.4
en
</table>
<tableCaption confidence="0.9591105">
Table 1: Accuracy of the created taggers evaluated
on Section 23 of the Penn Treebank
</tableCaption>
<subsectionHeader confidence="0.990916">
3.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999438132075472">
We designed different experimental setups to
evaluate the impact of pre-annotation and pre-
annotation accuracy on the quality of the resulting
corpus. The subparts of Section 23 that we used
for these experiments are identified by sentence
ids (e.g., 1–100 denotes the 100 first sentences in
Section 23).
Two annotators were involved in the experi-
ments. They both have a good knowledge of lin-
guistics, without being linguists themselves and
had only little prior knowledge of the Penn Tree-
bank POS tagset. One of them had previous exper-
tise in POS tagging (Annotator1). It should also
be noticed that, though they speak fluent English,
they are not native speakers of the language. They
were asked to keep track of their annotation time,
noting the time it took them to annotate or correct
each series of 10 sentences. They were also asked
to use only a basic text editor, with no macro or
specific feature that could help them, apart from
2More precisely, MElt�en is trained on the i first sentences
of the overall training corpus, i.e. Sections 2 to 21.
the usual ones, like Find, Replace, etc. The set
of 36 tags used in the Penn Treebank and quite
a number of particular cases is a lot to keep in
mind. This implies a heavy cognitive load in short-
term memory, especially as no specific interface
was used to help annotating or correcting the pre-
annotations.
It was demonstrated that training improves
the quality of manual annotation in a significant
way as well as allows for a significant gain in
time (Marcus et al., 1993; Dandapat et al., 2009;
Mikulov´a and ˇSt˘ep´anek, 2009). In particular, Mar-
cus et al. (1993) observed that it took the Penn
Treebank annotators 1 month to get fully efficient
on the POS-tagging correction task, reaching a
speed of 20 minutes per 1,000 words. The speed of
annotation in our experiments cannot be compared
to this, as our annotators only annotated and cor-
rected small samples of the Penn Treebank. How-
ever, the annotators’ speed and correctness did
improve with practice. As explained below, we
took this learning curve into account, as previous
work (Rehbein et al., 2009) showed it has an sig-
nificant impact on the results.
Also, during each experiment, sentences were
annotated sequentially. Moreover, the experiments
were conducted in the order we describe them be-
low. For example, both annotators started their
first annotation task (sentences 1–100) with sen-
tence 1.
We conducted the following experiments:
</bodyText>
<listItem confidence="0.871817894736842">
1. Impact of the pre-annotation accuracy on
precision and inter-annotator agreement:
In this experiment, we used sentences 1–
400 with random pre-annotation: for each
sentence, one pre-annotation is randomly
selected among its possible pre-annotations
(one for each tagger instance). The aim of
this is to eliminate the bias caused by the an-
notators’ learning curve. Annotation time for
each series of 10 consecutive sentences was
gathered, as well as precision w.r.t. the refer-
ence and inter-annotator agreement (both an-
notators annotated sentences 1–100 and 301–
400, while only one annotated 101–200 and
the other 201–300).
2. Impact of the pre-annotation accuracy on
annotation time: This experiment is based
on sentences 601–760, with pre-annotation.
We divided them in series of 10 sentences.
</listItem>
<page confidence="0.993044">
58
</page>
<bodyText confidence="0.999786">
For each series, one pre-annotation is se-
lected (i.e., the pre-annotation produced by
one of the 8 taggers), in such a way that each
pre-annotation is used for 2 series. We mea-
sured the manual annotation time for each se-
ries and each annotator.
</bodyText>
<listItem confidence="0.5904895">
3. Bias induced by pre-annotation: In this
experiment, both annotators annotated sen-
</listItem>
<bodyText confidence="0.983720428571429">
tences 451–500 fully manually.3 Later,
they annotated sentences 451–475 with the
pre-annotation from MEltALL
en (the best tag-
ger) and sentences 476–500 with the pre-
annotation from MElt50 (the second-worst
en
tagger). We then compared the fully man-
ual annotations with those based on pre-
annotations to check if and how they diverge
from the Penn Treebank “gold-standard”; we
also compared annotation times, in order to
get a confirmation of the gain in time ob-
served in previous experiments.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.899635">
4.1 Impact of the Pre-annotation Accuracy
</subsectionHeader>
<bodyText confidence="0.965382631578948">
on Precision and Inter-annotator
Agreement
The quality of the annotations created during ex-
periment 1 was evaluated using two methods.
First, we considered the original Penn Treebank
annotations as reference and calculated a simple
precision as compared to this reference. Figure 1
gives an overview of the obtained results (note that
the scale is not regular).
However, this is not sufficient to evaluate the
quality of the annotation as, actually, the reference
annotation is not perfect (see below). We therefore
evaluated the reliability of the annotation, calcu-
lating the inter-annotator agreement between An-
notator1 and Annotator2 on the 100-sentence se-
ries they both annotated. We calculated this agree-
ment on some of the subcorpora using 7r, aka Car-
letta’s Kappa (Carletta, 1996)4. The results of this
are shown in table 2.
</bodyText>
<footnote confidence="0.8140446">
3During this manual annotation step (with no pre-
annotation), we noticed that the annotators used the
Find/Replace all feature of the text editor to fasten
the tagging of some obvious tokens like the or Corp., which
partly explains that the first groups of 10 sentences took
longer to annotate. Also, as no specific interface was use to
help annotating, a (very) few typographic errors were made,
such as DET instead of DT.
4For more information on the terminology issue, refer to
the introduction of (Artstein and Poesio, 2008).
</footnote>
<table confidence="0.805190666666667">
Subcorpus 7r
1-100 0.955
301-400 0.963
</table>
<tableCaption confidence="0.996272">
Table 2: Inter-annotator agreement on subcorpora
</tableCaption>
<bodyText confidence="0.9998045">
The results show a very good agreement accord-
ing to all scales (Krippendorff, 1980; Neuendorf,
2002; Krippendorff, 2004) as 7r is always superior
to 0.9. Besides, it improves with training (from
0.955 at the beginning to 0.963 at the end).
We also calculated 7r on the corpus we used to
evaluate the pre-annotation bias (Experiment 3).
The results of this are shown in table 3.
</bodyText>
<table confidence="0.998919833333333">
Subcorpus Nb sent. it
No pre-annotation 50 0.947
MElt50 25 0.944
en
MEltALL 25 0.983
en
</table>
<tableCaption confidence="0.9926535">
Table 3: Inter-annotator agreement on subcorpora
used to evaluate bias
</tableCaption>
<bodyText confidence="0.952744142857143">
Here again, the results are very good, though a
little bit less so than at the beginning of the mixed
annotation session. They are almost perfect with
MEltALL
en .
Finally, we calculated 7r throughout Experi-
ment 2. The results are given in Figure 2 and,
apart from a bizarre peak at MElt50
en, they show a
steady progression of the accuracy and the inter-
annotator agreement, which are correlated. As for
the MElt50
en peak, it does not appear in Figure 1, we
therefore interpret it as an artifact.
</bodyText>
<subsectionHeader confidence="0.991397">
4.2 Impact of the Pre-annotation Accuracy
on Annotation Time
</subsectionHeader>
<bodyText confidence="0.951607105263158">
Before discussing the results of Experiment 2, an-
notation time measurements during Experiment 3
confirm that using a good quality pre-annotation
(say, MEltALL) strongly reduces the annotation
en
time as compared with fully manual annotation.
For example, Annotator1 needed an average time
of approximately 7.5 minutes to annotate 10 sen-
tences without pre-annotation (Experiment 3),
whereas Experiment 2 shows that it goes down to
approximately 2.5 minutes when using MEltALL
en
pre-annotation. For Annotator2, the correspond-
ing figures are respectively 11.5 and 2.5 minutes.
Figure 3 shows the impact on the pre-annotation
type on annotation times. Surprisingly, only the
worst tagger (MElt10) produces pre-annotations
en
that lead to a significantly slower annotation. In
</bodyText>
<page confidence="0.998653">
59
</page>
<figureCaption confidence="0.999987">
Figure 1: Accuracy of annotation
</figureCaption>
<bodyText confidence="0.999980481481481">
other words, a 96.4% accurate pre-annotation does
not significantly speed up the annotation process
with respect to a 81.6% accurate pre-annotation.
This is very interesting, since it could mean that
the development of a POS-annotated corpus for a
new language with no POS tagger could be drasti-
cally sped up. Annotating approximately 50 sen-
tences could be sufficient to train a POS tagger
such as MElt and use it as a pre-annotator, even
though its quality is not yet satisfying.
One interpretation of this could be the follow-
ing. Annotation based on pre-annotations involves
two different tasks: reading the pre-annotated sen-
tence and replacing incorrect tags. The reading
task takes a time that does not really depends on
the pre-annotation quality. But the correction task
takes a time that is, say, linear w.r.t. the num-
ber of pre-annotation errors. Therefore, when the
number of pre-annotation errors is below a cer-
tain level, the correction task takes significantly
less time than the reading task. Therefore, be-
low this level, variations in the pre-annotation er-
ror rate do not lead to significant overall annota-
tion time. Apparently, this threshold is between
66.5% and 81.6% pre-annotation accuracy, which
can be reached with a surprisingly small training
corpus.
</bodyText>
<subsectionHeader confidence="0.998177">
4.3 Bias Induced by Pre-annotation
</subsectionHeader>
<bodyText confidence="0.99664075">
We evaluated both the bias induced by a pre-
annotation with the best tagger, MEltALL
en , and the
one induced by one of the least accurate taggers,
</bodyText>
<equation confidence="0.477613">
MElt50
</equation>
<bodyText confidence="0.998961416666666">
en. The results are given in table 4 and 5, re-
spectively.
They show a very different bias according to
the annotator. Annotator2’s accuracy raises from
94.6% to 95.2% with a 81.6% accuracy tagger
(MElt50
en) and from 94.1% to 97.1% with a 96.4%
accuracy tagger (MEltALL
en ). Therefore, Annota-
tor2, whose accuracy is less than that of Annota-
tor1 under all circumstances (see figure 1), seems
to be positively influenced by pre-annotation,
whether it be good or bad. The gain is however
much more salient with the best pre-annotation
(plus 3 points).
As for Annotator1, who is the most accurate an-
notator (see figure 1), the results are more surpris-
ing as they show a significant degradation of ac-
curacy, from 98.1 without pre-annotation to 95.8
with pre-annotation using MElt50
en, the less accu-
rate tagger. Examining the actual results allowed
us to see that, first, Annotator1 non pre-annotated
version is better than the reference, and second,
the errors made in the pre-annotated version with
MElt50
en are so obvious that they can only be due to
a lapse in concentration.
The results, however, remain stable with pre-
annotation using the best tagger (from 98.4 to
98.2), which is consistent with the results obtained
by Dandapat et al. (2009), who showed that bet-
ter trained annotators are less influenced by pre-
annotation and show stable performance.
When asked about it, both annotators say
they felt they concentrated more without pre-
</bodyText>
<page confidence="0.9839625">
60
61
</page>
<figure confidence="0.9571815">
JJ
36 4
2
WDT
</figure>
<figureCaption confidence="0.957704">
Figure 2: Annotation accuracy and 7r depending on the type of pre-annotation
</figureCaption>
<table confidence="0.99747525">
Annotator No pre-annotation with MElt�LL
�n
Annotator1 98.4 98.2
Annotator2 94.1 97.1
</table>
<tableCaption confidence="0.648017">
Table 4: Accuracy with or without pre-annotation
</tableCaption>
<table confidence="0.989181333333333">
with MEltALL
en (sentences 451-475)
Annotator No pre-annotation with MElt�0
�n
Annotator1 98.1 95.8
Annotator2 94.6 95.2
</table>
<tableCaption confidence="0.8890085">
Table 5: Accuracy with or without pre-annotation
with MElt50 (sentences 476-500)
</tableCaption>
<bodyText confidence="0.970454454545455">
en
annotation. It seems that the rather good results
of the taggers cause the attention of the annotators
to be reduced, even more so as the task is repeti-
tive and tedious. However, annotators also had the
feeling that fully manual annotation could be more
subject to oversights.
These impressions are confirmed by the com-
parison of the contingency tables, as can be seen
from Tables 6, 7 and 8 (in these tables, lines cor-
respond to tags from the annotation and columns
to reference tags; only lines containing at least
one cell with 2 errors or more are shown, with
all corresponding columns). For example, Anno-
tator1 makes more random errors when no pre-
annotation is available and more systematic er-
rors when MEltALL
en pre-annotations are used (typ-
ically, JJ instead of VBN, i.e., adjective instead of
past participle, which corresponds to a systematic
trend in MEltALL
en ’s results).
</bodyText>
<tableCaption confidence="0.689856">
Table 6: Excerpts of the contingency tables for
</tableCaption>
<figure confidence="0.952746055555556">
sentences 451–457 (512 tokens) with MEltALL
en
pre-annotation
IN JJ NN NNP NNS RB VBD VBN
30 2 2
1 2 40
2 16
1 17 2
(Annotator 1)
JJ NN RB VBN
28 3
2 75 1
2 16
2 10
(Annotator 2)
Table 7: Excerpts of the contingency tables for
sentences 476–500 (523 tokens) with MElt50 enpre-
annotation
JJ VBN
(Annotator 1)
36 4
JJ NN NNP NNPS VB VBN
1 68 2
24 2
(Annotator 2)
JJ
NNS
RB
VBD
JJ
NN
RB
VBN
JJ
NN
NNP
</figure>
<figureCaption confidence="0.99998">
Figure 3: Annotation time depending on the type of pre-annotation
</figureCaption>
<bodyText confidence="0.995898125">
intend to use this method in a near future to de-
velop a POS tagger for Sorani Kurdish.
We also want to experiment on other, more
precision-driven, annotation tasks, like complex
relations annotation or definition segmentation,
that are more intrinsically complex and for which
there exist no automatic tool as accurate as for
POS tagging.
</bodyText>
<figure confidence="0.975822111111111">
CD DT JJ NN NNP NNS
CD 30 2
JJ 2 72
NN 2 148
NNS 3 68
(Annotator 1)
CD DT IN JJ JJR NN NNP NNS RB VBN
IN
JJ
104 2
2 61 2 1 9
1 4 145
2
NN
NNPS
1 2 68
2
(Annotator 2)
</figure>
<figureCaption confidence="0.395496333333333">
Table 8: Excerpts of the contingency tables for
sentences 450–500 (1,035 tokens) without pre-
annotation
</figureCaption>
<sectionHeader confidence="0.982172" genericHeader="conclusions">
5 Conclusion and Further Work
</sectionHeader>
<bodyText confidence="0.999936384615385">
The series of experiments we detailed in this arti-
cle confirms that pre-annotation allows for a gain
in quality, both in terms of accuracy w.r.t. a ref-
erence and in terms of inter-annotator agreement,
i.e., reliability. We also demonstrated that this
comes with biases that should be identified and
notified to the annotators, so that they can be extra
careful during correction. Finally, we discovered
that a surprisingly small training corpus could be
sufficient to build a pre-annotation tool that would
help drastically speeding up the annotation.
This should help developing taggers for under-
resourced languages. In order to check that, we
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999889333333333">
This work was partly realized as part of the
Quaero Programme5, funded by OSEO, French
State agency for innovation.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994405625">
Beatrice Alex, Claire Grover, Barry Haddow, Mijail
Kabadjov, Ewan Klein, Michael Matthews, Stu-
art Roebuck, Richard Tobin, and Xinglong Wang.
2008. Assisted Curation: Does Text Mining Really
Help? In Pacific Symposium on Biocomputing.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
Agreement for Computational Linguistics. Compu-
tational Linguistics, 34(4):555–596.
Lucie Barque, Alexis Nasr, and Alain Polgu`ere. 2010.
From the Definitions of the Tr´esor de la Langue
Franc¸aise to a Semantic Database of the French Lan-
guage. In Proceedings of the 14th EURALEX Inter-
national Congress, Leeuwarden.
Jean Carletta. 1996. Assessing Agreement on Classi-
fication Tasks: the Kappa Statistic. Computational
Linguistics, 22:249–254.
</reference>
<footnote confidence="0.830418">
5http://quaero.org/
</footnote>
<sectionHeader confidence="0.25672" genericHeader="acknowledgments">
NNS
RBR
</sectionHeader>
<page confidence="0.993138">
62
</page>
<reference confidence="0.998924117647059">
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic
Annotation - No Easy Way Out! a Case from Bangla
and Hindi POS Labeling Tasks. In Proceedings of
the third ACL Linguistic Annotation Workshop.
Pascal Denis and Benoit Sagot. 2009. Coupling an An-
notated Corpus and a Morphosyntactic Lexicon for
State-of-the-art POS Tagging with Less Human Ef-
fort. In Proceedings of PACLIC 2009, Hong-Kong,
China.
Kar¨en Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Vers une m´ethodologie d’annotation des
entit´es nomm´ees en corpus ? In Actes de la
16`eme Conf´erence sur le Traitement Automatique
des Langues Naturelles 2009 Traitement Automa-
tique des Langues Naturelles 2009, Senlis, France.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12. Sage, Bev-
erly Hills, CA.
Klaus Krippendorff, 2004. Content Analysis: An In-
troduction to Its Methodology, second edition, chap-
ter 11. Sage, Thousand Oaks, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Marie Mikulov´a and Jan ˇSt˘ep´anek. 2009. Annotation
Quality Checking and its Implications for Design
of Treebank (in Building the Prague Czech-English
Dependency Treebank). In Proceedings of the Eight
International Workshop on Treebanks and Linguistic
Theories, volume 4-5, Milan, Italy, December.
Kimberly Neuendorf. 2002. The Content Analysis
Guidebook. Sage, Thousand Oaks CA.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-Of-Speech Tagging. In Proceedings
of International Conference on Empirical Methods
in Natural Language Processing, pages 133–142.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the Benefits of Partial
Automatic Pre-labeling for Frame-semantic Anno-
tation. In Proceedings of the Third Linguistic Anno-
tation Workshop, pages 19–26, Suntec, Singapore,
August. Association for Computational Linguistics.
Drahomira “Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
Training for the Averaged Perceptron POS Tagger.
In EACL ’09: Proceedings of the 12th Conference
of the European Chapter of the Association for Com-
putational Linguistics, pages 763–771, Morristown,
NJ, USA.
</reference>
<page confidence="0.999459">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651756">
<title confidence="0.99789">Influence of Pre-annotation on POS-tagged Corpus Development</title>
<author confidence="0.981162">Kar¨en Fort Benoit Sagot</author>
<note confidence="0.9249845">INIST CNRS / LIPN INRIA Paris-Rocquencourt / Paris 7 Nancy / Paris, France. Paris, France.</note>
<email confidence="0.785918">karen.fort@inist.frbenoit.sagot@inria.fr</email>
<abstract confidence="0.9977069">This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beatrice Alex</author>
<author>Claire Grover</author>
<author>Barry Haddow</author>
<author>Mijail Kabadjov</author>
<author>Ewan Klein</author>
<author>Michael Matthews</author>
<author>Stuart Roebuck</author>
<author>Richard Tobin</author>
<author>Xinglong Wang</author>
</authors>
<title>Assisted Curation: Does Text Mining Really Help? In Pacific Symposium on Biocomputing.</title>
<date>2008</date>
<contexts>
<context position="4332" citStr="Alex et al. (2008)" startWordPosition="689" endWordPosition="692">on Workshop, ACL 2010, pages 56–63, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics tency, which depends largely on the pre-tagging quality. They also noticed that untrained annotators were more influenced by pre-annotation than the trained ones, who showed “consistent performance”. However, this very complete and interesting experiment lacked a reference allowing for an evaluation of the quality of the annotations. Besides, it only took into account two types of pretagging quality, high accuracy and low accuracy. 2.2 Pre-annotation in Other Annotation Tasks Alex et al. (2008) led some experiments in the biomedical domain, within the framework of a “curation” task of protein-protein interaction. Curation consists in reading through electronic version of papers and entering retrieved information into a template. They showed that perfectly preannotating the corpus leads to a reduction of more than 1/3 in curation time, as well as a better recall from the annotators. Less perfect pre-annotation still leads to a gain in time, but less so (a little less than 1/4th). They also tested the effect of higher recall or precision of pre-annotation on one annotator (curator), w</context>
</contexts>
<marker>Alex, Grover, Haddow, Kabadjov, Klein, Matthews, Roebuck, Tobin, Wang, 2008</marker>
<rawString>Beatrice Alex, Claire Grover, Barry Haddow, Mijail Kabadjov, Ewan Klein, Michael Matthews, Stuart Roebuck, Richard Tobin, and Xinglong Wang. 2008. Assisted Curation: Does Text Mining Really Help? In Pacific Symposium on Biocomputing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder Agreement for Computational Linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="14992" citStr="Artstein and Poesio, 2008" startWordPosition="2432" endWordPosition="2435">orpora using 7r, aka Carletta’s Kappa (Carletta, 1996)4. The results of this are shown in table 2. 3During this manual annotation step (with no preannotation), we noticed that the annotators used the Find/Replace all feature of the text editor to fasten the tagging of some obvious tokens like the or Corp., which partly explains that the first groups of 10 sentences took longer to annotate. Also, as no specific interface was use to help annotating, a (very) few typographic errors were made, such as DET instead of DT. 4For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). Subcorpus 7r 1-100 0.955 301-400 0.963 Table 2: Inter-annotator agreement on subcorpora The results show a very good agreement according to all scales (Krippendorff, 1980; Neuendorf, 2002; Krippendorff, 2004) as 7r is always superior to 0.9. Besides, it improves with training (from 0.955 at the beginning to 0.963 at the end). We also calculated 7r on the corpus we used to evaluate the pre-annotation bias (Experiment 3). The results of this are shown in table 3. Subcorpus Nb sent. it No pre-annotation 50 0.947 MElt50 25 0.944 en MEltALL 25 0.983 en Table 3: Inter-annotator agreement on subcor</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder Agreement for Computational Linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucie Barque</author>
<author>Alexis Nasr</author>
<author>Alain Polgu`ere</author>
</authors>
<title>From the Definitions of the Tr´esor de la Langue Franc¸aise to a Semantic Database of the French Language.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th EURALEX International Congress,</booktitle>
<location>Leeuwarden.</location>
<marker>Barque, Nasr, Polgu`ere, 2010</marker>
<rawString>Lucie Barque, Alexis Nasr, and Alain Polgu`ere. 2010. From the Definitions of the Tr´esor de la Langue Franc¸aise to a Semantic Database of the French Language. In Proceedings of the 14th EURALEX International Congress, Leeuwarden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: the Kappa Statistic. Computational Linguistics,</title>
<date>1996</date>
<pages>22--249</pages>
<contexts>
<context position="14420" citStr="Carletta, 1996" startWordPosition="2337" endWordPosition="2338">n Treebank annotations as reference and calculated a simple precision as compared to this reference. Figure 1 gives an overview of the obtained results (note that the scale is not regular). However, this is not sufficient to evaluate the quality of the annotation as, actually, the reference annotation is not perfect (see below). We therefore evaluated the reliability of the annotation, calculating the inter-annotator agreement between Annotator1 and Annotator2 on the 100-sentence series they both annotated. We calculated this agreement on some of the subcorpora using 7r, aka Carletta’s Kappa (Carletta, 1996)4. The results of this are shown in table 2. 3During this manual annotation step (with no preannotation), we noticed that the annotators used the Find/Replace all feature of the text editor to fasten the tagging of some obvious tokens like the or Corp., which partly explains that the first groups of 10 sentences took longer to annotate. Also, as no specific interface was use to help annotating, a (very) few typographic errors were made, such as DET instead of DT. 4For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). Subcorpus 7r 1-100 0.955 3</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing Agreement on Classification Tasks: the Kappa Statistic. Computational Linguistics, 22:249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Dandapat</author>
<author>Priyanka Biswas</author>
<author>Monojit Choudhury</author>
<author>Kalika Bali</author>
</authors>
<title>Complex Linguistic Annotation - No Easy Way Out! a Case from Bangla and Hindi POS Labeling Tasks.</title>
<date>2009</date>
<booktitle>In Proceedings of the third ACL Linguistic Annotation Workshop.</booktitle>
<contexts>
<context position="803" citStr="Dandapat et al., 2009" startWordPosition="116" endWordPosition="119">.fort@inist.fr benoit.sagot@inria.fr Abstract This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 Introduction Training a machine-learning based part-of-speech (POS) tagger implies manually tagging a significant amount of text. The cost of this, in terms of human effort, slows down the development of taggers for under-resourced languages. One usual way to improve this situation is to automatically pre-annotate the corpus, so that the work of the annotators is limited to the validation of this pre-an</context>
<context position="3502" citStr="Dandapat et al. (2009)" startWordPosition="562" endWordPosition="565">twice as long as correcting pre-tagged text and resulted in twice the inter-annotator disagreement rate, as well as an error rate (using a gold-standard annotation) about 50% higher. The pre-annotation was done using a tagger trained on the Brown Corpus, which, due to errors introduced by an automatic mapping of tags from the Brown tagset to the Penn Treebank tagset, had an error rate of 7–9%. However, they report neither the influence of the training of the annotators on the potential biases in correction, nor that of the quality of the tagger on the correction time and the obtained quality. Dandapat et al. (2009) went further and showed that, for complex POS-tagging (for Hindi and Bangla), pre-annotation of the corpus allows for a gain in time, but not necessarily in consis56 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 56–63, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics tency, which depends largely on the pre-tagging quality. They also noticed that untrained annotators were more influenced by pre-annotation than the trained ones, who showed “consistent performance”. However, this very complete and interesting experiment lacked a refer</context>
<context position="11037" citStr="Dandapat et al., 2009" startWordPosition="1802" endWordPosition="1805">part from 2More precisely, MElt�en is trained on the i first sentences of the overall training corpus, i.e. Sections 2 to 21. the usual ones, like Find, Replace, etc. The set of 36 tags used in the Penn Treebank and quite a number of particular cases is a lot to keep in mind. This implies a heavy cognitive load in shortterm memory, especially as no specific interface was used to help annotating or correcting the preannotations. It was demonstrated that training improves the quality of manual annotation in a significant way as well as allows for a significant gain in time (Marcus et al., 1993; Dandapat et al., 2009; Mikulov´a and ˇSt˘ep´anek, 2009). In particular, Marcus et al. (1993) observed that it took the Penn Treebank annotators 1 month to get fully efficient on the POS-tagging correction task, reaching a speed of 20 minutes per 1,000 words. The speed of annotation in our experiments cannot be compared to this, as our annotators only annotated and corrected small samples of the Penn Treebank. However, the annotators’ speed and correctness did improve with practice. As explained below, we took this learning curve into account, as previous work (Rehbein et al., 2009) showed it has an significant imp</context>
<context position="19696" citStr="Dandapat et al. (2009)" startWordPosition="3202" endWordPosition="3205">results are more surprising as they show a significant degradation of accuracy, from 98.1 without pre-annotation to 95.8 with pre-annotation using MElt50 en, the less accurate tagger. Examining the actual results allowed us to see that, first, Annotator1 non pre-annotated version is better than the reference, and second, the errors made in the pre-annotated version with MElt50 en are so obvious that they can only be due to a lapse in concentration. The results, however, remain stable with preannotation using the best tagger (from 98.4 to 98.2), which is consistent with the results obtained by Dandapat et al. (2009), who showed that better trained annotators are less influenced by preannotation and show stable performance. When asked about it, both annotators say they felt they concentrated more without pre60 61 JJ 36 4 2 WDT Figure 2: Annotation accuracy and 7r depending on the type of pre-annotation Annotator No pre-annotation with MElt�LL �n Annotator1 98.4 98.2 Annotator2 94.1 97.1 Table 4: Accuracy with or without pre-annotation with MEltALL en (sentences 451-475) Annotator No pre-annotation with MElt�0 �n Annotator1 98.1 95.8 Annotator2 94.6 95.2 Table 5: Accuracy with or without pre-annotation wit</context>
</contexts>
<marker>Dandapat, Biswas, Choudhury, Bali, 2009</marker>
<rawString>Sandipan Dandapat, Priyanka Biswas, Monojit Choudhury, and Kalika Bali. 2009. Complex Linguistic Annotation - No Easy Way Out! a Case from Bangla and Hindi POS Labeling Tasks. In Proceedings of the third ACL Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Coupling an Annotated Corpus and a Morphosyntactic Lexicon for State-of-the-art POS Tagging with Less Human Effort.</title>
<date>2009</date>
<booktitle>In Proceedings of PACLIC 2009, Hong-Kong,</booktitle>
<contexts>
<context position="8016" citStr="Denis and Sagot, 2009" startWordPosition="1293" endWordPosition="1296"> is the following. We split the Penn Treebank corpus (Marcus et al., 1993) in a usual manner, namely we use Sections 2 to 21 to train various instances of a POS tagger, and Section 23 to perform the actual experiments. In order to measure the impact of the POS tagger’s quality, we trained it on subcorpora of increasing sizes, and pre-annotated Section 23 with these various POS taggers. Then, we manually annotated parts of Section 23 under various experimental setups, either from scratch or using various pre-annotations, as explained below. 3.1 Creating the Taggers We used the MElt POS tagger (Denis and Sagot, 2009), a maximum-entropy based system that is able to take into account both information extracted from a training corpus and information extracted from an external morphological lexicon.1 It has been shown to lead to a state-of-the-art POS tagger for French. Trained on Sections 2 to 21 1MElt is freely available under LGPL license, on the web page of its hosting project (http://gforge.inria. fr/projects/lingwb/). 57 of the Penn Treebank (MEltALL en ), and evaluated on Section 23, MElt exhibits a 96.4% accuracy, which is reasonably close to the state-of-the-art (Spoustov´a et al. (2009) report 97.4%</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>Pascal Denis and Benoit Sagot. 2009. Coupling an Annotated Corpus and a Morphosyntactic Lexicon for State-of-the-art POS Tagging with Less Human Effort. In Proceedings of PACLIC 2009, Hong-Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kar¨en Fort</author>
<author>Maud Ehrmann</author>
<author>Adeline Nazarenko</author>
</authors>
<title>Vers une m´ethodologie d’annotation des entit´es nomm´ees en corpus ?</title>
<date>2009</date>
<booktitle>In Actes de la 16`eme Conf´erence sur le Traitement Automatique des Langues Naturelles 2009 Traitement Automatique des Langues Naturelles</booktitle>
<location>Senlis, France.</location>
<contexts>
<context position="6082" citStr="Fort et al. (2009)" startWordPosition="973" endWordPosition="976">s are a bit disappointing as they could not find a direct improvement of annotation time using pre-annotation. The authors reckon this might be at least partly due to “an interaction between time savings from pre-annotation and time savings due to a training effect.” For the same reason, they had to exclude some of the annotation results for quality evaluation in order to show that, in line with (Marcus et al., 1993), quality pre-annotation helps increasing annotation quality. They also found that noisy and low quality pre-annotation does not overall corrupt human judgment. On the other hand, Fort et al. (2009) claim that pre-annotation introduces a bias in named entity annotation, due to the preference given by annotators to what is already annotated, thus preventing them from noticing entities that were not preannotated. This particular type of bias should not appear in POS-tagging, as all the elements are to be annotated, but a pre-tagging could influence the annotators, preventing them from asking themselves questions about a specific pre-annotation. In a completely different field, Barque et al. (2010) used a series of NLP tools, called MACAON, to automatically identify the central component an</context>
</contexts>
<marker>Fort, Ehrmann, Nazarenko, 2009</marker>
<rawString>Kar¨en Fort, Maud Ehrmann, and Adeline Nazarenko. 2009. Vers une m´ethodologie d’annotation des entit´es nomm´ees en corpus ? In Actes de la 16`eme Conf´erence sur le Traitement Automatique des Langues Naturelles 2009 Traitement Automatique des Langues Naturelles 2009, Senlis, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology, chapter 12. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="15164" citStr="Krippendorff, 1980" startWordPosition="2460" endWordPosition="2461">annotators used the Find/Replace all feature of the text editor to fasten the tagging of some obvious tokens like the or Corp., which partly explains that the first groups of 10 sentences took longer to annotate. Also, as no specific interface was use to help annotating, a (very) few typographic errors were made, such as DET instead of DT. 4For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). Subcorpus 7r 1-100 0.955 301-400 0.963 Table 2: Inter-annotator agreement on subcorpora The results show a very good agreement according to all scales (Krippendorff, 1980; Neuendorf, 2002; Krippendorff, 2004) as 7r is always superior to 0.9. Besides, it improves with training (from 0.955 at the beginning to 0.963 at the end). We also calculated 7r on the corpus we used to evaluate the pre-annotation bias (Experiment 3). The results of this are shown in table 3. Subcorpus Nb sent. it No pre-annotation 50 0.947 MElt50 25 0.944 en MEltALL 25 0.983 en Table 3: Inter-annotator agreement on subcorpora used to evaluate bias Here again, the results are very good, though a little bit less so than at the beginning of the mixed annotation session. They are almost perfect</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff, 1980. Content Analysis: An Introduction to Its Methodology, chapter 12. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology,</title>
<date>2004</date>
<location>Thousand Oaks, CA.</location>
<note>second edition, chapter 11. Sage,</note>
<contexts>
<context position="15202" citStr="Krippendorff, 2004" startWordPosition="2464" endWordPosition="2465">feature of the text editor to fasten the tagging of some obvious tokens like the or Corp., which partly explains that the first groups of 10 sentences took longer to annotate. Also, as no specific interface was use to help annotating, a (very) few typographic errors were made, such as DET instead of DT. 4For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). Subcorpus 7r 1-100 0.955 301-400 0.963 Table 2: Inter-annotator agreement on subcorpora The results show a very good agreement according to all scales (Krippendorff, 1980; Neuendorf, 2002; Krippendorff, 2004) as 7r is always superior to 0.9. Besides, it improves with training (from 0.955 at the beginning to 0.963 at the end). We also calculated 7r on the corpus we used to evaluate the pre-annotation bias (Experiment 3). The results of this are shown in table 3. Subcorpus Nb sent. it No pre-annotation 50 0.947 MElt50 25 0.944 en MEltALL 25 0.983 en Table 3: Inter-annotator agreement on subcorpora used to evaluate bias Here again, the results are very good, though a little bit less so than at the beginning of the mixed annotation session. They are almost perfect with MEltALL en . Finally, we calcula</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff, 2004. Content Analysis: An Introduction to Its Methodology, second edition, chapter 11. Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="780" citStr="Marcus et al., 1993" startWordPosition="112" endWordPosition="115"> Paris, France. karen.fort@inist.fr benoit.sagot@inria.fr Abstract This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 Introduction Training a machine-learning based part-of-speech (POS) tagger implies manually tagging a significant amount of text. The cost of this, in terms of human effort, slows down the development of taggers for under-resourced languages. One usual way to improve this situation is to automatically pre-annotate the corpus, so that the work of the annotators is limited to the va</context>
<context position="2723" citStr="Marcus et al., 1993" startWordPosition="430" endWordPosition="433">annotation with purely manual tagging, while minimizing all kinds of biases. We quantify the results in terms of error rate in the resulting annotated corpus, manual annotation time and inter-annotator agreement. This article is organized as follows. In Section 2, we mention some related work, while Section 3 describes the experimental setup, followed by a discussion on the obtained results (Section 4) and a conclusion. 2 Related Work 2.1 Pre-annotation for POS Tagging Very few manual annotation projects give details about the campaign itself. One major exception is the Penn Treebank project (Marcus et al., 1993), that provided detailed information about the manual annotation methodology, evaluation and cost. Marcus et al. (1993) thus showed that manual tagging took twice as long as correcting pre-tagged text and resulted in twice the inter-annotator disagreement rate, as well as an error rate (using a gold-standard annotation) about 50% higher. The pre-annotation was done using a tagger trained on the Brown Corpus, which, due to errors introduced by an automatic mapping of tags from the Brown tagset to the Penn Treebank tagset, had an error rate of 7–9%. However, they report neither the influence of </context>
<context position="5884" citStr="Marcus et al., 1993" startWordPosition="943" endWordPosition="946">ct frame assignment using a task-specific annotation tool. Here again, pre-annotation was done using only two types of pre-tagging quality, state-of-the-art and enhanced. The results of the experiments are a bit disappointing as they could not find a direct improvement of annotation time using pre-annotation. The authors reckon this might be at least partly due to “an interaction between time savings from pre-annotation and time savings due to a training effect.” For the same reason, they had to exclude some of the annotation results for quality evaluation in order to show that, in line with (Marcus et al., 1993), quality pre-annotation helps increasing annotation quality. They also found that noisy and low quality pre-annotation does not overall corrupt human judgment. On the other hand, Fort et al. (2009) claim that pre-annotation introduces a bias in named entity annotation, due to the preference given by annotators to what is already annotated, thus preventing them from noticing entities that were not preannotated. This particular type of bias should not appear in POS-tagging, as all the elements are to be annotated, but a pre-tagging could influence the annotators, preventing them from asking the</context>
<context position="7468" citStr="Marcus et al., 1993" startWordPosition="1199" endWordPosition="1202">low for a significant gain in time. The authors consider that the bad results are due to the quality of the tool that they wish to improve as they believe that “an automatic segmentation of better quality would surely yield some gains.” Yet, the question remains: is there a quality threshold for pre-annotation to be useful? and if so, how can we evaluate it? We tried to answer at least part of these questions for a quite simple task for which data is available: POS-tagging in English. 3 Experimental Setup The idea underlying our experiments is the following. We split the Penn Treebank corpus (Marcus et al., 1993) in a usual manner, namely we use Sections 2 to 21 to train various instances of a POS tagger, and Section 23 to perform the actual experiments. In order to measure the impact of the POS tagger’s quality, we trained it on subcorpora of increasing sizes, and pre-annotated Section 23 with these various POS taggers. Then, we manually annotated parts of Section 23 under various experimental setups, either from scratch or using various pre-annotations, as explained below. 3.1 Creating the Taggers We used the MElt POS tagger (Denis and Sagot, 2009), a maximum-entropy based system that is able to tak</context>
<context position="11014" citStr="Marcus et al., 1993" startWordPosition="1798" endWordPosition="1801">at could help them, apart from 2More precisely, MElt�en is trained on the i first sentences of the overall training corpus, i.e. Sections 2 to 21. the usual ones, like Find, Replace, etc. The set of 36 tags used in the Penn Treebank and quite a number of particular cases is a lot to keep in mind. This implies a heavy cognitive load in shortterm memory, especially as no specific interface was used to help annotating or correcting the preannotations. It was demonstrated that training improves the quality of manual annotation in a significant way as well as allows for a significant gain in time (Marcus et al., 1993; Dandapat et al., 2009; Mikulov´a and ˇSt˘ep´anek, 2009). In particular, Marcus et al. (1993) observed that it took the Penn Treebank annotators 1 month to get fully efficient on the POS-tagging correction task, reaching a speed of 20 minutes per 1,000 words. The speed of annotation in our experiments cannot be compared to this, as our annotators only annotated and corrected small samples of the Penn Treebank. However, the annotators’ speed and correctness did improve with practice. As explained below, we took this learning curve into account, as previous work (Rehbein et al., 2009) showed it</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Mikulov´a</author>
<author>Jan ˇSt˘ep´anek</author>
</authors>
<title>Annotation Quality Checking and its Implications for Design of Treebank (in Building the Prague Czech-English Dependency Treebank).</title>
<date>2009</date>
<booktitle>In Proceedings of the Eight International Workshop on Treebanks and Linguistic Theories,</booktitle>
<volume>volume</volume>
<pages>4--5</pages>
<location>Milan, Italy,</location>
<marker>Mikulov´a, ˇSt˘ep´anek, 2009</marker>
<rawString>Marie Mikulov´a and Jan ˇSt˘ep´anek. 2009. Annotation Quality Checking and its Implications for Design of Treebank (in Building the Prague Czech-English Dependency Treebank). In Proceedings of the Eight International Workshop on Treebanks and Linguistic Theories, volume 4-5, Milan, Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimberly Neuendorf</author>
</authors>
<title>The Content Analysis Guidebook. Sage, Thousand Oaks CA.</title>
<date>2002</date>
<contexts>
<context position="15181" citStr="Neuendorf, 2002" startWordPosition="2462" endWordPosition="2463">Find/Replace all feature of the text editor to fasten the tagging of some obvious tokens like the or Corp., which partly explains that the first groups of 10 sentences took longer to annotate. Also, as no specific interface was use to help annotating, a (very) few typographic errors were made, such as DET instead of DT. 4For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). Subcorpus 7r 1-100 0.955 301-400 0.963 Table 2: Inter-annotator agreement on subcorpora The results show a very good agreement according to all scales (Krippendorff, 1980; Neuendorf, 2002; Krippendorff, 2004) as 7r is always superior to 0.9. Besides, it improves with training (from 0.955 at the beginning to 0.963 at the end). We also calculated 7r on the corpus we used to evaluate the pre-annotation bias (Experiment 3). The results of this are shown in table 3. Subcorpus Nb sent. it No pre-annotation 50 0.947 MElt50 25 0.944 en MEltALL 25 0.983 en Table 3: Inter-annotator agreement on subcorpora used to evaluate bias Here again, the results are very good, though a little bit less so than at the beginning of the mixed annotation session. They are almost perfect with MEltALL en </context>
</contexts>
<marker>Neuendorf, 2002</marker>
<rawString>Kimberly Neuendorf. 2002. The Content Analysis Guidebook. Sage, Thousand Oaks CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging. In</title>
<date>1996</date>
<booktitle>Proceedings of International Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="8758" citStr="Ratnaparkhi, 1996" startWordPosition="1411" endWordPosition="1412">ormation extracted from an external morphological lexicon.1 It has been shown to lead to a state-of-the-art POS tagger for French. Trained on Sections 2 to 21 1MElt is freely available under LGPL license, on the web page of its hosting project (http://gforge.inria. fr/projects/lingwb/). 57 of the Penn Treebank (MEltALL en ), and evaluated on Section 23, MElt exhibits a 96.4% accuracy, which is reasonably close to the state-of-the-art (Spoustov´a et al. (2009) report 97.4%). Since it is trained without any external lexicon, MEltALL en is very close to the original maximum-entropy based tagger (Ratnaparkhi, 1996), which has indeed a similar 96.6% accuracy. We trained MElt on increasingly larger parts of the POS-tagged Penn Treebank,2 thus creating different taggers with growing degrees of accuracy (see table 1). We then POS-tagged the Section 23 with each of these taggers, thus obtaining for each sentence in Section 23 a set of pre-annotations, one from each tagger. Tagger Nb train. sent. Nb tokens Acc. (%) MElt10 10 189 66.5 en MElt50 50 1,254 81.6 en MElt100 100 2,774 86.7 en MElt500 500 12,630 92.1 en MElt1000 1,000 25,994 93.6 en MElt5000 5,000 126,376 95.8 en MElt10000 10,000 252,416 96.2 en MElt</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proceedings of International Conference on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
</authors>
<title>Assessing the Benefits of Partial Automatic Pre-labeling for Frame-semantic Annotation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Linguistic Annotation Workshop,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="826" citStr="Rehbein et al., 2009" startWordPosition="120" endWordPosition="123">agot@inria.fr Abstract This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 Introduction Training a machine-learning based part-of-speech (POS) tagger implies manually tagging a significant amount of text. The cost of this, in terms of human effort, slows down the development of taggers for under-resourced languages. One usual way to improve this situation is to automatically pre-annotate the corpus, so that the work of the annotators is limited to the validation of this pre-annotation. This method p</context>
<context position="5118" citStr="Rehbein et al. (2009)" startWordPosition="818" endWordPosition="821">onic version of papers and entering retrieved information into a template. They showed that perfectly preannotating the corpus leads to a reduction of more than 1/3 in curation time, as well as a better recall from the annotators. Less perfect pre-annotation still leads to a gain in time, but less so (a little less than 1/4th). They also tested the effect of higher recall or precision of pre-annotation on one annotator (curator), who rated recall more positively than precision. However, as they notice, this result can be explained by the curation style and should be tested on more annotators. Rehbein et al. (2009) led quite thorough experiments on the subject, in the field of semantic frame assignment annotation. They asked 6 annotators to annotate or correct frame assignment using a task-specific annotation tool. Here again, pre-annotation was done using only two types of pre-tagging quality, state-of-the-art and enhanced. The results of the experiments are a bit disappointing as they could not find a direct improvement of annotation time using pre-annotation. The authors reckon this might be at least partly due to “an interaction between time savings from pre-annotation and time savings due to a trai</context>
<context position="11604" citStr="Rehbein et al., 2009" startWordPosition="1895" endWordPosition="1898">in in time (Marcus et al., 1993; Dandapat et al., 2009; Mikulov´a and ˇSt˘ep´anek, 2009). In particular, Marcus et al. (1993) observed that it took the Penn Treebank annotators 1 month to get fully efficient on the POS-tagging correction task, reaching a speed of 20 minutes per 1,000 words. The speed of annotation in our experiments cannot be compared to this, as our annotators only annotated and corrected small samples of the Penn Treebank. However, the annotators’ speed and correctness did improve with practice. As explained below, we took this learning curve into account, as previous work (Rehbein et al., 2009) showed it has an significant impact on the results. Also, during each experiment, sentences were annotated sequentially. Moreover, the experiments were conducted in the order we describe them below. For example, both annotators started their first annotation task (sentences 1–100) with sentence 1. We conducted the following experiments: 1. Impact of the pre-annotation accuracy on precision and inter-annotator agreement: In this experiment, we used sentences 1– 400 with random pre-annotation: for each sentence, one pre-annotation is randomly selected among its possible pre-annotations (one for</context>
</contexts>
<marker>Rehbein, Ruppenhofer, Sporleder, 2009</marker>
<rawString>Ines Rehbein, Josef Ruppenhofer, and Caroline Sporleder. 2009. Assessing the Benefits of Partial Automatic Pre-labeling for Frame-semantic Annotation. In Proceedings of the Third Linguistic Annotation Workshop, pages 19–26, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira “Johanka” Spoustov´a</author>
<author>Jan Hajiˇc</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised Training for the Averaged Perceptron POS Tagger.</title>
<date>2009</date>
<booktitle>In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>763--771</pages>
<location>Morristown, NJ, USA.</location>
<marker>Spoustov´a, Hajiˇc, Raab, Spousta, 2009</marker>
<rawString>Drahomira “Johanka” Spoustov´a, Jan Hajiˇc, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised Training for the Averaged Perceptron POS Tagger. In EACL ’09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 763–771, Morristown, NJ, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>