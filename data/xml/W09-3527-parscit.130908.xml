<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000158">
<title confidence="0.940697">
ǫ-extension Hidden Markov Models and Weighted Transducers for
Machine Transliteration
</title>
<author confidence="0.949919">
Balakrishnan Vardarajan Delip Rao
</author>
<affiliation confidence="0.869029">
Dept. of Electrical and Computer Engineering Dept. of Computer Science
Johns Hopkins University Johns Hopkins University
</affiliation>
<email confidence="0.998314">
bvarada2@jhu.edu delip@cs.jhu.edu
</email>
<sectionHeader confidence="0.994769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999705">
We describe in detail a method for translit-
erating an English string to a foreign
language string evaluated on five differ-
ent languages, including Tamil, Hindi,
Russian, Chinese, and Kannada. Our
method involves deriving substring align-
ments from the training data and learning a
weighted finite state transducer from these
alignments. We define an c-extension Hid-
den Markov Model to derive alignments
between training pairs and a heuristic to
extract the substring alignments. Our
method involves only two tunable parame-
ters that can be optimized on held-out data.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946789473684">
Transliteration is a letter by letter mapping of one
writing system to another. Apart from the obvi-
ous use in writing systems, transliteration is also
useful in conjunction with translation. For exam-
ple, machine translation BLEU scores are known
to improve when named entities are transliterated.
This engendered several investigations into auto-
matic transliteration of strings, named entities in
particular, from one language to another. See
Knight and Graehl(1997) and later papers on this
topic for an overview.
Hidden Markov Model (HMM) (Rabiner,
1989) is a standard sequence modeling tool used
in various problems in natural language process-
ing like machine translation, speech recognition,
part of speech tagging and information extraction.
There have been earlier attempts in using HMMs
for automatic transliteration. See (Abdul Jaleel
and Larkey, 2003; Zhou et al., 2008) for exam-
ple. In this paper, we define an c-extension Hid-
den Markov Model that allows us to align source
and target language strings such that the charac-
ters in the source string may be optionally aligned
to the c symbol. We also introduce a heuristic that
allows us to extract high quality sub-alignments
from the c-aligned word pairs. This allows us to
define a weighted finite state transducer that pro-
duces transliterations for an English string by min-
imal segmentation.
The overview of this paper is as follows: Sec-
tion 2 introduces c-extension Hidden Markov
Model and describes our alignment procedure.
Section 3 describes the substring alignment
heuristic and our weighted finite state transducer
to derive the final n-best transliterations. We con-
clude with a result section describing results from
the NEWS 2009 shared task on five different lan-
guages.
</bodyText>
<sectionHeader confidence="0.851872" genericHeader="method">
2 Learning Alignments
</sectionHeader>
<bodyText confidence="0.999233272727273">
The training data D is given as pairs of strings
(e, f) where e is the English string with the cor-
responding foreign transliteration f. The English
string e consists of a sequence of English letters
(e1, e2, ... , eN) while f = (f1, f2, ... , fm) .
We represent E as the set of all English symbols
and F as the set of all foreign symbols.1 We also
assume both languages have a special null symbol
c, that is c ∈ E and c ∈ F.
Our alignment model is a Hidden Markov
Model H(X, Y, S, T, P3), where
</bodyText>
<listItem confidence="0.990336875">
• X is the start state and Y is the end state.
• S is the set of emitting states with S = |S|.
The emitting states are indexed from 1 to S.
The start state X is indexed as state 0 and the
end state Y is indexed as state S + 1.
• T is an (S + 1) × (S + 1) stochastic matrix
with T = [tib] for i ∈ {0, 1,... , S} and j ∈
{1,2,... ,S + 1}.
</listItem>
<footnote confidence="0.747343">
1Alphabets and diacritics are treated as separate symbols.
</footnote>
<page confidence="0.850715">
120
</page>
<note confidence="0.9824205">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 120–123,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<listItem confidence="0.744786666666667">
• Ps = [pef] is an |£ |x |F |matrix of joint
emission probabilities with pef = P(e, f|s)
Vs E S.
</listItem>
<bodyText confidence="0.999873545454545">
We define s to be an ǫ-extension of a string of
characters s = (c1,c2,... ,ck) as the string ob-
tained by pumping an arbitrary number of ǫ sym-
bols between any two adjacent characters cl and
cl+1. That is, s = (di1, ... , di2, ... , dik) where
dij = cj and dl = ǫ for im &lt; l &lt; im+1 where
1 &lt; l &lt; k. Observe that there are countably infi-
nite ǫ-extensions for a given string s since an arbi-
trary number of ǫ symbols can be inserted between
characters cm and cm+1. Let T (s) denote the set
of all possible ǫ-extensions for a given string s.
For a given pair of strings (u, v), we define a
joint ǫ-extension of (u, v) as the pair (n, v) s.t. u� E
T (u) and v� E T (v) with |u |= |�v |and ∄i s.t.
�ui = vi = ǫ. Due to this restriction, there are finite
ǫ-extensions for a pair (u, v) with the length of u�
and v� bounded above by |u |+ |v|. 2 Let J(u, v)
denote the set of all joint ǫ-extensions of (u, v).
Given a pair of strings (e, f) with e =
(e1, e2, ... , eN) and f = (f1, f2, ... , fM), we
compute the probability α(e, f, s′) that they are
transliteration pairs ending in state s′ as
</bodyText>
<equation confidence="0.972180666666667">
α(e, f, s′) =
X X
(˜e,˜f)∈J(e,f) 0=s0,...,s|e|=s′
</equation>
<bodyText confidence="0.999882">
In order to compute the probability Q(e, f) of a
given transliteration pair, the final state has to be
the end state S + 1. Hence
</bodyText>
<equation confidence="0.988002">
Q(e, f) = XS α(e, f, s)ts,S+1 (1)
s=1
</equation>
<bodyText confidence="0.994978">
We also write the probability β(e, f, s′) that they
are transliteration pairs starting in state s′ as
Again noting that the start state of the HMM
</bodyText>
<equation confidence="0.729788333333333">
x is 0, we have Q(e, f) = XS β(e, f, s)t0,s. We
s=1
2|�u |= |�v |&gt; |u |+ |v |would imply ∃i s.t. ui = vi = ǫ
</equation>
<bodyText confidence="0.9644795">
which contradicts the definition of joint ǫ-extension.
denote a subsequence of a string u as umn =
(un, un+1, ... , um) . Using these definitions, we
can define α(ei1, fj1, s) as
</bodyText>
<equation confidence="0.917986838709677">


 t�o,,sP(e1, f1 |s) i = j = 1
0 i = j = 0,s =60
/mss =1 ts′,sα(ei1, fj−1
ES s′=1 ts′,sα(ei−1
1 i = j = 0,s = 0
1 , fj 1 , s′)P(ei, ǫ|s) i &gt; 1, j = 1
1 , s′)P(ǫ, fj|s) i = 1, j &gt; 1
Finally for i &gt; 1 and j &gt; 1,
α(i
e1, fj1, s) =
X ts′,s[α(ei1, fj−1
1 , s′)P(ǫ, fj|s)+
s′∈S
α(ei−1
1 , fj1, s′)P(ei, ǫ|s)+
α(ei−1
1 , fj−1
1 , s′)P(ei, fj|s)]
Similarly the recurrence for β(eNi , fMj , s)


 ts,S+1 i = N + 1,
Es =1 ts,s′β(eNi , fMj+1, s′)P(ǫ, fj|s′) i = N, j &lt; M
Es′ =1 ts,s′β(eNi+1, fMj , s′)P(ei, ǫ|s′) i &lt; N, j = M
j = M + 1
For i &lt; N and j &lt; M, β(eNi , fMj , s) =
ts,s′[β(eNi , fMj+1, s′)P(ǫ, fj|s′)+
β(eNi+1, fMj , s′)P(ei, ǫ|s′)+
β(eNi+1, fMj+1, s′)P(ei, fj|s′)]
</equation>
<bodyText confidence="0.9998643">
In order to proceed with the E.M. estimation
of the parameters T and Ps , we collect the
soft counts c(e, f|s) for emission probabilities by
looping over the training data D as shown in Fig-
ure 1.
Similarly the soft counts ct(s′, s) for the tran-
sition probabilities are estimated as shown in Fig-
ure 2.
Finally the probabilities P(e, f|s) and tij are re-
estimated as
</bodyText>
<equation confidence="0.93141">
c(e, f|s)
P� (e, f|s) = (2)
Pe∈E, f ∈F c(e, f |s)
�ts′,s =ct(s′, s) (3)
Ps ct(s′, s)
</equation>
<bodyText confidence="0.7296095">
We can also compute the most probable align-
ment (e, f) between the two strings e and f as
</bodyText>
<equation confidence="0.987617647058824">
β(e, f, s′) =
X
(˜e,˜f)∈J(e,f)
X
s′=s0,...,s|��|+1=S+1
ts0,s1
Y |��|
i=1
tsi,si+1P(�ei,
Y |��|
i=1
t0,s1
�fi|
X
s′∈S
si)
tsi,si+1P(�ei, �fi|si)
</equation>
<page confidence="0.96524">
121
</page>
<figure confidence="0.937751583333333">
� 1 N M � α(ei−1
c(e, f|s) = i=1 j=1 s′ 1 , fj−1
(e,f)∈D 1 , s′)ts′,sP(ei, fj|s)β(eNi , fMj , s)1(ei = e, fj = f)
Q(e, f)
� 1 N M � α(ei−1
+ i=1 j=1 s′ 1 , fj1, s′)ts′,sP(ei, ǫ|s)β(eNi , fMj , s)1(ei = e, fj = f)
(e,f)∈D
Q(e, f)
� 1 N M � α(ei1, fj−1
+ i=1 j=1 s′ 1 , s′)ts′,sP(ǫ, fj|s)β(eNi , fMj , s)1(ei = e, fj = f)
(e,f)∈D
Q(e, f)
</figure>
<figureCaption confidence="0.991205">
Figure 1: EM soft count c(e, f|s) estimation.
</figureCaption>
<table confidence="0.901854705882353">
�ct(s′, s) = 1 N M α(ei−1
(e,f)∈D i=1 j=1 1 , fj−1
1 , s′)ts′,sP(ei, fj|s)β(eNi , fMj , s)
Q(e, f)
� 1 N M α(ei−1
+ i=1 j=1 1 , fj1, s′)ts′,sP(ei, ǫ|s)β(eNi , fMj , s)
(e,f)∈D
Q(e ,f)
� 1 N M α(ei 1, fj−1
+ i=1 j=1 1 , s′)ts′,sP (ǫ, fj|s)β(eN i , fM j , s)
(e,f)∈D
Q(e, f)
�
+
(e,f)∈D
Q(e, f)α(eN1 , fM1 , s′)ts′,S+11(s = S + 1)
1
</table>
<figureCaption confidence="0.958394">
Figure 2: EM soft count ct(s′, s) estimation.
</figureCaption>
<page confidence="0.892425">
122
</page>
<equation confidence="0.59539175">
tsz,sz+1P(�ei,
� |˜e|
i=1
t0,s1
si
�fi|
�
0=s0,...,s|��|+1=S+1
</equation>
<bodyText confidence="0.9760285">
The pair (e, f) is considered as an alignment be-
tween the training pair (e, f).
</bodyText>
<sectionHeader confidence="0.692353" genericHeader="method">
3 Transduction of the Transliterated
Output
</sectionHeader>
<bodyText confidence="0.997646357142857">
Given an alignment (e, f), we consider all possi-
ble sub-alignments (6ji, �fji ) as pairs of substrings
Ej+1 =� E and fj+1 =� E . We extract all pos-
sible sub-alignments of all the alignments from
the training data. Let A be the bag of all sub-
alignments obtained from the training data. We
build a weighted finite state transducer that trans-
duces any string in £+ to F+ using these sub-
alignments.
Let (u, v) be an element of A. From the train-
ing data D, observe that A can have multiple re-
alizations of (u, v). Let N(u, v) be the number
of times (u, v) is observed in A. The empirical
probability of transducing string u to v is simply
</bodyText>
<equation confidence="0.995514333333333">
N(u, v)
P(v|u) =
wu,v = − log(P(v|u))−A log(Q(u,v))+δ (4)
</equation>
<bodyText confidence="0.999555">
Finally we construct a global weighted finite
state transducer F by taking the union of all the
Fu,v and taking its closure.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999662">
We evaluated our system on the standard track data
)provided by the NEWS 2009 shared task orga-
nizers on five different languages – Tamil, Hindi,
Russian, and Kannada was derived from (Ku-
maran and Kellner, 2007) and Chinese from (Li et
al., 2004). The results of this evaluation on the test
data is shown in Table 1. For a detailed description
</bodyText>
<table confidence="0.999757142857143">
Language Top-1 mean MRR
Accuracy F1 score
Tamil 0.327 0.870 0.458
Hindi 0.398 0.855 0.515
Russian 0.506 0.901 0.609
Chinese 0.450 0.755 0.514
Kannada 0.235 0.817 0.353
</table>
<tableCaption confidence="0.999964">
Table 1: Results on NEWS 2009 test data.
</tableCaption>
<bodyText confidence="0.976871666666667">
of the evaluation measures used we refer the read-
ers to NEWS 2009 shared task whitepaper (Li et
al., 2009).
</bodyText>
<sectionHeader confidence="0.998749" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999908777777778">
We described a system for automatic translitera-
tion of pairs of strings from one language to an-
other using E-extension hidden markov models and
weighted finite state transducers. We evaluated
our system on all the languages for the NEWS
2009 standard track. The system presented is lan-
guage agnostic and can be trained for any language
pair within a few minutes on a single core desktop
computer.
</bodyText>
<sectionHeader confidence="0.983356" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.539759">
Nasreen Abdul Jaleel and Leah Larkey. 2003. Statistical transliteration for english-arabic
cross language information retrieval. In Proceedings of the twelfth international con-
ference on Information and knowledge management, pages 139–146.
Kevin Knight and Jonathan Graehl. 1997. Machine transliteration. In Computational Lin-
guistics, pages 128–135.
</bodyText>
<figure confidence="0.997998">
arg max
(˜e,
˜f)EJ(e,f)
obtained from (e, � fi =� E,
� f) such that Ei =� E,
Ev:(u,v′)EA N(u, v′)
</figure>
<figureCaption confidence="0.879378666666667">
For every pair (u, v) E A , we also compute the
probability of transliteration from the HMM x as
Q(u, v) from Equation 1.
</figureCaption>
<bodyText confidence="0.989695545454546">
We construct a finite state transducer Fu,v that
accepts only u and emits v with a weight wu,v
defined as
The weight δ is typically sufficiently high so
that a new english string is favored to be broken
into fewest possible sub-strings whose translitera-
tions are available in the training data.
We tune the weights A and δ by evaluating the
accuracy on the held-out data. The n-best paths
in the weighted finite state transducer F represent
our n-best transliterations.
</bodyText>
<reference confidence="0.9820249375">
A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration.
In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference
on Research and development in information retrieval, pages 721–722, New York, NY,
USA. ACM.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine
transliteration. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 159, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of
news 2009 machine transliteration shared task. In Proceedings ofACL-IJCNLP 2009
Named Entities Workshop (NEWS 2009).
Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in
speech recognition. In Proceedings of the IEEE, pages 257–286.
Yilu Zhou, Feng Huang, and Hsinchun Chen. 2008. Combining probability models and web
mining models: a framework for jproper name transliteration. Information Technology
and Management, 9(2):91–103.
</reference>
<equation confidence="0.6620698">
�
�UF = Fu,v
(u,v)EA
� +
�(5)
</equation>
<page confidence="0.990452">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938964">
<title confidence="0.9994755">Hidden Markov Models and Weighted Transducers Machine Transliteration</title>
<author confidence="0.995896">Balakrishnan Vardarajan Delip Rao</author>
<affiliation confidence="0.999863">Dept. of Electrical and Computer Engineering Dept. of Computer Science Johns Hopkins University Johns Hopkins University</affiliation>
<email confidence="0.997245">bvarada2@jhu.edudelip@cs.jhu.edu</email>
<abstract confidence="0.994905666666667">We describe in detail a method for transliterating an English string to a foreign language string evaluated on five different languages, including Tamil, Hindi, Russian, Chinese, and Kannada. Our method involves deriving substring alignments from the training data and learning a weighted finite state transducer from these We define an Hidden Markov Model to derive alignments between training pairs and a heuristic to extract the substring alignments. Our method involves only two tunable parameters that can be optimized on held-out data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--722</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8848" citStr="Kumaran and Kellner, 2007" startWordPosition="1710" endWordPosition="1714"> element of A. From the training data D, observe that A can have multiple realizations of (u, v). Let N(u, v) be the number of times (u, v) is observed in A. The empirical probability of transducing string u to v is simply N(u, v) P(v|u) = wu,v = − log(P(v|u))−A log(Q(u,v))+δ (4) Finally we construct a global weighted finite state transducer F by taking the union of all the Fu,v and taking its closure. 4 Results We evaluated our system on the standard track data )provided by the NEWS 2009 shared task organizers on five different languages – Tamil, Hindi, Russian, and Kannada was derived from (Kumaran and Kellner, 2007) and Chinese from (Li et al., 2004). The results of this evaluation on the test data is shown in Table 1. For a detailed description Language Top-1 mean MRR Accuracy F1 score Tamil 0.327 0.870 0.458 Hindi 0.398 0.855 0.515 Russian 0.506 0.901 0.609 Chinese 0.450 0.755 0.514 Kannada 0.235 0.817 0.353 Table 1: Results on NEWS 2009 test data. of the evaluation measures used we refer the readers to NEWS 2009 shared task whitepaper (Li et al., 2009). 5 Conclusion We described a system for automatic transliteration of pairs of strings from one language to another using E-extension hidden markov mode</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 721–722, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>159</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8883" citStr="Li et al., 2004" startWordPosition="1718" endWordPosition="1721">rve that A can have multiple realizations of (u, v). Let N(u, v) be the number of times (u, v) is observed in A. The empirical probability of transducing string u to v is simply N(u, v) P(v|u) = wu,v = − log(P(v|u))−A log(Q(u,v))+δ (4) Finally we construct a global weighted finite state transducer F by taking the union of all the Fu,v and taking its closure. 4 Results We evaluated our system on the standard track data )provided by the NEWS 2009 shared task organizers on five different languages – Tamil, Hindi, Russian, and Kannada was derived from (Kumaran and Kellner, 2007) and Chinese from (Li et al., 2004). The results of this evaluation on the test data is shown in Table 1. For a detailed description Language Top-1 mean MRR Accuracy F1 score Tamil 0.327 0.870 0.458 Hindi 0.398 0.855 0.515 Russian 0.506 0.901 0.609 Chinese 0.450 0.755 0.514 Kannada 0.235 0.817 0.353 Table 1: Results on NEWS 2009 test data. of the evaluation measures used we refer the readers to NEWS 2009 shared task whitepaper (Li et al., 2009). 5 Conclusion We described a system for automatic transliteration of pairs of strings from one language to another using E-extension hidden markov models and weighted finite state transd</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 159, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>Vladimir Pervouchine</author>
</authors>
<title>Whitepaper of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="9296" citStr="Li et al., 2009" startWordPosition="1791" endWordPosition="1794">ack data )provided by the NEWS 2009 shared task organizers on five different languages – Tamil, Hindi, Russian, and Kannada was derived from (Kumaran and Kellner, 2007) and Chinese from (Li et al., 2004). The results of this evaluation on the test data is shown in Table 1. For a detailed description Language Top-1 mean MRR Accuracy F1 score Tamil 0.327 0.870 0.458 Hindi 0.398 0.855 0.515 Russian 0.506 0.901 0.609 Chinese 0.450 0.755 0.514 Kannada 0.235 0.817 0.353 Table 1: Results on NEWS 2009 test data. of the evaluation measures used we refer the readers to NEWS 2009 shared task whitepaper (Li et al., 2009). 5 Conclusion We described a system for automatic transliteration of pairs of strings from one language to another using E-extension hidden markov models and weighted finite state transducers. We evaluated our system on all the languages for the NEWS 2009 standard track. The system presented is language agnostic and can be trained for any language pair within a few minutes on a single core desktop computer. References Nasreen Abdul Jaleel and Leah Larkey. 2003. Statistical transliteration for english-arabic cross language information retrieval. In Proceedings of the twelfth international conf</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A Kumaran, Min Zhang, and Vladimir Pervouchine. 2009. Whitepaper of news 2009 machine transliteration shared task. In Proceedings ofACL-IJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE,</booktitle>
<pages>257--286</pages>
<contexts>
<context position="1421" citStr="Rabiner, 1989" startWordPosition="205" endWordPosition="206">rs that can be optimized on held-out data. 1 Introduction Transliteration is a letter by letter mapping of one writing system to another. Apart from the obvious use in writing systems, transliteration is also useful in conjunction with translation. For example, machine translation BLEU scores are known to improve when named entities are transliterated. This engendered several investigations into automatic transliteration of strings, named entities in particular, from one language to another. See Knight and Graehl(1997) and later papers on this topic for an overview. Hidden Markov Model (HMM) (Rabiner, 1989) is a standard sequence modeling tool used in various problems in natural language processing like machine translation, speech recognition, part of speech tagging and information extraction. There have been earlier attempts in using HMMs for automatic transliteration. See (Abdul Jaleel and Larkey, 2003; Zhou et al., 2008) for example. In this paper, we define an c-extension Hidden Markov Model that allows us to align source and target language strings such that the characters in the source string may be optionally aligned to the c symbol. We also introduce a heuristic that allows us to extract</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, pages 257–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yilu Zhou</author>
<author>Feng Huang</author>
<author>Hsinchun Chen</author>
</authors>
<title>Combining probability models and web mining models: a framework for jproper name transliteration.</title>
<date>2008</date>
<journal>Information Technology and Management,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="1744" citStr="Zhou et al., 2008" startWordPosition="251" endWordPosition="254"> named entities are transliterated. This engendered several investigations into automatic transliteration of strings, named entities in particular, from one language to another. See Knight and Graehl(1997) and later papers on this topic for an overview. Hidden Markov Model (HMM) (Rabiner, 1989) is a standard sequence modeling tool used in various problems in natural language processing like machine translation, speech recognition, part of speech tagging and information extraction. There have been earlier attempts in using HMMs for automatic transliteration. See (Abdul Jaleel and Larkey, 2003; Zhou et al., 2008) for example. In this paper, we define an c-extension Hidden Markov Model that allows us to align source and target language strings such that the characters in the source string may be optionally aligned to the c symbol. We also introduce a heuristic that allows us to extract high quality sub-alignments from the c-aligned word pairs. This allows us to define a weighted finite state transducer that produces transliterations for an English string by minimal segmentation. The overview of this paper is as follows: Section 2 introduces c-extension Hidden Markov Model and describes our alignment pr</context>
</contexts>
<marker>Zhou, Huang, Chen, 2008</marker>
<rawString>Yilu Zhou, Feng Huang, and Hsinchun Chen. 2008. Combining probability models and web mining models: a framework for jproper name transliteration. Information Technology and Management, 9(2):91–103.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>