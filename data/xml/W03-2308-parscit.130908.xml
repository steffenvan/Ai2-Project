<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001318">
<title confidence="0.946782">
Applied NLG system evaluation: FlexyCAT
</title>
<author confidence="0.998975">
Nestor Miliaev, Alison Cawsey and Greg Michaelson
</author>
<affiliation confidence="0.9955785">
Department of Computer Science
Heriot-Watt University
</affiliation>
<email confidence="0.96477">
Iceenym, alison, gregl@macs.hw.ac.uk
</email>
<sectionHeader confidence="0.995182" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999788">
Evaluation is an important part of NLG
projects, however NLG system evalua-
tion often consists of usability or static
text quality assessment. This paper
presents an NLG system, FlexyCAT,
and experiments that enabled us to eval-
uate the degree of knowledge re-use
and the task-specific value of generated
texts.
</bodyText>
<sectionHeader confidence="0.997908" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974681818182">
Many applied natural language generation (NLG)
systems have been created recently producing var-
ious kinds of texts for different applications. How-
ever, only a few of these systems have been for-
mally evaluated, and the evaluation performed
have focused on the grammaticality and fluency
of the output text, rather that on its effectiveness
(Colineau et al., 2002). Issues such as knowledge
re-use were largely ignored in these projects.
This paper describes the evaluation of an
NLG system Flexible Computer-Aided Technical
Writer (FlexyCAT) focusing on the assessment of
the task-specific quality of generated texts and
knowledge re-use when the system is used for
the description of different devices. This work
has shown, firstly, that the task-oriented quality
of generated texts can be comparable to that of
human-crafted texts; and secondly, how knowl-
edge re-use allows us to extend the applicability
of an NLG system for the description of different
technical systems and to reduce time taken to cre-
ate a document.
</bodyText>
<sectionHeader confidence="0.9621965" genericHeader="method">
2 Existing practise of NLG System
Evaluation
</sectionHeader>
<bodyText confidence="0.998658625">
Recently, it had become widely accepted that work
in NLG should pay closer attention to the evalua-
tion of results. The aspects of an NLG system to
evaluate and the metrics to use for this are defined
by the goals of the NLG system. The most com-
mon advantages of using NLG comparable with,
e.g., Machine Translation (MT) are considered the
following (Reiter and Dale, 2000):
</bodyText>
<listItem confidence="0.997398818181818">
• High quality output text that is generated
based on machine data and does not require
post-processing
• Simultaneous production of text versions in
different languages
• Consistency of text between the versions and
with a domain model
• Lower cost and time of text revision (e.g.,
when domain changes)
• Dynamic text generation upon user query
• Potentially good knowledge re-use
</listItem>
<bodyText confidence="0.979526333333333">
Unfortunately, most NLG systems evaluation
carried out to date did not cover all these as-
pects. Usually NLG evaluation deals with 1) Sys-
tem quality; 2) Text quality.
System quality evaluation consists of measur-
ing characteristics like usability, generation speed
</bodyText>
<page confidence="0.997424">
55
</page>
<bodyText confidence="0.999485368421053">
and system robustness. Such evaluation follows
common patterns of software systems evaluation,
see e.g., (Newman and Lamming, 1995). NLG-
specific principles and metrics of system evalua-
tion are described in (Mellish and Dale, 1998) and
(EAGLES, 1995).
One of the most thorough usability evaluation
of an NLG system has been carried out in the
AGILE project; the course of experiments and
their results are presented in (Hartley et al., 2000).
Assessing the quality of a generated text (based
on (Mellish and Dale, 1998) and (Hartley et al.,
2000)) includes rating of the text against such cri-
teria as Accuracy, Fluency (including Acceptabil-
ity and Grammaticality) and Lexico-grammar cov-
erage.
Methods of evaluation appropriate to these cri-
teria can be grouped into three classes, as sug-
gested in (Bangalore et al., 1998):
Intrinsic that typically consists in asking human
judges to rate the quality of generated texts.
Intrinsic evaluation is most common, and was
carried out e.g., in AGILE (Hartley et al.,
2000) and MIRADOR (Eddy and Cawsey,
2002); it is fairly simple and straightforward,
however depends a great deal on the evalua-
tors&apos; expertise and personal preferences.
Extrinsic or task evaluation, where the user&apos;s
ability to perform some task using a gener-
ated text or the impact of the text on the user
behaviour is assessed. Extrinsic evaluation is
less common because experiments are more
difficult to carry out. However, it allows us
to estimate the task-oriented worth of a doc-
ument. This kind of evaluation was used in
Isolde (Colineau et al., 2002), STOP (Reiter
et al., 2001), and other NLG projects.
Comparative where the objective is to directly
compare the performance of different gener-
ation systems and formalisms. Comparative
evaluation is not often used because of its
complexity. An example of this kind of trial
for the XTAG project is described in (Banga-
lore et al., 1998).
Other aspects of NLG, such as knowledge re-
use, time of document creation or system flexibil-
ity have not received enough attention so far. Al-
though there were some reports on task-oriented
evaluation, these experiments are not common and
many of those did not include the comparison of
the user performance on human-crafted and gen-
erated manuals, like in IDAS (Levine and Mellish,
1995).
During FlexyCAT evaluation, we paid extra at-
tention to task-oriented text quality, knowledge re-
use and the benefits the author obtains by using a
flexible planning technique.
</bodyText>
<sectionHeader confidence="0.993455" genericHeader="method">
3 FlexyCAT: System Description
</sectionHeader>
<bodyText confidence="0.999908484848485">
FlexyCAT is an NLG system for producing manu-
als for technical devices, mainly home appliances,
e.g., TVs, VCRs, cameras, etc. Manuals consist
of texts divided up into sections, each of which
describes an individual procedure, consisting of a
sequence of steps that the user is to perform to at-
tain his/her goal. No generation of explanatory or
warning information is included; generated texts
are purely instructional and are thus like the &apos;min-
imalist instructions&apos; described in (Colineau et al.,
2002).
Texts are generated in two languages (English
and Russian) given a domain model. The domain
model represents an object-oriented description of
a device being documented. Such a representa-
tion contains a description of all device elementary
constituents and their functioning. The descrip-
tion of the functioning of elementary objects in-
cludes events (user actions) and object functions,
that have preconditions and actions. Both precon-
ditions and actions are described in terms of el-
ementary object properties. Objects, events and
actions are represented using linguistic classes. A
generation system uses both linguistic classes and
domain knowledge for the production of a gram-
matical manual text in two languages. The out-
put of FlexyCAT is a ready to use manual for the
device. FlexyCAT&apos;s GUI provides the user with
facilities for creating and editing a domain model
and dictionaries of linguistic classes. It also allows
the user to easily assign linguistic values to the ele-
ments of the domain model. For more information
see (Miliaev et al., 2002).
</bodyText>
<page confidence="0.954337">
56
</page>
<bodyText confidence="0.837067">
Two main advantages of FlexyCAT compared &apos;motivation&apos; to &apos;means&apos;; in the second sentence
</bodyText>
<listItem confidence="0.953780777777778">
with other NLG systems are:
• FlexyCAT is an integrated tool allowing ex-
tensive editing of both linguistic and domain
knowledge. This, and an object-oriented de-
sign of the knowledge, enables, firstly, good
knowledge re-use, and, secondly, the produc-
tion of manuals for a variety of different tech-
nical devices using a single NLG tool.
• FlexyCAT offers a flexible approach to text
</listItem>
<bodyText confidence="0.66398025">
planning. The planner produces a candidate
text plan automatically, based on the domain
model. When there is a necessity to obtain a
better quality text, the user edits this plan us-
ing an interactive planning utility. A text is
generated based on the text plan. For auto-
matically generated plan it is of a draft qual-
ity, while a better quality text is obtained
from an edited plan. The automatic feature
facilitates the work of the technical author by
producing a text draft quickly and at fairly lit-
tle cost.
An example original text from the corpus is
given below; it contains a considerable proportion
of explanatory and causative information:
How to play a CD
</bodyText>
<listItem confidence="0.910939333333333">
1. Press the PLAY/PAUSE button
The CD will begin to play and
the track number will be shown
in the display
2. To pause the CD press the
PLAY/PAUSE button
</listItem>
<bodyText confidence="0.9984896">
The CD will stop and the PAUSE
INDICATOR will flash in the
display
An automatically generated corresponding text
looks like this:
</bodyText>
<listItem confidence="0.9439028">
Playback CD
1. Press PLAY/PAUSE button to
start CD playback
2. Press PLAY/PAUSE button to
pause CD
</listItem>
<bodyText confidence="0.9933308">
After the plan refinement, we get the following
text, with richer rhetorical structure (the rhetori-
cal relation in the first sentence is changed from
the sequence of the nucleus and satellite clauses
changed) and, we believe, a better quality:
</bodyText>
<listItem confidence="0.9378286">
Playback CD
1. Start CD playback by pressing
PLAY/PAUSE button
2. To pause CD, press PLAY/PAUSE
button
</listItem>
<bodyText confidence="0.999822">
Below we will describe our experience and re-
sults of the evaluation of the FlexyCAT NLG sys-
tem.
</bodyText>
<sectionHeader confidence="0.995498" genericHeader="method">
4 FlexyCAT Evaluation
</sectionHeader>
<bodyText confidence="0.9881166">
The evaluation of FlexyCAT is ongoing. This
section describes evaluation experiments that have
been carried out to date and their results.
So far, three stages of the evaluation have been
performed. These are:
</bodyText>
<listItem confidence="0.99948175">
• Experiments on knowledge re-use and text
production for different devices
• Subjective assessment of text quality
• Task-oriented text quality evaluation
</listItem>
<subsectionHeader confidence="0.81551">
4.1 Knowledge Re-use and Resource
Management
</subsectionHeader>
<bodyText confidence="0.99998385">
This experiment was targeted, firstly, to assess the
appropriateness and effectiveness of the chosen
domain description structure and the domain en-
gineering tool for documenting different technical
devices. The second goal was to assess the degree
of knowledge (both linguistic and domain) re-use.
The experiment was set up as follows. Man-
ual texts, each about 3 pages long, were selected
for three different pieces of hardware. All texts
contained sections, subsections and paragraphs.
FlexyCAT was used to build a domain model for
the pertinent subset of each of these devices and
manual texts were generated in both English and
Russian. The interactive planning utility was used
to obtain text as close to the original manual as
possible, in terms of wording and word order, see
the description of the metrics for generation accu-
racy and their importance in (Hartley et al., 2000).
In many cases the generated texts were identical to
the original ones. The time taken to complete the
</bodyText>
<page confidence="0.990938">
57
</page>
<bodyText confidence="0.990617791666667">
entire process of document creation was measured
and the number of knowledge base (KB) elements
re-used was estimated for each task.
The course of the experiment was as follows.
A domain model and a manual for the first VCR
(VCR1) was created. Then the manual for a simi-
lar device, VCR2 was created. Both manuals con-
tained similar set of functions (however the con-
trols and operation were different in some cases).
During the creation of the manual for VCR2 both
linguistic and domain resources created during the
production of the manual for VCR1 were available
and were re-used.
The second phase of the experiment consisted in
the creation of a manual for a fairly distant device,
a combined CD-Radio-Cassette (Combine). That
device contained parts similar to that of a VCR,
namely a tape recorder and a CD player; and a
part different from any known in a VCR, a radio
tuner. The manual for Combine was created twice
— once from scratch without using any of the exist-
ing resources, and a second time making use of the
resources created for VCR I. The role of a techni-
cal author was played by the system author, who
knows the system very well and had created man-
uals for these and similar devices number of times
in the course of the system development and eval-
uation. Thus, the learning effect that may have af-
fected time of subsequent document creation may
be largely ignored.
FlexyCAT uses three classes of linguistic primi-
tives: nominal expressions (consisting of up to two
nouns), verbs and adverbials. Adverbials were not
considered in the evaluation experiments. There
exist two versions of each nominal expression and
verb, one for English and one for Russian. Com-
plex classes, that encapsulate the references to
both versions, are called Nominals and Verbals re-
spectively. Nominals denote nominal expressions
that could be used to name a domain entry. Ver-
bats are used to specify events and actions in a do-
main model. A Domain object represents a single
elementary constituent of a device; an object&apos;s de-
scription may include a number of references to
nominals and verbals.
Figures 1, 2 and 3 show the results of knowl-
edge re-use which occurred in the course of the
experiment.
</bodyText>
<table confidence="0.999677666666667">
KB element Reused Total Percentage
Nominals 30 84 36%
Nominals* 12 84 14%
English nouns 41 83 49%
Russian nouns 52 87 60%
Verbals 32 46 70%
English verbs 31 38 82%
Russian verbs 33 35 94%
Domain objects 23 38 61%
</table>
<figureCaption confidence="0.996791">
Figure 1: Knowledge re-use for VCR2
</figureCaption>
<bodyText confidence="0.999755142857143">
In Figures 1, 2 and 3 Nominals* denote nomi-
nals that were amended slightly for the description
of new objects in a new device.
Domain objects almost always underwent
change to conform the description of a new de-
vice. &apos;Re-used&apos; objects, hence, are those that were
amended only slightly.
</bodyText>
<table confidence="0.9863685">
KB element Reused Total Percentage
Nominals 12 24 50%
Nominals* 5 24 21%
English nouns 23 30 77%
Russian nouns 23 27 85%
Verbals 19 24 79%
English verbs 18 18 100%
Russian verbs 20 21 95%
Domain objects 10 15 67%
Figure 2: Knowledge re-use for CD: Tape recorder
sub-set
KB element Reused Total Percentage
Nominals 13 51 25%
Nominals* 8 51 16%
English nouns 28 58 48%
Russian nouns 30 59 51%
Verbals 22 31 71%
English verbs 23 31 74%
Russian verbs 23 34 68%
Domain objects 15 28 54%
</table>
<figureCaption confidence="0.990169">
Figure 3: Knowledge re-use for CD: whole device
</figureCaption>
<bodyText confidence="0.999023">
As can be seen from Figures 1 - 3, the percent-
age of knowledge re-use is fairly high. By that,
the percentage is higher between similar devices
and increases for simpler elements; nouns and
</bodyText>
<page confidence="0.99144">
58
</page>
<bodyText confidence="0.999557">
verbs, show the best degree of re-use, with verbs
in some cases being re-used by 100%. This could
be explained by the fact that the way controls
operate are fairly persistent across many devices.
Nominals are re-used a little less. That is caused
mainly by the difference in control names in
different devices.
For the comparison of time taken to produce
documents using FlexyCAT it can be assumed that
manuals for VCR1 and VCR2 are pretty much
identical in size and labour-intensity to produce.
The manual for Combine is slightly shorter. That
explains the necessity of producing it from scratch
first — we needed it to assess the initial time re-
quired to compare it with that when re-using exist-
ing resources. Figure 4 shows the time measures
for each experiment.
</bodyText>
<table confidence="0.913959555555556">
Experiment Time
580 mins
340 mins
430 mins
270 mins
VCR1, no resource exists
VCR2, VCR1 resource used
Combine, no resource used
Combine, VCR1 resource used
</table>
<figureCaption confidence="0.999661">
Figure 4: Time of manual creation in FlexyCAT
</figureCaption>
<bodyText confidence="0.9999378">
As can be seen, the time to create a manual is
being reduced significantly when re-using existing
resources. This suggests good knowledge re-use
and effort reduction when making use of existing
resources, even for a fairly different device.
</bodyText>
<subsectionHeader confidence="0.990963">
4.2 Subjective Assessment of Text Quality
</subsectionHeader>
<bodyText confidence="0.999982586206897">
An experiment on subjective assessment of text
quality was conceived as a pilot study for the
task-oriented text quality evaluation. However, it
has yielded some interesting results, especially in
comparison with the latter, so we will refer to both.
The experiment was set up as follows. We used
a within subject design. A group of nine native
speakers of English were offered a set of nine text
excerpts each. Text excerpts were short instruc-
tions up to half a page long. Texts were given in
three different versions: original manual text; au-
tomatically generated draft; and a text generated
after plan editing (further regarded as &apos;original&apos;,
&apos;generated&apos; and &apos;edited&apos; respectively). See text ex-
amples in section 3. Each set contained only one
version of each text (either original, automatically
generated or generated after plan editing). Text
versions were evenly distributed across different
sets and the evaluators did not know which ver-
sion of each text excerpt s/he got. Illustrations
from original manuals were included in all ver-
sions, where applicable. Text layout was the same
in all versions.
The evaluators were asked to assess quality and
understandability of each text on the scale from 1
to 5 (1 is bad, 5 is excellent).
The average scores and standard deviation
across different document versions were as fol-
lows, see Figure 5.
</bodyText>
<figure confidence="0.9786675">
Text quality
Original
Generated
Generated+edited
Text understandability
Original 3.85(1.03)
Generated 3.93(1.04)
Generated+edited 3.81(1.08)
</figure>
<figureCaption confidence="0.995372">
Figure 5: Mean(standard deviation) of subjective
scores of text quality
</figureCaption>
<bodyText confidence="0.999747529411764">
This figure shows that different text versions
were ranked quite closely, which lets us conclude
that all text versions are similar from the reader&apos;s
point of view. This is similar to the results ob-
tained in the AGILE evaluation.
Surprisingly, automatically generated text
drafts (having no diverse sentence structures and
lacking rhetorical markers) have been preferred
over other text versions. However, a t-test has
shown that the difference in user preference is
not significant. The results of the t-test at df=57
are: t=-0.408, p=0.685 between original and
generated; t=0.272, p=0.7866 between original
and edited; and t=-0.680, p=0.4993 between
generated and edited texts.
After this pilot study, a larger scale task-
oriented experiment was run.
</bodyText>
<equation confidence="0.981255333333333">
4.07(0.92)
4.19(0.79)
4.00(0.68)
</equation>
<page confidence="0.996596">
59
</page>
<subsectionHeader confidence="0.982342">
4.3 Task-oriented text quality evaluation
</subsectionHeader>
<bodyText confidence="0.999935527777778">
This time the user had to carry out real tasks with
actual pieces of hardware, instead of subjectively
assessing text qualities. There were three sets of
hardware: VCR1+TV, VCR2 and a CD player.
The tasks included: 1) Assemble all compo-
nents of VCR1+TV; start, stop and eject the video-
tape; 2) Program VCR1 to automatically record a
certain program; 3) Set clock of VCR2 to a certain
time; 4) Power a CD player, load a CD, start play-
back and program the CD player to play certain
tracks.
As in the pilot study, sets of manuals were pre-
pared, produced according the same principles;
the only difference was that this time the manu-
als were longer— up to 3 pages. There were 21
evaluators — seven for each set of manuals.
The users were encouraged to use the manual as
much as possible, but they were free to use their
prior knowledge as well. However, because of the
complexity of the tasks the users had to use man-
uals for almost all tasks.
The user performance of each task was timed.
At the end of each task, the users were asked to
assess the following: &amp;quot;Task difficulty&amp;quot;, &amp;quot;How use-
ful was manual to cope with the task&amp;quot; and &amp;quot;Quality
of the manual text&amp;quot;. The scale was 1 to 5 (1 is an
easy task or useless manual or bad text quality; 5 is
a difficult task, helpful manual or good text quality
respectively). Also the users were encouraged to
give their informal comments about tasks and used
manuals at the end of the experimental session.
Figure 6 shows average time taken to perform
tasks given different sets of manuals and standard
deviation of the task accomplishment time. As can
be seen, the time of task completion varied greatly
between different tasks and users.
</bodyText>
<table confidence="0.9846338">
Task.No Orig Gen Edit
1 3:37(0:58) 3:19(0:47) 2:58(0:31)
2 6:02(1:26) 5:53(1:21) 4:38(1:11)
3 3:59(1:17) 2:43(0:58) 4:20(2:26)
4 4:29(1:44) 3:48(0:50) 2:57(0:37)
</table>
<figureCaption confidence="0.9795675">
Figure 6: Mean(standard deviation) of task ac-
complishment time
</figureCaption>
<bodyText confidence="0.999101935483871">
While performing the first task the users often
did not resort to the manual, so we discard the re-
sults of this test from the further discussion as not
being reliable. After normalising experiment time
(mean time(deviation) is 1.10(0.22), 0.96(0.29)
and 0.98(0.48) for original, generated and edited
texts respectively), a t-test has been done to esti-
mate the difference in user performance depend-
ing on the text version used.
The results of the t-test indicate that users per-
formed faster on generated versions of the man-
uals; however the difference in the user perfor-
mance on different text versions is not significant,
only approaching the level of significance between
original and generated texts. The results at df=40
are: t=1.731, p=0.0912 between original and gen-
erated; t=1.032, p=0.3083; and t=0.152, p=0.880
between edited and generated text versions.
The small dependency of the task accom-
plishment time on the text version may suggest
that users seldom pay much attention to the text
detail, preferring to quickly skim through the text
looking for the pertinent information or use the
diagrams. Any attempts to improve text quality
(adding rhetorical markers, etc.) in our case made
little difference to the user performance.
The results of user assessment of the text qual-
ity/usefulness after performing their tasks are pre-
sented in Figure 7. This figure shows average
score of text quality and usefulness for completing
the task and standard deviation of these scores.
</bodyText>
<figure confidence="0.60752825">
Text quality
Original
Generated
Generated+edited
Text usefulness
Original 3.90(0.92)
Generated 3.25(1.18)
Generated+edited 3.90(1.64)
</figure>
<figureCaption confidence="0.9477855">
Figure 7: Mean(standard deviation) of text qual-
ity/usefulness results
</figureCaption>
<bodyText confidence="0.99714325">
As is the case with the time of task accomplish-
ment, scores varied a lot between different users.
The results of a t-test of text quality at df=40
are: t=1.936, p=0.06 between original and gen-
</bodyText>
<equation confidence="0.674151333333333">
3.45(0.80)
2.75(1.36)
3.25(1.62)
</equation>
<page confidence="0.981698">
60
</page>
<bodyText confidence="0.999798829268293">
erated; t=0.481, p=0.633 between original and
edited; and t=1.030, p=0.3092 between edited and
generated texts.
The results of a t-test of text usefulness at df=40
are: t=1.898, p=0.065 between original and gener-
ated; t=0, p=1 between original and edited; and
t=1.403, p=0.168 between edited and generated
texts.
These results show that on average the users re-
garded the quality and usefulness of the original
texts being equal or slightly better than these of
the generated or edited texts. However the differ-
ence is not significant.
The comparison of data in Figures 6 and 7 gives
rise to an interesting paradox. The users have as-
sessed the quality and usefulness of original man-
uals slightly higher than of their generated coun-
terparts. Nonetheless, they performed slightly bet-
ter on the generated versions. This shows that the
subjective judgement of text quality may not reli-
ably represent its task-oriented worth. A similar
opinion was expressed by the participants of the
AGILE evaluation that &apos;it is difficult to evaluate
the acceptability of a technical instruction text per
.se, without real knowledge of the ... system it de-
scribes&apos;.
Another discrepancy is that in the pilot study
the users preferred generated texts over other ver-
sions, whereas in the main experiment original
texts were favourites. We do not have other ex-
planation than either users&apos; subjectivity or a low
number of participants that played role. The first
premise indicates the insufficiency of subjective
methods of text quality evaluation; the second one
entails us to re-do the experiments with greater
number of participants.
However, any differences in user performance
and text quality scores described above were in-
significant, which means that both subjective and
task-specific quality of all manual versions was
pretty much equal.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="evaluation">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.999967538461539">
The results obtained in the course of evaluation of
FlexyCAT have confirmed again the advantages of
NLG systems and emphasised the aspects which
had previously received less attention.
Firstly, FlexyCAT has proved a versatile tool
allowing us to create manuals for different de-
vices. The extent of knowledge re-use (both lin-
guistic and domain) is very promising not only
across similar devices, but even between fairly dis-
tant ones.
Re-using existing knowledge also allows the
user to save valuable time in the production of sub-
sequent documents. We have found that the time
of manual creation in two languages, when re-
using existing linguistic and domain resources, is
comparable to that of manual document creation.
These results suggest that NLG could become a
promising competitor to other ways of multilin-
gual document creation in terms of time and effort
saving.
Secondly, the users did not show any significant
preference of any text version over others, not did
they perform significantly better on a certain ver-
sion of text. This is a very promising result sug-
gesting that the quality of generated texts was as
good as that of manually-crafted ones.
Thirdly, time of task completion varies a great
deal between different users, but generally de-
pends little on the manual text quality. An attempt
to improve text quality by enriching its rhetorical
structure made little difference to the user perfor-
mance. This also indicates that text .fluency mat-
ters little for accomplishing user task. Sometimes
users subjectively assessed a manual as having
poor quality, nonetheless finding it useful for com-
pleting their task — Tad manual is better than no
manual&apos;.
Fourthly, a subjective evaluation of text quality
may not be a true indication of manual worth
with regard to performing a task. Task-oriented
evaluation is a more objective way of assessing
how helpful the manual is.
The collected data and informal comments have
given rise to the following suggestions to improve
manual quality, that support those found in (Hay-
don, 1995):
Firstly, the consistency and structure of a text
are very important. All pertinent information
should be presented in an overt form and at the
place where it is vital.
Secondly, small pieces of text are better than
big dense chunks. Each piece should describe a
</bodyText>
<page confidence="0.977596">
61
</page>
<bodyText confidence="0.815364">
single function. 1995 EAGLES, Evaluation of Natural Language Pro-
cessing Systems. FINAL REPORT, EAGLES DOC-
</bodyText>
<sectionHeader confidence="0.652865" genericHeader="conclusions">
6 Conclusions and Further Work UMENT EWG-PR. Version of September 1995.
</sectionHeader>
<bodyText confidence="0.999972333333333">
In the course of FlexyCAT evaluation some novel
experiments were carried out that have shown that
the NLG system may be used to produce texts for
different devices and that it is possible to achieve
good knowledge re-use, that allows significant
saving of time and effort for the production of sub-
sequent manuals.
The text quality experiments have indicated that
an NLG system is capable of producing good
quality texts. Our task-oriented experiment have
shown the advantages of using generated manu-
als for performing a task and that that subjective
methods of assessing text quality are not always
adequate for the estimation of text usefulness.
Although FlexyCAT is a bilingual text gener-
ation tool, so far it has not been possible to as-
sess the Russian part of it because of the lack of
both original manuals in Russian and of Russian-
speaking evaluators. All experiments described in
this paper are pertinent to the English part of the
generation; it was assumed that Russian texts have
comparable quality and properties. Further exper-
iments to assess the quality of Russian texts and
their consistency with English ones are required.
Also, larger-scale experiments are desirable to
evaluate the our conclusions.
This work has indicated the importance of eval-
uation in NLG, especially task-oriented evaluation
and the necessity of broadening of scopes of NLG
evaluation.
</bodyText>
<sectionHeader confidence="0.985792" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999807">
We thank Dr. Diana Bental for her help with sta-
tistical processing of experimental data.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999141441860465">
Srinivas Bangalore, Anoop Sarkar, Christine Doran,
and Beth Ann Hockey. 1998. Grammar and Parser
Evaluation in the XTAG Project. In Workshop on
The Evaluation of Parsing Systems, Granada, Spain.
Nathalie Colineau, Ccile Paris, Keith Vander Linden.
2002. An Evaluation of Procedural Instructional
Text. In Proceedings of INLG 2002. pp 128-135.
Bruce Eddy and Alison Cawsey. 2002. Balancing
Conciseness, Readability and Salience in Generated
Text. In Proceedings of the 3rd International Work-
shop on Natural Language and Information Systems,
Aix-en-Provence, France.
Antony Hartley, Donia Scott, I. Kruijff-Korbayouva,
Serge Sharoff, E. Teich, Lena Sokolova, Kamenka
Staykova, Danail Dochev, Martin Cmajrek, and Jiri
Hana. 2000. Evaluation of the final prototype.
Technical report, Brighton University, October, 12.
Leslie M. Haydon July 1995. The Complete Guide
to Writing and Producing Technical Manuals. John
Wiley and Sons, Ltd.
John Levine and Chris Mellish. 1995. The IDAS User
Trials: Quantitative Evaluation. In Proceedings of
the 5th European Workshop on NLG, pages 75-93.
Rijks Universiteit Leiden.
Chris Mellish and Robert Dale. 1998. Evaluation in
the context of natural language generation.
Nestor Miliaev, Alison Cawsey, and Greg Michaelson.
2002. Technical Documentation: An Integrated Ar-
chitecture for Supporting the Author in Generation
and Resource Editing. In The Tenth International
Conference on Artificial Intelligence: Methodology,
Systems, Applications A1MSA 2002, Varna, Bul-
garia, September 4-6. Springer-Verlag.
Ehud Reiter and Robert Dale. 2000. Building Applied
Natural Language Generation System. Natural Lan-
guage Engineering. Cambridge University Pres.
Ehud Reiter, Roma Robertson, A. Scott Lennox, and
Liesl Osman. 2001. Using a Randomised Con-
trolled Clinical Trial to Evaluate an NLG System.
In In Proceedings of ACL-2001, pages pp. 434-441.
William M. Newman, Michael G. Lamming 1995. In-
teractive System Design. Addison-Wesley Pub Co.
1st edition.
</reference>
<page confidence="0.999158">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317378">
<title confidence="0.98996">Applied NLG system evaluation: FlexyCAT</title>
<author confidence="0.983946">Nestor Miliaev</author>
<author confidence="0.983946">Alison Cawsey</author>
<author confidence="0.983946">Greg</author>
<affiliation confidence="0.712337">Department of Computer Heriot-Watt</affiliation>
<email confidence="0.801293">Iceenym,alison,gregl@macs.hw.ac.uk</email>
<abstract confidence="0.9903506">Evaluation is an important part of NLG projects, however NLG system evaluation often consists of usability or static text quality assessment. This paper presents an NLG system, FlexyCAT, and experiments that enabled us to evaluate the degree of knowledge re-use and the task-specific value of generated texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Anoop Sarkar</author>
<author>Christine Doran</author>
<author>Beth Ann Hockey</author>
</authors>
<date>1998</date>
<booktitle>Grammar and Parser Evaluation in the XTAG Project. In Workshop on The Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="3422" citStr="Bangalore et al., 1998" startWordPosition="536" endWordPosition="539">cribed in (Mellish and Dale, 1998) and (EAGLES, 1995). One of the most thorough usability evaluation of an NLG system has been carried out in the AGILE project; the course of experiments and their results are presented in (Hartley et al., 2000). Assessing the quality of a generated text (based on (Mellish and Dale, 1998) and (Hartley et al., 2000)) includes rating of the text against such criteria as Accuracy, Fluency (including Acceptability and Grammaticality) and Lexico-grammar coverage. Methods of evaluation appropriate to these criteria can be grouped into three classes, as suggested in (Bangalore et al., 1998): Intrinsic that typically consists in asking human judges to rate the quality of generated texts. Intrinsic evaluation is most common, and was carried out e.g., in AGILE (Hartley et al., 2000) and MIRADOR (Eddy and Cawsey, 2002); it is fairly simple and straightforward, however depends a great deal on the evaluators&apos; expertise and personal preferences. Extrinsic or task evaluation, where the user&apos;s ability to perform some task using a generated text or the impact of the text on the user behaviour is assessed. Extrinsic evaluation is less common because experiments are more difficult to carry </context>
</contexts>
<marker>Bangalore, Sarkar, Doran, Hockey, 1998</marker>
<rawString>Srinivas Bangalore, Anoop Sarkar, Christine Doran, and Beth Ann Hockey. 1998. Grammar and Parser Evaluation in the XTAG Project. In Workshop on The Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathalie Colineau</author>
</authors>
<title>Ccile Paris, Keith Vander Linden.</title>
<date>2002</date>
<booktitle>In Proceedings of INLG</booktitle>
<pages>128--135</pages>
<marker>Colineau, 2002</marker>
<rawString>Nathalie Colineau, Ccile Paris, Keith Vander Linden. 2002. An Evaluation of Procedural Instructional Text. In Proceedings of INLG 2002. pp 128-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Eddy</author>
<author>Alison Cawsey</author>
</authors>
<title>Balancing Conciseness, Readability and Salience in Generated Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Workshop on Natural Language and Information Systems,</booktitle>
<location>Aix-en-Provence, France.</location>
<contexts>
<context position="3651" citStr="Eddy and Cawsey, 2002" startWordPosition="573" endWordPosition="576"> et al., 2000). Assessing the quality of a generated text (based on (Mellish and Dale, 1998) and (Hartley et al., 2000)) includes rating of the text against such criteria as Accuracy, Fluency (including Acceptability and Grammaticality) and Lexico-grammar coverage. Methods of evaluation appropriate to these criteria can be grouped into three classes, as suggested in (Bangalore et al., 1998): Intrinsic that typically consists in asking human judges to rate the quality of generated texts. Intrinsic evaluation is most common, and was carried out e.g., in AGILE (Hartley et al., 2000) and MIRADOR (Eddy and Cawsey, 2002); it is fairly simple and straightforward, however depends a great deal on the evaluators&apos; expertise and personal preferences. Extrinsic or task evaluation, where the user&apos;s ability to perform some task using a generated text or the impact of the text on the user behaviour is assessed. Extrinsic evaluation is less common because experiments are more difficult to carry out. However, it allows us to estimate the task-oriented worth of a document. This kind of evaluation was used in Isolde (Colineau et al., 2002), STOP (Reiter et al., 2001), and other NLG projects. Comparative where the objective</context>
</contexts>
<marker>Eddy, Cawsey, 2002</marker>
<rawString>Bruce Eddy and Alison Cawsey. 2002. Balancing Conciseness, Readability and Salience in Generated Text. In Proceedings of the 3rd International Workshop on Natural Language and Information Systems, Aix-en-Provence, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antony Hartley</author>
<author>Donia Scott</author>
<author>I Kruijff-Korbayouva</author>
<author>Serge Sharoff</author>
<author>E Teich</author>
<author>Lena Sokolova</author>
<author>Kamenka Staykova</author>
<author>Danail Dochev</author>
<author>Martin Cmajrek</author>
<author>Jiri Hana</author>
</authors>
<title>Evaluation of the final prototype.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Brighton University,</institution>
<contexts>
<context position="3043" citStr="Hartley et al., 2000" startWordPosition="475" endWordPosition="478">e aspects. Usually NLG evaluation deals with 1) System quality; 2) Text quality. System quality evaluation consists of measuring characteristics like usability, generation speed 55 and system robustness. Such evaluation follows common patterns of software systems evaluation, see e.g., (Newman and Lamming, 1995). NLGspecific principles and metrics of system evaluation are described in (Mellish and Dale, 1998) and (EAGLES, 1995). One of the most thorough usability evaluation of an NLG system has been carried out in the AGILE project; the course of experiments and their results are presented in (Hartley et al., 2000). Assessing the quality of a generated text (based on (Mellish and Dale, 1998) and (Hartley et al., 2000)) includes rating of the text against such criteria as Accuracy, Fluency (including Acceptability and Grammaticality) and Lexico-grammar coverage. Methods of evaluation appropriate to these criteria can be grouped into three classes, as suggested in (Bangalore et al., 1998): Intrinsic that typically consists in asking human judges to rate the quality of generated texts. Intrinsic evaluation is most common, and was carried out e.g., in AGILE (Hartley et al., 2000) and MIRADOR (Eddy and Cawse</context>
<context position="9933" citStr="Hartley et al., 2000" startWordPosition="1595" endWordPosition="1598">both linguistic and domain) re-use. The experiment was set up as follows. Manual texts, each about 3 pages long, were selected for three different pieces of hardware. All texts contained sections, subsections and paragraphs. FlexyCAT was used to build a domain model for the pertinent subset of each of these devices and manual texts were generated in both English and Russian. The interactive planning utility was used to obtain text as close to the original manual as possible, in terms of wording and word order, see the description of the metrics for generation accuracy and their importance in (Hartley et al., 2000). In many cases the generated texts were identical to the original ones. The time taken to complete the 57 entire process of document creation was measured and the number of knowledge base (KB) elements re-used was estimated for each task. The course of the experiment was as follows. A domain model and a manual for the first VCR (VCR1) was created. Then the manual for a similar device, VCR2 was created. Both manuals contained similar set of functions (however the controls and operation were different in some cases). During the creation of the manual for VCR2 both linguistic and domain resource</context>
</contexts>
<marker>Hartley, Scott, Kruijff-Korbayouva, Sharoff, Teich, Sokolova, Staykova, Dochev, Cmajrek, Hana, 2000</marker>
<rawString>Antony Hartley, Donia Scott, I. Kruijff-Korbayouva, Serge Sharoff, E. Teich, Lena Sokolova, Kamenka Staykova, Danail Dochev, Martin Cmajrek, and Jiri Hana. 2000. Evaluation of the final prototype. Technical report, Brighton University, October, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie M Haydon</author>
</authors>
<title>The Complete Guide to Writing and Producing Technical Manuals.</title>
<date>1995</date>
<publisher>John Wiley and Sons, Ltd.</publisher>
<contexts>
<context position="24821" citStr="Haydon, 1995" startWordPosition="4054" endWordPosition="4056">indicates that text .fluency matters little for accomplishing user task. Sometimes users subjectively assessed a manual as having poor quality, nonetheless finding it useful for completing their task — Tad manual is better than no manual&apos;. Fourthly, a subjective evaluation of text quality may not be a true indication of manual worth with regard to performing a task. Task-oriented evaluation is a more objective way of assessing how helpful the manual is. The collected data and informal comments have given rise to the following suggestions to improve manual quality, that support those found in (Haydon, 1995): Firstly, the consistency and structure of a text are very important. All pertinent information should be presented in an overt form and at the place where it is vital. Secondly, small pieces of text are better than big dense chunks. Each piece should describe a 61 single function. 1995 EAGLES, Evaluation of Natural Language Processing Systems. FINAL REPORT, EAGLES DOC6 Conclusions and Further Work UMENT EWG-PR. Version of September 1995. In the course of FlexyCAT evaluation some novel experiments were carried out that have shown that the NLG system may be used to produce texts for different </context>
</contexts>
<marker>Haydon, 1995</marker>
<rawString>Leslie M. Haydon July 1995. The Complete Guide to Writing and Producing Technical Manuals. John Wiley and Sons, Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Levine</author>
<author>Chris Mellish</author>
</authors>
<title>The IDAS User Trials: Quantitative Evaluation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 5th European Workshop on NLG,</booktitle>
<pages>75--93</pages>
<institution>Rijks Universiteit Leiden.</institution>
<contexts>
<context position="4888" citStr="Levine and Mellish, 1995" startWordPosition="778" endWordPosition="781">tly compare the performance of different generation systems and formalisms. Comparative evaluation is not often used because of its complexity. An example of this kind of trial for the XTAG project is described in (Bangalore et al., 1998). Other aspects of NLG, such as knowledge reuse, time of document creation or system flexibility have not received enough attention so far. Although there were some reports on task-oriented evaluation, these experiments are not common and many of those did not include the comparison of the user performance on human-crafted and generated manuals, like in IDAS (Levine and Mellish, 1995). During FlexyCAT evaluation, we paid extra attention to task-oriented text quality, knowledge reuse and the benefits the author obtains by using a flexible planning technique. 3 FlexyCAT: System Description FlexyCAT is an NLG system for producing manuals for technical devices, mainly home appliances, e.g., TVs, VCRs, cameras, etc. Manuals consist of texts divided up into sections, each of which describes an individual procedure, consisting of a sequence of steps that the user is to perform to attain his/her goal. No generation of explanatory or warning information is included; generated texts</context>
</contexts>
<marker>Levine, Mellish, 1995</marker>
<rawString>John Levine and Chris Mellish. 1995. The IDAS User Trials: Quantitative Evaluation. In Proceedings of the 5th European Workshop on NLG, pages 75-93. Rijks Universiteit Leiden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Robert Dale</author>
</authors>
<title>Evaluation in the context of natural language generation.</title>
<date>1998</date>
<contexts>
<context position="2833" citStr="Mellish and Dale, 1998" startWordPosition="439" endWordPosition="442">ime of text revision (e.g., when domain changes) • Dynamic text generation upon user query • Potentially good knowledge re-use Unfortunately, most NLG systems evaluation carried out to date did not cover all these aspects. Usually NLG evaluation deals with 1) System quality; 2) Text quality. System quality evaluation consists of measuring characteristics like usability, generation speed 55 and system robustness. Such evaluation follows common patterns of software systems evaluation, see e.g., (Newman and Lamming, 1995). NLGspecific principles and metrics of system evaluation are described in (Mellish and Dale, 1998) and (EAGLES, 1995). One of the most thorough usability evaluation of an NLG system has been carried out in the AGILE project; the course of experiments and their results are presented in (Hartley et al., 2000). Assessing the quality of a generated text (based on (Mellish and Dale, 1998) and (Hartley et al., 2000)) includes rating of the text against such criteria as Accuracy, Fluency (including Acceptability and Grammaticality) and Lexico-grammar coverage. Methods of evaluation appropriate to these criteria can be grouped into three classes, as suggested in (Bangalore et al., 1998): Intrinsic</context>
</contexts>
<marker>Mellish, Dale, 1998</marker>
<rawString>Chris Mellish and Robert Dale. 1998. Evaluation in the context of natural language generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nestor Miliaev</author>
<author>Alison Cawsey</author>
<author>Greg Michaelson</author>
</authors>
<title>Technical Documentation: An Integrated Architecture for Supporting the Author in Generation and Resource Editing.</title>
<date>2002</date>
<booktitle>In The Tenth International Conference on Artificial Intelligence: Methodology, Systems, Applications A1MSA 2002,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Varna, Bulgaria,</location>
<contexts>
<context position="6652" citStr="Miliaev et al., 2002" startWordPosition="1055" endWordPosition="1058">conditions and actions are described in terms of elementary object properties. Objects, events and actions are represented using linguistic classes. A generation system uses both linguistic classes and domain knowledge for the production of a grammatical manual text in two languages. The output of FlexyCAT is a ready to use manual for the device. FlexyCAT&apos;s GUI provides the user with facilities for creating and editing a domain model and dictionaries of linguistic classes. It also allows the user to easily assign linguistic values to the elements of the domain model. For more information see (Miliaev et al., 2002). 56 Two main advantages of FlexyCAT compared &apos;motivation&apos; to &apos;means&apos;; in the second sentence with other NLG systems are: • FlexyCAT is an integrated tool allowing extensive editing of both linguistic and domain knowledge. This, and an object-oriented design of the knowledge, enables, firstly, good knowledge re-use, and, secondly, the production of manuals for a variety of different technical devices using a single NLG tool. • FlexyCAT offers a flexible approach to text planning. The planner produces a candidate text plan automatically, based on the domain model. When there is a necessity to o</context>
</contexts>
<marker>Miliaev, Cawsey, Michaelson, 2002</marker>
<rawString>Nestor Miliaev, Alison Cawsey, and Greg Michaelson. 2002. Technical Documentation: An Integrated Architecture for Supporting the Author in Generation and Resource Editing. In The Tenth International Conference on Artificial Intelligence: Methodology, Systems, Applications A1MSA 2002, Varna, Bulgaria, September 4-6. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Applied Natural Language Generation System. Natural Language Engineering.</title>
<date>2000</date>
<publisher>Cambridge University Pres.</publisher>
<contexts>
<context position="1953" citStr="Reiter and Dale, 2000" startWordPosition="302" endWordPosition="305">n-crafted texts; and secondly, how knowledge re-use allows us to extend the applicability of an NLG system for the description of different technical systems and to reduce time taken to create a document. 2 Existing practise of NLG System Evaluation Recently, it had become widely accepted that work in NLG should pay closer attention to the evaluation of results. The aspects of an NLG system to evaluate and the metrics to use for this are defined by the goals of the NLG system. The most common advantages of using NLG comparable with, e.g., Machine Translation (MT) are considered the following (Reiter and Dale, 2000): • High quality output text that is generated based on machine data and does not require post-processing • Simultaneous production of text versions in different languages • Consistency of text between the versions and with a domain model • Lower cost and time of text revision (e.g., when domain changes) • Dynamic text generation upon user query • Potentially good knowledge re-use Unfortunately, most NLG systems evaluation carried out to date did not cover all these aspects. Usually NLG evaluation deals with 1) System quality; 2) Text quality. System quality evaluation consists of measuring ch</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Applied Natural Language Generation System. Natural Language Engineering. Cambridge University Pres.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Roma Robertson</author>
<author>A Scott Lennox</author>
<author>Liesl Osman</author>
</authors>
<title>Using a Randomised Controlled Clinical Trial to Evaluate an NLG System. In</title>
<date>2001</date>
<booktitle>In Proceedings of ACL-2001,</booktitle>
<pages>434--441</pages>
<contexts>
<context position="4194" citStr="Reiter et al., 2001" startWordPosition="664" endWordPosition="667">out e.g., in AGILE (Hartley et al., 2000) and MIRADOR (Eddy and Cawsey, 2002); it is fairly simple and straightforward, however depends a great deal on the evaluators&apos; expertise and personal preferences. Extrinsic or task evaluation, where the user&apos;s ability to perform some task using a generated text or the impact of the text on the user behaviour is assessed. Extrinsic evaluation is less common because experiments are more difficult to carry out. However, it allows us to estimate the task-oriented worth of a document. This kind of evaluation was used in Isolde (Colineau et al., 2002), STOP (Reiter et al., 2001), and other NLG projects. Comparative where the objective is to directly compare the performance of different generation systems and formalisms. Comparative evaluation is not often used because of its complexity. An example of this kind of trial for the XTAG project is described in (Bangalore et al., 1998). Other aspects of NLG, such as knowledge reuse, time of document creation or system flexibility have not received enough attention so far. Although there were some reports on task-oriented evaluation, these experiments are not common and many of those did not include the comparison of the us</context>
</contexts>
<marker>Reiter, Robertson, Lennox, Osman, 2001</marker>
<rawString>Ehud Reiter, Roma Robertson, A. Scott Lennox, and Liesl Osman. 2001. Using a Randomised Controlled Clinical Trial to Evaluate an NLG System. In In Proceedings of ACL-2001, pages pp. 434-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Newman</author>
<author>Michael G Lamming</author>
</authors>
<title>Interactive System Design. Addison-Wesley Pub Co. 1st edition.</title>
<date>1995</date>
<contexts>
<context position="2734" citStr="Newman and Lamming, 1995" startWordPosition="423" endWordPosition="426">erent languages • Consistency of text between the versions and with a domain model • Lower cost and time of text revision (e.g., when domain changes) • Dynamic text generation upon user query • Potentially good knowledge re-use Unfortunately, most NLG systems evaluation carried out to date did not cover all these aspects. Usually NLG evaluation deals with 1) System quality; 2) Text quality. System quality evaluation consists of measuring characteristics like usability, generation speed 55 and system robustness. Such evaluation follows common patterns of software systems evaluation, see e.g., (Newman and Lamming, 1995). NLGspecific principles and metrics of system evaluation are described in (Mellish and Dale, 1998) and (EAGLES, 1995). One of the most thorough usability evaluation of an NLG system has been carried out in the AGILE project; the course of experiments and their results are presented in (Hartley et al., 2000). Assessing the quality of a generated text (based on (Mellish and Dale, 1998) and (Hartley et al., 2000)) includes rating of the text against such criteria as Accuracy, Fluency (including Acceptability and Grammaticality) and Lexico-grammar coverage. Methods of evaluation appropriate to th</context>
</contexts>
<marker>Newman, Lamming, 1995</marker>
<rawString>William M. Newman, Michael G. Lamming 1995. Interactive System Design. Addison-Wesley Pub Co. 1st edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>