<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014308">
<title confidence="0.9919455">
An LSA Implementation Against
Parallel Texts in French and English
</title>
<author confidence="0.751654">
Katri A. Clodfelder
</author>
<affiliation confidence="0.972819">
Indiana University
</affiliation>
<email confidence="0.992588">
kclodfel@indiana.edu
</email>
<sectionHeader confidence="0.993729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984363636364">
This paper presents the results of applying
the Latent Semantic Analysis (LSA)
methodology to a small collection of
parallel texts in French and English. The
goal of the analysis was to determine what
the methodology might reveal regarding
the difficulty level of either the machine-
translation (MT) task or the text-alignment
(TA) task.
In a perfectly parallel corpus where the
texts are exactly aligned, it is expected that
the word distributions between the two
languages be perfectly symmetrical.
Where they are symmetrical, the difficulty
level of the machine-translation or the text-
alignment task should be low. The results
of this analysis show that even in a
perfectly aligned corpus, the word
distributions between the two languages
deviate and because they do, LSA may
contribute much to our understanding of
the difficulty of the MT and TA tasks.
</bodyText>
<sectionHeader confidence="0.979001" genericHeader="keywords">
1. Credits
</sectionHeader>
<bodyText confidence="0.9992288">
This paper discusses an implementation of the
Latent Semantic Analysis (LSA) methodology against
a small collection of perfectly parallel texts in French
and English1. The texts were made available by the
HLT-NAACL and are taken from daily House journals
of the Canadian Parliament. They were edited by
Ulrich Germann. The LSA procedures were
implemented in F, a system for statistical computation
and graphics, and were written by John C. Paolillo at
Indiana University, Bloomington.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="introduction">
2. Introduction
</sectionHeader>
<bodyText confidence="0.914266">
LSA is an analytical methodology that uses
mathematical procedures and vector space modeling
</bodyText>
<footnote confidence="0.9619495">
1 At present, the collection of texts used consists of 30
English and 30 French documents, all perfectly mated.
Cognates were not distinguished between languages, e.g.,
revolution is counted as both a French and an English term.
</footnote>
<bodyText confidence="0.999919763157895">
techniques to generate an abstract, numerical
representation of the relationships among words and
documents in a collection of texts (the corpus). In this
analysis, the methodology is used to identify the
symmetry that exists among the pattern of
relationships and associations in the parallel texts.
Where texts are perfectly aligned, it is expected that
for every occurrence of a word in one language, an
exact correspondent exists in the other language.
However, the analysis shows that even in a perfectly
aligned corpus, the word distributions between the two
languages deviate and a one-to-one association does
not exist.
An example of how word symmetry breaks down
in parallel texts can be seen in two &amp;quot;sets&amp;quot; of parallel
documents, F1-E1 and F2-E2. In these paired
documents, the cross-language term correspondence
between the French term &amp;quot;je&amp;quot; and the English term &amp;quot;I&amp;quot;
in the two sets shows that in the first pair, &amp;quot;je&amp;quot; occurs
42 times and &amp;quot;I&amp;quot; occurs only 37 times. In the second
pair, &amp;quot;je&amp;quot; occurs 59 times in the French document and
62 times in the English document. Such differences in
word usage patterns between corresponding terms are
very common and create difficulties for the MT or TA
tasks.
Because of the way LSA represents word-usage
associations and patterns among documents and terms,
it may have much to offer in understanding the
difficulty levels of these tasks. This analysis shows
that, in spite of usage differences resulting in non-
symmetrical cross-language word distributions
between the corresponding terms of any given
language pair, the LSA methodology is capable of
identifying the appropriate usage pattern for each of
the terms, within its own language. This paper
presents a first look at the alignment patterns found in
a parallel corpus and how LSA may offer some
insights into the MA and TA tasks.
</bodyText>
<sectionHeader confidence="0.985669" genericHeader="method">
3. LSA
</sectionHeader>
<bodyText confidence="0.999358545454546">
The LSA methodology begins with the term-by-
document matrix, an n x m matrix where each value in
the matrix is the frequency of the nth word in the mth
document. A weighting procedure is applied that
weights each of the term frequencies (TF) by the
inverse document frequency (IDF)2 (Salton, G. et al
1968). A very powerful mathematical procedure,
known as singular value decomposition (SVD), is then
performed against the transformed matrix. SVD
permits the reduction of any n x m matrix to a set of
three matrices, such that M = UΣVT, where
</bodyText>
<equation confidence="0.8762158">
U = ( m x m matrix of left singular vectors )
Σ = ( n x m diagonal matrix containing the
singular values of �)
[IT = ( n x n transposed row matrix of the
right singular vectors ).
</equation>
<bodyText confidence="0.999580571428572">
While the SVD solution of any given matrix can re-
create the original matrix, exactly, its primary value
lies in its capacity to infer what the pattern of
relationships and associations is for the words in the
documents, if all the linguistic data in the corpus is
represented on a smaller number of dimensions than
that of the original matrix (Landauer et al 1998).
</bodyText>
<sectionHeader confidence="0.996509" genericHeader="method">
4. Interpretation
</sectionHeader>
<bodyText confidence="0.996953166666667">
Although it is useful to think of the &amp;quot;dimensions&amp;quot;
as representing the document vectors, it should be
emphasized that a &amp;quot;dimension&amp;quot; is a more abstract
notion related to the contribution that a given
document vector makes in explaining the relationships
and associations of the linguistic data contained in the
corpus. In this section, an example of how the SVD
procedure depicts the word and document
relationships on the different dimensions is discussed.
In Figure 1, the location of the documents is
shown on Dimensions 1 and 2. Notice that on
Dimension 2 (the vertical axis), all of the English
documents (En) lie above zero and all of the French
documents (Fn) lie below. Moreover, the
corresponding French and English document pairs are
almost perfectly aligned across the horizontal axis. As
an abstract notion, Dimension 2 clearly represents the
two languages.
On Dimension 1, the documents occur from left to
right, in the order of smallest document to largest
document, making this dimension representative of
document size. No attention was paid to ordering the
documents according to size when the data was input
into the model. The LSA model is able to identify this
relationship, automatically, and represent it, as shown
here in Figure 1.
2 The IDF of term, t, is the ratio of the total number of
documents to the total number of documents in which the
given term, t, occurs, e.g., if term, t, occurred in 6 of the 60
documents in the Corpus, its IDF ratio is 60/6 (or 10).
</bodyText>
<figureCaption confidence="0.980501714285714">
Document Plots on Dimensions 1 and 2
Figure 1
Figure 2 shows the word relationships on
Dimensions 1 and 2. The English words are
represented by &amp;quot;x&amp;quot; and the French words by &amp;quot;+&amp;quot;.
Word Plots on Dimensions 1 and 2
Figure 2
</figureCaption>
<bodyText confidence="0.981056">
Where the two symbols cross, the data point represents
a cognate.3 In the same way that the documents were
split with the English documents occuring above zero
3 The term cognate is used to include words such as &amp;quot;chose&amp;quot;,
which means &amp;quot;thing&amp;quot; in French and is the past tense of &amp;quot;to
choose&amp;quot; in English; &amp;quot;pays&amp;quot;, which is &amp;quot;country&amp;quot; in French and
present tense of &amp;quot;to pay&amp;quot; in English; and so on.
and the French documents below in Figure 1, so are
the English and French words split in Figure 2. As
shown, the English words are located above zero on
the vertical axis and the French words are located
below. Whether a cognate is represented as an English
term and appears above zero or as a French term
appearing below, is entirely dependent on which set of
language documents &amp;quot;drives&amp;quot; the association for the
cognate.
Figure 2 shows that, in spite of the very high
degree of symmetry between the document-pairs of
the French and English texts (Figure 1), the cross-
language patterns of association among the words in
the documents are not completely symmetrical. If
they were, there would be a corresponding &amp;quot;x&amp;quot; sign
above the horizontal axis in exactly the same position
as every &amp;quot;+&amp;quot; sign below the axis. From an MT or TA
perspective, the greater the degree of cross-language
symmetry among words in the documents, the easier
the task of selecting the appropriate target term. When
cross-language symmetry is low, however, the task of
finding the appropriate target term is more difficult.
</bodyText>
<sectionHeader confidence="0.830805" genericHeader="method">
5. Symmetry of Query Results
</sectionHeader>
<bodyText confidence="0.999965111111111">
For the most part, single-term queries accurately
identified the most relevant, same-language
documents and they did so in spite of non-
symmetrical, language-specific, usage-associations of
the query terms. For example, in Table 1, the query
using the English term &amp;quot;aboriginals&amp;quot; returned E22 as
its most relevant document; however, the query using
the corresponding French term &amp;quot;autochtones&amp;quot; returned
F09 as its most relevant document.
</bodyText>
<figure confidence="0.521384">
Query E
ABORIGINALS
E22
E09
E17
E11
E27
E19
E12
E07
E24
E29
</figure>
<tableCaption confidence="0.752563">
Aboriginal-Autochtones Query Results
Table 1
</tableCaption>
<bodyText confidence="0.999961678571429">
At first glance, these query results would seem to be
undesirable. However, they are perfectly consistent
with the language-specific usage patterns of these two
terms.
In French, because of number agreement between
adjectives and nouns, the plural form of &amp;quot;autochtone&amp;quot;
is used quite frequently in comparison to the plural
form of its English counterpart &amp;quot;aboriginal&amp;quot;, where
number agreement is not required. For example, the
French usage of &amp;quot;les (peuples) autochtones&amp;quot; is often
realized as &amp;quot;the aboriginal people(s)&amp;quot; in the
corresponding English document. The impact of this
non-symmetrical usage pattern of corresponding
language terms is seen in the query results. While the
most relevant French document, F09, contains 50
occurrences of the plural &amp;quot;autochtones&amp;quot;, its
corresponding English document (returned as second
relevant in Query E) contains two occurrences of the
plural &amp;quot;aboriginals&amp;quot; and 49 occurrences of the singular
&amp;quot;aboriginal&amp;quot;.
Continuing on with this example, Query E
identified the English document, E22, as the most
relevant to the query term. E22 contains nine
occurrences of the plural &amp;quot;aboriginals&amp;quot; and no
occurrences of the singular &amp;quot;aboriginal&amp;quot;. Thus, the
results of Queries E and F shown in Table 1
demonstrate that not only is the LSA methodology
sensitive to the language-specific word distributions of
cross-language word pairs, it is also capable of
distinguishing those distributional variations in order
to identify the most relevant documents for the
language of the query term, accordingly. In other
words, given the dissimilarity in the distributions of
the plural forms of each term in the cross-language
word pair, the LSA methodology behaved
appropriately for each of the queries.
The order of the documents returned as relevant to
the queries in Table 1 is important from an MT and
TA perspective also because it shows that LSA has
some capability to &amp;quot;align&amp;quot; similar, but not exact,
terms. For example, in Query E, the first three
documents all contain the exact query term,
&amp;quot;aboriginals&amp;quot;. The next five documents contain only
the singular form of the query term. The remaining
two documents do not contain either form of the query
term. In other words, after the documents that
contained the exact query term, the LSA methodology
chose as more relevant, documents containing the
singular form of the query term over documents that
contained no form of the query term. If the LSA
methodology were only identifying relevant
documents on the basis of finding terms with an exact
match to the query term, it would have no preference
for choosing documents containing the singular form
of the query term over documents that contained no
form of the query term.
</bodyText>
<figure confidence="0.941584916666667">
Query F
AUTOCHTONES
F09
F17
F07
F06
F27
F19
F12
F11
F04
F23
</figure>
<sectionHeader confidence="0.957832" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999977">
This paper presented a brief discussion on the
possibility that the LSA methodology may have
something to contribute with respect to identifying the
difficulty level of the MT and TA tasks. In particular,
it was shown that LSA can represent the symmetrical
and non-symmetrical relationships that exist among
the terms in cross-language document pairs. It was
also shown that LSA has some capability to &amp;quot;align&amp;quot;
similar terms in order to identify relevant documents
in response to a query.
Such positive results using the large, non-
homegenous documents that were used in this analysis
are very promising and suggest that further research in
this area is needed. Additional work is planned that
will separate the documents into smaller, syntactical
units that will be analyzed using a very similar
approach. However, the words and &amp;quot;documents&amp;quot;
represented in the semantic space that is generated by
these very small syntactic units will have very
different associations and relationships than they do
as part of the larger, non-homegenous texts.
It is believed that by restricting the input texts to
very small syntactic units, the LSA methodology will
be able to make the proper &amp;quot;alignment&amp;quot; associations
between the cross-language word pairs and, as a result,
provide some information regarding the types of word
pairs that are most difficult to align by an MT or TA
system.
Finally, the small number of documents used in
this analysis4 raises questions about the optimal
number of input texts that are needed to obtain valid
results using the LSA methodology. It may be that a
much smaller number of input texts is needed than
formerly believed, in order to &amp;quot;train&amp;quot; an LSA-based
system to correlate the appropriate cross-language
word pairs.
</bodyText>
<sectionHeader confidence="0.996458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.941909272727273">
Laundauer, Foltz, Laham. 1998. &amp;quot;An Introduction to
Latent Semantic Analysis.&amp;quot; Discourse Processses, 25,
259-284. 1998.
Rehder, Bob, Michael L. Littman, Susan Dumais,
Thomas K. Landauer. 1997. &amp;quot;Automatic 3-Language
Cross-Language Information Retrieval with Latent
Semantic Indexing.&amp;quot; TREC-6, 233-239.
Salton, G., and Lesk, M. E. 1968. &amp;quot;Computer
evaluation of indexing and text processing.&amp;quot; Journal
of the Association for Computing Machinery 15.1.
4 30 mated pairs (or 60 documents in total)
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.478491">
<title confidence="0.9088585">An LSA Implementation Parallel Texts in French and English</title>
<author confidence="0.588799">A Katri</author>
<affiliation confidence="0.974451">Indiana</affiliation>
<email confidence="0.999831">kclodfel@indiana.edu</email>
<abstract confidence="0.993833272727273">This paper presents the results of applying the Latent Semantic Analysis (LSA) methodology to a small collection of parallel texts in French and English. The goal of the analysis was to determine what the methodology might reveal regarding the difficulty level of either the machinetranslation (MT) task or the text-alignment (TA) task. In a perfectly parallel corpus where the texts are exactly aligned, it is expected that the word distributions between the two languages be perfectly symmetrical. Where they are symmetrical, the difficulty level of the machine-translation or the textalignment task should be low. The results of this analysis show that even in a perfectly aligned corpus, the word distributions between the two languages deviate and because they do, LSA may contribute much to our understanding of the difficulty of the MT and TA tasks. 1. Credits This paper discusses an implementation of the Latent Semantic Analysis (LSA) methodology against a small collection of perfectly parallel texts in French The texts were made available by the HLT-NAACL and are taken from daily House journals of the Canadian Parliament. They were edited by Ulrich Germann. The LSA procedures implemented in F, a system for statistical computation and graphics, and were written by John C. Paolillo at</abstract>
<affiliation confidence="0.951293">Indiana University, Bloomington.</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Foltz Laundauer</author>
<author>Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.&amp;quot;</title>
<date>1998</date>
<booktitle>Discourse Processses,</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<marker>Laundauer, Laham, 1998</marker>
<rawString>Laundauer, Foltz, Laham. 1998. &amp;quot;An Introduction to Latent Semantic Analysis.&amp;quot; Discourse Processses, 25, 259-284. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Rehder</author>
<author>Michael L Littman</author>
<author>Susan Dumais</author>
<author>Thomas K Landauer</author>
</authors>
<title>Automatic 3-Language Cross-Language Information Retrieval with Latent Semantic Indexing.&amp;quot;</title>
<date>1997</date>
<tech>TREC-6,</tech>
<pages>233--239</pages>
<marker>Rehder, Littman, Dumais, Landauer, 1997</marker>
<rawString>Rehder, Bob, Michael L. Littman, Susan Dumais, Thomas K. Landauer. 1997. &amp;quot;Automatic 3-Language Cross-Language Information Retrieval with Latent Semantic Indexing.&amp;quot; TREC-6, 233-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M E Lesk</author>
</authors>
<title>Computer evaluation of indexing and text processing.&amp;quot;</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery</journal>
<volume>15</volume>
<note>mated pairs (or 60 documents in total)</note>
<marker>Salton, Lesk, 1968</marker>
<rawString>Salton, G., and Lesk, M. E. 1968. &amp;quot;Computer evaluation of indexing and text processing.&amp;quot; Journal of the Association for Computing Machinery 15.1. 4 30 mated pairs (or 60 documents in total)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>