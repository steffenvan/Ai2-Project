<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002361">
<title confidence="0.9922565">
How Well can We Learn
Interpretable Entity Types from Text?
</title>
<author confidence="0.998226">
Dirk Hovy
</author>
<affiliation confidence="0.99819">
Center for Language Technology
University of Copenhagen
</affiliation>
<address confidence="0.863465">
Njalsgade 140, 2300 Copenhagen
</address>
<email confidence="0.998314">
dirk@cst.dk
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969409090909">
Many NLP applications rely on type sys-
tems to represent higher-level classes.
Domain-specific ones are more informa-
tive, but have to be manually tailored to
each task and domain, making them in-
flexible and expensive. We investigate a
largely unsupervised approach to learning
interpretable, domain-specific entity types
from unlabeled text. It assumes that any
common noun in a domain can function as
potential entity type, and uses those nouns
as hidden variables in a HMM. To con-
strain training, it extracts co-occurrence
dictionaries of entities and common nouns
from the data. We evaluate the learned
types by measuring their prediction ac-
curacy for verb arguments in several do-
mains. The results suggest that it is pos-
sible to learn domain-specific entity types
from unlabeled data. We show significant
improvements over an informed baseline,
reducing the error rate by 56%.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999460738095238">
Many NLP applications, such as question answer-
ing (QA) or information extraction (IE), use type
systems to represent relevant semantic classes.
Types allow us to find similarities at a higher level
to group lexically different entities together. This
helps to filter out candidates that violate certain
constraints (e.g., in QA, if the intended answer
type is PERSON, we can ignore all candidate an-
swers with a different type), but is also used for
feature generation and fact-checking.
A central question is: where do the types
come from? Typically, they come from a hand-
constructed set. This has some disadvantages.
Domain-general types, such as named entities or
WordNet supersenses (Fellbaum, 1998), often fail
to capture critical domain-specific information (in
the medical domain, we might want ANTIBI-
OTIC, SEDATIVE, etc., rather than just ARTI-
FACT). Domain-specific types perform much bet-
ter (Ferrucci et al., 2010), but must be manually
adapted to each new domain, which is expensive.
Alternatively, unsupervised approaches (Ritter et
al., 2010) can be used to learn clusters of similar
words, but the resulting types (=cluster numbers)
are not human-interpretable, which makes analy-
sis difficult. Furthermore, it requires us to define
the number of clusters beforehand.
Ideally, we would like to learn domain-specific
types directly from data. To this end, pattern-
based approaches have long been used to induce
type systems (Hearst, 1992; Kozareva et al., 2008).
Recently, Hovy et al. (2011) proposed an ap-
proach that uses co-occurrence patterns to find en-
tity type candidates, and then learns their appli-
cability to relation arguments by using them as la-
tent variables in a first-order HMM. However, they
only evaluate their method using human sensibil-
ity judgements for one domain. While this shows
that the types are coherent, it does not tell us much
about their applicability.
We extend their approach with three important
changes:
</bodyText>
<listItem confidence="0.64802625">
1. we evaluate the types by measuring accuracy
when using them in an extrinsic task,
2. we evaluate on more than one domain, and
3. we explore a variety of different models.
</listItem>
<bodyText confidence="0.999781125">
We measure prediction accuracy when us-
ing the learned types in a selectional restriction
task for frequent verbs. E.g., given the rela-
tion throw(X, pass) in the football domain, we
compare the model prediction to the gold data
X=QUARTERBACK. The results indicate that the
learned types can be used to in relation extraction
tasks.
</bodyText>
<page confidence="0.980574">
482
</page>
<bodyText confidence="0.756401">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 482–487,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Our contributions in this paper are:
</bodyText>
<listItem confidence="0.9361204">
• we empirically evaluate an approach to learn-
ing types from unlabeled data
• we investigate several domains and models
• the learned entity types can be used to predict
selectional restrictions with high accuracy
</listItem>
<sectionHeader confidence="0.999583" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997924137931">
In relation extraction, we have to identify the re-
lation elements, and then map the arguments to
types. We follow an open IE approach (Banko and
Etzioni, 2008) and use dependencies to identify
the elements. In contrast to most previous work
(Pardo et al., 2006; Yao et al., 2011; Yao et al.,
2012), we have no pre-defined set of types, but
try to learn it along with the relations. Some ap-
proaches use types from general data bases such
as Wikipedia, Freebase, etc. (Yan et al., 2009;
Eichler et al., 2008; Syed and Viegas, 2010), side-
stepping the question how to construct those DBs
in the first place. We are less concerned with ex-
traction performance, but focus on the accuracy of
the learned type system by measuring how well it
performs in a prediction task.
Talukdar et al. (2008) and Talukdar and Pereira
(2010) present graph-based approaches to the sim-
ilar problem of class-instance learning. While
this provides a way to discover types, it requires
a large graph that does not easily generalize to
new instances (transductive), since it produces no
predictive model. The models we use are trans-
ductive and can be applied to unseen data. Our
approach follows Hovy et al. (2011). However,
they only evaluate one model on football by col-
lecting sensibility ratings from Mechanical Turk.
Our method provides extrinsic measures of perfor-
mance on several domains.
</bodyText>
<sectionHeader confidence="0.992719" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.998783222222222">
Our goal is to find semantic type candidates in the
data, and apply them in relation extraction to see
which ones are best suited. We restrict ourselves
to verbal relations. We build on the approach by
Hovy et al. (2011), which we describe briefly be-
low. It consists of two parts: extracting the type
candidates and fitting the model.
The basic idea is that semantic types are usu-
ally common nouns, often frequent ones from the
</bodyText>
<figureCaption confidence="0.900529">
Figure 1: Exampleof input sentence x and out-
quarterback
</figureCaption>
<bodyText confidence="0.866920666666667">
b)
put types for the HMM. Note that the verb type is
throw payer ball
treated as observed variable.
domain at hand. Thus all common nouns are pos-
sible types, and can be used as latent variables in
</bodyText>
<equation confidence="0.97252">
x2 x3
</equation>
<bodyText confidence="0.9965232">
an HMM. By estimating emission and transition
thow Mntan ball
parameters with EM, we can learn the subset of
nouns to apply.
However, assuming the set of all common
nouns as types is intractable, and would not al-
low for efficient learning. To restrict the search
space and improve learning, we first have to learn
which types modify entities and record their co-
occurrence, and use this as dictionary.
</bodyText>
<equation confidence="0.980099">
Kleiman: professor:25, expert:13, (specialist:1)
Tilton: executive:37, economist:17, (chairman:4, presi-
dent:2)
</equation>
<figureCaption confidence="0.9692695">
Figure 2: Examples of dictionary entries with
counts. Types in brackets are not considered.
</figureCaption>
<bodyText confidence="0.997446727272727">
Dictionary Construction The number of com-
mon nouns in a domain is generally too high to
consider all of them for every entity. A com-
mon way to restrict the number of types is to pro-
vide a dictionary that lists all legal types for each
entity (Merialdo, 1994; Ravi and Knight, 2009;
T¨ackstr¨om et al., 2013). To construct this dictio-
nary, we collect for each entity (i.e., a sequence
of words labeled with NNP or NNPS tags) in our
data all common nouns (NN, NNS) that modify it.
These are
</bodyText>
<listItem confidence="0.998755">
1. nominal modifiers (“judge Scalosi ...”),
2. appositions (“Tilton, a professor at ...”), and
3. copula constructions (“Finton, who is the in-
vestor ...”).
</listItem>
<bodyText confidence="0.9579865">
These modifications can be collected from the de-
pendency parse trees. For each entity, we store the
</bodyText>
<figure confidence="0.909543">
quarterback throw ball
player
Montana throw ball
y1 y2
x1 x3
y3
</figure>
<page confidence="0.997665">
483
</page>
<bodyText confidence="0.962614959183673">
type candidates and their associated counts. See
Figure 2 for examples. We only consider types
observed more than 10 times. Any entity with-
out type information, as well as dictionary entities
with only singleton types are treated as unknown
tokens (“UNK”). We map UNK to the 50 most
common types in the dictionary. Verbs are con-
sidered to each have their own type, i.e., token and
label for verbs are the same.
We do not modify this step.
Original Model Hovy et al. (2011) construct
a HMM using subject-verb-object (SVO) parse
triples as observations, and the type candidates as
hidden variables. Similar models have been used
in (Abney and Light, 1999; Pardo et al., 2006).
We estimate the free model parameters with EM
(Dempster et al., 1977), run for a fixed number of
iterations (30) or until convergence.
Note that Forward-backward EM has time com-
plexity of O(N2T), where N is the number of
states, and T the number of time steps. T = 3 in
the model formulations used here, but N is much
larger than typically found in NLP tasks (see also
Table 3). The only way to make this tractable is
to restrict the free parameters the model needs to
estimate to the transitions.
The model is initialized by jointly normalizing
1 the dictionary counts to obtain the emission pa-
rameters, which are then fixed (except for the un-
known entities (P(word = UNK|type = ·)). Tran-
sition parameters are initialized uniformly (re-
stricted to potentially observable type sequences),
and kept as free parameters for the model to opti-
mize.
Common nouns can be both hidden variables
and observations in the model, so they act like an-
notated items: their legal types are restricted to the
identity. All entities are thus constrained by the
dictionary, as in (Merialdo, 1994). To further con-
strain the model, only the top three types of each
entity are considered. Since the type distribution
typically follows a Zipf curve, this still captures
most of the information.
1This preserves the observed entity-specific distributions.
Under conditional normalization, the type candidates from
frequent entities tend to dominate those of infrequent entities.
I.e., the model favors an unlikely candidate for entity a if it is
frequent for entity b.
The model can be fully specified as
</bodyText>
<equation confidence="0.9930635">
3
P(x, y) = P(y1)·P(x1|y1) ri P(yi|yi−1)·P(xi|yi)
i=2
(1)
</equation>
<bodyText confidence="0.9999165">
where x is an input triple of a verb and its argu-
ments, and y a sequence of types.
</bodyText>
<sectionHeader confidence="0.998143" genericHeader="method">
4 Extending the Model
</sectionHeader>
<bodyText confidence="0.999939416666667">
The model used by Hovy et al. (2011) was a sim-
ple first order HMM, with the elements in SVO or-
der (see Figure 3a). We observe two points: we al-
ways deal with the same number of elements, and
we have observed variables. We can thus move
from a sequential model to a general graphical
model by adding transitions and re-arranging the
structure.
Since we do not model verbs (they each have
their identity as type), they act like observed vari-
ables. We can thus move them in first position and
condition the subject on it (3b).
</bodyText>
<equation confidence="0.800135">
y1 y2
S V O
c) d)
V
y1 y2
S O
</equation>
<figureCaption confidence="0.9306915">
Figure 3: Original SVO. model (a), modified VSO
order (b), extension to general models (c and d)
</figureCaption>
<bodyText confidence="0.999897642857143">
By adding additional transitions, we can con-
strain the latent variables further. This is similar
to moving from a first to a second order HMM. In
contrast to the original model, we also distinguish
between unknown entities in the first and second
argument position.
The goal of these modifications is to restrict the
number of potential values for the argument po-
sitions. This allows us to use the models to type
individual instances. In contrast, the objective in
Hovy et al. (2011) was to collect frequent relation
templates from a domain to populate a knowledge
base.
The modifications presented here extend to
</bodyText>
<equation confidence="0.650215285714286">
a) b)
y1 y2 y3
y1 y2
S O
V
S O
V
</equation>
<page confidence="0.990703">
484
</page>
<table confidence="0.999002714285714">
system Football Finances Law
arg1 arg2 avg ABL arg1 arg2 avg ABL arg1 arg2 avg ABL
baseline 0.28 0.26 0.27 — 0.39 0.42 0.41 — 0.37 0.32 0.35 —
orig. 0.05 0.23 0.14 –0.13 0.08 0.39 0.23 –0.18 0.06 0.31 0.18 –0.17
VSO, seq. 0.37 0.28 0.32 +0.05 0.38 0.45 0.41 0.0 0.45 0.37 0.41 +0.06
SVO, net 0.63 0.60 0.62 +0.35 0.55 0.63 0.59 +0.18 0.69 0.68 0.68 +0.33
VSO, net 0.66 0.58 0.62 +0.35 0.61 0.54 0.57 +0.16 0.71 0.62 0.66 +0.31
</table>
<tableCaption confidence="0.99790475">
Table 1: Accuracy for most frequent sense baseline and different models on three domains. Italic num-
bers denote significant improvement over baseline (two-tailed t-test at p &lt; 0.01). ABL = difference to
baseline.
Table 2: Mean reciprocal rank for models on three domains.
</tableCaption>
<figure confidence="0.997351815789474">
VSO, seq.
VSO, net
system
orig.
SVO, net
arg1 arg2
0.17 0.38
0.56 0.42
0.75 0.69
0.78 0.67
Football
0.27
0.49
0.72
0.72
avg
arg1 arg2
0.18 0.52
0.55 0.58
0.68 0.73
0.74 0.66
Finances
0.35
0.57
0.71
0.70
avg
arg1 arg2
0.17 0.48
0.61 0.51
0.78 0.77
0.81 0.72
Law
0.32
0.56
0.78
0.76
avg
</figure>
<bodyText confidence="0.8657135">
verbs with more than two arguments, but in the
present paper, we focus on binary relations.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999984722222222">
Since the labels are induced dynamically from the
data, traditional precision/recall measures, which
require a known ground truth, are difficult to ob-
tain. Hovy et al. (2011) measured sensibility by
obtaining human ratings and measuring weighted
accuracies over all relations. While this gives an
intuition of the general methodology, it is harder
to put in context. Here, we want to evaluate the
model’s performance in a downstream task. We
measure its ability to predict the correct types for
verbal arguments. We evaluate on three different
domains.
As test case, we use a cloze test, or fill-in-the-
blank. We select instances that contain a type-
candidate word in subject or object position and
replace that word with the unknown token. We can
then compare the model’s prediction to the origi-
nal word to measure accuracy.
</bodyText>
<subsectionHeader confidence="0.975342">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.9998692">
Like Yao et al. (2012) and Hovy et al. (2011), we
derive our data from the New York Times (NYT)
corpus (Sandhaus, 2008). It contains several years
worth of articles, manually annotated with meta-
data such as author, content, etc. Similar to Yao
et al. (2012), we use articles whose content meta-
data field contains certain labels to distinguish data
from different domains. We use the labels Foot-
ball2, Law and Legislation, and Finances.
We remove meta-data and lists, tokenize, parse,
and lemmatize all articles. We then automatically
extract subject-verb-object (SVO) triples from the
parses, provided the verb is a full verb. Similarly
to (Pardo et al., 2006), we focus on the top 100
full verbs for efficiency reasons, though nothing
in our approach prevents us from extending it to
all verbs. For each domain, we select all instances
which have a potential type (common noun) in at
least one argument position. These serve as cor-
pus.
</bodyText>
<table confidence="0.578639">
Football Finances Law
unique types 7,139 18,186 10,618
unique entities 38,282 27,528 12,782
</table>
<tableCaption confidence="0.978184">
Table 3: Statistics for the three domains.
</tableCaption>
<bodyText confidence="0.971249375">
As test data, we randomly select a subset of
1000 instances for each argument, provided they
contain one of the 50 most frequent types in sub-
ject or object position, such as player in “player
throw pass”. This serves as gold data. We then
replace those types by UNK (i.e., we get “UNK
throw pass”) and use this as test set for our model.3
Table 3 shows that the domains vary with re-
</bodyText>
<footnote confidence="0.9941135">
2The data likely differs from Hovy et al. (2011).
3We omit cases with two unknown arguments, since this
</footnote>
<page confidence="0.998455">
485
</page>
<bodyText confidence="0.999966333333333">
spect to the ratio of unique types to unique enti-
ties. Football uses many different entities (e.g.,
team and player names), but has few types (e.g.,
player positions), while the other domains use
more types, but fewer entities (e.g., company
names, law firms, etc.).
</bodyText>
<subsectionHeader confidence="0.998595">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999859">
We run Viterbi decoding on each test set with our
trained model to predict the most likely type for
the unknown entities. We then compare these pre-
dictions to the type in the respective gold data and
compute the accuracy for each argument position.
As baseline, we predict the argument types most
frequently observed for the particular verb in train-
ing, e.g., predict PLAYER as subject of tackle in
football. We evaluate the influence of the different
model structures on performance.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999800428571429">
Table 1 shows the accuracy of the different mod-
els in the prediction task for the three different do-
mains. The low results of the informed baseline
indicate the task complexity.
We note that the original model, a bigram HMM
with SVO order (Figure 3a), fails to improve accu-
racy over the baseline (although its overall results
were judged sensible). Changing the input order
to VSO (Figure 3b) improves accuracy for both
arguments over SVO order and the baseline, albeit
not significantly. The first argument gains more,
since conditioning the subject type on the (unam-
biguous) verb is more constrained than starting out
with the subject. Conditioning the object directly
upon the subject creates sparser bigrams, which
capture “who does what to whom”.
Moving from the HMMs to a general graphi-
cal model structure (Figures 3c and d) creates a
sparser distribution and significantly improves ac-
curacy across the board. Again, the position of the
verb makes a difference: in SVO order, accuracy
for the second argument is better, while in VSO
order, accuracy for the subject increases. This in-
dicates that direct conditioning on the verb is the
strongest predictor. Intuitively, knowing the verb
restricts the possible arguments much more than
knowing the arguments restrict the possible verbs
(the types of entities who can throw something are
becomes almost impossible to predict without further context,
even for humans (compare “UNK make UNK”).
limited, but knowing that the subject is a quarter-
back still allows all kinds of actions).
We also compute the mean reciprocal rank
(MRR) for each condition (see Table 2). MRR de-
notes the inverse rank in the model’s k-best output
at which the correct answer occurs, i.e., k. The
result gives us an intuition of “how far off” the
model predictions are. Across domains, the cor-
rect answer is found on average among the top
two (rank 1.36). Note that since MRR require k-
best outputs, we cannot compute a measure for the
baseline.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999948333333333">
We evaluated an approach to learning domain-
specific interpretable entity types from unlabeled
data. Type candidates are collected from patterns
and modeled as hidden variables in graphical mod-
els. Rather than using human sensibility judge-
ments, we evaluate prediction accuracy for selec-
tional restrictions when using the learned types in
three domains. The best model improves 35 per-
centage points over an informed baseline. On av-
erage, we reduce the error rate by 56%. We con-
clude that it is possible to learn interpretable type
systems directly from data.
</bodyText>
<sectionHeader confidence="0.996926" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998501333333333">
The author would like to thank Victoria Fossum,
Eduard Hovy, Kevin Knight, and the anonymous
reviewers for their invaluable feedback.
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990425166666667">
Steven Abney and Marc Light. 1999. Hiding a seman-
tic hierarchy in a Markov model. In Proceedings
of the ACL Workshop on Unsupervised Learning in
Natural Language Processing, volume 67.
Michele Banko and Oren. Etzioni. 2008. The trade-
offs between open and traditional relation extraction.
Proceedings ofACL-08: HLT, pages 28–36.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39(1):1–
38.
Kathrin Eichler, Holmer Hemsen, and G¨unter Neu-
mann. 2008. Unsupervised relation extraction
from web documents. LREC. http://www. lrecconf.
org/proceedings/lrec2008.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
</reference>
<page confidence="0.995368">
486
</page>
<reference confidence="0.994496066666667">
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59–79.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539–545. Association for Compu-
tational Linguistics.
Dirk Hovy, Chunliang Zhang, Eduard Hovy, and
Anselmo Pe˜nas. 2011. Unsupervised discovery of
domain-specific knowledge from text. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 1466–1475, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. Proceedings of
ACL-08: HLT, pages 1048–1056.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155–171.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Struc-
tures. Computational Linguistics and Intelligent
Text Processing, pages 59–70.
Sujith Ravi and Kevin Knight. 2009. Minimized
Models for Unsupervised Part-of-Speech Tagging.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 504–512. Association
for Computational Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet allocation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 424–434, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Evan Sandhaus, editor. 2008. The New York Times An-
notated Corpus. Number LDC2008T19. Linguistic
Data Consortium, Philadelphia.
Zareen Syed and Evelyne Viegas. 2010. A hybrid
approach to unsupervised relation discovery based
on linguistic analysis and semantic typing. In Pro-
ceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology
for Learning by Reading, pages 105–113. Associ-
ation for Computational Linguistics.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Partha P. Talukdar and Fernando Pereira. 2010. Ex-
periments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1473–1481.
Association for Computational Linguistics.
Partha P. Talukdar, Joseph Reisinger, Marcus Pas¸ca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 582–
590. Association for Computational Linguistics.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining wikipedia texts using
information from the web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1021–1029. Association for
Computational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456–1466. Association
for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense
disambiguation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 712–720.
Association for Computational Linguistics.
</reference>
<page confidence="0.998399">
487
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.854102">
<title confidence="0.9993765">How Well can We Learn Interpretable Entity Types from Text?</title>
<author confidence="0.98308">Dirk</author>
<affiliation confidence="0.999551">Center for Language University of</affiliation>
<address confidence="0.927981">Njalsgade 140, 2300</address>
<email confidence="0.986717">dirk@cst.dk</email>
<abstract confidence="0.997504434782609">Many NLP applications rely on type systems to represent higher-level classes. Domain-specific ones are more informative, but have to be manually tailored to each task and domain, making them inflexible and expensive. We investigate a largely unsupervised approach to learning interpretable, domain-specific entity types from unlabeled text. It assumes that any common noun in a domain can function as potential entity type, and uses those nouns as hidden variables in a HMM. To constrain training, it extracts co-occurrence dictionaries of entities and common nouns from the data. We evaluate the learned types by measuring their prediction accuracy for verb arguments in several domains. The results suggest that it is possible to learn domain-specific entity types from unlabeled data. We show significant improvements over an informed baseline, reducing the error rate by 56%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Marc Light</author>
</authors>
<title>Hiding a semantic hierarchy in a Markov model.</title>
<date>1999</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing,</booktitle>
<volume>67</volume>
<contexts>
<context position="8136" citStr="Abney and Light, 1999" startWordPosition="1333" endWordPosition="1336">iated counts. See Figure 2 for examples. We only consider types observed more than 10 times. Any entity without type information, as well as dictionary entities with only singleton types are treated as unknown tokens (“UNK”). We map UNK to the 50 most common types in the dictionary. Verbs are considered to each have their own type, i.e., token and label for verbs are the same. We do not modify this step. Original Model Hovy et al. (2011) construct a HMM using subject-verb-object (SVO) parse triples as observations, and the type candidates as hidden variables. Similar models have been used in (Abney and Light, 1999; Pardo et al., 2006). We estimate the free model parameters with EM (Dempster et al., 1977), run for a fixed number of iterations (30) or until convergence. Note that Forward-backward EM has time complexity of O(N2T), where N is the number of states, and T the number of time steps. T = 3 in the model formulations used here, but N is much larger than typically found in NLP tasks (see also Table 3). The only way to make this tractable is to restrict the free parameters the model needs to estimate to the transitions. The model is initialized by jointly normalizing 1 the dictionary counts to obta</context>
</contexts>
<marker>Abney, Light, 1999</marker>
<rawString>Steven Abney and Marc Light. 1999. Hiding a semantic hierarchy in a Markov model. In Proceedings of the ACL Workshop on Unsupervised Learning in Natural Language Processing, volume 67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>Proceedings ofACL-08: HLT,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="4164" citStr="Etzioni, 2008" startWordPosition="655" endWordPosition="656">d Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 482–487, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Our contributions in this paper are: • we empirically evaluate an approach to learning types from unlabeled data • we investigate several domains and models • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction t</context>
</contexts>
<marker>Etzioni, 2008</marker>
<rawString>Michele Banko and Oren. Etzioni. 2008. The tradeoffs between open and traditional relation extraction. Proceedings ofACL-08: HLT, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<pages>38</pages>
<contexts>
<context position="8228" citStr="Dempster et al., 1977" startWordPosition="1349" endWordPosition="1352">. Any entity without type information, as well as dictionary entities with only singleton types are treated as unknown tokens (“UNK”). We map UNK to the 50 most common types in the dictionary. Verbs are considered to each have their own type, i.e., token and label for verbs are the same. We do not modify this step. Original Model Hovy et al. (2011) construct a HMM using subject-verb-object (SVO) parse triples as observations, and the type candidates as hidden variables. Similar models have been used in (Abney and Light, 1999; Pardo et al., 2006). We estimate the free model parameters with EM (Dempster et al., 1977), run for a fixed number of iterations (30) or until convergence. Note that Forward-backward EM has time complexity of O(N2T), where N is the number of states, and T the number of time steps. T = 3 in the model formulations used here, but N is much larger than typically found in NLP tasks (see also Table 3). The only way to make this tractable is to restrict the free parameters the model needs to estimate to the transitions. The model is initialized by jointly normalizing 1 the dictionary counts to obtain the emission parameters, which are then fixed (except for the unknown entities (P(word = </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Eichler</author>
<author>Holmer Hemsen</author>
<author>G¨unter Neumann</author>
</authors>
<title>Unsupervised relation extraction from web documents.</title>
<date>2008</date>
<note>LREC. http://www. lrecconf. org/proceedings/lrec2008.</note>
<contexts>
<context position="4510" citStr="Eichler et al., 2008" startWordPosition="716" endWordPosition="719">odels • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transdu</context>
</contexts>
<marker>Eichler, Hemsen, Neumann, 2008</marker>
<rawString>Kathrin Eichler, Holmer Hemsen, and G¨unter Neumann. 2008. Unsupervised relation extraction from web documents. LREC. http://www. lrecconf. org/proceedings/lrec2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press USA.</publisher>
<contexts>
<context position="1775" citStr="Fellbaum, 1998" startWordPosition="274" endWordPosition="275">use type systems to represent relevant semantic classes. Types allow us to find similarities at a higher level to group lexically different entities together. This helps to filter out candidates that violate certain constraints (e.g., in QA, if the intended answer type is PERSON, we can ignore all candidate answers with a different type), but is also used for feature generation and fact-checking. A central question is: where do the types come from? Typically, they come from a handconstructed set. This has some disadvantages. Domain-general types, such as named entities or WordNet supersenses (Fellbaum, 1998), often fail to capture critical domain-specific information (in the medical domain, we might want ANTIBIOTIC, SEDATIVE, etc., rather than just ARTIFACT). Domain-specific types perform much better (Ferrucci et al., 2010), but must be manually adapted to each new domain, which is expensive. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John Prager</author>
</authors>
<title>Building Watson: An overview of the DeepQA project.</title>
<date>2010</date>
<journal>AI magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1995" citStr="Ferrucci et al., 2010" startWordPosition="305" endWordPosition="308">in constraints (e.g., in QA, if the intended answer type is PERSON, we can ignore all candidate answers with a different type), but is also used for feature generation and fact-checking. A central question is: where do the types come from? Typically, they come from a handconstructed set. This has some disadvantages. Domain-general types, such as named entities or WordNet supersenses (Fellbaum, 1998), often fail to capture critical domain-specific information (in the medical domain, we might want ANTIBIOTIC, SEDATIVE, etc., rather than just ARTIFACT). Domain-specific types perform much better (Ferrucci et al., 2010), but must be manually adapted to each new domain, which is expensive. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like to learn domain-specific types directly from data. To this end, patternbased approaches have long been used to induce type systems (Hearst, 1992; Kozareva et al., 2008). Recently, Hovy et al. (2011) proposed an approach </context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building Watson: An overview of the DeepQA project. AI magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguisticsVolume 2,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2519" citStr="Hearst, 1992" startWordPosition="387" endWordPosition="388">r than just ARTIFACT). Domain-specific types perform much better (Ferrucci et al., 2010), but must be manually adapted to each new domain, which is expensive. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like to learn domain-specific types directly from data. To this end, patternbased approaches have long been used to induce type systems (Hearst, 1992; Kozareva et al., 2008). Recently, Hovy et al. (2011) proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability to relation arguments by using them as latent variables in a first-order HMM. However, they only evaluate their method using human sensibility judgements for one domain. While this shows that the types are coherent, it does not tell us much about their applicability. We extend their approach with three important changes: 1. we evaluate the types by measuring accuracy when using them in an extrinsic task, 2. we evaluate </context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguisticsVolume 2, pages 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Chunliang Zhang</author>
<author>Eduard Hovy</author>
<author>Anselmo Pe˜nas</author>
</authors>
<title>Unsupervised discovery of domain-specific knowledge from text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1466--1475</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Hovy, Zhang, Hovy, Pe˜nas, 2011</marker>
<rawString>Dirk Hovy, Chunliang Zhang, Eduard Hovy, and Anselmo Pe˜nas. 2011. Unsupervised discovery of domain-specific knowledge from text. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1466–1475, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Ellen Riloff</author>
<author>Eduard Hovy</author>
</authors>
<title>Semantic class learning from the web with hyponym pattern linkage graphs.</title>
<date>2008</date>
<booktitle>Proceedings of ACL-08: HLT,</booktitle>
<pages>1048--1056</pages>
<contexts>
<context position="2543" citStr="Kozareva et al., 2008" startWordPosition="389" endWordPosition="392">TIFACT). Domain-specific types perform much better (Ferrucci et al., 2010), but must be manually adapted to each new domain, which is expensive. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like to learn domain-specific types directly from data. To this end, patternbased approaches have long been used to induce type systems (Hearst, 1992; Kozareva et al., 2008). Recently, Hovy et al. (2011) proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability to relation arguments by using them as latent variables in a first-order HMM. However, they only evaluate their method using human sensibility judgements for one domain. While this shows that the types are coherent, it does not tell us much about their applicability. We extend their approach with three important changes: 1. we evaluate the types by measuring accuracy when using them in an extrinsic task, 2. we evaluate on more than one domain,</context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008. Semantic class learning from the web with hyponym pattern linkage graphs. Proceedings of ACL-08: HLT, pages 1048–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="6928" citStr="Merialdo, 1994" startWordPosition="1128" endWordPosition="1129">rict the search space and improve learning, we first have to learn which types modify entities and record their cooccurrence, and use this as dictionary. Kleiman: professor:25, expert:13, (specialist:1) Tilton: executive:37, economist:17, (chairman:4, president:2) Figure 2: Examples of dictionary entries with counts. Types in brackets are not considered. Dictionary Construction The number of common nouns in a domain is generally too high to consider all of them for every entity. A common way to restrict the number of types is to provide a dictionary that lists all legal types for each entity (Merialdo, 1994; Ravi and Knight, 2009; T¨ackstr¨om et al., 2013). To construct this dictionary, we collect for each entity (i.e., a sequence of words labeled with NNP or NNPS tags) in our data all common nouns (NN, NNS) that modify it. These are 1. nominal modifiers (“judge Scalosi ...”), 2. appositions (“Tilton, a professor at ...”), and 3. copula constructions (“Finton, who is the investor ...”). These modifications can be collected from the dependency parse trees. For each entity, we store the quarterback throw ball player Montana throw ball y1 y2 x1 x3 y3 483 type candidates and their associated counts.</context>
<context position="9234" citStr="Merialdo, 1994" startWordPosition="1526" endWordPosition="1527">needs to estimate to the transitions. The model is initialized by jointly normalizing 1 the dictionary counts to obtain the emission parameters, which are then fixed (except for the unknown entities (P(word = UNK|type = ·)). Transition parameters are initialized uniformly (restricted to potentially observable type sequences), and kept as free parameters for the model to optimize. Common nouns can be both hidden variables and observations in the model, so they act like annotated items: their legal types are restricted to the identity. All entities are thus constrained by the dictionary, as in (Merialdo, 1994). To further constrain the model, only the top three types of each entity are considered. Since the type distribution typically follows a Zipf curve, this still captures most of the information. 1This preserves the observed entity-specific distributions. Under conditional normalization, the type candidates from frequent entities tend to dominate those of infrequent entities. I.e., the model favors an unlikely candidate for entity a if it is frequent for entity b. The model can be fully specified as 3 P(x, y) = P(y1)·P(x1|y1) ri P(yi|yi−1)·P(xi|yi) i=2 (1) where x is an input triple of a verb a</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thiago Pardo</author>
<author>Daniel Marcu</author>
<author>Maria Nunes</author>
</authors>
<date>2006</date>
<booktitle>Unsupervised Learning of Verb Argument Structures. Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>59--70</pages>
<contexts>
<context position="4265" citStr="Pardo et al., 2006" startWordPosition="670" endWordPosition="673">Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Our contributions in this paper are: • we empirically evaluate an approach to learning types from unlabeled data • we investigate several domains and models • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the sim</context>
<context position="8157" citStr="Pardo et al., 2006" startWordPosition="1337" endWordPosition="1340">e 2 for examples. We only consider types observed more than 10 times. Any entity without type information, as well as dictionary entities with only singleton types are treated as unknown tokens (“UNK”). We map UNK to the 50 most common types in the dictionary. Verbs are considered to each have their own type, i.e., token and label for verbs are the same. We do not modify this step. Original Model Hovy et al. (2011) construct a HMM using subject-verb-object (SVO) parse triples as observations, and the type candidates as hidden variables. Similar models have been used in (Abney and Light, 1999; Pardo et al., 2006). We estimate the free model parameters with EM (Dempster et al., 1977), run for a fixed number of iterations (30) or until convergence. Note that Forward-backward EM has time complexity of O(N2T), where N is the number of states, and T the number of time steps. T = 3 in the model formulations used here, but N is much larger than typically found in NLP tasks (see also Table 3). The only way to make this tractable is to restrict the free parameters the model needs to estimate to the transitions. The model is initialized by jointly normalizing 1 the dictionary counts to obtain the emission param</context>
<context position="13786" citStr="Pardo et al., 2006" startWordPosition="2322" endWordPosition="2325">011), we derive our data from the New York Times (NYT) corpus (Sandhaus, 2008). It contains several years worth of articles, manually annotated with metadata such as author, content, etc. Similar to Yao et al. (2012), we use articles whose content metadata field contains certain labels to distinguish data from different domains. We use the labels Football2, Law and Legislation, and Finances. We remove meta-data and lists, tokenize, parse, and lemmatize all articles. We then automatically extract subject-verb-object (SVO) triples from the parses, provided the verb is a full verb. Similarly to (Pardo et al., 2006), we focus on the top 100 full verbs for efficiency reasons, though nothing in our approach prevents us from extending it to all verbs. For each domain, we select all instances which have a potential type (common noun) in at least one argument position. These serve as corpus. Football Finances Law unique types 7,139 18,186 10,618 unique entities 38,282 27,528 12,782 Table 3: Statistics for the three domains. As test data, we randomly select a subset of 1000 instances for each argument, provided they contain one of the 50 most frequent types in subject or object position, such as player in “pla</context>
</contexts>
<marker>Pardo, Marcu, Nunes, 2006</marker>
<rawString>Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006. Unsupervised Learning of Verb Argument Structures. Computational Linguistics and Intelligent Text Processing, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Minimized Models for Unsupervised Part-of-Speech Tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>504--512</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6951" citStr="Ravi and Knight, 2009" startWordPosition="1130" endWordPosition="1133">space and improve learning, we first have to learn which types modify entities and record their cooccurrence, and use this as dictionary. Kleiman: professor:25, expert:13, (specialist:1) Tilton: executive:37, economist:17, (chairman:4, president:2) Figure 2: Examples of dictionary entries with counts. Types in brackets are not considered. Dictionary Construction The number of common nouns in a domain is generally too high to consider all of them for every entity. A common way to restrict the number of types is to provide a dictionary that lists all legal types for each entity (Merialdo, 1994; Ravi and Knight, 2009; T¨ackstr¨om et al., 2013). To construct this dictionary, we collect for each entity (i.e., a sequence of words labeled with NNP or NNPS tags) in our data all common nouns (NN, NNS) that modify it. These are 1. nominal modifiers (“judge Scalosi ...”), 2. appositions (“Tilton, a professor at ...”), and 3. copula constructions (“Finton, who is the investor ...”). These modifications can be collected from the dependency parse trees. For each entity, we store the quarterback throw ball player Montana throw ball y1 y2 x1 x3 y3 483 type candidates and their associated counts. See Figure 2 for examp</context>
</contexts>
<marker>Ravi, Knight, 2009</marker>
<rawString>Sujith Ravi and Kevin Knight. 2009. Minimized Models for Unsupervised Part-of-Speech Tagging. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 504–512. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>424--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2126" citStr="Ritter et al., 2010" startWordPosition="324" endWordPosition="327">s also used for feature generation and fact-checking. A central question is: where do the types come from? Typically, they come from a handconstructed set. This has some disadvantages. Domain-general types, such as named entities or WordNet supersenses (Fellbaum, 1998), often fail to capture critical domain-specific information (in the medical domain, we might want ANTIBIOTIC, SEDATIVE, etc., rather than just ARTIFACT). Domain-specific types perform much better (Ferrucci et al., 2010), but must be manually adapted to each new domain, which is expensive. Alternatively, unsupervised approaches (Ritter et al., 2010) can be used to learn clusters of similar words, but the resulting types (=cluster numbers) are not human-interpretable, which makes analysis difficult. Furthermore, it requires us to define the number of clusters beforehand. Ideally, we would like to learn domain-specific types directly from data. To this end, patternbased approaches have long been used to induce type systems (Hearst, 1992; Kozareva et al., 2008). Recently, Hovy et al. (2011) proposed an approach that uses co-occurrence patterns to find entity type candidates, and then learns their applicability to relation arguments by using</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424–434, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>The New York Times Annotated Corpus.</title>
<date>2008</date>
<booktitle>Number LDC2008T19. Linguistic Data Consortium,</booktitle>
<editor>Evan Sandhaus, editor.</editor>
<location>Philadelphia.</location>
<contexts>
<context position="4791" citStr="(2008)" startWordPosition="768" endWordPosition="768"> to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transductive and can be applied to unseen data. Our approach follows Hovy et al. (2011). However, they only evaluate one model on football by collecting sensibility ratings from Mechanical Turk. Our method provides extrinsic measures of performance on several domains. 3 Model Our goal is</context>
</contexts>
<marker>2008</marker>
<rawString>Evan Sandhaus, editor. 2008. The New York Times Annotated Corpus. Number LDC2008T19. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zareen Syed</author>
<author>Evelyne Viegas</author>
</authors>
<title>A hybrid approach to unsupervised relation discovery based on linguistic analysis and semantic typing.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading,</booktitle>
<pages>105--113</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4534" citStr="Syed and Viegas, 2010" startWordPosition="720" endWordPosition="723">tity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transductive and can be applied</context>
</contexts>
<marker>Syed, Viegas, 2010</marker>
<rawString>Zareen Syed and Evelyne Viegas. 2010. A hybrid approach to unsupervised relation discovery based on linguistic analysis and semantic typing. In Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 105–113. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<journal>Transactions of the ACL.</journal>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. Transactions of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha P Talukdar</author>
<author>Fernando Pereira</author>
</authors>
<title>Experiments in graph-based semi-supervised learning methods for class-instance acquisition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1473--1481</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4823" citStr="Talukdar and Pereira (2010)" startWordPosition="770" endWordPosition="773">y the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The models we use are transductive and can be applied to unseen data. Our approach follows Hovy et al. (2011). However, they only evaluate one model on football by collecting sensibility ratings from Mechanical Turk. Our method provides extrinsic measures of performance on several domains. 3 Model Our goal is to find semantic type candidate</context>
</contexts>
<marker>Talukdar, Pereira, 2010</marker>
<rawString>Partha P. Talukdar and Fernando Pereira. 2010. Experiments in graph-based semi-supervised learning methods for class-instance acquisition. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473–1481. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha P Talukdar</author>
<author>Joseph Reisinger</author>
<author>Marcus Pas¸ca</author>
<author>Deepak Ravichandran</author>
<author>Rahul Bhagat</author>
<author>Fernando Pereira</author>
</authors>
<title>Weakly-supervised acquisition of labeled class instances using graph random walks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Talukdar, Reisinger, Pas¸ca, Ravichandran, Bhagat, Pereira, 2008</marker>
<rawString>Partha P. Talukdar, Joseph Reisinger, Marcus Pas¸ca, Deepak Ravichandran, Rahul Bhagat, and Fernando Pereira. 2008. Weakly-supervised acquisition of labeled class instances using graph random walks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 582– 590. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yulan Yan</author>
<author>Naoaki Okazaki</author>
<author>Yutaka Matsuo</author>
<author>Zhenglu Yang</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Unsupervised relation extraction by mining wikipedia texts using information from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP:</booktitle>
<volume>2</volume>
<pages>1021--1029</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4488" citStr="Yan et al., 2009" startWordPosition="712" endWordPosition="715">eral domains and models • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learning. While this provides a way to discover types, it requires a large graph that does not easily generalize to new instances (transductive), since it produces no predictive model. The mod</context>
</contexts>
<marker>Yan, Okazaki, Matsuo, Yang, Ishizuka, 2009</marker>
<rawString>Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu Yang, and Mitsuru Ishizuka. 2009. Unsupervised relation extraction by mining wikipedia texts using information from the web. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1021–1029. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Aria Haghighi</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Structured relation discovery using generative models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1456--1466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4283" citStr="Yao et al., 2011" startWordPosition="674" endWordPosition="677"> USA, June 23-25 2014. c�2014 Association for Computational Linguistics Our contributions in this paper are: • we empirically evaluate an approach to learning types from unlabeled data • we investigate several domains and models • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of cl</context>
</contexts>
<marker>Yao, Haghighi, Riedel, McCallum, 2011</marker>
<rawString>Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew McCallum. 2011. Structured relation discovery using generative models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1456–1466. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Unsupervised relation discovery with sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>712--720</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4302" citStr="Yao et al., 2012" startWordPosition="678" endWordPosition="681">014. c�2014 Association for Computational Linguistics Our contributions in this paper are: • we empirically evaluate an approach to learning types from unlabeled data • we investigate several domains and models • the learned entity types can be used to predict selectional restrictions with high accuracy 2 Related Work In relation extraction, we have to identify the relation elements, and then map the arguments to types. We follow an open IE approach (Banko and Etzioni, 2008) and use dependencies to identify the elements. In contrast to most previous work (Pardo et al., 2006; Yao et al., 2011; Yao et al., 2012), we have no pre-defined set of types, but try to learn it along with the relations. Some approaches use types from general data bases such as Wikipedia, Freebase, etc. (Yan et al., 2009; Eichler et al., 2008; Syed and Viegas, 2010), sidestepping the question how to construct those DBs in the first place. We are less concerned with extraction performance, but focus on the accuracy of the learned type system by measuring how well it performs in a prediction task. Talukdar et al. (2008) and Talukdar and Pereira (2010) present graph-based approaches to the similar problem of class-instance learni</context>
<context position="13148" citStr="Yao et al. (2012)" startWordPosition="2219" endWordPosition="2222">eighted accuracies over all relations. While this gives an intuition of the general methodology, it is harder to put in context. Here, we want to evaluate the model’s performance in a downstream task. We measure its ability to predict the correct types for verbal arguments. We evaluate on three different domains. As test case, we use a cloze test, or fill-in-theblank. We select instances that contain a typecandidate word in subject or object position and replace that word with the unknown token. We can then compare the model’s prediction to the original word to measure accuracy. 5.1 Data Like Yao et al. (2012) and Hovy et al. (2011), we derive our data from the New York Times (NYT) corpus (Sandhaus, 2008). It contains several years worth of articles, manually annotated with metadata such as author, content, etc. Similar to Yao et al. (2012), we use articles whose content metadata field contains certain labels to distinguish data from different domains. We use the labels Football2, Law and Legislation, and Finances. We remove meta-data and lists, tokenize, parse, and lemmatize all articles. We then automatically extract subject-verb-object (SVO) triples from the parses, provided the verb is a full v</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2012</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2012. Unsupervised relation discovery with sense disambiguation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 712–720. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>