<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.882558">
Using LDA to detect semantically incoherent documents
</title>
<author confidence="0.714069">
Hemant Misra and Olivier Capp´e Franc¸ois Yvon
</author>
<affiliation confidence="0.400075">
LTCI/CNRS and TELECOM ParisTech Univ Paris-Sud 11 and LMISI-CNRS
</affiliation>
<email confidence="0.982392">
{misra,cappe}@enst.fr yvon@limsi.fr
</email>
<sectionHeader confidence="0.993447" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999820222222222">
Detecting the semantic coherence of a doc-
ument is a challenging task and has sev-
eral applications such as in text segmenta-
tion and categorization. This paper is an
attempt to distinguish between a ‘semanti-
cally coherent’ true document and a ‘ran-
domly generated’ false document through
topic detection in the framework of latent
Dirichlet analysis. Based on the premise
that a true document contains only a few
topics and a false document is made up of
many topics, it is asserted that the entropy
of the topic distribution will be lower for
a true document than that for a false docu-
ment. This hypothesis is tested on several
false document sets generated by various
methods and is found to be useful for fake
content detection applications.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954153846154">
The “Internet revolution” has dramatically in-
creased the monetary value of higher ranking on
the web search engines index, fostering the ex-
pansion of techniques, collectively known as “Web
Spam”, that fraudulently help to do so. Internet is
indeed “polluted” with fake Web sites whose only
purpose is to deceive the search engines by arti-
ficially pushing up the popularity of commercial
sites, or sites promoting illegal content 1. These
fake sites are often forged using very crude content
generation techniques, ranging from web scrap-
ping (blending of chunks of actual contents) to
simple-minded text generation techniques based
</bodyText>
<footnote confidence="0.948157571428571">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1The annual AirWeb challenge http://airweb.
cse.lehigh.edu gives a state-of-the art on current Web
Spam detection techniques.
</footnote>
<bodyText confidence="0.999730794871795">
on random sampling of words (“word salads”),
or randomly replacing words in actual documents
(“word stuffing”) 2. Among these, the latter two
are easy to detect using simple statistical models
of natural texts, but the former is more challeng-
ing, it being made up of actual sentences: recog-
nizing these texts as forged requires either to resort
to plagiarism detection techniques, or to automati-
cally identify their lack of semantic consistency.
Detecting the consistency of texts or of text
chunks has many applications in Natural Language
Processing. So far, it has been used mainly in the
context of automatic text segmentation, where a
change in vocabulary is often the mark of topic
change (Hearst, 1997), and, to a lesser extent, in
discourse studies (see, e.g., (Foltz et al., 1998)).
It could also serve to devise automatic metrics for
text summarization or machine translation tasks.
This paper is an attempt to address the issue
of differentiating between ‘true’ and ‘false’ doc-
uments on the basis of their consistency through
topic modeling approach. We have used La-
tent Dirichlet allocation (LDA) (Blei et al., 2002)
model as our main topic modeling tool. One of the
aims of LDA and similar methods, including prob-
abilistic latent semantic analysis (PLSA) (Hof-
mann, 2001), is to produce low dimensionality rep-
resentations of texts in a “semantic space” such
that most of their inherent statistical characteristics
are preserved. A reduction in dimensionality facil-
itates storage as well as faster retrieval. Modeling
discrete data has many applications in classifica-
tion, categorization, topic detection, data mining,
information retrieval (IR), summarization and col-
laborative filtering (Buntine and Jakulin, 2004).
The aim of this paper is to test LDA for es-
tablishing the semantic coherence of a document
based on the premise that a real (coherent) docu-
ment should discuss only a few number of topics,
</bodyText>
<footnote confidence="0.9847265">
2The same techniques are commonly used in mail spams
also.
</footnote>
<page confidence="0.981043">
41
</page>
<note confidence="0.881262">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 41–48
Manchester, August 2008
</note>
<bodyText confidence="0.99985004">
a property hardly granted for forged documents
which are often made up of random assemblage
of words or sentences. As a consequence, the co-
herence of a document may reflect in the entropy
of its posterior topic distribution or in its perplex-
ity for the model. The entropy of the estimated
topic distribution of a true document is expected to
be lower than that of a fake document. Moreover,
the length normalized log-likelihood of a true and
coherent document may be higher as compared to
that of a false and incoherent document.
In this paper, we compare two methods to esti-
mate the posterior topic distribution of test docu-
ments, and this study is also an attempt to inves-
tigate the role of different parameters on the effi-
ciency of these methods.
This paper is organized as follows: In Section 2,
the basics of the LDA model are set. We then dis-
cuss and contrast several approaches to the prob-
lem of inferring the topic distribution of a new
document in Section 3. In Section 4, we describe
the corpus and experimental set-up that are used
to produce the results presented in Section 5. We
summarize our main findings and draw perspec-
tives for future research in Section 6.
</bodyText>
<sectionHeader confidence="0.919697" genericHeader="method">
2 Latent Dirichlet Allocation
</sectionHeader>
<subsectionHeader confidence="0.973201">
2.1 Basics
</subsectionHeader>
<bodyText confidence="0.999990465116279">
LDA is a probabilistic model of text data which
provides a generative analog of PLSA (Blei et al.,
2002), and is primarily meant to reveal hidden top-
ics in text documents. In (Griffiths and Steyvers,
2004), the authors used LDA for identifying “hot
topics” by analyzing the temporal dynamics of top-
ics over a period of time. More recently LDA has
also been used for unsupervised language model
(LM) adaptation in the context of automatic speech
recognition (ASR) (Hsu and Glass, 2006; Tam
and Schultz, 2007; Heidel et al., 2007). Several
extensions of the LDA model, such as hierarchi-
cal LDA (Blei et al., 2004), HMM-LDA (Grif-
fiths et al., 2005), correlated topic models (Blei
and Lafferty, 2005) and hidden topic Markov mod-
els (Gruber et al., 2007), have been proposed, that
introduce more complex dependency patterns in
the model.
Like most of the text mining techniques, LDA
assumes that documents are made up of words and
the ordering of the words within a document is
unimportant (“bag-of-words” assumption). Con-
trary to the simpler Multinomial Mixture Model
(see, e.g., (Nigam et al., 2000) and Section 2.4),
LDA assumes that every document is represented
by a topic distribution and that each topic defines
an underlying distribution on words.
The generative history of a document (a bag-
of-words) collection is the following: Assuming
a fixed and known number of topics nT, for each
topic t, a distribution βt over the indexing vocab-
ulary (w = 1... nW) is drawn from a Dirichlet
distribution. Then, for each document d, a distri-
bution θd over the topics (t = 1... nT) is drawn
from a Dirichlet distribution. For a document d,
the document length ld being an exogenous vari-
able, the next step consists of drawing a topic ti
from θd for each position i = 1...ld. Finally, a
word is selected from the chosen topic ti. Given
the topic distribution, each word is thus drawn in-
dependently from every other word using a docu-
ment specific mixture model. The probability of
ith word token is thus:
</bodyText>
<equation confidence="0.99948475">
P(wi|θd, β) = XnT P(ti=t|θd)P(wi|ti, β) (1)
t=1
XnT θdtβtw (2)
t=1
</equation>
<bodyText confidence="0.99593525">
Conditioned on β and θd, the likelihood of doc-
ument d is a mere product of terms such as (2),
which can be rewritten as:
where Cdw is the count of word w in d.
</bodyText>
<subsectionHeader confidence="0.997647">
2.2 LDA: Training
</subsectionHeader>
<bodyText confidence="0.999947333333333">
LDA training consists of estimating the following
two parameter vectors from a text collection: the
topic distribution in each document d (θdt, t =
1...nT, d = 1...nD) and the word distribution in
each topic (βtw, t = 1...nT, w = 1...nW). Both
θd and βt define discrete distributions, respectively
over the set of topics and over the set of words.
Various methods have been proposed to estimate
LDA parameters, such as variational method (Blei
et al., 2002), expectation propagation (Minka and
Lafferty, 2002) and Gibbs sampling (Griffiths and
Steyvers, 2004). In this paper, we have used
the latter approach, which boils down to repeat-
edly going through the training data and sampling
the topic assigned to each word token conditioned
</bodyText>
<equation confidence="0.999552444444444">
P(Cd|θd,β) =
&amp;quot; nW nT Cdw
θ
β
(
dt
tw) (3)
w=1 t=1
X
</equation>
<page confidence="0.985522">
42
</page>
<bodyText confidence="0.999659125">
on the topic assigned to all the other word to-
kens. Given a particular Gibbs sample, the pos-
teriors for θ and β are 3: Dirichlet with parameters
(Kt1+λ, ... , KtnW +λ) and Dirichlet with param-
eters (Jd1 + α, ... , JdnT + α), respectively, where
Ktw is the number of times word w is assigned to
topic t and Jdt is the number of times topic t is as-
signed to some word token in document d. Hence,
</bodyText>
<equation confidence="0.9996584">
Ktw + λ
βtw= PnW (4)
k=1 Ktk + nWλ
Jdt + α θdt = PnT (5)
k=1 Jdk + nT α
</equation>
<bodyText confidence="0.9998968">
During the Gibbs sampling phase, βt and θd are
sampled from the above posteriors while the final
estimates for these parameters are obtained by av-
eraging the posterior means over the complete set
of Gibbs iteration.
</bodyText>
<subsectionHeader confidence="0.997024">
2.3 LDA: Testing
</subsectionHeader>
<bodyText confidence="0.999994086956522">
Training LDA model on a text collection already
provides interesting insights regarding the the-
matic structure of the collection. This has been the
primary application of LDA in (Blei et al., 2002;
Griffiths and Steyvers, 2004). Even better, being
a generative model, LDA can be used to make
prediction regarding novel documents (assuming
they use the same vocabulary as the training cor-
pus). In a typical IR setting, where the main fo-
cus is on computing the similarity between a doc-
ument d and a query d&apos;, a natural similarity mea-
sure is given by P(Cd&apos;|θd, β), computed according
to (3) (Buntine et al., 2004).
An alternative would be to compute the KL di-
vergence between the topic distribution in d and d&apos;,
which however requires to infer the latter quantity.
As the topic distribution of a (new) document gives
its representation along the latent semantic dimen-
sions, computing this value is helpful for many
applications, including text segmentation and text
classification. Methods for efficiently and accu-
rately estimating topic distribution for text docu-
ments are presented and evaluated in Section 3.
</bodyText>
<subsectionHeader confidence="0.99577">
2.4 Baseline: Multinomial Mixture Model
</subsectionHeader>
<bodyText confidence="0.964483076923077">
The performance of LDA model is compared
with that of the simpler multinomial mixture
model (Nigam et al., 2000; Rigouste et al., 2007).
3assuming non-informative priors with hyper-parameters
α and A for the Dirichlet distribution over topics and the
Dirichlet distribution over words respectively
In this model, every word in a document belongs
to the same topic, as if the document specific topic
distribution θd in LDA were bound to lie on one
vertex of the [0,1]nT simplex. Using the same no-
tations as before (except for θt, which now denotes
the position independent probability of topic t in
the collection), the probability of a document is:
</bodyText>
<equation confidence="0.9359085">
βCdw (6)
tw
</equation>
<bodyText confidence="0.99935625">
This model can be trained through expectation
maximization (EM), using the following reestima-
tion formulas, where (7) defines the E-step; (8) and
(9) define the M-step.
</bodyText>
<equation confidence="0.998768">
θt QnW w=1(β&apos;P(t|Cd, θ, β) = PnT1 θt QnW 1(βtwd)Cdw (7)
θ&apos; t  α + XnD P(t|Cd, θ, β) (8)
d=1
β&apos;tw  λ + XnD CdwP(t|Cd, θ, β) (9)
d=1
</equation>
<bodyText confidence="0.999880666666667">
As suggested in (Rigouste et al., 2007), we ini-
tialize the EM algorithm by drawing initial topic
distributions from a prior Dirichlet distribution
with hyper-parameter α = 1. β = 0.1 in all the
experiments.
During testing, the parameters of the multino-
mial models are used to estimate the posterior topic
distribution in each document using (7). The like-
lihood of a test document is given by (6).
</bodyText>
<sectionHeader confidence="0.9863065" genericHeader="method">
3 Inferring the Topic Distribution of Test
Documents
</sectionHeader>
<bodyText confidence="0.999975571428571">
P(Cd|θd), the conditional probability of a docu-
ment d given θd is obtained using (3) 4. Computing
the likelihood of a test document requires to inte-
grate this quantity over θ; likewise for the compu-
tation of the posterior distribution of θ. This inte-
gral has no close form solution, but can be approx-
imated using Monte-Carlo sampling techniques as:
</bodyText>
<equation confidence="0.991077666666667">
1 X
P(Cd)  M
m=1
</equation>
<bodyText confidence="0.9997065">
where θ(m) denotes the mth sample from the
Dirichlet prior, and M is the number of Monte
</bodyText>
<footnote confidence="0.5630035">
4The dependence on Q is dropped for simplicity. Q is
learned during training and kept fixed during testing.
</footnote>
<equation confidence="0.99947225">
P(Cd|θt,β) =
θt
nWY
w=1
XnT
t=1
M
P(Cd|θ(m)) (10)
</equation>
<page confidence="0.994565">
43
</page>
<bodyText confidence="0.9961975">
Carlo samples. Given the typical length of doc-
uments and the large vocabulary size, small scale
experiments convinced us that a cruder approxima-
tion was in order, as the sum in (10) is dominated
by the maximum value. We thus contend ourselves
to solve:
</bodyText>
<equation confidence="0.88248">
θ∗ = argmax P(Cd|θ) (11)
θ,Pt θt=1
</equation>
<bodyText confidence="0.999733272727273">
and use this value to approximate P(Cd) using (3).
The maximization program (11) has no close
form solution. However, the objective function is
differentiable and log-concave, and can be opti-
mized in a number of ways. We considered two
different algorithms: an EM-like approach, ini-
tially introduced in (Heidel et al., 2007), and an ex-
ponentiated gradient approach (Kivinen and War-
muth, 1997; Globerson et al., 2007).
The first approach implements an iterative pro-
cedure based on the following update rule:
</bodyText>
<equation confidence="0.996605">
θdt +— ld w=1
1 XnW PnTCdwθdtβtw (12)
t0=1 θdt0βt0w
</equation>
<bodyText confidence="0.9971816">
Although no justification was given in (Hei-
del et al., 2007), it can be shown that this
update rule converges towards a global opti-
mum of the likelihood. Let θ and θ0 be two
topic distributions in the nT-dimensional simplex,
</bodyText>
<equation confidence="0.9964055">
L(θ) = log P(Cd|θ), and ρt(w, θ) =P θtβtw
t0 θt0βt0w .
</equation>
<bodyText confidence="0.96287625">
We define an auxiliary function Q(θ, θ0) =
Pw Cw(Pt ρt(w, θ) log(θ0t)). Q(θ, θ0) is concave
in θ0, and performs the role played by the auxil-
iary function in the EM algorithm. Simple cal-
culus suffices to prove that (i) the update (12)
maximizes in θ0 the function Q(θ, θ0), and (ii)
Q(θ, θ0) — Q(θ, θ) &gt; L(θ0) — L(θ), which stems
from the concavity of the log. At an optimum of
Q(θ, θ0) the positivity of the first term implies the
positivity of the second. Maximizing Q using the
update rule (12) thus increases the likelihood and
repeating this update converges towards the opti-
mum value. We experimented both with an un-
smoothed (12) and with a smoothed version of this
update rule. The unsmoothed version yielded a
slightly better result than the smoothed one.
Exponentiated gradient (Kivinen and Warmuth,
1997; Globerson et al., 2007) yields an alternative
update rule:
!
</bodyText>
<equation confidence="0.992260333333333">
Cdwβtw (13)
PnT
t0=1 θdt0βt0w
</equation>
<bodyText confidence="0.999880833333333">
where η defines the convergence rate. In this form,
the update rule does not preserve the normaliza-
tion of θ, which needs to be performed after every
iteration.
A systematic comparison of these rules was car-
ried out, yielding the following conclusions:
</bodyText>
<listItem confidence="0.8950755">
• the convergence of the EM-like method is
very fast. Typically, it requires less than half
a dozen iterations to converge. After conver-
gence, the topic distribution estimated by this
method for a subset of train documents was
always very close (as measured by the KL-
divergence) to the respective topic distribu-
tion of the same documents observed at the
end of the LDA training. Taking nT = 50,
the average KL divergence for a set of 4,500
</listItem>
<bodyText confidence="0.929610285714286">
documents was found to be less than 0.5.
• exponentiated gradient has a more erratic be-
haviour, and requires a careful tuning of η on
a per document basis. For large values of η,
the update rule (13) sometimes fails to con-
verge; smaller values of η allowed to consis-
tently reach convergence, but required more
iterations (typically 20-30). On a positive
side, on an average, the topic distributions
estimated by this method are better than the
ones obtained with the EM-like algorithm.
Based on these findings, we decided to use the
EM-like algorithm in all our subsequent experi-
ments.
</bodyText>
<sectionHeader confidence="0.996875" genericHeader="method">
4 Experimental protocol
</sectionHeader>
<subsectionHeader confidence="0.991757">
4.1 Training and test corpora
</subsectionHeader>
<bodyText confidence="0.9997605625">
The Reuters Corpus Volume 1 (RCV1) (Lewis et
al., 2004) is a collection of over 800,000 news
items in English from August 1996 to August
1997. Out of the entire RCV1 dataset, we se-
lected 27,672 documents (news items) for training
(TrainReuters) and 23,326 documents for testing
(TestReuters). The first 4000 documents from the
TestReuters dataset were used as true documents
(TrueReuters) in the experiments reported in this
paper. The vocabulary size in the train set, after
removing the function words, is 93, 214.
Along with these datasets of “true” documents,
three datasets of fake documents were also cre-
ated. Document generation techniques are many:
here we consider documents made by mixing short
passages from various texts and documents made
</bodyText>
<equation confidence="0.3564215">
θdt +— θdt exp η XnW
w=1
</equation>
<page confidence="0.988671">
44
</page>
<bodyText confidence="0.9999592">
by assembling randomly chosen words (sometimes
called as “word salads”). In addition, we also
consider the case of documents generated with a
stochastic language model (LM). Our “fake” test
documents are thus composed of:
</bodyText>
<listItem confidence="0.999494">
• (SentenceSalad) obtained by randomly pick-
ing sentences from TestReuters.
• (WordSalad) created by generating random
sentences from a conventional unigram LM
trained on TrainReuters.
• (Markovian) created by generating random
sentences from a conventional 3-gram LM
trained on TrainReuters.
</listItem>
<bodyText confidence="0.9966073">
Each of these forged document set contains 4,000
documents.
To assess the performance on out-of-domain
data, we replicated the same tests using 2,000
Medline abstracts (Ohta et al., 2002). 1,500 doc-
uments were used either to generate fake docu-
ments by picking sentences randomly or to train an
LM and then using the LM to generate fake docu-
ments. The remaining 500 abstracts were set aside
as “true” documents (TrueMedline).
</bodyText>
<subsectionHeader confidence="0.964616">
4.2 Performance Measurements: EER
</subsectionHeader>
<bodyText confidence="0.999841523809524">
The entropy of the topic distribution is computed
as H = − ET j=1 ˆθdj log ˆθdj. The other measure
of interest is the average ‘log-likelihood per word’
(LLPW) 5.
While evaluating the performance of our sys-
tem, two types of errors are encountered: false ac-
ceptance (FA) when a false document is accepted
as a true document and false rejection (FR) when a
true document is rejected as a false document. The
rate of FA and FR is dependent on the threshold
used for taking the decision, and usually the per-
formance of a system is shown by its receiver op-
erating characteristic (ROC) curve which is a plot
between FA and FR rates for different values of
threshold. Instead of reporting the performance of
a system based on two error rates (FA and FR),
the general practice is to report the performance in
terms of equal-error-rate (EER). The EER is the
error rate at the threshold where FA rate = FR rate.
In our system, a threshold on entropy (or
LLPW) is used for taking the decision, and all the
</bodyText>
<footnote confidence="0.993624333333333">
5This measure is directly related to the text per-
plexity in the model, according to perplexity =
2−average log-likelihood per word
</footnote>
<bodyText confidence="0.9998474">
documents having their entropy (or LLPW) below
(or above) the threshold are accepted as true doc-
uments. The EER is obtained on the test set by
changing the threshold on the test set itself, and
the best results thus obtained are reported.
</bodyText>
<sectionHeader confidence="0.977918" genericHeader="method">
5 Detecting semantic inconsistency
</sectionHeader>
<subsectionHeader confidence="0.9982025">
5.1 Detecting fake documents with LDA and
Multinomial mixtures
</subsectionHeader>
<bodyText confidence="0.997113428571429">
In the first set of experiments, the LLPW and en-
tropy of the topic distribution (the two measures)
of the Multinomial mixture and LDA models were
compared to check the ability of these two mea-
sures and models in discriminating between true
and false documents. These results are summa-
rized in Table 1.
</bodyText>
<table confidence="0.9993591">
TrueReuters vs. Multinomial
LLPW Entropy
SentenceSalad 15.3% 48.8%
WordSalad 9.3% 35.8%
Markovian 17.6% 38.9%
TrueReuters vs. LDA
LLPW Entropy
SentenceSalad 18.9% 0.88%
WordSalad 9.9% 0.13%
Markovian 25.0% 0.28%
</table>
<tableCaption confidence="0.982747">
Table 1: Performance of the Multinomial Mixture
and LDA
</tableCaption>
<bodyText confidence="0.998334380952381">
For the multinomial mixture model, the LLPW
measure is able to discriminate between true and
false documents to a certain extent. As expected
(not shown here), the LLPW of the true documents
is usually higher than that of the false documents.
In contrast, the entropy of the posterior topic dis-
tribution does not help much in discriminating be-
tween true and false documents. In fact it remains
close to zero (meaning that only one topic is “ac-
tive”) both for true and false documents.
The behaviour of the LDA scores is entirely dif-
ferent. The perplexity scores (LLPW) of true and
fake texts are comparable, and do not make useful
predictors. In contrast, the entropy of the topic dis-
tribution allows to sort true documents from fake
ones with a very high accuracy for all kinds of fake
texts considered in this paper. Both results stem
from the ability of LDA to assign a different topic
to each word occurrence.
Similar pattern is observed for our three false
test sets (against the TrueReuters set) with small
</bodyText>
<page confidence="0.99845">
45
</page>
<bodyText confidence="0.999718833333333">
variations The texts generated with a Markov
model, no matter the order, have the highest en-
tropy, reflecting the absence of long range corre-
lation in the generation model. Though the texts
generated by mixing sentences are more confus-
ing with the true documents, the performance is
still less than 1% EER. Texts mixing a high num-
ber of topics (e.g., Sentence Salads) are almost as
likely as natural texts that address only a few top-
ics. However, the former has much higher entropy
of the topic distribution due to a large number of
topics being active in such texts (see also Figure 1).
</bodyText>
<figure confidence="0.537899">
Entropy
</figure>
<figureCaption confidence="0.965726">
Figure 1: Histogram of entropy of θ for different
true and false document sets.
</figureCaption>
<bodyText confidence="0.99987075">
It is noteworthy that both the predictors (LLPW
and Entropy) give complementary clues regarding
a text category. A linear combination of these two
scores (the weight to the LLPW score is 0.1) al-
lows to substantially improve over these baseline
results, yielding a relative improvement (in EER)
of +20.0% for the sentence salads, +20.8% for the
word salads, and +27.3% for the Markov Models.
</bodyText>
<subsectionHeader confidence="0.999668">
5.2 Effect of the number of topics
</subsectionHeader>
<bodyText confidence="0.9999662">
In this part, we investigate the performance of
LDA in detecting false documents when the num-
ber of topics is changed. Increasing the number
of topics means higher memory requirements both
during training and testing. Though the results are
shown only for SentenceSalad, similar trend is ob-
served for WordSalad and Markovian.
The numbers in Table 2 show that the perfor-
mance obtained with the LLPW score consistently
improve with an increase in the number of top-
ics, though the % improvement obtained when the
number of topics exceeds 200 is marginal. In con-
trast, the best performance in case of entropy is
achieved at 50 topics and slowly degrades when a
more complex model is used.
</bodyText>
<table confidence="0.998778714285714">
Number of Topics LLPW Entropy
10 27.9 1.88
50 18.9 0.88
100 16.0 0.93
200 14.8 0.90
300 13.8 1.05
400 13.6 1.10
</table>
<tableCaption confidence="0.984458">
Table 2: EER from LLPW and Entropy distribution
for TrueReuters against SentenceSalad.
</tableCaption>
<subsectionHeader confidence="0.997831">
5.3 Detecting “noisy” documents
</subsectionHeader>
<bodyText confidence="0.999567125">
In this section, we study fake documents produced
by randomly changing words in true documents
(the TrueReuters dataset). In each document, a
fixed percentage of content words is randomly re-
placed by any other word from the training vocab-
ulary 6. This percentage was varied from 5 to 100
and EER for these corrupted document sets is com-
puted at each % corruption level (Figure 2). As
</bodyText>
<figure confidence="0.959428">
0 5 10 15 20 25 30 35 40 45 50 60 70 80 90 100
Noise level (% words changed)
</figure>
<figureCaption confidence="0.999063">
Figure 2: EER at various noise levels
</figureCaption>
<bodyText confidence="0.999900333333333">
expected, the EER is very high at low noise levels,
and as the noise level is increased, EER gets lower.
When only a few words are changed in a true doc-
ument, it retrains the properties of a true document
(high LLPW and low entropy). However, as more
number of words are changed in a true document,
</bodyText>
<footnote confidence="0.988044666666667">
6When the replacement words are chosen from a small set
of very specific words, the fake document generation strategy
is termed as “word stuffing”.
</footnote>
<figure confidence="0.990629310344828">
Normalized frequency of occurence
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
0 1 2 3 4 5 6
True: TrueReuters
False: SentenceSalad
False: WordSalad
False: Markovian
EER: LLPW and Entropy
50
45
40
35
30
25
20
15
10
5
0
LLPW
Entropy
</figure>
<page confidence="0.999124">
46
</page>
<bodyText confidence="0.999959571428572">
it starts showing the characteristics of a false docu-
ment (low LLPW and high entropy). These results
suggest that our semantic consistency tests are too
crude a measure to detect a small number of in-
consistencies, such as the ones found in the state-
of-the-art OCR or ASR systems’ outputs. On the
other hand, it confirms the numerous studies that
have shown that topic detection (and topic adapta-
tion) or text categorization tasks can be performed
with the same accuracy for moderately noisy texts
and clean texts, a finding which warrants the topic-
based LM adaptation strategies deployed in (Hei-
del et al., 2007; Tam and Schultz, 2007).
The difference in the behavior of our two pre-
dictors is striking. The EER obtained using LLPW
drops more quickly than the one obtained with en-
tropy of the topic distribution. It suggests that the
influence of “corrupting” content words (mostly
with low βtw) is heavy on the LLPW, but the topic
information is not lost till a majority of the “uncor-
rupted” content words belong to the same topic.
</bodyText>
<subsectionHeader confidence="0.989569">
5.4 Effect of the document length
</subsectionHeader>
<bodyText confidence="0.999968076923077">
In this section, we study the robustness of our
two predictors with respect to the document length
by progressively increasing the number of content
words in a document (true or fake). As can be seen
from Figure 3, the entropy of the posterior topic
distribution starts to provide a reasonable discrim-
ination (5% EER) when the test documents contain
about 80 to 100 content words, and attains results
comparable to those reported earlier in this paper
when this number doubles. This definitely rules
out this method as a predictor of the semantic con-
sistency of a sentence: we need to consider at least
a paragraph to get acceptable results.
</bodyText>
<subsectionHeader confidence="0.999349">
5.5 Testing with out-of-domain data
</subsectionHeader>
<bodyText confidence="0.9997886">
In this section, we study the robustness of our pre-
dictors on out-of-domain data using a small ex-
cerpt of abstracts from the Medline database. Both
true and fake documents are from this dataset.
The results are summarized in Table 3. The per-
</bodyText>
<table confidence="0.999252">
TrueMedline vs. LLPW Entropy
SentenceSalad 31.23% 22.13%
WordSalad 30.03% 19.46%
Markovian 36.51% 23.63%
</table>
<tableCaption confidence="0.746974666666667">
Table 3: Performance of LDA on PubMed ab-
stracts
formance on out-of-domain documents is poor,
</tableCaption>
<figure confidence="0.6499015">
TrueReuters against False sets
Number of content words
</figure>
<figureCaption confidence="0.7884238">
Figure 3: EER with change in number of con-
tent words used for LDA analysis. EER based
on: LLPW of TrueReuters and false document sets
(solid line) and Entropy of topic distribution of
TrueReuters andfalse document sets (dashed line).
</figureCaption>
<bodyText confidence="0.999871769230769">
though the entropy of the topic distribution is still
the best predictor. The reasons for this failure are
obvious: a majority of the words occurring in these
documents (true or fake) are, from the perspective
of the model, characteristic of one single Reuters
topic (health and medicine). They cannot be dis-
tinguished either in terms of perplexity or in terms
of topic distribution (the entropy is low for all the
documents). It is interesting to note that all the
out-of-domain Medline data can be separated from
the in-domain TrueReuters data with good accu-
racy on the basis of the lower LLPW of the former
as compared to the higher LLPW of the latter.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998924">
In the LDA framework, this paper investigated two
methods to infer the topic distribution in a test
document. Further, the paper suggested that the
coherence of a document can be evaluated based
on its topic distribution and average LLPW, and
these measures can help to discriminate between
true and false documents. Indeed, through exper-
imental results, it was shown that entropy of the
topic distribution is lower and average LLPW of
true documents is higher for true documents and
the former measure was found to be more effective.
However, the poor performance of this method on
out-of-domain data suggests that we need to use a
much larger training corpus to build a robust fake
document detector. This raises the issue of train-
</bodyText>
<figure confidence="0.999376947368421">
10 20 30 40 50 60 70 80 90100 125 150 175 200 225
EER: LLPW and Entropy
50
45
40
35
30
25
20
15
10
5
0
LLPW: SentenceSalad
LLPW: WordSalad
LLPW: Markovian
Entropy: SentenceSalad
Entropy: WordSalad
Entropy: Markovian
</figure>
<page confidence="0.99767">
47
</page>
<bodyText confidence="0.956577285714286">
ing LDA model with very large collections. In fu-
ture we would like to explore the potential of this
method for text segmentation tasks.
Gruber, Amit, Michal Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Proceedings
ofInternational Conference on Artificial Intelligence
and Statistics, San Juan, Puerto Rico, March.
</bodyText>
<sectionHeader confidence="0.955768" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99992">
This research was supported by the European
Commission under the contract FP6-027026-K-
Space. The views expressed in this paper are those
of the authors and do not necessarily represent the
views of the commission.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909188888889">
Blei, David and John Lafferty. 2005. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS’18), Vancouver, Canada.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2002. Latent Dirichlet allocation. In Dietterich,
Thomas G., Suzanna Becker, and Zoubin Ghahra-
mani, editors, Advances in Neural Information Pro-
cessing Systems (NIPS), volume 14, pages 601–608,
Cambridge, MA. MIT Press.
Blei, David M., Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested Chinese restaurant process. In
Advances in Neural Information Processing Systems
(NIPS), volume 16, Vancouver, Canada.
Buntine, Wray and Aleks Jakulin. 2004. Apply-
ing discrete PCA in data analysis. In Chickering,
M. and J. Halpern, editors, Proceedings of the 20th
Conference on Uncertainty in Artificial Intelligence
(UAI’04), pages 59–66. AUAI Press 2004.
Buntine, Wray, Jaakko L¨ofstr¨om, Jukka Perki¨o, Sami
Perttu, Vladimir Poroshin, Tomi Silander, Henry
Tirri, Antti Tuominen, and Ville Tuulos. 2004.
A scalable topic-based open source search engine.
In Proceedings of the IEEE/WIC/ACM International
Conference on Web Intelligence, pages 228–234,
Beijing, China.
Foltz, P.W., W. Kintsch, and T.K. Landauer. 1998. The
measurement of textual coherence with Latent Se-
mantic Analysis. Discourse Processes, 25(2-3):285–
307.
Globerson, Amir, Terry Y. Koo, Xavier Carreras, and
Michael Collins. 2007. Exponentiated gradient al-
gorithms for log-linear structured prediction. In In-
ternational Conference on Machine Learning, Cor-
vallis, Oregon.
Griffiths, Thomas L. and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101 (supl 1):5228–5235.
Griffiths, Thomas L., Mark Steyvers, David M. Blei,
and Joshua Tenenbaum. 2005. Integrating topics
and syntax. In Proceedings ofNIPS, 17, Vancouver,
CA.
Hearst, Marti. 1997. TextTiling: Segmenting texts into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64.
Heidel, Aaron, Hung an Chang, and Lin shan Lee.
2007. Language model adaptation using latent
Dirichlet allocation and an efficient topic inference
algorithm. In Proceedings of European Confer-
ence on Speech Communication and Technology,
Antwerp, Belgium.
Hofmann, Thomas. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning Journal, 42(1):177–196.
Hsu, Bo-June (Paul) and Jim Glass. 2006. Style
&amp; topic language model adaptation using HMM-
LDA. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Syd-
ney, Australia.
Kivinen, Jyrki and Manfrud K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1–
63.
Lewis, David D., Yiming Yang, Tony Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. Machine Learning Re-
search, 5:361–397.
Minka, Thomas and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Nigam, K., A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103–134.
Ohta, Tomoko, Yuka Tateisi, Hideki Mima, Jun ichi
Tsujii, and Jin-Dong Kim. 2002. The GENIA cor-
pus: an annotated research abstract corpus in molec-
ular biology domain. In Proceedings ofHuman Lan-
guage Technology Conference, pages 73–77.
Rigouste, Lois, Olivier Capp´e, and Franc¸ois Yvon.
2007. Inference and evaluation of the multino-
mial mixture model for text clustering. Informa-
tion Processing and Management, 43(5):1260–1280,
September.
Tam, Yik-Cheung and Tanja Schultz. 2007. Correlated
latent semantic model for unsupervised LM adapta-
tion. In Proceedings of the International Conference
on Acoustics, Speech and Signal Processing, Hon-
olulu, Hawaii, U.S.A.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.356811">
<title confidence="0.999932">Using LDA to detect semantically incoherent documents</title>
<author confidence="0.916594">Misra Capp´e Yvon</author>
<affiliation confidence="0.3752">LTCI/CNRS and TELECOM ParisTech Univ Paris-Sud 11 and LMISI-CNRS</affiliation>
<email confidence="0.879274">yvon@limsi.fr</email>
<abstract confidence="0.999902210526316">Detecting the semantic coherence of a document is a challenging task and has several applications such as in text segmentation and categorization. This paper is an attempt to distinguish between a ‘semantically coherent’ true document and a ‘randomly generated’ false document through topic detection in the framework of latent Dirichlet analysis. Based on the premise that a true document contains only a few topics and a false document is made up of many topics, it is asserted that the entropy of the topic distribution will be lower for a true document than that for a false document. This hypothesis is tested on several false document sets generated by various methods and is found to be useful for fake content detection applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>John Lafferty</author>
</authors>
<title>Correlated topic models.</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS’18),</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5923" citStr="Blei and Lafferty, 2005" startWordPosition="949" endWordPosition="952">alog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Section 2.4), LDA assumes that every document is represented by a topic distribution and that each topic defines an underlying distribution on words. The generative history of a document (a bagof</context>
</contexts>
<marker>Blei, Lafferty, 2005</marker>
<rawString>Blei, David and John Lafferty. 2005. Correlated topic models. In Advances in Neural Information Processing Systems (NIPS’18), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2002</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>14</volume>
<pages>601--608</pages>
<editor>In Dietterich, Thomas G., Suzanna Becker, and Zoubin Ghahramani, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3031" citStr="Blei et al., 2002" startWordPosition="465" endWordPosition="468">plications in Natural Language Processing. So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al., 1998)). It could also serve to devise automatic metrics for text summarization or machine translation tasks. This paper is an attempt to address the issue of differentiating between ‘true’ and ‘false’ documents on the basis of their consistency through topic modeling approach. We have used Latent Dirichlet allocation (LDA) (Blei et al., 2002) model as our main topic modeling tool. One of the aims of LDA and similar methods, including probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), is to produce low dimensionality representations of texts in a “semantic space” such that most of their inherent statistical characteristics are preserved. A reduction in dimensionality facilitates storage as well as faster retrieval. Modeling discrete data has many applications in classification, categorization, topic detection, data mining, information retrieval (IR), summarization and collaborative filtering (Buntine and Jakulin, 2004).</context>
<context position="5331" citStr="Blei et al., 2002" startWordPosition="850" endWordPosition="853">arameters on the efficiency of these methods. This paper is organized as follows: In Section 2, the basics of the LDA model are set. We then discuss and contrast several approaches to the problem of inferring the topic distribution of a new document in Section 3. In Section 4, we describe the corpus and experimental set-up that are used to produce the results presented in Section 5. We summarize our main findings and draw perspectives for future research in Section 6. 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hid</context>
<context position="7925" citStr="Blei et al., 2002" startWordPosition="1302" endWordPosition="1305">and θd, the likelihood of document d is a mere product of terms such as (2), which can be rewritten as: where Cdw is the count of word w in d. 2.2 LDA: Training LDA training consists of estimating the following two parameter vectors from a text collection: the topic distribution in each document d (θdt, t = 1...nT, d = 1...nD) and the word distribution in each topic (βtw, t = 1...nT, w = 1...nW). Both θd and βt define discrete distributions, respectively over the set of topics and over the set of words. Various methods have been proposed to estimate LDA parameters, such as variational method (Blei et al., 2002), expectation propagation (Minka and Lafferty, 2002) and Gibbs sampling (Griffiths and Steyvers, 2004). In this paper, we have used the latter approach, which boils down to repeatedly going through the training data and sampling the topic assigned to each word token conditioned P(Cd|θd,β) = &amp;quot; nW nT Cdw θ β ( dt tw) (3) w=1 t=1 X 42 on the topic assigned to all the other word tokens. Given a particular Gibbs sample, the posteriors for θ and β are 3: Dirichlet with parameters (Kt1+λ, ... , KtnW +λ) and Dirichlet with parameters (Jd1 + α, ... , JdnT + α), respectively, where Ktw is the number of </context>
<context position="9154" citStr="Blei et al., 2002" startWordPosition="1535" endWordPosition="1538"> is assigned to topic t and Jdt is the number of times topic t is assigned to some word token in document d. Hence, Ktw + λ βtw= PnW (4) k=1 Ktk + nWλ Jdt + α θdt = PnT (5) k=1 Jdk + nT α During the Gibbs sampling phase, βt and θd are sampled from the above posteriors while the final estimates for these parameters are obtained by averaging the posterior means over the complete set of Gibbs iteration. 2.3 LDA: Testing Training LDA model on a text collection already provides interesting insights regarding the thematic structure of the collection. This has been the primary application of LDA in (Blei et al., 2002; Griffiths and Steyvers, 2004). Even better, being a generative model, LDA can be used to make prediction regarding novel documents (assuming they use the same vocabulary as the training corpus). In a typical IR setting, where the main focus is on computing the similarity between a document d and a query d&apos;, a natural similarity measure is given by P(Cd&apos;|θd, β), computed according to (3) (Buntine et al., 2004). An alternative would be to compute the KL divergence between the topic distribution in d and d&apos;, which however requires to infer the latter quantity. As the topic distribution of a (ne</context>
</contexts>
<marker>Blei, Ng, Jordan, 2002</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2002. Latent Dirichlet allocation. In Dietterich, Thomas G., Suzanna Becker, and Zoubin Ghahramani, editors, Advances in Neural Information Processing Systems (NIPS), volume 14, pages 601–608, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical topic models and the nested Chinese restaurant process.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS),</booktitle>
<volume>16</volume>
<location>Vancouver, Canada.</location>
<contexts>
<context position="5838" citStr="Blei et al., 2004" startWordPosition="936" endWordPosition="939">Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Section 2.4), LDA assumes that every document is represented by a topic distribution and that each topic defin</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2004</marker>
<rawString>Blei, David M., Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2004. Hierarchical topic models and the nested Chinese restaurant process. In Advances in Neural Information Processing Systems (NIPS), volume 16, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wray Buntine</author>
<author>Aleks Jakulin</author>
</authors>
<title>Applying discrete PCA in data analysis.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI’04),</booktitle>
<pages>59--66</pages>
<editor>In Chickering, M. and J. Halpern, editors,</editor>
<publisher>AUAI Press</publisher>
<contexts>
<context position="3630" citStr="Buntine and Jakulin, 2004" startWordPosition="552" endWordPosition="555">n (LDA) (Blei et al., 2002) model as our main topic modeling tool. One of the aims of LDA and similar methods, including probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), is to produce low dimensionality representations of texts in a “semantic space” such that most of their inherent statistical characteristics are preserved. A reduction in dimensionality facilitates storage as well as faster retrieval. Modeling discrete data has many applications in classification, categorization, topic detection, data mining, information retrieval (IR), summarization and collaborative filtering (Buntine and Jakulin, 2004). The aim of this paper is to test LDA for establishing the semantic coherence of a document based on the premise that a real (coherent) document should discuss only a few number of topics, 2The same techniques are commonly used in mail spams also. 41 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 41–48 Manchester, August 2008 a property hardly granted for forged documents which are often made up of random assemblage of words or sentences. As a consequence, the coherence of a document may reflect in the entropy of its posterior topic distributi</context>
</contexts>
<marker>Buntine, Jakulin, 2004</marker>
<rawString>Buntine, Wray and Aleks Jakulin. 2004. Applying discrete PCA in data analysis. In Chickering, M. and J. Halpern, editors, Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence (UAI’04), pages 59–66. AUAI Press 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wray Buntine</author>
<author>Jaakko L¨ofstr¨om</author>
</authors>
<title>Jukka Perki¨o, Sami Perttu, Vladimir Poroshin, Tomi Silander, Henry Tirri, Antti Tuominen, and Ville Tuulos.</title>
<date>2004</date>
<booktitle>In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>228--234</pages>
<location>Beijing, China.</location>
<marker>Buntine, L¨ofstr¨om, 2004</marker>
<rawString>Buntine, Wray, Jaakko L¨ofstr¨om, Jukka Perki¨o, Sami Perttu, Vladimir Poroshin, Tomi Silander, Henry Tirri, Antti Tuominen, and Ville Tuulos. 2004. A scalable topic-based open source search engine. In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence, pages 228–234, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>The measurement of textual coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2692" citStr="Foltz et al., 1998" startWordPosition="411" endWordPosition="414">ple statistical models of natural texts, but the former is more challenging, it being made up of actual sentences: recognizing these texts as forged requires either to resort to plagiarism detection techniques, or to automatically identify their lack of semantic consistency. Detecting the consistency of texts or of text chunks has many applications in Natural Language Processing. So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al., 1998)). It could also serve to devise automatic metrics for text summarization or machine translation tasks. This paper is an attempt to address the issue of differentiating between ‘true’ and ‘false’ documents on the basis of their consistency through topic modeling approach. We have used Latent Dirichlet allocation (LDA) (Blei et al., 2002) model as our main topic modeling tool. One of the aims of LDA and similar methods, including probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), is to produce low dimensionality representations of texts in a “semantic space” such that most of their </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, P.W., W. Kintsch, and T.K. Landauer. 1998. The measurement of textual coherence with Latent Semantic Analysis. Discourse Processes, 25(2-3):285– 307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
<author>Terry Y Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Exponentiated gradient algorithms for log-linear structured prediction.</title>
<date>2007</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="12841" citStr="Globerson et al., 2007" startWordPosition="2164" endWordPosition="2167">e, small scale experiments convinced us that a cruder approximation was in order, as the sum in (10) is dominated by the maximum value. We thus contend ourselves to solve: θ∗ = argmax P(Cd|θ) (11) θ,Pt θt=1 and use this value to approximate P(Cd) using (3). The maximization program (11) has no close form solution. However, the objective function is differentiable and log-concave, and can be optimized in a number of ways. We considered two different algorithms: an EM-like approach, initially introduced in (Heidel et al., 2007), and an exponentiated gradient approach (Kivinen and Warmuth, 1997; Globerson et al., 2007). The first approach implements an iterative procedure based on the following update rule: θdt +— ld w=1 1 XnW PnTCdwθdtβtw (12) t0=1 θdt0βt0w Although no justification was given in (Heidel et al., 2007), it can be shown that this update rule converges towards a global optimum of the likelihood. Let θ and θ0 be two topic distributions in the nT-dimensional simplex, L(θ) = log P(Cd|θ), and ρt(w, θ) =P θtβtw t0 θt0βt0w . We define an auxiliary function Q(θ, θ0) = Pw Cw(Pt ρt(w, θ) log(θ0t)). Q(θ, θ0) is concave in θ0, and performs the role played by the auxiliary function in the EM algorithm. Si</context>
<context position="14101" citStr="Globerson et al., 2007" startWordPosition="2388" endWordPosition="2391">(i) the update (12) maximizes in θ0 the function Q(θ, θ0), and (ii) Q(θ, θ0) — Q(θ, θ) &gt; L(θ0) — L(θ), which stems from the concavity of the log. At an optimum of Q(θ, θ0) the positivity of the first term implies the positivity of the second. Maximizing Q using the update rule (12) thus increases the likelihood and repeating this update converges towards the optimum value. We experimented both with an unsmoothed (12) and with a smoothed version of this update rule. The unsmoothed version yielded a slightly better result than the smoothed one. Exponentiated gradient (Kivinen and Warmuth, 1997; Globerson et al., 2007) yields an alternative update rule: ! Cdwβtw (13) PnT t0=1 θdt0βt0w where η defines the convergence rate. In this form, the update rule does not preserve the normalization of θ, which needs to be performed after every iteration. A systematic comparison of these rules was carried out, yielding the following conclusions: • the convergence of the EM-like method is very fast. Typically, it requires less than half a dozen iterations to converge. After convergence, the topic distribution estimated by this method for a subset of train documents was always very close (as measured by the KLdivergence) </context>
</contexts>
<marker>Globerson, Koo, Carreras, Collins, 2007</marker>
<rawString>Globerson, Amir, Terry Y. Koo, Xavier Carreras, and Michael Collins. 2007. Exponentiated gradient algorithms for log-linear structured prediction. In International Conference on Machine Learning, Corvallis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="5432" citStr="Griffiths and Steyvers, 2004" startWordPosition="867" endWordPosition="870">on 2, the basics of the LDA model are set. We then discuss and contrast several approaches to the problem of inferring the topic distribution of a new document in Section 3. In Section 4, we describe the corpus and experimental set-up that are used to produce the results presented in Section 5. We summarize our main findings and draw perspectives for future research in Section 6. 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex depend</context>
<context position="8027" citStr="Griffiths and Steyvers, 2004" startWordPosition="1315" endWordPosition="1318">ewritten as: where Cdw is the count of word w in d. 2.2 LDA: Training LDA training consists of estimating the following two parameter vectors from a text collection: the topic distribution in each document d (θdt, t = 1...nT, d = 1...nD) and the word distribution in each topic (βtw, t = 1...nT, w = 1...nW). Both θd and βt define discrete distributions, respectively over the set of topics and over the set of words. Various methods have been proposed to estimate LDA parameters, such as variational method (Blei et al., 2002), expectation propagation (Minka and Lafferty, 2002) and Gibbs sampling (Griffiths and Steyvers, 2004). In this paper, we have used the latter approach, which boils down to repeatedly going through the training data and sampling the topic assigned to each word token conditioned P(Cd|θd,β) = &amp;quot; nW nT Cdw θ β ( dt tw) (3) w=1 t=1 X 42 on the topic assigned to all the other word tokens. Given a particular Gibbs sample, the posteriors for θ and β are 3: Dirichlet with parameters (Kt1+λ, ... , KtnW +λ) and Dirichlet with parameters (Jd1 + α, ... , JdnT + α), respectively, where Ktw is the number of times word w is assigned to topic t and Jdt is the number of times topic t is assigned to some word to</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Griffiths, Thomas L. and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101 (supl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>David M Blei</author>
<author>Joshua Tenenbaum</author>
</authors>
<title>Integrating topics and syntax.</title>
<date>2005</date>
<booktitle>In Proceedings ofNIPS, 17,</booktitle>
<location>Vancouver, CA.</location>
<contexts>
<context position="5872" citStr="Griffiths et al., 2005" startWordPosition="941" endWordPosition="945"> model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Section 2.4), LDA assumes that every document is represented by a topic distribution and that each topic defines an underlying distribution on w</context>
</contexts>
<marker>Griffiths, Steyvers, Blei, Tenenbaum, 2005</marker>
<rawString>Griffiths, Thomas L., Mark Steyvers, David M. Blei, and Joshua Tenenbaum. 2005. Integrating topics and syntax. In Proceedings ofNIPS, 17, Vancouver, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting texts into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="2612" citStr="Hearst, 1997" startWordPosition="399" endWordPosition="400">ord stuffing”) 2. Among these, the latter two are easy to detect using simple statistical models of natural texts, but the former is more challenging, it being made up of actual sentences: recognizing these texts as forged requires either to resort to plagiarism detection techniques, or to automatically identify their lack of semantic consistency. Detecting the consistency of texts or of text chunks has many applications in Natural Language Processing. So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al., 1998)). It could also serve to devise automatic metrics for text summarization or machine translation tasks. This paper is an attempt to address the issue of differentiating between ‘true’ and ‘false’ documents on the basis of their consistency through topic modeling approach. We have used Latent Dirichlet allocation (LDA) (Blei et al., 2002) model as our main topic modeling tool. One of the aims of LDA and similar methods, including probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), is to produce low dimens</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti. 1997. TextTiling: Segmenting texts into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron Heidel</author>
<author>Hung an Chang</author>
<author>Lin shan Lee</author>
</authors>
<title>Language model adaptation using latent Dirichlet allocation and an efficient topic inference algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="5755" citStr="Heidel et al., 2007" startWordPosition="921" endWordPosition="924">raw perspectives for future research in Section 6. 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Section 2.4), LDA assumes t</context>
<context position="12749" citStr="Heidel et al., 2007" startWordPosition="2149" endWordPosition="2152">(10) 43 Carlo samples. Given the typical length of documents and the large vocabulary size, small scale experiments convinced us that a cruder approximation was in order, as the sum in (10) is dominated by the maximum value. We thus contend ourselves to solve: θ∗ = argmax P(Cd|θ) (11) θ,Pt θt=1 and use this value to approximate P(Cd) using (3). The maximization program (11) has no close form solution. However, the objective function is differentiable and log-concave, and can be optimized in a number of ways. We considered two different algorithms: an EM-like approach, initially introduced in (Heidel et al., 2007), and an exponentiated gradient approach (Kivinen and Warmuth, 1997; Globerson et al., 2007). The first approach implements an iterative procedure based on the following update rule: θdt +— ld w=1 1 XnW PnTCdwθdtβtw (12) t0=1 θdt0βt0w Although no justification was given in (Heidel et al., 2007), it can be shown that this update rule converges towards a global optimum of the likelihood. Let θ and θ0 be two topic distributions in the nT-dimensional simplex, L(θ) = log P(Cd|θ), and ρt(w, θ) =P θtβtw t0 θt0βt0w . We define an auxiliary function Q(θ, θ0) = Pw Cw(Pt ρt(w, θ) log(θ0t)). Q(θ, θ0) is c</context>
<context position="24110" citStr="Heidel et al., 2007" startWordPosition="4099" endWordPosition="4103">ropy 46 it starts showing the characteristics of a false document (low LLPW and high entropy). These results suggest that our semantic consistency tests are too crude a measure to detect a small number of inconsistencies, such as the ones found in the stateof-the-art OCR or ASR systems’ outputs. On the other hand, it confirms the numerous studies that have shown that topic detection (and topic adaptation) or text categorization tasks can be performed with the same accuracy for moderately noisy texts and clean texts, a finding which warrants the topicbased LM adaptation strategies deployed in (Heidel et al., 2007; Tam and Schultz, 2007). The difference in the behavior of our two predictors is striking. The EER obtained using LLPW drops more quickly than the one obtained with entropy of the topic distribution. It suggests that the influence of “corrupting” content words (mostly with low βtw) is heavy on the LLPW, but the topic information is not lost till a majority of the “uncorrupted” content words belong to the same topic. 5.4 Effect of the document length In this section, we study the robustness of our two predictors with respect to the document length by progressively increasing the number of cont</context>
</contexts>
<marker>Heidel, Chang, Lee, 2007</marker>
<rawString>Heidel, Aaron, Hung an Chang, and Lin shan Lee. 2007. Language model adaptation using latent Dirichlet allocation and an efficient topic inference algorithm. In Proceedings of European Conference on Speech Communication and Technology, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<journal>Machine Learning Journal,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="3186" citStr="Hofmann, 2001" startWordPosition="492" endWordPosition="494"> the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al., 1998)). It could also serve to devise automatic metrics for text summarization or machine translation tasks. This paper is an attempt to address the issue of differentiating between ‘true’ and ‘false’ documents on the basis of their consistency through topic modeling approach. We have used Latent Dirichlet allocation (LDA) (Blei et al., 2002) model as our main topic modeling tool. One of the aims of LDA and similar methods, including probabilistic latent semantic analysis (PLSA) (Hofmann, 2001), is to produce low dimensionality representations of texts in a “semantic space” such that most of their inherent statistical characteristics are preserved. A reduction in dimensionality facilitates storage as well as faster retrieval. Modeling discrete data has many applications in classification, categorization, topic detection, data mining, information retrieval (IR), summarization and collaborative filtering (Buntine and Jakulin, 2004). The aim of this paper is to test LDA for establishing the semantic coherence of a document based on the premise that a real (coherent) document should dis</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Hofmann, Thomas. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning Journal, 42(1):177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo-June Hsu</author>
<author>Jim Glass</author>
</authors>
<title>Style &amp; topic language model adaptation using HMMLDA.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="5710" citStr="Hsu and Glass, 2006" startWordPosition="913" endWordPosition="916">tion 5. We summarize our main findings and draw perspectives for future research in Section 6. 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam </context>
</contexts>
<marker>Hsu, Glass, 2006</marker>
<rawString>Hsu, Bo-June (Paul) and Jim Glass. 2006. Style &amp; topic language model adaptation using HMMLDA. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jyrki Kivinen</author>
<author>Manfrud K Warmuth</author>
</authors>
<title>Exponentiated gradient versus gradient descent for linear predictors.</title>
<date>1997</date>
<journal>Information and Computation,</journal>
<volume>132</volume>
<pages>63</pages>
<contexts>
<context position="12816" citStr="Kivinen and Warmuth, 1997" startWordPosition="2159" endWordPosition="2163">nd the large vocabulary size, small scale experiments convinced us that a cruder approximation was in order, as the sum in (10) is dominated by the maximum value. We thus contend ourselves to solve: θ∗ = argmax P(Cd|θ) (11) θ,Pt θt=1 and use this value to approximate P(Cd) using (3). The maximization program (11) has no close form solution. However, the objective function is differentiable and log-concave, and can be optimized in a number of ways. We considered two different algorithms: an EM-like approach, initially introduced in (Heidel et al., 2007), and an exponentiated gradient approach (Kivinen and Warmuth, 1997; Globerson et al., 2007). The first approach implements an iterative procedure based on the following update rule: θdt +— ld w=1 1 XnW PnTCdwθdtβtw (12) t0=1 θdt0βt0w Although no justification was given in (Heidel et al., 2007), it can be shown that this update rule converges towards a global optimum of the likelihood. Let θ and θ0 be two topic distributions in the nT-dimensional simplex, L(θ) = log P(Cd|θ), and ρt(w, θ) =P θtβtw t0 θt0βt0w . We define an auxiliary function Q(θ, θ0) = Pw Cw(Pt ρt(w, θ) log(θ0t)). Q(θ, θ0) is concave in θ0, and performs the role played by the auxiliary functio</context>
<context position="14076" citStr="Kivinen and Warmuth, 1997" startWordPosition="2384" endWordPosition="2387">lus suffices to prove that (i) the update (12) maximizes in θ0 the function Q(θ, θ0), and (ii) Q(θ, θ0) — Q(θ, θ) &gt; L(θ0) — L(θ), which stems from the concavity of the log. At an optimum of Q(θ, θ0) the positivity of the first term implies the positivity of the second. Maximizing Q using the update rule (12) thus increases the likelihood and repeating this update converges towards the optimum value. We experimented both with an unsmoothed (12) and with a smoothed version of this update rule. The unsmoothed version yielded a slightly better result than the smoothed one. Exponentiated gradient (Kivinen and Warmuth, 1997; Globerson et al., 2007) yields an alternative update rule: ! Cdwβtw (13) PnT t0=1 θdt0βt0w where η defines the convergence rate. In this form, the update rule does not preserve the normalization of θ, which needs to be performed after every iteration. A systematic comparison of these rules was carried out, yielding the following conclusions: • the convergence of the EM-like method is very fast. Typically, it requires less than half a dozen iterations to converge. After convergence, the topic distribution estimated by this method for a subset of train documents was always very close (as measu</context>
</contexts>
<marker>Kivinen, Warmuth, 1997</marker>
<rawString>Kivinen, Jyrki and Manfrud K. Warmuth. 1997. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132:1– 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony Rose</author>
<author>Fan Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<booktitle>Machine Learning Research,</booktitle>
<pages>5--361</pages>
<contexts>
<context position="15557" citStr="Lewis et al., 2004" startWordPosition="2637" endWordPosition="2640"> erratic behaviour, and requires a careful tuning of η on a per document basis. For large values of η, the update rule (13) sometimes fails to converge; smaller values of η allowed to consistently reach convergence, but required more iterations (typically 20-30). On a positive side, on an average, the topic distributions estimated by this method are better than the ones obtained with the EM-like algorithm. Based on these findings, we decided to use the EM-like algorithm in all our subsequent experiments. 4 Experimental protocol 4.1 Training and test corpora The Reuters Corpus Volume 1 (RCV1) (Lewis et al., 2004) is a collection of over 800,000 news items in English from August 1996 to August 1997. Out of the entire RCV1 dataset, we selected 27,672 documents (news items) for training (TrainReuters) and 23,326 documents for testing (TestReuters). The first 4000 documents from the TestReuters dataset were used as true documents (TrueReuters) in the experiments reported in this paper. The vocabulary size in the train set, after removing the function words, is 93, 214. Along with these datasets of “true” documents, three datasets of fake documents were also created. Document generation techniques are many</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>Lewis, David D., Yiming Yang, Tony Rose, and Fan Li. 2004. RCV1: A new benchmark collection for text categorization research. Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Minka</author>
<author>John Lafferty</author>
</authors>
<title>Expectationpropagation for the generative aspect model.</title>
<date>2002</date>
<booktitle>In Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="7977" citStr="Minka and Lafferty, 2002" startWordPosition="1308" endWordPosition="1311">e product of terms such as (2), which can be rewritten as: where Cdw is the count of word w in d. 2.2 LDA: Training LDA training consists of estimating the following two parameter vectors from a text collection: the topic distribution in each document d (θdt, t = 1...nT, d = 1...nD) and the word distribution in each topic (βtw, t = 1...nT, w = 1...nW). Both θd and βt define discrete distributions, respectively over the set of topics and over the set of words. Various methods have been proposed to estimate LDA parameters, such as variational method (Blei et al., 2002), expectation propagation (Minka and Lafferty, 2002) and Gibbs sampling (Griffiths and Steyvers, 2004). In this paper, we have used the latter approach, which boils down to repeatedly going through the training data and sampling the topic assigned to each word token conditioned P(Cd|θd,β) = &amp;quot; nW nT Cdw θ β ( dt tw) (3) w=1 t=1 X 42 on the topic assigned to all the other word tokens. Given a particular Gibbs sample, the posteriors for θ and β are 3: Dirichlet with parameters (Kt1+λ, ... , KtnW +λ) and Dirichlet with parameters (Jd1 + α, ... , JdnT + α), respectively, where Ktw is the number of times word w is assigned to topic t and Jdt is the n</context>
</contexts>
<marker>Minka, Lafferty, 2002</marker>
<rawString>Minka, Thomas and John Lafferty. 2002. Expectationpropagation for the generative aspect model. In Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A K McCallum</author>
<author>S Thrun</author>
<author>T M Mitchell</author>
</authors>
<title>Text classification from labeled and unlabeled documents using EM.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="6323" citStr="Nigam et al., 2000" startWordPosition="1014" endWordPosition="1017">, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Section 2.4), LDA assumes that every document is represented by a topic distribution and that each topic defines an underlying distribution on words. The generative history of a document (a bagof-words) collection is the following: Assuming a fixed and known number of topics nT, for each topic t, a distribution βt over the indexing vocabulary (w = 1... nW) is drawn from a Dirichlet distribution. Then, for each document d, a distribution θd over the topics (t = 1... nT) is drawn from a Dirichlet distribution. For a document d, the document length ld being an exogenous variable, the next st</context>
<context position="10218" citStr="Nigam et al., 2000" startWordPosition="1709" endWordPosition="1712">pute the KL divergence between the topic distribution in d and d&apos;, which however requires to infer the latter quantity. As the topic distribution of a (new) document gives its representation along the latent semantic dimensions, computing this value is helpful for many applications, including text segmentation and text classification. Methods for efficiently and accurately estimating topic distribution for text documents are presented and evaluated in Section 3. 2.4 Baseline: Multinomial Mixture Model The performance of LDA model is compared with that of the simpler multinomial mixture model (Nigam et al., 2000; Rigouste et al., 2007). 3assuming non-informative priors with hyper-parameters α and A for the Dirichlet distribution over topics and the Dirichlet distribution over words respectively In this model, every word in a document belongs to the same topic, as if the document specific topic distribution θd in LDA were bound to lie on one vertex of the [0,1]nT simplex. Using the same notations as before (except for θt, which now denotes the position independent probability of topic t in the collection), the probability of a document is: βCdw (6) tw This model can be trained through expectation maxi</context>
</contexts>
<marker>Nigam, McCallum, Thrun, Mitchell, 2000</marker>
<rawString>Nigam, K., A. K. McCallum, S. Thrun, and T. M. Mitchell. 2000. Text classification from labeled and unlabeled documents using EM. Machine Learning, 39(2/3):103–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Hideki Mima</author>
</authors>
<title>The GENIA corpus: an annotated research abstract corpus in molecular biology domain.</title>
<date></date>
<booktitle>In Proceedings ofHuman Language Technology Conference,</booktitle>
<pages>73--77</pages>
<marker>Ohta, Tateisi, Mima, </marker>
<rawString>Ohta, Tomoko, Yuka Tateisi, Hideki Mima, Jun ichi Tsujii, and Jin-Dong Kim. 2002. The GENIA corpus: an annotated research abstract corpus in molecular biology domain. In Proceedings ofHuman Language Technology Conference, pages 73–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lois Rigouste</author>
<author>Olivier Capp´e</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Inference and evaluation of the multinomial mixture model for text clustering.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>43</volume>
<issue>5</issue>
<marker>Rigouste, Capp´e, Yvon, 2007</marker>
<rawString>Rigouste, Lois, Olivier Capp´e, and Franc¸ois Yvon. 2007. Inference and evaluation of the multinomial mixture model for text clustering. Information Processing and Management, 43(5):1260–1280, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yik-Cheung Tam</author>
<author>Tanja Schultz</author>
</authors>
<title>Correlated latent semantic model for unsupervised LM adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Honolulu, Hawaii, U.S.A.</location>
<contexts>
<context position="5733" citStr="Tam and Schultz, 2007" startWordPosition="917" endWordPosition="920">our main findings and draw perspectives for future research in Section 6. 2 Latent Dirichlet Allocation 2.1 Basics LDA is a probabilistic model of text data which provides a generative analog of PLSA (Blei et al., 2002), and is primarily meant to reveal hidden topics in text documents. In (Griffiths and Steyvers, 2004), the authors used LDA for identifying “hot topics” by analyzing the temporal dynamics of topics over a period of time. More recently LDA has also been used for unsupervised language model (LM) adaptation in the context of automatic speech recognition (ASR) (Hsu and Glass, 2006; Tam and Schultz, 2007; Heidel et al., 2007). Several extensions of the LDA model, such as hierarchical LDA (Blei et al., 2004), HMM-LDA (Griffiths et al., 2005), correlated topic models (Blei and Lafferty, 2005) and hidden topic Markov models (Gruber et al., 2007), have been proposed, that introduce more complex dependency patterns in the model. Like most of the text mining techniques, LDA assumes that documents are made up of words and the ordering of the words within a document is unimportant (“bag-of-words” assumption). Contrary to the simpler Multinomial Mixture Model (see, e.g., (Nigam et al., 2000) and Secti</context>
<context position="24134" citStr="Tam and Schultz, 2007" startWordPosition="4104" endWordPosition="4107">wing the characteristics of a false document (low LLPW and high entropy). These results suggest that our semantic consistency tests are too crude a measure to detect a small number of inconsistencies, such as the ones found in the stateof-the-art OCR or ASR systems’ outputs. On the other hand, it confirms the numerous studies that have shown that topic detection (and topic adaptation) or text categorization tasks can be performed with the same accuracy for moderately noisy texts and clean texts, a finding which warrants the topicbased LM adaptation strategies deployed in (Heidel et al., 2007; Tam and Schultz, 2007). The difference in the behavior of our two predictors is striking. The EER obtained using LLPW drops more quickly than the one obtained with entropy of the topic distribution. It suggests that the influence of “corrupting” content words (mostly with low βtw) is heavy on the LLPW, but the topic information is not lost till a majority of the “uncorrupted” content words belong to the same topic. 5.4 Effect of the document length In this section, we study the robustness of our two predictors with respect to the document length by progressively increasing the number of content words in a document </context>
</contexts>
<marker>Tam, Schultz, 2007</marker>
<rawString>Tam, Yik-Cheung and Tanja Schultz. 2007. Correlated latent semantic model for unsupervised LM adaptation. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, Honolulu, Hawaii, U.S.A.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>