<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003415">
<title confidence="0.720508">
The Automatic Acquisition of Frequencies of Verb
Subcategorization Frames from Tagged Corpora
</title>
<author confidence="0.539938">
Akira Ushioda, David A. Evans, Ted Gibson, Alex Waibel
</author>
<affiliation confidence="0.462354">
Computational Linguistics Program
Carnegie Mellon University
Pittsburgh, PA 15213-3890
</affiliation>
<email confidence="0.994373">
aushioda@lcl.cmu.edu
</email>
<sectionHeader confidence="0.969614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911857142857">
We describe a mechanism for automatically acquiring verb subcategorization frames
and their frequencies in a large corpus. A tagged corpus is first partially parsed to
identify noun phrases and then a linear grammar is used to estimate the appropri-
ate subcategorization frame for each verb token in the corpus. In an experiment
involving the identification of six fixed subcategorization frames, our current system
showed more than 80% accuracy. In addition, a new statistical approach substan-
tially improves the accuracy of the frequency estimation.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986307692308">
When we construct a grammar, there is always a trade-off between the coverage of the
grammar and the ambiguity of the grammar. If we hope to develop an efficient high-
coverage parser for unrestricted texts, we must have some means of dealing with the
combinatorial explosion of syntactic ambiguities. While a general probabilistic optimiza-
tion technique such as the Inside-Outside algorithm ([Baker, 1979], [Lauri and Young,
1990], [Jelinek et al., 1990], [Carroll and Charniak, 1992]) can be used to reduce ambi-
guity by providing estimates on the applicability of the context-free rules in a grammar
(for example), the algorithm does not take advantage of lexical information, including
such information as verb subcategorization frame preferences. Discovering or acquiring
lexically-sensitive linguistic structures from large corpora may offer an essential comple-
mentary approach.
Verb subcategorization (verb-subcat) frames represent one of the most important ele-
ments of grammatical/lexical knowledge for efficient and reliable parsing. At this stage in
the computational-linguistic exploration of corpora, dictionaries are still probably more re-
liable than automatic acquisition systems as a source of subcategorization (subcat) frames
for verbs. The Oxford Advanced Learners Dictionary (OALD) [Hornby, 19891, for exam-
ple, uses 32 verb patterns to describe a usage of each verb for each meaning of the verb.
However, dictionaries do not provide quantitative information such as how often each
verb is used with each of the possible subcat frames. Since dictionaries are repositories,
primarily, of what is possible, not what is most likely, they tend to contain information
about rare usage [de Marken, 1992]. But without information about the frequencies of the
subcat frames we find in dictionaries, we face the prospect of having to treat each frame
as equiprobable in parsing. This can lead to serious inefficiency. We also know that the
frequency of subcat frames can vary by domain; frames that are very rare in one domain
can be quite common in another. If we could automatically determine the frequencies
of subcat frames for domains, we would be able to tailor parsing with domain-specific
</bodyText>
<page confidence="0.997553">
95
</page>
<bodyText confidence="0.99935862962963">
heuristics. Indeed, it would be desirable to have a subcat dictionary for each possible
domain.
This paper describes a mechanism for automatically acquiring subcat frames and their
frequencies based on a tagged corpus. The method utilizes a tagged corpus because (i)
we don&apos;t have to deal with a lexical ambiguity (ii) tagged corpora in various domains
are becoming readily available and (iii) simple and robust tagging techniques using such
corpora recently have been developed ([Church, 1988], [Brill, 1992]).
Brent reports a method for automatically acquiring subcat frames but without fre-
quency measurements ([Brent and Berwick, 1991], [Brent, 1991]). His approach is to
count occurrences of those unambiguous verb phrases that contain no noun phrases other
than pronouns or proper nouns. By thus restricting the &amp;quot;features&amp;quot; that trigger identifi-
cation of a verb phrase, he avoids possible errors due to syntactic ambiguity. Although
the rate of false positives is very low in his system, his syntactic features are so selective
that most verb tokens fail to satisfy them. (For example, verbs that occurred fewer than
20 times in the corpus tend to have no co-occurrences with the features.) Therefore his
approach is not useful in determining verb-subcat frame frequencies.
To measure frequencies, we need, ideally, to identify a subcht frame for each verb token
in the corpus. This, in turn, requires a full parse of the corpus. Since manually parsed
corpora are rare and typically small, and since automatically parsed corpora contain
many errors (given current parsing technologies), an alternative source of useful linguistic
structure is needed. We have elected to use partially parsed sentences automatically
derived from a lexically-tagged corpus. The partial parse contains information about
minimal noun phrases (without PP attachment or clausal complements). While such
derived information about linguistic structure is less accurate and complete than that
available in certified, hand-parsed corpora, the approach promises to generalize and to
yield large sample sizes. In particular, we can use partially parsed corpora to measure
verb-subcat frame frequencies.
</bodyText>
<sectionHeader confidence="0.953079" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.997569">
The procedure to find verb-subcat frequencies, automatically, is as follows.
</bodyText>
<listItem confidence="0.842435333333333">
(1) Make a list of verbs out of the tagged corpus.
(2) For each verb on the list (the &amp;quot;target verb&amp;quot;),
(2.1) Tokenize each sentence containing the target verb in the following way:
</listItem>
<bodyText confidence="0.8765679">
All the noun phrases except pronouns are tokenized as &amp;quot;n&amp;quot; by a noun phrase
parser and all the rest of the words are also tokenized following the schmema
in Table 1. For example, the sentence &amp;quot;The corresponding mental-state verbs
do not follow [target verb] these rules in a straightforward way&amp;quot; is transformed
to a sequence of tokens &amp;quot;bnvaknpne&amp;quot;.
(2.2) Apply a set of subcat extraction rules to the tokenized sentences. These rules
are written as regular expressions and they are obtained through the examina-
tion of occurrences of a small sample of verbs in a training text.
Note that in the actual implementation of the procedure, all of the redundant oper-
ations are eliminated. Our NP parser also uses a finite-state grammar. It is designed
</bodyText>
<page confidence="0.999055">
96
</page>
<tableCaption confidence="0.999127">
Table 1: List of Symbols/Categories
</tableCaption>
<bodyText confidence="0.99551625">
especially to support identification of verb-subcat frames. One of its special features is
that it detects time-adjuncts such as &amp;quot;yesterday&amp;quot;, &amp;quot;two months ago&amp;quot;, or &amp;quot;the following
day&amp;quot;, and eliminates them in the tokenization process. For example, the sentence &amp;quot;He told
the reporters the following day that...&amp;quot; is tokenized to &amp;quot;bivnc...&amp;quot; instead of &amp;quot;bivnnc...&amp;quot;.
</bodyText>
<sectionHeader confidence="0.981221" genericHeader="method">
3 Experiment on Wall Street Journal Corpus
</sectionHeader>
<bodyText confidence="0.970373694444445">
We used the above method in experiments involving a tagged corpus of Wall Street Journal
(WSJ) articles, provided by the Penn Treebank project. Our experiment was limited in
two senses. First, we treated all prepositional phrases as adjuncts. (It is generally difficult
to distinguish complement and adjunct PPs.) Second, we measured the frequencies of
only six fixed subcat frames for verbs in non-participle form. (This does not represent
an essential shortcoming in the method; we only need to have additional subcat frame
extraction rules to accommodate participles.)
We extracted two sets of tagged sentences from the WSJ corpus, each representing 3-
MBytes and approximately 300,000 words of text. One set was used as a training corpus,
the other as a test corpus. Table 2 gives the list of verb-subcat frame extraction rules
obtained (via examination) for four verbs &amp;quot;expect&amp;quot;, &amp;quot;reflect&amp;quot;, &amp;quot;tell&amp;quot;, and &amp;quot;give&amp;quot;, as they
occurred in the training corpus. Sample sentences that can be captured by each set of
rules are attached to the list. Table 3 shows the result of the hand comparison of the
automatically identified verb-subcat frames for &amp;quot;give&amp;quot; and &amp;quot;expect&amp;quot; in the test corpus.
The tabular columns give actual frequencies for each verb-subcat frame based on man-
ual review and the tabular rows give the frequencies as determined automatically by the
system. The count of each cell j]) gives the number of occurrences of the verb that
are assigned the i-th subcat frame by the system and assigned the j-th frame by manual
review. The frame/column labeled &amp;quot;REST&amp;quot; represents all other subcat frames, encom-
passing such subcat frames as those involving wh-clauses, verb-particle combinations (such
as &amp;quot;give up&amp;quot;), and no complements.
Despite the simplicity of the rules, the frequencies for subcat frames determined under
automatic processing are very close to the real distributions. Most of the errors are
attributable to errors in the noun phrase parser. For example, 10 out of the 13 errors
in the [NP,NP+NP] cell under &amp;quot;give&amp;quot; are due to noun phrase parsing errors such as the
misidentification of a N—N sequence (e.g., *&amp;quot;give [NI. government officials rights] against
the press&amp;quot; vs. &amp;quot;give [NE. government officials] [Nr, rights] against the press&amp;quot;).
b: sentence initial maker
k: target verb
i: pronoun
n: noun phrase
v: finite verb
u: participial verb
d: base form verb
p: preposition
e: sentence final maker
</bodyText>
<figure confidence="0.967460448275862">
t: &amp;quot;to&amp;quot;
m: modal
w: relative pronoun
a: adverb
x: punctuation
c: complementizer &amp;quot;that&amp;quot;
s: the rest
97
Frame Rule
1. NP-FNP k (i I n)n
2. NP-FCL k(i I n(pn)*)c
k(iln)(iln)a*(ulv)
3. NP+INF k(i I n(pn)*) ta*d
4. CL kc
k (i. I n)a*(all v)
5. NP k(i I 0/ [ourd]
Spy (ii n (pn)*) a*m?a*k/ rt]
6. INF kta*d
Notes:
NP: noun phrase
CL: that-clause with and without the complementizer &amp;quot;that&amp;quot;
INF: &amp;quot;to&amp;quot; + infinitive
x* matches a sequence of any number of x&apos;s including zero x
x? is either x or empty
(x I y) matches either x or y
[xyz] matches any token except x, y, and z
Sic(sequence) matches (sequence) that is not directly preceded by x
x/y matches x if x is immediately followed by y
Sample Sentences:
</figure>
<figureCaption confidence="0.9552988">
Frame I. &amp;quot;...gives current management enough time to work on...&amp;quot;
Frame 2. &amp;quot;...tell the people in the hall that...&amp;quot;; &amp;quot;...told him the man would...&amp;quot;
Frame 3. &amp;quot;...expected the impact from the restructuring to make...&amp;quot;
Frame 4. &amp;quot;...think that...&amp;quot;; &amp;quot;...thought the company eventually responded...&amp;quot;
Frame 5. &amp;quot;...saw the man...&amp;quot;; &amp;quot;...which the president of the company wanted...&amp;quot;
</figureCaption>
<table confidence="0.851854666666667">
but not
&amp;quot;...saw him swim...&amp;quot;; &amp;quot;...(hotel) in which he stayed...&amp;quot;; &amp;quot;...(gift) which he expected to get...&amp;quot;
Frame 6. &amp;quot;...expects to gain...&amp;quot;
</table>
<tableCaption confidence="0.994618">
Table 2: Set of Subcategorization Frame Extraction Rules
</tableCaption>
<page confidence="0.927644">
98
</page>
<table confidence="0.9979915">
NP-I-NP &amp;quot;Give&amp;quot; CL INF REST Total
Real Occurrences
NP+CL NP+INF NP
NP+NP 52 0 0 0 0 0 0 52
NP+CL 1 0 0 0 0 0 0 1
Output NP+INF 2 0 0 0 0 0 0 2
of NP 13 0 0 27 0 0 0 40
System CL 0 0 0 0 0 0 0 0
INF 0 0 0 0 0 0 0 0
REST 1 0 0 4 0 0 9 14
Total 69 0 0 31 0 0 9 109
NP-I-NP &amp;quot;Expect&amp;quot; CL INF REST Total
Real Occurrences
NP-I-CL NP+INF NP
NP-I-NP 0 0 0 0 0 0 0 0
NP+CL 0 0 0 0 0 0 0 0
Output NP+INF 0 0 55 1 0 0 0 56
of NP 0 0 4 28 0 0 0 32
System CL 0 0 0 0 8 0 0 8
INF 0 0 0 0 0 40 0 40
REST 0 0 1 6 0 0 7 14
Total 0 0 60 35 8 40 7 150
Table 3: Subcategorization Frame Frequencies
acquire end like spend
build expand need total
close fail produce try
comment file prove use
consider follow reach want
continue get receive work
design help reduce
develop hold see
elect let sign
</table>
<tableCaption confidence="0.998283">
Table 4: Verbs Tested
</tableCaption>
<page confidence="0.978352">
99
</page>
<sectionHeader confidence="0.551041" genericHeader="method">
THIS PAGE INTENTIONALLY LEFT BLANK
</sectionHeader>
<page confidence="0.484477">
100
</page>
<figure confidence="0.997132375">
10
6---
1-1 r71-1
Number
of
Verbs
&lt;5 5-10 10-15 15-20 20-25 25-30 30-35 34-40 40-45
Error Rate (%)
</figure>
<figureCaption confidence="0.999999">
Figure 1: Distribution of Errors
</figureCaption>
<bodyText confidence="0.999983777777778">
To measure the total accuracy of the system, we randomly chose 33 verbs from the
300 most frequent verbs in the test corpus (given in Table 4), automatically estimated
the subcat frames for each occurrence of these verbs in the test corpus, and compared the
results to manually determined subcat frames.
The overall results are quite promising. The total number of occurrences of the 33
verbs in the test corpus (excluding participle forms) is 2,242. Of these, 1,933 were assigned
correct subcat frames by the system. (The `correct&apos;-assignment counts always appear in
the diagonal cells in a comparison table such as in Table 3.) This indicates an overall
accuracy for the method of 86%.
If we exclude the subcat frame &amp;quot;REST&amp;quot; from our statistics, the total number of oc-
currences of the 33 verbs in one of the six subcat frames is 1,565. Of these, 1,311 were
assigned correct subcat frames by the system. This represents 83% accuracy.
For 30 of the 33 verbs, both the first and the second (if any) most frequent subcat
frames as determined by the system were correct. For all of the verbs except one (&amp;quot;need&amp;quot;),
the most frequent frame was correct.
Figure 1 is a histogram showing the number of verbs within each error-rate zone.
In computing the error rate, we divide the total `off-diagonal&apos;-cell counts, excluding the
counts in the &amp;quot;REST&amp;quot; column, by the total cell counts, again excluding the &amp;quot;REST&amp;quot; col-
umn margin. Thus, the off-diagonal cell counts in the &amp;quot;REST&amp;quot; row, representing instances
where one of the six actual subcat frames was misidentified as &amp;quot;REST&amp;quot;, are counted as
errors. This formula, in general, gives higher error rates than would result from simply
dividing the off-diagonal cell counts by the total cell counts.
Overall, the most frequent source of errors, again, was errors in noun phrase boundary
detection. The second most frequent source was misidentification of infinitival &apos;purpose&apos;
clauses, as in &amp;quot;he used a crowbar to open the door&amp;quot;. &amp;quot;To open the door&amp;quot; is a &apos;purpose&apos;
adjunct modifying either the verb phrase &amp;quot;used a crowbar&amp;quot; or the main clause &amp;quot;he used a
crowbar&amp;quot;. But such adjuncts are incorrectly judged to be complements of their main verbs
</bodyText>
<page confidence="0.997842">
101
</page>
<bodyText confidence="0.999955333333333">
by the subcat frame extraction rules in Table 2. In formulating the rules, we assumed that
a &apos;purpose&apos; adjunct appears effectively randomly and much less frequently than infinitival
complements. This is true for our corpus in general; but some verbs, such as &amp;quot;use&amp;quot; and
&amp;quot;need&amp;quot;, appear relatively frequently with &apos;purpose&apos; infinitivals. In addition to errors from
parsing and &apos;purpose&apos; infinitives, we observed several other, less frequent types of errors.
These, too, pattern with specific verbs and do not occur randomly across verbs.
</bodyText>
<sectionHeader confidence="0.979926" genericHeader="method">
4 Statistical Analysis
</sectionHeader>
<bodyText confidence="0.999935083333333">
For most of the verbs in the experiment, our method provides a good measure of subcat
frame frequencies. However, some of the verbs seem to appear in syntactic structures that
cannot be captured by our inventory of subcat frames. For example, &amp;quot;need&amp;quot; is frequently
used in relative clauses without relative pronouns, as in &amp;quot;the last thing they need&amp;quot;. Since
this kind of relative clauses cannot be captured by the rules in Table 2, each occurrence
of these relative clause causes an error in measurement. It is likely that there are many
other classes of verbs with distinctive syntactic preferences. If we try to add rules for each
such class, it will become increasingly difficult to write rules that affect only the target
class and to eliminate undesirable rule interactions.
In the following sections, we describe a statistical method which, based on a set of
training samples, enables the system to learn patterns of errors and substantially increase
the accuracy of estimated verb-subcat frequencies.
</bodyText>
<subsectionHeader confidence="0.997043">
4.1 General Scheme
</subsectionHeader>
<bodyText confidence="0.99997747826087">
The method described in Section 2 is wholly deterministic; it depends only on one set
of subcat extraction rules which serve as filters. Instead of treating the system output
for each verb token as an estimated subcat frame, we can think of the output as one
feature associated with the occurrence of the verb. This single feature can be combined,
statistically, with other features in the corpus to yield more accurate characterizations
of verb contexts and more accurate subcat-frame frequency estimates. If the other fea-
tures are capturable via regular-expression rules, they can also be automatically detected
in the manner described in the Section 2. For example, main verbs in relative clauses
without relative pronouns may have a higher probability of having the feature &amp;quot;nnk&amp;quot;, i.e.,
&amp;quot;(NP)(NP)(VERB)&amp;quot;.
More formally, let Y be a response variable taking as its value a subcat frame. Let
X1, X2, ..., XN be explanatory variables. Each Xi is associated with a feature expressed
by one or a set of regular expressions. If a feature is expressed by one regular expression
(R), the value of the feature is 1 if the occurrence of the verb matches R and 0 otherwise.
If the feature is expressed by a set of regular expressions, its value is the label of the
regular expression that the occurrence of the verb matches. The set of regular expressions
in Table 2 can therefore be considered to characterize one explanatory variable whose
value ranges from (NP-1-NP) to (REST).
Now, we assume that a training corpus is available in which all verb tokens are given
along with their subcat frames. By running our system on the training corpus, we can
automatically generate a (N 1)-dimensional contingency table. Table 3 is an example
of a 2-dimensional contingency table with X = &lt;OUTPUT OF SYSTEM&gt; and Y = &lt;REAL
OCCURRENCES&gt;. Using loglinear models [Agresti, 1990], we can derive fitted values of
</bodyText>
<page confidence="0.997536">
102
</page>
<bodyText confidence="0.9999166">
each cell in the (N 1)-dimensional contingency table. In the case of a saturated model,
in which all kinds of interaction of variables up to (N 1)-way interactions are included,
the raw cell counts are the Maximum Likelihood solution. The fitted values are then used
to estimate the subcat frame frequencies of a new corpus as follows.
First, the system is run on the new corpus to obtain an N-dimensional contingency
table. This table is considered to be an XI — X2 - • • • - XN-marginal table. What we
are aiming at is the Y margins that represent the real subcat frame frequencies of the
new corpus. Assuming that the training corpus and the new corpus are homogeneous
(e.g., reflecting similar sub-domains or samples of a common domain), we estimate the Y
margins using Bayes theorem on the fitted values of the training corpus as follows:
</bodyText>
<equation confidence="0.9956042">
E(Y = kIX1 X2 - • • • - XN marginal table of the new corpus)
=EE•••EAriii2..iN+P(Y= kIXj = i1,X2 = j2, • • • , X N = iN)
11 12 IN
11 12 IN
11 12 IN
P(Xi = i1, X2 = i2, • • XN = iN 1Y = k) P(Y = k)
•I
N EVIP(Xi = i1, X2 = i2, • • • , XN = iN 1Y = lei) PO&apos; = k9]
A41112.••iNk
Eki
</equation>
<bodyText confidence="0.999222666666667">
where Al;,i2. is the cell count of the X1 — X2 - • • • - XN marginal table of the new
corpus obtained as the system output, and M1i12. .iNk is the fitted value of the (N 1)-
dimensional contingency table of the training corpus based on a particular loglinear model.
</bodyText>
<subsectionHeader confidence="0.99911">
4.2 Lexical Heuristics
</subsectionHeader>
<bodyText confidence="0.9995403">
The simplest application of the above method is to use a 2-way contingency table, as in
Table 3. There are two possibilities to explore in constructing a 2-way contingency table.
One is to sum up the cell counts of all the verbs in the training corpus and produce a
single (large) general table. The other is to construct a table for each verb. Obviously
the former approach is preferable if it works. Unfortunately, such a table is typically too
general to be useful; the estimated frequencies based on it are less accurate than raw
system output. This is because the sources of errors, viz., the distribution of off-diagonal
cell counts of 2-way contingency tables, differ considerably from verb to verb. The latter
approach is problematic if we have to make such a table for each domain. However, if we
have a training corpus in one domain, and if the heuristics for each verb extracted from
the training corpus are also applicable to other domains, the approach may work.
To test the latter possibility, we constructed a contingency table for the verb from
the test corpus described in the Section 3 that was most problematic (least accurately
estimated) among the 33 verbs—&amp;quot;need&amp;quot;. Note that we are using the test corpus described
in the Section 3 as a training corpus here, because we already know both the measured
frequency and the hand-judged frequency of &amp;quot;need&amp;quot; which are necessary to construct a
contingency table. The total occurrence of this verb was 75. To smooth the table, 0.1 is
added to all the cell counts. As new test corpora, we extracted another 300,000 words of
tagged text from the WSJ corpus (labeled &amp;quot;W3&amp;quot;) and also three sets of 300,000 words of
tagged text from the Brown corpus (labeled &amp;quot;B1&amp;quot;, &amp;quot;B2&amp;quot;, and &amp;quot;B3&amp;quot;), as retagged under the
</bodyText>
<page confidence="0.995563">
103
</page>
<table confidence="0.9981703">
W3 NPA-NP NPA-CL NP-FINF NP CL INF REST
Measured 2.4 0.0 10.6 44.7 1.2 31.8 9.4
By Hand 0.0 0.0 0.0 69.4 0.0 30.6 0.0
Estimated 0.0 0.0 0.0 66.3 0.0 30.1 3.6
Total Occurrences: 85
B1 NPA-NP NPA-CL NP-FINF NP CL INF REST
Measured 1.8 0.9 7.9 38.6 1.8 14.9 34.2
By Hand 0.0 0.0 0.0 72.8 0.0 15.8 11.4
Estimated 0.0 0.0 0.0 76.6 0.0 14.4 9.1
Total Occurrences: 114
B2 NP-FNP NP-FCL NP-FINF NP CL INF REST
Measured 0.0 1.4 8.7 40.6 1.4 17.4 30.4
By Hand 0.0 0.0 0.0 73.9 0.0 18.8 7.2
Estimated 0.0 0.0 0.0 76.1 0.0 16.4 7.5
Total Occurrences: 69
B3 NP-FNP NPA-CL NP-FINF NP CL INF REST
Measured 3.3 0.0 1.7 30.0 3.3 31.7 30.0
By Hand 0.0 0.0 0.0 60.0 0.0 28.3 11.7
Estimated 0.0 0.0 0.0 61.4 0.0 29.8 8.8
Total Occurrences: 60
</table>
<tableCaption confidence="0.998412">
Table 5: Statistical Estimation (Unit = %) for the Verb &amp;quot;Need&amp;quot;
</tableCaption>
<bodyText confidence="0.999669071428572">
Penn Treebank tagset. All the training and test corpora were reviewed-and judged-by
hand.
Table 5 gives the frequency distributions based on the system output, hand judge-
ment, and statistical analysis. (As before, we take the hand judgement to be the gold
standard, the actual frequency of a particular frame.) After the Y margins are statisti-
cally estimated, the least estimated Y values less than 1.0 are truncated to 0. (These are
considered to have appeared due to the smoothing.)
In all of the test corpora, the method gives very accurate frequency distribution es-
timates. Big gaps between the automatically-measured and manually-determined fre-
quencies of &amp;quot;NP&amp;quot; and &amp;quot;REST&amp;quot; are shown to be substantially reduced through the use of
statistical estimation. This result is especially encouraging because the heuristics obtained
in one domain are shown to be applicable to a considerably different domain. Further-
more, by combining more feature sets and making use of multi-dimensional analysis, we
can expect to obtain more accurate estimations.
</bodyText>
<page confidence="0.99855">
104
</page>
<sectionHeader confidence="0.987826" genericHeader="conclusions">
5 Conclusion and Future Direction
</sectionHeader>
<bodyText confidence="0.99999395">
We have demonstrated that by combining syntactic and statistical analysis, the frequencies
of verb-subcat frames can be estimated with high accuracy. Although the present system
measures the frequencies of only six subcat frames, the method is general enough to be
extended to many more frames. The traditional application of regular expressions as
rules for deterministic processing has self-evident limitations since a linear grammar is
not powerful enough to capture general linguistic phenomena. The statistical method we
propose uses regular expressions as filters for detecting specific features of the occurrences
of verbs and employs multi-dimensional analysis of the features based on loglinear models
and Bayes Theorem.
We expect that by identifying other useful syntactic features we can further improve
the accuracy of the frequency estimation. Such features can be regarded as characterizing
the syntactic context of the verbs, quite broadly. The features need not be linked to a
local verb context. For example, a regular expression such as &amp;quot;w [vex] *k&amp;quot; can be used
to find cases where the target verb is preceded by a relative pronoun such that there is
no other finite verb or punctuation or sentence final period between the relative pronoun
and the target verb.
If the syntactic structure of a sentence can be predicted using only syntactic and lexical
knowledge, we can hope to estimate the subcat frame of each occurrence of a verb using
the context expressed by a set of features. We thus can aim to extend and refine this
method for use with general probabilistic parsing of unrestricted text.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998538333333333">
We thank Teddy Seidenfeld, Jeremy York, and Alex Franz for their comments and dis-
cussions with us. We remain, of course, solely responsible for any errors or inadequacies
in the paper.
</bodyText>
<sectionHeader confidence="0.999078" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996523">
[Agresti, 1990] A. Agresti. Categorical Data Analysis. New York, NY: John Wiley and
Sons, 1990.
[Baker, 1979] J. Baker. &amp;quot;Trainable Grammars for Speech Recognition&amp;quot;. In D.H. Klatt
and J.J. Wolf (eds.), Speech Communication Papers for the 97th Meeting of the Acoustic
Society of America, 1979, pp. 547-550.
[Brent, 1991] M.R. Brent. &amp;quot;Automatic Acquisition of Subcategorization Frames from
Untagged Text&amp;quot;. Proceedings of the 29th Annual Meeting of the ACL, 1991.
[Brent and Berwick, 1991] M.R. Brent and R.C. Berwick. &amp;quot;Automatic Acquisition of Sub-
categorization Frames from Tagged Text&amp;quot;. In Proceedings of the DARPA Speech and
Natural Language Workshop, Morgan Kaufmann, 1991.
[Brill, 19921 E. Brill. &amp;quot;A Simple Rule-Based Part of Speech Tagger&amp;quot;. In Proceedings of
the DARPA Speech and Natural Language Workshop, Morgan Kaufmann, 1992.
</reference>
<page confidence="0.98366">
105
</page>
<reference confidence="0.9996304375">
[Carroll and Charniak, 1992] G. Carroll and E. Charniak. &amp;quot;Learning Probabilistic De-
pendency Grammars from Labelled Text&amp;quot;. In Working Notes of the Symposium on
Probabilistic Approaches to Natural Language, AAAI Fall Symposium Series, 1992.
[Church, 1988] K.W. Church. &amp;quot;A Stochastic Parts Program and Noun Phrase Parser
for Unrestricted Text&amp;quot;. In Proceedings of the Second Conference on Applied Natural
Language Processing, 1988.
[de Marken, 1992] C.G. de Marcken. &amp;quot;Parsing the LOB Corpus&amp;quot;. In Proceedings of the
28th Annual Meeting of the ACL, 1990, pp. 243-251.
[Hornby, 1989] A.S. Hornby, (ed.). Oxford Advanced Learner&apos;s Dictionary of Current
English. Oxford, UK: Oxford University Press, 1989.
[Jelinek et al., 1990] F. Jelinek, L.D. Lafferty, and R.L. Mercer. Basic Method of Proba-
bilistic Context Free Grammars. Technical Report RC 16374 (72684), IBM, Yorktown
Heights, NY 10598, 1990.
[Lauri and Young, 1990] K. Lan i and S.J. Young. &amp;quot;The Estimation of Stochastic Context-
Free Grammars Using the Inside-Outside Algorithm&amp;quot;. Computer Speech and Language,
4, 1990, pp. 35-56.
</reference>
<page confidence="0.997323">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789768">
<title confidence="0.9966465">The Automatic Acquisition of Frequencies of Subcategorization Frames from Tagged Corpora</title>
<author confidence="0.999512">Akira Ushioda</author>
<author confidence="0.999512">David A Evans</author>
<author confidence="0.999512">Ted Gibson</author>
<author confidence="0.999512">Alex Waibel</author>
<affiliation confidence="0.997297">Computational Linguistics Carnegie Mellon</affiliation>
<address confidence="0.828164">Pittsburgh, PA</address>
<email confidence="0.999052">aushioda@lcl.cmu.edu</email>
<abstract confidence="0.995698875">We describe a mechanism for automatically acquiring verb subcategorization frames and their frequencies in a large corpus. A tagged corpus is first partially parsed to identify noun phrases and then a linear grammar is used to estimate the appropriate subcategorization frame for each verb token in the corpus. In an experiment involving the identification of six fixed subcategorization frames, our current system showed more than 80% accuracy. In addition, a new statistical approach substantially improves the accuracy of the frequency estimation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Agresti</author>
</authors>
<title>Categorical Data Analysis.</title>
<date>1990</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York, NY:</location>
<marker>[Agresti, 1990]</marker>
<rawString>A. Agresti. Categorical Data Analysis. New York, NY: John Wiley and Sons, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baker</author>
</authors>
<title>Trainable Grammars for Speech Recognition&amp;quot;.</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustic Society of America,</booktitle>
<pages>547--550</pages>
<editor>In D.H. Klatt and J.J. Wolf (eds.),</editor>
<marker>[Baker, 1979]</marker>
<rawString>J. Baker. &amp;quot;Trainable Grammars for Speech Recognition&amp;quot;. In D.H. Klatt and J.J. Wolf (eds.), Speech Communication Papers for the 97th Meeting of the Acoustic Society of America, 1979, pp. 547-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
</authors>
<title>Automatic Acquisition of Subcategorization Frames from Untagged Text&amp;quot;.</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the ACL,</booktitle>
<marker>[Brent, 1991]</marker>
<rawString>M.R. Brent. &amp;quot;Automatic Acquisition of Subcategorization Frames from Untagged Text&amp;quot;. Proceedings of the 29th Annual Meeting of the ACL, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Brent</author>
<author>R C Berwick</author>
</authors>
<title>Automatic Acquisition of Subcategorization Frames from Tagged Text&amp;quot;.</title>
<date>1991</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<marker>[Brent and Berwick, 1991]</marker>
<rawString>M.R. Brent and R.C. Berwick. &amp;quot;Automatic Acquisition of Subcategorization Frames from Tagged Text&amp;quot;. In Proceedings of the DARPA Speech and Natural Language Workshop, Morgan Kaufmann, 1991. [Brill, 19921 E. Brill. &amp;quot;A Simple Rule-Based Part of Speech Tagger&amp;quot;. In Proceedings of the DARPA Speech and Natural Language Workshop, Morgan Kaufmann, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>E Charniak</author>
</authors>
<title>Learning Probabilistic Dependency Grammars from Labelled Text&amp;quot;.</title>
<date>1992</date>
<booktitle>In Working Notes of the Symposium on Probabilistic Approaches to Natural Language, AAAI Fall Symposium Series,</booktitle>
<marker>[Carroll and Charniak, 1992]</marker>
<rawString>G. Carroll and E. Charniak. &amp;quot;Learning Probabilistic Dependency Grammars from Labelled Text&amp;quot;. In Working Notes of the Symposium on Probabilistic Approaches to Natural Language, AAAI Fall Symposium Series, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text&amp;quot;.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<marker>[Church, 1988]</marker>
<rawString>K.W. Church. &amp;quot;A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text&amp;quot;. In Proceedings of the Second Conference on Applied Natural Language Processing, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C G de Marcken</author>
</authors>
<title>Parsing the LOB Corpus&amp;quot;.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the ACL,</booktitle>
<pages>243--251</pages>
<marker>[de Marken, 1992]</marker>
<rawString>C.G. de Marcken. &amp;quot;Parsing the LOB Corpus&amp;quot;. In Proceedings of the 28th Annual Meeting of the ACL, 1990, pp. 243-251.</rawString>
</citation>
<citation valid="true">
<date>1989</date>
<booktitle>Oxford Advanced Learner&apos;s Dictionary of Current English.</booktitle>
<editor>A.S. Hornby, (ed.).</editor>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK:</location>
<marker>[Hornby, 1989]</marker>
<rawString>A.S. Hornby, (ed.). Oxford Advanced Learner&apos;s Dictionary of Current English. Oxford, UK: Oxford University Press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>L D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic Method of Probabilistic Context Free Grammars.</title>
<date>1990</date>
<tech>Technical Report RC 16374 (72684), IBM,</tech>
<location>Yorktown Heights, NY 10598,</location>
<marker>[Jelinek et al., 1990]</marker>
<rawString>F. Jelinek, L.D. Lafferty, and R.L. Mercer. Basic Method of Probabilistic Context Free Grammars. Technical Report RC 16374 (72684), IBM, Yorktown Heights, NY 10598, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The Estimation of Stochastic ContextFree Grammars Using the Inside-Outside Algorithm&amp;quot;.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<pages>35--56</pages>
<marker>[Lauri and Young, 1990]</marker>
<rawString>K. Lan i and S.J. Young. &amp;quot;The Estimation of Stochastic ContextFree Grammars Using the Inside-Outside Algorithm&amp;quot;. Computer Speech and Language, 4, 1990, pp. 35-56.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>