<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002085">
<title confidence="0.9943695">
BUAP: An Unsupervised Approach to Automatic Keyphrase Extraction
from Scientific Articles
</title>
<author confidence="0.999374">
Roberto Ortiz, David Pinto, Mireya Tovar H´ector Jim´enez-Salazar
</author>
<affiliation confidence="0.999409">
Faculty of Computer Science, BUAP Information Technologies Dept., UAM
</affiliation>
<address confidence="0.90308">
Puebla, Mexico DF, Mexico
</address>
<email confidence="0.938078">
korn resorte2003@hotmail.com, hgimenezs@gmail.com
{dpinto, mtovar}@cs.buap.mx
</email>
<sectionHeader confidence="0.997098" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999007647058824">
In this paper, it is presented an unsuper-
vised approach to automatically discover
the latent keyphrases contained in scien-
tific articles. The proposed technique is
constructed on the basis of the combi-
nation of two techniques: maximal fre-
quent sequences and pageranking. We
evaluated the obtained results by using
micro-averaged precision, recall and F-
scores with respect to two different gold
standards: 1) reader’s keyphrases, and 2)
a combined set of author’s and reader’s
keyphrases. The obtained results were
also compared against three different base-
lines: one unsupervised (TF-IDF based)
and two supervised (Naive Bayes and
Maximum Entropy).
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99984518">
The task of automatic keyphrase extraction has
been studied for several years. Firstly, as semantic
metadata useful for tasks such as summarization
(Barzilay and Elhadad, 1997; Lawrie et al., 2001;
DAvanzo and Magnini, 2005), but later rec-
ognizing the impact that good keyphrases
would have on the quality of various Nat-
ural Language Processing (NLP) applica-
tions (Frank et al., 1999; Witten et al., 1999;
Turney, 1999; Barker and Corrnacchia, 2000;
Medelyan and Witten, 2008). Thus, the selection
of important, topical phrases from within the
body of a document may be used in order to
improve the performance of systems dealing
with different NLP problems such as, clustering,
question-answering, named entity recognition,
information retrieval, etc.
In general, a keyphrase may be considered as
a sequence of one or more words that capture the
main topic of the document, as that keyphrase is
expected to represent one of the key ideas ex-
pressed by the document author. Following the
previously mentioned hypothesis, we may take ad-
vantage of two different techniques of text analy-
sis: maximal frequent sequences to extract a se-
quence of one or more words from a given text,
and pageranking, expecting to extract those word
sequences that represent the key ideas of the au-
thor.
The interest on extracting high quality
keyphrases from raw text has motivated forums,
such as SemEval, where different systems may
evaluate their performances. The purpose of
SemEval is to evaluate semantic analysis systems.
In particular, in this paper we are reporting the
results obtained in Task #5 of SemEval-2 2010,
which has been named: “Automatic Keyphrase
Extraction from Scientific Articles”. We focused
this paper on the description of our approach and,
therefore, we do not describe into detail the task
nor the dataset used. For more information about
this information read the “Task #5 Description
paper”, also published in this proceedings volume
(Nam Kim et al., 2010).
The rest of this paper is structured as follows.
Section 2 describes into detail the components of
the proposed approach. In Section 3 it is shown
the performance of the presented system. Finally,
in Section 4 a discussion of findings and further
work is given.
</bodyText>
<sectionHeader confidence="0.711775" genericHeader="method">
2 Description of the approach
</sectionHeader>
<bodyText confidence="0.999910111111111">
The approach presented in this paper relies on the
combination of two different techniques for select-
ing the most prominent terms of a given text: max-
imal frequent sequences and pageranking. In Fig-
ure 1 we may see this two step approach, where
we are considering a sequence to be equivalent to
an n-gram. The complete description of the pro-
cedure is given as follows.
We select maximal frequent sequences which
</bodyText>
<page confidence="0.97534">
174
</page>
<bodyText confidence="0.907069888888889">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 174–177,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
we consider to be candidate keyphrases and, there-
after, we ranking them in order to determine which
ones are the most importants (according to the
pageranking algorithm). In the following subsec-
tions we give a brief description of these two tech-
niques. Afterwards, we provide an algorithm of
the presented approach.
</bodyText>
<figureCaption confidence="0.9927115">
Figure 1: Two step approach of BUAP Team at the
Task #5 of SemEval-2
</figureCaption>
<subsectionHeader confidence="0.987687">
2.1 Maximal Frequent Sequences
</subsectionHeader>
<bodyText confidence="0.989498722222222">
Definition: If a sequence p is a subsequence of q
and the number of elements in p is equal to n, then
the p is called an n-gram in q.
Definition: A sequence p = a1 · · · ak is a sub-
sequence of a sequence q if all the items ai occur
in q and they occur in the same order as in p. If
a sequence p is a subsequence of a sequence q we
say that p occurs in q.
Definition: A sequence p is frequent in S if p is
a subsequence of at least Q documents in S where
Q is a given frequency threshold. Only one oc-
currence of sequence in the document is counted.
Several occurrences within one document do not
make the sequence more frequent.
Definition: A sequence p is a maximal frequent
sequence in S if there does not exists any sequence
q in S such that p is a subsequence of q and p is
frequent in S.
</bodyText>
<subsectionHeader confidence="0.999045">
2.2 PageRanking
</subsectionHeader>
<bodyText confidence="0.998767454545455">
The algorithm of PageRanking was defined by
Brin and Page in (Brin and Page, 1998). It is a
graph-based algorithm used for ranking webpages.
The algorithm considers input and output links of
each page in order to construct a graph, where
each vertex is a webpage and each edge may be
the input or output links for this webpage. They
denote as In(Vi) the set of input links of webpage
Vi, and Out(Vi) their output links. The algorithm
proposed to rank each webpage based on the vot-
ing or recommendation of other webpages. The
higher the number of votes that are cast for a ver-
tex, the higher the importance of the vertex. More-
over, the importance of the vertex casting the vote
determines how important the vote itself is, and
this information is also taken into account by the
ranking model.
Although this algoritm has been initially pro-
posed for webpages ranking, it has been also used
for other NLP applications which may model their
corresponding problem in a graph structure. Eq.
(1) is the formula proposed by Brin and Page.
</bodyText>
<equation confidence="0.963986666666667">
1
|Out(Vj)|S(Vj)
(1)
</equation>
<bodyText confidence="0.999947066666667">
where d is a damping factor that can be set be-
tween 0 and 1, which has the role of integrat-
ing into the model the probability of jumping
from a given vertex to another random vertex
in the graph. This factor is usually set to 0.85
(Brin and Page, 1998).
There are some other propossals, like the one
presented in (Mihalcea and Tarau, 2004), where a
textranking algorithm is presented. The authors
consider a weighted version of PageRank and
present some applications to NLP using unigrams.
They also construct multi-word terms by exploring
the conections among ranked words in the graph.
Our algorithm differs from textranking in that we
use MFS for feeding the PageRanking algorithm.
</bodyText>
<subsectionHeader confidence="0.986981">
2.3 Algorithm
</subsectionHeader>
<bodyText confidence="0.995158777777778">
The complete algoritmic description of the pre-
sented approach is given in Algorithm 1. Read-
ers and writers keyphrases may be quite dif-
ferent. In particular, writers usually introduce
acronyms in their text, but they use the complete
or expanded representation of these acronyms
for their keyphrases. Therefore, we have in-
cluded a module (Extract Acronyms) for ex-
tracting both, acronyms with their corresponding
expanded version, which are used afterwards as
output of our system. We have preprocessed the
dataset removing stopwords and punctuation sym-
bols. Lemmatization (TreeTagger1) and stemming
(Porter Stemmer (Porter, 1980)) were also applied
in some stages of preprocessing.
The Maximal Freq Sequences module ex-
tracts maximal frequent sequences of words and
we feed the PageRaking module (PageRanking)
</bodyText>
<footnote confidence="0.664976">
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
</footnote>
<equation confidence="0.956790333333333">
�
S(Vi) = (1 − d) + d ∗
j∈In(Vi)
</equation>
<page confidence="0.974468">
175
</page>
<bodyText confidence="0.942502157894737">
with all these sequences for determining the most
important ones. We use the structure of the sci-
entific articles in order to determine in and out
links of the sequences found. In fact, we use a
neighborhood criterion (a pair of MFS in the same
sentence) for determining the links between those
MFS’s. Once the ranking is calculated, we may se-
lect those sequences of a given length (unigrams,
bigrams and trigrams) as output of our system. We
also return a maximum of three acronyms, and
their associated multiterm phrases (MultiTerm),
as candidate keyphrases. Determining the length
and quantity of the sequences (n-grams) was ex-
perimentally deduced from the training corpus.
Algorithm 1: Algorithm of the Two Step ap-
proach for the Task #5 at SemEval-2
Input: A document set: D = {d1, d2, · · · }
Output: A set K = {K1, K2, · · · } of
keyphrases for each document di:
</bodyText>
<equation confidence="0.946143103448276">
Ki = {ki,1, ki,2, ··· }
1 foreach di E D do
AcronymSet = Extract Acronyms(di);
d1i = Pre Processing(di);
MFS =Maximal Freq Sequences(d1i );
CK = PageRanking(d1i , MFS);
CU = Top Nine Unigrams(CK);
CT = Top Three Trigrams(CK);
Ki = CT;
NU = 0;
Acronyms = 0;
foreach unigram E CU do
if unigram E AcronymSet then
if Acronyms &lt; 3 then
Ki = Ki U {unigram};
EA = MultiTerm(unigram);
Ki = Ki U {EA};
Acronyms++;
end
else
Ki = Ki U {unigram};
NU++;
end
end
N = (15−(2*Acronyms+jCTj+NU));
CB = Top N Bigrams(CK, N);
Ki = Ki U CB;
27 end
28 return K = {K1, K2, · · · }
</equation>
<bodyText confidence="0.999770875">
In this edition of the Task #5 of SemEval-2
2010, we tested three different runs, which were
named: BUAP −1, BUAP − 2 and BUAP − 3.
Definition and differences among the three runs
are given in Table 3.
The results obtained with each run, together
with three different baselines are given in the fol-
lowing section.
</bodyText>
<sectionHeader confidence="0.997091" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.999973173913044">
In all tables, P, R, F mean micro-averaged pre-
cision, recall and F-scores. For baselines, there
were provided 1,2,-3 grams as candidates and
TFIDF as features. In Table 2, TFIDF is an
unsupervised method to rank the candidates based
on TFIDF scores. NB and ME are super-
vised methods using Naive Bayes and maximum
entropy in WEKA. In second column, R means
to use the reader-assigned keyword set as gold-
standard data and C means to use both author-
assigned and reader-assigned keyword sets as an-
swers.
Notice from Tables 2 and 3 that we outper-
formed all the baselines for the Top 15 candidates.
However, the Top 10 candidates were only outper-
formed by the Reader-Assigned keyphrases found.
This implies that the Writer keyphrases we ob-
tained were not of as good as the Reader ones. As
we mentioned, readers and writers assign different
keywords. The former write keyphrases based on
the lecture done, by the latter has a wider context
and their keyphrases used to be more complex. We
plan to investigate this issue in the future.
</bodyText>
<sectionHeader confidence="0.99738" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9991985">
We have presented an approach based on the ex-
traction of maximal frequent sequences which are
then ranked by using the pageranking algorithm.
Three different runs were tested, modifying the
preprocessing stage and the number of bigrams
given as output. We did not see an improve-
ment when we used lemmatization of the docu-
ments. The run which obtained the best results
was ranking by the organizer according to the top
15 best keyphrases, however, we may see that our
runs need to be analysed more into detail in order
to provide a re-ranking procedure for the best 15
keyphrases found. This procedure may improve
the top 5 candidates precision.
</bodyText>
<page confidence="0.584712">
2
</page>
<figure confidence="0.959613875">
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</figure>
<page confidence="0.933467">
176
</page>
<bodyText confidence="0.62557575">
Run name Description
BUAP − 1 : This run is exactly the one described in Algorithm 1.
BUAP − 2 : Same as BUAP − 1 but lemmatization was applied a priori and stemming at the end.
BUAP − 3 : Same as BUAP − 2 but output twice the number ofbigrams.
</bodyText>
<tableCaption confidence="0.998066">
Table 1: Description of the three runs submitted to the Task #5 of SemEval-2 2010
</tableCaption>
<table confidence="0.99902275">
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
TF − IDF R 17.80% 7.39% 10.44% 13.90% 11.54% 12.61% 11.60% 14.45% 12.87%
C 22.00% 7.50% 11.19% 17.70% 12.07% 14.35% 14.93% 15.28% 15.10%
NB R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
ME R 16.80% 6.98% 9.86% 13.30% 11.05% 12.07% 11.40% 14.20% 12.65%
C 21.40% 7.30% 10.89% 17.30% 11.80% 14.03% 14.53% 14.87% 14.70%
</table>
<tableCaption confidence="0.872781">
Table 2: Baselines
</tableCaption>
<table confidence="0.998940875">
Method by top 5 candidates top 10 candidates top 15 candidates
P R F P R F P R F
BUAP − 1 R 10.40% 4.32% 6.10% 13.90% 11.54% 12.61% 14.93% 18.60% 16.56%
C 13.60% 4.64% 6.92% 17.60% 12.01% 14.28% 19.00% 19.44% 19.22%
BUAP − 2 R 10.40% 4.32% 6.10% 13.80% 11.46% 12.52% 14.67% 18.27% 16.27%
C 14.40% 4.91% 7.32% 17.80% 12.14% 14.44% 18.73% 19.17% 18.95%
BUAP − 3 R 10.40% 4.32% 6.10% 12.10% 10.05% 10.98% 12.33% 15.37% 13.68%
C 14.40% 4.91% 7.32% 15.60% 10.64% 12.65% 15.67% 16.03% 15.85%
</table>
<tableCaption confidence="0.999881">
Table 3: The three different runs submitted to the competition
</tableCaption>
<sectionHeader confidence="0.998256" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999726">
This work has been partially supported by CONA-
CYT (Project #106625) and PROMEP (Grant
#103.5/09/4213).
</bodyText>
<sectionHeader confidence="0.999138" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99961506">
[Barker and Corrnacchia2000] K. Barker and N. Cor-
rnacchia. 2000. Using noun phrase heads to extract
document keyphrases. In 13th Biennial Conference
of the Canadian Society on Computational Studies
ofIntelligence: Advances in Artificial Intelligence.
[Barzilay and Elhadad1997] R. Barzilay and M. El-
hadad. 1997. Using lexical chains for text sum-
marization. In ACL/EACL 1997 Workshop on Intel-
ligent Scalable Text Summarization, pages 10–17.
[Brin and Page1998] S. Brin and L. Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. In COMPUTER NETWORKS AND ISDN
SYSTEMS, pages 107–117. Elsevier Science Pub-
lishers B. V.
[DAvanzo and Magnini2005] E. DAvanzo and
B. Magnini. 2005. A keyphrase-based approach
to summarization:the lake system. In Document
Understanding Conferences (DUC-2005).
[Frank et al.1999] E. Frank, G.W. Paynter, I. Witten,
C. Gutwin, and C.G. Nevill-Manning. 1999. Do-
main specific keyphrase extraction. In 16th Interna-
tional Joint Conference on AI, pages 668–673.
[Lawrie et al.2001] D. Lawrie, W. B. Croft, and
A. Rosenberg. 2001. Finding topic words for hi-
erarchical summarization. In SIGIR 2001.
[Medelyan and Witten2008] O. Medelyan and I. H.
Witten. 2008. Domain independent automatic
keyphrase indexing with small training sets. Jour-
nal ofAmerican Societyfor Information Science and
Technology, 59(7):1026–1040.
[Mihalcea and Tarau2004] R. Mihalcea and P. Tarau.
2004. Textrank: Bringing order into texts. In
EMNLP 2004, ACL, pages 404–411.
[Nam Kim et al.2010] S. Nam Kim, O. Medelyan, and
M.Y. Kan. 2010. Semeval-2010 task5: Auto-
matic keyphrase extraction from scientific articles.
In Proceedings of the Fifth International Workshop
on Semantic Evaluations (SemEval-2010). Associa-
tion for Computational Linguistics.
[Porter1980] M. F. Porter. 1980. An algorithm for suf-
fix stripping. Program, 14(3).
[Turney1999] P. Turney. 1999. Learning to extract
keyphrases from text. Technical Report ERB-1057.
(NRC #41622), National Research Council, Institute
for Information Technology.
[Witten et al.1999] I. Witten, G. Paynter, E. Frank,
C. Gutwin, and G. Nevill-Manning. 1999.
Kea:practical automatic key phrase extraction. In
fourth ACM conference on Digital libraries, pages
254–256.
</reference>
<page confidence="0.997705">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.484176">
<title confidence="0.996034">BUAP: An Unsupervised Approach to Automatic Keyphrase Extraction from Scientific Articles</title>
<author confidence="0.999777">Roberto Ortiz</author>
<author confidence="0.999777">David Pinto</author>
<author confidence="0.999777">Mireya Tovar H´ector Jim´enez-Salazar</author>
<affiliation confidence="0.954009">Faculty of Computer Science, BUAP Information Technologies Dept., UAM</affiliation>
<address confidence="0.725744">Puebla, Mexico DF, Mexico</address>
<email confidence="0.984524">kornresorte2003@hotmail.com,hgimenezs@gmail.com</email>
<abstract confidence="0.994649647058823">In this paper, it is presented an unsupervised approach to automatically discover the latent keyphrases contained in scientific articles. The proposed technique is constructed on the basis of the combination of two techniques: maximal frequent sequences and pageranking. We evaluated the obtained results by using micro-averaged precision, recall and Fscores with respect to two different gold standards: 1) reader’s keyphrases, and 2) a combined set of author’s and reader’s keyphrases. The obtained results were also compared against three different baselines: one unsupervised (TF-IDF based) and two supervised (Naive Bayes and</abstract>
<intro confidence="0.757176">Maximum Entropy).</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Barker</author>
<author>N Corrnacchia</author>
</authors>
<title>Using noun phrase heads to extract document keyphrases.</title>
<date>2000</date>
<booktitle>In 13th Biennial Conference of the Canadian Society on Computational Studies ofIntelligence: Advances in Artificial Intelligence.</booktitle>
<marker>[Barker and Corrnacchia2000]</marker>
<rawString>K. Barker and N. Corrnacchia. 2000. Using noun phrase heads to extract document keyphrases. In 13th Biennial Conference of the Canadian Society on Computational Studies ofIntelligence: Advances in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In ACL/EACL 1997 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<marker>[Barzilay and Elhadad1997]</marker>
<rawString>R. Barzilay and M. Elhadad. 1997. Using lexical chains for text summarization. In ACL/EACL 1997 Workshop on Intelligent Scalable Text Summarization, pages 10–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In COMPUTER NETWORKS AND ISDN SYSTEMS,</booktitle>
<pages>107--117</pages>
<publisher>Elsevier Science Publishers</publisher>
<marker>[Brin and Page1998]</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual web search engine. In COMPUTER NETWORKS AND ISDN SYSTEMS, pages 107–117. Elsevier Science Publishers B. V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E DAvanzo</author>
<author>B Magnini</author>
</authors>
<title>A keyphrase-based approach to summarization:the lake system.</title>
<date>2005</date>
<booktitle>In Document Understanding Conferences (DUC-2005).</booktitle>
<marker>[DAvanzo and Magnini2005]</marker>
<rawString>E. DAvanzo and B. Magnini. 2005. A keyphrase-based approach to summarization:the lake system. In Document Understanding Conferences (DUC-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Frank</author>
<author>G W Paynter</author>
<author>I Witten</author>
<author>C Gutwin</author>
<author>C G Nevill-Manning</author>
</authors>
<title>Domain specific keyphrase extraction.</title>
<date>1999</date>
<booktitle>In 16th International Joint Conference on AI,</booktitle>
<pages>668--673</pages>
<marker>[Frank et al.1999]</marker>
<rawString>E. Frank, G.W. Paynter, I. Witten, C. Gutwin, and C.G. Nevill-Manning. 1999. Domain specific keyphrase extraction. In 16th International Joint Conference on AI, pages 668–673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lawrie</author>
<author>W B Croft</author>
<author>A Rosenberg</author>
</authors>
<title>Finding topic words for hierarchical summarization.</title>
<date>2001</date>
<booktitle>In SIGIR</booktitle>
<marker>[Lawrie et al.2001]</marker>
<rawString>D. Lawrie, W. B. Croft, and A. Rosenberg. 2001. Finding topic words for hierarchical summarization. In SIGIR 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>I H Witten</author>
</authors>
<title>Domain independent automatic keyphrase indexing with small training sets.</title>
<date>2008</date>
<journal>Journal ofAmerican Societyfor Information Science and Technology,</journal>
<volume>59</volume>
<issue>7</issue>
<marker>[Medelyan and Witten2008]</marker>
<rawString>O. Medelyan and I. H. Witten. 2008. Domain independent automatic keyphrase indexing with small training sets. Journal ofAmerican Societyfor Information Science and Technology, 59(7):1026–1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In EMNLP 2004, ACL,</booktitle>
<pages>404--411</pages>
<marker>[Mihalcea and Tarau2004]</marker>
<rawString>R. Mihalcea and P. Tarau. 2004. Textrank: Bringing order into texts. In EMNLP 2004, ACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nam Kim</author>
<author>O Medelyan</author>
<author>M Y Kan</author>
</authors>
<title>Semeval-2010 task5: Automatic keyphrase extraction from scientific articles.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</booktitle>
<marker>[Nam Kim et al.2010]</marker>
<rawString>S. Nam Kim, O. Medelyan, and M.Y. Kan. 2010. Semeval-2010 task5: Automatic keyphrase extraction from scientific articles. In Proceedings of the Fifth International Workshop on Semantic Evaluations (SemEval-2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>[Porter1980]</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Learning to extract keyphrases from text.</title>
<date>1999</date>
<tech>Technical Report ERB-1057. (NRC #41622),</tech>
<institution>National Research Council, Institute for Information Technology.</institution>
<marker>[Turney1999]</marker>
<rawString>P. Turney. 1999. Learning to extract keyphrases from text. Technical Report ERB-1057. (NRC #41622), National Research Council, Institute for Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>G Paynter</author>
<author>E Frank</author>
<author>C Gutwin</author>
<author>G Nevill-Manning</author>
</authors>
<title>Kea:practical automatic key phrase extraction.</title>
<date>1999</date>
<booktitle>In fourth ACM conference on Digital libraries,</booktitle>
<pages>254--256</pages>
<marker>[Witten et al.1999]</marker>
<rawString>I. Witten, G. Paynter, E. Frank, C. Gutwin, and G. Nevill-Manning. 1999. Kea:practical automatic key phrase extraction. In fourth ACM conference on Digital libraries, pages 254–256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>