<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003248">
<note confidence="0.654262">
SOME CHART-BASED TECHNIQUES FOR
PARSING ILL-FORMED INPUT
Chris S. Mellish
Department of Artificial Intelligence,
University of Edinburgh,
80 South Bridge,
EDINBURGH EH1 1HN,
</note>
<sectionHeader confidence="0.843863" genericHeader="abstract">
Scotland.
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999772285714286">
We argue for the usefulness of an active chart as the
basis of a system that searches for the globally most
plausible explanation of failure to syntactically parse
a given input. We suggest semantics-free, grammar-
independent techniques for parsing inputs displaying
simple kinds of ill-formedness and discuss the search
issues involved.
</bodyText>
<sectionHeader confidence="0.847084" genericHeader="introduction">
THE PROBLEM
</sectionHeader>
<bodyText confidence="0.999950782608696">
Although the ultimate solution to the problem
of processing ill-formed input must take into account
semantic and pragmatic factors, nevertheless it is
important to understand the limits of recovery stra-
tegies that are based entirely on syntax and which are
independent of any particular grammar. The aim of
this work is therefore to explore purely syntactic and
grammar-independent techniques to enable a parser
to recover from simple kinds of ill-formedness in tex-
tual inputs. Accordingly, we present a generalised
parsing strategy based on an active chart which is
capable of diagnosing simple errors
(unknown/misspelled words, omitted words, extra
noise words) in sentences (from languages described
by context free phrase structure grammars without e-
productions). This strategy has the advantage that
the recovery process can run after a standard (active
chart) parser has terminated unsuccessfully, without
causing existing work to be repeated or the original
parser to be slowed down in any way, and that,
unlike previous systems, it allows the full syntactic
context to be exploited in the determination of a
&amp;quot;best&amp;quot; parse for an ill-formed sentence.
</bodyText>
<sectionHeader confidence="0.995917" genericHeader="method">
EXPLOITING SYNTACTIC CONTEXT
</sectionHeader>
<bodyText confidence="0.999548666666667">
Weischedel and Sondheimer (1983) present an
approach to processing ill-formed input based on a
modified AIN parser. The basic idea is, when an ini-
tial parse fails, to select the incomplete parsing path
that consumes the longest initial portion of the input,
apply a special rule to allow the blocked parse to
continue, and then to iterate this process until a
successful parse is generated. The result is a &amp;quot;hill-
climbing&amp;quot; search for the &amp;quot;best&amp;quot; parse, relying at each
point on the &amp;quot;longest path&amp;quot; heuristic. Unfortunately,
sometimes this heuristic will yield several possible
parses, for instance with the sentence:
The snow blocks T te road
(no partial parse getting past the point shown) where
the parser can fail expecting either a verb or a deter-
miner. Moreover, sometimes the heuristic will cause
the most &amp;quot;obvious&amp;quot; error to be missed:
He said that the snow the road T
The paper will T the best news is the Times
where we might suspect that there is a missing verb
and a misspelled &amp;quot;with&amp;quot; respectively. In all these
cases, the &amp;quot;longest path&amp;quot; heuristic fails to indicate
unambiguously the minimal change that would be
necessary to make the whole input acceptable as a
sentence. This is not surprising, as the left-right bias
of an ATN parser allows the system to take no
account of the right context of a possible problem
element.
Weischedel and Sondheimer&apos;s use of the
&amp;quot;longest path&amp;quot; heuristic is similar to the use of locally
least-cost error recovery in Anderson and
Backhouse&apos;s (1981) scheme for compilers. It seems
to be generally accepted that any form of globally
&amp;quot;minimum-distance&amp;quot; error correction will be too
costly to implement (Aho and Ullman, 1977). Such
work has, however, not considered heuristic
approaches, such as the one we are developing.
Mother feature of Weischedel and
Sondheimer&apos;s system is the use of grammar-specific
recovery rules (&amp;quot;meta-rules&amp;quot; in their terminology).
The same is true of many other systems for dealing
with ill-formed input (e.g. Carbonell and Hayes
(1983), Jensen et al. (1983)). Although grammar-
specific recovery rules are likely in the end always to
be more powerful than grammar-independent rules, it
does seem to be worth investigating how far one can
get with rules that only depend on the grammar for-
malism used.
</bodyText>
<page confidence="0.995839">
102
</page>
<table confidence="0.628254">
ir the T gardener T collects T manure T if T the T autumn
0 1 2 3 4 5 6 7
&lt;Need S from 0 to 7&gt;
&lt;Need NP+VP from 0 to 7&gt;
&lt;Need VP from 2 to 7&gt;
&lt;Need VP+PP from 2 to 7&gt;
&lt;Need PP from 4 to 7&gt;
&lt;Need P+NP from 4 to 7&gt;
&lt;Need P from 4 to 5&gt;
(hypothesis)
(by top-down rule)
(by fundamental rule with NP found bottom-up)
(by top-down rule)
(by fundamental rule with VP found bottom-up)
(by top-down rule)
(by fundamental rule with NP found bottom-up)
</table>
<figureCaption confidence="0.966074">
Figure I: Focusing on an error.
</figureCaption>
<bodyText confidence="0.995450642857143">
In adapting an AIN parser to compare partial
parses, Weischedel and Sondheimer have already
introduced machinery to represent several alternative
partial parses simultaneously. From this, it is a rela-
tively small step to introduce a well-formed substring
table, or even an active chart, which allows for a glo-
bal assessment of the state of the parser. If the gram-
mar formalism is also changed to a declarative for-
malism (e.g. CF-PSGs, DCGs (Pereira and Warren
1980), Patr-II (Shieber 1984)), then there is a possi-
bility of constructing other partial parses that do not
start at the beginning of the input. In this way, right
context can play a role in the determination of the
&amp;quot;best&amp;quot; parse
</bodyText>
<subsectionHeader confidence="0.712708">
WHAT A CHART PARSER LEAVES BEHIND
</subsectionHeader>
<bodyText confidence="0.99969644">
The information that an active chart parser
leaves behind for consideration by a &amp;quot;post mortem&amp;quot;
obviously depends on the parsing strategy used (Kay
1980, Gazdar and Mellish 1989). Active edges are
particularly important from the point of view of diag-
nosing errors, as an unsatisfied active edge suggests a
place where an input error may have occurred. So
we might expect to combine violated expectations
with found constituents to hypothesise complete
parses. For simplicity, we assume here that the
grammar is a simple CF-PSG, although there are
obvious generalisations.
(Left-right) top-down parsing is guaranteed to
create active edges for each kind of phrase that could
continue a partial parse starting at the beginning of
the input On the other hand, bottom-up parsing (by
which we mean left corner parsing without top-down
filtering) is guaranteed to find all complete consti-
tuents of every possible parse. In addition, whenever
a non-empty initial segment of a rule RHS has been
found, the parser will create active edges for the kind
of phrase predicted to occur after this segment. Top-
down parsing will always create an edge for a phrase
that is needed for a parse, and so it will always
indicate by the presence of an unsatisfied active edge
the first error point, if there is one. If a subsequent
error is present, top-down parsing will not always
create an active edge corresponding to it, because the
second may occur within a constituent that will not
be predicted until the first error is corrected. Simi-
larly, right-to-left top-down parsing will always indi-
cate the last error point, and a combination of the two
will find the first and last, but not necessarily any
error points in between. On the other hand, bottom-
up parsing will only create an active edge for each
error point that comes immediately after a sequence
of phrases corresponding to an initial segment of the
RHS of a grammar rule. Moreover, it will not neces-
sarily refine its predictions to the most detailed level
(e.g. having found an NP, it may predict the
existence of a following VP, but not the existence of
types of phrases that can start a VP). Weischedel and
Sondheimer&apos;s approach can be seen as an incremen-
tal top-down parsing, where at each stage the right-
most unsatisfied active edge is artificially allowed to
be satisfied in some way. As we have seen, there is
no guarantee that this sort of hill-climbing will find
the &amp;quot;best&amp;quot; solution for multiple errors, or even for
single errors. How can we combine bottom-up and
top-down parsing for a more effective solution?
</bodyText>
<sectionHeader confidence="0.699375" genericHeader="method">
FOCUSING ON AN ERROR
</sectionHeader>
<bodyText confidence="0.999119692307692">
Our basic strategy is to run a bottom-up parser
over the input and then, if this fails to find a complete
parse, to run a modified top-down parser over the
resulting chart to hypothesise possible complete
parses. The modified top-down parser attempts to
find the minimal errors that, when taken account of,
enable a complete parse to be constructed. Imagine
that a bottom-up parser has already run over the input
&amp;quot;the gardener collects manure if the autumn&amp;quot;. Then
Figure 1 shows (informally) how a top-down parser
might focus on a possible error. To implement this
kind of reasoning, we need a top-down parsing rule
that knows how to refine a set of global needs and a
</bodyText>
<page confidence="0.996693">
103
</page>
<bodyText confidence="0.99996992">
fundamental rule that is able to incorporate found
constituents from either direction. When we may
encounter multiple errors, however, we need to
express multiple needs (e.g. &lt;Need N from 3 to 4 and
PP from 8 to 10&gt;). We also need to have a funda-
mental rule that can absorb found phrases from any-
where in a relevant portion of the chart (e.g. given a
rule &amp;quot;NP —&gt; Det Adj N&amp;quot; and a sequence &amp;quot;as marvel-
lous sihgt&amp;quot;, we need to be able to hypothesise that
&amp;quot;as&amp;quot; should be a Det and &amp;quot;sihgt&amp;quot; a N). To save
repeating work, we need a version of the top-down
rule that stops when it reaches an appropriate
category that has already been found bottom-up.
Finally, we need to handle both &amp;quot;anchored&amp;quot; and
&amp;quot;unanchored&amp;quot; needs. In an anchored need (e.g.
&lt;Need NP from 0 to 4&gt;) we know the beginning and
end of the portion of the chart within which the
search is to take place. In looking for a NP VP
sequence in &amp;quot;the happy blageon smupled the bait&amp;quot;,
however, we can&apos;t initially find a complete (initial)
NP or (final) VP and hence don&apos;t know where in the
chart these phrases meet. We express this by &lt;Need
NP from 0 to *, VP from * to 6&gt;, the symbol &amp;quot;*&amp;quot;
denoting a position in the chart that remains to be
determined.
</bodyText>
<sectionHeader confidence="0.947332" genericHeader="method">
GENERALISED TOP-DOWN PARSING
</sectionHeader>
<bodyText confidence="0.999696226666667">
If we adopt a chart parsing strategy with only
edges that carry information about global needs,
there will be considerable duplicated effort. For
instance, the further refinement of the two edges:
&lt;Need NP from 0 to 3 and V from 9 to 10&gt;
&lt;Need NP from 0 to 3 and Adj from 10 to 11&gt;
can lead to any analysis of possible NPs between 0
and 3 being done twice. Restricting the possible for-
mat of edges in this way would be similar to allowing
the &amp;quot;functional composition rule&amp;quot; (Steedman 1987) in
standard chart parsing, and in general this is not done
for efficiency reasons. Instead, we need to produce a
single edge that is &amp;quot;in charge&amp;quot; of the computation
looking for NPs between 0 and 3. When possible NPs
are then found, these then need to be combined with
the original edges by an appropriate form of the fun-
damental rule. We are thus led to the following form
for a generalised edge in our chart parser:
&lt;C from S to E needs
CSI nom si to el,
c32 from s2 to e2,
cs from sa to ea&gt;
where C is a category, the csi are lists of categories
(which we will show inside square brackets), S. E,
the si and the ei are positions in the chart (or the spe-
cial symbol &amp;quot;*&amp;quot;). The presence of an edge of this
kind in the chart indicates that the parser is attempt-
ing to find a phrase of category C covering the por-
tion of the chart from S to E, but that in order to
succeed it must still satisfy all the needs listed. Each
need specifies a sequence of categories csi that must
be found contiguously to occupy the portion of the
chart extending from si to ei .
Now that the format of the edges is defined, we
can be precise about the parsing rules used. Our
modified chart parsing rules are shown in Figure 2.
The modified top-down rule allows us to refine a
need into a more precise one, using a rule of the
grammar (the extra conditions on the rule prevent
further refinement where a phrase of a given category
has already been found within the precise part of the
chart being considered). The modified fundamental
rule allows a need to be satisfied by an edge that is
completely satisfied (ie. an inactive edge, in the stan-
dard terminology). A new rule, the simplification
rule, is now required to do the relevant housekeeping
when one of an edge&apos;s needs has been completely
satisfied. One way that these rules could run would
be as follows. The chart starts off with the inactive
edges left by bottom-up parsing, together with a sin-
gle &amp;quot;seed&amp;quot; edge for the top-down phase &lt;GOAL from
0 to n needs (S] from 0 to n&gt;, where n is the final
position in the chart. At any point the fundamental
rule is run as much as possible. When we can
proceed no further, the first need is refined by the
top-down rule (hopefully search now being
anchored). The fundamental rule may well again
apply, taking account of smaller phrases that have
already been found. When this has run, the top-down
rule may then further refine the system&apos;s expectations
about the parts of the phrase that cannot be found.
And so on. This is just the kind of &amp;quot;focusing&amp;quot; that we
discussed in the last section.. If an edge expresses
needs in several separate places, the first will eventu-
ally ger resolved, the simplification rule will then
apply and the rest of the needs will then be worked
on.
For this all to make sense, we must assume that
all hypothesised needs can eventually be resolved
(otherwise the rules do not suffice for more than one
error to be narrowed down). We can ensure this by
introducing special rules for recognising the most
primitive kinds of errors. The results of these rules
must obviously be scored in some way, so that errors
are not wildly hypothesised in all sorts of places.
</bodyText>
<table confidence="0.984585740740741">
1 0 4
Top-down rule:
&lt;C from S to E needs rci...csi] from si to el, cs2 from sz to e2, cs. from:, to e,&gt;
ClRHS (in the grammar)
&lt;cifromsj toe needs RHS from si to e &gt;
where e = if CSI is not empty ore = * then * else e
(ei = * or csi is non-empty or there is no category Ci from Sito el)
Fundamental rule:
&lt;C from S to E needs [...csii ci ...cs12] from s to e 1, cs2
&lt;cl from Si to Ei needs &lt;nothing»
&lt;C from S to E needs esti fromsi to S 1, c:i2fromEi to el, c52...&gt;
(Si 5Si, ei =*orEiSei)
Simplification rule:
&lt;C from S to E needs from s to s, cs2 from s2 to e2, cs. from s. to e.&gt;
&lt;C from S to E needs cs2 from s2 to e2, cs. from:, to e,&gt;
Garbage rule:
&lt;C from S to E needs[]fromsi to el, cs2 from s2 to e2, cs. from:, to e.&gt;
&lt;C from S to E needs cs2 from s2 to e2, cs. from s. to e,&gt;
(s e)
Empty category rule:
&lt;C from S to E needs [c 1...csi] from s to s, cs2 from s2 to e2, cs. from to e,&gt;
&lt;C from S to E needs cs2 from s2 to e2, cs, from:, to en&gt;
Unknown word rule:
&lt;C from S to E needs [ci...csi] from si to el, cs2 from s2 to e2, cs. from:, to e.&gt;
&lt;C from S to E needs csi from :1+1 to e 1, cs2 from s2 to e2, cs. from s, to e.&gt;
l a lexical category, si &lt; the end of the chart and
the word at s I not of category c1).
</table>
<figureCaption confidence="0.785093">
Figure 2: Generalised Top-down Parsing Rules
</figureCaption>
<sectionHeader confidence="0.9833745" genericHeader="method">
SEARCH CONTROL AND EVALUATION
FUNCTIONS
</sectionHeader>
<bodyText confidence="0.999619208333334">
Even without the extra rules for recognising
primitive errors, we have now introduced a large
parsing search space. For instance, the new funda-
mental rule means that top-down processing can take
place in many different pans of the chart. Chart
parsers already use the notion of an agenda, in which
possible additions to the chart are given priority, and
so we have sought to make use of this in organising a
heuristic search for the &amp;quot;best&amp;quot; possible parse. We
have considered a number of parameters for deciding
which edges should have priority:
MDE (mode of formation) We prefer edges
that arise from the fundamental rule to those that
arise from the top-down rule; we disprefer edges that
arise from unanchored applications of the top-down
rule.
PSF (penalty so far) Edges resulting from the
garbage, empty category and unknown word rules are
given penalty scores. PSF counts the penalties that
have been accumulated so far in an edge.
PB (best penalty) This is an estimate of the
best possible penalty that this edge, when complete,
could have. This score can use the PSF, together with
information about the parts of the chart covered - for
</bodyText>
<page confidence="0.998498">
105
</page>
<bodyText confidence="0.988762583333333">
instance, the number of words in these parts which
do not have lexical entries.
GUS (the maximum number of words that have
been used so far in a partial parse using this edge)
We prefer edges that lead to parses accounting for
more words of the input.
PBG (the best possible penalty for any com-
plete hypothesis involving this edge). This is a short-
fall score in the sense of Woods (1982).
UBG (the best possible number of words that
could be used in any complete hypothesis containing
this edge).
In our implementation, each rule calculates
each of these scores for the new edge from those of
the contributing edges. We have experimented with a
number of ways of using these scores in comparing
two possible edges to be added to the chart. At
present, the most promising approach seems to be to
compare in turn the scores for PBG, MDE, UBG,
GUS, PSF and PB. As soon as a difference in scores
is encountered, the edge that wins on this account is
chosen as the preferred one. Putting PBG first in this
sequence ensures that the first solution found will be
a solution with a minimal penalty score.
</bodyText>
<listItem confidence="0.391206">
•
</listItem>
<bodyText confidence="0.999974104166667">
The rules for computing scores need to make
estimates about the possible penalty scores that might
arise from attempting to find given types of phrases
in given parts of the chart. We use a number of
heuristics to compute these. For instance, the pres-
ence of a word not appearing in the lexicon means
that every parse covering that word must have a
non-zero penalty score. In general, an attempt to find
an instance of a given category in a given portion of
the chart must produce a penalty score if the bottom-
up parsing phase has not yielded an inactive edge of
the correct kind within that portion. Finally, the fact
that the grammar is assumed to have no e-
productions means that an attempt to find a long
sequence of categories in a short piece of chart is
doomed to produce a penalty score; similarly a
sequence of lexical categories cannot be found
without penalty in a portion of chart that is too long.
Some of the above scoring parameters score an
edge according what sorts of parses it could contri-
bute to, not just according to how internally plausible
it seems. This is desirable, as we wish the construc-
tion of globally most plausible solutions to drive the
parsing. On the other hand, it introduces a number of
problems for chart organisation. As the same edge
(apart from its score) may be generated in different
ways, we may end up with multiple possible scores
for it. It would make sense at each point to consider
the best of the possible scores associated with an
edge to be the current score. In this way we would
not have to repeat work for every differently scored
version of an edge. But consider the following
scenario:
Edge A is added to the chart. Later edge B
is spawned using A and is placed in the
agenda. Subsequently A&apos;s score increases
because it is derived in a new and better
way. This should affect B&apos;s score (and
hence B&apos;s position on the agenda).
If the score of an edge increases then the scores of
edges on the agenda which were spawned from it
should also increase. To cope with this sort of prob-
lem, we need some sort of dependency analysis, a
mechanism for the propagation of changes and an
easily resorted agenda. We have not addressed these
problems so far - our current implementation treats
the score as an integral part of an edge and suffers
from the resulting duplication problem.
</bodyText>
<sectionHeader confidence="0.995268" genericHeader="method">
PRELIMINARY EXPERIMENTS
</sectionHeader>
<bodyText confidence="0.999908928571428">
To see whether the ideas of this paper make
sense in practice, we have performed some very prel-
iminary experiments, with an inefficient implementa-
tion of the chart parser and a small CF-PSG (84 rules
and 34 word lexicon, 18 of whose entries indicate
category ambiguity) for a fragment of English. We
generated random sentences (30 of each length con-
sidered) from the grammar and then introduced ran-
dom occurrences of specific types of errors into these
sentences. The errors considered were none (i.e. leav-
ing the correct sentence as it was), deleting a word,
adding a word (either a completely unknown word or
a word with an entry in the lexicon) and substituting
a completely unknown word for one word of the sen-
tence. For each length of original sentence, the
results were averaged over the 30 sentences ran-
domly generated. We collected the following statis-
tics (see Table 1 for the results):
BU cycles - the number of cycles taken (see
below) to exhaust the chart in the initial (standard)
bottom-up parsing phase.
#Solns - the number of different &amp;quot;solutions&amp;quot;
found. A &amp;quot;solution&amp;quot; was deemed to be a description
of a possible set of errors which has a minimal
penalty score and if corrected would enable a com-
plete parse to be constructed. Possible errors were
adding an extra word, deleting a word and substitut-
ing a word for an instance of a given lexical category.
</bodyText>
<page confidence="0.998942">
106
</page>
<tableCaption confidence="0.999692">
Table 1: Preliminary experimental results
</tableCaption>
<table confidence="0.998986476190476">
Error Length of original BU cycles #Solns First Last TD cycles
None 3 31 1 0 0 0
6 69 1 0 0 0
9 135 1 0 0 0
12 198 1 0 0 0
Delete one word 3 17 5 14 39 50
6 50 5 18 73 114
9 105 6 27 137 350
12 155 7 33 315 1002
Add unknown word 3 29 1 9 17 65
6 60 2 24 36 135
9 105 2 39 83 526
12 156 3 132 289 1922
Add known word 3 37 3 29 51 88
6 72 3 43 88 216
9 137 3 58 124 568
12 170 5 99 325 1775
Subst unknown word 3 17 2 17 28 46
6 49 2 23 35 105
9 96 2 38 56 300
12 150 3 42 109 1162
</table>
<bodyText confidence="0.996765435897436">
The penalty associated with a given set of errors was
the number of errors in the set.
First - the number of cycles of generalised
top-down parsing required to find the first solution.
Last - the number of cycles of generalised top-
down parsing required to find the last solution.
TD cycles - the number of cycles of generalised
top-down parsing required to exhaust all possibilities
of sets of errors with the same penalty as the first
solution found.
It was important to have an implementation-
independent measure of the amount of work done by
the parser, and for this we used the concept of a
&amp;quot;cycle&amp;quot; of the chart parser. A &amp;quot;cycle&amp;quot; in this context
represents the activity of the parser in removing one
item from the agenda, adding the relevant edge to the
chart and adding to the agenda any new edges that
are suggested by the rules as a result of the new addi-
tion. For instance, in conventional top-down chart
parsing a cycle might consist of removing the edge
4 from 0 to 6 needs [NP VP] from 0 to 6&gt; from the
front of the agenda, adding this to the chart and then
adding new edges to the agenda, as follows. Fust of
all, for each edge of the form &lt;NP from 0 to a needs
[I&gt; in the chart the fundamental rule determines that
4 from 0 to 6 needs [VP] from a to 6&gt; should be
added. Secondly, for each rule NP yin the gram-
mar the top-down rule determines that &lt;NP from 0 to
* needs y from Oto *&gt; should be added. With gen-
eralised top-down parsing, there are more rules to be
considered, but the idea is the same. Actually, for the
top-down rule our implementation schedules a whole
collection of single additions (&amp;quot;apply the top down
rule to edge a&amp;quot;) as a single item on the agenda. When
such a request reaches the front of the queue, the
actual new edges are then computed and themselves
added to the agenda. The result of this strategy is to
make the agenda smaller but more structured, at the
cost of some extra cycles.
</bodyText>
<sectionHeader confidence="0.99307" genericHeader="conclusions">
EVALUATION AND FUTURE WORK
</sectionHeader>
<bodyText confidence="0.99876825">
The preliminary results show that, for small
sentences and only one error, enumerating all the
possible minimum-penalty errors takes no worse than
10 times as long as parsing the correct sentences.
Finding the first minimal-penalty error can also be
quite fast. There is, however, a great variability
between the types of error. Errors involving com-
pletely unknown words can be diagnosed reasonably
</bodyText>
<page confidence="0.99636">
107
</page>
<bodyText confidence="0.99998104">
quickly because the presence of an unknown word
allows the estimation of penalty scores to be quite
accurate (the system still has to work out whether the
word can be an addition and for what categories it
can substitute for an instance of, however). We have
not yet considered multiple errors in a sentence, and
we can expect the behaviour to worsten dramatically
as the number of errors increases. Although Table 1
does not show this, there is also a great deal of varia-
bility between sentences of the same length with the
same kind of introduced error. It is noticeable that
errors towards the end of a sentence are harder to
diagnose than those at the start. This reflects the left-
right orientation of the parsing rules - an attempt to
find phrases starting to the right of an error will have
a PBG score at least one more than the estimated PB,
whereas an attempt to find phrases in an open-ended
portion of the chart starting before an error may have
a PBG score the same as the PB (as the error may
occur within the phrases to be found). Thus more
parsing attempts will be relegated to the lower parts
of the agenda in the first case than in the second.
One disturbing fact about the statistics is that
the number of minimal-penalty solutions may be
quite large. For instance, the ill-formed sentence:
</bodyText>
<subsectionHeader confidence="0.613888">
who has John seen on that had
</subsectionHeader>
<bodyText confidence="0.9999885">
was formed by adding the extra word &amp;quot;had&amp;quot; to the
sentence &amp;quot;who has John seen on that&amp;quot;. Our parser
found three other possible single errors to account for
the sentence. The word &amp;quot;on&amp;quot; could have been an
added word, the word &amp;quot;on&amp;quot; could have been a substi-
tution for a complementiser and there could have
been a missing NP after &amp;quot;on&amp;quot;. This large number of
solutions could be an artefact of our particular gram-
mar and lexicon; certainly it is unclear how one
should choose between possible solutions in a
grammar-independent way. In a few cases, the intro-
duction of a random error actually produced a gram-
matical sentence - this occurred, for instance, twice
with sentences of length 5 given one random added
word.
At this stage, we cannot claim that our experi-
ments have done anything more than indicate a cer-
tain concreteness to the ideas and point to a number
of unresolved problems. It remains to be seen how
the performance will scale up for a realistic grammar
and parser. There are a number of detailed issues to
resolve before a really practical implementation of
the above ideas can be produced. The indexing stra-
tegy of the chart needs to be altered to take into
account the new parsing rules, and remaining prob-
lems of duplication of effort need to be addressed.
For instance, the generalised version of the funda-
mental rule allows an active edge to combine with a
set of inactive edges satisfying its needs in any order.
The scoring of errors is another area which
should be better investigated. Where extra words are
introduced accidentally into a text, in practice they
are perhaps unlikely to be words that are already in
the lexicon. Thus when we gave our system sen-
tences with known words added, this may not have
been a fair test. Perhaps the scoring system should
prefer added words to be words outside the lexicon,
substituted words to substitute for words in open
categories, deleted words to be non-content words,
and so on. Perhaps also the confidence of the system
about possible substitutions could take into account
whether a standard spelling corrector can rewrite the
actual word to a known word of the hypothesised
category. A more sophisticated error scoring strategy
could improve the system&apos;s behaviour considerably
for real examples (it might of course make less
difference for random examples like the ones in our
experiments).
Finally the behaviour of the approach with
realistic grammars written in more expressive nota-
tions needs to be established. At present, we are
investigating whether any of the current ideas can be
used in conjunction with Allport&apos;s (1988) &amp;quot;interest-
ing corner&amp;quot; parser.
</bodyText>
<sectionHeader confidence="0.996812" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.999895">
This work was done in conjunction with the
SERC-supported project GR/D/16130. I am
currently supported by an SERC Advanced Fellow-
ship.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.995440333333334">
Aho, Alfred V. and Ullman, Jeffrey D. 1977 Princi-
ples of Compiler Design. Addison-Wesley.
Allport, David. 1988 The &apos;TICC: Parsing Interesting
Text. In: Proceedings of the Second ACL
Conference on Applied Natural Language
Processing, Austin, Texas.
Anderson, S. 0. and Backhouse, Roland C. 1981
Locally Least-Cost Error-Recovery in
Earley&apos;s Algorithm. ACM TOPLAS 3(3):
318-347.
Carbonell, Jaime G. and Hayes, Philip J. 1983
Recovery Strategies for Parsing
</reference>
<page confidence="0.987216">
108
</page>
<reference confidence="0.999688939393939">
Extragrammatical Language. AJCL 9(3-4):
123-146.
Gazdar, Gerald and Mellish, Chris. 1989 Natural
Language Processing in MP - An Intro-
duction to Computational Linguistics.
Addison-Wesley.
Jensen, Karen, Heidom, George E., Miller, Lance A.
and Ravin, Yael. 1983 Parse Fitting and
Prose Fitting: Getting a Hold on 111-
Formedness. AJCL 9(3-4): 147-160.
Kay, Martin. 1980 Algorithm Schemata and Data
Structures in Syntactic Processing.
Research Report CSL-80-12, Xerox PARC.
Pereira, Fernando C. N. and Warren, David IL D.
1980 Definite Clause Grammars for
Language Analysis - A Survey of the For-
malism and a Comparison with Augmented
Transition Networks. Artificial Intelli-
gence 13(3): 231-278.
Shieber, Stuart M. 1984 The Design of a Computer
Language for Linguistic Information. In
Proceedings of COLING-84, 362-366.
Steedman, Mark. 1987 Combinatory Grammars and
Human Language Processing. In: Garfield,
J., Ed., Modularity in Knowledge
Representation and Natural Language Pro-
cessing. Bradford Books/MIT Press.
Weischedel, Ralph M. and Sondheimer, Norman K.
1983 Meta-rules as a Basis for Processing
Ill-Formed Input. AJCL 9(3-4): 161-177.
Woods, William A. 1982 Optimal Search Strategies
for Speech Understanding Control.
Artificial Intelligence 18(3): 295-326.
</reference>
<page confidence="0.99896">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9991075">SOME CHART-BASED TECHNIQUES FOR PARSING ILL-FORMED INPUT</title>
<author confidence="0.999997">Chris S Mellish</author>
<affiliation confidence="0.9998725">Department of Artificial Intelligence, University of Edinburgh,</affiliation>
<address confidence="0.869149666666667">80 South Bridge, EDINBURGH EH1 1HN, Scotland.</address>
<abstract confidence="0.998909475">We argue for the usefulness of an active chart as the basis of a system that searches for the globally most plausible explanation of failure to syntactically parse a given input. We suggest semantics-free, grammarindependent techniques for parsing inputs displaying simple kinds of ill-formedness and discuss the search issues involved. THE PROBLEM Although the ultimate solution to the problem of processing ill-formed input must take into account semantic and pragmatic factors, nevertheless it is important to understand the limits of recovery strategies that are based entirely on syntax and which are independent of any particular grammar. The aim of this work is therefore to explore purely syntactic and grammar-independent techniques to enable a parser to recover from simple kinds of ill-formedness in textual inputs. Accordingly, we present a generalised parsing strategy based on an active chart which is capable of diagnosing simple errors (unknown/misspelled words, omitted words, extra noise words) in sentences (from languages described by context free phrase structure grammars without eproductions). This strategy has the advantage that the recovery process can run after a standard (active chart) parser has terminated unsuccessfully, without causing existing work to be repeated or the original parser to be slowed down in any way, and that, unlike previous systems, it allows the full syntactic context to be exploited in the determination of a &amp;quot;best&amp;quot; parse for an ill-formed sentence. EXPLOITING SYNTACTIC CONTEXT Weischedel and Sondheimer (1983) present an approach to processing ill-formed input based on a modified AIN parser. The basic idea is, when an initial parse fails, to select the incomplete parsing path that consumes the longest initial portion of the input, apply a special rule to allow the blocked parse to continue, and then to iterate this process until a successful parse is generated. The result is a &amp;quot;hillclimbing&amp;quot; search for the &amp;quot;best&amp;quot; parse, relying at each point on the &amp;quot;longest path&amp;quot; heuristic. Unfortunately, sometimes this heuristic will yield several possible parses, for instance with the sentence: snow blocks road (no partial parse getting past the point shown) where the parser can fail expecting either a verb or a determiner. Moreover, sometimes the heuristic will cause the most &amp;quot;obvious&amp;quot; error to be missed: said that the snow the road paper will best news is the Times where we might suspect that there is a missing verb and a misspelled &amp;quot;with&amp;quot; respectively. In all these cases, the &amp;quot;longest path&amp;quot; heuristic fails to indicate unambiguously the minimal change that would be necessary to make the whole input acceptable as a sentence. This is not surprising, as the left-right bias of an ATN parser allows the system to take no account of the right context of a possible problem element. Weischedel and Sondheimer&apos;s use of the &amp;quot;longest path&amp;quot; heuristic is similar to the use of locally least-cost error recovery in Anderson and Backhouse&apos;s (1981) scheme for compilers. It seems to be generally accepted that any form of globally &amp;quot;minimum-distance&amp;quot; error correction will be too costly to implement (Aho and Ullman, 1977). Such work has, however, not considered heuristic approaches, such as the one we are developing. Mother feature of Weischedel and Sondheimer&apos;s system is the use of grammar-specific recovery rules (&amp;quot;meta-rules&amp;quot; in their terminology). The same is true of many other systems for dealing with ill-formed input (e.g. Carbonell and Hayes (1983), Jensen et al. (1983)). Although grammarspecific recovery rules are likely in the end always to be more powerful than grammar-independent rules, it does seem to be worth investigating how far one can with rules that only depend on the grammar for-</abstract>
<note confidence="0.8164185">102 the 2 3 4 5 6 7 &lt;Need S from 0 to 7&gt; &lt;Need NP+VP from 0 to 7&gt; &lt;Need VP from 2 to 7&gt; &lt;Need VP+PP from 2 to 7&gt; &lt;Need PP from 4 to 7&gt; &lt;Need P+NP from 4 to 7&gt; &lt;Need P from 4 to 5&gt;</note>
<abstract confidence="0.976059770000001">(hypothesis) (by top-down rule) (by fundamental rule with NP found bottom-up) (by top-down rule) (by fundamental rule with VP found bottom-up) (by top-down rule) (by fundamental rule with NP found bottom-up) Figure I: Focusing on an error. In adapting an AIN parser to compare partial parses, Weischedel and Sondheimer have already introduced machinery to represent several alternative parses simultaneously. From it is a relastep to introduce a well-formed substring table, or even an active chart, which allows for a global assessment of the state of the parser. If the grammar formalism is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), Patr-II (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the &amp;quot;best&amp;quot; parse WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &amp;quot;post mortem&amp;quot; obviously depends on the parsing strategy used (Kay Gazdar and Mellish 1989). edges particularly important from the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have occurred. So we might expect to combine violated expectations with found constituents to hypothesise complete parses. For simplicity, we assume here that the grammar is a simple CF-PSG, although there are obvious generalisations. parsing guaranteed to create active edges for each kind of phrase that could continue a partial parse starting at the beginning of input On the other hand, parsing which we mean left corner parsing without top-down filtering) is guaranteed to find all complete constituents of every possible parse. In addition, whenever a non-empty initial segment of a rule RHS has been the parser will create for the kind of phrase predicted to occur after this segment. Topdown parsing will always create an edge for a phrase is a parse, and so it will always indicate by the presence of an unsatisfied active edge the first error point, if there is one. If a subsequent error is present, top-down parsing will not always create an active edge corresponding to it, because the second may occur within a constituent that will not be predicted until the first error is corrected. Similarly, right-to-left top-down parsing will always indicate the last error point, and a combination of the two will find the first and last, but not necessarily any error points in between. On the other hand, bottomup parsing will only create an active edge for each error point that comes immediately after a sequence of phrases corresponding to an initial segment of the RHS of a grammar rule. Moreover, it will not necessarily refine its predictions to the most detailed level (e.g. having found an NP, it may predict the existence of a following VP, but not the existence of of that start a VP). Weischedel and Sondheimer&apos;s approach can be seen as an incremental top-down parsing, where at each stage the rightmost unsatisfied active edge is artificially allowed to be satisfied in some way. As we have seen, there is no guarantee that this sort of hill-climbing will find the &amp;quot;best&amp;quot; solution for multiple errors, or even for single errors. How can we combine bottom-up and top-down parsing for a more effective solution? FOCUSING ON AN ERROR Our basic strategy is to run a bottom-up parser over the input and then, if this fails to find a complete parse, to run a modified top-down parser over the resulting chart to hypothesise possible complete parses. The modified top-down parser attempts to find the minimal errors that, when taken account of, enable a complete parse to be constructed. Imagine that a bottom-up parser has already run over the input &amp;quot;the gardener collects manure if the autumn&amp;quot;. Then Figure 1 shows (informally) how a top-down parser might focus on a possible error. To implement this kind of reasoning, we need a top-down parsing rule knows how to refine a set of and a 103 fundamental rule that is able to incorporate found constituents from either direction. When we may encounter multiple errors, however, we need to express multiple needs (e.g. &lt;Need N from 3 to 4 and PP from 8 to 10&gt;). We also need to have a fundamental rule that can absorb found phrases from anywhere in a relevant portion of the chart (e.g. given a rule &amp;quot;NP —&gt; Det Adj N&amp;quot; and a sequence &amp;quot;as marvellous sihgt&amp;quot;, we need to be able to hypothesise that &amp;quot;as&amp;quot; should be a Det and &amp;quot;sihgt&amp;quot; a N). To save repeating work, we need a version of the top-down rule that stops when it reaches an appropriate category that has already been found bottom-up. Finally, we need to handle both &amp;quot;anchored&amp;quot; and &amp;quot;unanchored&amp;quot; needs. In an anchored need (e.g. &lt;Need NP from 0 to 4&gt;) we know the beginning and end of the portion of the chart within which the search is to take place. In looking for a NP VP sequence in &amp;quot;the happy blageon smupled the bait&amp;quot;, however, we can&apos;t initially find a complete (initial) NP or (final) VP and hence don&apos;t know where in the chart these phrases meet. We express this by &lt;Need NP from 0 to *, VP from * to 6&gt;, the symbol &amp;quot;*&amp;quot; denoting a position in the chart that remains to be determined. GENERALISED TOP-DOWN PARSING If we adopt a chart parsing strategy with only edges that carry information about global needs, there will be considerable duplicated effort. For instance, the further refinement of the two edges: &lt;Need NP from 0 to 3 and V from 9 to 10&gt; &lt;Need NP from 0 to 3 and Adj from 10 to 11&gt; can lead to any analysis of possible NPs between 0 and 3 being done twice. Restricting the possible format of edges in this way would be similar to allowing the &amp;quot;functional composition rule&amp;quot; (Steedman 1987) in standard chart parsing, and in general this is not done for efficiency reasons. Instead, we need to produce a single edge that is &amp;quot;in charge&amp;quot; of the computation looking for NPs between 0 and 3. When possible NPs are then found, these then need to be combined with the original edges by an appropriate form of the fundamental rule. We are thus led to the following form a generalised edge our chart parser: &lt;C from S to E needs si to el, c32 from s2 to e2, sa to ea&gt; C is a category, the lists of categories (which we will show inside square brackets), S. E, the positions in the chart (or the special symbol &amp;quot;*&amp;quot;). The presence of an edge of this kind in the chart indicates that the parser is attempting to find a phrase of category C covering the portion of the chart from S to E, but that in order to it must still satisfy needs listed. Each specifies a sequence of categories must be found contiguously to occupy the portion of the extending from Now that the format of the edges is defined, we can be precise about the parsing rules used. Our modified chart parsing rules are shown in Figure 2. modified rule us to refine a need into a more precise one, using a rule of the grammar (the extra conditions on the rule prevent further refinement where a phrase of a given category has already been found within the precise part of the being considered). The modified a need to be satisfied by an edge that is completely satisfied (ie. an inactive edge, in the stanterminology). A new rule, the now required to do the relevant housekeeping when one of an edge&apos;s needs has been completely satisfied. One way that these rules could run would be as follows. The chart starts off with the inactive edges left by bottom-up parsing, together with a single &amp;quot;seed&amp;quot; edge for the top-down phase &lt;GOAL from to (S] from 0 to the final position in the chart. At any point the fundamental rule is run as much as possible. When we can proceed no further, the first need is refined by the top-down rule (hopefully search now being anchored). The fundamental rule may well again apply, taking account of smaller phrases that have already been found. When this has run, the top-down rule may then further refine the system&apos;s expectations about the parts of the phrase that cannot be found. And so on. This is just the kind of &amp;quot;focusing&amp;quot; that we discussed in the last section.. If an edge expresses needs in several separate places, the first will eventually ger resolved, the simplification rule will then apply and the rest of the needs will then be worked on. For this all to make sense, we must assume that all hypothesised needs can eventually be resolved (otherwise the rules do not suffice for more than one error to be narrowed down). We can ensure this by introducing special rules for recognising the most primitive kinds of errors. The results of these rules must obviously be scored in some way, so that errors are not wildly hypothesised in all sorts of places. 1 0 4 Top-down rule: si to el, cs2 from sz to e2, to (in the grammar) toe RHS from si to &gt; if not empty * then * else e * or non-empty or there is no category Sito el) Fundamental rule: from S &lt;cl from Si to Ei needs &lt;nothing» from esti el,</abstract>
<note confidence="0.986879">(Si 5Si, ei =*orEiSei) Simplification rule: from S from s to s, s2 to e2, s2 to e2, e,&gt; Garbage rule: to el, s2 to e2, to s2 to e2, e,&gt; (s e) Empty category rule: 1...csi] s to s, s2 to e2, e,&gt; s2 to e2, to Unknown word rule: [ci...csi] from si to el, s2 to e2, e.&gt; S to :1+1 to s2 to e2, s, to e.&gt;</note>
<abstract confidence="0.953366132352941">l a lexical category, si &lt; the end of the chart and word at s I not of category Figure 2: Generalised Top-down Parsing Rules SEARCH CONTROL AND EVALUATION FUNCTIONS Even without the extra rules for recognising primitive errors, we have now introduced a large parsing search space. For instance, the new fundamental rule means that top-down processing can take place in many different pans of the chart. Chart already use the notion of an which possible additions to the chart are given priority, and we have sought to make this in organising a heuristic search for the &amp;quot;best&amp;quot; possible parse. We have considered a number of parameters for deciding which edges should have priority: of formation) We prefer edges that arise from the fundamental rule to those that arise from the top-down rule; we disprefer edges that arise from unanchored applications of the top-down rule. so far) Edges resulting from the garbage, empty category and unknown word rules are given penalty scores. PSF counts the penalties that have been accumulated so far in an edge. penalty) This is an estimate of the best possible penalty that this edge, when complete, have. This score can use the with information about the parts of the chart covered for 105 instance, the number of words in these parts which do not have lexical entries. maximum number of words that have been used so far in a partial parse using this edge) We prefer edges that lead to parses accounting for more words of the input. best possible penalty for any comhypothesis involving this edge). This is a shortin the sense of Woods (1982). best possible number of words that could be used in any complete hypothesis containing this edge). In our implementation, each rule calculates each of these scores for the new edge from those of the contributing edges. We have experimented with a number of ways of using these scores in comparing two possible edges to be added to the chart. At present, the most promising approach seems to be to compare in turn the scores for PBG, MDE, UBG, GUS, PSF and PB. As soon as a difference in scores is encountered, the edge that wins on this account is chosen as the preferred one. Putting PBG first in this sequence ensures that the first solution found will be a solution with a minimal penalty score. • The rules for computing scores need to make estimates about the possible penalty scores that might arise from attempting to find given types of phrases in given parts of the chart. We use a number of heuristics to compute these. For instance, the presence of a word not appearing in the lexicon means that every parse covering that word must have a non-zero penalty score. In general, an attempt to find an instance of a given category in a given portion of the chart must produce a penalty score if the bottomup parsing phase has not yielded an inactive edge of the correct kind within that portion. Finally, the fact that the grammar is assumed to have no eproductions means that an attempt to find a long sequence of categories in a short piece of chart is doomed to produce a penalty score; similarly a sequence of lexical categories cannot be found without penalty in a portion of chart that is too long. Some of the above scoring parameters score an edge according what sorts of parses it could contribute to, not just according to how internally plausible it seems. This is desirable, as we wish the construction of globally most plausible solutions to drive the parsing. On the other hand, it introduces a number of problems for chart organisation. As the same edge (apart from its score) may be generated in different ways, we may end up with multiple possible scores for it. It would make sense at each point to consider the best of the possible scores associated with an edge to be the current score. In this way we would not have to repeat work for every differently scored version of an edge. But consider the following scenario: Edge A is added to the chart. Later edge B is spawned using A and is placed in the agenda. Subsequently A&apos;s score increases because it is derived in a new and better way. This should affect B&apos;s score (and hence B&apos;s position on the agenda). If the score of an edge increases then the scores of edges on the agenda which were spawned from it should also increase. To cope with this sort of problem, we need some sort of dependency analysis, a mechanism for the propagation of changes and an easily resorted agenda. We have not addressed these problems so far our current implementation treats the score as an integral part of an edge and suffers from the resulting duplication problem. PRELIMINARY EXPERIMENTS To see whether the ideas of this paper make sense in practice, we have performed some very preliminary experiments, with an inefficient implementation of the chart parser and a small CF-PSG (84 rules and 34 word lexicon, 18 of whose entries indicate category ambiguity) for a fragment of English. We generated random sentences (30 of each length considered) from the grammar and then introduced random occurrences of specific types of errors into these sentences. The errors considered were none (i.e. leaving the correct sentence as it was), deleting a word, adding a word (either a completely unknown word or a word with an entry in the lexicon) and substituting a completely unknown word for one word of the sentence. For each length of original sentence, the results were averaged over the 30 sentences randomly generated. We collected the following statistics (see Table 1 for the results): cycles the of cycles taken (see below) to exhaust the chart in the initial (standard) bottom-up parsing phase. number of different &amp;quot;solutions&amp;quot; found. A &amp;quot;solution&amp;quot; was deemed to be a description of a possible set of errors which has a minimal penalty score and if corrected would enable a complete parse to be constructed. Possible errors were adding an extra word, deleting a word and substituting a word for an instance of a given lexical category. 106 Table 1: Preliminary experimental results Error Length of original BU cycles #Solns First Last TD cycles None 3 31 1 0 0 0</abstract>
<phone confidence="0.678841894736842">6 69 1 0 0 0 9 135 1 0 0 0 12 198 1 0 0 0 Delete one word 3 17 5 14 39 50 6 50 5 18 73 114 9 105 6 27 137 350 12 155 7 33 315 1002 Add unknown word 3 29 1 9 17 65 6 60 2 24 36 135 9 105 2 39 83 526 12 156 3 132 289 1922 Add known word 3 37 3 29 51 88 6 72 3 43 88 216 9 137 3 58 124 568 12 170 5 99 325 1775 Subst unknown word 3 17 2 17 28 46 6 49 2 23 35 105 9 96 2 38 56 300 12 150 3 42 109 1162</phone>
<abstract confidence="0.995869615384615">The penalty associated with a given set of errors was the number of errors in the set. number of cycles of generalised top-down parsing required to find the first solution. number of cycles of generalised topdown parsing required to find the last solution. cycles number of cycles of generalised top-down parsing required to exhaust all possibilities of sets of errors with the same penalty as the first solution found. It was important to have an implementationindependent measure of the amount of work done by the parser, and for this we used the concept of a &amp;quot;cycle&amp;quot; of the chart parser. A &amp;quot;cycle&amp;quot; in this context represents the activity of the parser in removing one item from the agenda, adding the relevant edge to the chart and adding to the agenda any new edges that are suggested by the rules as a result of the new addition. For instance, in conventional top-down chart parsing a cycle might consist of removing the edge 4 from 0 to 6 needs [NP VP] from 0 to 6&gt; from the front of the agenda, adding this to the chart and then adding new edges to the agenda, as follows. Fust of all, for each edge of the form &lt;NP from 0 to a needs [I&gt; in the chart the fundamental rule determines that 4 from 0 to 6 needs [VP] from a to 6&gt; should be Secondly, for each rule the grammar the top-down rule determines that &lt;NP from 0 to * needs y from Oto *&gt; should be added. With generalised top-down parsing, there are more rules to be considered, but the idea is the same. Actually, for the top-down rule our implementation schedules a whole collection of single additions (&amp;quot;apply the top down to edge a single item on the agenda. When such a request reaches the front of the queue, the new edges are then computed and added to the agenda. The result of this strategy is to make the agenda smaller but more structured, at the cost of some extra cycles. EVALUATION AND FUTURE WORK The preliminary results show that, for small sentences and only one error, enumerating all the possible minimum-penalty errors takes no worse than 10 times as long as parsing the correct sentences. Finding the first minimal-penalty error can also be quite fast. There is, however, a great variability between the types of error. Errors involving completely unknown words can be diagnosed reasonably 107 quickly because the presence of an unknown word allows the estimation of penalty scores to be quite accurate (the system still has to work out whether the word can be an addition and for what categories it can substitute for an instance of, however). We have not yet considered multiple errors in a sentence, and we can expect the behaviour to worsten dramatically as the number of errors increases. Although Table 1 does not show this, there is also a great deal of variability between sentences of the same length with the same kind of introduced error. It is noticeable that errors towards the end of a sentence are harder to diagnose than those at the start. This reflects the leftright orientation of the parsing rules an attempt to find phrases starting to the right of an error will have a PBG score at least one more than the estimated PB, whereas an attempt to find phrases in an open-ended portion of the chart starting before an error may have a PBG score the same as the PB (as the error may occur within the phrases to be found). Thus more parsing attempts will be relegated to the lower parts of the agenda in the first case than in the second. One disturbing fact about the statistics is that the number of minimal-penalty solutions may be quite large. For instance, the ill-formed sentence: who has John seen on that had was formed by adding the extra word &amp;quot;had&amp;quot; to the sentence &amp;quot;who has John seen on that&amp;quot;. Our parser found three other possible single errors to account for the sentence. The word &amp;quot;on&amp;quot; could have been an added word, the word &amp;quot;on&amp;quot; could have been a substitution for a complementiser and there could have been a missing NP after &amp;quot;on&amp;quot;. This large number of solutions could be an artefact of our particular grammar and lexicon; certainly it is unclear how one should choose between possible solutions in a grammar-independent way. In a few cases, the introduction of a random error actually produced a grammatical sentence this occurred, for instance, twice with sentences of length 5 given one random added word. At this stage, we cannot claim that our experiments have done anything more than indicate a certain concreteness to the ideas and point to a number of unresolved problems. It remains to be seen how the performance will scale up for a realistic grammar and parser. There are a number of detailed issues to resolve before a really practical implementation of the above ideas can be produced. The indexing strategy of the chart needs to be altered to take into account the new parsing rules, and remaining problems of duplication of effort need to be addressed. For instance, the generalised version of the fundamental rule allows an active edge to combine with a set of inactive edges satisfying its needs in any order. The scoring of errors is another area which should be better investigated. Where extra words are introduced accidentally into a text, in practice they are perhaps unlikely to be words that are already in the lexicon. Thus when we gave our system sentences with known words added, this may not have been a fair test. Perhaps the scoring system should prefer added words to be words outside the lexicon, words to substitute for words in categories, deleted words to be non-content words, and so on. Perhaps also the confidence of the system about possible substitutions could take into account whether a standard spelling corrector can rewrite the actual word to a known word of the hypothesised category. A more sophisticated error scoring strategy could improve the system&apos;s behaviour considerably for real examples (it might of course make less difference for random examples like the ones in our experiments). Finally the behaviour of the approach with realistic grammars written in more expressive notations needs to be established. At present, we are investigating whether any of the current ideas can be used in conjunction with Allport&apos;s (1988) &amp;quot;interesting corner&amp;quot; parser. ACKNOWLEDGEMENTS</abstract>
<note confidence="0.877384230769231">This work was done in conjunction with the SERC-supported project GR/D/16130. I am currently supported by an SERC Advanced Fellowship. REFERENCES Alfred V. and Ullman, Jeffrey D. 1977 Princiof Compiler Design. Allport, David. 1988 The &apos;TICC: Parsing Interesting Text. In: Proceedings of the Second ACL Conference on Applied Natural Language Processing, Austin, Texas. Anderson, S. 0. and Backhouse, Roland C. 1981 Locally Least-Cost Error-Recovery in Algorithm. TOPLAS 318-347. Carbonell, Jaime G. and Hayes, Philip J. 1983 Recovery Strategies for Parsing 108 Language. 123-146. Gerald and Mellish, Chris. 1989 Language Processing in MP - An Introduction to Computational Linguistics. Addison-Wesley. Jensen, Karen, Heidom, George E., Miller, Lance A. and Ravin, Yael. 1983 Parse Fitting and Prose Fitting: Getting a Hold on 111- 147-160. Kay, Martin. 1980 Algorithm Schemata and Data Structures in Syntactic Processing. Research Report CSL-80-12, Xerox PARC. Pereira, Fernando C. N. and Warren, David IL D. 1980 Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Networks. Intelli- 231-278. Shieber, Stuart M. 1984 The Design of a Computer Language for Linguistic Information. In Proceedings of COLING-84, 362-366. Steedman, Mark. 1987 Combinatory Grammars and Human Language Processing. In: Garfield, Ed., in Knowledge Representation and Natural Language Pro- Books/MIT Press. Weischedel, Ralph M. and Sondheimer, Norman K. 1983 Meta-rules as a Basis for Processing Input. 161-177. Woods, William A. 1982 Optimal Search Strategies for Speech Understanding Control. Intelligence 295-326. 109</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1977</date>
<booktitle>Principles of Compiler Design.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="3391" citStr="Aho and Ullman, 1977" startWordPosition="533" endWordPosition="536">h&amp;quot; heuristic fails to indicate unambiguously the minimal change that would be necessary to make the whole input acceptable as a sentence. This is not surprising, as the left-right bias of an ATN parser allows the system to take no account of the right context of a possible problem element. Weischedel and Sondheimer&apos;s use of the &amp;quot;longest path&amp;quot; heuristic is similar to the use of locally least-cost error recovery in Anderson and Backhouse&apos;s (1981) scheme for compilers. It seems to be generally accepted that any form of globally &amp;quot;minimum-distance&amp;quot; error correction will be too costly to implement (Aho and Ullman, 1977). Such work has, however, not considered heuristic approaches, such as the one we are developing. Mother feature of Weischedel and Sondheimer&apos;s system is the use of grammar-specific recovery rules (&amp;quot;meta-rules&amp;quot; in their terminology). The same is true of many other systems for dealing with ill-formed input (e.g. Carbonell and Hayes (1983), Jensen et al. (1983)). Although grammarspecific recovery rules are likely in the end always to be more powerful than grammar-independent rules, it does seem to be worth investigating how far one can get with rules that only depend on the grammar formalism use</context>
</contexts>
<marker>Aho, Ullman, 1977</marker>
<rawString>Aho, Alfred V. and Ullman, Jeffrey D. 1977 Principles of Compiler Design. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Allport</author>
</authors>
<title>The &apos;TICC: Parsing Interesting Text. In:</title>
<date>1988</date>
<booktitle>Proceedings of the Second ACL Conference on Applied Natural Language Processing,</booktitle>
<location>Austin, Texas.</location>
<marker>Allport, 1988</marker>
<rawString>Allport, David. 1988 The &apos;TICC: Parsing Interesting Text. In: Proceedings of the Second ACL Conference on Applied Natural Language Processing, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland C Backhouse</author>
</authors>
<title>Locally Least-Cost Error-Recovery in Earley&apos;s Algorithm.</title>
<date>1981</date>
<journal>ACM TOPLAS</journal>
<volume>3</volume>
<issue>3</issue>
<pages>318--347</pages>
<marker>Backhouse, 1981</marker>
<rawString>Anderson, S. 0. and Backhouse, Roland C. 1981 Locally Least-Cost Error-Recovery in Earley&apos;s Algorithm. ACM TOPLAS 3(3): 318-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Philip J Hayes</author>
</authors>
<title>Recovery Strategies for Parsing Extragrammatical Language.</title>
<date>1983</date>
<journal>AJCL</journal>
<pages>9--3</pages>
<contexts>
<context position="3730" citStr="Carbonell and Hayes (1983)" startWordPosition="584" endWordPosition="587">gest path&amp;quot; heuristic is similar to the use of locally least-cost error recovery in Anderson and Backhouse&apos;s (1981) scheme for compilers. It seems to be generally accepted that any form of globally &amp;quot;minimum-distance&amp;quot; error correction will be too costly to implement (Aho and Ullman, 1977). Such work has, however, not considered heuristic approaches, such as the one we are developing. Mother feature of Weischedel and Sondheimer&apos;s system is the use of grammar-specific recovery rules (&amp;quot;meta-rules&amp;quot; in their terminology). The same is true of many other systems for dealing with ill-formed input (e.g. Carbonell and Hayes (1983), Jensen et al. (1983)). Although grammarspecific recovery rules are likely in the end always to be more powerful than grammar-independent rules, it does seem to be worth investigating how far one can get with rules that only depend on the grammar formalism used. 102 ir the T gardener T collects T manure T if T the T autumn 0 1 2 3 4 5 6 7 &lt;Need S from 0 to 7&gt; &lt;Need NP+VP from 0 to 7&gt; &lt;Need VP from 2 to 7&gt; &lt;Need VP+PP from 2 to 7&gt; &lt;Need PP from 4 to 7&gt; &lt;Need P+NP from 4 to 7&gt; &lt;Need P from 4 to 5&gt; (hypothesis) (by top-down rule) (by fundamental rule with NP found bottom-up) (by top-down rule) (</context>
</contexts>
<marker>Carbonell, Hayes, 1983</marker>
<rawString>Carbonell, Jaime G. and Hayes, Philip J. 1983 Recovery Strategies for Parsing Extragrammatical Language. AJCL 9(3-4): 123-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
<author>Chris Mellish</author>
</authors>
<title>Natural Language Processing in MP - An Introduction to Computational Linguistics.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="5373" citStr="Gazdar and Mellish 1989" startWordPosition="881" endWordPosition="884">ve chart, which allows for a global assessment of the state of the parser. If the grammar formalism is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), Patr-II (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the &amp;quot;best&amp;quot; parse WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &amp;quot;post mortem&amp;quot; obviously depends on the parsing strategy used (Kay 1980, Gazdar and Mellish 1989). Active edges are particularly important from the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have occurred. So we might expect to combine violated expectations with found constituents to hypothesise complete parses. For simplicity, we assume here that the grammar is a simple CF-PSG, although there are obvious generalisations. (Left-right) top-down parsing is guaranteed to create active edges for each kind of phrase that could continue a partial parse starting at the beginning of the input On the other hand, bottom-up parsing (by</context>
</contexts>
<marker>Gazdar, Mellish, 1989</marker>
<rawString>Gazdar, Gerald and Mellish, Chris. 1989 Natural Language Processing in MP - An Introduction to Computational Linguistics. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Jensen</author>
<author>George E Heidom</author>
<author>Lance A Miller</author>
<author>Yael Ravin</author>
</authors>
<title>Parse Fitting and Prose Fitting: Getting a Hold on 111-Formedness.</title>
<date>1983</date>
<journal>AJCL</journal>
<pages>9--3</pages>
<contexts>
<context position="3752" citStr="Jensen et al. (1983)" startWordPosition="588" endWordPosition="591">lar to the use of locally least-cost error recovery in Anderson and Backhouse&apos;s (1981) scheme for compilers. It seems to be generally accepted that any form of globally &amp;quot;minimum-distance&amp;quot; error correction will be too costly to implement (Aho and Ullman, 1977). Such work has, however, not considered heuristic approaches, such as the one we are developing. Mother feature of Weischedel and Sondheimer&apos;s system is the use of grammar-specific recovery rules (&amp;quot;meta-rules&amp;quot; in their terminology). The same is true of many other systems for dealing with ill-formed input (e.g. Carbonell and Hayes (1983), Jensen et al. (1983)). Although grammarspecific recovery rules are likely in the end always to be more powerful than grammar-independent rules, it does seem to be worth investigating how far one can get with rules that only depend on the grammar formalism used. 102 ir the T gardener T collects T manure T if T the T autumn 0 1 2 3 4 5 6 7 &lt;Need S from 0 to 7&gt; &lt;Need NP+VP from 0 to 7&gt; &lt;Need VP from 2 to 7&gt; &lt;Need VP+PP from 2 to 7&gt; &lt;Need PP from 4 to 7&gt; &lt;Need P+NP from 4 to 7&gt; &lt;Need P from 4 to 5&gt; (hypothesis) (by top-down rule) (by fundamental rule with NP found bottom-up) (by top-down rule) (by fundamental rule wi</context>
</contexts>
<marker>Jensen, Heidom, Miller, Ravin, 1983</marker>
<rawString>Jensen, Karen, Heidom, George E., Miller, Lance A. and Ravin, Yael. 1983 Parse Fitting and Prose Fitting: Getting a Hold on 111-Formedness. AJCL 9(3-4): 147-160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1980</date>
<tech>Research Report CSL-80-12, Xerox PARC.</tech>
<contexts>
<context position="5347" citStr="Kay 1980" startWordPosition="879" endWordPosition="880">en an active chart, which allows for a global assessment of the state of the parser. If the grammar formalism is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), Patr-II (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the &amp;quot;best&amp;quot; parse WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &amp;quot;post mortem&amp;quot; obviously depends on the parsing strategy used (Kay 1980, Gazdar and Mellish 1989). Active edges are particularly important from the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have occurred. So we might expect to combine violated expectations with found constituents to hypothesise complete parses. For simplicity, we assume here that the grammar is a simple CF-PSG, although there are obvious generalisations. (Left-right) top-down parsing is guaranteed to create active edges for each kind of phrase that could continue a partial parse starting at the beginning of the input On the other h</context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, Martin. 1980 Algorithm Schemata and Data Structures in Syntactic Processing. Research Report CSL-80-12, Xerox PARC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David IL D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<issue>3</issue>
<pages>231--278</pages>
<contexts>
<context position="4937" citStr="Pereira and Warren 1980" startWordPosition="807" endWordPosition="810"> top-down rule) (by fundamental rule with VP found bottom-up) (by top-down rule) (by fundamental rule with NP found bottom-up) Figure I: Focusing on an error. In adapting an AIN parser to compare partial parses, Weischedel and Sondheimer have already introduced machinery to represent several alternative partial parses simultaneously. From this, it is a relatively small step to introduce a well-formed substring table, or even an active chart, which allows for a global assessment of the state of the parser. If the grammar formalism is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), Patr-II (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the &amp;quot;best&amp;quot; parse WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &amp;quot;post mortem&amp;quot; obviously depends on the parsing strategy used (Kay 1980, Gazdar and Mellish 1989). Active edges are particularly important from the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have </context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, Fernando C. N. and Warren, David IL D. 1980 Definite Clause Grammars for Language Analysis - A Survey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence 13(3): 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>The Design of a Computer Language for Linguistic Information.</title>
<date>1984</date>
<booktitle>In Proceedings of COLING-84,</booktitle>
<pages>362--366</pages>
<contexts>
<context position="4961" citStr="Shieber 1984" startWordPosition="812" endWordPosition="813">e with VP found bottom-up) (by top-down rule) (by fundamental rule with NP found bottom-up) Figure I: Focusing on an error. In adapting an AIN parser to compare partial parses, Weischedel and Sondheimer have already introduced machinery to represent several alternative partial parses simultaneously. From this, it is a relatively small step to introduce a well-formed substring table, or even an active chart, which allows for a global assessment of the state of the parser. If the grammar formalism is also changed to a declarative formalism (e.g. CF-PSGs, DCGs (Pereira and Warren 1980), Patr-II (Shieber 1984)), then there is a possibility of constructing other partial parses that do not start at the beginning of the input. In this way, right context can play a role in the determination of the &amp;quot;best&amp;quot; parse WHAT A CHART PARSER LEAVES BEHIND The information that an active chart parser leaves behind for consideration by a &amp;quot;post mortem&amp;quot; obviously depends on the parsing strategy used (Kay 1980, Gazdar and Mellish 1989). Active edges are particularly important from the point of view of diagnosing errors, as an unsatisfied active edge suggests a place where an input error may have occurred. So we might ex</context>
</contexts>
<marker>Shieber, 1984</marker>
<rawString>Shieber, Stuart M. 1984 The Design of a Computer Language for Linguistic Information. In Proceedings of COLING-84, 362-366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Combinatory Grammars and Human Language Processing. In:</title>
<date>1987</date>
<booktitle>Modularity in Knowledge Representation and Natural Language Processing.</booktitle>
<publisher>Bradford Books/MIT Press.</publisher>
<contexts>
<context position="10169" citStr="Steedman 1987" startWordPosition="1731" endWordPosition="1732"> VP from * to 6&gt;, the symbol &amp;quot;*&amp;quot; denoting a position in the chart that remains to be determined. GENERALISED TOP-DOWN PARSING If we adopt a chart parsing strategy with only edges that carry information about global needs, there will be considerable duplicated effort. For instance, the further refinement of the two edges: &lt;Need NP from 0 to 3 and V from 9 to 10&gt; &lt;Need NP from 0 to 3 and Adj from 10 to 11&gt; can lead to any analysis of possible NPs between 0 and 3 being done twice. Restricting the possible format of edges in this way would be similar to allowing the &amp;quot;functional composition rule&amp;quot; (Steedman 1987) in standard chart parsing, and in general this is not done for efficiency reasons. Instead, we need to produce a single edge that is &amp;quot;in charge&amp;quot; of the computation looking for NPs between 0 and 3. When possible NPs are then found, these then need to be combined with the original edges by an appropriate form of the fundamental rule. We are thus led to the following form for a generalised edge in our chart parser: &lt;C from S to E needs CSI nom si to el, c32 from s2 to e2, cs from sa to ea&gt; where C is a category, the csi are lists of categories (which we will show inside square brackets), S. E, t</context>
</contexts>
<marker>Steedman, 1987</marker>
<rawString>Steedman, Mark. 1987 Combinatory Grammars and Human Language Processing. In: Garfield, J., Ed., Modularity in Knowledge Representation and Natural Language Processing. Bradford Books/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph M Weischedel</author>
<author>Norman K Sondheimer</author>
</authors>
<title>Meta-rules as a Basis for Processing Ill-Formed Input.</title>
<date>1983</date>
<journal>AJCL</journal>
<pages>9--3</pages>
<contexts>
<context position="1760" citStr="Weischedel and Sondheimer (1983)" startWordPosition="258" endWordPosition="261">ng simple errors (unknown/misspelled words, omitted words, extra noise words) in sentences (from languages described by context free phrase structure grammars without eproductions). This strategy has the advantage that the recovery process can run after a standard (active chart) parser has terminated unsuccessfully, without causing existing work to be repeated or the original parser to be slowed down in any way, and that, unlike previous systems, it allows the full syntactic context to be exploited in the determination of a &amp;quot;best&amp;quot; parse for an ill-formed sentence. EXPLOITING SYNTACTIC CONTEXT Weischedel and Sondheimer (1983) present an approach to processing ill-formed input based on a modified AIN parser. The basic idea is, when an initial parse fails, to select the incomplete parsing path that consumes the longest initial portion of the input, apply a special rule to allow the blocked parse to continue, and then to iterate this process until a successful parse is generated. The result is a &amp;quot;hillclimbing&amp;quot; search for the &amp;quot;best&amp;quot; parse, relying at each point on the &amp;quot;longest path&amp;quot; heuristic. Unfortunately, sometimes this heuristic will yield several possible parses, for instance with the sentence: The snow blocks T </context>
</contexts>
<marker>Weischedel, Sondheimer, 1983</marker>
<rawString>Weischedel, Ralph M. and Sondheimer, Norman K. 1983 Meta-rules as a Basis for Processing Ill-Formed Input. AJCL 9(3-4): 161-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Optimal Search Strategies for Speech Understanding Control.</title>
<date>1982</date>
<journal>Artificial Intelligence</journal>
<volume>18</volume>
<issue>3</issue>
<pages>295--326</pages>
<contexts>
<context position="16182" citStr="Woods (1982)" startWordPosition="2887" endWordPosition="2888">r in an edge. PB (best penalty) This is an estimate of the best possible penalty that this edge, when complete, could have. This score can use the PSF, together with information about the parts of the chart covered - for 105 instance, the number of words in these parts which do not have lexical entries. GUS (the maximum number of words that have been used so far in a partial parse using this edge) We prefer edges that lead to parses accounting for more words of the input. PBG (the best possible penalty for any complete hypothesis involving this edge). This is a shortfall score in the sense of Woods (1982). UBG (the best possible number of words that could be used in any complete hypothesis containing this edge). In our implementation, each rule calculates each of these scores for the new edge from those of the contributing edges. We have experimented with a number of ways of using these scores in comparing two possible edges to be added to the chart. At present, the most promising approach seems to be to compare in turn the scores for PBG, MDE, UBG, GUS, PSF and PB. As soon as a difference in scores is encountered, the edge that wins on this account is chosen as the preferred one. Putting PBG </context>
</contexts>
<marker>Woods, 1982</marker>
<rawString>Woods, William A. 1982 Optimal Search Strategies for Speech Understanding Control. Artificial Intelligence 18(3): 295-326.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>