<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<note confidence="0.90181">
Proceedings of the Workshop of the
ACL Special Interest Group on Computational Phonology (SIGPHON)
Association for Computations Linguistics
Barcelona, July 2004
</note>
<title confidence="0.999324">
Multilingual Noise-Robust Supervised Morphological Analysis
using the WordFrame Model
</title>
<author confidence="0.998167">
Richard Wicentowski
</author>
<affiliation confidence="0.979087">
Swarthmore College
</affiliation>
<address confidence="0.698715">
Swarthmore, Pennsylvania, USA 19081
</address>
<email confidence="0.999053">
richardw@cs.swarthmore.edu
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999885">
This paper presents the WordFrame model, a noise-
robust supervised algorithm capable of inducing
morphological analyses for languages which exhibit
prefixation, suffixation, and internal vowel shifts. In
combination with a n¨aive approach to suffix-based
morphology, this algorithm is shown to be remark-
ably effective across a broad range of languages, in-
cluding those exhibiting infixation and partial redu-
plication. Results are presented for over 30 lan-
guages with a median accuracy of 97.5% on test
sets including both regular and irregular verbal in-
flections. Because the proposed method trains ex-
tremely well under conditions of high noise, it is an
ideal candidate for use in co-training with unsuper-
vised algorithms.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731666666667">
This paper presents the WordFrame model, a novel
algorithm capable of inducing morphological anal-
yses for a large number of the world’s languages.
The WordFrame model learns a set of string trans-
ductions from inflection-root pairs and uses these to
transform unseen inflections into their correspond-
ing root forms. These string transductions directly
model prefixation, suffixation, associated point-of-
affixation changes and stem-internal vowel shifts.
Though not explicitly modeled, patterns extracted
from large amounts of noisy training data can be
highly effective at aligning inflections with roots in
languages which exhibit vowel harmony, agglutina-
tion, and partial word reduplication.
The WordFrame model contains no language-
specific parameters. While we make no claims that
the model works equally well for all languages, its
ability to analyze inflections in 32 diverse languages
with a median accuracy of 97.5% attests to its flex-
ibility in learning a wide range of morphological
phenomena.
The effectiveness of the model when trained from
noisy data makes it well-suited for co-training with
low-accuracy unsupervised algorithms.
</bodyText>
<sectionHeader confidence="0.997215" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999903837209303">
The development of the WordFrame model was mo-
tivated by work originally presented in Yarowsky
and Wicentowski (2000). In that work, a suite of
unsupervised learning algorithms and a supervised
morphological learner are co-trained to achieve high
accuracies for English and Spanish verb inflec-
tions. The supervised learner employed a naive
approach to morphology, only capable of learning
word-final stem changes between inflections and
roots. This “end-of-string model” of morphology
was used again in Yarowsky et al. (2001) where it
was applied to English, French and Czech. (More
complete details of the end-of-string model are pre-
sented in Section 3.3.1.)
Though simplistic, this end-of-string model is ro-
bust to noise, especially important in co-training
with low-accuracy unsupervised learners. However,
the end-of-string model relied heavily upon exter-
nally provided, noise-free lists of affixes in order to
correctly align inflections to roots. The WordFrame
model allows, but does not require, such affix lists,
thereby eliminating direct human supervision.
Much previous work has been done in automat-
ically acquiring such affix lists, most recently the
generative models built by Snover and Brent (2001)
which are able to identify suffixes in English and
Polish. Schone and Jurafsky (2001) use latent se-
mantic analysis to find prefixes, suffixes and cir-
cumfixes in German, Dutch and English. Ba-
roni (2003) treats morphology as a data compres-
sion problem to find English prefixes.
Goldsmith (2001) uses minimum description
length to successfully find paradigmatic classes of
suffixes in a number of European languages, includ-
ing Dutch and Russian, though the approach has
been less successful in handling prefixation.
The Boas project (Oflazer et al., 2001), (Hakkani-
T¨ur et al., 2000), and (Oflazer and Nirenburg, 1999)
has produced excellent results bootstrapping a mor-
phological analyzer, but rely on direct human su-
pervision to produce two-level rules (Koskenniemi,
1983) which are then compiled into a finite state ma-
chine.
</bodyText>
<sectionHeader confidence="0.888702" genericHeader="method">
3 The WordFrame Algorithm
</sectionHeader>
<subsectionHeader confidence="0.997588">
3.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999900083333333">
The supervised morphological learner presented
in Yarowsky and Wicentowski (2000) modeled
lemmatization as a word-final stem change plus a
suffix taken from a (possibly empty) list of potential
suffixes. Though effective for suffixation, this end-
of-string (EOS) based model can not model other
morphological phenomena, such as prefixation.
By including a pre-specified list of prefixes, we
can extend the EOS model to handle simple prefix-
ation: For each inflection, an analysis is performed
on the original string, plus on each substring re-
sulting from removing exactly one matching prefix
taken from the list of prefixes. While effective for
some simple prefixal morphologies, this extension
cannot model word-initial stem changes at the point
of prefixation. In contrast, the WordFrame (WF) al-
gorithm can isolate a potential prefix and model any
potential point-of-prefixation stem changes directly,
without pre-specified lists of prefixes.
The EOS model also fails to capture word-
internal vowel changes found in many languages.
The WF model directly models stem-internal vowel
changes in order to to learn higher-quality, less
sparse, transformation rules.
</bodyText>
<table confidence="0.31667925">
training pair EOS analysis WF analysis
acuerto→acortar uerto→ortar ue→o
apruebo→aprobar uebo→obar ue→o
muestro→mostrar uestro→ostrar ue→o
</table>
<tableCaption confidence="0.876897">
Table 1: The above Spanish examples are misana-
</tableCaption>
<bodyText confidence="0.79255275">
lyzed by the EOS algorithm, which results in learn-
ing rules with low productivity. The WF algo-
rithm is able to identify the productive ue→o stem-
internal vowel change.
</bodyText>
<subsectionHeader confidence="0.881877">
3.2 Required and Optional Resources
</subsectionHeader>
<listItem confidence="0.9710615">
a. Training data of the form &lt;inflection,root&gt; is
required for the WordFrame algorithm. Ideally,
this data should be high-quality and noise-free,
but algorithm is robust to noise, which allows
one to use lower-quality pairs extracted from
unsupervised techniques.
b. Pre-specified lists of prefixes and suffixes can
be incorporated, but are not required.
c. Precision can be improved (at the expense of
coverage) by providing a list of potential roots
extracted from a dictionary or large corpus.
d. In order to allow for word-internal vowel
changes, the WordFrame model requires a list
of the vowels of the language.
</listItem>
<subsectionHeader confidence="0.994516">
3.3 Formal Presentation
</subsectionHeader>
<bodyText confidence="0.9999765">
The WordFrame model is constructed explicitly as
an extension to the end-of-string model proposed by
Yarowsky and Wicentowski (2000); as such, we first
give a brief presentation of the model, then intro-
duce the WordFrame model.
In the discussion below, if affix lists are not ex-
plicitly provided, they are assumed to contain the
single element c (the empty string).
</bodyText>
<subsectionHeader confidence="0.705045">
3.3.1 The end-of-string model
</subsectionHeader>
<bodyText confidence="0.999835">
The end-of-string model makes use of two optional
externally provided sets: a set of acceptable suf-
fixes, Ψ0s, and a set of “canonical root endings”, Ψs.
The inclusion of a list of canonical root endings is
motivated by languages where verb roots can end in
only a limited number of ways (e.g. -er, -ir and -re
in French).
From inflection-root training pairs, a determinis-
tic analysis is made by removing the longest match-
ing suffix ( 0 s ∈ Ψ0s) from the inflection, removing
the longest matching canonical ending ( s ∈ Ψs)
from the root, and removing the longest common
initial substring (ry) from both words. The remain-
ing strings represent the word-final stem change
(S0s → Ss) necessary to transform the inflection
(ryS0s 0s) into the root (rySs s). The word-final stem
changes are stored in a hierarchically-smoothed suf-
fix trie representing P(S0s → Ss|ryS0s).
A simple extension allows the EOS model to
handle purely concatenative prefixation: the analy-
sis begins by removing the longest matching prefix
taken from a given set of prefixes ( 0p ∈ Ψ0p), then
continuing as above. This changes the inflection to
0pryS0s 0s, and leaves the root as rySs s. (See Table 2
for an overview of this notation.)
Given a previously unseen inflection, one finds
the root that maximizes P(rySs s |0pryS0s 0s). By
making strong independence assumptions and some
approximations, and assuming that all prefixes and
suffixes are equally likely, this is equivalent to:1
</bodyText>
<equation confidence="0.684189">
P(rySs s |0pryS0s 0s) = max P(S0 s → Ss|ryS0 s)
ψ�p,γδ�s,ψ�s
</equation>
<bodyText confidence="0.998779">
Note we are using a slightly different, but equiv-
alent, notation to that used in Yarowsky and Wicen-
towski (2000). Simply, we use 0s rather than ~, and
we use S0s → Ss rather than α → ~. This change
was made in order to make the formalization of the
WF model more clear.
</bodyText>
<footnote confidence="0.9153">
1Full details available in (Wicentowski, 2002).
</footnote>
<table confidence="0.996434375">
prefix point-of- secondary vowel primary point-of- suffix/
prefixation common change common suffixation ending
change substring substring change
Extended inflection 00 root rys S0 00
EOS p s s
Ss 0s
infleWordFrame rootction root 00p S0p ryp S0v rys S0s 00s
Sp Sv Ss 0s
</table>
<tableCaption confidence="0.989294666666667">
Table 2: Overview of the analyzed components of the inflection and root using the end-of-string (EOS)
model extended to allow for simple prefixation, and the WordFrame model. If lists of prefixes, suffixes and
endings are not specified, the prefix, suffix and ending are set to c.
</tableCaption>
<subsectionHeader confidence="0.779084">
3.3.2 The WordFrame model
</subsectionHeader>
<bodyText confidence="0.971579090909091">
The WordFrame model fills two major gaps in the
EOS model: the inability to model prefixation with-
out a list of provided prefixes, and the inability to
model stem-internal vowel shifts.
While not required, the WordFrame model does
allow for the inclusion of lists of prefixes, and when
provided, can automatically discover the point-of-
prefixation stem change, S0 p → Sp. When a list
of prefixes is not provided, the word-initial stem
change will model both the prefix and stem change.
Formally, this requires the inclusion of the point-
of-prefixation stem change into the notation used in
the EOS model. When presented with an inflection-
root pair, the longest common substring in the in-
flection and root, ry, is assumed to be the stem. The
string preceding the stem is the prefix and point-of-
prefixation stem change, 00pS0p; the string following
the stem is the suffix and point-of-suffixation stem
change, 00sS0 s. Combining these parts, the inflection
can be,/, P
represented as 00S0pryS0s00s, and the root as
4&apos;
</bodyText>
<subsectionHeader confidence="0.69465">
SPrySs s
</subsectionHeader>
<bodyText confidence="0.987258603773585">
In addition, the WordFrame model allows for a
single word-internal vowel change within the stem.
To accommodate this, the longest common sub-
string of the inflection and root, ry, is allowed to be
split in a single location to allow the vowel change
S0v → Sv where S0v and Sv are taken from a prede-
termined list of vowels for the language.2 The por-
tions of the stem located before and after the vowel
change are now ryp and rys, respectively.
Both S0v and Sv may contain more than vowel,
thereby allowing vowel changes such as ee→e.
However, as presented here, the WF model does not
allow for the insertion of vowels into the stem where
there were no vowels previously; more formally,
both S0v and Sv must contain at least one vowel, or
they both must be c. Though this restriction can
2If one wishes to model arbitrary internal changes, this
“vowel” list could be made to include every letter in the al-
phabet; results are not presented for this configuration.
be removed, initial results (not presented here) in-
dicated a significant drop in accuracy when entire
vowels clusters could be removed or inserted. In
addition, the vowel change must be internal to the
stem, and cannot be located at the boundary of the
stem; formally, unless both S0v and Sv are c, both
portions of the split stem (ryp and rys) must contain
at least one letter. This prevents confusion between
“stem-internal” vowel changes and stem-changes at
the point of affixation.
As with the EOS model, a deterministic analy-
sis is made from inflection-root training pairs. If
provided, the longest matching prefix and suffix are
removed from the inflection, and the longest match-
ing canonical ending is removed from the root.3 The
remaining string must then be analyzed to find the
longest common substring with at most one vowel
change, which we call the WordFrame.
The WordFrame (rypS0vrys, rypSvrys) is defined to
be the longest common substring with at most one
internal vowel cluster (V ∗ → V ∗) transformation.
Should there be multiple “longest” substrings, the
substring closest to the start of the inflection is cho-
sen.4 In practice, there is rarely more than one such
“longest” substring.
The remaining strings at the start and end of the
common substring form the point-of-prefixation and
point-of-suffixation stem changes.
The final representation of the inflection-root pair
in the WF model is shown in Table 2.
Given an unseen inflection, one finds the root
that maximizes P(SprypSvrysSs0s|00prysS0s00s). If we
make the simplifying assumption that all prefixes,
suffixes and endings are equally likely and remove
</bodyText>
<footnote confidence="0.988315285714286">
3A canonical prefix is not included in the model because we
knew of no language in which this occurred; introducing it to
the model would be straight-forward.
4This places a bias in favor of end-of-string changes and is
motivated by the number of languages which are suffixal and
the relative few that are not; this could be adjusted for prefixal
languages.
</footnote>
<table confidence="0.99219175">
END-OF-STRING
ψ0p δ0p → δp γp δ0v →δv γs δ0s → δs ψ0s →ψs
English kept→keep ke p→ep t→ E
sang→sing s ang→ing
Spanish acuerto→acortar ac uert→ort o→ar
muestro→mostrar m uestr→ostr o→ar
German gestunken→stinken gef gestunk→stink en→en
gefielt→gefallen iel→all t→en
WORDFRAME
ψ0p δ0p → δp γp δ0v → δv γs δ0s → δs ψ0s → ψs
English kept→keep k e→ee p t→ E
sang→sing s a→i ng
Spanish acuerto→acortar ac ue→o rt o→ar
muestro→mostrar m ue→o str o→ar
German gestunken→stinken ge→ E st u→i nk E →l en→en
gefielt→gefallen gef ie→a l t→en
</table>
<tableCaption confidence="0.997138">
Table 3: End-of-string and WordFrame analysis of training data assuming no provided lists of prefixes. The
</tableCaption>
<bodyText confidence="0.66983075">
EOS analysis yields non-productive rules such as gestunk→stink. The WF analysis captures the productive
Spanish vowel change ue → o, the German prefix ge, and English vowel changes e→ee and a→i.
the longest possible affixes deterministically, this is
equivalent to:
</bodyText>
<equation confidence="0.9308785">
P(δpγpδvγsδs|δ0 pγpδ0 vγsδ0 s)
= P(δ0v → δv,δ0p → δp,δ0 s → δs|δ0pγpδ0vγsδ0s)
</equation>
<bodyText confidence="0.99987175">
This can be expanded using the chain rule. As
before, the point-of-suffixation probabilities are
implicitly conditioned on the applicability of the
change to δ0pγpδ0vγsδ0s, and are taken from a suf-
fix trie created during training. The point-of-
prefixation probabilities are implicitly conditioned
on the applicability of the change to δ0pγpδ0vγs, i.e.
once δ0s has been removed, and are taken from an
analogous prefix trie. The vowel change probability
is conditioned on the applicability of the change to
γpδ0vγs. In the current implementation, this is ap-
proximated using the conditional probability of the
vowel change P(δv|δ0 v) without regard to the local
context. This is a major weakness in the current sys-
tem and one that will be addressed in future work.
The WordFrame model’s ability to capture stem-
internal vowel changes allows for proper analysis of
the Spanish examples from Table 1, and also allows
for the analysis of prefixes without the use of a pre-
specified list of prefixes, as shown in Table 3.
</bodyText>
<sectionHeader confidence="0.998937" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999683">
All of the experimental results presented here were
done using 10-fold cross-validation on the training
data. The majority of the training data used here
</bodyText>
<table confidence="0.9235515">
point-of-prefixation change ge → c
point-of-suffixation change c → l
vowel changes u → i
ie → a
</table>
<tableCaption confidence="0.9314435">
Table 4: String transductions derived from the Ger-
man examples listed in Table 3.
</tableCaption>
<bodyText confidence="0.993417894736842">
was obtained from web sources, although some has
been hand-entered or scanned from printed materi-
als then hand-corrected. All of the data used were
inflected verbs; there was no derivational morphol-
ogy in this evaluation.5 Unless otherwise specified,
all results are system accuracies at 100% coverage –
Section 5.3 addresses precision at lower coverages.
Space limits the number of results that can be
presented here since most of the evaluations have
been carried out in each of the 32 languages. There-
fore, in comparing the models, results will only be
shown for only a representative subset of the lan-
guages. When appropriate, a median or average for
all languages will also be given. Table 10 presents
the final results for all languages.
5Examples of derivational morphology, as well as nominal
and adjectival inflectional morphology, are excluded from this
presentation due to the lack of available training data for more
than a small number of well-studied languages.
</bodyText>
<subsectionHeader confidence="0.993864">
4.1 End-of-string vs. WordFrame
</subsectionHeader>
<bodyText confidence="0.999845285714286">
The most striking difference in performance be-
tween the EOS model and WordFrame model comes
from the evaluation of languages with prefixal mor-
phologies. The EOS model cannot handle prefixa-
tion without pre-specified lists of prefixes, so when
these are omitted, the WF model drastically outper-
forms the EOS model (Table 5).
</bodyText>
<table confidence="0.999209428571428">
Language EOS w/o Affixes EOS w/ Affixes
WF WF
Tagalog 1.6% 89.9% 92.0% 96.0%
Swahili 2.9% 96.8% 93.8% 96.9%
Irish 45.7% 89.5% - -
Spanish 94.7% 90.2% 96.5% 95.2%
Portuguese 97.4% 97.9% 97.3% 97.5%
</table>
<tableCaption confidence="0.994783">
Table 5: Accuracy of the EOS model vs the WF
</tableCaption>
<bodyText confidence="0.978924033333333">
model without and with pre-specified lists of affixes
(if available for that language).
Table 5 also shows that the simple EOS model
can sometimes significantly outperform the WF
model (e.g. in Spanish). Making things more dif-
ficult, predicting which model will be more suc-
cessful for a particular language and set of train-
ing data may not be possible, as illustrated by the
fact that EOS model performed better for Spanish,
but the closely-related Portuguese was better han-
dled by the WF model. Additionally, as illustrated
by the Portuguese example, it is not always benefi-
cial to include lists of affixes, making selection of
the model problematic.
Lists of prefixes and suffixes were not avail-
able for all languages.6 However, for the 25 lan-
guages where such lists were available, the Word-
Frame model performed equally or better on only 17
(68%). Evidence suggests that this occurs when the
affix lists have missing prefixes or suffixes. Since
these lists were extracted from printed grammars,
such gaps were unavoidable.
Regardless of whether or not affix lists were in-
cluded, the WordFrame model only outperformed
the EOS model for just over half the languages. An
examination of the output of the WF model suggests
that the relative parity in performance of the two
models is due to the poor estimation of the vowel
change probability which is approximated without
regard to the contextual clues.
</bodyText>
<footnote confidence="0.9565308">
6The affix lists used in this evaluation were hand-entered
from grammar references and were only available for 25 of the
32 languages evaluated here; therefore, the results presented
in this section omit these seven languages: Norwegian, Hindi,
Sanskrit, Tamil, Russian, Irish, and Welsh.
</footnote>
<sectionHeader confidence="0.838907" genericHeader="method">
5 WordFrame + EOS
</sectionHeader>
<bodyText confidence="0.999773166666667">
One of our goals in designing the WordFrame
model was to reduce or eliminate the dependence
on externally supplied affix lists. However, the re-
sults presented in Section 4.1 indicate that the WF
model outperforms the EOS model for just over half
(17/32) of the evaluated languages, even when affix
lists are included.
Predicting which model worked better for a par-
ticular language proved difficult, so we created a
new analyzer by combining our WordFrame model
with the end-of-string model. For each inflection,
the root which received the highest probability us-
ing an equally-weighted linear combination was se-
lected as the final analysis.
This new combination analyzer outperformed
both stand-alone models for 21 of the 25 languages
with significant overall accuracy improvements as
shown in Table 6(a).
</bodyText>
<table confidence="0.9992005">
w/o Affixes EOS WF Combined
Average 79.2% 91.0% 93.0%
Median 93.6% 95.9% 97.4%
w/ Affixes EOS WF Combined
Average 95.1% 95.0% 96.8%
Median 96.7% 96.7% 97.6%
</table>
<tableCaption confidence="0.952854">
Table 6: Average and median accuracy of the indi-
</tableCaption>
<bodyText confidence="0.954931363636364">
vidual models vs. the combined model (a) with and
(b) without affix lists.
When affix lists are available, combining the
WordFrame model and the end-of-string model
yielded very similar results: the combined model
outperformed either model on its own for 23 of the
25 languages. Of the two remaining languages, the
stand-alone WF model outperformed the combined
model by just one example out of 5197 in Danish,
and just 4 examples out of 9497 in Tagalog. As
before, the combined model showed significant ac-
curacy increases over either stand-alone model, as
shown in Table 6(b).
Finally, we build the WordFrame+EOS classifier,
by combining all four individual classifiers (EOS
with and without affix lists, and WF with and with-
out affix lists) using a simple equally-weighted lin-
ear combination. This is motivated from our ini-
tial observation that using affix lists does not always
improve overall accuracy. Cumulative results are
shown below in Table 7, and results for each indi-
vidual language is shown in Table 10.
</bodyText>
<table confidence="0.997915">
WF + EOS w/o Affixes w/ Affixes Combined
Average 93.0% 96.8% 97.2%
Median 97.4% 97.6% 97.9%
</table>
<tableCaption confidence="0.986119666666667">
Table 7: Accuracy of the combined models, plus a
combination of the combined models in the 25 lan-
guages for which affix lists were available.
</tableCaption>
<subsectionHeader confidence="0.997842">
5.1 Robustness to Noise
</subsectionHeader>
<bodyText confidence="0.96366285">
The WordFrame model was designed as an alter-
native to the end-of-string model. In Yarowsky
and Wicentowski (2000), the end-of-string model is
trained from inflection-root pairs acquired through
unsupervised methods. None of those previously
presented unsupervised models yielded high accura-
cies on their own, so it was important that the end-
of-string model was robust enough to learn string
transduction rules even in the presence of large
amounts of noise.
In order for the WF+EOS model to be an ade-
quate replacement for the end-of-string model, it
must also be robust to noise. To test this, we first
ran the WF+EOS model as before on all of the data
using 10-fold cross-validation. Then, we introduced
noise by randomly assigning a certain percentage of
the inflections to the roots of other inflections. For
example, the correct pair menaced-menace became
the incorrect pair menaced-move. The results of in-
troducing this noise are presented in Table 9 and
</bodyText>
<figureCaption confidence="0.845415">
Figure 1.
</figureCaption>
<table confidence="0.9998606">
Noise 0% 10% 25% 50% 75%
English 99.1% 98.6% 98.6% 98.4% 97.6%
French 99.6% 99.5% 99.5% 99.3% 98.9%
Estonian 96.8% 94.7% 94.3% 92.0% 87.0%
Turkish 99.5% 98.5% 98.2% 97.1% 91.4%
</table>
<tableCaption confidence="0.997211">
Table 9: The combined WordFrame and EOS model
</tableCaption>
<bodyText confidence="0.925151">
maintains high accuracy in the presence noise.
Above, up to 75% of the inflections in the training
data have been assigned incorrect roots.
As one might expect, the effect of introduc-
ing noise is particularly pronounced for highly in-
flected languages such as Estonian, as well as with
the vowel-harmony morphology found in Turkish7.
However, languages with minimal inflection (En-
glish) or a fairly regular inflection space (French)
show much less pronounced drops in accuracy as
noise increases.
</bodyText>
<footnote confidence="0.880037333333333">
7All of the data is inflectional verb morphology, making the
Turkish task substantially easier than most other attempts at
modeling Turkish morphology.
</footnote>
<subsectionHeader confidence="0.475343">
Percent Noise
</subsectionHeader>
<figureCaption confidence="0.968802">
Figure 1: The WF+EOS algorithm’s robustness to
</figureCaption>
<bodyText confidence="0.956749461538462">
noise yields only a 5% reduction in performance
even when 50% of the training samples are replaced
with noise.
It is important to point out that the incorrect pairs
were not added in addition to the correct pairs;
rather, they replaced the correct pairs. For exam-
ple, the Estonian training data was comprised of
5932 inflection-root pairs. When testing at 50%
noise, there were only 2966 correct training pairs,
and 2966 incorrect pairs. This means that real size
of the training data was also reduced, further lower-
ing accuracy, and making the model’s effective ro-
bustness to noise more impressive.
</bodyText>
<subsectionHeader confidence="0.997969">
5.2 Regular vs. Irregular Inflections
</subsectionHeader>
<bodyText confidence="0.98325">
For 13 of the languages evaluated, the inflections
were classified as either regular, irregular, or semi-
regular. As an example, the English pair jumped-
jump was classified as regular, the pair hopped-hop
was semi-regular (because of the doubling of the
final-p), and the pair threw-throw was labeled irreg-
ular.8
Table 8 shows the accuracy of the WF+EOS
model in each of the three categories, as well as
for all data in total.9 As expected, the WF+EOS
model performs very well on regular inflections and
reasonably well on the semi-regular inflections for
most languages.
The performance on the irregular verbs, though
clearly not as good as on the regular or semi-
regular verbs, was surprisingly good, most notably
in French, and to a lesser extent, Spanish and Ital-
</bodyText>
<footnote confidence="0.994612666666667">
8These classifications were assigned by the provider of our
training pairs, not by us.
9The small discrepancy between the data in Table 8 and Ta-
ble 10 is due to the fact that some of the inflection-root pairs
were not labeled. The “All” column of Table 8 reflects only
labeled inflections.
</footnote>
<figure confidence="0.988565692307692">
0% 10% 25% 50% 75%
Relative Accuracy
100%
98%
96%
95%
94%
92%
90%
French
English
Turkish
Estonian
</figure>
<table confidence="0.9883194">
Language All types Regular types Semi types Irregular types
accuracy accuracy accuracy accuracy
Spanish 97.28% 58589 97.60% 52709 95.38% 1665 93.40% 3861
Catalan 90.65% 4066 96.31% 2898 84.35% 230 74.73% 938
Occitan 93.39% 7583 98.46% 6096 97.55% 654 52.58% 795
French 99.58% 63644 99.79% 57255 99.95% 2221 97.00% 3866
Italian 98.43% 62920 98.75% 54643 99.58% 3335 93.64% 4496
Romanian 97.84% 24000 98.95% 21237 94.78% 920 85.36% 1660
English 98.95% 3703 99.45% 3073 99.50% 597 40.62% 32
Danish 97.87% 4185 98.59% 3760 95.00% 220 87.80% 205
Norwegian 95.85% 1954 97.57% 1731 90.62% 96 76.38% 127
Icelandic 92.58% 3692 97.78% 2884 97.79% 226 64.78% 582
Hindi 84.77% 256 98.58% 212 33.33% 9 14.29% 35
Turkish 99.46% 29131 99.95% 26134 95.66% 2811 88.71% 186
Welsh 88.55% 45812 89.27% 44060 86.69% 1180 32.84% 536
</table>
<tableCaption confidence="0.999692">
Table 8: Accuracy of WF+EOS on different types of inflections
</tableCaption>
<bodyText confidence="0.999797166666667">
ian. This is due in large part because our test set in-
cluded many irregular verbs which shared the same
irregularity. For example, in French, the inflection-
root pair prit-prendre is irregular; however, the pairs
apprit-apprendre and comprit-comprendre both fol-
low the same irregular rule. The inclusion of just
one of these three pairs in the training data will al-
low the WF+EOS model to correctly find the root
form of the other two. Our French test set included
many examples of this, including roots that ended
-tenir, -venir, -mettre, and -duire.
For most languages however, the performance on
the irregular set was not that good. We propose
no new solutions to handling irregular verb forms,
but suggest using non-string-based techniques, such
as those presented in (Yarowsky and Wicentowski,
2000), (Baroni et al., 2002) and (Wicentowski,
2002).
</bodyText>
<subsectionHeader confidence="0.998041">
5.3 Accuracy, Precision and Coverage
</subsectionHeader>
<bodyText confidence="0.999876789473684">
All of the previous results assumed that each inflec-
tion must be aligned to exactly one root, though one
can improve precision by relaxing this constraint.
The WF+EOS model transforms an inflection into
a new string which we can compare against a dic-
tionary, wordlist, or large corpus. In determining
the final inflection-root alignment, we can down-
weight, or even throw away, all proposed roots
which are are not found in such a wordlist. While
this will adversely affect coverage, precision may
be more important in early iterations of co-training.
Given a sufficiently large wordlist, such a weight-
ing scheme cannot discard correct analyses. In ad-
dition, a large majority of the incorrectly analyzed
inflections are proposed roots which are not actually
words. By excluding all proposed roots which were
not found in a broad coverage wordlist (available for
19 languages), median coverage fell to 97.4%, but
median precision increased from 97.5% to 99.1%.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999971285714286">
We have presented the WordFrame model, a noise-
robust supervised morphological analyzer which is
highly successful across abroad range of languages.
We have shown our model effective at learning
morphologies which exhibit prefixation, suffixation,
and stem-internal vowel changes. In addition, the
WordFrame model was successful in handling the
agglutination, infixation and partial reduplication
found in languages such as Tagalog without explic-
itly modeling these phenomena. Most importantly,
the WordFrame model is robust to large amounts
of noise, making it an ideal candidate for use in
co-training with lower-accuracy unsupervised algo-
rithms.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994">
M. Baroni, J. Matiasek, and T. Harald. 2002. Un-
supervised discovery of morphologically related
words based on orthographic and semantic simi-
larity. In Proceedings of the Workshop on Mor-
phological and Phonological Learning, pages
48–57.
M. Baroni. 2003. Distribution-driven morpheme
discovery: A computational/experimental study.
Yearbook ofMorphology, pages 213–248.
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computa-
tional Linguistics, 27(2):153–198.
</reference>
<table confidence="0.996749382352941">
Language Accuracy Wordlist Training Data
Entries Roots Infls
Spanish 97.3% 32895 1190 57224
Portuguese 97.9% 30145 584 22135
Catalan 90.7% - 103 4058
Occitan 93.4% - 180 7559
French 99.6% 27548 1829 63559
Italian 98.5% 27221 1582 62658
Romanian 97.9% 25228 1070 24877
Latin 91.4% - 279 26818
English 99.1% 264075 1218 4915
Danish 97.9% 51351 1062 5197
Norwegian 95.9% - 547 2489
Swedish 98.5% 46009 4035 13871
Icelandic 92.6% - 314 3987
Hindi 84.8% - 15 255
Sanskrit 89.5% - 867 1968
Tamil 91.0% - 24 602
Estonian 96.9% 344 147 5932
Finnish 97.5% - 1434 79734
Turkish 99.5% 25497 87 29130
Uzbek 99.5% - 434 27296
Basque 96.1% 33020 1185 5842
Czech 98.7% 29066 5715 23786
Polish 97.6% 42005 601 23725
Russian 90.8% 42740 191 3068
Greek 100% 35245 9 201
German 98.0% 45779 1213 14120
Dutch 98.4% 41962 1016 5768
Irish 95.5% - 54 1376
Welsh 88.6% - 1053 44295
Tagalog 97.5% - 212 9479
Swahili 97.0% 24985 818 27773
Klingon 100% 2114 699 5135
</table>
<tableCaption confidence="0.597615">
Table 10: For each language, the accuracy of the
WordFrame model combined with the end-of-string
</tableCaption>
<bodyText confidence="0.7893515">
model, the number of wordlist entries available
(Section 5.3), and the total training size used for
cross-validation.
D. Hakkani-T¨ur, K. Oflazer, and G. T¨ur. 2000. Sta-
tistical morphological disambiguation for agglu-
tinative languages. In 18th International Confer-
</bodyText>
<reference confidence="0.994023111111111">
ence on Computational Linguistics.
K. Koskenniemi. 1983. Two-level morphology: A
General Computational Model for Word-Form
Recognition and Production. Ph.D. thesis, De-
partment of Linguistics, University of Helsinki,
Finland.
K. Oflazer and S. Nirenburg. 1999. Practical boot-
strapping of morphological analyzers. In Confer-
ence on Natural Language Learning.
K. Oflazer, S. Nirenberg, and M. McShane. 2001.
Bootstrapping morphological analyzers by com-
bining human elicitation and maching learning.
Computational Linguistics, 27(1):59–84.
P. Schone and D. Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In Pro-
ceedings of the North American Chapter of the
Association of Computational Linguistics.
M. Snover and M. R. Brent. 2001. A bayesian
model for morpheme and paradigm identifica-
tion. In Proceedings of the Annual Meeting of the
Association of Computational Linguistics, vol-
ume 39, pages 482–490.
R. Wicentowski. 2002. Modeling and Learning
Multilingual Inflectional Morphology in a Mini-
mally Supervised Framework. Ph.D. thesis, The
Johns Hopkins University.
D. Yarowsky and R. Wicentowski. 2000. Mini-
mally supervised morphological analysis by mul-
timodal alignment. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics, pages 207–216.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Pro-
ceedings of the Human Language Technology
Conference, pages 161–168.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621434">
<note confidence="0.92190175">Proceedings of the Workshop of the ACL Special Interest Group on Computational Phonology (SIGPHON) Association for Computations Linguistics Barcelona, July 2004</note>
<title confidence="0.977415">Multilingual Noise-Robust Supervised Morphological Analysis using the WordFrame Model</title>
<author confidence="0.984396">Richard</author>
<affiliation confidence="0.914599">Swarthmore</affiliation>
<address confidence="0.999074">Swarthmore, Pennsylvania, USA</address>
<email confidence="0.999863">richardw@cs.swarthmore.edu</email>
<abstract confidence="0.99251625">This paper presents the WordFrame model, a noiserobust supervised algorithm capable of inducing morphological analyses for languages which exhibit prefixation, suffixation, and internal vowel shifts. In combination with a n¨aive approach to suffix-based morphology, this algorithm is shown to be remarkably effective across a broad range of languages, including those exhibiting infixation and partial reduplication. Results are presented for over 30 languages with a median accuracy of 97.5% on test sets including both regular and irregular verbal inflections. Because the proposed method trains extremely well under conditions of high noise, it is an ideal candidate for use in co-training with unsupervised algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>J Matiasek</author>
<author>T Harald</author>
</authors>
<title>Unsupervised discovery of morphologically related words based on orthographic and semantic similarity.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Morphological and Phonological Learning,</booktitle>
<pages>48--57</pages>
<contexts>
<context position="26611" citStr="Baroni et al., 2002" startWordPosition="4274" endWordPosition="4277">pairs apprit-apprendre and comprit-comprendre both follow the same irregular rule. The inclusion of just one of these three pairs in the training data will allow the WF+EOS model to correctly find the root form of the other two. Our French test set included many examples of this, including roots that ended -tenir, -venir, -mettre, and -duire. For most languages however, the performance on the irregular set was not that good. We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al., 2002) and (Wicentowski, 2002). 5.3 Accuracy, Precision and Coverage All of the previous results assumed that each inflection must be aligned to exactly one root, though one can improve precision by relaxing this constraint. The WF+EOS model transforms an inflection into a new string which we can compare against a dictionary, wordlist, or large corpus. In determining the final inflection-root alignment, we can downweight, or even throw away, all proposed roots which are are not found in such a wordlist. While this will adversely affect coverage, precision may be more important in early iterations of</context>
</contexts>
<marker>Baroni, Matiasek, Harald, 2002</marker>
<rawString>M. Baroni, J. Matiasek, and T. Harald. 2002. Unsupervised discovery of morphologically related words based on orthographic and semantic similarity. In Proceedings of the Workshop on Morphological and Phonological Learning, pages 48–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
</authors>
<title>Distribution-driven morpheme discovery: A computational/experimental study. Yearbook ofMorphology,</title>
<date>2003</date>
<pages>213--248</pages>
<contexts>
<context position="3668" citStr="Baroni (2003)" startWordPosition="533" endWordPosition="535">owever, the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots. The WordFrame model allows, but does not require, such affix lists, thereby eliminating direct human supervision. Much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by Snover and Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a</context>
</contexts>
<marker>Baroni, 2003</marker>
<rawString>M. Baroni. 2003. Distribution-driven morpheme discovery: A computational/experimental study. Yearbook ofMorphology, pages 213–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3759" citStr="Goldsmith (2001)" startWordPosition="548" endWordPosition="549">s of affixes in order to correctly align inflections to roots. The WordFrame model allows, but does not require, such affix lists, thereby eliminating direct human supervision. Much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by Snover and Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a finite state machine. 3 The WordFrame Algorithm 3.1 Motivation The supervised morphologica</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="false">
<note>ence on Computational Linguistics.</note>
<marker></marker>
<rawString>ence on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Koskenniemi</author>
</authors>
<title>Two-level morphology: A General Computational Model for Word-Form Recognition and Production.</title>
<date>1983</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Linguistics, University of Helsinki,</institution>
<contexts>
<context position="4237" citStr="Koskenniemi, 1983" startWordPosition="619" endWordPosition="620">fixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a finite state machine. 3 The WordFrame Algorithm 3.1 Motivation The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. Though effective for suffixation, this endof-string (EOS) based model can not model other morphological phenomena, such as prefixation. By including a pre-specified list of prefixes, we can extend the EOS model to handle simple prefixation: For each inflection, an analysis is performed on the origina</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>K. Koskenniemi. 1983. Two-level morphology: A General Computational Model for Word-Form Recognition and Production. Ph.D. thesis, Department of Linguistics, University of Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>S Nirenburg</author>
</authors>
<title>Practical bootstrapping of morphological analyzers.</title>
<date>1999</date>
<booktitle>In Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="4082" citStr="Oflazer and Nirenburg, 1999" startWordPosition="595" endWordPosition="598">Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a finite state machine. 3 The WordFrame Algorithm 3.1 Motivation The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. Though effective for suffixation, this endof-string (EOS) based model can not model other morphological phenomena, such as prefixation. By includi</context>
</contexts>
<marker>Oflazer, Nirenburg, 1999</marker>
<rawString>K. Oflazer and S. Nirenburg. 1999. Practical bootstrapping of morphological analyzers. In Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>S Nirenberg</author>
<author>M McShane</author>
</authors>
<title>Bootstrapping morphological analyzers by combining human elicitation and maching learning.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="4019" citStr="Oflazer et al., 2001" startWordPosition="585" endWordPosition="588">most recently the generative models built by Snover and Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a finite state machine. 3 The WordFrame Algorithm 3.1 Motivation The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. Though effective for suffixation, this endof-string (EOS) based model can not model</context>
</contexts>
<marker>Oflazer, Nirenberg, McShane, 2001</marker>
<rawString>K. Oflazer, S. Nirenberg, and M. McShane. 2001. Bootstrapping morphological analyzers by combining human elicitation and maching learning. Computational Linguistics, 27(1):59–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledge-free induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In Proceedings of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="3552" citStr="Schone and Jurafsky (2001)" startWordPosition="512" endWordPosition="515">stic, this end-of-string model is robust to noise, especially important in co-training with low-accuracy unsupervised learners. However, the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots. The WordFrame model allows, but does not require, such affix lists, thereby eliminating direct human supervision. Much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by Snover and Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>P. Schone and D. Jurafsky. 2001. Knowledge-free induction of inflectional morphologies. In Proceedings of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>M R Brent</author>
</authors>
<title>A bayesian model for morpheme and paradigm identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics,</booktitle>
<volume>39</volume>
<pages>482--490</pages>
<contexts>
<context position="3466" citStr="Snover and Brent (2001)" startWordPosition="498" endWordPosition="501">e details of the end-of-string model are presented in Section 3.3.1.) Though simplistic, this end-of-string model is robust to noise, especially important in co-training with low-accuracy unsupervised learners. However, the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots. The WordFrame model allows, but does not require, such affix lists, thereby eliminating direct human supervision. Much previous work has been done in automatically acquiring such affix lists, most recently the generative models built by Snover and Brent (2001) which are able to identify suffixes in English and Polish. Schone and Jurafsky (2001) use latent semantic analysis to find prefixes, suffixes and circumfixes in German, Dutch and English. Baroni (2003) treats morphology as a data compression problem to find English prefixes. Goldsmith (2001) uses minimum description length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and </context>
</contexts>
<marker>Snover, Brent, 2001</marker>
<rawString>M. Snover and M. R. Brent. 2001. A bayesian model for morpheme and paradigm identification. In Proceedings of the Annual Meeting of the Association of Computational Linguistics, volume 39, pages 482–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wicentowski</author>
</authors>
<title>Modeling and Learning Multilingual Inflectional Morphology in a Minimally Supervised Framework.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>The Johns Hopkins University.</institution>
<contexts>
<context position="8750" citStr="Wicentowski, 2002" startWordPosition="1344" endWordPosition="1345">reviously unseen inflection, one finds the root that maximizes P(rySs s |0pryS0s 0s). By making strong independence assumptions and some approximations, and assuming that all prefixes and suffixes are equally likely, this is equivalent to:1 P(rySs s |0pryS0s 0s) = max P(S0 s → Ss|ryS0 s) ψ�p,γδ�s,ψ�s Note we are using a slightly different, but equivalent, notation to that used in Yarowsky and Wicentowski (2000). Simply, we use 0s rather than ~, and we use S0s → Ss rather than α → ~. This change was made in order to make the formalization of the WF model more clear. 1Full details available in (Wicentowski, 2002). prefix point-of- secondary vowel primary point-of- suffix/ prefixation common change common suffixation ending change substring substring change Extended inflection 00 root rys S0 00 EOS p s s Ss 0s infleWordFrame rootction root 00p S0p ryp S0v rys S0s 00s Sp Sv Ss 0s Table 2: Overview of the analyzed components of the inflection and root using the end-of-string (EOS) model extended to allow for simple prefixation, and the WordFrame model. If lists of prefixes, suffixes and endings are not specified, the prefix, suffix and ending are set to c. 3.3.2 The WordFrame model The WordFrame model fi</context>
<context position="26635" citStr="Wicentowski, 2002" startWordPosition="4279" endWordPosition="4280"> comprit-comprendre both follow the same irregular rule. The inclusion of just one of these three pairs in the training data will allow the WF+EOS model to correctly find the root form of the other two. Our French test set included many examples of this, including roots that ended -tenir, -venir, -mettre, and -duire. For most languages however, the performance on the irregular set was not that good. We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al., 2002) and (Wicentowski, 2002). 5.3 Accuracy, Precision and Coverage All of the previous results assumed that each inflection must be aligned to exactly one root, though one can improve precision by relaxing this constraint. The WF+EOS model transforms an inflection into a new string which we can compare against a dictionary, wordlist, or large corpus. In determining the final inflection-root alignment, we can downweight, or even throw away, all proposed roots which are are not found in such a wordlist. While this will adversely affect coverage, precision may be more important in early iterations of co-training. Given a su</context>
</contexts>
<marker>Wicentowski, 2002</marker>
<rawString>R. Wicentowski. 2002. Modeling and Learning Multilingual Inflectional Morphology in a Minimally Supervised Framework. Ph.D. thesis, The Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="2370" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="336" endWordPosition="339">y, agglutination, and partial word reduplication. The WordFrame model contains no languagespecific parameters. While we make no claims that the model works equally well for all languages, its ability to analyze inflections in 32 diverse languages with a median accuracy of 97.5% attests to its flexibility in learning a wide range of morphological phenomena. The effectiveness of the model when trained from noisy data makes it well-suited for co-training with low-accuracy unsupervised algorithms. 2 Previous Work The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000). In that work, a suite of unsupervised learning algorithms and a supervised morphological learner are co-trained to achieve high accuracies for English and Spanish verb inflections. The supervised learner employed a naive approach to morphology, only capable of learning word-final stem changes between inflections and roots. This “end-of-string model” of morphology was used again in Yarowsky et al. (2001) where it was applied to English, French and Czech. (More complete details of the end-of-string model are presented in Section 3.3.1.) Though simplistic, this end-of-string model is robust to </context>
<context position="4413" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="643" endWordPosition="646">on length to successfully find paradigmatic classes of suffixes in a number of European languages, including Dutch and Russian, though the approach has been less successful in handling prefixation. The Boas project (Oflazer et al., 2001), (HakkaniT¨ur et al., 2000), and (Oflazer and Nirenburg, 1999) has produced excellent results bootstrapping a morphological analyzer, but rely on direct human supervision to produce two-level rules (Koskenniemi, 1983) which are then compiled into a finite state machine. 3 The WordFrame Algorithm 3.1 Motivation The supervised morphological learner presented in Yarowsky and Wicentowski (2000) modeled lemmatization as a word-final stem change plus a suffix taken from a (possibly empty) list of potential suffixes. Though effective for suffixation, this endof-string (EOS) based model can not model other morphological phenomena, such as prefixation. By including a pre-specified list of prefixes, we can extend the EOS model to handle simple prefixation: For each inflection, an analysis is performed on the original string, plus on each substring resulting from removing exactly one matching prefix taken from the list of prefixes. While effective for some simple prefixal morphologies, thi</context>
<context position="6653" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="981" endWordPosition="984">e, but algorithm is robust to noise, which allows one to use lower-quality pairs extracted from unsupervised techniques. b. Pre-specified lists of prefixes and suffixes can be incorporated, but are not required. c. Precision can be improved (at the expense of coverage) by providing a list of potential roots extracted from a dictionary or large corpus. d. In order to allow for word-internal vowel changes, the WordFrame model requires a list of the vowels of the language. 3.3 Formal Presentation The WordFrame model is constructed explicitly as an extension to the end-of-string model proposed by Yarowsky and Wicentowski (2000); as such, we first give a brief presentation of the model, then introduce the WordFrame model. In the discussion below, if affix lists are not explicitly provided, they are assumed to contain the single element c (the empty string). 3.3.1 The end-of-string model The end-of-string model makes use of two optional externally provided sets: a set of acceptable suffixes, Ψ0s, and a set of “canonical root endings”, Ψs. The inclusion of a list of canonical root endings is motivated by languages where verb roots can end in only a limited number of ways (e.g. -er, -ir and -re in French). From inflecti</context>
<context position="8546" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="1301" endWordPosition="1305">hing prefix taken from a given set of prefixes ( 0p ∈ Ψ0p), then continuing as above. This changes the inflection to 0pryS0s 0s, and leaves the root as rySs s. (See Table 2 for an overview of this notation.) Given a previously unseen inflection, one finds the root that maximizes P(rySs s |0pryS0s 0s). By making strong independence assumptions and some approximations, and assuming that all prefixes and suffixes are equally likely, this is equivalent to:1 P(rySs s |0pryS0s 0s) = max P(S0 s → Ss|ryS0 s) ψ�p,γδ�s,ψ�s Note we are using a slightly different, but equivalent, notation to that used in Yarowsky and Wicentowski (2000). Simply, we use 0s rather than ~, and we use S0s → Ss rather than α → ~. This change was made in order to make the formalization of the WF model more clear. 1Full details available in (Wicentowski, 2002). prefix point-of- secondary vowel primary point-of- suffix/ prefixation common change common suffixation ending change substring substring change Extended inflection 00 root rys S0 00 EOS p s s Ss 0s infleWordFrame rootction root 00p S0p ryp S0v rys S0s 00s Sp Sv Ss 0s Table 2: Overview of the analyzed components of the inflection and root using the end-of-string (EOS) model extended to allow</context>
<context position="21333" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="3413" endWordPosition="3416"> equally-weighted linear combination. This is motivated from our initial observation that using affix lists does not always improve overall accuracy. Cumulative results are shown below in Table 7, and results for each individual language is shown in Table 10. WF + EOS w/o Affixes w/ Affixes Combined Average 93.0% 96.8% 97.2% Median 97.4% 97.6% 97.9% Table 7: Accuracy of the combined models, plus a combination of the combined models in the 25 languages for which affix lists were available. 5.1 Robustness to Noise The WordFrame model was designed as an alternative to the end-of-string model. In Yarowsky and Wicentowski (2000), the end-of-string model is trained from inflection-root pairs acquired through unsupervised methods. None of those previously presented unsupervised models yielded high accuracies on their own, so it was important that the endof-string model was robust enough to learn string transduction rules even in the presence of large amounts of noise. In order for the WF+EOS model to be an adequate replacement for the end-of-string model, it must also be robust to noise. To test this, we first ran the WF+EOS model as before on all of the data using 10-fold cross-validation. Then, we introduced noise by</context>
<context position="26588" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="4270" endWordPosition="4273">rendre is irregular; however, the pairs apprit-apprendre and comprit-comprendre both follow the same irregular rule. The inclusion of just one of these three pairs in the training data will allow the WF+EOS model to correctly find the root form of the other two. Our French test set included many examples of this, including roots that ended -tenir, -venir, -mettre, and -duire. For most languages however, the performance on the irregular set was not that good. We propose no new solutions to handling irregular verb forms, but suggest using non-string-based techniques, such as those presented in (Yarowsky and Wicentowski, 2000), (Baroni et al., 2002) and (Wicentowski, 2002). 5.3 Accuracy, Precision and Coverage All of the previous results assumed that each inflection must be aligned to exactly one root, though one can improve precision by relaxing this constraint. The WF+EOS model transforms an inflection into a new string which we can compare against a dictionary, wordlist, or large corpus. In determining the final inflection-root alignment, we can downweight, or even throw away, all proposed roots which are are not found in such a wordlist. While this will adversely affect coverage, precision may be more important</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of the Annual Meeting of the Association of Computational Linguistics, pages 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
<author>R Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="2778" citStr="Yarowsky et al. (2001)" startWordPosition="396" endWordPosition="399">a makes it well-suited for co-training with low-accuracy unsupervised algorithms. 2 Previous Work The development of the WordFrame model was motivated by work originally presented in Yarowsky and Wicentowski (2000). In that work, a suite of unsupervised learning algorithms and a supervised morphological learner are co-trained to achieve high accuracies for English and Spanish verb inflections. The supervised learner employed a naive approach to morphology, only capable of learning word-final stem changes between inflections and roots. This “end-of-string model” of morphology was used again in Yarowsky et al. (2001) where it was applied to English, French and Czech. (More complete details of the end-of-string model are presented in Section 3.3.1.) Though simplistic, this end-of-string model is robust to noise, especially important in co-training with low-accuracy unsupervised learners. However, the end-of-string model relied heavily upon externally provided, noise-free lists of affixes in order to correctly align inflections to roots. The WordFrame model allows, but does not require, such affix lists, thereby eliminating direct human supervision. Much previous work has been done in automatically acquirin</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>D. Yarowsky, G. Ngai, and R. Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the Human Language Technology Conference, pages 161–168.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>