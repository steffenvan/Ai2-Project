<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000125">
<title confidence="0.999022">
Deep Grammars in a Tree Labeling Approach to
Syntax-based Statistical Machine Translation
</title>
<author confidence="0.997154">
Mark Hopkins
</author>
<affiliation confidence="0.998913">
Department of Linguistics
University of Potsdam, Germany
</affiliation>
<email confidence="0.988549">
hopkins@ling.uni-potsdam.de
</email>
<sectionHeader confidence="0.997302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936391304348">
In this paper, we propose a new syntax-
based machine translation (MT) approach
based on reducing the MT task to a tree-
labeling task, which is further decom-
posed into a sequence of simple decisions
for which discriminative classifiers can be
trained. The approach is very flexible and
we believe that it is particularly well-suited
for exploiting the linguistic knowledge en-
coded in deep grammars whenever possi-
ble, while at the same time taking advantage
of data-based techniques that have proven a
powerful basis for MT, as recent advances in
statistical MT show.
A full system using the Lexical-Functional
Grammar (LFG) parsing system XLE and
the grammars from the Parallel Grammar
development project (ParGram; (Butt et
al., 2002)) has been implemented, and we
present preliminary results on English-to-
German translation with a tree-labeling sys-
tem trained on a small subsection of the Eu-
roparl corpus.
</bodyText>
<sectionHeader confidence="0.990874" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999790545454545">
Machine translation (MT) is probably the oldest ap-
plication of what we call deep linguistic processing
techniques today. But from its inception, there have
been alternative considerations of approaching the
task with data-based statistical techniques (cf. War-
ren Weaver’s well-known memo from 1949). Only
with fairly recent advances in computer technology
have researchers been able to build effective statis-
tical MT prototypes, but in the last few years, the
statistical approach has received enormous research
interest and made significant progress.
</bodyText>
<author confidence="0.824468">
Jonas Kuhn
</author>
<affiliation confidence="0.9944895">
Department of Linguistics
University of Potsdam, Germany
</affiliation>
<email confidence="0.933">
kuhn@ling.uni-potsdam.de
</email>
<bodyText confidence="0.99968402631579">
The most successful statistical MT paradigm has
been, for a while now, the so-call phrase-based MT
approach (Och and Ney, 2003). In this paradigm,
sentences are translated from a source language to
a target language through the repeated substitution
of contiguous word sequences (“phrases”) from the
source language for word sequences in the target
language. Training of the phrase translation model
builds on top of a standard statistical word alignment
over the training corpus of parallel text (Brown et al.,
1993) for identifying corresponding word blocks,
assuming no further linguistic analysis of the source
or target language. In decoding, i.e. the application
of the acquired translation model to unseen source
sentences, these systems then typically rely on n-
gram language models and simple statistical reorder-
ing models to shuffle the phrases into an order that
is coherent in the target language.
An obvious advantage of statistical MT ap-
proaches is that they can adopt (often very id-
iomatic) translations of mid- to high-frequency con-
structions without requiring any language-pair spe-
cific engineering work. At the same time it is clear
that a linguistics-free approach is limited in what
it can ultimately achieve: only linguistically in-
formed systems can detect certain generalizations
from lower-frequency constructions in the data and
successfully apply them in a similar but different lin-
guistic context. Hence, the idea of “hybrid” MT, ex-
ploiting both linguistic and statistical information is
fairly old. Here we will not consider classical, rule-
based systems with some added data-based resource
acquisition (although they may be among the best
candidates for high-quality special-purpose transla-
tion – but adaption to new language pairs and sub-
languages is very costly for these systems). The
other form of hybridization – a statistical MT model
that is based on a deeper analysis of the syntactic
</bodyText>
<page confidence="0.991648">
33
</page>
<note confidence="0.485531">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999524">
structure of a sentence – has also long been iden-
tified as a desirable objective in principle (consider
(Wu, 1997; Yamada and Knight, 2001)). However,
attempts to retrofit syntactic information into the
phrase-based paradigm have not met with enormous
success (Koehn et al., 2003; Och et al., 2003)1, and
purely phrase-based MT systems continue to outper-
form these syntax/phrase-based hybrids.
In this work, we try to make a fresh start with
syntax-based statistical MT, discarding the phrase-
based paradigm and designing a MT system from
the ground up, using syntax as our central guid-
ing star – besides the word alignment over a par-
allel corpus. Our approach is compatible with and
can benefit substantially from rich linguistic rep-
resentations obtained from deep grammars like the
ParGram LFGs. Nevertheless, contrary to classi-
cal interlingual or deep transfer-based systems, the
generative stochastic model that drives our system
is grounded only in the cross-language word align-
ment and a surface-based phrase structure tree for
the source language and will thus degrade grace-
fully on input with parsing issues – which we sus-
pect is an important feature for making the overall
system competitive with the highly general phrase-
based MT approach.
Preliminary evaluation of our nascent system in-
dicates that this new approach might well have the
potential to finally realize some of the promises of
syntax in statistical MT.
</bodyText>
<sectionHeader confidence="0.989206" genericHeader="introduction">
2 General Task
</sectionHeader>
<bodyText confidence="0.999987461538462">
We want to build a system that can learn to translate
sentences from a source language to a destination
language. The general set-up is simple.
Firstly, we have a training corpus of paired sen-
tences f and e, where target sentence e is a gold
standard translation of source sentence f. These
sentence pairs are annotated with auxiliary informa-
tion, which can include word alignments and syntac-
tic information. We refer to these annotated sentence
pairs as complete translation objects.
Secondly, we have an evaluation corpus of source
sentences. These sentences are annotated with a sub-
set of the auxiliary information used to annotate the
</bodyText>
<footnote confidence="0.960002666666667">
1(Chiang, 2005) also reports that with his hierarchical gen-
eralization of the phrase-based approach, the addition of parser
information doesn’t lead to any improvements.
</footnote>
<figureCaption confidence="0.999776">
Figure 1: Example translation object.
</figureCaption>
<bodyText confidence="0.999254285714286">
training corpus. We refer to these partially annotated
source sentences as partial translation objects.
The task at hand: use the training corpus to learn
a procedure, through which we can successfully in-
duce a complete translation object from a partial
translation object. This is what we will define as
translation.
</bodyText>
<sectionHeader confidence="0.982014" genericHeader="method">
3 Specific Task Addressed by this Paper
</sectionHeader>
<bodyText confidence="0.997411714285714">
Before going on to actually describe a translation
procedure (and how to induce it), we need to spec-
ify our prior assumptions about how the translation
objects will be annotated. For this paper, we want to
exploit the syntax information that we can gain from
an LFG-parser, hence we will assume the following
annotations:
</bodyText>
<listItem confidence="0.9978962">
(1) In the training and evaluation corpora, the
source sentences will be parsed with the XLE-
parser. The attribute-value information from LFG’s
f-structure is restructured so it is indexed by (c-
structure) tree nodes; thus a tree node can bear mul-
tiple labels for various pieces of morphological, syn-
tactic and semantic information.
(2) In the training corpus, the source and target
sentence of every translation object will be aligned
using GIZA++ (http://www.fjoch.com/).
</listItem>
<bodyText confidence="0.9991996">
In other words, our complete translation objects
will be aligned tree-string pairs (for instance, Fig-
ure 1), while our partial translation objects will be
trees (the tree portion of Figure 1). No other annota-
tions will be assumed for this paper.
</bodyText>
<page confidence="0.99763">
34
</page>
<figureCaption confidence="0.996289">
Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the
GHKM tree.
</figureCaption>
<sectionHeader confidence="0.935194" genericHeader="method">
4 Syntax MT as Tree Labeling
</sectionHeader>
<bodyText confidence="0.999819911764706">
It is not immediately clear how one would learn a
process to map a parsed source sentence into an
aligned tree-string pair. To facilitate matters, we
will map complete translation objects to an alternate
representation. In (Galley et al., 2003), the authors
give a semantics to aligned tree-string pairs by asso-
ciating each with an annotated parse tree (hereafter
called a GHKM tree) representing a specific theory
about how the source sentence was translated into
the destination sentence.
In Figure 1, we show an example translation ob-
ject and in Figure 2, we show its associated GHKM
tree. The GHKM tree is simply the parse tree f of
the translation object, annotated with rules (hereafter
referred to as GHKM rules). We will not describe in
depth the mapping process from translation object to
GHKM tree. Suffice it to say that the alignment in-
duces a set of intuitive translation rules. Essentially,
a rule like: “not 1 → ne 1 pas” (see Figure 2) means:
if we see the word “not” in English, followed by a
phrase already translated into French, then translate
the entire thing as the word “ne” + the translated
phrase + the word “pas.” A parse tree node gets la-
beled with one of these rules if, roughly speaking,
its span is still contiguous when projected (via the
alignment) into the target language.
The advantage of using the GHKM interpretation
of a complete translation object is that our transla-
tion task becomes simpler. Now, to induce a com-
plete translation object from a partial translation ob-
ject (parse tree), all we need to do is label the nodes
of the tree with appropriate rules. We have reduced
the vaguely defined task of translation to the con-
crete task of tree labeling.
</bodyText>
<sectionHeader confidence="0.975532" genericHeader="method">
5 The Generative Process
</sectionHeader>
<bodyText confidence="0.999914344827586">
At the most basic level, we could design a naive gen-
erative process that takes a parse tree and then makes
a series of decisions, one for each node, about what
rule (if any) that node should be assigned. How-
ever it is a bit unreasonable to expect to learn such
a decision without breaking it down somewhat, as
there are an enormous number of rules that could po-
tentially be used to label any given parse tree node.
So let’s break this task down into simpler decisions.
Ideally, we would like to devise a generative process
consisting of decisions between a small number of
possibilities (ideally binary decisions).
We will begin by deciding, for each node, whether
or not it will be annotated with a rule. This is clearly
a binary decision. Once a generative process has
made this decision for each node, we get a conve-
nient byproduct. As seen in Figure 3, the LHS of
each rule is already determined. Hence after this se-
quence of binary decisions, half of our task is al-
ready completed.
The question remains: how do we determine the
RHS of these rules? Again, we could create a gen-
erative process that makes these decisions directly,
but choosing the RHS of a rule is still a rather wide-
open decision, so we will break it down further. For
each rule, we will begin by choosing the template of
its RHS, which is a RHS in which all sequences of
variables are replaced with an empty slot into which
variables can later be placed. For instance, the tem-
</bodyText>
<page confidence="0.998175">
35
</page>
<figureCaption confidence="0.910033">
Figure 3: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify
the rule node, the rule left-hand sides are already determined.
</figureCaption>
<bodyText confidence="0.999908071428571">
plate of (“ne”, x1, “pas”) is (“ne”, X, “pas”) and the
template of (x3, “,”, x1, x2) is (X,“,”, X), where X
represents the empty slots.
Once the template is chosen, it simply needs to be
filled with the variables from the LHS. To do so, we
process the LHS variables, one by one. By default,
they are placed to the right of the previously placed
variable (the first variable is placed in the first slot).
We repeatedly offer the option to push the variable
to the right until the option is declined or it is no
longer possible to push it further right. If the vari-
able was not pushed right at all, we repeatedly offer
the option to push the variable to the left until the
option is declined or it is no longer possible to push
it further left. Figure 4 shows this generative story
in action for the rule RHS (x3, “,”, x1, x2).
These are all of the decisions we need to make
in order to label a parse tree with GHKM rules. A
trace of this generative process for the GHKM tree
of Figure 2 is shown in Figure 5. Notice that, aside
from the template decisions, all of the decisions are
binary (i.e. feasible to learn discriminatively). Even
the template decisions are not terribly large-domain,
if we maintain a separate feature-conditional dis-
tribution for each LHS template. For instance, if
the LHS template is (“not”, X), then RHS template
(“ne”, X, “pas”) and a few other select candidates
should bear most of the probability mass.
</bodyText>
<subsectionHeader confidence="0.955707">
5.1 Training
</subsectionHeader>
<bodyText confidence="0.9984105">
Having established this generative story, training is
straightforward. As a first step, we can convert each
complete translation object of our training corpus
to the trace of its generative story (as in Figure 5).
</bodyText>
<figureCaption confidence="0.679009">
Decision to make Decision RHS so far
RHS template? X , X X , X
default placement of var 1 1 , X
push var 1 right? yes X , 1
default placement of var 2 X , 1 2
push var 2 left? no X , 1 2
default placement of var 3 X , 1 2 3
push var 3 left? yes X , 1 3 2
push var 3 left? yes X , 3 1 2
push var 3 left? yes 3 , 1 2
Figure 4: Trace of the generative story for the right-
hand side of a GHKM rule.
</figureCaption>
<bodyText confidence="0.999974909090909">
These decisions can be annotated with whatever fea-
ture information we might deem helpful. Then we
simply divide up these feature vectors by decision
type (for instance, rule node decisions, template de-
cisions, etc.) and train a separate discriminative clas-
sifier for each decision type from the feature vectors.
This method is quite flexible, in that it allows us to
use any generic off-the-shelf classification software
to train our system. We prefer learners that produce
distributions (rather than hard classifiers) as output,
but this is not required.
</bodyText>
<subsectionHeader confidence="0.997638">
5.2 Exploiting deep linguistic information
</subsectionHeader>
<bodyText confidence="0.999900555555556">
The use of discriminative classifiers makes our ap-
proach very flexible in terms of the information that
can be exploited in the labeling (or translation) pro-
cess. Any information that can be encoded as fea-
tures relative to GHKM tree nodes can be used. For
the experiments reported in this paper, we parsed
the source language side of a parallel corpus (a
small subsection of the English-German Europarl
corpus; (Koehn, 2002)) with the XLE system, using
</bodyText>
<page confidence="0.991623">
36
</page>
<bodyText confidence="0.999860333333333">
the ParGram LFG grammar and applying probabilis-
tic disambiguation (Riezler et al., 2002) to obtain
a single analysis (i.e., a c-structure [phrase struc-
ture tree] and an f-structure [an associated attribute-
value matrix with morphosyntactic feature informa-
tion and a shallow semantic interpretation]) for each
sentence. A fall-back mechanism integrated in the
parser/grammar ensures that even for sentences that
do not receive a full parse, substrings are deeply
parsed and can often be treated successfully.
We convert the c-structure/f-structure represen-
tation that is based on XLE’s sophisticated word-
internal analysis into a plain phrase structure tree
representation based on the original tokens in the
source language string. The morphosyntactic fea-
ture information from f-structure is copied as addi-
tional labeling information to the relevant GHKM
tree nodes, and the f-structural dependency relation
among linguistic units is translated into a relation
among corresponding GHKM tree nodes. The rela-
tional information is then used to systematically ex-
tend the learning feature set for the tree-node based
classifiers.
In future experiments, we also plan to exploit lin-
guistic knowledge about the target language by fac-
torizing the generation of target language words into
separate generation of lemmas and the various mor-
phosyntactic features. In decoding, a morphological
generator will be used to generate a string of surface
words.
</bodyText>
<subsectionHeader confidence="0.991978">
5.3 Decoding
</subsectionHeader>
<bodyText confidence="0.999944259259259">
Because we have purposely refused to make any
Markov assumptions in our model, decoding cannot
be accomplished in polynomial time. Our hypothe-
sis is that it is better to find a suboptimal solution of
a high-quality model than the optimal solution of a
poorer model. We decode through a simple search
through the space of assignments to our generative
process.
This is, potentially, a very large and intractible
search space. However, if most assignment deci-
sions can be made with relative confidence (i.e. the
classifiers we have trained make fairly certain deci-
sions), then the great majority of search nodes have
values which are inferior to those of the best so-
lutions. The standard search technique of depth-
first branch-and-bound search takes advantage of
search spaces with this particular characteristic by
first finding greedy good-quality solutions and using
their values to optimally prune a significant portion
of the search space. Depth-first branch-and-bound
search has the following advantage: it finds a good
(suboptimal) solution in linear time and continually
improves on this solution until it finds the optimal.
Thus it can be run either as an optimal decoder or as
a heuristic decoder, since we can interrupt its execu-
tion at any time to get the best solution found so far.
Additionally, it takes only linear space to run.
</bodyText>
<sectionHeader confidence="0.988211" genericHeader="method">
6 Preliminary results
</sectionHeader>
<bodyText confidence="0.999619852941176">
In this section, we present some preliminary results
for an English-to-German translation system based
on the ideas outlined in this paper.
Our data was a subset of the Europarl corpus
consisting of sentences of lengths ranging from 8
to 17 words. Our training corpus contained 50000
sentences and our test corpus contained 300 sen-
tences. We also had a small number of reserved
sentences for development. The English sentences
were parsed with XLE, using the English ParGram
LFG grammar, and the sentences were word-aligned
with GIZA++. We used the WEKA machine learn-
ing package (Witten and Frank, 2005) to train the
distributions (specifically, we used model trees).
For comparison, we also trained and evaluated
the phrase-based MT system Pharaoh (Koehn, 2005)
on this limited corpus, using Pharaoh’s default pa-
rameters. In a different set of MT-as-Tree-Labeling
experiments, we used a standard treebank parser
trained on the PennTreebank Wall Street Journal
section. Even with this parser, which produces less
detailed information than XLE, the results are com-
petitive when assessed with quantitative measures:
Pharaoh achieved a BLEU score of 11.17 on the test
set, whereas our system achieved a BLEU score of
11.52. What is notable here is not the scores them-
selves (low due to the size of the training corpus).
However our system managed to perform compara-
bly with Pharaoh in a very early stage of its devel-
opment, with rudimentary features and without the
benefit of an n-gram language model.
For the XLE-based system we cannot include
quantitative results for the same experimental setup
at the present time. As a preliminary qualitative
</bodyText>
<page confidence="0.997642">
37
</page>
<table confidence="0.999625523809524">
Decision to make Decision Active features
rule node (i)? YES NT=“S”; HEAD = “am”
rule node (ii)? YES NT=“NP”; HEAD = “I”
rule node (iv)? NO NT=“VP”; HEAD = “am”
rule node (v)? YES NT=“VP”; HEAD = “am”
rule node (vi)? NO NT=“MD”; HEAD = “am”
rule node (viii)? YES NT=“VP”; HEAD = “going”
rule node (ix)? NO NT=“RB”; HEAD = “not”
rule node (xi)? YES NT=“VB”; HEAD = “going”
rule node (xiii)? YES NT=“ADJP”; HEAD = “today”
RHS template? (i) X , X NT=“S”
push var 1 right? (i) YES VARNT=“NP”; PUSHPAST= “,”
push var 2 left? (i) NO VARNT=“VP”; PUSHPAST= “NP”
push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “VP”
push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “NP”
push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “,”
RHS template? (ii) je NT=“NP”; WD=“I”
RHS template? (v) X NT=“VP”
RHS template? (viii) ne X pas NT=“VP”; WD=“not”
RHS template? (xi) vais NT=“VB”; WD=“going”
RHS template? (xiii) aujourd’hui NT=“ADJP”; WD=“today”
</table>
<figureCaption confidence="0.999123">
Figure 5: Trace of a top-down generative story for the GHKM tree in Figure 2.
</figureCaption>
<bodyText confidence="0.95341225">
evaluation, let’s take a closer look at the sentences
produced by our system, to gain some insight as to
its current strengths and weaknesses.
Starting with the English sentence (1) (note that
all data is lowercase), our system produces (2).
(1) i agree with the spirit of those amendments .
¨anderungsantr¨age
change-proposals
zu .
to .
The GHKM tree is depicted in Figure 6. The key
feature of this translation is how the English phrase
“agree with” is translated as the German “stimme
... zu” construction. Such a feat is difficult to pro-
duce consistently with a purely phrase-based sys-
tem, as phrases of arbitrary length can be placed be-
tween the words “stimme” and “zu”, as we can see
happening in this particular example. By contrast,
Pharaoh opts for the following (somewhat less de-
sirable) translation:
</bodyText>
<figure confidence="0.51919925">
(3) ich stimme mit dem geist dieser
I vote with the.MASC spirit.MASC these
¨anderungsantr¨age .
change-proposals .
</figure>
<bodyText confidence="0.9998590625">
A weakness in our system is also evident here.
The German noun “Geist” is masculine, thus our
system uses the wrong article (a problem that
Pharaoh, with its embedded n-gram language model,
does not encounter).
In general, it seems that our system is superior to
Pharaoh at figuring out the proper way to arrange the
words of the output sentence, and inferior to Pharaoh
at finding what the actual translation of those words
should be.
Consider the English sentence (4). Here we have
an example of a modal verb with an embedded in-
finitival VP. In German, infinite verbs should go at
the end of the sentence, and this is achieved by our
system (translating “shall” as “werden”, and “sub-
mit” as “vorlegen”), as is seen in (5).
</bodyText>
<listItem confidence="0.981763">
(4) ) we shall submit a proposal along these lines before the
end of this year.
(5) wir werden eine vorschlag
</listItem>
<bodyText confidence="0.605339285714286">
we will a.FEM proposal.MASC
haushaltslinien vor die ende
budget-lines before the.FEM end.NEUT
jahres
year.NEUT
Pharaoh does not manage this (translating “sub-
mit” as “unterbreiten” and placing it mid-sentence).
</bodyText>
<figure confidence="0.31286175">
eine vorschlag in dieser
a proposal in these
dieser jahr .
this.FEM year.NEUT .
</figure>
<figureCaption confidence="0.1990965">
It is worth noting that while our system gets the
word order of the output system right, it makes sev-
</figureCaption>
<figure confidence="0.988849875">
(2) ich stimme die geist dieser
I vote the.FEM spirit.MASC these
in dieser
in these
dieser
this.FEM
vorlegen .
submit .
(6) werden wir unterbreiten
will we submit
haushaltslinien
budget-lines
ende
end
vor
before
</figure>
<page confidence="0.880593">
38
</page>
<figureCaption confidence="0.999641">
Figure 6: GHKM tree output for a test sentence.
</figureCaption>
<bodyText confidence="0.99888525">
eral agreement mistakes and (like Pharaoh) doesn’t
get the translation of “along these lines” right.
In Figure 7, we show sample translations by the
three systems under discussion for the first five sen-
tences in our evaluation set. For the LFG-based ap-
proach, we can at this point present only results for
a version trained on 10% of the sentence pairs. This
explains why more source words are left untrans-
lated. But note that despite the small training set,
the word ordering results are clearly superior for this
system: the syntax-driven rules place the untrans-
lated English words in the correct position in terms
of German syntax.
The translations with Pharaoh contain relatively
few agreement mistakes (note that it exploits a lan-
guage model of German trained on a much larger
corpus). The phrase-based approach does however
skip words and make positioning mistakes some of
which are so serious (like in the last sentence) that
they make the result hard to understand.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999983178571428">
In describing this pilot project, we have attempted
to give a “big picture” view of the essential ideas
behind our system. To avoid obscuring the presen-
tation, we have avoided many of the implementation
details, in particular our choice of features. There
are exactly four types of decisions that we need to
train: (1) whether a parse tree node should be a rule
node, (2) the RHS template of a rule, (3) whether a
rule variable should be pushed left, and (4) whether
a rule variable should be pushed right. For each
of these decisions, there are a number of possible
features that suggest themselves. For instance, re-
call that in German, embedded infinitival verbs get
placed at the end of the sentence or clause. So
when the system is considering whether to push a
rule’s noun phrase to the left, past an existing verb,
it would be useful for it to consider (as a feature)
whether that verb is the first or second verb of its
clause and what the morphological form of the verb
is.
Even in these early stages of development, the
MT-as-Tree-Labeling system shows promise in us-
ing syntactic information flexibly and effectively for
MT. Our preliminary comparison indicates that us-
ing deep syntactic analysis leads to improved trans-
lation behavior. We hope to develop the system
into a competitive alternative to phrase-based ap-
proaches.
</bodyText>
<sectionHeader confidence="0.999327" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998940166666667">
P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mer-
cer. 1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational Linguistics,
19(2):263–311.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Ma-
suichi, and Christian Rohrer. 2002. The parallel gram-
</reference>
<page confidence="0.998779">
39
</page>
<bodyText confidence="0.711824771428571">
source we believe that this is a fundamental element .
professional translation wir denken , dass dies ein grundlegender aspekt ist .
PHARAOH (50k) einegrundlegende element.
wir halten dies f¨ur ���
TL-WSJ (50k) diesen ist ein grundlegendes element.
wir glauben , dass �����
TL-LFG (5k) wir meinen , dass dies einegrundlegende element ist .
source it is true that lisbon is a programme for ten years .
professional translation nun ist lissabon ein programm f¨ur zehn jahre .
PHARAOH (50k) jahren .
eine programm f¨ur zehn �����
es ist richtig , dass lissabon ist ���
TL-WSJ (50k) jahren .
eine programm f¨ur zehn �����
es ist richtig , dass lissabon ist ���
TL-LFG (5k) jahren ist .
eine programm f¨ur zehn �����
es ist true , dass lisbon ���
source i completely agree with each of these points.
professional translation ich bin mit jeder einzelnen dieser aussagen voll und ganz einverstanden .
PHARAOH (50k) ich v¨ollig einverstanden mit jedem dieser punkte .
TL-WSJ (50k) diese fragen einer meinung .
ich bin v¨ollig mit jedes�����
TL-LFG (5k) jeder dieser punkte .
ich agree completely mit����
source however, i would like to add one point.
professional translation aber ich m¨ochte gern einen punkt hinzuf¨ugen .
PHARAOH (50k) allerdings m¨ochte ich noch eines sagen .
TL-WSJ (50k) ich m¨ochte jedoch an noch einen punkt hinzuf¨ugen .
TL-LFG (5k) allerdings m¨ochte ich einen punkt add.
source this is undoubtedly a point which warrants attention.
professional translation ohne jeden zweifel ist dies ein punkt, der aufmerksamkeit verdient .
PHARAOH (50k) das ist sicherlich eine punkt .... rechtfertigt das aufmerksamkeit .
TL-WSJ (50k) das ist ohne zweifel eine punkt , ��die warrants beachtung .
TL-LFG (5k) das ist undoubtedly .... sache , die attention warrants.
</bodyText>
<figureCaption confidence="0.970725">
Figure 7: Sample translations by (1) the PHARAOH system, (2) our system with a treebank parser (TL-WSJ),
(3) our system with the XLE parser (TL-LFG). (1) and (2) were trained on 50,000 sentence pairs, (3) just
on (3) sentence pairs. Error coding: ������wrong���������������morphological�����form, incorrectly positioned word, untranslated
source word, missed word:...., extra word.
</figureCaption>
<reference confidence="0.99841469047619">
mar project. In Proceedings of COLING-2002 Workshop on
Grammar Engineering and Evaluation, pages 1–7.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings ofACL, pages
263–270.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2003. What’s in a translation rule? In Proc. NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of the Hu-
man Language Technology Conference 2003 (HLT-NAACL
2003), Edmonton, Canada.
Philipp Koehn. 2002. Europarl: A multilingual corpus for eval-
uation of machine translation. Ms., University of Southern
California.
Philipp Koehn. 2005. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In Pro-
ceedings of the Sixth Conference of the Association for Ma-
chine Translation in the Americas, pages 115–124.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren
Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical ma-
chine translation. Technical report, Center for Language and
Speech Processing, Johns Hopkins University, Baltimore.
Summer Workshop Final Report.
Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John
Maxwell, and Mark Johnson. 2002. Parsing the Wall Street
Journal using a Lexical-Functional Grammar and discrim-
inative estimation techniques. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL’02), Pennsylvania, Philadelphia.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377–403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statis-
tical translation model. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguistics,
pages 523–530.
</reference>
<page confidence="0.998622">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000221">
<title confidence="0.999389">Deep Grammars in a Tree Labeling Approach Syntax-based Statistical Machine Translation</title>
<author confidence="0.999266">Mark</author>
<affiliation confidence="0.9998175">Department of University of Potsdam,</affiliation>
<email confidence="0.994965">hopkins@ling.uni-potsdam.de</email>
<abstract confidence="0.996551416666666">In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a treelabeling task, which is further decomposed into a sequence of simple decisions for which discriminative classifiers can be trained. The approach is very flexible and we believe that it is particularly well-suited for exploiting the linguistic knowledge encoded in deep grammars whenever possible, while at the same time taking advantage of data-based techniques that have proven a powerful basis for MT, as recent advances in statistical MT show. A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-to- German translation with a tree-labeling system trained on a small subsection of the Europarl corpus. 1 Motivation Machine translation (MT) is probably the oldest application of what we call deep linguistic processing techniques today. But from its inception, there have been alternative considerations of approaching the task with data-based statistical techniques (cf. Warren Weaver’s well-known memo from 1949). Only with fairly recent advances in computer technology have researchers been able to build effective statistical MT prototypes, but in the last few years, the statistical approach has received enormous research interest and made significant progress.</abstract>
<author confidence="0.961633">Jonas</author>
<affiliation confidence="0.999446">Department of University of Potsdam,</affiliation>
<email confidence="0.9465">kuhn@ling.uni-potsdam.de</email>
<abstract confidence="0.98452692293578">The most successful statistical MT paradigm has been, for a while now, the so-call phrase-based MT approach (Och and Ney, 2003). In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (“phrases”) from the source language for word sequences in the target language. Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text (Brown et al., 1993) for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language. In decoding, i.e. the application of the acquired translation model to unseen source sentences, these systems then typically rely on ngram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language. An obvious advantage of statistical MT approaches is that they can adopt (often very idiomatic) translations of midto high-frequency constructions without requiring any language-pair specific engineering work. At the same time it is clear that a linguistics-free approach is limited in what it can ultimately achieve: only linguistically informed systems can detect certain generalizations from lower-frequency constructions in the data and successfully apply them in a similar but different linguistic context. Hence, the idea of “hybrid” MT, exploiting both linguistic and statistical information is fairly old. Here we will not consider classical, rulebased systems with some added data-based resource acquisition (although they may be among the best candidates for high-quality special-purpose translation – but adaption to new language pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 of the ACL 2007 Workshop on Deep Linguistic pages 33–40, Czech Republic, June, 2007. Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous (Koehn et al., 2003; Och et al., and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from rich linguistic representations obtained from deep grammars like the ParGram LFGs. Nevertheless, contrary to classical interlingual or deep transfer-based systems, the generative stochastic model that drives our system is grounded only in the cross-language word alignment and a surface-based phrase structure tree for the source language and will thus degrade gracefully on input with parsing issues – which we suspect is an important feature for making the overall system competitive with the highly general phrasebased MT approach. Preliminary evaluation of our nascent system indicates that this new approach might well have the potential to finally realize some of the promises of syntax in statistical MT. 2 General Task We want to build a system that can learn to translate sentences from a source language to a destination language. The general set-up is simple. Firstly, we have a training corpus of paired senwhere target sentence a gold translation of source sentence These sentence pairs are annotated with auxiliary information, which can include word alignments and syntactic information. We refer to these annotated sentence as translation Secondly, we have an evaluation corpus of source sentences. These sentences are annotated with a subset of the auxiliary information used to annotate the 2005) also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesn’t lead to any improvements. Figure 1: Example translation object. training corpus. We refer to these partially annotated sentences as translation The task at hand: use the training corpus to learn a procedure, through which we can successfully induce a complete translation object from a partial translation object. This is what we will define as 3 Specific Task Addressed by this Paper Before going on to actually describe a translation procedure (and how to induce it), we need to specify our prior assumptions about how the translation objects will be annotated. For this paper, we want to exploit the syntax information that we can gain from an LFG-parser, hence we will assume the following annotations: (1) In the training and evaluation corpora, the source sentences will be parsed with the XLEparser. The attribute-value information from LFG’s f-structure is restructured so it is indexed by (cstructure) tree nodes; thus a tree node can bear multiple labels for various pieces of morphological, syntactic and semantic information. (2) In the training corpus, the source and target sentence of every translation object will be aligned using GIZA++ (http://www.fjoch.com/). In other words, our complete translation objects will be aligned tree-string pairs (for instance, Figure 1), while our partial translation objects will be trees (the tree portion of Figure 1). No other annotations will be assumed for this paper. 34 Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the GHKM tree. 4 Syntax MT as Tree Labeling It is not immediately clear how one would learn a process to map a parsed source sentence into an aligned tree-string pair. To facilitate matters, we will map complete translation objects to an alternate representation. In (Galley et al., 2003), the authors give a semantics to aligned tree-string pairs by associating each with an annotated parse tree (hereafter a representing a specific theory about how the source sentence was translated into the destination sentence. In Figure 1, we show an example translation object and in Figure 2, we show its associated GHKM The GHKM tree is simply the parse tree the translation object, annotated with rules (hereafter to as We will not describe in depth the mapping process from translation object to GHKM tree. Suffice it to say that the alignment induces a set of intuitive translation rules. Essentially, rule like: “not 1 1 pas” (see Figure 2) means: if we see the word “not” in English, followed by a phrase already translated into French, then translate the entire thing as the word “ne” + the translated phrase + the word “pas.” A parse tree node gets labeled with one of these rules if, roughly speaking, its span is still contiguous when projected (via the alignment) into the target language. The advantage of using the GHKM interpretation of a complete translation object is that our translation task becomes simpler. Now, to induce a complete translation object from a partial translation object (parse tree), all we need to do is label the nodes of the tree with appropriate rules. We have reduced the vaguely defined task of translation to the concrete task of tree labeling. 5 The Generative Process At the most basic level, we could design a naive generative process that takes a parse tree and then makes a series of decisions, one for each node, about what rule (if any) that node should be assigned. However it is a bit unreasonable to expect to learn such a decision without breaking it down somewhat, as there are an enormous number of rules that could potentially be used to label any given parse tree node. So let’s break this task down into simpler decisions. Ideally, we would like to devise a generative process consisting of decisions between a small number of possibilities (ideally binary decisions). We will begin by deciding, for each node, whether or not it will be annotated with a rule. This is clearly a binary decision. Once a generative process has made this decision for each node, we get a convenient byproduct. As seen in Figure 3, the LHS of each rule is already determined. Hence after this sequence of binary decisions, half of our task is already completed. The question remains: how do we determine the RHS of these rules? Again, we could create a generative process that makes these decisions directly, but choosing the RHS of a rule is still a rather wideopen decision, so we will break it down further. For rule, we will begin by choosing the its RHS, which is a RHS in which all sequences of variables are replaced with an empty slot into which can later be placed. For instance, the tem- 35 Figure 3: Partial GHKM tree, after rule nodes have been identified (light gray). Notice that once we identify the rule node, the rule left-hand sides are already determined. of X, the of where X represents the empty slots. Once the template is chosen, it simply needs to be filled with the variables from the LHS. To do so, we process the LHS variables, one by one. By default, they are placed to the right of the previously placed variable (the first variable is placed in the first slot). We repeatedly offer the option to push the variable to the right until the option is declined or it is no longer possible to push it further right. If the variable was not pushed right at all, we repeatedly offer the option to push the variable to the left until the option is declined or it is no longer possible to push it further left. Figure 4 shows this generative story action for the rule RHS These are all of the decisions we need to make in order to label a parse tree with GHKM rules. A trace of this generative process for the GHKM tree of Figure 2 is shown in Figure 5. Notice that, aside from the template decisions, all of the decisions are binary (i.e. feasible to learn discriminatively). Even the template decisions are not terribly large-domain, if we maintain a separate feature-conditional distribution for each LHS template. For instance, if LHS template is then RHS template X, a few other select candidates should bear most of the probability mass. 5.1 Training Having established this generative story, training is straightforward. As a first step, we can convert each complete translation object of our training corpus to the trace of its generative story (as in Figure 5). Decision to make Decision RHS so far RHS template? X , X X , X default placement of var 1 1 , X push var 1 right? yes X , 1 default placement of var 2 X , 1 2 push var 2 left? no X , 1 2 default placement of var 3 X , 1 2 3 push var 3 left? yes X , 1 3 2 push var 3 left? yes X , 3 1 2 push var 3 left? yes 3 , 1 2 Figure 4: Trace of the generative story for the righthand side of a GHKM rule. These decisions can be annotated with whatever feature information we might deem helpful. Then we simply divide up these feature vectors by decision type (for instance, rule node decisions, template decisions, etc.) and train a separate discriminative classifier for each decision type from the feature vectors. This method is quite flexible, in that it allows us to use any generic off-the-shelf classification software to train our system. We prefer learners that produce distributions (rather than hard classifiers) as output, but this is not required. 5.2 Exploiting deep linguistic information The use of discriminative classifiers makes our approach very flexible in terms of the information that can be exploited in the labeling (or translation) process. Any information that can be encoded as features relative to GHKM tree nodes can be used. For the experiments reported in this paper, we parsed the source language side of a parallel corpus (a small subsection of the English-German Europarl corpus; (Koehn, 2002)) with the XLE system, using 36 the ParGram LFG grammar and applying probabilistic disambiguation (Riezler et al., 2002) to obtain a single analysis (i.e., a c-structure [phrase structure tree] and an f-structure [an associated attributevalue matrix with morphosyntactic feature information and a shallow semantic interpretation]) for each sentence. A fall-back mechanism integrated in the parser/grammar ensures that even for sentences that do not receive a full parse, substrings are deeply parsed and can often be treated successfully. We convert the c-structure/f-structure representation that is based on XLE’s sophisticated wordinternal analysis into a plain phrase structure tree representation based on the original tokens in the source language string. The morphosyntactic feature information from f-structure is copied as additional labeling information to the relevant GHKM tree nodes, and the f-structural dependency relation among linguistic units is translated into a relation among corresponding GHKM tree nodes. The relational information is then used to systematically extend the learning feature set for the tree-node based classifiers. In future experiments, we also plan to exploit linguistic knowledge about the target language by factorizing the generation of target language words into separate generation of lemmas and the various morphosyntactic features. In decoding, a morphological generator will be used to generate a string of surface words. 5.3 Decoding Because we have purposely refused to make any Markov assumptions in our model, decoding cannot be accomplished in polynomial time. Our hypothesis is that it is better to find a suboptimal solution of a high-quality model than the optimal solution of a poorer model. We decode through a simple search through the space of assignments to our generative process. This is, potentially, a very large and intractible search space. However, if most assignment decisions can be made with relative confidence (i.e. the classifiers we have trained make fairly certain decisions), then the great majority of search nodes have values which are inferior to those of the best so- The standard search technique of depthbranch-and-bound search advantage of search spaces with this particular characteristic by first finding greedy good-quality solutions and using their values to optimally prune a significant portion of the search space. Depth-first branch-and-bound search has the following advantage: it finds a good (suboptimal) solution in linear time and continually improves on this solution until it finds the optimal. Thus it can be run either as an optimal decoder or as a heuristic decoder, since we can interrupt its execution at any time to get the best solution found so far. Additionally, it takes only linear space to run. 6 Preliminary results In this section, we present some preliminary results for an English-to-German translation system based on the ideas outlined in this paper. Our data was a subset of the Europarl corpus consisting of sentences of lengths ranging from 8 to 17 words. Our training corpus contained 50000 sentences and our test corpus contained 300 sentences. We also had a small number of reserved sentences for development. The English sentences were parsed with XLE, using the English ParGram LFG grammar, and the sentences were word-aligned with GIZA++. We used the WEKA machine learning package (Witten and Frank, 2005) to train the distributions (specifically, we used model trees). For comparison, we also trained and evaluated the phrase-based MT system Pharaoh (Koehn, 2005) on this limited corpus, using Pharaoh’s default parameters. In a different set of MT-as-Tree-Labeling experiments, we used a standard treebank parser trained on the PennTreebank Wall Street Journal section. Even with this parser, which produces less detailed information than XLE, the results are competitive when assessed with quantitative measures: Pharaoh achieved a BLEU score of 11.17 on the test set, whereas our system achieved a BLEU score of 11.52. What is notable here is not the scores themselves (low due to the size of the training corpus). However our system managed to perform comparably with Pharaoh in a very early stage of its development, with rudimentary features and without the benefit of an n-gram language model. For the XLE-based system we cannot include quantitative results for the same experimental setup at the present time. As a preliminary qualitative 37 Decision to make Decision Active features rule node (i)? YES NT=“S”; HEAD = “am” rule node (ii)? YES NT=“NP”; HEAD = “I” rule node (iv)? NO NT=“VP”; HEAD = “am” rule node (v)? YES NT=“VP”; HEAD = “am” rule node (vi)? NO NT=“MD”; HEAD = “am” rule node (viii)? YES NT=“VP”; HEAD = “going” rule node (ix)? NO NT=“RB”; HEAD = “not” rule node (xi)? YES NT=“VB”; HEAD = “going” rule node (xiii)? YES NT=“ADJP”; HEAD = “today” RHS template? (i) X , X NT=“S” push var 1 right? (i) YES VARNT=“NP”; PUSHPAST= “,” push var 2 left? (i) NO VARNT=“VP”; PUSHPAST= “NP” push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “VP” push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “NP” push var 3 left? (i) YES VARNT=“ADJP”; PUSHPAST= “,” RHS template? (ii) je NT=“NP”; WD=“I” RHS template? (v) X NT=“VP” RHS template? (viii) ne X pas NT=“VP”; WD=“not” RHS template? (xi) vais NT=“VB”; WD=“going” RHS template? (xiii) aujourd’hui NT=“ADJP”; WD=“today” Figure 5: Trace of a top-down generative story for the GHKM tree in Figure 2. evaluation, let’s take a closer look at the sentences produced by our system, to gain some insight as to its current strengths and weaknesses. Starting with the English sentence (1) (note that all data is lowercase), our system produces (2). (1) i agree with the spirit of those amendments . ¨anderungsantr¨age change-proposals zu . to . The GHKM tree is depicted in Figure 6. The key feature of this translation is how the English phrase “agree with” is translated as the German “stimme ... zu” construction. Such a feat is difficult to produce consistently with a purely phrase-based system, as phrases of arbitrary length can be placed between the words “stimme” and “zu”, as we can see happening in this particular example. By contrast, Pharaoh opts for the following (somewhat less desirable) translation: (3) ich stimme mit dem geist dieser I vote with the.MASC spirit.MASC ¨anderungsantr¨age . change-proposals . A weakness in our system is also evident here. The German noun “Geist” is masculine, thus our system uses the wrong article (a problem that Pharaoh, with its embedded n-gram language model, does not encounter). In general, it seems that our system is superior to Pharaoh at figuring out the proper way to arrange the words of the output sentence, and inferior to Pharaoh at finding what the actual translation of those words should be. Consider the English sentence (4). Here we have an example of a modal verb with an embedded infinitival VP. In German, infinite verbs should go at the end of the sentence, and this is achieved by our system (translating “shall” as “werden”, and “submit” as “vorlegen”), as is seen in (5). (4) ) we shall submit a proposal along these lines before the end of this year. (5) wir werden eine vorschlag we will a.FEM proposal.MASC haushaltslinien vor die ende budget-lines before the.FEM end.NEUT jahres year.NEUT Pharaoh does not manage this (translating “submit” as “unterbreiten” and placing it mid-sentence). eine vorschlag in dieser a proposal in these dieser jahr this.FEM year.NEUT . It is worth noting that while our system gets the order of the output system right, it makes sev- (2) ich stimme vote die the.FEM geist dieser these I spirit.MASC in dieser in these dieser this.FEM vorlegen submit . . (6) werden wir unterbreiten will we submit haushaltslinien budget-lines ende end vor before 38 Figure 6: GHKM tree output for a test sentence. eral agreement mistakes and (like Pharaoh) doesn’t get the translation of “along these lines” right. In Figure 7, we show sample translations by the three systems under discussion for the first five sentences in our evaluation set. For the LFG-based approach, we can at this point present only results for a version trained on 10% of the sentence pairs. This explains why more source words are left untranslated. But note that despite the small training set, the word ordering results are clearly superior for this system: the syntax-driven rules place the untranslated English words in the correct position in terms of German syntax. The translations with Pharaoh contain relatively few agreement mistakes (note that it exploits a language model of German trained on a much larger corpus). The phrase-based approach does however skip words and make positioning mistakes some of which are so serious (like in the last sentence) that they make the result hard to understand. 7 Discussion In describing this pilot project, we have attempted to give a “big picture” view of the essential ideas behind our system. To avoid obscuring the presentation, we have avoided many of the implementation details, in particular our choice of features. There are exactly four types of decisions that we need to train: (1) whether a parse tree node should be a rule node, (2) the RHS template of a rule, (3) whether a rule variable should be pushed left, and (4) whether a rule variable should be pushed right. For each of these decisions, there are a number of possible features that suggest themselves. For instance, recall that in German, embedded infinitival verbs get placed at the end of the sentence or clause. So when the system is considering whether to push a rule’s noun phrase to the left, past an existing verb, it would be useful for it to consider (as a feature) whether that verb is the first or second verb of its clause and what the morphological form of the verb is. Even in these early stages of development, the MT-as-Tree-Labeling system shows promise in using syntactic information flexibly and effectively for MT. Our preliminary comparison indicates that using deep syntactic analysis leads to improved translation behavior. We hope to develop the system into a competitive alternative to phrase-based approaches. References P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine trans- Parameter estimation. 19(2):263–311. Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Maand Christian Rohrer. 2002. The parallel gram- 39 source we believe that this is a fundamental element . professional translation wir denken , dass dies ein grundlegender aspekt ist . einegrundlegende element. halten dies f¨ur diesen ist ein grundlegendes element. glauben , dass wir meinen , dass dies einegrundlegende element ist . source it is true that lisbon is a programme for ten years . professional translation nun ist lissabon ein programm f¨ur zehn jahre . jahren . programm f¨ur zehn ist richtig , dass lissabon ist jahren . programm f¨ur zehn ist richtig , dass lissabon ist jahren ist . programm f¨ur zehn ist dass source i completely agree with each of these points. professional translation ich bin mit jeder einzelnen dieser aussagen voll und ganz einverstanden . ich v¨ollig einverstanden mit jedem dieser punkte . diese fragen einer meinung . bin v¨ollig mit jeder dieser punkte . completely source however, i would like to add one point. professional translation aber ich m¨ochte gern einen punkt hinzuf¨ugen . allerdings m¨ochte ich noch eines sagen . ich m¨ochte jedoch an noch einen punkt hinzuf¨ugen . m¨ochte ich einen punkt source this is undoubtedly a point which warrants attention. professional translation ohne jeden zweifel ist dies ein punkt, der aufmerksamkeit verdient . das ist sicherlich eine punkt .... rechtfertigt das aufmerksamkeit . ist ohne punkt , warrants . ist sache , die 7: Sample translations by (1) the (2) our system with a treebank parser our system with the XLE parser (1) and (2) were trained on 50,000 sentence pairs, (3) just (3) sentence pairs. Error coding: positioned word,untranslated missed word:...., extra word. project. In of COLING-2002 Workshop on Engineering and pages 1–7. David Chiang. 2005. A hierarchical phrase-based model for machine translation. In pages 263–270. Michel Galley, Mark Hopkins, Kevin Knight, and Daniel 2003. What’s in a translation rule? In Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Staphrase-based translation. In of the Hu-</abstract>
<address confidence="0.5563895">man Language Technology Conference 2003 (HLT-NAACL Edmonton, Canada.</address>
<author confidence="0.829213">Europarl A multilingual corpus for eval-</author>
<affiliation confidence="0.928824">uation of machine translation. Ms., University of Southern</affiliation>
<address confidence="0.583238">California.</address>
<author confidence="0.84731">Pharaoh a beam search decoder for</author>
<note confidence="0.879821625">statistical machine translation models. In Proceedings of the Sixth Conference of the Association for Ma- Translation in the pages 115–124. Franz Josef Och and Hermann Ney. 2003. A systematic comof various statistical alignment models. Computa- 29(1):19–51. F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical machine translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report. Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discrimestimation techniques. In of the 40th</note>
<affiliation confidence="0.419579">Annual Meeting of the Association for Computational Lin-</affiliation>
<address confidence="0.480084">(ACL’02), Pennsylvania, H. Witten and Eibe Frank. 2005. Mining: Practical</address>
<abstract confidence="0.708433625">learning tools and Morgan Kaufmann. Dekai Wu. 1997. Stochastic inversion transduction grammars bilingual parsing of parallel corpora. 23(3):377–403. Kenji Yamada and Kevin Knight. 2001. A syntax-based statistranslation model. In of the 39th Annual of the Association for Computational pages 523–530.</abstract>
<intro confidence="0.672182">40</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2277" citStr="Brown et al., 1993" startWordPosition="337" endWordPosition="340">rogress. Jonas Kuhn Department of Linguistics University of Potsdam, Germany kuhn@ling.uni-potsdam.de The most successful statistical MT paradigm has been, for a while now, the so-call phrase-based MT approach (Och and Ney, 2003). In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (“phrases”) from the source language for word sequences in the target language. Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text (Brown et al., 1993) for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language. In decoding, i.e. the application of the acquired translation model to unseen source sentences, these systems then typically rely on ngram language models and simple statistical reordering models to shuffle the phrases into an order that is coherent in the target language. An obvious advantage of statistical MT approaches is that they can adopt (often very idiomatic) translations of mid- to high-frequency constructions without requiring any language-pair specific engineering wo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Helge Dyvik</author>
<author>Tracy Holloway King</author>
<author>Hiroshi Masuichi</author>
<author>Christian Rohrer</author>
</authors>
<title>The parallel grammar project.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-2002 Workshop on Grammar Engineering and Evaluation,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="929" citStr="Butt et al., 2002" startWordPosition="137" endWordPosition="140">reelabeling task, which is further decomposed into a sequence of simple decisions for which discriminative classifiers can be trained. The approach is very flexible and we believe that it is particularly well-suited for exploiting the linguistic knowledge encoded in deep grammars whenever possible, while at the same time taking advantage of data-based techniques that have proven a powerful basis for MT, as recent advances in statistical MT show. A full system using the Lexical-Functional Grammar (LFG) parsing system XLE and the grammars from the Parallel Grammar development project (ParGram; (Butt et al., 2002)) has been implemented, and we present preliminary results on English-toGerman translation with a tree-labeling system trained on a small subsection of the Europarl corpus. 1 Motivation Machine translation (MT) is probably the oldest application of what we call deep linguistic processing techniques today. But from its inception, there have been alternative considerations of approaching the task with data-based statistical techniques (cf. Warren Weaver’s well-known memo from 1949). Only with fairly recent advances in computer technology have researchers been able to build effective statistical </context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>Miriam Butt, Helge Dyvik, Tracy Holloway King, Hiroshi Masuichi, and Christian Rohrer. 2002. The parallel grammar project. In Proceedings of COLING-2002 Workshop on Grammar Engineering and Evaluation, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="5943" citStr="Chiang, 2005" startWordPosition="918" endWordPosition="919">to translate sentences from a source language to a destination language. The general set-up is simple. Firstly, we have a training corpus of paired sentences f and e, where target sentence e is a gold standard translation of source sentence f. These sentence pairs are annotated with auxiliary information, which can include word alignments and syntactic information. We refer to these annotated sentence pairs as complete translation objects. Secondly, we have an evaluation corpus of source sentences. These sentences are annotated with a subset of the auxiliary information used to annotate the 1(Chiang, 2005) also reports that with his hierarchical generalization of the phrase-based approach, the addition of parser information doesn’t lead to any improvements. Figure 1: Example translation object. training corpus. We refer to these partially annotated source sentences as partial translation objects. The task at hand: use the training corpus to learn a procedure, through which we can successfully induce a complete translation object from a partial translation object. This is what we will define as translation. 3 Specific Task Addressed by this Paper Before going on to actually describe a translatio</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings ofACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule? In</title>
<date>2003</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="7926" citStr="Galley et al., 2003" startWordPosition="1235" endWordPosition="1238">r complete translation objects will be aligned tree-string pairs (for instance, Figure 1), while our partial translation objects will be trees (the tree portion of Figure 1). No other annotations will be assumed for this paper. 34 Figure 2: GHKM tree equivalent of example translation object. The light gray nodes are rule nodes of the GHKM tree. 4 Syntax MT as Tree Labeling It is not immediately clear how one would learn a process to map a parsed source sentence into an aligned tree-string pair. To facilitate matters, we will map complete translation objects to an alternate representation. In (Galley et al., 2003), the authors give a semantics to aligned tree-string pairs by associating each with an annotated parse tree (hereafter called a GHKM tree) representing a specific theory about how the source sentence was translated into the destination sentence. In Figure 1, we show an example translation object and in Figure 2, we show its associated GHKM tree. The GHKM tree is simply the parse tree f of the translation object, annotated with rules (hereafter referred to as GHKM rules). We will not describe in depth the mapping process from translation object to GHKM tree. Suffice it to say that the alignmen</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2003</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2003. What’s in a translation rule? In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4127" citStr="Koehn et al., 2003" startWordPosition="622" endWordPosition="625">ge pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1, and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from rich linguistic representations obtained from deep grammars like the ParGram LFGs. Nevertheless, contrary to classical interlingual or deep transfer-based systems, t</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference 2003 (HLT-NAACL 2003), Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2002</date>
<institution>Ms., University of Southern California.</institution>
<contexts>
<context position="14102" citStr="Koehn, 2002" startWordPosition="2354" endWordPosition="2355"> software to train our system. We prefer learners that produce distributions (rather than hard classifiers) as output, but this is not required. 5.2 Exploiting deep linguistic information The use of discriminative classifiers makes our approach very flexible in terms of the information that can be exploited in the labeling (or translation) process. Any information that can be encoded as features relative to GHKM tree nodes can be used. For the experiments reported in this paper, we parsed the source language side of a parallel corpus (a small subsection of the English-German Europarl corpus; (Koehn, 2002)) with the XLE system, using 36 the ParGram LFG grammar and applying probabilistic disambiguation (Riezler et al., 2002) to obtain a single analysis (i.e., a c-structure [phrase structure tree] and an f-structure [an associated attributevalue matrix with morphosyntactic feature information and a shallow semantic interpretation]) for each sentence. A fall-back mechanism integrated in the parser/grammar ensures that even for sentences that do not receive a full parse, substrings are deeply parsed and can often be treated successfully. We convert the c-structure/f-structure representation that is</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. Europarl: A multilingual corpus for evaluation of machine translation. Ms., University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="17713" citStr="Koehn, 2005" startWordPosition="2919" endWordPosition="2920">as a subset of the Europarl corpus consisting of sentences of lengths ranging from 8 to 17 words. Our training corpus contained 50000 sentences and our test corpus contained 300 sentences. We also had a small number of reserved sentences for development. The English sentences were parsed with XLE, using the English ParGram LFG grammar, and the sentences were word-aligned with GIZA++. We used the WEKA machine learning package (Witten and Frank, 2005) to train the distributions (specifically, we used model trees). For comparison, we also trained and evaluated the phrase-based MT system Pharaoh (Koehn, 2005) on this limited corpus, using Pharaoh’s default parameters. In a different set of MT-as-Tree-Labeling experiments, we used a standard treebank parser trained on the PennTreebank Wall Street Journal section. Even with this parser, which produces less detailed information than XLE, the results are competitive when assessed with quantitative measures: Pharaoh achieved a BLEU score of 11.17 on the test set, whereas our system achieved a BLEU score of 11.52. What is notable here is not the scores themselves (low due to the size of the training corpus). However our system managed to perform compara</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1887" citStr="Och and Ney, 2003" startWordPosition="277" endWordPosition="280">e been alternative considerations of approaching the task with data-based statistical techniques (cf. Warren Weaver’s well-known memo from 1949). Only with fairly recent advances in computer technology have researchers been able to build effective statistical MT prototypes, but in the last few years, the statistical approach has received enormous research interest and made significant progress. Jonas Kuhn Department of Linguistics University of Potsdam, Germany kuhn@ling.uni-potsdam.de The most successful statistical MT paradigm has been, for a while now, the so-call phrase-based MT approach (Och and Ney, 2003). In this paradigm, sentences are translated from a source language to a target language through the repeated substitution of contiguous word sequences (“phrases”) from the source language for word sequences in the target language. Training of the phrase translation model builds on top of a standard statistical word alignment over the training corpus of parallel text (Brown et al., 1993) for identifying corresponding word blocks, assuming no further linguistic analysis of the source or target language. In decoding, i.e. the application of the acquired translation model to unseen source sentenc</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
</authors>
<location>Viren</location>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, </marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, Viren</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Jin Jain</author>
<author>D Radev</author>
</authors>
<title>Syntax for statistical machine translation.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</institution>
<marker>Jain, Radev, 2003</marker>
<rawString>Jain, Z.Jin, and D. Radev. 2003. Syntax for statistical machine translation. Technical report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore. Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Dick Crouch</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02),</booktitle>
<location>Pennsylvania, Philadelphia.</location>
<contexts>
<context position="14222" citStr="Riezler et al., 2002" startWordPosition="2371" endWordPosition="2374">output, but this is not required. 5.2 Exploiting deep linguistic information The use of discriminative classifiers makes our approach very flexible in terms of the information that can be exploited in the labeling (or translation) process. Any information that can be encoded as features relative to GHKM tree nodes can be used. For the experiments reported in this paper, we parsed the source language side of a parallel corpus (a small subsection of the English-German Europarl corpus; (Koehn, 2002)) with the XLE system, using 36 the ParGram LFG grammar and applying probabilistic disambiguation (Riezler et al., 2002) to obtain a single analysis (i.e., a c-structure [phrase structure tree] and an f-structure [an associated attributevalue matrix with morphosyntactic feature information and a shallow semantic interpretation]) for each sentence. A fall-back mechanism integrated in the parser/grammar ensures that even for sentences that do not receive a full parse, substrings are deeply parsed and can often be treated successfully. We convert the c-structure/f-structure representation that is based on XLE’s sophisticated wordinternal analysis into a plain phrase structure tree representation based on the origi</context>
</contexts>
<marker>Riezler, Crouch, Kaplan, King, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Dick Crouch, Ron Kaplan, Tracy King, John Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL’02), Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="17554" citStr="Witten and Frank, 2005" startWordPosition="2894" endWordPosition="2897">Preliminary results In this section, we present some preliminary results for an English-to-German translation system based on the ideas outlined in this paper. Our data was a subset of the Europarl corpus consisting of sentences of lengths ranging from 8 to 17 words. Our training corpus contained 50000 sentences and our test corpus contained 300 sentences. We also had a small number of reserved sentences for development. The English sentences were parsed with XLE, using the English ParGram LFG grammar, and the sentences were word-aligned with GIZA++. We used the WEKA machine learning package (Witten and Frank, 2005) to train the distributions (specifically, we used model trees). For comparison, we also trained and evaluated the phrase-based MT system Pharaoh (Koehn, 2005) on this limited corpus, using Pharaoh’s default parameters. In a different set of MT-as-Tree-Labeling experiments, we used a standard treebank parser trained on the PennTreebank Wall Street Journal section. Even with this parser, which produces less detailed information than XLE, the results are competitive when assessed with quantitative measures: Pharaoh achieved a BLEU score of 11.17 on the test set, whereas our system achieved a BLE</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3961" citStr="Wu, 1997" startWordPosition="600" endWordPosition="601">ed data-based resource acquisition (although they may be among the best candidates for high-quality special-purpose translation – but adaption to new language pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1, and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="3987" citStr="Yamada and Knight, 2001" startWordPosition="602" endWordPosition="605">sed resource acquisition (although they may be among the best candidates for high-quality special-purpose translation – but adaption to new language pairs and sublanguages is very costly for these systems). The other form of hybridization – a statistical MT model that is based on a deeper analysis of the syntactic 33 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 33–40, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics structure of a sentence – has also long been identified as a desirable objective in principle (consider (Wu, 1997; Yamada and Knight, 2001)). However, attempts to retrofit syntactic information into the phrase-based paradigm have not met with enormous success (Koehn et al., 2003; Och et al., 2003)1, and purely phrase-based MT systems continue to outperform these syntax/phrase-based hybrids. In this work, we try to make a fresh start with syntax-based statistical MT, discarding the phrasebased paradigm and designing a MT system from the ground up, using syntax as our central guiding star – besides the word alignment over a parallel corpus. Our approach is compatible with and can benefit substantially from rich linguistic represent</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>