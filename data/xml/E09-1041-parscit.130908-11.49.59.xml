<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041690">
<title confidence="0.9982665">
Learning-Based Named Entity Recognition for Morphologically-Rich,
Resource-Scarce Languages
</title>
<author confidence="0.92017">
Kazi Saidul Hasan and Md. Altaf ur Rahman and Vincent Ng
</author>
<affiliation confidence="0.970629">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.917743">
Richardson, TX 75083-0688
</address>
<email confidence="0.999725">
{saidul,altaf,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.997419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999597333333333">
Named entity recognition for morpholog-
ically rich, case-insensitive languages, in-
cluding the majority of semitic languages,
Iranian languages, and Indian languages,
is inherently more difficult than its English
counterpart. Worse still, progress on ma-
chine learning approaches to named entity
recognition for many of these languages
is currently hampered by the scarcity of
annotated data and the lack of an accu-
rate part-of-speech tagger. While it is
possible to rely on manually-constructed
gazetteers to combat data scarcity, this
gazetteer-centric approach has the poten-
tial weakness of creating irreproducible
results, since these name lists are not
publicly available in general. Motivated
in part by this concern, we present a
learning-based named entity recognizer
that does not rely on manually-constructed
gazetteers, using Bengali as our represen-
tative resource-scarce, morphologically-
rich language. Our recognizer achieves
a relative improvement of 7.5% in F-
measure over a baseline recognizer. Im-
provements arise from (1) using in-
duced affixes, (2) extracting information
from online lexical databases, and (3)
jointly modeling part-of-speech tagging
and named entity recognition.
</bodyText>
<sectionHeader confidence="0.999628" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998415">
While research in natural language processing has
gained a lot of momentum in the past several
decades, much of this research effort has been fo-
cusing on only a handful of politically-important
languages such as English, Chinese, and Arabic.
On the other hand, being the fifth most spoken lan-
guage1 with more than 200 million native speakers
residing mostly in Bangladesh and the Indian state
of West Bengal, Bengali has far less electronic
resources than the aforementioned languages. In
fact, a major obstacle to the automatic processing
of Bengali is the scarcity of annotated corpora.
One potential solution to the problem of data
scarcity is to hand-annotate a small amount of
data with the desired linguistic information and
then develop bootstrapping algorithms for com-
bining this small amount of labeled data with
a large amount of unlabeled data. In fact, co-
training (Blum and Mitchell, 1998) has been suc-
cessfully applied to English named entity recog-
nition (NER) (Collins &amp; Singer [henceforth C&amp;S]
(1999)). In C&amp;S’s approach, consecutive words
tagged as proper nouns are first identified as poten-
tial NEs, and each such NE is then labeled by com-
bining the outputs of two co-trained classifiers.
Unfortunately, there are practical difficulties in ap-
plying this technique to Bengali NER. First, one
of C&amp;S’s co-trained classifiers uses features based
on capitalization, but Bengali is case-insensitive.
Second, C&amp;S identify potential NEs based on
proper nouns, but unlike English, (1) proper noun
identification for Bengali is non-trivial, due to the
lack of capitalization; and (2) there does not ex-
ist an accurate Bengali part-of-speech (POS) tag-
ger for providing such information, owing to the
scarcity of annotated data for training the tagger.
In other words, Bengali NER is complicated not
only by the scarcity of annotated data, but also by
the lack of an accurate POS tagger. One could
imagine building a Bengali POS tagger using un-
</bodyText>
<footnote confidence="0.997823">
1See http://en.wikipedia.org/wiki/Bengali language.
</footnote>
<note confidence="0.9623055">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998216">
354
</page>
<bodyText confidence="0.999939627659575">
supervised induction techniques that have been
successfully developed for English (e.g., Sch¨utze
(1995), Clark (2003)), including the recently-
proposed prototype-driven approach (Haghighi
and Klein, 2006) and Bayesian approach (Gold-
water and Griffiths, 2007). The majority of these
approaches operate by clustering distributionally
similar words, but they are unlikely to work well
for Bengali for two reasons. First, Bengali is a
relatively free word order language, and hence
the distributional information collected for Ben-
gali words may not be as reliable as that for En-
glish words. Second, many closed-class words
that typically appear in the distributional repre-
sentation of an English word (e.g., prepositions
and particles such as “in” and “to”) are realized
as inflections in Bengali, and the absence of these
informative words implies that the context vector
may no longer capture sufficient information for
accurately clustering the Bengali words.
In view of the above problems, many learning-
based Bengali NE recognizers have relied heavily
on manually-constructed name lists for identify-
ing persons, organizations, and locations. There
are at least two weaknesses associated with this
gazetteer-centric approach. First, these name lists
are typically not publicly available, making it dif-
ficult to reproduce the results of these NE recog-
nizers. Second, it is not clear how comprehen-
sive these lists are. Relying on comprehensive lists
that comprise a large portion of the names in the
test set essentially reduces the NER problem to a
dictionary-lookup problem, which is arguably not
very interesting from a research perspective.
In addition, many existing learning-based Ben-
gali NE recognizers have several common weak-
nesses. First, they use as features pseudo-affixes,
which are created by extracting the first n and the
last n characters of a word (where 1 &lt; n &lt; 4)
(e.g., Dandapat et al. (2007)). While affixes en-
code essential grammatical information in Ben-
gali due to its morphological richness, this extrac-
tion method is arguably too ad-hoc and does not
cover many useful affixes. Second, they typically
adopt a pipelined NER architecture, performing
POS tagging prior to NER and encoding the result-
ing not-so-accurate POS information as a feature.
In other words, errors in POS tagging are propa-
gated to the NE recognizer via the POS feature,
thus limiting its performance.
Motivated in part by these weaknesses, we in-
vestigate how to improve a learning-based NE rec-
ognizer that does not rely on manually-constructed
gazetteers. Specifically, we investigate two learn-
ing architectures for our NER system. The first
one is the aforementioned pipelined architecture
in which the NE recognizer uses as features the
output of a POS tagger that is trained indepen-
dently of the recognizer. Unlike existing Bengali
POS and NE taggers, however, we examine two
new knowledge sources for training these taggers:
(1) affixes induced from an unannotated corpus
and (2) semantic class information extracted from
Wikipedia. In the second architecture, we jointly
learn the POS tagging and the NER tasks, allow-
ing features for one task to be accessible to the
other task during learning. The goal is to exam-
ine whether any benefits can be obtained via joint
modeling, which could address the error propaga-
tion problem with the pipelined architecture.
While we focus on Bengali NER in this pa-
per, none of the proposed techniques are language-
specific. In fact, we believe that these techniques
are of relevance and interest to the EACL com-
munity because they can be equally applicable to
the numerous resource-scarce European and Mid-
dle Eastern languages that share similar linguis-
tic and extra-linguistic properties as Bengali. For
instance, the majority of semitic languages and
Iranian languages are, like Bengali, morpholog-
ically productive; and many East European lan-
guages such as Czech and Polish resemble Bengali
in terms of not only their morphological richness,
but also their relatively free word order.
The rest of the paper is organized as follows.
In Section 2, we briefly describe the related work.
Sections 3 and 4 show how we induce affixes from
an unannotated corpus and extract semantic class
information from Wikipedia. In Sections 5 and
6, we train and evaluate a POS tagger and an NE
recognizer independently, augmenting the feature
set typically used for these two tasks with our new
knowledge sources. Finally, we describe and eval-
uate our joint model in Section 7.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999507666666667">
Cucerzan and Yarowsky (1999) exploit morpho-
logical and contextual patterns to propose a
language-independent solution to NER. They use
affixes based on the paradigm that named enti-
ties corresponding to a particular class have sim-
ilar morphological structure. Their bootstrapping
</bodyText>
<page confidence="0.998318">
355
</page>
<bodyText confidence="0.999946633333333">
approach is tested on Romanian, English, Greek,
Turkish, and Hindi. The recall for Hindi is the
lowest (27.84%) among the five languages, sug-
gesting that the lack of case information can sig-
nificantly complicate the NER task.
To investigate the role of gazetteers in NER,
Mikheev et al. (1999) combine grammar rules with
maximum entropy models and vary the gazetteer
size. Experimental results show that (1) the F-
scores for NE classes like person and organiza-
tion are still high without gazetteers, ranging from
85% to 92%; and (2) a small list of country names
can improve the low F-score for locations substan-
tially. It is worth noting that their recognizer re-
quires that the input data contain POS tags and
simple semantic tags, whereas ours automatically
acquires such linguistic information. In addition,
their approach uses part of the dataset to extend the
gazetteer. Therefore, the resulting gazetteer list is
specific to a particular domain; on the other hand,
our approach does not generate a domain-specific
list, since it makes use of Wikipedia articles.
Kozareva (2006) generates gazetteer lists for
person and location names from unlabeled data
using common patterns and a graph exploration
algorithm. The location pattern is essentially
a preposition followed by capitalized context
words. However, this approach is inadequate for a
morphologically-rich language like Bengali, since
prepositions are often realized as inflections.
</bodyText>
<sectionHeader confidence="0.994162" genericHeader="method">
3 Affix Induction
</sectionHeader>
<bodyText confidence="0.999943714285714">
Since Bengali is morphologically productive, a lot
of grammatical information about Bengali words
is expressed via affixes. Hence, these affixes could
serve as useful features for training POS and NE
taggers. In this section, we show how to induce
affixes from an unannotated corpus.
We rely on a simple idea proposed by Keshava
and Pitler (2006) for inducing affixes. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) α
and Q are two character sequences, and (3) αQ is
the concatenation of α and Q. If αQ and α are
found in V , we extract Q as a suffix. Similarly, if
αQ and Q are found in V , we extract α as a prefix.
In principle, we can use all of the induced af-
fixes as features for training a POS tagger and an
NE recognizer. However, we choose to use only
those features that survive our feature selection
process (to be described below), for the follow-
ing reasons. First, the number of induced affixes
is large, and using only a subset of them as fea-
tures could make the training process more effi-
cient. Second, the above affix induction method is
arguably overly simplistic and hence many of the
induced affixes could be spurious.
Our feature selection process is fairly simple:
we (1) score each affix by multiplying its fre-
quency (i.e., the number of distinct words in V to
which each affix attaches) and its length2, and (2)
select only those whose score is above a certain
threshold. In our experiments, we set this thresh-
old to 50, and generate our vocabulary of 140K
words from five years of articles taken from the
Bengali newspaper Prothom Alo. This enables us
to induce 979 prefixes and 975 suffixes.
</bodyText>
<sectionHeader confidence="0.9769475" genericHeader="method">
4 Semantic Class Induction from
Wikipedia
</sectionHeader>
<bodyText confidence="0.999976666666667">
Wikipedia has recently been used as a knowl-
edge source for various language processing tasks,
including taxonomy construction (Ponzetto and
Strube, 2007a), coreference resolution (Ponzetto
and Strube, 2007b), and English NER (e.g.,
Bunescu and Pas¸ca (2006), Cucerzan (2007),
Kazama and Torisawa (2007), Watanabe et al.
(2007)). Unlike previous work on using Wikipedia
for NER, our goal here is to (1) generate a list
of phrases and tokens that are potentially named
entities from the 16914 articles in the Bengali
Wikipedia3 and (2) heuristically annotate each of
them with one of four classes, namely, PER (per-
son), ORG (organization), LOC (location), or OTH-
ERS (i.e., anything other than PER, ORG and LOC).
</bodyText>
<subsectionHeader confidence="0.998915">
4.1 Generating an Annotated List of Phrases
</subsectionHeader>
<bodyText confidence="0.999651090909091">
We employ the steps below to generate our anno-
tated list.
Generating and annotating the titles Recall
that each Wikipedia article has been optionally as-
signed to one or more categories by its creator
and/or editors. We use these categories to help an-
notate the title of an article. Specifically, if an ar-
ticle has a category whose name starts with “Born
on” or “Death on,” we label the corresponding ti-
tle with PER. Similarly, if it has a category whose
name starts with “Cities of” or “Countries of,” we
</bodyText>
<footnote confidence="0.9940066">
2The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
3See http://bn.wikipedia.org. In our experiments, we used
the Bengali Wikipedia dump obtained on October 22, 2007.
</footnote>
<page confidence="0.991757">
356
</page>
<table confidence="0.99739625">
NE Class Keywords
PER “born,” “died,” “one,” “famous”
LOC “city,” “area,” “population,” “located,” “part of”
ORG “establish,” “situate,” “publish”
</table>
<tableCaption confidence="0.998339">
Table 1: Keywords for each named entity class
</tableCaption>
<bodyText confidence="0.999919981818182">
label the title as LOC. If an article does not be-
long to one of the four categories above, we label
its title with the help of a small set of seed key-
words shown in Table 1. Specifically, for each of
the three NE classes shown on the left of Table
1, we compute a weighted sum of its keywords:
a keyword that appears in the first paragraph has
a weight of 3, a keyword that appears elsewhere
in the article has a weight of 1, and a keyword
that does not appear in the article has a weight of
0. The rationale behind using different weights is
simple: the first paragraph is typically a brief ex-
position of the title, so it should in principle con-
tain words that correlate more closely with the ti-
tle than words appearing in the rest of the article.
We then label the title with the class that has the
largest weighted sum. Note, however, that we ig-
nore any article that contains fewer than two key-
words, since we do not have reliable evidence for
labeling its title as one of the NE classes. We put
all these annotated titles into a title list.
Getting more location names To get more loca-
tion names, we search for the character sequences
“birth place:” and “death place:” in each article,
extracting the phrase following any of these se-
quences and label it as LOC. We put all such la-
beled locations into the title list.
Generating and annotating the tokens in the ti-
tles Next, we extract the word tokens from each
title in the title list and label each token with an
NE class. The reason for doing this is to improve
generalization: if “Dhaka University” is labeled as
ORG in the title list, then it is desirable to also label
the token “University” as ORG, because this could
help identify an unseen phrase that contains the
term “University” as an organization. Our token
labeling method is fairly simple. First, we gener-
ate the tokens from each title in the title list, as-
signing to each token the same NE label as that
of the title from which it is generated. For in-
stance, from the title “Anna Frank,” “Anna” will
be labeled as PER; and from “Anna University,” “
Anna” will be labeled as LOC. To resolve such
ambiguities (i.e., assigning different labels to the
same token), we keep a count of how many times
“Anna” is labeled with each NE class, and set its
final label to be the most frequent NE class. We
put all these annotated tokens into a token list. If
the title list and the token list have an element in
common, we remove the element from the token
list, since we have a higher confidence in the la-
bels of the titles.
Merging the lists Finally, we append the token
list to the title list. The resulting title list contains
4885 PERs, 15176 LOCs, and 188 ORGs.
</bodyText>
<subsectionHeader confidence="0.99724">
4.2 Applying the Annotated List to a Text
</subsectionHeader>
<bodyText confidence="0.999414">
We can now use the title list to annotate a text.
Specifically, we process each word w in the text in
a left-to-right manner, using the following steps:
</bodyText>
<listItem confidence="0.81265828">
1. Check whether w has been labeled. If so, we
skip this word and process the next one.
2. Check whether w appears in the Samsad
Bengali-English Dictionary4. If so, we as-
sume that w is more likely to be used as a
non-named entity, thus leaving the word un-
labeled and processing the next word instead.
3. Find the longest unlabeled word sequence5
that begins with w and appears in the title
list. If no such sequence exists, we leave w
unlabeled and process the next word. Oth-
erwise, we label it with the NE tag given
by the title list. To exemplify, consider a
text that starts with the sentence “Smith Col-
lege is in Massachusetts.” When processing
“Smith,” “Smith College” is the longest se-
quence that starts with “Smith” and appears
in the title list (as an ORG). As a result, we
label all occurrences of “Smith College” in
the text as an ORG. (Note that without using
the longest match heuristic, “Smith” would
likely be mislabeled as PER.) In addition, we
take the last word of the ORG (which in this
case is “College”) and annotate each of its oc-
currence in the rest of the text as ORG.6
</listItem>
<bodyText confidence="0.9983116">
These automatic annotations will then be used
to derive a set of WIKI features for training our
POS tagger and NE recognizer. Hence, unlike
existing Bengali NE recognizers, our “gazetteers”
are induced rather than manually created.
</bodyText>
<footnote confidence="0.996038333333333">
4See http://dsal.uchicago.edu/dictionaries/biswasbengali/.
5This is a sequence in which each word is unlabeled.
6However, if we have a PER match (e.g., “Anna Frank”)
or a LOC match (e.g., “Las Vegas”), we take each word in the
matched phrase and label each of its occurrence in the rest of
the text with the same NE tag.
</footnote>
<page confidence="0.961283">
357
</page>
<table confidence="0.9999005">
Current word wi
Previous word wi_1
2nd previous word wi_2
Next word wi+1
2nd next word wi+2
Current pseudo-affixes pfi (prefix), sfi (suffix)
Current induced affixes pii (prefix), sii (suffix)
Previous induced affixes pii_1 (prefix), sii_1 (suffix)
Induced affix bigrams pii_1pii (prefix), sii_1sii (suffix)
Current Wiki tag wikii
Previous Wiki tag wikii_1
Wikibigram wikii_1wikii
Word bigrams wi_2wi_1, wi_1wi, wiwi+1,
wi+1wi+2
Word trigrams wi_2wi_1wi
Current number qi
</table>
<tableCaption confidence="0.922705">
Table 2: Feature templates for the POS tagging
experiments
</tableCaption>
<sectionHeader confidence="0.994286" genericHeader="method">
5 Part-of-Speech Tagging
</sectionHeader>
<bodyText confidence="0.999823428571428">
In this section, we will show how we train and
evaluate our POS tagger. As mentioned before, we
hypothesize that introducing our two knowledge
sources into the feature set for the tagger could
improve its performance: using the induced affixes
could improve the extraction of grammatical infor-
mation from the words, and using the Wikipedia-
induced list, which in principle should comprise
mostly of names, could help improve the identifi-
cation of proper nouns.
Corpus Our corpus is composed of 77942 words
and is annotated with one of 26 POS tags in the
tagset defined by IIIT Hyderabad7. Using this cor-
pus, we perform 5-fold cross-validation (CV) ex-
periments in our evaluation. It is worth noting that
this dataset has a high unknown word rate of 15%
(averaged over the five folds), which is due to the
small size of the dataset. While this rate is compa-
rable to another Bengali POS dataset described in
Dandapat et al. (2007), it is much higher than the
2.6% unknown word rate in the test set for Ratna-
parkhi’s (1996) English POS tagging experiments.
Creating training instances Following previ-
ous work on POS tagging, we create one train-
ing instance for each word in the training set. The
class value of an instance is the POS tag of the cor-
responding word. Each instance is represented by
a set of linguistic features, as described next.
</bodyText>
<footnote confidence="0.937308666666667">
7A detailed description of these POS tags can be found in
http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf,
and are omitted here due to space limitations. This tagset
and the Penn Treebank tagset differ in that (1) nouns do not
have a number feature; (2) verbs do not have a tense feature;
and (3) adjectives and adverbs are not subcategorized.
</footnote>
<bodyText confidence="0.99882844">
Features Our feature set consists of (1) base-
line features motivated by those used in Danda-
pat et al.’s (2007) Bengali POS tagger and Singh
et al.’s (2006) Hindi POS tagger, as well as (2)
features derived from our induced affixes and the
Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams, bi-
grams and trigrams; (2) pseudo-affix features that
are created by taking the first three characters and
the last three characters of the current word; and
(3) a binary feature that determines whether the
current word is a number. As far as our new fea-
tures are concerned, we create one induced prefix
feature and one induced suffix feature from both
the current word and the previous word, as well
as two bigrams involving induced prefixes and in-
duced suffixes. We also create three WIKI features,
including the Wikipedia-induced NE tag of the
current word and that of the previous word, as well
as the combination of these two tags. Note that
the Wikipedia-induced tag of a word can be ob-
tained by annotating the test sentence under con-
sideration using the list generated from the Ben-
gali Wikipedia (see Section 4). To make the de-
scription of these features more concrete, we show
the feature templates in Table 2.
Learning algorithm We used CRF++8, a C++
implementation of conditional random fields (Laf-
ferty et al., 2001), as our learning algorithm for
training a POS tagging model.
Evaluating the model To evaluate the resulting
POS tagger, we generate test instances in the same
way as the training instances. 5-fold CV results of
the POS tagger are shown in Table 3. Each row
consists of three numbers: the overall accuracy,
as well as the accuracies on the seen and the un-
seen words. Row 1 shows the accuracy when the
baseline feature set is used; row 2 shows the ac-
curacy when the baseline feature set is augmented
with our two induced affix features; and the last
row shows the results when both the induced af-
fix and the WIKI features are incorporated into
the baseline feature set. Perhaps not surprisingly,
(1) adding more features improves performance,
and (2) accuracies on the seen words are substan-
tially better than those on the unseen words. In
fact, adding the induced affixes to the baseline fea-
ture set yields a 7.8% reduction in relative error
in overall accuracy. We also applied a two-tailed
paired t-test (p &lt; 0.01), first to the overall accura-
</bodyText>
<footnote confidence="0.993964">
8Available from http://crfpp.sourceforge.net
</footnote>
<page confidence="0.99112">
358
</page>
<table confidence="0.999442">
Experiment Overall Seen Unseen
Baseline 89.83 92.96 72.08
Baseline+Induced Affixes 90.57 93.39 74.64
Baseline+Induced Affixes+Wiki 90.80 93.50 75.58
</table>
<tableCaption confidence="0.858124">
Table 3: 5-fold cross-validation accuracies for
POS tagging
</tableCaption>
<table confidence="0.999768833333333">
Predicted Tag Correct Tag % of Error
NN NNP 22.7
NN JJ 9.6
JJ NN 7.4
NNP NN 5.0
NN VM 4.9
</table>
<tableCaption confidence="0.999405">
Table 4: Most frequent errors for POS tagging
</tableCaption>
<bodyText confidence="0.999953791666667">
cies in rows 1 and 2, and then to the overall accu-
racies in rows 2 and 3. Both pairs of numbers are
statistically significantly different from each other,
meaning that incorporating the two induced affix
features and then the WIKI features both yields sig-
nificant improvements.
Error analysis To better understand the results,
we examined the errors made by the tagger. The
most frequent errors are shown in Table 4. From
the table, we see that the largest source of errors
arises from mislabeling proper nouns as common
nouns. This should be expected, as proper noun
identification is difficult due to the lack of capital-
ization information. Unfortunately, failure to iden-
tify proper nouns could severely limit the recall of
an NE recognizer. Also, adjectives and common
nouns are difficult to distinguish, since these two
syntactic categories are morphologically and dis-
tributionally similar to each other. Finally, many
errors appear to involve mislabeling a word as a
common noun. The reason is that there is a larger
percentage of common nouns (almost 30%) in the
training set than other POS tags, thus causing the
model to prefer tagging a word as a common noun.
</bodyText>
<sectionHeader confidence="0.994914" genericHeader="method">
6 Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.964965727272727">
In this section, we show how to train and evaluate
our NE recognizer. The recognizer adopts a tradi-
tional architecture, assuming that POS tagging is
performed prior to NER. In other words, the NE
recognizer will use the POS acquired in Section 5
as one of its features. As in Section 5, we will fo-
cus on examining how our knowledge sources (the
induced affixes and the WIKI features) impact the
performance of our recognizer.
Corpus The corpus we used for NER evaluation
is the same as the one described in the previous
</bodyText>
<table confidence="0.998642571428571">
POS of current word ti
POS of previous word ti−1
POS of 2nd previous word ti−2
POS of next word ti+1
POS of 2nd next word ti+2
POS bigrams ti−2ti−1, ti−1ti, titi+1, ti+1ti+2
First word fwi
</table>
<tableCaption confidence="0.750262">
Table 5: Additional feature templates for the NER
experiments
</tableCaption>
<bodyText confidence="0.995861902439025">
section. Specifically, in addition to POS infor-
mation, each sentence in the corpus is annotated
with NE information. We focus on recognizing the
three major NE types in this paper, namely persons
(PER), organizations (ORG), and locations (LOC).
There are 1721 PERs, 104 ORGs, and 686 LOCs in
the corpus. As far as evaluation is concerned, we
conduct 5-fold CV experiments, dividing the cor-
pus into the same five folds as in POS tagging.
Creating training instances We view NE
recognition as a sequence labeling problem. In
other words, we combine NE identification and
classification into one step, labeling each word in
a test text with its NE tag. Any word that does not
belong to one of our three NE tags will be labeled
as OTHERS. We adopt the IOB convention, pre-
ceding an NE tag with a B if the word is the first
word of an NE and an I otherwise. Now, to train
the NE recognizer, we create one training instance
from each word in a training text. The class value
of an instance is the NE tag of the corresponding
word, or OTHERS if the word is not part of an NE.
Each instance is represented by a set of linguistic
features, as described next.
Features Our feature set consists of (1) base-
line features motivated by those used in Ekbal
et al.’s (2008) Bengali NE recognizer, as well as
(2) features derived from our induced affixes and
the Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams; (2)
pseudo-affix features that are created by taking the
first three characters and the last three characters
of the current word; (3) a binary feature that deter-
mines whether the current word is the first word of
a sentence; and (4) a set of POS-related features,
including the POS of the current word and its sur-
rounding words, as well as POS bigrams formed
from the current and surrounding words. Our in-
duced affixes and WIKI features are incorporated
into the baseline NE feature set in the same man-
ner as in POS tagging. In essence, the feature tem-
</bodyText>
<page confidence="0.996045">
359
</page>
<table confidence="0.999861076923077">
Experiment R P F
Baseline 60.97 74.46 67.05
Person 66.18 74.06 69.90
Organization 29.81 44.93 35.84
Location 52.62 80.40 63.61
Baseline+Induced Affixes 60.45 73.30 66.26
Person 65.70 72.61 69.02
Organization 31.73 46.48 37.71
Location 51.46 80.05 62.64
Baseline+Induced Affixes+Wiki 63.24 75.19 68.70
Person 66.47 75.16 70.55
Organization 30.77 43.84 36.16
Location 60.06 79.69 68.50
</table>
<tableCaption confidence="0.998762">
Table 6: 5-fold cross-validation results for NER
</tableCaption>
<bodyText confidence="0.999789105263158">
plates employed by the NE recognizer are the top
12 templates in Table 2 and those in Table 5.
Learning algorithm We again use CRF++ as
our sequence learner for acquiring the recognizer.
Evaluating the model To evaluate the resulting
NE tagger, we generate test instances in the same
way as the training instances. To score the output
of the recognizer, we use the CoNLL-2000 scor-
ing program9, which reports performance in terms
of recall (R), precision (P), and F-measure (F). All
NE results shown in Table 6 are averages of the
5-fold CV experiments. The first block of the Ta-
ble 6 shows the overall results when the baseline
feature set is used; in addition, we also show re-
sults for each of the three NE tags. As we can see,
the baseline achieves an F-measure of 67.05. The
second block shows the results when the baseline
feature set is augmented with our two induced af-
fix features. Somewhat unexpectedly, F-measure
drops by 0.8% in comparison to the baseline. Ad-
ditional experiments are needed to determine the
reason. Finally, when the WIKI features are in-
corporated into the augmented feature set, the sys-
tem achieves an F-measure of 68.70 (see the third
block), representing a statistically significant in-
crease of 1.6% in F-measure over the baseline.
As we can see, improvements stem primarily from
dramatic gains in recall for locations.
Discussions Several points deserve mentioning.
First, the model performs poorly on the ORGs, ow-
ing to the small number of organization names
in the corpus. Worse still, the recall drops after
adding the WIKI features. We examined the list
of induced ORG names and found that it is fairly
noisy. This can be attributed in part to the diffi-
culty in forming a set of seed words that can ex-
tract ORGs with high precision (e.g., the ORG seed
“situate” extracted many LOCs). Second, using the
</bodyText>
<footnote confidence="0.718839">
9http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
</footnote>
<bodyText confidence="0.999849909090909">
WIKI features does not help recalling the PERs. A
closer examination of the corpus reveals the rea-
son: many sentences describe fictitious characters,
whereas Wikipedia would be most useful for arti-
cles that describe famous people. Overall, while
the WIKI features provide our recognizer with a
small, but significant, improvement, the useful-
ness of the Bengali Wikipedia is currently lim-
ited by its small size. Nevertheless, we believe the
Bengali Wikipedia will become a useful resource
for language processing as its size increases.
</bodyText>
<sectionHeader confidence="0.988601" genericHeader="method">
7 A Joint Model for POS Tagging and
NER
</sectionHeader>
<bodyText confidence="0.999961756756757">
The NE recognizer described thus far has adopted
a pipelined architecture, and hence its perfor-
mance could be limited by the errors of the POS
tagger. In fact, as discussed before, the major
source of errors made by our POS tagger concerns
the confusion between proper nouns and common
nouns, and this type of error, when propagated
to the NE recognizer, could severely limit its re-
call. Also, there is strong empirical support for
this argument: the NE recognizers, when given ac-
cess to the correct POS tags, have F-scores rang-
ing from 76-79%, which are 10% higher on aver-
age than those with POS tags that were automat-
ically computed. Consequently, we hypothesize
that modeling POS tagging and NER jointly would
yield better performance than learning the two
tasks separately. In fact, many approaches have
been developed to jointly model POS tagging and
noun phrase chunking, including transformation-
based learning (Ngai and Florian, 2001), factorial
HMMs (Duh, 2005), and dynamic CRFs (Sutton
et al., 2007). Some of these approaches are fairly
sophisticated and also require intensive computa-
tions during inference. For instance, when jointly
modeling POS tagging and chunking, Sutton et al.
(2007) reduce the number of POS tags from 45
to 5 when training a factorial dynamic CRF on a
small dataset (with only 209 sentences) in order to
reduce training and inference time.
In contrast, we propose a relatively simple
model for jointly learning Bengali POS tagging
and NER, by exploiting the limited dependencies
between the two tasks. Specifically, we make the
observation that most of the Bengali words that are
part of an NE are also proper nouns. In fact, based
on statistics collected from our evaluation corpus
(see Sections 5 and 6), this observation is correct
</bodyText>
<page confidence="0.990024">
360
</page>
<table confidence="0.9985475">
Experiment R P F
Baseline 54.76 81.70 65.57
Baseline+Induced Affixes 56.79 88.96 69.32
Baseline+Induced Affixes+Wiki 61.73 86.35 71.99
</table>
<tableCaption confidence="0.9391155">
Table 7: 5-fold cross-validation joint modeling re-
sults for NER
</tableCaption>
<bodyText confidence="0.998831841269841">
97.3% of the time. Note, however, that this ob-
servation does not hold for English, since many
prepositions and determiners are part of an NE.
On the other hand, this observation largely holds
for Bengali because prepositions and determiners
are typically realized as noun suffixes.
This limited dependency between the POS tags
and the NE tags allows us to develop a simple
model for jointly learning the two tasks. More
specifically, we will use CRF++ to learn the joint
model. Training and test instances are generated
as described in the previous two subsections (i.e.,
one instance per word). The feature set will con-
sist of the union of the features that were used to
train the POS tagger and the NE tagger indepen-
dently, minus the POS-related features that were
used in the NE tagger. The class value of an in-
stance is computed as follows. If a word is not a
proper noun, its class is simply its POS tag. Oth-
erwise, its class is its NE tag, which can be PER,
ORG, LOC, or OTHERS. In other words, our joint
model exploits the observation that we made ear-
lier in the section by assuming that only proper
nouns can be part of a named entity. This allows
us to train a joint model without substantially in-
creasing the number of classes.
We again evaluate our joint model using 5-fold
CV experiments. The NE results of the model are
shown in Table 7. The rows here can be interpreted
in the same manner as those in Table 6. Compar-
ing these three experiments with their counterparts
in Table 6, we can see that, except for the base-
line, jointly modeling offers a significant improve-
ment of 3.3% in overall F-measure.10 In particu-
lar, the joint model benefits significantly from our
10The POS tagging results are not shown due to space lim-
itations. Overall, the POS accuracies drop insignificantly as
a result of joint modeling, for the following reason. Recall
from Section 5 that the major source of POS tagging errors
arises from the mislabeling of many proper nouns as com-
mon nouns, due primarily to the large number of common
nouns in the corpus. The joint model aggravates this prob-
lem by subcategorizing the proper nouns into different NE
classes, causing the tagger to have an even stronger bias to-
wards labeling a proper noun as a common noun than before.
Nevertheless, as seen from the results in Tables 6 and 7, such
a bias has yielded an increase in NER precision.
two knowledge sources, achieving an F-measure
of 71.99% when both of them are incorporated.
Finally, to better understand the value of the in-
duced affix features in the joint model as well as
the pipelined model described in Section 6, we
conducted an ablation experiment, in which we in-
corporated only the WIKI features into the base-
line feature set. With pipelined modeling, the F-
measure for NER is 68.87%, which is similar to
the case where both induced affixes and the WIKI
features are used. With joint modeling, however,
the F-measure for NER is 70.87%, which is 1%
lower than the best joint modeling score. These
results provide suggestive evidence that the in-
duced affix features play a significant role in the
improved performance of the joint model.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999998269230769">
We have explored two types of linguistic fea-
tures, namely the induced affix features and the
Wikipedia-related features, to improve a Bengali
POS tagger and NE recognizer. Our experimen-
tal results have demonstrated that (1) both types of
features significantly improve a baseline POS tag-
ger and (2) the Wikipedia-related features signif-
icantly improve a baseline NE recognizer. More-
over, by exploiting the limited dependencies be-
tween Bengali POS tags and NE tags, we pro-
posed a new model for jointly learning the two
tasks, which not only avoids the error-propagation
problem present in the pipelined system architec-
ture, but also yields statistically significant im-
provements over the NE recognizer that is trained
independently of the POS tagger. When applied in
combination, our three extensions contributed to a
relative improvement of 7.5% in F-measure over
the baseline NE recognizer. Most importantly, we
believe that these extensions are of relevance and
interest to the EACL community because many
European and Middle Eastern languages resemble
Bengali in terms of not only their morphological
richness but also their scarcity of annotated cor-
pora. We plan to empirically verify our belief in
future work.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9982308">
We thank the three anonymous reviewers for their
invaluable comments on the paper. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources. This work was
supported in part by NSF Grant IIS-0812261.
</bodyText>
<page confidence="0.99817">
361
</page>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929902173913">
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of COLT, pages 92–100.
Razvan Bunescu and Marius Pas¸ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of EACL, pages 9–16.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of EACL, pages 59–66.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings ofEMNLP/VLC, pages 100–110.
Silviu Cucerzan and David Yarowsky. 1999. Lan-
guage independent named entity recognition com-
bining morphological and contextual evidence. In
Proceedings of EMNLP/VLC, pages 90–99.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceed-
ings ofEMNLP-CoNLL, pages 708–716.
Sandipan Dandapat, Sudeshna Sarkar, and Anupam
Basu. 2007. Automatic part-of-speech tagging for
Bengali: An approach for morphologically rich lan-
guages in a poor resource scenario. In Proceedings
of the ACL Companion Volume, pages 221–224.
Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial HMM approach. In Proceedings of the
ACL Student Research Workshop, pages 19–24.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopad-
hyay. 2008. Named entity recognition in Bengali:
A conditional random field approach. In Proceed-
ings ofIJCNLP, pages 589–594.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153–198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744–751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320–327.
Jun’ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL, pages 698–707.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the EACL Student Research
Workshop, pages 15–22.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282–
289.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of EACL, pages 1–8.
Grace Ngai and Radu Florian. 2001. Transformation
based learning in the fast lane. In Proceedings of
NAACL, pages 40–47.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from wikipedia. In
Proceedings ofAAAI, pages 1440–1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181–212.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
ofEMNLP, pages 133–142.
Hinrich Sch¨utze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141–148.
Smriti Singh, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological rich-
ness offsets resource demand — Experiences in con-
structing a POS tagger for Hindi. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 779–786.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693–723.
Yotaro Watanabe, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. A graph-based approach to named
entity categorization in Wikipedia using conditional
random fields. In Proceedings of EMNLP-CoNLL,
pages 649–657.
</reference>
<page confidence="0.998262">
362
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974648">
<title confidence="0.9992475">Learning-Based Named Entity Recognition for Morphologically-Rich, Resource-Scarce Languages</title>
<author confidence="0.999141">Saidul Hasan Altaf ur Rahman Ng</author>
<affiliation confidence="0.999638">Human Language Technology Research Institute University of Texas at Dallas</affiliation>
<address confidence="0.987605">Richardson, TX 75083-0688</address>
<abstract confidence="0.999664032258065">Named entity recognition for morphologically rich, case-insensitive languages, including the majority of semitic languages, Iranian languages, and Indian languages, is inherently more difficult than its English counterpart. Worse still, progress on machine learning approaches to named entity recognition for many of these languages is currently hampered by the scarcity of annotated data and the lack of an accurate part-of-speech tagger. While it is possible to rely on manually-constructed gazetteers to combat data scarcity, this gazetteer-centric approach has the potential weakness of creating irreproducible results, since these name lists are not publicly available in general. Motivated in part by this concern, we present a learning-based named entity recognizer that does not rely on manually-constructed gazetteers, using Bengali as our representative resource-scarce, morphologicallyrich language. Our recognizer achieves a relative improvement of 7.5% in Fmeasure over a baseline recognizer. Improvements arise from (1) using induced affixes, (2) extracting information from online lexical databases, and (3) jointly modeling part-of-speech tagging and named entity recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="2406" citStr="Blum and Mitchell, 1998" startWordPosition="348" endWordPosition="351">most spoken language1 with more than 200 million native speakers residing mostly in Bangladesh and the Indian state of West Bengal, Bengali has far less electronic resources than the aforementioned languages. In fact, a major obstacle to the automatic processing of Bengali is the scarcity of annotated corpora. One potential solution to the problem of data scarcity is to hand-annotate a small amount of data with the desired linguistic information and then develop bootstrapping algorithms for combining this small amount of labeled data with a large amount of unlabeled data. In fact, cotraining (Blum and Mitchell, 1998) has been successfully applied to English named entity recognition (NER) (Collins &amp; Singer [henceforth C&amp;S] (1999)). In C&amp;S’s approach, consecutive words tagged as proper nouns are first identified as potential NEs, and each such NE is then labeled by combining the outputs of two co-trained classifiers. Unfortunately, there are practical difficulties in applying this technique to Bengali NER. First, one of C&amp;S’s co-trained classifiers uses features based on capitalization, but Bengali is case-insensitive. Second, C&amp;S identify potential NEs based on proper nouns, but unlike English, (1) proper </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Marius Pas¸ca</author>
</authors>
<title>Using encyclopedic knowledge for named entity disambiguation.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>9--16</pages>
<marker>Bunescu, Pas¸ca, 2006</marker>
<rawString>Razvan Bunescu and Marius Pas¸ca. 2006. Using encyclopedic knowledge for named entity disambiguation. In Proceedings of EACL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Combining distributional and morphological information for part-of-speech induction.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>59--66</pages>
<contexts>
<context position="3804" citStr="Clark (2003)" startWordPosition="564" endWordPosition="565">tion, owing to the scarcity of annotated data for training the tagger. In other words, Bengali NER is complicated not only by the scarcity of annotated data, but also by the lack of an accurate POS tagger. One could imagine building a Bengali POS tagger using un1See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions a</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>Alexander Clark. 2003. Combining distributional and morphological information for part-of-speech induction. In Proceedings of EACL, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings ofEMNLP/VLC,</booktitle>
<pages>100--110</pages>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings ofEMNLP/VLC, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>David Yarowsky</author>
</authors>
<title>Language independent named entity recognition combining morphological and contextual evidence.</title>
<date>1999</date>
<booktitle>In Proceedings of EMNLP/VLC,</booktitle>
<pages>90--99</pages>
<contexts>
<context position="8222" citStr="Cucerzan and Yarowsky (1999)" startWordPosition="1265" endWordPosition="1268">esemble Bengali in terms of not only their morphological richness, but also their relatively free word order. The rest of the paper is organized as follows. In Section 2, we briefly describe the related work. Sections 3 and 4 show how we induce affixes from an unannotated corpus and extract semantic class information from Wikipedia. In Sections 5 and 6, we train and evaluate a POS tagger and an NE recognizer independently, augmenting the feature set typically used for these two tasks with our new knowledge sources. Finally, we describe and evaluate our joint model in Section 7. 2 Related Work Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-independent solution to NER. They use affixes based on the paradigm that named entities corresponding to a particular class have similar morphological structure. Their bootstrapping 355 approach is tested on Romanian, English, Greek, Turkish, and Hindi. The recall for Hindi is the lowest (27.84%) among the five languages, suggesting that the lack of case information can significantly complicate the NER task. To investigate the role of gazetteers in NER, Mikheev et al. (1999) combine grammar rules with maximum entropy models a</context>
</contexts>
<marker>Cucerzan, Yarowsky, 1999</marker>
<rawString>Silviu Cucerzan and David Yarowsky. 1999. Language independent named entity recognition combining morphological and contextual evidence. In Proceedings of EMNLP/VLC, pages 90–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
</authors>
<title>Large-scale named entity disambiguation based on Wikipedia data.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL,</booktitle>
<pages>708--716</pages>
<contexts>
<context position="11941" citStr="Cucerzan (2007)" startWordPosition="1878" endWordPosition="1879">length2, and (2) select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC). 4.1 Generating an Annotated List of Phrases We employ the steps below to generate our annotated list. Generating and annotating the titles Recall that each Wikipedi</context>
</contexts>
<marker>Cucerzan, 2007</marker>
<rawString>Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings ofEMNLP-CoNLL, pages 708–716.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandipan Dandapat</author>
<author>Sudeshna Sarkar</author>
<author>Anupam Basu</author>
</authors>
<title>Automatic part-of-speech tagging for Bengali: An approach for morphologically rich languages in a poor resource scenario.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Companion Volume,</booktitle>
<pages>221--224</pages>
<contexts>
<context position="5596" citStr="Dandapat et al. (2007)" startWordPosition="840" endWordPosition="843">aking it difficult to reproduce the results of these NE recognizers. Second, it is not clear how comprehensive these lists are. Relying on comprehensive lists that comprise a large portion of the names in the test set essentially reduces the NER problem to a dictionary-lookup problem, which is arguably not very interesting from a research perspective. In addition, many existing learning-based Bengali NE recognizers have several common weaknesses. First, they use as features pseudo-affixes, which are created by extracting the first n and the last n characters of a word (where 1 &lt; n &lt; 4) (e.g., Dandapat et al. (2007)). While affixes encode essential grammatical information in Bengali due to its morphological richness, this extraction method is arguably too ad-hoc and does not cover many useful affixes. Second, they typically adopt a pipelined NER architecture, performing POS tagging prior to NER and encoding the resulting not-so-accurate POS information as a feature. In other words, errors in POS tagging are propagated to the NE recognizer via the POS feature, thus limiting its performance. Motivated in part by these weaknesses, we investigate how to improve a learning-based NE recognizer that does not re</context>
<context position="19408" citStr="Dandapat et al. (2007)" startWordPosition="3185" endWordPosition="3188"> the words, and using the Wikipediainduced list, which in principle should comprise mostly of names, could help improve the identification of proper nouns. Corpus Our corpus is composed of 77942 words and is annotated with one of 26 POS tags in the tagset defined by IIIT Hyderabad7. Using this corpus, we perform 5-fold cross-validation (CV) experiments in our evaluation. It is worth noting that this dataset has a high unknown word rate of 15% (averaged over the five folds), which is due to the small size of the dataset. While this rate is comparable to another Bengali POS dataset described in Dandapat et al. (2007), it is much higher than the 2.6% unknown word rate in the test set for Ratnaparkhi’s (1996) English POS tagging experiments. Creating training instances Following previous work on POS tagging, we create one training instance for each word in the training set. The class value of an instance is the POS tag of the corresponding word. Each instance is represented by a set of linguistic features, as described next. 7A detailed description of these POS tags can be found in http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf, and are omitted here due to space limitations. This tagset and th</context>
</contexts>
<marker>Dandapat, Sarkar, Basu, 2007</marker>
<rawString>Sandipan Dandapat, Sudeshna Sarkar, and Anupam Basu. 2007. Automatic part-of-speech tagging for Bengali: An approach for morphologically rich languages in a poor resource scenario. In Proceedings of the ACL Companion Volume, pages 221–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Jointly labeling multiple sequences: A factorial HMM approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Student Research Workshop,</booktitle>
<pages>pages</pages>
<contexts>
<context position="30808" citStr="Duh, 2005" startWordPosition="5114" endWordPosition="5115">d severely limit its recall. Also, there is strong empirical support for this argument: the NE recognizers, when given access to the correct POS tags, have F-scores ranging from 76-79%, which are 10% higher on average than those with POS tags that were automatically computed. Consequently, we hypothesize that modeling POS tagging and NER jointly would yield better performance than learning the two tasks separately. In fact, many approaches have been developed to jointly model POS tagging and noun phrase chunking, including transformationbased learning (Ngai and Florian, 2001), factorial HMMs (Duh, 2005), and dynamic CRFs (Sutton et al., 2007). Some of these approaches are fairly sophisticated and also require intensive computations during inference. For instance, when jointly modeling POS tagging and chunking, Sutton et al. (2007) reduce the number of POS tags from 45 to 5 when training a factorial dynamic CRF on a small dataset (with only 209 sentences) in order to reduce training and inference time. In contrast, we propose a relatively simple model for jointly learning Bengali POS tagging and NER, by exploiting the limited dependencies between the two tasks. Specifically, we make the obser</context>
</contexts>
<marker>Duh, 2005</marker>
<rawString>Kevin Duh. 2005. Jointly labeling multiple sequences: A factorial HMM approach. In Proceedings of the ACL Student Research Workshop, pages 19–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>Rejwanul Haque</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Named entity recognition in Bengali: A conditional random field approach.</title>
<date>2008</date>
<booktitle>In Proceedings ofIJCNLP,</booktitle>
<pages>589--594</pages>
<marker>Ekbal, Haque, Bandyopadhyay, 2008</marker>
<rawString>Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopadhyay. 2008. Named entity recognition in Bengali: A conditional random field approach. In Proceedings ofIJCNLP, pages 589–594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="13090" citStr="Goldsmith (2001)" startWordPosition="2074" endWordPosition="2075">ist. Generating and annotating the titles Recall that each Wikipedia article has been optionally assigned to one or more categories by its creator and/or editors. We use these categories to help annotate the title of an article. Specifically, if an article has a category whose name starts with “Born on” or “Death on,” we label the corresponding title with PER. Similarly, if it has a category whose name starts with “Cities of” or “Countries of,” we 2The dependence on frequency and length is motivated by the observation that less frequent and shorter affixes are more likely to be erroneous (see Goldsmith (2001)). 3See http://bn.wikipedia.org. In our experiments, we used the Bengali Wikipedia dump obtained on October 22, 2007. 356 NE Class Keywords PER “born,” “died,” “one,” “famous” LOC “city,” “area,” “population,” “located,” “part of” ORG “establish,” “situate,” “publish” Table 1: Keywords for each named entity class label the title as LOC. If an article does not belong to one of the four categories above, we label its title with the help of a small set of seed keywords shown in Table 1. Specifically, for each of the three NE classes shown on the left of Table 1, we compute a weighted sum of its k</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-ofspeech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>744--751</pages>
<contexts>
<context position="3944" citStr="Goldwater and Griffiths, 2007" startWordPosition="579" endWordPosition="583"> by the scarcity of annotated data, but also by the lack of an accurate POS tagger. One could imagine building a Bengali POS tagger using un1See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions and particles such as “in” and “to”) are realized as inflections in Bengali, and the absence of these informative words implies that the cont</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Thomas L. Griffiths. 2007. A fully Bayesian approach to unsupervised part-ofspeech tagging. In Proceedings of the ACL, pages 744–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven learning for sequence models.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>320--327</pages>
<contexts>
<context position="3890" citStr="Haghighi and Klein, 2006" startWordPosition="572" endWordPosition="575"> other words, Bengali NER is complicated not only by the scarcity of annotated data, but also by the lack of an accurate POS tagger. One could imagine building a Bengali POS tagger using un1See http://en.wikipedia.org/wiki/Bengali language. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354–362, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 354 supervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recentlyproposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Goldwater and Griffiths, 2007). The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons. First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words. Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions and particles such as “in” and “to”) are realized as inflections in Bengali, and the ab</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven learning for sequence models. In Proceedings of HLT-NAACL, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Exploiting Wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>698--707</pages>
<contexts>
<context position="11969" citStr="Kazama and Torisawa (2007)" startWordPosition="1880" endWordPosition="1883">select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC). 4.1 Generating an Annotated List of Phrases We employ the steps below to generate our annotated list. Generating and annotating the titles Recall that each Wikipedia article has been optionall</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2007. Exploiting Wikipedia as external knowledge for named entity recognition. In Proceedings of EMNLPCoNLL, pages 698–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A simpler, intuitive approach to morpheme induction.</title>
<date>2006</date>
<booktitle>In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes.</booktitle>
<contexts>
<context position="10287" citStr="Keshava and Pitler (2006)" startWordPosition="1586" endWordPosition="1589">a graph exploration algorithm. The location pattern is essentially a preposition followed by capitalized context words. However, this approach is inadequate for a morphologically-rich language like Bengali, since prepositions are often realized as inflections. 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes. Hence, these affixes could serve as useful features for training POS and NE taggers. In this section, we show how to induce affixes from an unannotated corpus. We rely on a simple idea proposed by Keshava and Pitler (2006) for inducing affixes. Assume that (1) V is a vocabulary (i.e., a set of distinct words) extracted from a large, unannotated corpus, (2) α and Q are two character sequences, and (3) αQ is the concatenation of α and Q. If αQ and α are found in V , we extract Q as a suffix. Similarly, if αQ and Q are found in V , we extract α as a prefix. In principle, we can use all of the induced affixes as features for training a POS tagger and an NE recognizer. However, we choose to use only those features that survive our feature selection process (to be described below), for the following reasons. First, t</context>
</contexts>
<marker>Keshava, Pitler, 2006</marker>
<rawString>Samarth Keshava and Emily Pitler. 2006. A simpler, intuitive approach to morpheme induction. In PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
</authors>
<title>Bootstrapping named entity recognition with automatically generated gazetteer lists.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL Student Research Workshop,</booktitle>
<pages>15--22</pages>
<contexts>
<context position="9559" citStr="Kozareva (2006)" startWordPosition="1480" endWordPosition="1481"> still high without gazetteers, ranging from 85% to 92%; and (2) a small list of country names can improve the low F-score for locations substantially. It is worth noting that their recognizer requires that the input data contain POS tags and simple semantic tags, whereas ours automatically acquires such linguistic information. In addition, their approach uses part of the dataset to extend the gazetteer. Therefore, the resulting gazetteer list is specific to a particular domain; on the other hand, our approach does not generate a domain-specific list, since it makes use of Wikipedia articles. Kozareva (2006) generates gazetteer lists for person and location names from unlabeled data using common patterns and a graph exploration algorithm. The location pattern is essentially a preposition followed by capitalized context words. However, this approach is inadequate for a morphologically-rich language like Bengali, since prepositions are often realized as inflections. 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes. Hence, these affixes could serve as useful features for training POS and NE taggers. In this s</context>
</contexts>
<marker>Kozareva, 2006</marker>
<rawString>Zornitsa Kozareva. 2006. Bootstrapping named entity recognition with automatically generated gazetteer lists. In Proceedings of the EACL Student Research Workshop, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="21536" citStr="Lafferty et al., 2001" startWordPosition="3545" endWordPosition="3549">wo bigrams involving induced prefixes and induced suffixes. We also create three WIKI features, including the Wikipedia-induced NE tag of the current word and that of the previous word, as well as the combination of these two tags. Note that the Wikipedia-induced tag of a word can be obtained by annotating the test sentence under consideration using the list generated from the Bengali Wikipedia (see Section 4). To make the description of these features more concrete, we show the feature templates in Table 2. Learning algorithm We used CRF++8, a C++ implementation of conditional random fields (Lafferty et al., 2001), as our learning algorithm for training a POS tagging model. Evaluating the model To evaluate the resulting POS tagger, we generate test instances in the same way as the training instances. 5-fold CV results of the POS tagger are shown in Table 3. Each row consists of three numbers: the overall accuracy, as well as the accuracies on the seen and the unseen words. Row 1 shows the accuracy when the baseline feature set is used; row 2 shows the accuracy when the baseline feature set is augmented with our two induced affix features; and the last row shows the results when both the induced affix a</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
<author>Claire Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8770" citStr="Mikheev et al. (1999)" startWordPosition="1351" endWordPosition="1354">our joint model in Section 7. 2 Related Work Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-independent solution to NER. They use affixes based on the paradigm that named entities corresponding to a particular class have similar morphological structure. Their bootstrapping 355 approach is tested on Romanian, English, Greek, Turkish, and Hindi. The recall for Hindi is the lowest (27.84%) among the five languages, suggesting that the lack of case information can significantly complicate the NER task. To investigate the role of gazetteers in NER, Mikheev et al. (1999) combine grammar rules with maximum entropy models and vary the gazetteer size. Experimental results show that (1) the Fscores for NE classes like person and organization are still high without gazetteers, ranging from 85% to 92%; and (2) a small list of country names can improve the low F-score for locations substantially. It is worth noting that their recognizer requires that the input data contain POS tags and simple semantic tags, whereas ours automatically acquires such linguistic information. In addition, their approach uses part of the dataset to extend the gazetteer. Therefore, the res</context>
</contexts>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers. In Proceedings of EACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Ngai</author>
<author>Radu Florian</author>
</authors>
<title>Transformation based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="30780" citStr="Ngai and Florian, 2001" startWordPosition="5108" endWordPosition="5111">hen propagated to the NE recognizer, could severely limit its recall. Also, there is strong empirical support for this argument: the NE recognizers, when given access to the correct POS tags, have F-scores ranging from 76-79%, which are 10% higher on average than those with POS tags that were automatically computed. Consequently, we hypothesize that modeling POS tagging and NER jointly would yield better performance than learning the two tasks separately. In fact, many approaches have been developed to jointly model POS tagging and noun phrase chunking, including transformationbased learning (Ngai and Florian, 2001), factorial HMMs (Duh, 2005), and dynamic CRFs (Sutton et al., 2007). Some of these approaches are fairly sophisticated and also require intensive computations during inference. For instance, when jointly modeling POS tagging and chunking, Sutton et al. (2007) reduce the number of POS tags from 45 to 5 when training a factorial dynamic CRF on a small dataset (with only 209 sentences) in order to reduce training and inference time. In contrast, we propose a relatively simple model for jointly learning Bengali POS tagging and NER, by exploiting the limited dependencies between the two tasks. Spe</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>Grace Ngai and Radu Florian. 2001. Transformation based learning in the fast lane. In Proceedings of NAACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Deriving a large scale taxonomy from wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings ofAAAI,</booktitle>
<pages>1440--1445</pages>
<contexts>
<context position="11819" citStr="Ponzetto and Strube, 2007" startWordPosition="1860" endWordPosition="1863">we (1) score each affix by multiplying its frequency (i.e., the number of distinct words in V to which each affix attaches) and its length2, and (2) select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC). 4.1 Generating an Annotated List of Phrase</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007a. Deriving a large scale taxonomy from wikipedia. In Proceedings ofAAAI, pages 1440–1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Michael Strube</author>
</authors>
<title>Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>30--181</pages>
<contexts>
<context position="11819" citStr="Ponzetto and Strube, 2007" startWordPosition="1860" endWordPosition="1863">we (1) score each affix by multiplying its frequency (i.e., the number of distinct words in V to which each affix attaches) and its length2, and (2) select only those whose score is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC). 4.1 Generating an Annotated List of Phrase</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>Simone Paolo Ponzetto and Michael Strube. 2007b. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings ofEMNLP,</booktitle>
<pages>133--142</pages>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings ofEMNLP, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging.</title>
<date>1995</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>141--148</pages>
<marker>Sch¨utze, 1995</marker>
<rawString>Hinrich Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of EACL, pages 141–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smriti Singh</author>
<author>Kuhoo Gupta</author>
<author>Manish Shrivastava</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Morphological richness offsets resource demand — Experiences in constructing a POS tagger for Hindi.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>779--786</pages>
<marker>Singh, Gupta, Shrivastava, Bhattacharyya, 2006</marker>
<rawString>Smriti Singh, Kuhoo Gupta, Manish Shrivastava, and Pushpak Bhattacharyya. 2006. Morphological richness offsets resource demand — Experiences in constructing a POS tagger for Hindi. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 779–786.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>8--693</pages>
<contexts>
<context position="30848" citStr="Sutton et al., 2007" startWordPosition="5119" endWordPosition="5122">lso, there is strong empirical support for this argument: the NE recognizers, when given access to the correct POS tags, have F-scores ranging from 76-79%, which are 10% higher on average than those with POS tags that were automatically computed. Consequently, we hypothesize that modeling POS tagging and NER jointly would yield better performance than learning the two tasks separately. In fact, many approaches have been developed to jointly model POS tagging and noun phrase chunking, including transformationbased learning (Ngai and Florian, 2001), factorial HMMs (Duh, 2005), and dynamic CRFs (Sutton et al., 2007). Some of these approaches are fairly sophisticated and also require intensive computations during inference. For instance, when jointly modeling POS tagging and chunking, Sutton et al. (2007) reduce the number of POS tags from 45 to 5 when training a factorial dynamic CRF on a small dataset (with only 209 sentences) in order to reduce training and inference time. In contrast, we propose a relatively simple model for jointly learning Bengali POS tagging and NER, by exploiting the limited dependencies between the two tasks. Specifically, we make the observation that most of the Bengali words th</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learning Research, 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yotaro Watanabe</author>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A graph-based approach to named entity categorization in Wikipedia using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>649--657</pages>
<contexts>
<context position="11993" citStr="Watanabe et al. (2007)" startWordPosition="1884" endWordPosition="1887">e is above a certain threshold. In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo. This enables us to induce 979 prefixes and 975 suffixes. 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)). Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (person), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC). 4.1 Generating an Annotated List of Phrases We employ the steps below to generate our annotated list. Generating and annotating the titles Recall that each Wikipedia article has been optionally assigned to one or mor</context>
</contexts>
<marker>Watanabe, Asahara, Matsumoto, 2007</marker>
<rawString>Yotaro Watanabe, Masayuki Asahara, and Yuji Matsumoto. 2007. A graph-based approach to named entity categorization in Wikipedia using conditional random fields. In Proceedings of EMNLP-CoNLL, pages 649–657.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>