<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015295">
<title confidence="0.949729">
Improved Typesetting Models for Historical OCR
</title>
<author confidence="0.998882">
Taylor Berg-Kirkpatrick Dan Klein
</author>
<affiliation confidence="0.998795">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.996016">
{tberg,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999272">
We present richer typesetting models
that extend the unsupervised historical
document recognition system of Berg-
Kirkpatrick et al. (2013). The first
model breaks the independence assump-
tion between vertical offsets of neighbor-
ing glyphs and, in experiments, substan-
tially decreases transcription error rates.
The second model simultaneously learns
multiple font styles and, as a result, is
able to accurately track italic and non-
italic portions of documents. Richer mod-
els complicate inference so we present a
new, streamlined procedure that is over
25x faster than the method used by Berg-
Kirkpatrick et al. (2013). Our final sys-
tem achieves a relative word error reduc-
tion of 22% compared to state-of-the-art
results on a dataset of historical newspa-
pers.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939233333333">
Modern OCR systems perform poorly on histor-
ical documents from the printing-press era, often
yielding error rates that are too high for down-
stream research projects (Arlitsch and Herbert,
2004; Shoemaker, 2005; Holley, 2010). The two
primary reasons that historical documents present
difficultly for automatic systems are (1) the type-
setting process used to produce such documents
was extremely noisy and (2) the fonts used in the
documents are unknown. Berg-Kirkpatrick et al.
(2013) proposed a system for historical OCR that
generatively models the noisy typesetting process
of printing-press era documents and learns the font
for each input document in an unsupervised fash-
ion. Their system achieves state-of-the-art results
on the task of historical document recognition.
We take the system of Berg-Kirkpatrick et al.
(2013) as a starting point and consider extensions
of the typesetting model that address two short-
comings of their model: (1) their layout model as-
sumes that baseline offset noise is independent for
each glyph and (2) their font model assumes a sin-
gle font is used in every document. Both of these
assumptions are untrue in many historical datasets.
The baseline of the text in printing-press era
documents is not rigid as in modern documents but
rather drifts up and down noisily (see Figure 2).
In practice, the vertical offsets of character glyphs
change gradually along a line. This means the ver-
tical offsets of neighboring glyphs are correlated,
a relationship that is not captured by the original
model. In our first extension, we let the vertical
offsets of character glyphs be generated from a
Markov chain, penalizing large changes in offset.
We find that this extension decreases transcription
error rates. Our system achieves a relative word
error reduction of 22% compared to the state-of-
the-art original model on a test set of historical
newspapers (see Section 4.1), and a 11% relative
reduction on a test set of historical court proceed-
ings.
Multiple font styles are also frequently used in
printing-press era documents; the most common
scenario is for a basic font style to co-occur with
an italic variant. For example, it is common for
proper nouns and quotations to be italicized in
the Old Bailey corpus (Shoemaker, 2005). In our
second extension, we incorporate a Markov chain
over font styles, extending the original model so
that it is capable of simultaneously learning italic
and non-italic fonts within a single document. In
experiments, this model is able to detect which
words are italicized with 93% precision at 74%
recall in a test set of historical court proceedings
(see Section 4.2).
These richer models that we propose do in-
crease the state space and therefore make infer-
ence more costly. To remedy this, we stream-
line inference by replacing the coarse-to-fine in-
ference scheme of Berg-Kirkpatrick et al. (2013)
</bodyText>
<page confidence="0.949181">
118
</page>
<bodyText confidence="0.285928">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–123,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<figure confidence="0.989711466666667">
Slow-vary Original model Italic
Font
Character
Glyph width
Pad width
Pixels
Vertical offset
9i-1 pi-1
ei-1
A-1
9i A
e2
Xi LYPH XiPA_1
i LYPH XiPAD
vz-1 vz
</figure>
<figureCaption confidence="0.990760666666667">
Figure 1: See Section 2 for a description of the generative process. We consider an extension of Berg-Kirkpatrick et al. (2013)
that generates vi conditioned on the previous vertical offset vi−1 (labeled Slow-vary) and an extension that generates a sequence
of font styles fi (labeled Italic).
</figureCaption>
<bodyText confidence="0.998079">
with a forward-cost-augmented beaming scheme.
Our method is over 25x faster on a typical docu-
ment, yet actually yields improved transcriptions.
</bodyText>
<sectionHeader confidence="0.973689" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.9932383">
We first describe the generative model used by
the ‘Ocular’ historical OCR system of Berg-
Kirkpatrick et al. (2013)1 and then describe our
extensions. The graphical model corresponding
to their basic generative process for a single line
of text is diagrammed in Figure 1. A Kneser-
Ney (Kneser and Ney, 1995) character 6-gram lan-
guage model generates a sequence of characters
E = (e1, e2, ... , en). For each character index i, a
glyph box width gi and a pad box width pi are gen-
erated, conditioned on the character ei. gi specifies
the width of the bounding box that will eventually
house the pixels of the glyph for character ei. pi
specifies the width of a padding box which con-
tains the horizontal space before the next character
begins. Next, a vertical offset vi is generated for
the glyph corresponding to character ei. vi allows
the model to capture variance in the baseline of the
text in the document. We will later let vi depend
on vi−1, as depicted in Figure 1, but in the baseline
1The model we describe and extend has two minor dif-
ferences from the one described by Berg-Kirkpatrick et al.
(2013). While Berg-Kirkpatrick et al. (2013) generate two
pad boxes for each character token, one to the left and one to
the right, we only generate one pad box, always to the right.
Additionally, Berg-Kirkpatrick et al. (2013) do not carry over
the language model context between lines, while we do.
system they are independent. Finally, the pixels in
the ith glyph bounding box XGLYPH are generated
i
conditioned on the character ei, width gi, and ver-
tical offset vi, and the pixels in the ith pad bound-
ing box XPAD
i are generated conditioned on the
width pi. We refer the reader to Berg-Kirkpatrick
et al. (2013) for the details of the pixel generation
process. We have omitted the token-level inking
random variables for the purpose of brevity. These
can be treated as part of the pixel generation pro-
cess.
</bodyText>
<equation confidence="0.962663464285714">
Let X denote the matrix of pixels for the entire
line, V = (v1, ... , vn), P = (p1, ... ,pn), and
G = (g1, ... , gn). The joint distribution is writ-
ten:
P(X,V, P, G, E) =
P(E) [Language model]
P(gi|ei; Φ) [Glyph widths]
P(pi|ei; Φ) [Pad widths]
P(vi) [Vertical offsets]
P(XP(XPAD
i |pi) [Pad pixels]
GLYPH
i |vi, gi, ei; Φ) [Glyph pixels]
��n77
·11
i=1
n
·
i=1
n
·
i=1
n
·
i=1
n
·
i=1
</equation>
<page confidence="0.996562">
119
</page>
<figure confidence="0.9716164">
Learned typsetting
slow-varying offsets:
Learned typsetting
independent offsets:
Document image:
</figure>
<figureCaption confidence="0.9802065">
Figure 2: The first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-SV model. The second line
depicts the same, but for the OCULAR-BEAM model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display
the Bernoulli template probabilities used to generate the observed pixels. The third line shows the corresponding portion of the
input image.
</figureCaption>
<bodyText confidence="0.998717666666667">
The font is parameterized by the vector Φ which
governs the shapes of glyphs and the distributions
over box widths. Φ is learned in an unsupervised
fashion. Document recognition is accomplished
via Viterbi decoding over the character random
variables ei.
</bodyText>
<subsectionHeader confidence="0.937513">
2.1 Slow-varying Offsets
</subsectionHeader>
<bodyText confidence="0.999911777777778">
The original model generates the vertical offsets
vi independently, and therefore cannot model how
neighboring offsets are correlated. This correla-
tion is actually strong in printing-press era docu-
ments. The baseline of the text wanders in the in-
put image for two reasons: (1) the physical groove
along which character templates were set was un-
even and (2) the original document was imaged in
a way that produced distortion. Both these under-
lying causes are likely to yield baselines that wan-
der slowly up and down across a document. We
refer to this behavior of vertical offsets as slow-
varying, and extend the model to capture it.
In our first extension, we augment the model
by incorporating a Markov chain over the verti-
cal offset random variables vi, as depicted in Fig-
ure 1. Specifically, vi is generated from a dis-
cretized Gaussian centered at vi−1:
</bodyText>
<equation confidence="0.924034">
P(vi|vi−1) a exp C(vi − vi−1)21
2u2 J
</equation>
<bodyText confidence="0.999645">
This means that the if vi differs substantially from
vi−1, a large penalty is incurred. As a result,
the model should prefer sequences of vi that vary
slowly. In experiments, we set u2 = 0.05.
</bodyText>
<subsectionHeader confidence="0.984033">
2.2 Italic Font Styles
</subsectionHeader>
<bodyText confidence="0.99997521875">
Many of the documents in the Old Bailey corpus
contain both italic and non-italic font styles (Shoe-
maker, 2005). The way that italic fonts are used
depends on the year the document was printed,
but generally italics are reserved for proper nouns,
quotations, and sentences that have a special role
(e.g. the final judgment made in a court case). The
switch between font styles almost always occurs
at space characters.
Our second extension of the typesetting model
deals with both italic and non-italic font styles.
We augment the model with a Markov chain
over font styles fi, as depicted in Figure 1.
Each font style token fi takes on a value in
{ITALIC, NON-ITALIC} and is generated condi-
tioned on the previous font style fi−1 and the cur-
rent character token ei. Specifically, after generat-
ing a character token that is not a space, the lan-
guage model deterministically generates the last
font used. If the language model generates a space
character token, the decision of whether to switch
font styles is drawn from a Bernoulli distribution.
This ensures that the font style only changes at
space characters.
The font parameters Φ are extended to contain
entries for the italic versions of all characters. This
means the shapes and widths of italic glyphs can
be learned separately from non-italic ones. Like
Berg-Kirkpatrick et al. (2013), we initialize the
font parameters from mixtures of modern fonts,
using mixtures of modern italic font styles for
italic characters.
</bodyText>
<sectionHeader confidence="0.990818" genericHeader="method">
3 Streamlined Inference
</sectionHeader>
<bodyText confidence="0.999904">
Inference in our extended typesetting models is
costly because the state space is large; we propose
an new inference procedure that is fast and simple.
Berg-Kirkpatrick et al. (2013) used EM to learn
the font parameters Φ, and therefore required ex-
pected sufficient statistics (indicators on (ei, gi, vi)
tuples), which they computed using coarse-to-
fine inference (Petrov et al., 2008; Zhang and
Gildea, 2008) with a semi-Markov dynamic pro-
gram (Levinson, 1986). This approach is effec-
</bodyText>
<page confidence="0.965181">
120
</page>
<figure confidence="0.846477">
Learned typesetting:
Document image:
</figure>
<figureCaption confidence="0.825138">
Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown
in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels.
The second line shows the corresponding portion of the input image.
</figureCaption>
<bodyText confidence="0.999808857142857">
tive, but slow. For example, while transcribing a
typical document consisting of 30 lines of text,
their system spends 63 minutes computing ex-
pected sufficient statistics and decoding when run
on a 4.5GHz 4-core CPU.
We instead use hard counts of the sufficient
statistics for learning (i.e. perform hard-EM). As a
result, we are free to use inference procedures that
are specialized for Viterbi computation. Specif-
ically, we use beam-search with estimated for-
ward costs. Because the model is semi-Markov,
our beam-search procedure is very similar the
one used by Pharaoh (Koehn, 2004) for phrase-
based machine translation, only without a distor-
tion model. We use a beam of size 20, and estimate
forward costs using a character bigram language
model. On the machine mentioned above, tran-
scribing the same document, our simplified system
that uses hard-EM and beam-search spends only
2.4 minutes computing sufficient statistics and de-
coding. This represents a 26x speedup.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99995675862069">
We ran experiments with four different systems.
The first is our baseline, the system presented
by Berg-Kirkpatrick et al. (2013), which we re-
fer to as OCULAR. The second system uses the
original model, but uses beam-search for infer-
ence. We refer to this system as OCULAR-BEAM.
The final two systems use beam-search for infer-
ence, but use extended models: OCULAR-BEAM-
SV uses the slow-varying vertical offset extension
described in Section 2.1 and OCULAR-BEAM-
IT uses the italic font extension described in Sec-
tion 2.2.
We evaluate on two different test sets of histor-
ical documents. The first test set is called Trove,
and is used by Berg-Kirkpatrick et al. (2013) for
evaluation. Trove consists of 10 documents that
were printed between 1803 and 1954, each con-
sisting of 30 lines, all taken from a collection of
historical Australian newspapers hosted by the Na-
tional Library of Australia (Holley, 2010). The
second test set, called Old Bailey, consists of 20
documents that were printed between 1716 and
1906, each consisting of 30 lines, all taken from
a the proceedings of the Old Bailey Courthouse
in London (Shoemaker, 2005).2 Following Berg-
Kirkpatrick et al. (2013), we train the language
model using 36 millions words from the New York
Times portion of the Gigaword corpus (Graff et al.,
2007).3
</bodyText>
<subsectionHeader confidence="0.997608">
4.1 Document Recognition Performance
</subsectionHeader>
<bodyText confidence="0.99997137037037">
We evaluate predicted transcriptions using both
character error rate (CER) and word error rate
(WER). CER is the edit distance between the
guessed transcription and the gold transcription,
divided by the number of characters in the gold
transcription. WER is computed in the same way,
but words are treated as tokens instead of charac-
ters.
First we compare the baseline, OCULAR, to
our system with simplified inference, OCULAR-
BEAM. To our surprise, we found that OCULAR-
BEAM produced better transcriptions than OCU-
LAR. On Trove, OCULAR achieved a WER of
33.0 while OCULAR-BEAM achieved a WER of
30.7. On Old Bailey, OCULAR achieved a WER
of 30.8 while OCULAR-BEAM achieved a WER of
28.8. These results are shown in Table 1, where we
also report the performance of Google Tesseract
(Smith, 2007) and ABBYY FineReader, a state-
of-the-art commercial system, on the Trove test set
(taken from Berg-Kirkpatrick et al. (2013)).
Next, we evaluate our slow-varying vertical off-
set model. OCULAR-BEAM-SV out-performs
OCULAR-BEAM on both test sets. On Trove,
OCULAR-BEAM-SV achieved a WER of 25.6,
and on Old Bailey, OCULAR-BEAM-SV achieved
a WER of 27.5. Overall, compared to our baseline
</bodyText>
<footnote confidence="0.978939875">
2Old Bailey is comparable to the the second test set used
by Berg-Kirkpatrick et al. (2013) since it is derived from the
same collection and covers a similar time span, but it consists
of different documents.
3This means the language model is out-of-domain on both
test sets. Berg-Kirkpatrick et al. (2013) also consider a per-
fectly in-domain language model, though this setting is some-
what unrealistic.
</footnote>
<page confidence="0.996588">
121
</page>
<bodyText confidence="0.999956514285714">
system, OCULAR-BEAM-SV achieved a relative
reduction in WER of 22% on Trove and 11% on
Old Bailey.
By looking at the predicted typesetting layouts
we can make a qualitative comparison between the
vertical offsets predicted by OCULAR-BEAM and
OCULAR-BEAM-SV. Figure 2 shows representa-
tions of the Viterbi estimates of the typesetting
random variables predicted by the models on a
portion of an example document. The first line
is the typesetting layout predicted by OCULAR-
BEAM-SV and the second line is same, but for
OCULAR-BEAM. The locations of padding boxes
are depicted in blue. The white glyph bounding
boxes reveal the values of the Bernoulli template
probabilities used to generate the observed pixels.
The Bernoulli templates are produced from type-
level font parameters, but are modulated by token-
level widths gi and vertical offsets vi (and ink-
ing random variables, whose description we have
omitted for brevity). The predicted vertical off-
sets are visible in the shifted baselines of the tem-
plate probabilities. The third line shows the corre-
sponding portion of the input image. In this ex-
ample, the text baseline predicted by OCULAR-
BEAM-SV is contiguous, while the one predicted
by OCULAR-BEAM is not. Given how OCULAR-
BEAM-SV was designed, this meets our expecta-
tions. The text baseline predicted by OCULAR-
BEAM has a discontinuity in the middle of its pre-
diction for the gold word Surplus. In contrast,
the vertical offsets predicted by OCULAR-BEAM-
SV at this location vary smoothly and more ac-
curately match the true text baseline in the input
image.
</bodyText>
<subsectionHeader confidence="0.992791">
4.2 Font Detection Performance
</subsectionHeader>
<bodyText confidence="0.999927384615385">
We ran experiments with the italic font style
model, OCULAR-BEAM-IT, on the Old Bai-
ley test set (italics are infrequent in Trove). We
evaluated the learned styles by measuring how ac-
curately OCULAR-BEAM-IT was able to distin-
guish between italic and non-italic styles. Specifi-
cally, we computed the precision and recall for the
system’s predictions about which words were ital-
icized. We found that, across the entire Old Bai-
ley test set, OCULAR-BEAM-IT was able to detect
which words were italicized with 93% precision
at 74% recall, suggesting that the system did suc-
cessfully learn both italic and non-italic styles.4
</bodyText>
<footnote confidence="0.9634165">
4While it seems plausible that learning italics could also
improve transcription accuracy, we found that OCULAR-
</footnote>
<table confidence="0.998901">
System CER WER
Trove
Google Tesseract 37.5 59.3
ABBYY FineReader 22.9 49.2
OCULAR (baseline) 14.9 33.0
OCULAR-BEAM 12.9 30.7
OCULAR-BEAM-SV 11.2 25.6
Old Bailey
OCULAR (baseline) 14.9 30.8
OCULAR-BEAM 10.9 28.8
OCULAR-BEAM-SV 10.3 27.5
</table>
<tableCaption confidence="0.9191394">
Table 1: We evaluate the output of each system on two test
sets: Trove, a collection of historical newspapers, and Old
Bailey, a collection of historical court proceedings. We report
character error rate (CER) and word error rate (WER), macro-
averaged across documents.
</tableCaption>
<bodyText confidence="0.999972214285714">
We can look at the typesetting layout predicted
by OCULAR-BEAM-IT to gain insight into what
has been learned by the model. The first line of
Figure 3 shows the typesetting layout predicted by
the OCULAR-BEAM-IT model for a line of a doc-
ument image that contains italics. The second line
of Figure 3 displays the corresponding portion of
the input document image. From this example,
it appears that the model has effectively learned
separate glyph shapes for italic and non-italic ver-
sions of certain characters. For example, compare
the template probabilities used to generate the d’s
in defraud to the template probabilities used to
generate the d in hard.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999779">
We began with an efficient simplification of the
state-of-the-art historical OCR system of Berg-
Kirkpatrick et al. (2013) and demonstrated two ex-
tensions to its underlying model. We saw an im-
provement in transcription quality as a result of re-
moving a harmful independence assumption. This
suggests that it may be worthwhile to consider still
further extensions of the model, designed to more
faithfully reflect the generative process that pro-
duced the input documents.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.978216142857143">
This work was supported by Grant IIS-1018733
from the National Science Foundation and also a
National Science Foundation fellowship to the first
author.
BEAM-IT actually performed slightly worse than OCULAR-
BEAM. This negative result is possibly due to the extra diffi-
culty of learning a larger number of font parameters.
</bodyText>
<page confidence="0.996724">
122
</page>
<sectionHeader confidence="0.993842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999643404761905">
Kenning Arlitsch and John Herbert. 2004. Microfilm,
paper, and OCR: Issues in newspaper digitization.
the Utah digital newspapers program. Microform &amp;
Imaging Review.
Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein.
2013. Unsupervised transcription of historical doc-
uments. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword third edi-
tion. Linguistic Data Consortium, Catalog Number
LDC2007T07.
Rose Holley. 2010. Trove: Innovation in access to
information in Australia. Ariadne.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Machine translation: From real
users to research, pages 115–124. Springer.
Stephen Levinson. 1986. Continuously variable du-
ration hidden Markov models for automatic speech
recognition. Computer Speech &amp; Language.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Shoemaker. 2005. Digital London: Creating a
searchable web of interlinked sources on eighteenth
century London. Electronic Library and Informa-
tion Systems.
Ray Smith. 2007. An overview of the Tesseract OCR
engine. In Proceedings of the Ninth International
Conference on Document Analysis and Recognition.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing.
</reference>
<page confidence="0.998939">
123
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.867166">
<title confidence="0.997508">Improved Typesetting Models for Historical OCR</title>
<author confidence="0.999388">Taylor Berg-Kirkpatrick Dan</author>
<affiliation confidence="0.99992">Computer Science University of California,</affiliation>
<abstract confidence="0.993704238095238">We present richer typesetting models that extend the unsupervised historical recognition system of Berg- Kirkpatrick et al. (2013). The model breaks the independence assumption between vertical offsets of neighboring glyphs and, in experiments, substantially decreases transcription error rates. The second model simultaneously learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by Berg- Kirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenning Arlitsch</author>
<author>John Herbert</author>
</authors>
<title>Microfilm, paper, and OCR: Issues in newspaper digitization. the Utah digital newspapers program.</title>
<date>2004</date>
<journal>Microform &amp; Imaging Review.</journal>
<contexts>
<context position="1145" citStr="Arlitsch and Herbert, 2004" startWordPosition="165" endWordPosition="168">ly learns multiple font styles and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers. 1 Introduction Modern OCR systems perform poorly on historical documents from the printing-press era, often yielding error rates that are too high for downstream research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). The two primary reasons that historical documents present difficultly for automatic systems are (1) the typesetting process used to produce such documents was extremely noisy and (2) the fonts used in the documents are unknown. Berg-Kirkpatrick et al. (2013) proposed a system for historical OCR that generatively models the noisy typesetting process of printing-press era documents and learns the font for each input document in an unsupervised fashion. Their system achieves state-of-the-art results on the task of historical document recognition. We take the syst</context>
</contexts>
<marker>Arlitsch, Herbert, 2004</marker>
<rawString>Kenning Arlitsch and John Herbert. 2004. Microfilm, paper, and OCR: Issues in newspaper digitization. the Utah digital newspapers program. Microform &amp; Imaging Review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised transcription of historical documents.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1437" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="209" endWordPosition="212">system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers. 1 Introduction Modern OCR systems perform poorly on historical documents from the printing-press era, often yielding error rates that are too high for downstream research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). The two primary reasons that historical documents present difficultly for automatic systems are (1) the typesetting process used to produce such documents was extremely noisy and (2) the fonts used in the documents are unknown. Berg-Kirkpatrick et al. (2013) proposed a system for historical OCR that generatively models the noisy typesetting process of printing-press era documents and learns the font for each input document in an unsupervised fashion. Their system achieves state-of-the-art results on the task of historical document recognition. We take the system of Berg-Kirkpatrick et al. (2013) as a starting point and consider extensions of the typesetting model that address two shortcomings of their model: (1) their layout model assumes that baseline offset noise is independent for each glyph and (2) their font model assumes a single font is us</context>
<context position="3821" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="598" endWordPosition="601"> corpus (Shoemaker, 2005). In our second extension, we incorporate a Markov chain over font styles, extending the original model so that it is capable of simultaneously learning italic and non-italic fonts within a single document. In experiments, this model is able to detect which words are italicized with 93% precision at 74% recall in a test set of historical court proceedings (see Section 4.2). These richer models that we propose do increase the state space and therefore make inference more costly. To remedy this, we streamline inference by replacing the coarse-to-fine inference scheme of Berg-Kirkpatrick et al. (2013) 118 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 118–123, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Slow-vary Original model Italic Font Character Glyph width Pad width Pixels Vertical offset 9i-1 pi-1 ei-1 A-1 9i A e2 Xi LYPH XiPA_1 i LYPH XiPAD vz-1 vz Figure 1: See Section 2 for a description of the generative process. We consider an extension of Berg-Kirkpatrick et al. (2013) that generates vi conditioned on the previous vertical offset vi−1 (labeled Slow-vary) and an extensi</context>
<context position="5745" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="921" endWordPosition="924">rated, conditioned on the character ei. gi specifies the width of the bounding box that will eventually house the pixels of the glyph for character ei. pi specifies the width of a padding box which contains the horizontal space before the next character begins. Next, a vertical offset vi is generated for the glyph corresponding to character ei. vi allows the model to capture variance in the baseline of the text in the document. We will later let vi depend on vi−1, as depicted in Figure 1, but in the baseline 1The model we describe and extend has two minor differences from the one described by Berg-Kirkpatrick et al. (2013). While Berg-Kirkpatrick et al. (2013) generate two pad boxes for each character token, one to the left and one to the right, we only generate one pad box, always to the right. Additionally, Berg-Kirkpatrick et al. (2013) do not carry over the language model context between lines, while we do. system they are independent. Finally, the pixels in the ith glyph bounding box XGLYPH are generated i conditioned on the character ei, width gi, and vertical offset vi, and the pixels in the ith pad bounding box XPAD i are generated conditioned on the width pi. We refer the reader to Berg-Kirkpatrick et </context>
<context position="10153" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="1676" endWordPosition="1679"> font style fi−1 and the current character token ei. Specifically, after generating a character token that is not a space, the language model deterministically generates the last font used. If the language model generates a space character token, the decision of whether to switch font styles is drawn from a Bernoulli distribution. This ensures that the font style only changes at space characters. The font parameters Φ are extended to contain entries for the italic versions of all characters. This means the shapes and widths of italic glyphs can be learned separately from non-italic ones. Like Berg-Kirkpatrick et al. (2013), we initialize the font parameters from mixtures of modern fonts, using mixtures of modern italic font styles for italic characters. 3 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei, gi, vi) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic progr</context>
<context position="12267" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="2007" endWordPosition="2010">the model is semi-Markov, our beam-search procedure is very similar the one used by Pharaoh (Koehn, 2004) for phrasebased machine translation, only without a distortion model. We use a beam of size 20, and estimate forward costs using a character bigram language model. On the machine mentioned above, transcribing the same document, our simplified system that uses hard-EM and beam-search spends only 2.4 minutes computing sufficient statistics and decoding. This represents a 26x speedup. 4 Results We ran experiments with four different systems. The first is our baseline, the system presented by Berg-Kirkpatrick et al. (2013), which we refer to as OCULAR. The second system uses the original model, but uses beam-search for inference. We refer to this system as OCULAR-BEAM. The final two systems use beam-search for inference, but use extended models: OCULAR-BEAMSV uses the slow-varying vertical offset extension described in Section 2.1 and OCULAR-BEAMIT uses the italic font extension described in Section 2.2. We evaluate on two different test sets of historical documents. The first test set is called Trove, and is used by Berg-Kirkpatrick et al. (2013) for evaluation. Trove consists of 10 documents that were printed</context>
<context position="14396" citStr="Berg-Kirkpatrick et al. (2013)" startWordPosition="2359" endWordPosition="2362">t words are treated as tokens instead of characters. First we compare the baseline, OCULAR, to our system with simplified inference, OCULARBEAM. To our surprise, we found that OCULARBEAM produced better transcriptions than OCULAR. On Trove, OCULAR achieved a WER of 33.0 while OCULAR-BEAM achieved a WER of 30.7. On Old Bailey, OCULAR achieved a WER of 30.8 while OCULAR-BEAM achieved a WER of 28.8. These results are shown in Table 1, where we also report the performance of Google Tesseract (Smith, 2007) and ABBYY FineReader, a stateof-the-art commercial system, on the Trove test set (taken from Berg-Kirkpatrick et al. (2013)). Next, we evaluate our slow-varying vertical offset model. OCULAR-BEAM-SV out-performs OCULAR-BEAM on both test sets. On Trove, OCULAR-BEAM-SV achieved a WER of 25.6, and on Old Bailey, OCULAR-BEAM-SV achieved a WER of 27.5. Overall, compared to our baseline 2Old Bailey is comparable to the the second test set used by Berg-Kirkpatrick et al. (2013) since it is derived from the same collection and covers a similar time span, but it consists of different documents. 3This means the language model is out-of-domain on both test sets. Berg-Kirkpatrick et al. (2013) also consider a perfectly in-dom</context>
</contexts>
<marker>Berg-Kirkpatrick, Durrett, Klein, 2013</marker>
<rawString>Taylor Berg-Kirkpatrick, Greg Durrett, and Dan Klein. 2013. Unsupervised transcription of historical documents. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword third edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</title>
<date>2007</date>
<contexts>
<context position="13439" citStr="Graff et al., 2007" startWordPosition="2204" endWordPosition="2207">rove consists of 10 documents that were printed between 1803 and 1954, each consisting of 30 lines, all taken from a collection of historical Australian newspapers hosted by the National Library of Australia (Holley, 2010). The second test set, called Old Bailey, consists of 20 documents that were printed between 1716 and 1906, each consisting of 30 lines, all taken from a the proceedings of the Old Bailey Courthouse in London (Shoemaker, 2005).2 Following BergKirkpatrick et al. (2013), we train the language model using 36 millions words from the New York Times portion of the Gigaword corpus (Graff et al., 2007).3 4.1 Document Recognition Performance We evaluate predicted transcriptions using both character error rate (CER) and word error rate (WER). CER is the edit distance between the guessed transcription and the gold transcription, divided by the number of characters in the gold transcription. WER is computed in the same way, but words are treated as tokens instead of characters. First we compare the baseline, OCULAR, to our system with simplified inference, OCULARBEAM. To our surprise, we found that OCULARBEAM produced better transcriptions than OCULAR. On Trove, OCULAR achieved a WER of 33.0 wh</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword third edition. Linguistic Data Consortium, Catalog Number LDC2007T07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rose Holley</author>
</authors>
<title>Trove: Innovation in access to information in</title>
<date>2010</date>
<journal>Australia. Ariadne.</journal>
<contexts>
<context position="1177" citStr="Holley, 2010" startWordPosition="171" endWordPosition="172">lt, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers. 1 Introduction Modern OCR systems perform poorly on historical documents from the printing-press era, often yielding error rates that are too high for downstream research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). The two primary reasons that historical documents present difficultly for automatic systems are (1) the typesetting process used to produce such documents was extremely noisy and (2) the fonts used in the documents are unknown. Berg-Kirkpatrick et al. (2013) proposed a system for historical OCR that generatively models the noisy typesetting process of printing-press era documents and learns the font for each input document in an unsupervised fashion. Their system achieves state-of-the-art results on the task of historical document recognition. We take the system of Berg-Kirkpatrick et al. (2</context>
<context position="13042" citStr="Holley, 2010" startWordPosition="2139" endWordPosition="2140">o systems use beam-search for inference, but use extended models: OCULAR-BEAMSV uses the slow-varying vertical offset extension described in Section 2.1 and OCULAR-BEAMIT uses the italic font extension described in Section 2.2. We evaluate on two different test sets of historical documents. The first test set is called Trove, and is used by Berg-Kirkpatrick et al. (2013) for evaluation. Trove consists of 10 documents that were printed between 1803 and 1954, each consisting of 30 lines, all taken from a collection of historical Australian newspapers hosted by the National Library of Australia (Holley, 2010). The second test set, called Old Bailey, consists of 20 documents that were printed between 1716 and 1906, each consisting of 30 lines, all taken from a the proceedings of the Old Bailey Courthouse in London (Shoemaker, 2005).2 Following BergKirkpatrick et al. (2013), we train the language model using 36 millions words from the New York Times portion of the Gigaword corpus (Graff et al., 2007).3 4.1 Document Recognition Performance We evaluate predicted transcriptions using both character error rate (CER) and word error rate (WER). CER is the edit distance between the guessed transcription an</context>
</contexts>
<marker>Holley, 2010</marker>
<rawString>Rose Holley. 2010. Trove: Innovation in access to information in Australia. Ariadne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="4943" citStr="Kneser and Ney, 1995" startWordPosition="774" endWordPosition="777">at generates vi conditioned on the previous vertical offset vi−1 (labeled Slow-vary) and an extension that generates a sequence of font styles fi (labeled Italic). with a forward-cost-augmented beaming scheme. Our method is over 25x faster on a typical document, yet actually yields improved transcriptions. 2 Model We first describe the generative model used by the ‘Ocular’ historical OCR system of BergKirkpatrick et al. (2013)1 and then describe our extensions. The graphical model corresponding to their basic generative process for a single line of text is diagrammed in Figure 1. A KneserNey (Kneser and Ney, 1995) character 6-gram language model generates a sequence of characters E = (e1, e2, ... , en). For each character index i, a glyph box width gi and a pad box width pi are generated, conditioned on the character ei. gi specifies the width of the bounding box that will eventually house the pixels of the glyph for character ei. pi specifies the width of a padding box which contains the horizontal space before the next character begins. Next, a vertical offset vi is generated for the glyph corresponding to character ei. vi allows the model to capture variance in the baseline of the text in the docume</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the International Conference on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Machine translation: From real users to research,</title>
<date>2004</date>
<pages>115--124</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="11742" citStr="Koehn, 2004" startWordPosition="1926" endWordPosition="1927">ortion of the input image. tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computing expected sufficient statistics and decoding when run on a 4.5GHz 4-core CPU. We instead use hard counts of the sufficient statistics for learning (i.e. perform hard-EM). As a result, we are free to use inference procedures that are specialized for Viterbi computation. Specifically, we use beam-search with estimated forward costs. Because the model is semi-Markov, our beam-search procedure is very similar the one used by Pharaoh (Koehn, 2004) for phrasebased machine translation, only without a distortion model. We use a beam of size 20, and estimate forward costs using a character bigram language model. On the machine mentioned above, transcribing the same document, our simplified system that uses hard-EM and beam-search spends only 2.4 minutes computing sufficient statistics and decoding. This represents a 26x speedup. 4 Results We ran experiments with four different systems. The first is our baseline, the system presented by Berg-Kirkpatrick et al. (2013), which we refer to as OCULAR. The second system uses the original model, b</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Machine translation: From real users to research, pages 115–124. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Levinson</author>
</authors>
<title>Continuously variable duration hidden Markov models for automatic speech recognition.</title>
<date>1986</date>
<journal>Computer Speech &amp; Language.</journal>
<contexts>
<context position="10772" citStr="Levinson, 1986" startWordPosition="1774" endWordPosition="1775"> initialize the font parameters from mixtures of modern fonts, using mixtures of modern italic font styles for italic characters. 3 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei, gi, vi) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic program (Levinson, 1986). This approach is effec120 Learned typesetting: Document image: Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels. The second line shows the corresponding portion of the input image. tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computing expected sufficient statistics and decoding when run on a 4.5GHz 4-core CPU</context>
</contexts>
<marker>Levinson, 1986</marker>
<rawString>Stephen Levinson. 1986. Continuously variable duration hidden Markov models for automatic speech recognition. Computer Speech &amp; Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coarse-to-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10695" citStr="Petrov et al., 2008" startWordPosition="1760" endWordPosition="1763"> learned separately from non-italic ones. Like Berg-Kirkpatrick et al. (2013), we initialize the font parameters from mixtures of modern fonts, using mixtures of modern italic font styles for italic characters. 3 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei, gi, vi) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic program (Levinson, 1986). This approach is effec120 Learned typesetting: Document image: Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels. The second line shows the corresponding portion of the input image. tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computin</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>Slav Petrov, Aria Haghighi, and Dan Klein. 2008. Coarse-to-fine syntactic machine translation using language projections. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Shoemaker</author>
</authors>
<title>Digital London: Creating a searchable web of interlinked sources on eighteenth century London. Electronic Library and Information Systems.</title>
<date>2005</date>
<contexts>
<context position="1162" citStr="Shoemaker, 2005" startWordPosition="169" endWordPosition="170">es and, as a result, is able to accurately track italic and nonitalic portions of documents. Richer models complicate inference so we present a new, streamlined procedure that is over 25x faster than the method used by BergKirkpatrick et al. (2013). Our final system achieves a relative word error reduction of 22% compared to state-of-the-art results on a dataset of historical newspapers. 1 Introduction Modern OCR systems perform poorly on historical documents from the printing-press era, often yielding error rates that are too high for downstream research projects (Arlitsch and Herbert, 2004; Shoemaker, 2005; Holley, 2010). The two primary reasons that historical documents present difficultly for automatic systems are (1) the typesetting process used to produce such documents was extremely noisy and (2) the fonts used in the documents are unknown. Berg-Kirkpatrick et al. (2013) proposed a system for historical OCR that generatively models the noisy typesetting process of printing-press era documents and learns the font for each input document in an unsupervised fashion. Their system achieves state-of-the-art results on the task of historical document recognition. We take the system of Berg-Kirkpa</context>
<context position="3216" citStr="Shoemaker, 2005" startWordPosition="501" endWordPosition="502">ng large changes in offset. We find that this extension decreases transcription error rates. Our system achieves a relative word error reduction of 22% compared to the state-ofthe-art original model on a test set of historical newspapers (see Section 4.1), and a 11% relative reduction on a test set of historical court proceedings. Multiple font styles are also frequently used in printing-press era documents; the most common scenario is for a basic font style to co-occur with an italic variant. For example, it is common for proper nouns and quotations to be italicized in the Old Bailey corpus (Shoemaker, 2005). In our second extension, we incorporate a Markov chain over font styles, extending the original model so that it is capable of simultaneously learning italic and non-italic fonts within a single document. In experiments, this model is able to detect which words are italicized with 93% precision at 74% recall in a test set of historical court proceedings (see Section 4.2). These richer models that we propose do increase the state space and therefore make inference more costly. To remedy this, we streamline inference by replacing the coarse-to-fine inference scheme of Berg-Kirkpatrick et al. (</context>
<context position="8921" citStr="Shoemaker, 2005" startWordPosition="1471" endWordPosition="1473">d the model to capture it. In our first extension, we augment the model by incorporating a Markov chain over the vertical offset random variables vi, as depicted in Figure 1. Specifically, vi is generated from a discretized Gaussian centered at vi−1: P(vi|vi−1) a exp C(vi − vi−1)21 2u2 J This means that the if vi differs substantially from vi−1, a large penalty is incurred. As a result, the model should prefer sequences of vi that vary slowly. In experiments, we set u2 = 0.05. 2.2 Italic Font Styles Many of the documents in the Old Bailey corpus contain both italic and non-italic font styles (Shoemaker, 2005). The way that italic fonts are used depends on the year the document was printed, but generally italics are reserved for proper nouns, quotations, and sentences that have a special role (e.g. the final judgment made in a court case). The switch between font styles almost always occurs at space characters. Our second extension of the typesetting model deals with both italic and non-italic font styles. We augment the model with a Markov chain over font styles fi, as depicted in Figure 1. Each font style token fi takes on a value in {ITALIC, NON-ITALIC} and is generated conditioned on the previo</context>
<context position="13268" citStr="Shoemaker, 2005" startWordPosition="2177" endWordPosition="2178">2.2. We evaluate on two different test sets of historical documents. The first test set is called Trove, and is used by Berg-Kirkpatrick et al. (2013) for evaluation. Trove consists of 10 documents that were printed between 1803 and 1954, each consisting of 30 lines, all taken from a collection of historical Australian newspapers hosted by the National Library of Australia (Holley, 2010). The second test set, called Old Bailey, consists of 20 documents that were printed between 1716 and 1906, each consisting of 30 lines, all taken from a the proceedings of the Old Bailey Courthouse in London (Shoemaker, 2005).2 Following BergKirkpatrick et al. (2013), we train the language model using 36 millions words from the New York Times portion of the Gigaword corpus (Graff et al., 2007).3 4.1 Document Recognition Performance We evaluate predicted transcriptions using both character error rate (CER) and word error rate (WER). CER is the edit distance between the guessed transcription and the gold transcription, divided by the number of characters in the gold transcription. WER is computed in the same way, but words are treated as tokens instead of characters. First we compare the baseline, OCULAR, to our sys</context>
</contexts>
<marker>Shoemaker, 2005</marker>
<rawString>Robert Shoemaker. 2005. Digital London: Creating a searchable web of interlinked sources on eighteenth century London. Electronic Library and Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Smith</author>
</authors>
<title>An overview of the Tesseract OCR engine.</title>
<date>2007</date>
<booktitle>In Proceedings of the Ninth International Conference on Document Analysis and Recognition.</booktitle>
<contexts>
<context position="14272" citStr="Smith, 2007" startWordPosition="2342" endWordPosition="2343">iption, divided by the number of characters in the gold transcription. WER is computed in the same way, but words are treated as tokens instead of characters. First we compare the baseline, OCULAR, to our system with simplified inference, OCULARBEAM. To our surprise, we found that OCULARBEAM produced better transcriptions than OCULAR. On Trove, OCULAR achieved a WER of 33.0 while OCULAR-BEAM achieved a WER of 30.7. On Old Bailey, OCULAR achieved a WER of 30.8 while OCULAR-BEAM achieved a WER of 28.8. These results are shown in Table 1, where we also report the performance of Google Tesseract (Smith, 2007) and ABBYY FineReader, a stateof-the-art commercial system, on the Trove test set (taken from Berg-Kirkpatrick et al. (2013)). Next, we evaluate our slow-varying vertical offset model. OCULAR-BEAM-SV out-performs OCULAR-BEAM on both test sets. On Trove, OCULAR-BEAM-SV achieved a WER of 25.6, and on Old Bailey, OCULAR-BEAM-SV achieved a WER of 27.5. Overall, compared to our baseline 2Old Bailey is comparable to the the second test set used by Berg-Kirkpatrick et al. (2013) since it is derived from the same collection and covers a similar time span, but it consists of different documents. 3This </context>
</contexts>
<marker>Smith, 2007</marker>
<rawString>Ray Smith. 2007. An overview of the Tesseract OCR engine. In Proceedings of the Ninth International Conference on Document Analysis and Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10720" citStr="Zhang and Gildea, 2008" startWordPosition="1764" endWordPosition="1767">rom non-italic ones. Like Berg-Kirkpatrick et al. (2013), we initialize the font parameters from mixtures of modern fonts, using mixtures of modern italic font styles for italic characters. 3 Streamlined Inference Inference in our extended typesetting models is costly because the state space is large; we propose an new inference procedure that is fast and simple. Berg-Kirkpatrick et al. (2013) used EM to learn the font parameters Φ, and therefore required expected sufficient statistics (indicators on (ei, gi, vi) tuples), which they computed using coarse-tofine inference (Petrov et al., 2008; Zhang and Gildea, 2008) with a semi-Markov dynamic program (Levinson, 1986). This approach is effec120 Learned typesetting: Document image: Figure 3: This first line depicts the Viterbi typesetting layout predicted by the OCULAR-BEAM-IT model. Pad boxes are shown in blue. Glyphs boxes are shown in white and display the Bernoulli template probabilities used to generate the observed pixels. The second line shows the corresponding portion of the input image. tive, but slow. For example, while transcribing a typical document consisting of 30 lines of text, their system spends 63 minutes computing expected sufficient sta</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>