<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006356">
<title confidence="0.999735">
A Clustering Approach for
Unsupervised Chinese Coreference Resolution
</title>
<author confidence="0.997807">
Chi-shing Wang Grace NGAI
</author>
<affiliation confidence="0.9309435">
Department of Computing
Hong Kong Polytechnic University
</affiliation>
<sectionHeader confidence="0.370462" genericHeader="abstract">
Kowloon, HONG KONG
</sectionHeader>
<email confidence="0.922311">
{cscswang, csgngai}@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.996546" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999945545454545">
Coreference resolution is the process of
identifying expressions that refer to the same
entity. This paper presents a clustering algo-
rithm for unsupervised Chinese coreference
resolution. We investigate why Chinese
coreference is hard and demonstrate that
techniques used in coreference resolution for
English can be extended to Chinese. The
proposed system exploits clustering as it has
advantages over traditional classification
methods, such as the fact that no training
data is required and it is easily extended to
accommodate additional features. We con-
duct a set of experiments to investigate how
noun phrase identification and feature selec-
tion can contribute to coreference resolution
performance. Our system is evaluated on an
annotated version of the TDT3 corpus using
the MUC-7 scorer, and obtains comparable
performance. We believe that this is the first
attempt at an unsupervised approach to Chi-
nese noun phrase coreference resolution.
</bodyText>
<sectionHeader confidence="0.999552" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.992232764705882">
Noun phrase coreference resolution is the proc-
ess of detecting noun phrases (NPs) in a docu-
ment and determining whether the NPs refer to
the same entity, where an entity is defined as “a
construct that represents an abstract identity”.
The NPs that refer to the entity are known as
mentions. Mentions can be antecedents or ana-
phors. An anaphor is an expression that refers
back to a previous expression in a discourse. In
Figure 1, 克林頓總統 (President Clinton) refers
to 克林頓 (Clinton) and is described as an ana-
phoric reference to克林頓 (Clinton). 克林頓總
統 (President Clinton) is described as the antece-
dent of 他 (he). 克林頓 (Clinton), 克林頓總統
(President Clinton) and the second 他 (he) are all
mentions of the same entity that refers to former
U.S. president Bill Clinton.
</bodyText>
<figureCaption confidence="0.796793">
Figure 1: An excerpt from the text, with core-
ferring noun phrases annotated. English trans-
lation in italics.
</figureCaption>
<bodyText confidence="0.996472466666667">
NP coreference resolution is an important sub-
task in natural language processing (NLP) appli-
cations such as text summarization, information
extraction, data mining and question answering.
This task has attracted much attention in recent
years (Cardie and Wagstaff, 1999; Harabagiu et
al., 2001; Soon et al., 2001; Ng and Cardie,
2002; Yang et al., 2004; Florian et al., 2004;
Zhou et al., 2005), and has been included as a
subtask in the MUC (Message Understanding
Conferences) and ACE (Automatic Content
Extraction) competitions.
Coreference resolution is a difficult task for
various reasons. Firstly, a list of features can play
a role to support coreference resolution such as
</bodyText>
<figure confidence="0.7459154">
[克林頓1]說,華盛頓將逐步落實對[韓國2]的
經濟援助。[金大中3]對[克林頓1]的講話報以
掌聲。[他3]說:「[克林頓總統1]在會談中重
申,[他1]堅定地支持[韓國2]擺脫經濟危
機。」
</figure>
<bodyText confidence="0.910651571428571">
[Clinton1] said that Washington would progres-
sively follow through on economic aid to [Ko-
rea2]. [Kim Dae-Jung3] applauded [Clinton1]’s
speech. [He1] said, “[President Clinton1] reiter-
ated in the talks that [he1] would provide solid
support for [Korea2] to shake off the economic
crisis.
</bodyText>
<page confidence="0.97474">
40
</page>
<bodyText confidence="0.947181857142857">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 40–47,
Sydney, July 2006. c�2006 Association for Computational Linguistics
segmentation system (Lancashire, 2005) to seg-
ment the Chinese characters into words. The
segmented words are then labeled with POS tags
by a statistical POS tagging system (Fung et al.,
2004).
</bodyText>
<sectionHeader confidence="0.947514" genericHeader="method">
3 Mention Detection
</sectionHeader>
<bodyText confidence="0.997898729166667">
After the corpus has been preprocessed, mention
detection involves the identification of NPs in
the corpus that refer to some entity. Most of
these NPs correspond to non-recursive NPs,
which makes this task simpler as most syntactic
parsers identify NPs as part of the parsing proc-
ess. This approach, however, suffers from two
problems: firstly, the parser itself is unlikely to
be 100% accurate; and secondly, the boundaries
of the NPs identified by the parser may not cor-
respond exactly with those of the entities identi-
fied by the human annotator.
Another approach is simply to use heuristics
based on the POS tag sequence to identify poten-
tial NPs of interest. The advantage of this
method is that the NPs thus extracted should be
closer to the human-annotated entities since the
heuristics will be constructed specifically for this
task.
To investigate the effect of different ap-
proaches on the result of the coreference resolu-
tion, we applied both methods separately to our
corpus. The corpus was parsed with a state-of-
the-art multilingual statistical parser (Bikel
2004), which is trained on the Chinese Penn
Treebank. After parsing, we extracted all non-
recursive NP chunks tagged by the parser as pos-
sible mentions.
For the heuristic-based approach, we applied
a few simple heuristics, which had been previ-
ously developed during unrelated work for Eng-
lish named-entity resolution (i.e. they were not
written with foreknowledge of the gold standard
entities) and which are based on the part-of-
speech tags of the words. Some examples of our
heuristics were to look for pronouns, or to extract
all noun sequences, or sequences of determiners
followed by adjectives and nouns.
Table 1 shows the performance of the pars-
ing-based approach versus the heuristic-based
approach. The parser-based approach suffers
姐,
mainly because the NPs that it extracts tend to be
on the long side, resulting in recall errors when
the boundaries of the parser-identified NPs mis-
match with the human-annotated entities. In ad-
dition, the parser also tends to extract more NPs
than needed, which results in a hit to precision.
</bodyText>
<sectionHeader confidence="0.962289" genericHeader="method">
4 Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999905666666667">
The final step after the mention detection phase
is to determine which of the extracted phrases
refer to the same entity, or are coreferent.
The small size of our corpus made it quite ob-
vious that we would not be able to perform su-
pervised learning, as there would not be enough
data for generalization purposes. Therefore we
chose to use an unsupervised clustering approach
for this step. Clustering is a natural choice as it
partitions the data into groups; used on corefer-
ence resolution, we expect to gather coreferrent
NPs into the same cluster. Furthermore, most
clustering methods can easily incorporate both
context-dependent and independent constraints
into their features.
</bodyText>
<subsectionHeader confidence="0.91107">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.99945525">
Our features use both lexical and syntactic in-
formation designed to capture both the content of
the phrase and its role within the sentence. With
the exception of the last three features, which are
defined with respect to a noun phrase pair, all our
features describe various aspects of a single noun
phrase:
Lexical String – This is just simply the string of
words in the phrase.
Head Noun – The head noun in a phrase is the
noun that is not a modifier for another noun.
Sentence Position – This measures the position
of the phrase within the document.
Gender – For each phrase, we use a gazetteer to
assign it a gender. The possible values are male
(e.g. AI, mister), female (e.g. 小
</bodyText>
<page confidence="0.994967">
42
</page>
<bodyText confidence="0.9829205">
Semantic Class – To give the system more in-
formation on each phrase, we generated our own
gazetteer from a combination of gazetteers com-
piled from web sources and heuristics. Our gaz-
etteer consists of 4700 entries, each of which is
labeled with one of the following semantic
classes: person, organization, location, facility,
GPE, date, money, vehicle and weapon. Phrases
in the corpus that are found in the gazetteer are
given the same semantic class label; phrases not
in the gazetteer are marked as UNKNOWN.
Proper Name – The part-of-speech tag “NR”
and a list of common proper names were used to
label each noun phrase as to whether it is a
proper name (values: true/false).
Pronoun – Determined by the part-of-speech
“PN”. Values: true/false.
Demonstrative Noun Phrase – A demonstrative
noun phrase is a phrase that consists of a noun
phrases preceded by one of the characters [
</bodyText>
<equation confidence="0.491802">
這那
</equation>
<page confidence="0.994154">
43
</page>
<bodyText confidence="0.9996925">
are very commonly used in Chinese and even
though the previous feature will work on most of
them, there are some common exceptions. To
make sure that we catch those as well, we intro-
duced a Chinese-specific feature as a further test.
Since abbreviations and nicknames are not usu-
ally substrings of the original strings, but will
still share some common characters, we measure
the Levenshtein distance, defined as the number
of character insertions, deletions and substitu-
tions, between every potential antecedent-
anaphor pair.
</bodyText>
<subsectionHeader confidence="0.99379">
4.2 Distance Metric
</subsectionHeader>
<bodyText confidence="0.9995095">
In order for the clustering algorithm to be able to
group instances together by similarity, we need
to determine a distance metric between two in-
stances – in our case, two noun phrases. For our
system, we borrowed a simple distance metric
from Cardie and Wagstaff (1999) that sums up
the results of a series of functions over the two
phrases:
</bodyText>
<equation confidence="0.902517">
dist NP NP
( i , j ) _ ∀ functionf (NP, NP ) f ! F
</equation>
<bodyText confidence="0.999946096774194">
Table 2 presents the features and the correspond-
ing functions that were used in our system. Each
function calculates a distance between the two
phrases that is an indicator of the degree of in-
compatibility between the two phrases with re-
spect to a particular feature. The NOUN
PHRASE, HEAD NOUN, DEMONSTRATIVE,
APPOSITIVE and ABBREVIATIVE functions
test for compatibility and return a negative value
when the two phrases are compatible for that
term’s feature. The reason for the negative value
returned is that if the two phrases match on this
particular feature, then it is a strong indicator of
coreference. Therefore, we reduce the distance
between two phrases, making it more likely that
they will be clustered together into the same en-
tity. When there is a mismatch, however, it does
not necessarily indicate that the two NPs are non-
coreferential, so we leave the distance between
the NPs unchanged.
Conversely, there are some features where a
mismatch would indicate that the two NPs are
absolutely non-compatible and will definitely not
refer to the same entity. The DISTANCE,
GENDER, NUMBER, SEMANTIC, PROPER
NAME, PRONOUN and EDIT_DISTANCE
functions return a positive value when the two
phrases mismatch on that particular feature. A
positive value results in a greater distance be-
tween two phrases, which makes it less likely for
them to be clustered together.
</bodyText>
<subsectionHeader confidence="0.999227">
4.3 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.9999458">
Most of the previous work in clustering-based
noun phrase coreference resolution has centered
around the use of bottom-up clustering methods,
where each noun phrase is initially assigned to a
singleton cluster by itself, and clusters which are
“close enough” to each other are merged (Cardie
&amp; Wagstaff, 1999; Angheluta et al., 2004).
In our system, we use a method called modi-
fied k-means clustering (Wilpon &amp; Rabiner
1985), which takes the opposite approach and
uses a top-down approach to split clusters, inter-
leaved with a k-means iterative phase. Modified
k-means clustering has been successfully applied
to speech recognition and it has the advantage of
always being able to come to the optimal cluster-
ing (i.e. it is not dependent upon the starting state
or merging order).
Modified k-means starts off with all the in-
stances in one big cluster. The system then itera-
tively performs the following steps:
</bodyText>
<listItem confidence="0.974902909090909">
1. For each cluster, find its centroid, de-
fined as the instance which is the closest
to all other instances in the same cluster.
2. For each instance:
a. Calculate its distance to all the
centroids.
b. Find the centroid with the mini-
mum distance, and join its clus-
ter.
3. Iterate 1-2 until instances stop moving
between clusters.
4. Find the cluster with the largest intra-
cluster distance. (Call this Clustermax and
its centroid, Centroidmax.) If this distance
is smaller than some threshold r, stop.
5. From the instances inside Clustermax, find
the pair that are the furthest apart from
each other.
a. Add the pair of instances to the
list of centroids and remove
Centroidmax from the list.
b. Repeat from Step 2.
</listItem>
<bodyText confidence="0.999822909090909">
The algorithm thus alternates traditional k-means
clustering with a step that adds new clusters to
the pool of existing ones. Used for coreference
resolution, it splits up the instances into clusters
in which the instances are more similar to each
other than to instances in other clusters.
The only thing left to do is to determine a suit-
able threshold. As functions that check for com-
patibility return negative values while positive
distances indicate incompatibility, a threshold of
0 would separate compatible and incompatible
</bodyText>
<page confidence="0.998837">
44
</page>
<table confidence="0.999853666666667">
Recall Precision F-Measure
Gold Standard Entities 78 88.5 82.9
Baseline (Heuristic-based Entities) 80.9 44.1 57.1
Baseline (Noun Phrase Match Only) 50.9 77.2 61.3
Heuristic-Based Entity Recognition 62.9 77.1 69.3
Parsing-Based Entity Recognition 42.5 62.9 50.7
</table>
<tableCaption confidence="0.998886">
Table 3: Coreference Resolution Performance
</tableCaption>
<bodyText confidence="0.999584857142857">
elements. However, since the feature extraction
will not be totally accurate, (especially for the
GENDER and NUMBER features which test for
incompatibility) we chose to be more lenient
with deciding whether two phrases should be
clustered together, and used a threshold of r = 1
to allow for possible errors.
</bodyText>
<sectionHeader confidence="0.998683" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99932625">
Evaluation of coreference resolution systems has
traditionally been performed with precision and
recall. The MUC competition defines recall as
follows (Vilain et al., 1995):
</bodyText>
<equation confidence="0.989145333333333">
( |Ci  |-  |p(C) |)
I ( |Ci|-1)
Each C; is a gold standard cluster (i.e. a set of
</equation>
<bodyText confidence="0.999565333333333">
phrases which we know refer to the same entity),
and p(C;) is the partitioning of C; by the auto-
matically-generated clusters. For precision, the
role of the automatic and gold standard clusters
are reversed. Our results were evaluated using
the MUC scoring program which reports recall,
precision and F-measure, where the F-measure is
defined as the harmonic mean of precision and
recall:
</bodyText>
<equation confidence="0.995833333333333">
2PR
F =
P + R
</equation>
<bodyText confidence="0.993841777777778">
Table 3 presents the results of our coreference
resolution system on the outputs of both the pars-
ing-based and heuristic-based entity detection
systems, as measured by the MUC-7 scoring
!
program. For the purposes of comparison, we
also present results of our clustering algorithm
on the gold standard entities. This gives us a
sense of the upper bound that we could poten-
tially achieve if we got 100% accuracy on our
mention detection phase. An additional baseline
is generated by implementing a system that as-
sumes that all phrases refer to the same entity –
i.e. it takes all the heuristically-generated phrases
and puts them into one big cluster. This gives us
an upper bound on the recall of the system. Yet
another baseline, to see how easy the task is, is to
merge mentions together if the “Noun Phrase
Match” function tests true.
From the results, it can be seen that our system
achieves a performance gain of over 10 F-
Measure points over the simplest baseline, and
over 8 F-Measure points over the more sophisti-
cated baseline. Unfortunately, due to corpus dif-
ferences, we cannot conduct a comparison with
results found in previous work.
An interesting observation is the fact that the
heuristic-based entity recognizer achieves better
performance than the one based on statistical
parsing. The parser is trained on the Chinese
Penn Treebank, which tends to have relatively
longer noun phrases, and as result, the phrases
generated by the parser also tend to be on the
long side. This causes errors at the entity recog-
nition phase, which results in a performance hit
for the overall system.
</bodyText>
<sectionHeader confidence="0.96836" genericHeader="method">
6 Analysis
</sectionHeader>
<bodyText confidence="0.999970222222222">
One interesting question to ask about the results
is the contribution of any given individual fea-
ture to the result of the overall system. We have
already investigated the effect of entity recogni-
tion, and in this section, we take a look at the
features for the clustering algorithm. Error!
Reference source not found. presents the results
of a series of experiments in which one feature at
a time was removed from the clustering algo-
rithm. The last entry in the table shows the re-
sults of the full system; the drop in performance
when a feature is removed is indicative of its
contribution. Judging from the results, the 3 fea-
tures that contribute the most to performance are
the NOUN PHRASE MATCH, SEMANTIC
AGREEMENT and EDIT DISTANCE features.
Two out of the three, NOUN PHRASE and
EDIT DISTANCE, operate on lexical informa-
tion. The importance of string matching to
coreference resolution is consistent with findings
in previous work (Yang et al. 2004), which ar-
rived at the same conclusion for English.
In addition, we note that the two Chinese-
specific features that were introduced, ABBRE-
VIATION and EDIT DISTANCE, both contrib-
ute significantly (as measured by a student’s t-
test) to the performance of the final system.
</bodyText>
<equation confidence="0.964825">
R = I
</equation>
<page confidence="0.996149">
45
</page>
<table confidence="0.999967928571428">
Removed feature Recall Precision F-measure
Noun Phrase Match 59.8 75.9 66.9
Head Noun Match 60.4 76.2 67.4
Sentence Distance 63.2 73.3 67.8
Gender Agreement 62.9 76.3 68.9
Number Agreement 63.2 75.9 69
Semantic Agreement 60.5 73 66.2
Proper Name Agreement 63 76.2 69
Pronoun Agreement 61.3 76.9 68.2
Demonstrative Noun Phrase 62.2 77.9 69.2
Appositive 60.1 76.9 67.5
Abbreviation 61.6 77 68.4
Edit Distance 62.4 72.8 67.2
None (All Features) 62.9 77.1 69.3
</table>
<tableCaption confidence="0.999881">
Table 4: Contribution of individual features to overall performance.
</tableCaption>
<bodyText confidence="0.999927066666667">
Of our features, those that contribute the least
to the overall performance are the GENDER,
NUMBER and DEMONSTRATIVE NOUN
PHRASE features. For DEMONSTRATIVE
NOUN PHRASE, the reason is because of data
sparsity – there are just simply not enough ex-
amples that it would make any significant im-
pact. For the GENDER and NUMBER features,
we find that the problem is mostly with errors in
feature generation.
To our knowledge, this is the first published
result on unsupervised Chinese coreference reso-
lution. Due to differences in data, it is not possi-
ble to conduct a comparison of our work with
previous results.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999984217391304">
Coreference resolution has attracted much atten-
tion in recent years, especially as a result of the
MUC and ACE competitions. The approaches
taken have exhibited a shift from knowledge-
based approaches to learning-based approaches.
Many of the learning-based approaches recast
coreference resolution as a binary classification
task, which, given a pair of NPs, uses a trained
classifier to determine whether they are corefer-
ent. Soon et al. (2001) used this approach with a
12-feature decision tree-based classifier and Ng
and Cardie (2002) extended this approach with
extra machine learning frameworks and a larger
set of features. Yang et al. (2004) extended this
approach into an NP-cluster based approach,
which considers the relationships between
phrases and coreferential clusters.
In addition, several unsupervised approaches
have been proposed. Cardie and Wagstaff (1999)
re-cast the problem as a clustering task which
applied a set of incompatibility functions and
weights in the distance metric. Bean and Riloff
(2004) used information extraction patterns to
identify contextual clues that would determine
the compatibility between NPs.
All of the previously mentioned work has been
for English. There has been relatively little work
in Chinese: Florian et al. (2004) provides results
using a language-independent framework on the
Entity Detection and Tracking task (EDT). They
formulate the detection subtask as a classification
problem using a Robust Risk Minimization clas-
sifier combined with a Maximum Entropy classi-
fier. Their system performs significantly well on
English, Chinese and Arabic, however, the sys-
tem suffers from small amount of training data
(90K characters for Chinese, in contrast with
340K words for English). Their system obtained
an ACE value of 58.8 on the ACE evaluation
data on Chinese. Finally, Zhou et al. (2005) pro-
posed a unified Transformation-Based Learning
framework on Chinese EDT. The TBL tracking
model looks at pairs of NPs at a time and classi-
fies them as being coreferent or not based on the
values of six features. They report an ACE score
of 63.3 on their dataset.
</bodyText>
<sectionHeader confidence="0.995376" genericHeader="conclusions">
8 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999956545454546">
In this paper, we have presented an unsupervised
approach to Chinese coreference resolution. Our
approach performs resolution by clustering, with
the advantage that no annotated training data is
needed. We evaluated our approach using a cor-
pus which we developed using standard annota-
tion schemes, and find that our system achieves
an error reduction rate of almost 30% over the
baseline. We also analyze the performance of our
system by investigating the contribution of indi-
vidual features to our system. The analysis illus-
</bodyText>
<page confidence="0.997758">
46
</page>
<bodyText confidence="0.9915263">
trates the contribution of the new language-
specific features.
While the results produced by our system are
impressive, it should be noted that all our fea-
tures consider only the mention phrase itself. We
consider this to be a rather simplistic and incom-
plete. In future work, we plan to investigate the
use of more sophisticated features, including
contextual features, to improve the performance
of our system.
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849344444445">
ANGHELUTA R., JEUNIAUX P., RUDRADEB M.,
MOENS M.F. 2004. Clustering Algorithms for
Noun Phrase Coreference Resolution. Proceedings
of the 7th International Conference on the Statisti-
cal Analysis of Textual Data.
BEAN, D. and RILOFF, E. 2004. Unsupervised learn-
ing of contextual role knowledge for coreference
resolution. In Proc. of HLT/NAACL, pages 297–
304.
BIKEL, D. M. 2004. A Distributional Analysis of a
Lexicalized Statistical Parsing Model. In Proceed-
ings of EMNLP, Barcelona
CARDIE, C. and WAGSTAFF, K. 1999. Noun phrase
coreference as clustering. In Proceedings of the
1999 Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 82-89.
FLORIAN, R., HASSAN, H., ITTYCHERIAH, A.,
JING, H., KAMBHATLA, N., LUO, X.,
NICOLOV, N., and ROUKOS, S. 2004. Statistical
Model for Multilingual Entity Detection and
Tracking. In Proceedings of 2004 annual meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2004).
FUNG, P., NGAI, G., YANG, Y. S., and CHEN, B.F.
2004. A maximum-entropy Chinese parser aug-
mented by Transformation-Based Learning. ACM
Transactions on Asian Language Information
Processing (TALIP), 3(2), pp 159-168.
GAO J.F., LI M. and HUANG C.N. 2003. Improved
souce-channel model for Chinese wordsegmenta-
tion. In Proc. of ACL2003.
HARABAGIU, S., BUNESCU, R.,and MAIORANO,
S. 2001. Text and Knowledge Mining for Corefer-
ence Resolution, in Proceedings of the 2nd Meet-
ing of the North American Chapter of the Associa-
tion of Computational Linguistics (NAACL-2001).
HIRSCHMAN, L. and CHINCHOR, N. 1997. MUC7
Coreference Task Definition,
http://www.itl.nist.gov/iaui/894.02/related_projects
/muc/proceedings/co_task.html.
LANCASHIRE, D. 2005. Adsotrans Chinese-English
annotation. http://www.adsotrans.com/.
LDC. 2004. Chinese Annotation Guidelines for Entity
Detection and Tracking.
http://www.ldc.upenn.edu/Projects/ACE/Data.
MUC-7. 1998. Proceedings of the Seventh Message
Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, CA.
NG V. 2005. Machine learning for coreference
resolution: From local classification to global
ranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), 2005.
NG, V. and CARDIE, C. 2002. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages
104-111.
NG V. and CARDIE C. 2003. Bootstrapping Corefer-
ence Classifiers with Multiple Machine Learning
Algorithms. Proceedings of the 2003 Conference
on Empirical Methods in Natural Language Proc-
essing (EMNLP-2003), Association for Computa-
tional Linguistics, 2003.
SOON, W., NG, H., and LIM, D. 2001. A machine
learning approach to coreference resolution of
noun phrases. Computational Linguistics,
27(4):521-544.
VILAIN, M., BURGER, J., ABERDEEN, J., CON-
NOLLY, D., and HIRSCHMAN, L. 1995. A
model-theoretic coreference scoring scheme. In
Proceedings of the Sixth Message Understanding
Conference (MUC-6), pages 45–52, San Francisco,
CA. Morgan Kaufmann.
WILPON, J., AND RABINER, L. 1985. A modified
K-means clustering algorithm for use in isolated
word recognition. In IEEE Transactions on Acous-
tics, Speech, Signal Processing. ASSP-33(3), 587-
594.
YANG, X., ZHOU, G., SU, J., and TAN, C. L. 2004.
An NP-Cluster Based Approach to Coreference
Resolution. Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING2004).
ZHOU Y., HUANG C., GAO J., WU L. 2005. Trans-
formation Based Chinese Entity Detection and
Tracking. Proceedings of the Second International
Joint Conference on Natural Language Processing.
</reference>
<page confidence="0.999488">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.617505">
<title confidence="0.99785">A Clustering Approach Unsupervised Chinese Coreference Resolution</title>
<author confidence="0.994949">Chi-shing Wang Grace NGAI</author>
<affiliation confidence="0.8343125">Department of Computing Kong Polytechnic</affiliation>
<address confidence="0.912259">Kowloon, HONG KONG</address>
<email confidence="0.978427">cscswang@comp.polyu.edu.hk</email>
<email confidence="0.978427">csgngai@comp.polyu.edu.hk</email>
<abstract confidence="0.997830347826087">Coreference resolution is the process of identifying expressions that refer to the same entity. This paper presents a clustering algorithm for unsupervised Chinese coreference resolution. We investigate why Chinese coreference is hard and demonstrate that techniques used in coreference resolution for English can be extended to Chinese. The proposed system exploits clustering as it has advantages over traditional classification methods, such as the fact that no training data is required and it is easily extended to accommodate additional features. We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance. Our system is evaluated on an annotated version of the TDT3 corpus using the MUC-7 scorer, and obtains comparable performance. We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resolution.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R ANGHELUTA</author>
<author>P JEUNIAUX</author>
<author>M RUDRADEB</author>
<author>M F MOENS</author>
</authors>
<title>Clustering Algorithms for Noun Phrase Coreference Resolution.</title>
<date>2004</date>
<booktitle>Proceedings of the 7th International Conference on the Statistical Analysis of Textual Data.</booktitle>
<marker>ANGHELUTA, JEUNIAUX, RUDRADEB, MOENS, 2004</marker>
<rawString>ANGHELUTA R., JEUNIAUX P., RUDRADEB M., MOENS M.F. 2004. Clustering Algorithms for Noun Phrase Coreference Resolution. Proceedings of the 7th International Conference on the Statistical Analysis of Textual Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D BEAN</author>
<author>E RILOFF</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>297--304</pages>
<marker>BEAN, RILOFF, 2004</marker>
<rawString>BEAN, D. and RILOFF, E. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297– 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M BIKEL</author>
</authors>
<title>A Distributional Analysis of a Lexicalized Statistical Parsing Model.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Barcelona</location>
<marker>BIKEL, 2004</marker>
<rawString>BIKEL, D. M. 2004. A Distributional Analysis of a Lexicalized Statistical Parsing Model. In Proceedings of EMNLP, Barcelona</rawString>
</citation>
<citation valid="true">
<authors>
<author>C CARDIE</author>
<author>K WAGSTAFF</author>
</authors>
<title>Noun phrase coreference as clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>82--89</pages>
<marker>CARDIE, WAGSTAFF, 1999</marker>
<rawString>CARDIE, C. and WAGSTAFF, K. 1999. Noun phrase coreference as clustering. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 82-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R FLORIAN</author>
<author>H HASSAN</author>
<author>A ITTYCHERIAH</author>
<author>H JING</author>
<author>N KAMBHATLA</author>
<author>X LUO</author>
<author>N NICOLOV</author>
<author>S ROUKOS</author>
</authors>
<title>Statistical Model for Multilingual Entity Detection and Tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of 2004 annual meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<marker>FLORIAN, HASSAN, ITTYCHERIAH, JING, KAMBHATLA, LUO, NICOLOV, ROUKOS, 2004</marker>
<rawString>FLORIAN, R., HASSAN, H., ITTYCHERIAH, A., JING, H., KAMBHATLA, N., LUO, X., NICOLOV, N., and ROUKOS, S. 2004. Statistical Model for Multilingual Entity Detection and Tracking. In Proceedings of 2004 annual meeting of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P FUNG</author>
<author>G NGAI</author>
<author>Y S YANG</author>
<author>B F CHEN</author>
</authors>
<title>A maximum-entropy Chinese parser augmented by Transformation-Based Learning.</title>
<date>2004</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>3</volume>
<issue>2</issue>
<pages>159--168</pages>
<marker>FUNG, NGAI, YANG, CHEN, 2004</marker>
<rawString>FUNG, P., NGAI, G., YANG, Y. S., and CHEN, B.F. 2004. A maximum-entropy Chinese parser augmented by Transformation-Based Learning. ACM Transactions on Asian Language Information Processing (TALIP), 3(2), pp 159-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F GAO</author>
<author>M LI</author>
<author>C N HUANG</author>
</authors>
<title>Improved souce-channel model for Chinese wordsegmentation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL2003.</booktitle>
<marker>GAO, LI, HUANG, 2003</marker>
<rawString>GAO J.F., LI M. and HUANG C.N. 2003. Improved souce-channel model for Chinese wordsegmentation. In Proc. of ACL2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S HARABAGIU</author>
<author>R BUNESCU</author>
<author>S MAIORANO</author>
</authors>
<title>Text and Knowledge Mining for Coreference Resolution,</title>
<date>2001</date>
<booktitle>in Proceedings of the 2nd Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-2001).</booktitle>
<marker>HARABAGIU, BUNESCU, MAIORANO, 2001</marker>
<rawString>HARABAGIU, S., BUNESCU, R.,and MAIORANO, S. 2001. Text and Knowledge Mining for Coreference Resolution, in Proceedings of the 2nd Meeting of the North American Chapter of the Association of Computational Linguistics (NAACL-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L HIRSCHMAN</author>
<author>N CHINCHOR</author>
</authors>
<date>1997</date>
<note>MUC7 Coreference Task Definition, http://www.itl.nist.gov/iaui/894.02/related_projects /muc/proceedings/co_task.html.</note>
<marker>HIRSCHMAN, CHINCHOR, 1997</marker>
<rawString>HIRSCHMAN, L. and CHINCHOR, N. 1997. MUC7 Coreference Task Definition, http://www.itl.nist.gov/iaui/894.02/related_projects /muc/proceedings/co_task.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D LANCASHIRE</author>
</authors>
<title>Adsotrans Chinese-English annotation.</title>
<date>2005</date>
<note>http://www.adsotrans.com/.</note>
<marker>LANCASHIRE, 2005</marker>
<rawString>LANCASHIRE, D. 2005. Adsotrans Chinese-English annotation. http://www.adsotrans.com/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Chinese Annotation Guidelines for Entity Detection and Tracking.</title>
<date>2004</date>
<note>http://www.ldc.upenn.edu/Projects/ACE/Data.</note>
<marker>LDC, 2004</marker>
<rawString>LDC. 2004. Chinese Annotation Guidelines for Entity Detection and Tracking. http://www.ldc.upenn.edu/Projects/ACE/Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V NG</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<marker>NG, 2005</marker>
<rawString>NG V. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V NG</author>
<author>C CARDIE</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<marker>NG, CARDIE, 2002</marker>
<rawString>NG, V. and CARDIE, C. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40rd Annual Meeting of the Association for Computational Linguistics, Pages 104-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V NG</author>
<author>C CARDIE</author>
</authors>
<title>Bootstrapping Coreference Classifiers with Multiple Machine Learning Algorithms.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003), Association for Computational Linguistics,</booktitle>
<marker>NG, CARDIE, 2003</marker>
<rawString>NG V. and CARDIE C. 2003. Bootstrapping Coreference Classifiers with Multiple Machine Learning Algorithms. Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003), Association for Computational Linguistics, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W SOON</author>
<author>H NG</author>
<author>D LIM</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--4</pages>
<marker>SOON, NG, LIM, 2001</marker>
<rawString>SOON, W., NG, H., and LIM, D. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M VILAIN</author>
<author>J BURGER</author>
<author>J ABERDEEN</author>
<author>D CONNOLLY</author>
<author>L HIRSCHMAN</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco, CA.</location>
<marker>VILAIN, BURGER, ABERDEEN, CONNOLLY, HIRSCHMAN, 1995</marker>
<rawString>VILAIN, M., BURGER, J., ABERDEEN, J., CONNOLLY, D., and HIRSCHMAN, L. 1995. A model-theoretic coreference scoring scheme. In Proceedings of the Sixth Message Understanding Conference (MUC-6), pages 45–52, San Francisco, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J WILPON</author>
<author>L RABINER</author>
</authors>
<title>A modified K-means clustering algorithm for use in isolated word recognition.</title>
<date>1985</date>
<booktitle>In IEEE Transactions on Acoustics, Speech, Signal Processing.</booktitle>
<volume>33</volume>
<issue>3</issue>
<pages>587--594</pages>
<marker>WILPON, RABINER, 1985</marker>
<rawString>WILPON, J., AND RABINER, L. 1985. A modified K-means clustering algorithm for use in isolated word recognition. In IEEE Transactions on Acoustics, Speech, Signal Processing. ASSP-33(3), 587-594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X YANG</author>
<author>G ZHOU</author>
<author>J SU</author>
<author>C L TAN</author>
</authors>
<title>An NP-Cluster Based Approach to Coreference Resolution.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th International Conference on Computational Linguistics (COLING2004).</booktitle>
<marker>YANG, ZHOU, SU, TAN, 2004</marker>
<rawString>YANG, X., ZHOU, G., SU, J., and TAN, C. L. 2004. An NP-Cluster Based Approach to Coreference Resolution. Proceedings of the 20th International Conference on Computational Linguistics (COLING2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y ZHOU</author>
<author>C HUANG</author>
<author>J GAO</author>
<author>L WU</author>
</authors>
<title>Transformation Based Chinese Entity Detection and Tracking.</title>
<date>2005</date>
<booktitle>Proceedings of the Second International Joint Conference on Natural Language Processing.</booktitle>
<marker>ZHOU, HUANG, GAO, WU, 2005</marker>
<rawString>ZHOU Y., HUANG C., GAO J., WU L. 2005. Transformation Based Chinese Entity Detection and Tracking. Proceedings of the Second International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>