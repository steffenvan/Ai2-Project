<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004476">
<title confidence="0.9987235">
Learning Lexical Alignment Policies for Generating
Referring Expressions in Spoken Dialogue Systems
</title>
<author confidence="0.996602">
Srinivasan Janarthanam
</author>
<affiliation confidence="0.998504">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.797127">
Edinburgh EH8 9AB
</address>
<email confidence="0.996865">
s.janarthanam@ed.ac.uk
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807363636364">
We address the problem that different
users have different lexical knowledge
about problem domains, so that automated
dialogue systems need to adapt their gen-
eration choices online to the users’ domain
knowledge as it encounters them. We ap-
proach this problem using policy learning
in Markov Decision Processes (MDP). In
contrast to related work we propose a new
statistical user model which incorporates
the lexical knowledge of different users.
We evaluate this user model by showing
that it allows us to learn dialogue poli-
cies that automatically adapt their choice
of referring expressions online to differ-
ent users, and that these policies are sig-
nificantly better than adaptive hand-coded
policies for this problem. The learned
policies are consistently between 2 and
8 turns shorter than a range of different
hand-coded but adaptive baseline lexical
alignment policies.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996816875">
In current “troubleshooting” spoken dialogue sys-
tems (SDS)(Williams, 2007), the major part of the
conversation is directed by the system, while the
user follows the system’s instructions. Once the
system decides what instruction to give the user
(at the dialogue management level), it faces sev-
eral decisions to be made at the natural language
generation (NLG) level. These include, deciding
which concepts to include in the utterance, decid-
ing the referring expressions (RE) to use in the ut-
terance and so on. A little-studied problem is to
what extent a system could automatically align to
the user’s lexical knowledge by adapting its RE
choices, in particular based on his domain exper-
tise, and how this can be modelled and optimised
computationally.
</bodyText>
<author confidence="0.72161">
Oliver Lemon
</author>
<affiliation confidence="0.992542">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.592397">
Edinburgh EH8 9AB
</address>
<email confidence="0.947162">
olemon@inf.ed.ac.uk
</email>
<bodyText confidence="0.999561448275862">
(Issacs and Clark, 1987) show how two in-
terlocutors adapt their language in a conversa-
tion by assessing each other’s domain expertise
during dialogue, by observing how they react to
each other’s RE choices. This is called align-
ment through Audience Design (Clark, 1996; Bell,
1984). Using inappropriate REs in instructions has
been identified as a serious problem affecting a
system’s usability (Molich and Nielsen, 1990).
In this paper, we treat NLG within a com-
putational learning paradigm (Lemon, 2008;
Rieser and Lemon, 2009; Janarthanam and
Lemon, 2008). We examine whether a SDS can
automatically learn a lexical alignment policy
for audience design, which enables it to choose
appropriate REs by predicting the user’s lexical
knowledge dynamically during the course of the
dialogue. This can avoid clarification requests
from the users and keep the dialogues short. The
example given below describes the kind of lexical
alignment behaviour that we want the system to
learn. The system chooses “small white box”
instead of “ADSL filter” and “monitor symbol”
instead of “network icon”, because it learnt that
the user is a novice based on their clarification
requests. However, it switches to using technical
terms like “browser”, when it learns that the
user is not a complete novice (as he verifies the
description for the network icon in Usr 4).
</bodyText>
<footnote confidence="0.425561846153846">
Example 1.
Sys 1&gt;&gt; Is your router connected
to the computer?
Usr 1&gt;&gt; Uh. What’s a router?
Sys 2&gt;&gt; It’s the big black box.
Usr 2&gt;&gt; Ok.. yes.
Sys 3&gt;&gt; Do you see a small white
box connected to the router?
Usr 3&gt;&gt; Yes.
Sys 4&gt;&gt; Ok. Is there a flashing
monitor symbol at the bottom
Proceedings of the 12th European Workshop on Natural Language Generation, pages 74–81,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</footnote>
<page confidence="0.99332">
74
</page>
<bodyText confidence="0.966014416666667">
right of the screen?
Usr 4&gt;&gt; the network icon?
Sys 5&gt;&gt; Yes. Is it flashing?
Usr 5&gt;&gt; Yes. It is flashing.
Sys 6&gt;&gt; Ok. Please open your
browser.
We study lexical alignment in the context of
troubleshooting dialogues – where users wish to
mend their broken internet connection. This task
demands a new type of User Simulation.
In this paper, after a review of related work
(section 2), we first present (section 3) a statisti-
cal User Simulation which supports different do-
main knowledge profiles and reacts accordingly to
questions or instructions from an SDS. We then
present a reinforcement learning model of lexical
alignment due to audience design (in sections 4 &amp;
5). We then evaluate the User Simulation (section
6), testing whether a simulation that is sensitive to
a system’s RE choices can be used to learn good
lexical alignment policies. Finally, we compare
policies learned in interaction with the User Sim-
ulation with hand-coded policies, and present the
results in section 7.
</bodyText>
<sectionHeader confidence="0.999856" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999488192307692">
Several statistical user simulation models that
model a user’s behaviour in a conversation have
been proposed (Georgila et al., 2005; Schatzmann
et al., 2006; Schatzmann et al., 2007). These mod-
els issue task specific dialogue acts like inform-
ing their search constraints, confirming values, re-
jecting misrecognised values, etc. However, they
do not model a user population with varying do-
main expertise. Also, none of these models seek
clarification at conceptual or lexical levels that oc-
cur naturally in conversations between real users.
(Komatani et al., 2003) proposed using user mod-
els with features like skills, domain knowledge
and hastiness as a part of the dialogue manager
to produce adaptive responses. (Janarthanam and
Lemon, 2008) presented a user simulation model
that simulates a variety of users with different do-
main knowledge profiles. Although this model
incorporated clarification acts at the conceptual
level, these users ignore the issues concerning the
user’s understanding of the REs used by the sys-
tem. In this work, in contrast to the above, we
present a User Simulation model which explicitly
encodes the user’s lexical knowledge of the do-
main, understands descriptive expressions, and is-
sues clarification requests at the lexical level.
</bodyText>
<sectionHeader confidence="0.988529" genericHeader="method">
3 User Simulation
</sectionHeader>
<bodyText confidence="0.999114210526316">
Our User Simulation module simulates dialogue
behaviour of different users, and interacts with the
dialogue system by exchanging both dialogue acts
and REs. It produces users with different knowl-
edge profiles. The user population produced by
the simulation comprises a spectrum from com-
plete novices to experts in the domain. Simulated
users behave differently from one another because
of differences in their knowledge profiles. Simu-
lated users are also able to learn new REs during
interaction with the SDS. These new expressions
are held in the user simulation’s short term mem-
ory for later use in the conversation. Simulated
users interact with the environment using an in-
teractive mechanism that allows them to observe
and manipulate the states of various domain ob-
jects. The interaction between the user and the
other components is given in figure 1 (notations
explained in later sections).
</bodyText>
<figureCaption confidence="0.99693">
Figure 1: Experimental setup
</figureCaption>
<subsectionHeader confidence="0.997848">
3.1 Domain knowledge model
</subsectionHeader>
<bodyText confidence="0.999951818181818">
Domain experts know most of the technical terms
that are used to refer to domain objects whereas
novice users can only reliably identify them when
descriptive expressions are used. While in the
model of (Janarthanam and Lemon, 2008) knowl-
edge profiles were presented only at conceptual
levels (e.g. does the user know what a modem is?),
we present them in a more granular fashion. In
this model, the user’s domain knowledge profile
is factored into lexical (LKu,t), factual (FKu,t)
and procedural knowledge (PKu,t) components.
</bodyText>
<page confidence="0.9956">
75
</page>
<table confidence="0.9994386">
Lexical knowledge LK,,,t
vocab([modem, router], dobj1)
vocab([wireless, WiFi], dobj3)
vocab([modem power light], dobj7)
Factual knowledge FK,,,t
location(dobj1)
location(dobj7)
Procedural knowledge PK,,,t
procedure(replace filter)
procedure(refresh page)
</table>
<tableCaption confidence="0.999922">
Table 1: Knowledge profile - Intermediate user.
</tableCaption>
<bodyText confidence="0.997542304347826">
A user’s lexical knowledge is encoded in the for-
mat:
vocab(referring expressions, domain object)
where referring expressions can be a list of ex-
pressions that the user knows can be used to talk
about each domain object.
Whether the user knows facts like the location
of the domain objects (location(domain object)) is
encoded in the factual component. Similarly, the
procedural component encodes the user’s knowl-
edge of how to find or manipulate domain objects
(procedure(domain action)). Table 1 shows an ex-
ample user knowledge profile.
In order to create a knowledge spectrum, a
Bayesian knowledge model is used. The current
model incorporates patterns of only the lexical
knowledge among the users. For instance, peo-
ple who know the word “router” most likely also
know “DSL light” and “modem” and so on. These
dependencies between REs are encoded as condi-
tional probabilities in the Bayesian model. Figure
2 shows the dependencies between knowledge of
REs.
</bodyText>
<figureCaption confidence="0.973137">
Figure 2: Bayes Net for User Lexical Knowledge
</figureCaption>
<bodyText confidence="0.999982409090909">
Using this Bayesian model, we instantiate dif-
ferent knowledge profiles for different users. The
current conditional probabilities were set by hand
based on intuition. In future work, these values
will be populated based on simple knowledge sur-
veys performed on real users (Janarthanam and
Lemon, 2009). This method creates a spectrum of
users from ones who have no knowledge of tech-
nical terms to ones who know all the technical
jargon, though every profile will have a different
frequency of occurrence. This difference in fre-
quency reflects that expert users are less common
than novice users.
The user’s domain knowledge can be dynami-
cally updated. The new REs, both technical and
descriptive, presented by the system through clar-
ification moves are stored in the user’s short term
memory. Exactly how long (in terms of dialogue
turns) to retain the newly acquired knowledge is
given by a retention index RI, At the end of RI,,
turns, the lexical item is removed from user’s short
term memory.
</bodyText>
<subsectionHeader confidence="0.999467">
3.2 User Dialogue Action set
</subsectionHeader>
<bodyText confidence="0.999980739130435">
Apart from environment-directed acts, simulated
users issue a number of dialogue acts. The list of
dialogue actions that the user can perform in this
model is given in Table 2. It consists of default
moves like provide info and acknowledge as well
as some clarification moves. Request description
is issued when the SDS uses technical terms that
the simulated user does not know, e.g. “What is
a router?”. Request verification is issued when
the SDS uses descriptive lexical items for do-
main objects that the user knows more techni-
cal terms for, e.g. System: “Is the black box
plugged in?” User: “Do you mean the router?”.
Request disambiguation is issued when the user
faces an underspecified and ambiguous descrip-
tive expression, e.g.“User: I have two black boxes
here - one with lights and one without. Which
one is it?”. These clarification strategies have
been modeled based on (Schlangen, 2004). The
user simulation also issues request location and
request procedure dialogue acts, when it does not
know the location of domain objects or how to ma-
nipulate them, respectively.
</bodyText>
<subsectionHeader confidence="0.998419">
3.3 Environment simulation
</subsectionHeader>
<bodyText confidence="0.9999758">
The environment simulation includes both physi-
cal objects, such as the computer, modem, ADSL
filter, etc and virtual objects, such as the browser,
control panel, etc in the user’s environment. Phys-
ical and virtual connections between these objects
</bodyText>
<page confidence="0.946682">
76
</page>
<table confidence="0.985909">
report problem
provide info(dobj, info)
acknowledge
request verification(x, y)
request description(x)
request disambiguation(x, [y1,y2])
request location(dobj)
request procedure(daction)
thank system
</table>
<tableCaption confidence="0.996262">
Table 2: User Dialogue Acts.
</tableCaption>
<bodyText confidence="0.999733142857143">
are also simulated. At the start of every dialogue,
the environment is initiated to a faulty condition.
Following a system instruction or question, the
user issues two kinds of environment acts. It is-
sues an observation act Ou,t to observe the status
of a domain object and a manipulation act Mu,t
to change the state of the environment (Se,t). The
simulation also includes task irrelevant objects in
order to confuse the users with underspecified de-
scriptive expressions. For instance, we simulate
two domain objects that are black in colour - an
external hard disk and a router. So, the users may
get confused when the system uses the expression,
“black box”.
</bodyText>
<subsectionHeader confidence="0.966869">
3.4 User Action Selection
</subsectionHeader>
<bodyText confidence="0.999466870967742">
User Action selection has several steps. The user’s
dialogue behaviour is described in the action se-
lection algorithm (Table 3). Firstly, the user must
identify all the RE choices (RECs,t) that are used
to refer to different domain objects (dobj) and
domain actions (daction) in the system instruc-
tion (step 1). Secondly, the user’s knowledge of
the prerequisite factual (FKprereq) and procedural
(PKprereq) knowledge components connected to
the observation or manipulation action is checked.
If the user does not satisfy the knowledge re-
quirements, the user simulation issues an appro-
priate clarification request (steps 2 &amp; 3). After
the knowledge requirements are satisfied, the user
issues environment directed actions and responds
to system instruction As,t (steps 4 &amp; 5). When
the system provides the user specific information,
they are added to the user’s short term memory
(steps 6-8). Although, the action selection process
is deterministic at this level, it is dependent on
the users’ diverse knowledge profiles, which en-
sures stochastic dialogue behaviour amongst dif-
ferent users created by the module.
greet the user
request status(x)
request action(x)
give description(x)
accept verification(x,y)
give location(dobj)
give procedure(daction)
close dialogue
</bodyText>
<tableCaption confidence="0.88838">
Table 4: System Dialogue acts.
</tableCaption>
<sectionHeader confidence="0.997902" genericHeader="method">
4 Dialogue System Model
</sectionHeader>
<bodyText confidence="0.999325666666666">
The dialogue system is modeled as a reinforce-
ment learning agent in a Markov Decision Pro-
cess framework (Levin et al., 1997). At every
turn, it interacts with the Simulated User by issu-
ing a System Dialogue Act (As,t) along with a set
of REs, called the System RE Choices (RECs,t).
RECs,t contains the REs that refer to various do-
main objects in the dialogue act As,t. First, the
system decides the dialogue act to issue using a
hand-coded dialogue strategy. Troubleshooting in-
structions are coded in the troubleshooting deci-
sion tree1. Dialogue repair moves include select-
ing clarification moves in response to user’s re-
quest. The list of system dialogue acts is given
Table 4.
The system issues various repair moves when
the users are unable to carry out the system’s in-
structions due to ignorance, non-understanding or
the ambiguous nature of the instructions. The
give description act is used to give the user a de-
scription of the domain object previously referred
to using a technical term. It is also used when
the user requests disambiguation. Similarly, ac-
cept verification is given when the user wants to
verify whether the system is referring to a certain
domain object y using the expression x.
After selecting the dialogue act As,t, a set
of REs must be chosen to refer to each of
the domain objects/actions used in the dia-
logue act. For instance, the dialogue act re-
quest status(router dsl light) requires references
to be made to domain objects “router” and “DSL
light”. For each of these references, the system
chooses a RE, creating the System RE Choice
RECs,t. In this study, we have 7 domain objects
and they can either be referred to using technical
</bodyText>
<footnote confidence="0.999245333333333">
1The Troubleshooting decision tree was hand-built using
guidelines from www.orange.co.uk and is similar to the one
used by their Customer Support personnel
</footnote>
<page confidence="0.998183">
77
</page>
<figureCaption confidence="0.996882869565218">
Input: System Dialogue Act A,,t, System Referring Expressions Choice REC,,t
and User State S.,t: LK.,t, FK.,t, PK.,t
Step 1. ∀ x ∈ REC,,t
Step 1a. if (vocab(x, dobj)∈ LK.,t) then next x.
Step 1b. else if (description(x, dobj) &amp; ∃ j ((is jargon(j) &amp; vocab(j, dobj) ∈� LK.,t))) then next x.
Step 1c. else if (is jargon(x) &amp; (vocab(x, dobj) ∈� LK.,t)) then return request description(x).
Step 1d. else if (is ambiguous(x)) then return request disambiguation(x).
Step 1e. else if (description(x, dobj) &amp; ∃ j ((is jargon(j) &amp; vocab(j, dobj) ∈ LK.,t)))
then return request verification(x, j).
Step 2. if (∃dobj location(dobj) ∈ FKp....q &amp; location(dobj) ∈� FK.,t)
then return request location(dobj).
Step 3. else if (∃daction procedure(daction) ∈ PKp,.T..q &amp; procedure(daction) ∈� PK.,t)
then return request procedure(daction).
Step 4. else if (A,,t = request status(dobj))
then observe env(dobj, status), return provide info(dobj, status)
Step 5. else if (A,,t = request action(daction))
then manipulate env(daction), return acknowledge.
Step 6. else if (A,,t = give description(j, d) &amp; description(d, dobj))
then add to short term memory(vocab(j, dobj)), return acknowledge.
Step 7. else if (A,,t = give location(dobj))
then add to short term memory(location(dobj)), return acknowledge.
Step 8. else if (A,,t = give procedure(daction))
then add to short term memory(procedure(daction)), return acknowledge.
</figureCaption>
<tableCaption confidence="0.993852">
Table 3: Algorithm: Simulated User Action Selection
</tableCaption>
<bodyText confidence="0.9999804">
terms or descriptive expressions. For instance, the
DSL light on the router can be descriptively re-
ferred to as the “second light on the panel” or us-
ing the technical term, “DSL light”. Sometimes
the system has to choose between a lesser known
technical term and a well-known one. Some de-
scriptive expressions may be underspecified and
therefore can be ambiguous to the user (for ex-
ample, “the black box”). Choosing inappropri-
ate expressions can make the conversation longer
with lots of clarification and repair episodes. This
can lead to long frustrating dialogues, affecting the
task success rate. Therefore, the dialogue system
must learn to use appropriate REs in its utterances.
The RE choices available to the system are given
in Table 5.
The system’s RE choices are based on a part
of the dialogue state that records which of the
technical terms the user knows. These variables
are initially set to unknown (u). During the di-
alogue, they are updated to user knows (y) or
user doesnot know (n) states. We therefore record
the user’s lexical knowledge during the course of
the dialogue and let the system learn the statistical
usage patterns by itself. Part of the dialogue state
</bodyText>
<listItem confidence="0.999876">
1. router / black box / black box with lights
2. power light / first light on the panel
3. DSL light / second light on the panel
4. online light / third light on the panel
5. network icon / flashing computer symbol
6. network connections / earth with plug
7. WiFi / wireless
</listItem>
<tableCaption confidence="0.981373">
Table 5: System RE choices.
</tableCaption>
<bodyText confidence="0.99884">
relevant to system’s RE choices is given in Table 6.
The state can be extended to include other rele-
vant information like the usage of various REs by
the user as well to enable alignment with the user
through priming (Pickering and Garrod, 2004) and
personal experience (Clark, 1996). However they
are not yet implemented in the present work.
</bodyText>
<sectionHeader confidence="0.996668" genericHeader="method">
5 Reward function
</sectionHeader>
<bodyText confidence="0.9998544">
The reward function calculates the reward
awarded to the reinforcement learning agent at
the end of each dialogue session. Successful
task completion is rewarded with 1000 points.
Dialogues running beyond 50 turns are deemed
</bodyText>
<page confidence="0.996219">
78
</page>
<table confidence="0.999697">
Feature Values
user knows router y/n/u
user knows power light y/n/u
user knows dsl light y/n/u
user knows online light y/n/u
user knows network icon y/n/u
user knows network connections y/n/u
user knows wifi y/n/u
</table>
<tableCaption confidence="0.942507">
Table 6: (Part of) Dialogue state for Lexical Align-
ment.
</tableCaption>
<bodyText confidence="0.9946034">
unsuccessful and are awarded 0 points. The
number of turns in each dialogue varies according
to the system’s RE choices and the simulated
user’s response moves. Each turn costs 10 points.
The final reward is calculated as follows:
</bodyText>
<equation confidence="0.99985425">
TaskCompletionReward(TCR) = 1000
TurnCost(TC) = 10
TotalTurnCost(TTC) = #(Turns) * TC
FinalReward = TCR − TTC
</equation>
<bodyText confidence="0.999966">
The reward function therefore gives high re-
wards when the system produces shorter dia-
logues, which is possible by adaptively using ap-
propriate REs for each user.
</bodyText>
<sectionHeader confidence="0.989664" genericHeader="method">
6 Training
</sectionHeader>
<bodyText confidence="0.999769410958905">
The system was trained to produce an adaptive
lexical alignment policy, which can adapt to users
with different lexical knowledge profiles. Ideally,
the system must interact with a number of dif-
ferent users in order to learn to align with them.
However, with a large number of distinct Bayesian
user profiles (there are 90 possible user profiles),
the time taken for learning to converge is exorbi-
tantly high. Hence the system was trained with
selected profiles from the distribution. It was
initially trained using two user profiles from the
very extremes of the knowledge spectrum pro-
duced by the Bayesian model - complete experts
and complete novices. In this study, we cali-
brated all users to know all the factual and proce-
dural knowledge components, because the learn-
ing exercise was targeted only at the lexical level.
With respect to the lexical knowledge, complete
experts knew all the technical terms in the do-
main. Complete novices, on the other hand, knew
only one: power light. We set the RIu to 10,
so that the users do not forget newly learned lexi-
cal items for 10 subsequent turns. Ideally, we ex-
pected the system to learn to use technical terms
with experts and to use descriptive expressions
with novices and a mixture for intermediates. The
system was trained using SARSA reinforcement
learning algorithm (Sutton and Barto, 1998), with
linear function approximation, for 50000 cycles.
It produced around 1500 dialogues and produced
an alignment policy (RL1) that adapted to users
after the first turn which provides evidence about
the kind of user the system is dealing with.
The system learns to get high reward by pro-
ducing shorter dialogues. By learning to choose
REs by adapting to the lexical knowledge of the
user, it avoids unnecessary clarification and repair
episodes. It learns to choose descriptive expres-
sions for novice users and jargon for expert users.
It also learns to use technical terms when all users
know them (for instance, “power light”). Due to
the user’s high retention (10 turns), the system
learned to use newly learned items later in the di-
alogue.
We also trained another alignment policy (RL2)
with two other intermediate high frequency user
lexical profiles. These profiles (Int1 and Int2)
were chosen from either ends of the knowledge
spectrum close to the extremes. Int1 is a knowl-
edge profile that is close to the novice end. It
only knows two technical terms: “power light”
and “WiFi”. On the other hand, Int2 is profile
that is close to the expert end and knows all tech-
nical terms except: “dsl light” and “online light”
(which are the least well-known technical terms
in the user population). With respect to the other
knowledge components - factual and procedural,
both users know every component equally. We
trained the system for 50000 cycles following the
same procedure as above. This produced an align-
ment policy (RL2) that learned to optimize the
moves, similar to RL1, but with respect to the
given distinct intermediate users.
Figure 3 shows the overall dialogue reward for
the 2 policies during training.
Both policies RL1 and RL2, apart from learn-
ing to adapt to the users, also learned not to use
ambiguous expressions. Ambiguous expressions
lead to confusion and the system has to spend ex-
tra turns for clarification. Therefore both policies
learnt to avoid using ambiguous expressions.
Figure 4 shows the dialogue length variation for
the 2 policies during training.
</bodyText>
<page confidence="0.997985">
79
</page>
<sectionHeader confidence="0.986146" genericHeader="evaluation">
7 Evaluation and baselines
</sectionHeader>
<bodyText confidence="0.999863818181818">
We evaluated both the learned policies using a test-
ing simulation and compared the results to other
baseline hand-coded policies. Unlike the train-
ing simulation, the testing simulation used the
Bayesian knowledge model to produce all differ-
ent kinds of user knowledge profiles. It produced
around 90 different profiles in varying distribution,
resembling a realistic user population. The tests
were run over 250 simulated dialogues each.
Several rule-based baseline policies were man-
ually created for the sake of comparison:
</bodyText>
<listItem confidence="0.991742">
1. Random - Choose REs at random.
2. Descriptive only - Only choose descriptive
expressions. If there is more than one de-
scriptive expression it picks one randomly.
3. Jargon only - Chooses the technical terms.
4. Adaptive 1 - It starts with a descriptive ex-
pression. If the user asks for verification, it
</listItem>
<figureCaption confidence="0.9998865">
Figure 3: Final reward for RL1 &amp; RL2.
Figure 4: Dialogue length for RL1 &amp; RL2.
</figureCaption>
<table confidence="0.997378">
Policy Avg. Reward Avg. Length
RL2 830.4 16.98
RL1 812.3 18.77
Adaptive 1 809.6 19.04
Adaptive 2 792.1 20.79
Adaptive 3 780.2 21.98
Random 749.8 25.02
Desc only 796.6 20.34
Jargon only 762.0 23.8
</table>
<tableCaption confidence="0.999388">
Table 7: Rewards and Dialogue Length.
</tableCaption>
<bodyText confidence="0.9764705">
switches to technical terms for the rest of the
dialogue.
</bodyText>
<listItem confidence="0.8208655">
5. Adaptive 2 - It starts with a technical term
and switches to descriptive expressions if the
user does not understand in the first turn.
6. Adaptive 3 - This rule-based policy adapts
</listItem>
<bodyText confidence="0.95328778125">
continuously based on the previous expres-
sion. For instance, if the user did not un-
derstand the technical reference to the current
object, it uses a descriptive expression for the
next object in the dialogue.
The first three policies (random, descriptive
only and jargon only) are equivalent to policies
learned using user simulations that are not sensi-
tive to system’s RE choices. In such cases, the
learned policies will not have a well-defined strat-
egy to choose REs based on user’s lexical knowl-
edge. Table 7 shows the comparative results for
the different policies. RL (1 &amp; 2) are significantly
better than all the hand-coded policies. Also, RL2
is significantly better than RL1 (p &lt; 0.05).
Ideally the system with complete knowledge of
the user would be able to finish the dialogue in
13 turns. Similarly, if it got it wrong every time
it would take 28 turns. From table 7 we see that
RL2 performs better than other policies, with an
average dialogue length of around 17 turns. The
learned policies were able to discover the hid-
den dependencies between lexical items that were
encoded in the Bayesian knowledge model. Al-
though trained only on two knowledge profiles, the
learned policies adapt well to unseen users, due to
the generalisation properties of the linear function
approximation method. Many unseen states arise
when interacting with users with new profiles and
both the learned policies generalise very well in
such situations, whereas the baseline policies do
not.
</bodyText>
<page confidence="0.996039">
80
</page>
<sectionHeader confidence="0.998699" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999953882352941">
In this paper, we have shown that by using a sta-
tistical User Simulation that is sensitive to RE
choices we are able to learn NLG policies that
adaptively decide which REs to use based on audi-
ence design. We have shown that the lexical align-
ment policies learned with this type of simulation
are better than a range of hand-coded policies.
Although lexical alignment policies could be
hand-coded, the designers would need to invest
significant resources every time the list of referring
expressions is revised or the conditions of the dia-
logue change. Using reinforcement learning, near-
optimal lexical alignment policies can be learned
quickly and automatically. This model can be used
in any task where interactions need to be tailored
to different users’ lexical knowledge of the do-
main.
</bodyText>
<sectionHeader confidence="0.751675" genericHeader="discussions">
8.1 Future work
</sectionHeader>
<bodyText confidence="0.9998636">
Lexical alignment in dialogue also happens due
to priming (Pickering and Garrod, 2004) and per-
sonal experience (Clark, 1996). We will examine
trade-offs in various conditions, like ‘instruct’ ver-
sus ‘teach’ and low versus high retention users.
Using Wizard-of-Oz studies and knowledge sur-
veys, we plan to make the model more data-driven
and realistic (Janarthanam and Lemon, 2009). We
will also evaluate the learned policies with real
users.
</bodyText>
<sectionHeader confidence="0.998406" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999915142857143">
The research leading to these results has re-
ceived funding from the European Community’s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
</bodyText>
<sectionHeader confidence="0.999603" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999727534482759">
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145–204.
H. H. Clark. 1996. Using Language. Cambridge Uni-
versity Press, Cambridge.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proceedings of Eu-
rospeech/Interspeech.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26–37.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. SEM-
dial’08.
S. Janarthanam and O. Lemon. 2009. A Wizard-of-Oz
environment to study Referring Expression Genera-
tion in a Situated Spoken Dialogue Task. In Proc.
ENLG’09.
K. Komatani, S. Ueno, T. Kawahara, and H. G. Okuno.
2003. Flexible Guidance Generation using User
Model in Spoken Dialogue Systems. In Proc.
ACL’03.
O. Lemon. 2008. Adaptive Natural Language Genera-
tion in Dialogue using Reinforcement Learning. In
Proc. SEMdial’08.
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proceedings ofASRU97.
R. Molich and J. Nielsen. 1990. Improving a Human-
Computer Dialogue. Communications of the ACM,
33-3:338–348.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169–225.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL’09.
J. Schatzmann, K. WeilHammer, M. N. Stuttle, and
S. J. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97–126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL 2007.
D. Schlangen. 2004. Causes and strategies for request-
ing clarification in dialogue. Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue (SIG-
DIAL 04), Boston.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technol-
ogy.
</reference>
<page confidence="0.999258">
81
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918578">
<title confidence="0.9999625">Learning Lexical Alignment Policies for Generating Referring Expressions in Spoken Dialogue Systems</title>
<author confidence="0.994958">Srinivasan</author>
<affiliation confidence="0.9993415">School of University of</affiliation>
<address confidence="0.931013">Edinburgh EH8</address>
<email confidence="0.998389">s.janarthanam@ed.ac.uk</email>
<abstract confidence="0.999697434782608">We address the problem that different users have different lexical knowledge about problem domains, so that automated dialogue systems need to adapt their generation choices online to the users’ domain knowledge as it encounters them. We approach this problem using policy learning in Markov Decision Processes (MDP). In contrast to related work we propose a new statistical user model which incorporates the lexical knowledge of different users. We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are significantly better than adaptive hand-coded policies for this problem. The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bell</author>
</authors>
<title>Language style as audience design.</title>
<date>1984</date>
<journal>Language in Society,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="2243" citStr="Bell, 1984" startWordPosition="340" endWordPosition="341">tudied problem is to what extent a system could automatically align to the user’s lexical knowledge by adapting its RE choices, in particular based on his domain expertise, and how this can be modelled and optimised computationally. Oliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The exam</context>
</contexts>
<marker>Bell, 1984</marker>
<rawString>A. Bell. 1984. Language style as audience design. Language in Society, 13(2):145–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="2230" citStr="Clark, 1996" startWordPosition="338" endWordPosition="339">n. A little-studied problem is to what extent a system could automatically align to the user’s lexical knowledge by adapting its RE choices, in particular based on his domain expertise, and how this can be modelled and optimised computationally. Oliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues sh</context>
<context position="18527" citStr="Clark, 1996" startWordPosition="2951" endWordPosition="2952">art of the dialogue state 1. router / black box / black box with lights 2. power light / first light on the panel 3. DSL light / second light on the panel 4. online light / third light on the panel 5. network icon / flashing computer symbol 6. network connections / earth with plug 7. WiFi / wireless Table 5: System RE choices. relevant to system’s RE choices is given in Table 6. The state can be extended to include other relevant information like the usage of various REs by the user as well to enable alignment with the user through priming (Pickering and Garrod, 2004) and personal experience (Clark, 1996). However they are not yet implemented in the present work. 5 Reward function The reward function calculates the reward awarded to the reinforcement learning agent at the end of each dialogue session. Successful task completion is rewarded with 1000 points. Dialogues running beyond 50 turns are deemed 78 Feature Values user knows router y/n/u user knows power light y/n/u user knows dsl light y/n/u user knows online light y/n/u user knows network icon y/n/u user knows network connections y/n/u user knows wifi y/n/u Table 6: (Part of) Dialogue state for Lexical Alignment. unsuccessful and are aw</context>
<context position="26900" citStr="Clark, 1996" startWordPosition="4337" endWordPosition="4338"> hand-coded policies. Although lexical alignment policies could be hand-coded, the designers would need to invest significant resources every time the list of referring expressions is revised or the conditions of the dialogue change. Using reinforcement learning, nearoptimal lexical alignment policies can be learned quickly and automatically. This model can be used in any task where interactions need to be tailored to different users’ lexical knowledge of the domain. 8.1 Future work Lexical alignment in dialogue also happens due to priming (Pickering and Garrod, 2004) and personal experience (Clark, 1996). We will examine trade-offs in various conditions, like ‘instruct’ versus ‘teach’ and low versus high retention users. Using Wizard-of-Oz studies and knowledge surveys, we plan to make the model more data-driven and realistic (Janarthanam and Lemon, 2009). We will also evaluate the learned policies with real users. Acknowledgements The research leading to these results has received funding from the European Community’s Seventh Framework (FP7) under grant agreement no. 216594 (CLASSiC Project www.classicproject.org), EPSRC project no. EP/E019501/1, and the British Council (UKIERI PhD Scholarsh</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>H. H. Clark. 1996. Using Language. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Georgila</author>
<author>J Henderson</author>
<author>O Lemon</author>
</authors>
<title>Learning User Simulations for Information State Update Dialogue Systems.</title>
<date>2005</date>
<booktitle>In Proceedings of Eurospeech/Interspeech.</booktitle>
<contexts>
<context position="4901" citStr="Georgila et al., 2005" startWordPosition="778" endWordPosition="781">ly to questions or instructions from an SDS. We then present a reinforcement learning model of lexical alignment due to audience design (in sections 4 &amp; 5). We then evaluate the User Simulation (section 6), testing whether a simulation that is sensitive to a system’s RE choices can be used to learn good lexical alignment policies. Finally, we compare policies learned in interaction with the User Simulation with hand-coded policies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and </context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2005</marker>
<rawString>K. Georgila, J. Henderson, and O. Lemon. 2005. Learning User Simulations for Information State Update Dialogue Systems. In Proceedings of Eurospeech/Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Issacs</author>
<author>H H Clark</author>
</authors>
<title>References in conversations between experts and novices.</title>
<date>1987</date>
<journal>Journal of Experimental Psychology: General,</journal>
<pages>116--26</pages>
<contexts>
<context position="1986" citStr="Issacs and Clark, 1987" startWordPosition="296" endWordPosition="299">e dialogue management level), it faces several decisions to be made at the natural language generation (NLG) level. These include, deciding which concepts to include in the utterance, deciding the referring expressions (RE) to use in the utterance and so on. A little-studied problem is to what extent a system could automatically align to the user’s lexical knowledge by adapting its RE choices, in particular based on his domain expertise, and how this can be modelled and optimised computationally. Oliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical align</context>
</contexts>
<marker>Issacs, Clark, 1987</marker>
<rawString>E. A. Issacs and H. H. Clark. 1987. References in conversations between experts and novices. Journal of Experimental Psychology: General, 116:26–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems.</title>
<date>2008</date>
<booktitle>In Proc. SEMdial’08.</booktitle>
<contexts>
<context position="2520" citStr="Janarthanam and Lemon, 2008" startWordPosition="380" endWordPosition="383">rmatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” instead of “ADSL filter” and “monitor symbol” instead of “network icon”, because it learnt that the user is a novice based on their clarific</context>
<context position="5513" citStr="Janarthanam and Lemon, 2008" startWordPosition="873" endWordPosition="876">ila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that simulates a variety of users with different domain knowledge profiles. Although this model incorporated clarification acts at the conceptual level, these users ignore the issues concerning the user’s understanding of the REs used by the system. In this work, in contrast to the above, we present a User Simulation model which explicitly encodes the user’s lexical knowledge of the domain, understands descriptive expressions, and issues clarification requests at the lexical level. 3 User Simulation Our User Simulation module simulates dialogue behaviour of d</context>
<context position="7238" citStr="Janarthanam and Lemon, 2008" startWordPosition="1145" endWordPosition="1148">lation’s short term memory for later use in the conversation. Simulated users interact with the environment using an interactive mechanism that allows them to observe and manipulate the states of various domain objects. The interaction between the user and the other components is given in figure 1 (notations explained in later sections). Figure 1: Experimental setup 3.1 Domain knowledge model Domain experts know most of the technical terms that are used to refer to domain objects whereas novice users can only reliably identify them when descriptive expressions are used. While in the model of (Janarthanam and Lemon, 2008) knowledge profiles were presented only at conceptual levels (e.g. does the user know what a modem is?), we present them in a more granular fashion. In this model, the user’s domain knowledge profile is factored into lexical (LKu,t), factual (FKu,t) and procedural knowledge (PKu,t) components. 75 Lexical knowledge LK,,,t vocab([modem, router], dobj1) vocab([wireless, WiFi], dobj3) vocab([modem power light], dobj7) Factual knowledge FK,,,t location(dobj1) location(dobj7) Procedural knowledge PK,,,t procedure(replace filter) procedure(refresh page) Table 1: Knowledge profile - Intermediate user.</context>
</contexts>
<marker>Janarthanam, Lemon, 2008</marker>
<rawString>S. Janarthanam and O. Lemon. 2008. User simulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems. In Proc. SEMdial’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
</authors>
<title>A Wizard-of-Oz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In</title>
<date>2009</date>
<booktitle>Proc. ENLG’09.</booktitle>
<contexts>
<context position="9147" citStr="Janarthanam and Lemon, 2009" startWordPosition="1432" endWordPosition="1435">edge among the users. For instance, people who know the word “router” most likely also know “DSL light” and “modem” and so on. These dependencies between REs are encoded as conditional probabilities in the Bayesian model. Figure 2 shows the dependencies between knowledge of REs. Figure 2: Bayes Net for User Lexical Knowledge Using this Bayesian model, we instantiate different knowledge profiles for different users. The current conditional probabilities were set by hand based on intuition. In future work, these values will be populated based on simple knowledge surveys performed on real users (Janarthanam and Lemon, 2009). This method creates a spectrum of users from ones who have no knowledge of technical terms to ones who know all the technical jargon, though every profile will have a different frequency of occurrence. This difference in frequency reflects that expert users are less common than novice users. The user’s domain knowledge can be dynamically updated. The new REs, both technical and descriptive, presented by the system through clarification moves are stored in the user’s short term memory. Exactly how long (in terms of dialogue turns) to retain the newly acquired knowledge is given by a retention</context>
</contexts>
<marker>Janarthanam, Lemon, 2009</marker>
<rawString>S. Janarthanam and O. Lemon. 2009. A Wizard-of-Oz environment to study Referring Expression Generation in a Situated Spoken Dialogue Task. In Proc. ENLG’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Komatani</author>
<author>S Ueno</author>
<author>T Kawahara</author>
<author>H G Okuno</author>
</authors>
<title>Flexible Guidance Generation using User Model in Spoken Dialogue Systems. In</title>
<date>2003</date>
<booktitle>Proc. ACL’03.</booktitle>
<contexts>
<context position="5333" citStr="Komatani et al., 2003" startWordPosition="845" endWordPosition="848">ies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that simulates a variety of users with different domain knowledge profiles. Although this model incorporated clarification acts at the conceptual level, these users ignore the issues concerning the user’s understanding of the REs used by the system. In this work, in contrast to the above, we present a User Simulation model which explicitly encodes the user’s lexical knowledge of the</context>
</contexts>
<marker>Komatani, Ueno, Kawahara, Okuno, 2003</marker>
<rawString>K. Komatani, S. Ueno, T. Kawahara, and H. G. Okuno. 2003. Flexible Guidance Generation using User Model in Spoken Dialogue Systems. In Proc. ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
</authors>
<title>Adaptive Natural Language Generation in Dialogue using Reinforcement Learning.</title>
<date>2008</date>
<booktitle>In Proc. SEMdial’08.</booktitle>
<contexts>
<context position="2466" citStr="Lemon, 2008" startWordPosition="374" endWordPosition="375">tionally. Oliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” instead of “ADSL filter” and “monitor symbol” instead of “network icon”, because it le</context>
<context position="5513" citStr="Lemon, 2008" startWordPosition="875" endWordPosition="876">; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that simulates a variety of users with different domain knowledge profiles. Although this model incorporated clarification acts at the conceptual level, these users ignore the issues concerning the user’s understanding of the REs used by the system. In this work, in contrast to the above, we present a User Simulation model which explicitly encodes the user’s lexical knowledge of the domain, understands descriptive expressions, and issues clarification requests at the lexical level. 3 User Simulation Our User Simulation module simulates dialogue behaviour of d</context>
<context position="7238" citStr="Lemon, 2008" startWordPosition="1147" endWordPosition="1148">erm memory for later use in the conversation. Simulated users interact with the environment using an interactive mechanism that allows them to observe and manipulate the states of various domain objects. The interaction between the user and the other components is given in figure 1 (notations explained in later sections). Figure 1: Experimental setup 3.1 Domain knowledge model Domain experts know most of the technical terms that are used to refer to domain objects whereas novice users can only reliably identify them when descriptive expressions are used. While in the model of (Janarthanam and Lemon, 2008) knowledge profiles were presented only at conceptual levels (e.g. does the user know what a modem is?), we present them in a more granular fashion. In this model, the user’s domain knowledge profile is factored into lexical (LKu,t), factual (FKu,t) and procedural knowledge (PKu,t) components. 75 Lexical knowledge LK,,,t vocab([modem, router], dobj1) vocab([wireless, WiFi], dobj3) vocab([modem power light], dobj7) Factual knowledge FK,,,t location(dobj1) location(dobj7) Procedural knowledge PK,,,t procedure(replace filter) procedure(refresh page) Table 1: Knowledge profile - Intermediate user.</context>
</contexts>
<marker>Lemon, 2008</marker>
<rawString>O. Lemon. 2008. Adaptive Natural Language Generation in Dialogue using Reinforcement Learning. In Proc. SEMdial’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<date>1997</date>
<booktitle>Learning Dialogue Strategies within the Markov Decision Process Framework. In Proceedings ofASRU97.</booktitle>
<contexts>
<context position="13596" citStr="Levin et al., 1997" startWordPosition="2135" endWordPosition="2138">ion, they are added to the user’s short term memory (steps 6-8). Although, the action selection process is deterministic at this level, it is dependent on the users’ diverse knowledge profiles, which ensures stochastic dialogue behaviour amongst different users created by the module. greet the user request status(x) request action(x) give description(x) accept verification(x,y) give location(dobj) give procedure(daction) close dialogue Table 4: System Dialogue acts. 4 Dialogue System Model The dialogue system is modeled as a reinforcement learning agent in a Markov Decision Process framework (Levin et al., 1997). At every turn, it interacts with the Simulated User by issuing a System Dialogue Act (As,t) along with a set of REs, called the System RE Choices (RECs,t). RECs,t contains the REs that refer to various domain objects in the dialogue act As,t. First, the system decides the dialogue act to issue using a hand-coded dialogue strategy. Troubleshooting instructions are coded in the troubleshooting decision tree1. Dialogue repair moves include selecting clarification moves in response to user’s request. The list of system dialogue acts is given Table 4. The system issues various repair moves when t</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 1997</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 1997. Learning Dialogue Strategies within the Markov Decision Process Framework. In Proceedings ofASRU97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Molich</author>
<author>J Nielsen</author>
</authors>
<title>Improving a HumanComputer Dialogue.</title>
<date>1990</date>
<journal>Communications of the ACM,</journal>
<pages>33--3</pages>
<contexts>
<context position="2383" citStr="Molich and Nielsen, 1990" startWordPosition="358" endWordPosition="361">, in particular based on his domain expertise, and how this can be modelled and optimised computationally. Oliver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” ins</context>
</contexts>
<marker>Molich, Nielsen, 1990</marker>
<rawString>R. Molich and J. Nielsen. 1990. Improving a HumanComputer Dialogue. Communications of the ACM, 33-3:338–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Pickering</author>
<author>S Garrod</author>
</authors>
<title>Toward a mechanistic psychology of dialogue.</title>
<date>2004</date>
<booktitle>Behavioral and Brain Sciences,</booktitle>
<pages>27--169</pages>
<contexts>
<context position="18489" citStr="Pickering and Garrod, 2004" startWordPosition="2944" endWordPosition="2947">tem learn the statistical usage patterns by itself. Part of the dialogue state 1. router / black box / black box with lights 2. power light / first light on the panel 3. DSL light / second light on the panel 4. online light / third light on the panel 5. network icon / flashing computer symbol 6. network connections / earth with plug 7. WiFi / wireless Table 5: System RE choices. relevant to system’s RE choices is given in Table 6. The state can be extended to include other relevant information like the usage of various REs by the user as well to enable alignment with the user through priming (Pickering and Garrod, 2004) and personal experience (Clark, 1996). However they are not yet implemented in the present work. 5 Reward function The reward function calculates the reward awarded to the reinforcement learning agent at the end of each dialogue session. Successful task completion is rewarded with 1000 points. Dialogues running beyond 50 turns are deemed 78 Feature Values user knows router y/n/u user knows power light y/n/u user knows dsl light y/n/u user knows online light y/n/u user knows network icon y/n/u user knows network connections y/n/u user knows wifi y/n/u Table 6: (Part of) Dialogue state for Lexi</context>
<context position="26862" citStr="Pickering and Garrod, 2004" startWordPosition="4329" endWordPosition="4332">th this type of simulation are better than a range of hand-coded policies. Although lexical alignment policies could be hand-coded, the designers would need to invest significant resources every time the list of referring expressions is revised or the conditions of the dialogue change. Using reinforcement learning, nearoptimal lexical alignment policies can be learned quickly and automatically. This model can be used in any task where interactions need to be tailored to different users’ lexical knowledge of the domain. 8.1 Future work Lexical alignment in dialogue also happens due to priming (Pickering and Garrod, 2004) and personal experience (Clark, 1996). We will examine trade-offs in various conditions, like ‘instruct’ versus ‘teach’ and low versus high retention users. Using Wizard-of-Oz studies and knowledge surveys, we plan to make the model more data-driven and realistic (Janarthanam and Lemon, 2009). We will also evaluate the learned policies with real users. Acknowledgements The research leading to these results has received funding from the European Community’s Seventh Framework (FP7) under grant agreement no. 216594 (CLASSiC Project www.classicproject.org), EPSRC project no. EP/E019501/1, and the</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>M. J. Pickering and S. Garrod. 2004. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences, 27:169–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rieser</author>
<author>O Lemon</author>
</authors>
<title>Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems.</title>
<date>2009</date>
<booktitle>In Proc. EACL’09.</booktitle>
<contexts>
<context position="2490" citStr="Rieser and Lemon, 2009" startWordPosition="376" endWordPosition="379">ver Lemon School of Informatics University of Edinburgh Edinburgh EH8 9AB olemon@inf.ed.ac.uk (Issacs and Clark, 1987) show how two interlocutors adapt their language in a conversation by assessing each other’s domain expertise during dialogue, by observing how they react to each other’s RE choices. This is called alignment through Audience Design (Clark, 1996; Bell, 1984). Using inappropriate REs in instructions has been identified as a serious problem affecting a system’s usability (Molich and Nielsen, 1990). In this paper, we treat NLG within a computational learning paradigm (Lemon, 2008; Rieser and Lemon, 2009; Janarthanam and Lemon, 2008). We examine whether a SDS can automatically learn a lexical alignment policy for audience design, which enables it to choose appropriate REs by predicting the user’s lexical knowledge dynamically during the course of the dialogue. This can avoid clarification requests from the users and keep the dialogues short. The example given below describes the kind of lexical alignment behaviour that we want the system to learn. The system chooses “small white box” instead of “ADSL filter” and “monitor symbol” instead of “network icon”, because it learnt that the user is a </context>
</contexts>
<marker>Rieser, Lemon, 2009</marker>
<rawString>V. Rieser and O. Lemon. 2009. Natural Language Generation as Planning Under Uncertainty for Spoken Dialogue Systems. In Proc. EACL’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K WeilHammer</author>
<author>M N Stuttle</author>
<author>S J Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review,</title>
<date>2006</date>
<pages>97--126</pages>
<contexts>
<context position="4926" citStr="Schatzmann et al., 2006" startWordPosition="782" endWordPosition="785">ructions from an SDS. We then present a reinforcement learning model of lexical alignment due to audience design (in sections 4 &amp; 5). We then evaluate the User Simulation (section 6), testing whether a simulation that is sensitive to a system’s RE choices can be used to learn good lexical alignment policies. Finally, we compare policies learned in interaction with the User Simulation with hand-coded policies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a </context>
</contexts>
<marker>Schatzmann, WeilHammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. WeilHammer, M. N. Stuttle, and S. J. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement Learning of Dialogue Management Strategies. Knowledge Engineering Review, pages 97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Weilhammer</author>
<author>H Ye</author>
<author>S J Young</author>
</authors>
<title>Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In</title>
<date>2007</date>
<booktitle>Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="4952" citStr="Schatzmann et al., 2007" startWordPosition="786" endWordPosition="789">then present a reinforcement learning model of lexical alignment due to audience design (in sections 4 &amp; 5). We then evaluate the User Simulation (section 6), testing whether a simulation that is sensitive to a system’s RE choices can be used to learn good lexical alignment policies. Finally, we compare policies learned in interaction with the User Simulation with hand-coded policies, and present the results in section 7. 2 Related work Several statistical user simulation models that model a user’s behaviour in a conversation have been proposed (Georgila et al., 2005; Schatzmann et al., 2006; Schatzmann et al., 2007). These models issue task specific dialogue acts like informing their search constraints, confirming values, rejecting misrecognised values, etc. However, they do not model a user population with varying domain expertise. Also, none of these models seek clarification at conceptual or lexical levels that occur naturally in conversations between real users. (Komatani et al., 2003) proposed using user models with features like skills, domain knowledge and hastiness as a part of the dialogue manager to produce adaptive responses. (Janarthanam and Lemon, 2008) presented a user simulation model that</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and S. J. Young. 2007. Agenda-based User Simulation for Bootstrapping a POMDP Dialogue System. In Proceedings of HLT/NAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Schlangen</author>
</authors>
<title>Causes and strategies for requesting clarification in dialogue.</title>
<date>2004</date>
<booktitle>Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue (SIGDIAL 04),</booktitle>
<location>Boston.</location>
<contexts>
<context position="10772" citStr="Schlangen, 2004" startWordPosition="1707" endWordPosition="1708"> description is issued when the SDS uses technical terms that the simulated user does not know, e.g. “What is a router?”. Request verification is issued when the SDS uses descriptive lexical items for domain objects that the user knows more technical terms for, e.g. System: “Is the black box plugged in?” User: “Do you mean the router?”. Request disambiguation is issued when the user faces an underspecified and ambiguous descriptive expression, e.g.“User: I have two black boxes here - one with lights and one without. Which one is it?”. These clarification strategies have been modeled based on (Schlangen, 2004). The user simulation also issues request location and request procedure dialogue acts, when it does not know the location of domain objects or how to manipulate them, respectively. 3.3 Environment simulation The environment simulation includes both physical objects, such as the computer, modem, ADSL filter, etc and virtual objects, such as the browser, control panel, etc in the user’s environment. Physical and virtual connections between these objects 76 report problem provide info(dobj, info) acknowledge request verification(x, y) request description(x) request disambiguation(x, [y1,y2]) req</context>
</contexts>
<marker>Schlangen, 2004</marker>
<rawString>D. Schlangen. 2004. Causes and strategies for requesting clarification in dialogue. Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue (SIGDIAL 04), Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="20965" citStr="Sutton and Barto, 1998" startWordPosition="3351" endWordPosition="3354">ocedural knowledge components, because the learning exercise was targeted only at the lexical level. With respect to the lexical knowledge, complete experts knew all the technical terms in the domain. Complete novices, on the other hand, knew only one: power light. We set the RIu to 10, so that the users do not forget newly learned lexical items for 10 subsequent turns. Ideally, we expected the system to learn to use technical terms with experts and to use descriptive expressions with novices and a mixture for intermediates. The system was trained using SARSA reinforcement learning algorithm (Sutton and Barto, 1998), with linear function approximation, for 50000 cycles. It produced around 1500 dialogues and produced an alignment policy (RL1) that adapted to users after the first turn which provides evidence about the kind of user the system is dealing with. The system learns to get high reward by producing shorter dialogues. By learning to choose REs by adapting to the lexical knowledge of the user, it avoids unnecessary clarification and repair episodes. It learns to choose descriptive expressions for novice users and jargon for expert users. It also learns to use technical terms when all users know the</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. Sutton and A. Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
</authors>
<title>Applying POMDPs to Dialog Systems in the Troubleshooting Domain.</title>
<date>2007</date>
<booktitle>In Proc HLT/NAACL Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technology.</booktitle>
<contexts>
<context position="1185" citStr="Williams, 2007" startWordPosition="170" endWordPosition="171">opose a new statistical user model which incorporates the lexical knowledge of different users. We evaluate this user model by showing that it allows us to learn dialogue policies that automatically adapt their choice of referring expressions online to different users, and that these policies are significantly better than adaptive hand-coded policies for this problem. The learned policies are consistently between 2 and 8 turns shorter than a range of different hand-coded but adaptive baseline lexical alignment policies. 1 Introduction In current “troubleshooting” spoken dialogue systems (SDS)(Williams, 2007), the major part of the conversation is directed by the system, while the user follows the system’s instructions. Once the system decides what instruction to give the user (at the dialogue management level), it faces several decisions to be made at the natural language generation (NLG) level. These include, deciding which concepts to include in the utterance, deciding the referring expressions (RE) to use in the utterance and so on. A little-studied problem is to what extent a system could automatically align to the user’s lexical knowledge by adapting its RE choices, in particular based on hi</context>
</contexts>
<marker>Williams, 2007</marker>
<rawString>J. Williams. 2007. Applying POMDPs to Dialog Systems in the Troubleshooting Domain. In Proc HLT/NAACL Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>