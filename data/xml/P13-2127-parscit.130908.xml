<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.913894">
Annotation of regular polysemy and underspecification
</title>
<author confidence="0.318025">
N´uria Bel
</author>
<affiliation confidence="0.189243">
Universitat Pompeu Fabra
</affiliation>
<address confidence="0.529274">
Barcelona (Spain)
</address>
<email confidence="0.853089">
nuria.bel@upf.edu
</email>
<author confidence="0.9590195">
H´ector Martinez Alonso,
Bolette Sandford Pedersen
</author>
<affiliation confidence="0.885299">
University of Copenhagen
Copenhagen (Denmark)
</affiliation>
<email confidence="0.997237">
alonso@hum.ku.dk, bsp@hum.ku.dk
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955727272727">
We present the result of an annotation task
on regular polysemy for a series of seman-
tic classes or dot types in English, Dan-
ish and Spanish. This article describes
the annotation process, the results in terms
of inter-encoder agreement, and the sense
distributions obtained with two methods:
majority voting with a theory-compliant
backoff strategy, and MACE, an unsuper-
vised system to choose the most likely
sense from all the annotations.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998556291666667">
This article shows the annotation task of a corpus
in English, Danish and Spanish for regular poly-
semy. Regular polysemy (Apresjan, 1974; Puste-
jovsky, 1995; Briscoe et al., 1995; Nunberg, 1995)
has received a lot of attention in computational
linguistics (Boleda et al., 2012; Rumshisky et al.,
2007; Shutova, 2009). The lack of available sense-
annotated gold standards with underspecification
is a limitation for NLP applications that rely on
dot types1 (Rumshisky et al., 2007; Poibeau, 2006;
Pustejovsky et al., 2009).
Our goal is to obtain human-annotated corpus
data to study regular polysemy and to detect it in
an automatic manner. We have collected a cor-
pus of annotated examples in English, Danish and
Spanish to study the alternation between senses
and the cases of underspecification, including a
contrastive study between languages. Here we de-
scribe the annotation process, its results in terms
of inter-encoder agreement, and the sense distri-
butions obtained with two methods: majority vot-
ing with a theory-compliant backoff strategy and,
MACE an unsupervised system to choose the most
likely sense from all the annotations.
</bodyText>
<footnote confidence="0.6413385">
1The corpus is freely available at
http://metashare.cst.dk/repository/search/?q=regular+polysemy
</footnote>
<sectionHeader confidence="0.98118" genericHeader="introduction">
2 Regular polysemy
</sectionHeader>
<bodyText confidence="0.9974352">
Very often a word that belongs to a semantic type,
like Location, can behave as a member of another
semantic type, like Organization, as shown by the
following examples from the American National
Corpus (Ide and Macleod, 2001) (ANC):
</bodyText>
<listItem confidence="0.782717">
a) Manuel died in exile in 1932 in England.
b) England was being kept busy with other con-
cerns
c) England was, after all, an important wine
market
</listItem>
<bodyText confidence="0.999395571428571">
In case a), England refers to the English terri-
tory (Location), whereas in b) it refers arguably to
England as a political entity (Organization). The
third case refers to both. The ability of certain
words to switch between semantic types in a pre-
dictable manner is referred to as regular polysemy.
Unlike other forms of meaning variation caused
by metaphor or homonymy, regular polysemy is
considered to be caused by metonymy (Apresjan,
1974; Lapata and Lascarides, 2003). Regular pol-
ysemy is different from other forms of polysemy
in that both senses can be active at the same in a
predicate, which we refer to as underspecification.
Underspecified instances can be broken down in:
</bodyText>
<listItem confidence="0.996856125">
1. Contextually complex: England was, after
all, an important wine market
2. Zeugmatic, in which two mutually exclusive
readings are coordinated: England is conser-
vative and rainy
3. Vague, in which no contextual element en-
forces a reading: The case of England is sim-
ilar
</listItem>
<sectionHeader confidence="0.627565" genericHeader="method">
3 Choice of semantic classes
</sectionHeader>
<bodyText confidence="0.99965725">
The Generative Lexicon (GL) (Pustejovsky, 1995)
groups nouns with their most frequent metonymic
sense in a semantic class called a dot type. For
English, we annotate 5 dot types from the GL:
</bodyText>
<listItem confidence="0.90315">
1. Animal/Meat: ”The chicken ran away” vs.
</listItem>
<page confidence="0.958885">
725
</page>
<bodyText confidence="0.245388666666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 725–730,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
”the chicken was delicious”.
</bodyText>
<listItem confidence="0.996339705882353">
2. Artifact/Information : ”The book fell” vs.
”the book was boring”.
3. Container/Content: ”The box was red” vs.
”I hate the whole box”.
4. Location/Organization: ”England is far”
vs. ”England starts a tax reform”.
5. Process/Result: ”The building took months
to finish” vs. ”the building is sturdy”.
For Danish and Spanish, we have chosen Con-
tainer/Content and Location/Organization. We
chose the first one because we consider it the
most prototypical case of metonymy from the ones
listed in the GL. We chose the second one because
the metonymies in locations are a common con-
cern for Named-Entity Recognition (Johannessen
et al., 2005) and a previous area of research in
metonymy resolution (Markert and Nissim, 2009).
</listItem>
<sectionHeader confidence="0.94736" genericHeader="method">
4 Annotation Scheme
</sectionHeader>
<bodyText confidence="0.99979412">
For each of the nine (five for English, two for Dan-
ish, two for Spanish) dot types, we have randomly
selected 500 corpus examples. Each example con-
sists of a sentence with a selected headword be-
longing to the corresponding dot type. In spite of
a part of the annotation being made with a con-
trastive study in mind, no parallel text was used
to avoid using translated text. For English and
Danish we used freely available reference corpora
(Ide and Macleod, 2001; Andersen et al., 2002)
and, for Spanish, a corpus built from newswire and
technical text (Vivaldi, 2009).
For most of the English examples we used the
words in Rumshisky (2007), except for Loca-
tion/Organization. For Danish and Spanish we
translated the words from English. We expanded
the lists using each language’s wordnet (Pedersen
et al., 2009; Gonzalez-Agirre et al., 2012) as the-
saurus to make the total of occurrences reach 500
after we had removed homonyms and other forms
of semantic variation outside of the purview of
regular polysemy.
For Location/Organization we have used high-
frequency names of geopolitical locations from
each of the corpora. Many of them are corpus-
specific (e.g. Madrid is more frequent in the
Spanish corpus) but a set of words is shared:
Afghanistan, Africa, America, China, England,
Europe,Germany, London.
Every dot type has its particularities that we had
to deal with. For instance, English has lexical al-
ternatives for the meat of several common animals,
like venison or pork instead of deer and pig. This
lexical phenomenon does not impede metonymy
for the animal names, it just makes it less likely.
In order to assess this, we have included 20 ex-
amples of cow. The rest of the dataset consists of
animal names that do not participate in this lexical
alternation, like eel, duck, chicken, or sardine.
We call the first sense in the pair of metonyms
that make up the dot type the literal sense, and the
second sense the metonymic sense, e.g. Location
is the literal sense in Location/Organization.
Each block of 500 sentences belonging to a
dot type was an independent annotation subtask
with an isolated description. The annotator was
shown an example and had to determine whether
the headword in the example had the literal,
metonymic or the underspecified sense. Figure 1
shows an instance of the annotation process.
</bodyText>
<figureCaption confidence="0.98856">
Figure 1: Screen capture for a Mechanical Turk
annotation instance or HIT
</figureCaption>
<bodyText confidence="0.999639130434783">
This annotation scheme is designed with the in-
tention of capturing literal, metonymic and under-
specified senses, and we use an inventory of three
possible answers, instead of using Markert and
Nissim’s (Markert and Nissim, 2002; Nissim and
Markert, 2005) approach with fine-grained sense
distinctions, which are potentially more difficult to
annotate and resolve automatically. Markert and
Nissim acknowledge a mixed sense they define as
being literal and metonymic at the same time.
For English we used Amazon Mechanical Turk
(AMT) with five annotations per example by turk-
ers certified as Classification Masters. Using AMT
provides annotations very quickly, possibly at the
expense of reliability, but it has been proven suit-
able for sense-disambiguation task (Snow et al.,
2008). Moreover, it is not possible to obtain an-
notations for every language using AMT. Thus,
for Danish and Spanish, we obtained annotations
from volunteers, most of them native or very pro-
ficient non-natives. See Table 1 for a summary of
the annotation setup for each language.
After the annotation task we obtained the agree-
</bodyText>
<page confidence="0.994519">
726
</page>
<table confidence="0.98321225">
Language annotators type
Danish 3-4 volunteer
English 5 AMT
Spanish 6-7 volunteer
</table>
<tableCaption confidence="0.9771025">
Table 1: Amount and type of annotators per in-
stance for each language.
ment values shown in Table 2. The table also pro-
vides the abbreviated names of the datasets.
</tableCaption>
<table confidence="0.9999493">
Dot type Ao ± σ α
eng:animeat 0.86 ± 0.24 0.69
eng:artinfo 0.48 ± 0.23 0.12
eng:contcont 0.65 ± 0.28 0.31
eng:locorg 0.72 ± 0.29 0.46
eng:procres 0.5 ± 0.24 0.10
da:contcont 0.32 ± 0.37 0.39
da:locorg 0.73 ± 0.37 0.47
spa:contcont 0.36 ± 0.3 0.42
spa:locorg 0.52 ±0.28 0.53
</table>
<tableCaption confidence="0.794474">
Table 2: Averaged observed agreement and its
standard deviation and alpha
</tableCaption>
<bodyText confidence="0.999881866666667">
Average observed agreement (Ao) is the mean
across examples for the proportion of matching
senses assigned by the annotators. Krippendorff’s
alpha is an aggregate measure that takes chance
disagreement in consideration and accounts for the
replicability of an annotation scheme. There are
large differences in α across datasets.
The scheme can only provide reliable (Artstein
and Poesio, 2008) annotations (α &gt; 0.6) for one
dot type2. This indicates that not all dot types are
equally easy to annotate, regardless of the kind of
annotator. In spite of the number and type of an-
notators, the Location/Organization dot type gives
fairly high agreement values for a semantic task,
and this behavior is consistent across languages.
</bodyText>
<sectionHeader confidence="0.697227" genericHeader="method">
5 Assigning sense by majority voting
</sectionHeader>
<bodyText confidence="0.9997879">
Each example has more than one annotation and
we need to determine a single sense tag for each
example. However, if we assign senses by major-
ity voting, we need a backoff strategy in case of
ties.
The common practice of backing off to the most
frequent sense is not valid in this scenario, where
there can be a tie between the metonymic and
the underspecified sense. We use a backoff that
incorporates our assumption about the relations
</bodyText>
<footnote confidence="0.755016">
2We have made the data freely available at
http://metashare.cst.dk/repository/search/?q=regular+polysemy
</footnote>
<bodyText confidence="0.893658666666667">
between senses, namely that the underspecified
sense sits between the literal and the metonymic
senses:
</bodyText>
<listItem confidence="0.98638175">
1. If there is a tie between the underspecified
and literal senses, the sense is literal.
2. If there is a tie between the underspec-
ified and metonymic sense, the sense is
metonymic.
3. If there is a tie between the literal and
metonymic sense or between all three senses,
the sense is underspecified.
</listItem>
<table confidence="0.9998793">
Dot type L M U V B
eng:animeat 358 135 7 3 4
eng:artinfo 141 305 54 8 48
eng:contcont 354 120 25 0 25
eng:locorg 307 171 22 3 19
eng:procres 153 298 48 3 45
da:contcont 328 82 91 53 38
da:locorg 322 95 83 44 39
spa:contcont 291 140 69 54 15
soa:locorg 314 139 47 40 7
</table>
<tableCaption confidence="0.99216">
Table 3: Literal, Metonymic and Underspecified
</tableCaption>
<bodyText confidence="0.984152620689655">
sense distributions, and underspecified senses bro-
ken down in Voting and Backoff
The columns labelled L, M and U in Table 3
provide the sense distributions for each dot type.
The preference for the underspecified sense varies
greatly, from the very infrequent for English in
Animal/Meat to the two Danish datasets where the
underspecified sense evens with the metonymic
one. However, the Danish examples have mostly
three annotators, and chance disagreement is the
highest for this language in this setup, i.e., the
chance for an underspecified sense in Danish to
be assigned by our backoff strategy is the highest.
Columns V and B show respectively whether
the underspecified senses are a result of majority
voting or backoff. In contrast to volunteers, turk-
ers disprefer the underspecified option and most
of the English underspecified senses are assigned
by backoff. However, it cannot be argued that
turkers have overused clicking on the first option
(a common spamming behavior) because we can
see that two of the English dot types (eng:artinfo,
eng:procres) have majority of metonymic senses,
which are always second in the scheme (cf. Fig.
1). Looking at the amount of underspecified
senses that have been obtained by majority vot-
ing for Danish and Spanish, we suggest that the
level of abstraction required by this annotation is
too high for turkers to perform at a level compara-
</bodyText>
<page confidence="0.991679">
727
</page>
<bodyText confidence="0.597296">
ble to that of our volunteer annotators.
</bodyText>
<figureCaption confidence="0.7221195">
Figure 2: Proportion of non-literality in location
names across languages
Figure 2 shows the proportion of non-literal
(metonymic+underspecified) examples for the
Location/Organization words that are common
across languages. We can see that individual
words show sense skewdness. This skewdness is
a consequence of the kind of text in the corpus:
</figureCaption>
<bodyText confidence="0.921962833333333">
e.g. America has a high proportion of non-literal
senses in the ANC, where it usually means ”the
population or government of the US”. Similarly,
it is literal less than 50% of the times for the other
two languages. In contrast, Afghanistan is most
often used in its literal location sense.
</bodyText>
<sectionHeader confidence="0.950526" genericHeader="method">
6 Assigning senses with MACE
</sectionHeader>
<bodyText confidence="0.997303666666667">
Besides using majority voting with backoff , we
use MACE (Hovy et al., 2013) to obtain the sense
tag for each example.
</bodyText>
<table confidence="0.9977135">
Dot type L M U D I
eng:animeat 340 146 14 .048 3
eng:artinfo 170 180 150 .296 46
eng:contcont 295 176 28 .174 0
eng:locorg 291 193 16 .084 3
eng:procres 155 210 134 .272 33
da:contcont 223 134 143 .242 79
da:locorg 251 144 105 .206 53
spa:contcont 270 155 75 .074 56
spa:locorg 302 146 52 .038 40
</table>
<tableCaption confidence="0.990917">
Table 4: Sense distributions calculated with
</tableCaption>
<bodyText confidence="0.98763776744186">
MACE, plus Difference and Intersection of under-
specified senses between methods
MACE is an unsupervised system that uses
Expectation-Maximization (EM) to estimate the
competence of annotators and recover the most
likely answer. MACE is designed as a Bayesian
network that treats the ”correct” labels as latent
variables. This EM method can also be understood
as a clustering that assigns the value of the closest
calculated latent variable (the sense tag) to each
data point (the distribution of annotations).
Datasets that show less variation between
senses calculated using majority voting and using
MACE will be more reliable. Along the sense dis-
tribution in the first three columns, Table 4 pro-
vides the proportion of the senses that is different
between majority voting and MACE (D), and the
size of the intersection (I) of the set of underspec-
ified examples by voting and by MACE, namely
the overlap of the U columns of Tables 3 and 4.
Table 4 shows a smoother distribution of senses
than Table 3, as majority classes are down-
weighted by MACE. It takes very different de-
cisions than majority voting for the two En-
glish datasets with lowest agreement (eng:artinfo,
eng:procres) and for the Danish datasets, which
have the fewest annotators. For these cases, the
diferences oscillate between 0.206 and 0.296.
Although MACE increases the frequency of
the underspecified senses for all datasets but one
(eng:locorg), the underspecified examples in Ta-
ble 3 are not subsumed by the MACE results. The
values in the I column show that none of the un-
derspecified senses of eng:contcont receive the un-
derspecified sense by MACE. All of these exam-
ples, however, were resolved by backoff, as well
as most of the other underspecified cases in the
other English datasets. In contrast to the voting
method, MACE does not operate with any theo-
retical assumption about the relation between the
three senses and treats them independently when
assigning the most likely sense tag to each distri-
bution of annotations.
</bodyText>
<sectionHeader confidence="0.942391" genericHeader="method">
7 Comparison between methods
</sectionHeader>
<bodyText confidence="0.9996712">
The voting system and MACE provide different
sense tags. The following examples (three from
eng:contcont and four from eng:locorg) show dis-
agreement between the sense tag assigned by vot-
ing and by MACE:
</bodyText>
<listItem confidence="0.985676888888889">
d) To ship a crate of lettuce across the country,
a trucker needed permission from a federal
regulatory agency.
e) Controls were sent a package containing
stool collection vials and instructions for col-
lection and mailing of samples.
f) In fact, it was the social committee, and
our chief responsibilities were to arrange for
bands and kegs of beer.
</listItem>
<page confidence="0.960649">
728
</page>
<bodyText confidence="0.702686666666667">
g) The most unpopular PM in Canada’s mod-
ern history, he introduced the Goods and Ser-
vices Tax, a VAT-like national sales tax.
</bodyText>
<listItem confidence="0.629299444444444">
h) This is Boston’s commercial and financial
heart, but it s farfrom being an homogeneous
district [...]
i) California has the highest number of people
in poverty in the nation — 6.4 million, includ-
ing nearly one in five children.
j) Under the Emperor Qianlong (Chien Lung),
Kangxi’s grandson, conflict arose between
Europe’s empires and the Middle Kingdom.
</listItem>
<bodyText confidence="0.999587285714286">
All of the previous examples were tagged as un-
derspecified by either the voting system or MACE,
but not by both. Table 5 breaks down the five an-
notations that each example received by turkers in
literal, metonymic and underspecified. The last
two columns show the sense tag provided by vot-
ing or MACE.
</bodyText>
<table confidence="0.99859625">
Example L M U VOTING MACE
d) 2 2 1 U L
e) 3 1 1 L U
f) 1 2 2 M U
g) 2 2 1 U M
h) 2 2 1 U M
i) 3 0 2 L U
j) 1 2 2 M U
</table>
<tableCaption confidence="0.7966">
Table 5: Annotation summary and sense tags for
the examples in this section
</tableCaption>
<bodyText confidence="0.999705129032258">
Just by looking at the table it is not immediate
which method is preferable to assign sense tags
in cases that are not clear-cut. In the case of i),
we consider the underspecified sense more ade-
quate than the literal one obtained by voting, just
like we are also more prone to prefer the under-
specified meaning in f), which has been assigned
by MACE. In the case of h), we consider that the
strictly metonymic sense assigned by MACE does
not capture both the organization- (”commercial
and financial”) and location-related (”district”) as-
pects of the meaning, and we would prefer the un-
derspecifed reading. However, MACE can also
overgenerate the underspecified sense, as the vials
mentioned in example e) are empty and have no
content yet, thereby being literal containers and
not their content.
Examples d), g) and h) have the same dis-
tribution of annotations—namely 2 literal, 2
metonymic and 1 underspecified—but d) has re-
ceived the literal sense from MACE, whereas the
other two are metonymic. This difference is a re-
sult of having trained MACE independently for
each dataset. The three examples receive the un-
derspecified sense from the voting scheme, since
neither the literal or metonymic sense is more
present in the annotations.
On the other hand, e) and i) are skewed towards
literality and receive the literal sense by plurality
without having to resort to any backoff, but they
are marked as underspecified by MACE.
</bodyText>
<sectionHeader confidence="0.998804" genericHeader="evaluation">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999967538461539">
We have described the annotation process of a
regular-polysemy corpus in English, Danish and
Spanish which deals with five different dot types.
After annotating the examples for their literal,
metonymic or underspecified reading, we have
determined that this scheme can provide reliable
(α over 0.60) annotations for one dot type. Not
all the dot types are equally easy to annotate.
The main source of variation in agreement, and
thus annotation reliability, is the dot type itself.
While eng:animeat and eng:locorg appear the eas-
iest, eng:artinfo and eng:procres obtain very low α
scores.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="conclusions">
9 Further work
</sectionHeader>
<bodyText confidence="0.99993025">
After collecting annotated data, the natural next
step is to attempt class-based word-sense disam-
biguation (WSD) to predict the senses in Tables 3
and 4 using a state-of-the-art system like Nastase
et al. (2012). We will consider a sense-assignment
method (voting or MACE) as more appropriate if
it provides the sense tags that are easiest to learn
by our WSD system.
However, learnability is only one possible pa-
rameter for quality, and we also want to develop
an expert-annotated gold standard to compare our
data against. We also consider the possibility of
developing a sense-assignment method that relies
both on the theoretical assumption behind the vot-
ing scheme and the latent-variable approach used
by MACE.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998472">
The research leading to these results has been
funded by the European Commission’s 7th Frame-
work Program under grant agreement 238405
(CLARA).
</bodyText>
<page confidence="0.99699">
729
</page>
<sectionHeader confidence="0.930552" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994409197802198">
Mette Skovgaard Andersen, Helle Asmussen, and Jørg
Asmussen. 2002. The project of korpus 2000 go-
ing public. In The Tenth EURALEX International
Congress: EURALEX 2002.
J. D. Apresjan. 1974. Regular polysemy. Linguistics.
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555–596.
Gemma Boleda, Sabine Schulte im Walde, and Toni
Badia. 2012. Modeling regular polysemy: A study
on the semantic classification of catalan adjectives.
Computational Linguistics, 38(3):575–616.
Ted Briscoe, Ann Copestake, and Alex Lascarides.
1995. Blocking. In Computational Lexical Seman-
tics. Citeseer.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual central repository ver-
sion 3.0. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC 2012), pages 2525–2529.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning whom to trust
with mace. In Proceedings of NAACL-HLT 2013.
N. Ide and C. Macleod. 2001. The american national
corpus: A standardized resource of american en-
glish. In Proceedings of Corpus Linguistics 2001,
pages 274–280. Citeseer.
J. B. Johannessen, K. Haagen, K. Haaland, A. B.
J´onsdottir, A. Nøklestad, D. Kokkinakis, P. Meurer,
E. Bick, and D. Haltrup. 2005. Named entity recog-
nition for the mainland scandinavian languages. Lit-
erary and Linguistic Computing, 20(1):91.
M. Lapata and A. Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Lin-
guistics, 29(2):261–315.
K. Markert and M. Nissim. 2002. Towards a cor-
pus annotated for metonymies: the case of location
names. In Proc. of LREC. Citeseer.
K. Markert and M. Nissim. 2009. Data and models
for metonymy resolution. Language Resources and
Evaluation, 43(2):123–138.
Vivi Nastase, Alex Judea, Katja Markert, and Michael
Strube. 2012. Local and global context for super-
vised and unsupervised metonymy resolution. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
183–193. Association for Computational Linguis-
tics.
Malvina Nissim and Katja Markert. 2005. Learning
to buy a renault and talk to bmw: A supervised ap-
proach to conventional metonymy. In Proceedings
of the 6th International Workshop on Computational
Semantics, Tilburg.
Geoffrey Nunberg. 1995. Transfers of meaning. Jour-
nal of semantics, 12(2):109–132.
B. S. Pedersen, S. Nimb, J. Asmussen, N. H. Sørensen,
L. Trap-Jensen, and H. Lorentzen. 2009. Dan-
net: the challenge of compiling a wordnet for danish
by reusing a monolingual dictionary. Language re-
sources and evaluation, 43(3):269–299.
Thierry Poibeau. 2006. Dealing with metonymic read-
ings of named entities. arXiv preprint cs/0607052.
J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and
O. Batiukova. 2009. Glml: Annotating argument
selection and coercion. In IWCS-8: Eighth Interna-
tional Conference on Computational Semantics.
J. Pustejovsky. 1995. The generative lexicon: a theory
of computational lexical semantics.
A. Rumshisky, VA Grinberg, and J. Pustejovsky. 2007.
Detecting selectional behavior of complex types in
text. In Fourth International Workshop on Genera-
tive Approaches to the Lexicon, Paris, France. Cite-
seer.
Ekaterina Shutova. 2009. Sense-based interpretation
of logical metonymy using a statistical method. In
Proceedings of the ACL-IJCNLP 2009 Student Re-
search Workshop, pages 1–9. Association for Com-
putational Linguistics.
R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng.
2008. Cheap and fast—but is it good?: evalu-
ating non-expert annotations for natural language
tasks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 254–263. Association for Computational Lin-
guistics.
Jorge Vivaldi. 2009. Corpus and exploitation tool:
Iulact and bwananet. In I International Confer-
ence on Corpus Linguistics (CICL 2009), A survey
on corpus-based research, Universidad de Murcia,
pages 224–239.
</reference>
<page confidence="0.997204">
730
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239965">
<title confidence="0.995009">Annotation of regular polysemy and underspecification</title>
<author confidence="0.795127">N´uria</author>
<affiliation confidence="0.972469">Universitat Pompeu</affiliation>
<address confidence="0.683461">Barcelona</address>
<email confidence="0.99961">nuria.bel@upf.edu</email>
<author confidence="0.921178">H´ector Martinez Bolette Sandford</author>
<affiliation confidence="0.998812">University of</affiliation>
<address confidence="0.52182">Copenhagen</address>
<email confidence="0.982405">alonso@hum.ku.dk,bsp@hum.ku.dk</email>
<abstract confidence="0.998906416666667">We present the result of an annotation task on regular polysemy for a series of semanclasses or types English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mette Skovgaard Andersen</author>
<author>Helle Asmussen</author>
<author>Jørg Asmussen</author>
</authors>
<title>The project of korpus</title>
<date>2002</date>
<booktitle>In The Tenth EURALEX International Congress: EURALEX</booktitle>
<contexts>
<context position="5021" citStr="Andersen et al., 2002" startWordPosition="788" endWordPosition="791">nition (Johannessen et al., 2005) and a previous area of research in metonymy resolution (Markert and Nissim, 2009). 4 Annotation Scheme For each of the nine (five for English, two for Danish, two for Spanish) dot types, we have randomly selected 500 corpus examples. Each example consists of a sentence with a selected headword belonging to the corresponding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical </context>
</contexts>
<marker>Andersen, Asmussen, Asmussen, 2002</marker>
<rawString>Mette Skovgaard Andersen, Helle Asmussen, and Jørg Asmussen. 2002. The project of korpus 2000 going public. In The Tenth EURALEX International Congress: EURALEX 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Apresjan</author>
</authors>
<title>Regular</title>
<date>1974</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="857" citStr="Apresjan, 1974" startWordPosition="121" endWordPosition="122">p@hum.ku.dk Abstract We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alter</context>
<context position="2797" citStr="Apresjan, 1974" startWordPosition="427" endWordPosition="428">nd Macleod, 2001) (ANC): a) Manuel died in exile in 1932 in England. b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). The third case refers to both. The ability of certain words to switch between semantic types in a predictable manner is referred to as regular polysemy. Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. Underspecified instances can be broken down in: 1. Contextually complex: England was, after all, an important wine market 2. Zeugmatic, in which two mutually exclusive readings are coordinated: England is conservative and rainy 3. Vague, in which no contextual element enforces a reading: The case of England is similar 3 Choice of semantic classes The Generative Lexicon (GL) (Pustejovsky, 1995) groups no</context>
</contexts>
<marker>Apresjan, 1974</marker>
<rawString>J. D. Apresjan. 1974. Regular polysemy. Linguistics. R. Artstein and M. Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Sabine Schulte im Walde</author>
<author>Toni Badia</author>
</authors>
<title>Modeling regular polysemy: A study on the semantic classification of catalan adjectives.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="996" citStr="Boleda et al., 2012" startWordPosition="141" endWordPosition="144">English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation</context>
</contexts>
<marker>Boleda, Walde, Badia, 2012</marker>
<rawString>Gemma Boleda, Sabine Schulte im Walde, and Toni Badia. 2012. Modeling regular polysemy: A study on the semantic classification of catalan adjectives. Computational Linguistics, 38(3):575–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
</authors>
<title>Ann Copestake, and Alex Lascarides.</title>
<date>1995</date>
<journal>Blocking. In Computational Lexical Semantics. Citeseer.</journal>
<marker>Briscoe, 1995</marker>
<rawString>Ted Briscoe, Ann Copestake, and Alex Lascarides. 1995. Blocking. In Computational Lexical Semantics. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonzalez-Agirre</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Multilingual central repository version 3.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>2525--2529</pages>
<contexts>
<context position="5377" citStr="Gonzalez-Agirre et al., 2012" startWordPosition="844" endWordPosition="847">responding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alternatives for the meat of several</context>
</contexts>
<marker>Gonzalez-Agirre, Laparra, Rigau, 2012</marker>
<rawString>Aitor Gonzalez-Agirre, Egoitz Laparra, and German Rigau. 2012. Multilingual central repository version 3.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012), pages 2525–2529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Ashish Vaswani</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning whom to trust with mace.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="12837" citStr="Hovy et al., 2013" startWordPosition="2076" endWordPosition="2079">nymic+underspecified) examples for the Location/Organization words that are common across languages. We can see that individual words show sense skewdness. This skewdness is a consequence of the kind of text in the corpus: e.g. America has a high proportion of non-literal senses in the ANC, where it usually means ”the population or government of the US”. Similarly, it is literal less than 50% of the times for the other two languages. In contrast, Afghanistan is most often used in its literal location sense. 6 Assigning senses with MACE Besides using majority voting with backoff , we use MACE (Hovy et al., 2013) to obtain the sense tag for each example. Dot type L M U D I eng:animeat 340 146 14 .048 3 eng:artinfo 170 180 150 .296 46 eng:contcont 295 176 28 .174 0 eng:locorg 291 193 16 .084 3 eng:procres 155 210 134 .272 33 da:contcont 223 134 143 .242 79 da:locorg 251 144 105 .206 53 spa:contcont 270 155 75 .074 56 spa:locorg 302 146 52 .038 40 Table 4: Sense distributions calculated with MACE, plus Difference and Intersection of underspecified senses between methods MACE is an unsupervised system that uses Expectation-Maximization (EM) to estimate the competence of annotators and recover the most li</context>
</contexts>
<marker>Hovy, Berg-Kirkpatrick, Vaswani, Hovy, 2013</marker>
<rawString>Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with mace. In Proceedings of NAACL-HLT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>C Macleod</author>
</authors>
<title>The american national corpus: A standardized resource of american english.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>274--280</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="2200" citStr="Ide and Macleod, 2001" startWordPosition="323" endWordPosition="326">ibe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the most likely sense from all the annotations. 1The corpus is freely available at http://metashare.cst.dk/repository/search/?q=regular+polysemy 2 Regular polysemy Very often a word that belongs to a semantic type, like Location, can behave as a member of another semantic type, like Organization, as shown by the following examples from the American National Corpus (Ide and Macleod, 2001) (ANC): a) Manuel died in exile in 1932 in England. b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). The third case refers to both. The ability of certain words to switch between semantic types in a predictable manner is referred to as regular polysemy. Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; L</context>
<context position="4997" citStr="Ide and Macleod, 2001" startWordPosition="784" endWordPosition="787"> for Named-Entity Recognition (Johannessen et al., 2005) and a previous area of research in metonymy resolution (Markert and Nissim, 2009). 4 Annotation Scheme For each of the nine (five for English, two for Danish, two for Spanish) dot types, we have randomly selected 500 corpus examples. Each example consists of a sentence with a selected headword belonging to the corresponding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequenc</context>
</contexts>
<marker>Ide, Macleod, 2001</marker>
<rawString>N. Ide and C. Macleod. 2001. The american national corpus: A standardized resource of american english. In Proceedings of Corpus Linguistics 2001, pages 274–280. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Johannessen</author>
<author>K Haagen</author>
<author>K Haaland</author>
<author>A B J´onsdottir</author>
<author>A Nøklestad</author>
<author>D Kokkinakis</author>
<author>P Meurer</author>
<author>E Bick</author>
<author>D Haltrup</author>
</authors>
<title>Named entity recognition for the mainland scandinavian languages. Literary and Linguistic Computing,</title>
<date>2005</date>
<marker>Johannessen, Haagen, Haaland, J´onsdottir, Nøklestad, Kokkinakis, Meurer, Bick, Haltrup, 2005</marker>
<rawString>J. B. Johannessen, K. Haagen, K. Haaland, A. B. J´onsdottir, A. Nøklestad, D. Kokkinakis, P. Meurer, E. Bick, and D. Haltrup. 2005. Named entity recognition for the mainland scandinavian languages. Literary and Linguistic Computing, 20(1):91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>A Lascarides</author>
</authors>
<title>A probabilistic account of logical metonymy.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="2827" citStr="Lapata and Lascarides, 2003" startWordPosition="429" endWordPosition="432">) (ANC): a) Manuel died in exile in 1932 in England. b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). The third case refers to both. The ability of certain words to switch between semantic types in a predictable manner is referred to as regular polysemy. Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. Underspecified instances can be broken down in: 1. Contextually complex: England was, after all, an important wine market 2. Zeugmatic, in which two mutually exclusive readings are coordinated: England is conservative and rainy 3. Vague, in which no contextual element enforces a reading: The case of England is similar 3 Choice of semantic classes The Generative Lexicon (GL) (Pustejovsky, 1995) groups nouns with their most frequent m</context>
</contexts>
<marker>Lapata, Lascarides, 2003</marker>
<rawString>M. Lapata and A. Lascarides. 2003. A probabilistic account of logical metonymy. Computational Linguistics, 29(2):261–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Towards a corpus annotated for metonymies: the case of location names.</title>
<date>2002</date>
<booktitle>In Proc. of LREC.</booktitle>
<publisher>Citeseer.</publisher>
<contexts>
<context position="7161" citStr="Markert and Nissim, 2002" startWordPosition="1140" endWordPosition="1143"> Each block of 500 sentences belonging to a dot type was an independent annotation subtask with an isolated description. The annotator was shown an example and had to determine whether the headword in the example had the literal, metonymic or the underspecified sense. Figure 1 shows an instance of the annotation process. Figure 1: Screen capture for a Mechanical Turk annotation instance or HIT This annotation scheme is designed with the intention of capturing literal, metonymic and underspecified senses, and we use an inventory of three possible answers, instead of using Markert and Nissim’s (Markert and Nissim, 2002; Nissim and Markert, 2005) approach with fine-grained sense distinctions, which are potentially more difficult to annotate and resolve automatically. Markert and Nissim acknowledge a mixed sense they define as being literal and metonymic at the same time. For English we used Amazon Mechanical Turk (AMT) with five annotations per example by turkers certified as Classification Masters. Using AMT provides annotations very quickly, possibly at the expense of reliability, but it has been proven suitable for sense-disambiguation task (Snow et al., 2008). Moreover, it is not possible to obtain annot</context>
</contexts>
<marker>Markert, Nissim, 2002</marker>
<rawString>K. Markert and M. Nissim. 2002. Towards a corpus annotated for metonymies: the case of location names. In Proc. of LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markert</author>
<author>M Nissim</author>
</authors>
<title>Data and models for metonymy resolution.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="4514" citStr="Markert and Nissim, 2009" startWordPosition="698" endWordPosition="701"> box was red” vs. ”I hate the whole box”. 4. Location/Organization: ”England is far” vs. ”England starts a tax reform”. 5. Process/Result: ”The building took months to finish” vs. ”the building is sturdy”. For Danish and Spanish, we have chosen Container/Content and Location/Organization. We chose the first one because we consider it the most prototypical case of metonymy from the ones listed in the GL. We chose the second one because the metonymies in locations are a common concern for Named-Entity Recognition (Johannessen et al., 2005) and a previous area of research in metonymy resolution (Markert and Nissim, 2009). 4 Annotation Scheme For each of the nine (five for English, two for Danish, two for Spanish) dot types, we have randomly selected 500 corpus examples. Each example consists of a sentence with a selected headword belonging to the corresponding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most </context>
</contexts>
<marker>Markert, Nissim, 2009</marker>
<rawString>K. Markert and M. Nissim. 2009. Data and models for metonymy resolution. Language Resources and Evaluation, 43(2):123–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Alex Judea</author>
<author>Katja Markert</author>
<author>Michael Strube</author>
</authors>
<title>Local and global context for supervised and unsupervised metonymy resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>183--193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19050" citStr="Nastase et al. (2012)" startWordPosition="3145" endWordPosition="3148">tonymic or underspecified reading, we have determined that this scheme can provide reliable (α over 0.60) annotations for one dot type. Not all the dot types are equally easy to annotate. The main source of variation in agreement, and thus annotation reliability, is the dot type itself. While eng:animeat and eng:locorg appear the easiest, eng:artinfo and eng:procres obtain very low α scores. 9 Further work After collecting annotated data, the natural next step is to attempt class-based word-sense disambiguation (WSD) to predict the senses in Tables 3 and 4 using a state-of-the-art system like Nastase et al. (2012). We will consider a sense-assignment method (voting or MACE) as more appropriate if it provides the sense tags that are easiest to learn by our WSD system. However, learnability is only one possible parameter for quality, and we also want to develop an expert-annotated gold standard to compare our data against. We also consider the possibility of developing a sense-assignment method that relies both on the theoretical assumption behind the voting scheme and the latent-variable approach used by MACE. Acknowledgments The research leading to these results has been funded by the European Commissi</context>
</contexts>
<marker>Nastase, Judea, Markert, Strube, 2012</marker>
<rawString>Vivi Nastase, Alex Judea, Katja Markert, and Michael Strube. 2012. Local and global context for supervised and unsupervised metonymy resolution. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 183–193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malvina Nissim</author>
<author>Katja Markert</author>
</authors>
<title>Learning to buy a renault and talk to bmw: A supervised approach to conventional metonymy.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Workshop on Computational Semantics,</booktitle>
<location>Tilburg.</location>
<contexts>
<context position="7188" citStr="Nissim and Markert, 2005" startWordPosition="1144" endWordPosition="1147">es belonging to a dot type was an independent annotation subtask with an isolated description. The annotator was shown an example and had to determine whether the headword in the example had the literal, metonymic or the underspecified sense. Figure 1 shows an instance of the annotation process. Figure 1: Screen capture for a Mechanical Turk annotation instance or HIT This annotation scheme is designed with the intention of capturing literal, metonymic and underspecified senses, and we use an inventory of three possible answers, instead of using Markert and Nissim’s (Markert and Nissim, 2002; Nissim and Markert, 2005) approach with fine-grained sense distinctions, which are potentially more difficult to annotate and resolve automatically. Markert and Nissim acknowledge a mixed sense they define as being literal and metonymic at the same time. For English we used Amazon Mechanical Turk (AMT) with five annotations per example by turkers certified as Classification Masters. Using AMT provides annotations very quickly, possibly at the expense of reliability, but it has been proven suitable for sense-disambiguation task (Snow et al., 2008). Moreover, it is not possible to obtain annotations for every language u</context>
</contexts>
<marker>Nissim, Markert, 2005</marker>
<rawString>Malvina Nissim and Katja Markert. 2005. Learning to buy a renault and talk to bmw: A supervised approach to conventional metonymy. In Proceedings of the 6th International Workshop on Computational Semantics, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Nunberg</author>
</authors>
<title>Transfers of meaning.</title>
<date>1995</date>
<journal>Journal of semantics,</journal>
<pages>12--2</pages>
<contexts>
<context position="914" citStr="Nunberg, 1995" startWordPosition="130" endWordPosition="131">on task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification</context>
</contexts>
<marker>Nunberg, 1995</marker>
<rawString>Geoffrey Nunberg. 1995. Transfers of meaning. Journal of semantics, 12(2):109–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B S Pedersen</author>
<author>S Nimb</author>
<author>J Asmussen</author>
<author>N H Sørensen</author>
<author>L Trap-Jensen</author>
<author>H Lorentzen</author>
</authors>
<title>Dannet: the challenge of compiling a wordnet for danish by reusing a monolingual dictionary. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="5346" citStr="Pedersen et al., 2009" startWordPosition="840" endWordPosition="843">rd belonging to the corresponding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alter</context>
</contexts>
<marker>Pedersen, Nimb, Asmussen, Sørensen, Trap-Jensen, Lorentzen, 2009</marker>
<rawString>B. S. Pedersen, S. Nimb, J. Asmussen, N. H. Sørensen, L. Trap-Jensen, and H. Lorentzen. 2009. Dannet: the challenge of compiling a wordnet for danish by reusing a monolingual dictionary. Language resources and evaluation, 43(3):269–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
</authors>
<title>Dealing with metonymic readings of named entities. arXiv preprint cs/0607052.</title>
<date>2006</date>
<contexts>
<context position="1213" citStr="Poibeau, 2006" startWordPosition="176" endWordPosition="177">koff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the</context>
</contexts>
<marker>Poibeau, 2006</marker>
<rawString>Thierry Poibeau. 2006. Dealing with metonymic readings of named entities. arXiv preprint cs/0607052.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>A Rumshisky</author>
<author>J Moszkowicz</author>
<author>O Batiukova</author>
</authors>
<title>Glml: Annotating argument selection and coercion.</title>
<date>2009</date>
<booktitle>In IWCS-8: Eighth International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="1240" citStr="Pustejovsky et al., 2009" startWordPosition="178" endWordPosition="181">and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the most likely sense from all</context>
</contexts>
<marker>Pustejovsky, Rumshisky, Moszkowicz, Batiukova, 2009</marker>
<rawString>J. Pustejovsky, A. Rumshisky, J. Moszkowicz, and O. Batiukova. 2009. Glml: Annotating argument selection and coercion. In IWCS-8: Eighth International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The generative lexicon: a theory of computational lexical semantics.</title>
<date>1995</date>
<contexts>
<context position="876" citStr="Pustejovsky, 1995" startWordPosition="123" endWordPosition="125">ract We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between sens</context>
<context position="3387" citStr="Pustejovsky, 1995" startWordPosition="523" endWordPosition="524"> by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. Underspecified instances can be broken down in: 1. Contextually complex: England was, after all, an important wine market 2. Zeugmatic, in which two mutually exclusive readings are coordinated: England is conservative and rainy 3. Vague, in which no contextual element enforces a reading: The case of England is similar 3 Choice of semantic classes The Generative Lexicon (GL) (Pustejovsky, 1995) groups nouns with their most frequent metonymic sense in a semantic class called a dot type. For English, we annotate 5 dot types from the GL: 1. Animal/Meat: ”The chicken ran away” vs. 725 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 725–730, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ”the chicken was delicious”. 2. Artifact/Information : ”The book fell” vs. ”the book was boring”. 3. Container/Content: ”The box was red” vs. ”I hate the whole box”. 4. Location/Organization: ”England is far” vs. ”England </context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>J. Pustejovsky. 1995. The generative lexicon: a theory of computational lexical semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rumshisky</author>
<author>VA Grinberg</author>
<author>J Pustejovsky</author>
</authors>
<title>Detecting selectional behavior of complex types in text.</title>
<date>2007</date>
<booktitle>In Fourth International Workshop on Generative Approaches to the Lexicon,</booktitle>
<location>Paris, France. Citeseer.</location>
<contexts>
<context position="1020" citStr="Rumshisky et al., 2007" startWordPosition="145" endWordPosition="148">panish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in</context>
</contexts>
<marker>Rumshisky, Grinberg, Pustejovsky, 2007</marker>
<rawString>A. Rumshisky, VA Grinberg, and J. Pustejovsky. 2007. Detecting selectional behavior of complex types in text. In Fourth International Workshop on Generative Approaches to the Lexicon, Paris, France. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
</authors>
<title>Sense-based interpretation of logical metonymy using a statistical method.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1036" citStr="Shutova, 2009" startWordPosition="149" endWordPosition="150">cribes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-</context>
</contexts>
<marker>Shutova, 2009</marker>
<rawString>Ekaterina Shutova. 2009. Sense-based interpretation of logical metonymy using a statistical method. In Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 254–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Vivaldi</author>
</authors>
<title>Corpus and exploitation tool: Iulact and bwananet.</title>
<date>2009</date>
<booktitle>In I International Conference on Corpus Linguistics (CICL</booktitle>
<pages>224--239</pages>
<contexts>
<context position="5103" citStr="Vivaldi, 2009" startWordPosition="803" endWordPosition="804">Markert and Nissim, 2009). 4 Annotation Scheme For each of the nine (five for English, two for Danish, two for Spanish) dot types, we have randomly selected 500 corpus examples. Each example consists of a sentence with a selected headword belonging to the corresponding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid i</context>
</contexts>
<marker>Vivaldi, 2009</marker>
<rawString>Jorge Vivaldi. 2009. Corpus and exploitation tool: Iulact and bwananet. In I International Conference on Corpus Linguistics (CICL 2009), A survey on corpus-based research, Universidad de Murcia, pages 224–239.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>