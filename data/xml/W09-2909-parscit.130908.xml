<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.990123">
Abbreviation Generation for Japanese Multi-Word Expressions
</title>
<author confidence="0.753037">
Hiromi Wakakit Hiroko Fujiit Masaru Suzukit
Mika Fukuit Kazuo Sumitat
</author>
<affiliation confidence="0.965733">
tToshiba Corporation
</affiliation>
<address confidence="0.759248">
1 Komukai-Toshiba, Saiwai-ku, Kawasaki, 212-8582, Japan
{hiromi.wakaki, hiroko.fujii, masarul.suzuki,
</address>
<email confidence="0.988441">
mika.fukui, kazuo.sumita}@toshiba.co.jp
</email>
<sectionHeader confidence="0.979979" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988222222222">
This paper proposes a novel method for
generating Japanese abbreviations from
their full forms with the Log-Linear Model
(LLM) in order to take advantage of char-
acteristic patterns of Japanese abbrevia-
tion. Our experimental results show that
the method is effective for TV program
titles that contain colloquial expressions.
The proposed method achieved 78.8% re-
call for the top 30 candidates, whereas a
baseline method using Conditional Ran-
dom Fields (CRFs) achieved 68.3% re-
call. Moreover, from the results of experi-
ments using six data sets classified accord-
ing to types of character and semantic cat-
egories, we show that each performance
of the above two methods depends on the
types of the full forms.
</bodyText>
<sectionHeader confidence="0.996269" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897">
Much research has been done on abbreviation ex-
traction to detect terms having the same mean-
ing. However, most previous studies (Hisamitsu
and Niwa, 2001; Park and Byrd, 2001; Schwart2
and Hearst, 2003; Adar, 2004; Sakai and Ma-
suyama, 2005; Nadeau and Turney, 2005; Oka2aki
and Ananiadou, 2006; Oka2aki et al., 2008(1);
Oka2aki et al., 2008(2)) aimed at extracting abbre-
viations of organi2ation names and technical terms
from well-written documents such as news articles
and techincal papers.
Many Japanese terms indicating individual TV
programs, songs, comics, novels, and so on, are
multi-word expressions and have the character-
istics distinct from terms treated in most previ-
ous studies on abbreviation extraction. These
terms can take several grammatical forms: a noun
phrase, a sentence fragment, and even a sentence.
Also, many of these expressions contain a variety
of types of characters: kanji, hiragana, katakana,
alphabet, digit, and symbol, and some of them
contain colloquial expressions&apos;. Abbreviations of
these expressions are often used in colloquial text
such as chat or blog, and spoken sentences. To
treat an abbreviation as a term having the same
meaning as the original expression for NLP appli-
cations such as keyboard-based and speech-based
information retrieval, an abbreviation generation
method effective for this type of multi-word ex-
pressions is needed. However, it is not easy to
ascertain abbreviations associated with their full
forms. This is because although these terms be-
come widely used in speech, they do not appear
in well-written documents, such as newspaper ar-
ticles or research papers, in which the abbrevi-
ations are clearly defined for use in the subse-
quent texts with certain lexical patterns, such as
parenthesis. Therefore this paper describes an ap-
proach to generate abbreviation candidates from
an original term and to rank them according to
their probabilities of abbreviation. We assume that
top-ranked abbreviations will be narrowed down
by using Web search results in the future.
</bodyText>
<sectionHeader confidence="0.8114865" genericHeader="method">
2 Japanese Abbreviation
2.1 Data Sets
</sectionHeader>
<bodyText confidence="0.9998614">
Transformations into abbreviations are strongly
dependent on languages. For instance, the term &amp;quot;
77;1)—LIZ�7;/ (family restaurant)&amp;quot; is ab-
breviated as &amp;quot;77;LIZ (famires)&amp;quot; in Japanese,
whereas English speakers do not abbreviate it in
the same way as Japanese do. To investigate
Japanese abbreviations, we collected them from
different perspectives, that is, types of character
and semantic categories. Table 1 shows abbrevia-
tion data types, their word counts, and so on. Ex-
</bodyText>
<footnote confidence="0.847058">
&apos;TV program titles contain colloquial expressions such as
slang, pun, coined words, and dialect. For example, in well-
written documents, we do not see such a expression as &amp;quot;I&apos;m
Not An Errand Boy!&amp;quot; showed in Figure 2.
</footnote>
<page confidence="0.985223">
63
</page>
<note confidence="0.9943075">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 63–70,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999863446808511">
amples are given in Figure 2 at the end of this pa-
per.
We extracted abbreviations listed and described
on the Japanese Wikipedia site 2, which is a multi-
lingual project to create a complete and accurate
open content encyclopedia. First, we collected
lists of abbreviations classified according to types
of Japanese character. Japanese has three origi-
nal types of character: kanji, katakana, and hira-
gana. Other types of character are used, such as
alphabets, numbers, and symbols. However, hira-
gana is mainly used with kanji, and numbers and
symbols are used with other characters. There-
fore, we used three abbreviation lists classified ac-
cording to alphabetical words 3, katakana words
4, and kanji words with hiraganas(Figure 2) on
Wikipedia. We extracted pairs of abbreviations
and their full forms from each list and obtained
928, 245, and 399 abbreviations, respectively.
Also, we extracted pairs of university names
and their abbreviations from a list of university
abbreviations on Wikipedia 6. In Japanese, many
names of organizations have a noun phrase struc-
ture combining several nouns, such as names of
places (&amp;quot;日本 (Japan)&amp;quot;, &amp;quot;東京 (Tokyo)&amp;quot;), names
of fields (&amp;quot;医科 (medical)&amp;quot;, &amp;quot;科学 (science)&amp;quot;), for
whom (&amp;quot;女子 (female)&amp;quot;), and the type of organiza-
tion(&amp;quot;大学 (university)&amp;quot;, &amp;quot;研究所&amp;quot;(research labo-
ratory)). Therefore, we used names of universities
and extracted 523 abbreviations. Almost all of the
nouns are kanji.
Additionally, we extracted abbreviations of TV
program titles from descriptions on each page of
Wikipedia. This is because many TV program ti-
tles contain various types of characters or collo-
quial expressions different from the others we ex-
tracted. However, there are no lists of TV pro-
gram titles in Wikipedia. Therefore, we gathered
TV program titles satisfying the following crite-
rion: the first sentence of the description of the
Wikipedia page of the TV program title indicates
that the page is about the TV program. And, in the
same paragraph, if abbreviations are introduced by
using key phrases such as &amp;quot;略語は A&amp;quot;(it means &amp;quot;it
is abbreviated as A&amp;quot;), we extracted bold or paren-
thetical words in the key phrases. There were 326
abbreviations.
</bodyText>
<footnote confidence="0.9987336">
2http://ja.wikipedia.org/wiki/
3http://ja.wikipedia.org/wiki/欧文略語一覧
4http://ja.wikipedia.org/wiki/カタカナ略語一覧
5http://ja.wikipedia.org/wiki/漢字略語一覧
6http://ja.wikipedia.org/wiki/大学の略称
</footnote>
<bodyText confidence="0.9998944">
Finally, we gathered abbreviations of TV pro-
gram titles in TV schedules written in short form
because of space limitations. In this process, we
used program titles in TV schedules in newspa-
pers as short forms and EPG 7 data as long forms.
When a title in the schedule is written with short
form of the title with the same date, time, and
channel as EPG data, we recognized that it is an
abbreviation and the other is its full form. We ex-
tracted 603 abbreviations.
</bodyText>
<subsectionHeader confidence="0.996458">
2.2 Characteristics
</subsectionHeader>
<bodyText confidence="0.998879428571429">
In this paper, we focus on abbreviations that lack
some characters compared with the full forms.
The followings are well-known characteristics of
Japanese abbreviations (Sakai and Masuyama,
2005; Enoki et al., 2007; Murayama and Oku-
mura, 2008).Abbreviations are created according
to rules: (1) retain the beginning of a word and
omit the rest (truncation); (2) divide an original
term into base words, retaining several substrings
from some of them, and combine them (contrac-
tion). In particular, four-mora$ katakana abbrevi-
ations are often created by combining two-mora
as in the case of the katakana words in Figure 2.
Also, the length of an abbreviation in kanji tends
to be two or three letters as in the case of the
kanji words in Figure 2. Moreover, if an original
term consists of katakana with the specific char-
acters such as sokuon 9 and chonio in the middle,
these characters tend to be dropped in abbrevia-
tions. The second and third of katakana terms in
Figure 2 are an example of this.
</bodyText>
<sectionHeader confidence="0.989016" genericHeader="method">
3 Proposed Method
</sectionHeader>
<bodyText confidence="0.9997758">
In this section, we propose a new method to gen-
erate Japanese abbreviations by using the Log-
Linear Model to rank abbreviation candidates. As
mentioned in Section 2.2, Japanese abbreviation
characteristics are evident in the composition of
abbreviations, not in generation rules from their
full forms. Therefore, we first generate possi-
ble abbreviations from an original term and rank
them in descending order of probability of abbre-
viations. Our method uses a three-step process as
</bodyText>
<footnote confidence="0.44324425">
7EPG (Electronic Program Guide) broadcast on some
multiplexes that provide detailed information about programs
in an upcoming week on some stations.
&amp;quot;The minimal unit of a syllable.
</footnote>
<bodyText confidence="0.799715333333333">
Sokuon is written as &amp;quot;ッ&amp;quot; in katakana and &amp;quot;っ&amp;quot; in hira-
gana to show a geminate consonant
10Ch5n&apos; is written as &amp;quot;―&amp;quot; to show a long vowel.
</bodyText>
<page confidence="0.998068">
64
</page>
<table confidence="0.999122666666667">
Class Type NT Average number of characters SC(%)
(#)
Av SD (a) (b) (c) (d) (e) (f)
Alpha. 92 8 23.5 9.0 0.0 0.0 0.0 21.4 0.0 2.1 100.0
Character Kata. 245 8. 8 3.1 0.4 8.0 0.3 0.0 0.0 0.2 79.2
type Kanji 399 6.3 3. 8 5.9 0.3 0.2 0.0 0.0 0.0 91.0
Univ. 523 6.0 1.4 6.0 0.0 0.0 0.0 0.0 0.0 9 8.1
Semantic TV1 326 10.5 5.6 3.1 3.6 2.1 1.2 0.3 0.3 2 8.8
category TV2 603 10.9 4.2 1.6 4.5 2.6 1.7 0.1 0.4 19.1
</table>
<tableCaption confidence="0.999558">
Table 1: Abbreviation data sets, their types and number of terms(NT), average number of characters
</tableCaption>
<bodyText confidence="0.9731246">
with standard deviation(SD),average number of characters per term in each type of character( (a)kanji,
(b)katakana,(c)hiragana,(d)alphabet,(e)number,(f)space), and proportion of terms with a single
type of character (SC).
follows: 1)base word division, 2)candidate gener-
ation, and 3)ranking abbreviations.
</bodyText>
<subsectionHeader confidence="0.989571">
3.1 Step1: Base Word Division
</subsectionHeader>
<bodyText confidence="0.999830454545454">
In this step, we divide terms into base words for
abbreviations because Japanese is an agglutinative
language. In order to deal with neologisms and
colloquial expressions, we divide terms by using
web search results instead of morphological ana-
lyzer.
When a term t is divided into two substrings af-
ter the ith charcter t, we denote the anterior half
by si,ant and the posterior half by si,post. A link
strength D(ti) between si,ant and si,post is defined
as follows:
</bodyText>
<equation confidence="0.993556333333333">
hit(s)
D(ti) =
min(hit(sz,ant),hit(sz,,_t))
</equation>
<bodyText confidence="0.96877985">
Note that hit(t) is calculated as the number of
search results by using the term t in double quotes
as one query on the Web&amp;quot;. The formulation of
D(ti) is mostly the same as Simpson&apos;s Coefficient
except that the numerator is modified. We divide
the term t after the hth character where D(tk) is
the smallest and repeat this process by using sub-
strings divided in the previous operation as new tis
recursively. We heuristically set the stopping con-
ditions as two kanji characters or four characters
of other types. This dividing process works well
because a set of words containing a term is styl-
ized expression that is different from a sentence.
For example, suppose that a term t is &amp;quot;VivaVi-
vaV6&amp;quot;, which is one of the TV program titles. All
divisions into two of the term are &amp;quot;V/ivaVivaV6&amp;quot;,
&amp;quot;Vi/vaVivaV6&amp;quot;, • • •, and &amp;quot;VivaVivaV/6&amp;quot;. Here,
the symbol &amp;quot;/&amp;quot;&amp;quot; indicates a division point. Then,
11. We used Yahoo web search API.
http://developer.yahoo.co.jp/search/web/�l/webSearch.html
</bodyText>
<equation confidence="0.948660333333333">
D(ti)s are calculated as follows:
hit(&amp;quot;VivaVivaV6&amp;quot;)
D(ti) = min(hit(&amp;quot;V&amp;quot;), hit(&amp;quot;ivaVivaV6&amp;quot;))
...
hit(&amp;quot;VivaVivaV6&amp;quot;)
D(tg) = min(hit(&amp;quot;VivaVivaV&amp;quot;), hit(&amp;quot;6&amp;quot;))
</equation>
<bodyText confidence="0.988037052631579">
When D(ts) is the smallest of all D(ti), &amp;quot;VivaVi-
vaV6&amp;quot; is divided into &amp;quot;VivaViva&amp;quot; and &amp;quot;V6&amp;quot;. The
length of &amp;quot;V6&amp;quot; is two and is satisfied with the stop-
ping conditions. Then, we continue to calculate
D(ti) of &amp;quot;VivaViva&amp;quot; because the length of the sub-
string is not satisfied with the stopping conditions.
Finally, the divisions are fixed based on the fol-
lowing modifications. If a division is just before
the sokuon or the chon, we eliminate the division
because these cannot appear at the beginning of a
word. Also, if the division is just before &amp;quot;O&amp;quot;(no),
which is a hiragana character and one of the par-
ticles used to indicate possession and so on, we
insert a division after the &amp;quot;O&amp;quot;(no) to make it one
word because of the stopping conditions. Addi-
tionally, we combine some segments to form one
word when there is a word in a transliteration dic-
tionary of katakana corresponding to an English
word.
</bodyText>
<subsectionHeader confidence="0.995263">
3.2 Step2:Candidate Generation
</subsectionHeader>
<bodyText confidence="0.9990172">
In this step, we generate abbreviation candidates
by applying the following simple rules to all words
containing a certain term. These rules are based
on the Japanese abbreviation characteristics de-
scribed in Section 2.2.
</bodyText>
<listItem confidence="0.965517666666667">
1) Do not use this word
2) Use this word in full
3) Use the first character of this word
</listItem>
<page confidence="0.91403">
65
</page>
<listItem confidence="0.97584625">
4) Use the first two characters of this word
5) Use the first three characters of this word
6) Drop sokuon and chon, and do 4)
7) Drop sokuon and chon, and do 5)
</listItem>
<bodyText confidence="0.999776833333333">
All rules are applied to all words divided by the
process in step 1 .For example, in the case of
&amp;quot;Viva/Viva/V6&amp;quot;, all rules are used for &amp;quot;Viva&amp;quot;,
&amp;quot;Viva&amp;quot;, and &amp;quot;V6&amp;quot; .Then, if 3), 3), 2) are used for
each base word, we get a candidate &amp;quot;VVV6&amp;quot; .With
the rules, we can get all candidates combining sub-
strings at the beginning of each word because we
used the stop conditions of character length of less
than four in step 1 .However, note that we use
mora instead of character in the case of phono-
graphic characters .Also, we eliminate duplicative
candidates.
</bodyText>
<subsectionHeader confidence="0.991147">
3.3 Step3: Ranking Abbreviations
</subsectionHeader>
<bodyText confidence="0.9998785">
LLM is a probabilistic model widely used as a
maximum entropy model for many NLP tasks
(Manning and Schutze, 1999) .We use standard
LLM to rank the abbreviations.
Consider a set of observations x for each sam-
ple of an object or event with y .Log-Linear Model
gives a probability p(yIx; A) of an event by repre-
senting an event y as features fj(xl, yk).
</bodyText>
<equation confidence="0.9972355">
1
p(y�x; A) = Z(x, A) exp(EjAjfj(x, y)) (1)
</equation>
<bodyText confidence="0.98855375">
Here, Aj(j = 1, ..., M) or aj is a model pa-
rameter, and it represents the weight of a feature
fj(xl, yk) .Also, regularization term Z(x, A) is
calculated as follows:
</bodyText>
<equation confidence="0.744829">
Z(x, A) = EY Ey(x) exp(EjAjfj(x, y,))
</equation>
<bodyText confidence="0.98654725">
Note that Y (x) represents a set of output y corre-
sponding to x .The numerator of the Formula (1)
is the same as the following by replacing eAi as
aj.
</bodyText>
<equation confidence="0.9952345">
s(x, y, A) = exp(EjAjfj(x, y))
= a1�(x&gt;Y)a2z(x&gt;Y) ...am (x&gt;Y)(2)
</equation>
<bodyText confidence="0.9980923">
We formalize the abbreviation generation task
as a ranking problem in which the probability
p(yIx; A) of abbreviation y in a given set Y(x) of
abbreviation candidates is modeled when its full
form x is observed .For example, assume that
you assign a full form &amp;quot;VivavivaV6&amp;quot; to x .The
set Y (x) contains abbreviation candidates gener-
ated from the full form in Step2 such as &amp;quot;VVV6&amp;quot;,
&amp;quot;VivaV&amp;quot;, &amp;quot;ViVi&amp;quot;, and so on .We used Amis imple-
mentation 12 for Log-Linear Model.
</bodyText>
<sectionHeader confidence="0.953789" genericHeader="method">
1) Features
</sectionHeader>
<bodyText confidence="0.960643285714286">
We use the features below for the Japanese ab-
breviation characteristics with letter length and
so on as mentioned in the Section 2 .2 .We de-
note a substring of a ith base word containing an
abbreviation candidate by subi (i = 1, • • • , m),
where m is the total number of base words .Then,
let ch(subi) denote letter type of character of
subi, and let len(subi) denote length of subi.
Additionally, let sum(len(subi),1, m) denote a
summation of len(subi)(i = 1, • • • , m), and let
com((fi(i), f2(i)), 1, m) denote a combination of
a feature f,(i) and a feature f2(i) from i = 1 to
i = m .Here, we show all categories of features
we used as follows:.
</bodyText>
<listItem confidence="0.999790666666667">
• tp = com((ch(subi, len(subi))), 1, m)
• tl = com((ch(subi)), 1, m)
• e = com((len(subi)), 1, m)
• w = subi (i=1,•••,m)
• ab = sum(len(subi), 1, m)
• enum = m
</listItem>
<bodyText confidence="0.99933408">
A substring of a ith base word is generated by
applying one of the rules from 2) to 7) in Step 2.
However, when an abbreviation candidate corre-
sponds to one substring of its full form, we set its
base word to the candidate itself even if the candi-
date was generated by combining some substrings.
Table 2 shows features for &amp;quot;VVV6&amp;quot; whose orig-
inal term is &amp;quot;VivaVivaV6&amp;quot; .Its base words subi
are &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, and &amp;quot;V6&amp;quot; because of the division
as &amp;quot;V/V/V6&amp;quot; .When i = 1, ch(subj is equal
to ALPHA, that is, an alphabetical character, and
len(subt) is equal to 1 .Therefore, for &amp;quot;VVV6&amp;quot;,
a feature in a category tp is generated by combi-
nation of the ch(subi) and len(subi) from i = 1
to i = m, that is, 1ALPHA 1ALPHA 2ALPHA.
Other features are also generated by calculating in
the same way as tp.
We cannot list all possible features because they
depend on compositions of abbreviation candi-
dates .Therefore, we prepare a zero feature for
each category.If features do not appear in positive
examples in a training data set, we assign them
to zero features .For example, because a feature
&amp;quot;1KANJI 5KANJI&amp;quot; in category &amp;quot;tp&amp;quot; does not ap-
pear in positive examples of a training set, we use
</bodyText>
<footnote confidence="0.843919">
12http://www-tsujii.is.s.u-tokyo.ae.jp/amis/
</footnote>
<page confidence="0.97601">
66
</page>
<table confidence="0.951442125">
Category Feature
tp 1ALPHA 1ALPHA 2ALPHA
tl ALPHA ALPHA ALPHA
e 1 1 2
w V, V, V6
ab ab4
enum enum3
Table 2: Features for abbreviation &amp;quot;V/V/V6&amp;quot;
</table>
<bodyText confidence="0.9608066">
whose full form is &amp;quot;Viva/Viva/V6&amp;quot;.
&amp;quot;tp0&amp;quot; as an alternative feature. However, w0 is as-
signed when any features in category &amp;quot;w&amp;quot; do not
appear in them.
We assign 11 to a set of all features that appear
in positive examples in a training data set, such
as 1ALPHA 1ALPHA 2ALPHA, 1 1 2, V, V, V6,
ab4. We also assign 1o to a set of zero features, i.e.
tp0, tl0, e0, w0, ab0, enum0. Then, let L denote a
set merged 11 and 1o.
</bodyText>
<listItem confidence="0.47012">
2) Training and Test
</listItem>
<bodyText confidence="0.999862">
First, we obtain the above-mentioned feature set L
with a training data set. Next, these features are
assigned to all abbreviation candidates generated
from the training data set in step 2. Then, a param-
eter aj (j = 1, • • • , ILI) of the Log-Linear Model
is calculated by using Amis. Finally, the probabil-
ities of all abbreviation candidates generated from
a test data in step 2 are calculated by the Formula
(2).
</bodyText>
<sectionHeader confidence="0.998478" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.928805">
4.1 Baseline Method
</subsectionHeader>
<bodyText confidence="0.996614263157895">
CRFs (Lafferty et al., 2001) are Log-Linear Mod-
els, which are often used for the labeling or pars-
ing of sequential data and are widely applied
for many NLP tasks. Some researchers already
used CRFs for abbreviation extraction (Okazaki et
al., 2008(1)) or generation (Saikou et al., 2008).
Therefore, we evaluate a method using CRFs as a
baseline.
We formalize the abbreviation generation task
as a sequence labeling problem in which each let-
ter contained in an original term is to be used in its
abbreviation13(Fig. 1). We also designed features
attached to each character: morpheme word con-
taining the letter, reading of the morpheme word,
13In (Saikou et al., 2008), they formalized the abbreviation
generation task as a sequence labeling problem in which each
mora contained in a term is to be used in its abbreviation.
To avoid reading estimation, we generate abbreviations by
abbreviating their original characters.
</bodyText>
<table confidence="0.9807675">
Label Features
- word reading POS Head of word
ᮅ ࠐ ᮅ ࡕࡻ࣮ Noun 1 ࣭࣭࣭
ࡣ  ࡣ ࢃ Particle 1 ࣭࣭࣭
ࣅ ࠐ ࣅࢱ࣑ࣥ ࡧࡓࡳࢇ Noun 1 ࣭࣭࣭
ࢱ ࠐ ࣅࢱ࣑ࣥ ࡧࡓࡳࢇ Noun 0 ࣭࣭࣭
࣑  ࣅࢱ࣑ࣥ ࡧࡓࡳࢇ Noun 0 ࣭࣭࣭
ࣥ  ࣅࢱ࣑ࣥ ࡧࡓࡳࢇ Noun 0 ࣭࣭࣭
</table>
<figureCaption confidence="0.975918666666667">
Figure 1: Feature examples of CRFs and values
for the abbreviation &amp;quot;MI (asabita)&amp;quot; whose for-
mal form is &amp;quot; J:I ;;/ (asa wa bitamin)&amp;quot;.
</figureCaption>
<bodyText confidence="0.9965556">
type of character, the first character or not in the
morpheme word, the first character or not in the
segment, and so on. We used MeCab14 as a mor-
phological analysis and CRF++ implementation 15
for CRFs.
</bodyText>
<subsectionHeader confidence="0.495181">
4.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999609933333333">
We evaluate recall in the top 1, 5, 10, 30, and 50
abbreviation candidates generated with both the
proposed method and the baseline method on the
six data sets. The performance is measured under
a ten-fold cross-validation where the parameters
are fine-tuned in the top 30 in the training proce-
dure.
Table 3 shows recall with the baseline method.
Table 4 shows recall, and the bottom row in the
table shows differences between recall with CRFs
and that with proposed method in the top 30.
In the top 30, recall in Table 3 of alphabeti-
cal words, names of universities, and kanji words
are 99.1%, 97.9%, and 92.5% respectively. From
the point of view of types of character, most of
these are composed of a single type of character
as shown in column SC of Table 1. In contrast,
recall in Table 3 of TV program titles 1 and 2 are
68.3% and 80.9% respectively. These results are
much lower than the others. As a result of apply-
ing our method, Table 4 showed that recall of TV
program titles improved 10.5% compared with the
baseline method. This is because the method using
CRFs cannot use the features of generated abbre-
viations since it is an approach to decide whether
each character of an original form is to be used
in its abbreviation. It seems that this leads to the
disadvantages of generating abbreviations of TV
program titles containing various types of char-
acter and colloquial expressions. However, there
</bodyText>
<footnote confidence="0.9996105">
14http://mecab.sourceforge.net/
15http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.997791">
67
</page>
<table confidence="0.9998775">
Recall@n Alphabet Katakana Kanji Univ. TV1 TV2
1 89.1% 29.4% 47.9% 19.9% 11.1% 9.3%
5 97.0% 67.3% 71.7% 80.9% 37.5% 45.8%
10 98.4% 77.1% 81.5% 92.9% 48.6% 62.5%
30 99.1% 89.0% 92.5% 97.9% 68.3% 80.9%
50 99.4% 93.9% 94.7% 98.9% 73.8% 86.9%
</table>
<tableCaption confidence="0.998377">
Table 3: Recall in the top 1, 5, 10, 30, and 50 abbreviation candidate s generated with CRFs.
</tableCaption>
<table confidence="0.999944125">
Recall@n Alphabet Katakana Kanji Univ. TV1 TV2
1 87.4% 36.3% 39.1% 33.5% 19.9% 20.2%
5 92.2% 66.5% 65.2% 71.5% 48.2% 42.8%
10 93.0% 81.6% 73.9% 84.9% 61.3% 59.2%
30 94.1% 91.0% 85.2% 92.5% 78.8% 81.1%
50 94.4% 92.7% 86.7% 93.3% 85.3% 85.4%
all 95.6% 94.7% 90.4% 94.8% 93.9% 90.3%
Difference s (Recall@30) -5.1% +2.0% -7.3% -5.4% +10.5% +0.2%
</table>
<tableCaption confidence="0.9227835">
Table 4: Recall in the top 1, 5, 10, 30, and 50 abbreviation candidate s generated with the proposed
method, and difference s between the recall with CRFsand that with the proposed method in the top 30.
</tableCaption>
<bodyText confidence="0.998995027027027">
i s little difference of recall between the baseline
and the proposed method for the TV program ti-
tle s 2. Thi s i s becau se most of the TV program ti-
tle s 2 were systematically created by simple rules
such as getting the initial several letters that satisfy
space limitations.
On the other hand, recall of the proposed
method for alphabetical words, kanji words, and
name sof universitie s was -5.1%, -7.3%, -5.4%
lower, respectively, than in the case of using the
baseline method. Thi s i s becau se some abbrevi-
ations could not be generated by the given gen-
eration rule sand, ascan be seen in Table 4, re-
call of these data set speaks. From these results,
we conclude that the baseline method i s suited to
a term containing a single type of character such
as alphabetical word sand kanji words, whereas
the proposed method i s suited to a term contain-
ing multiple type sof character.
When we used the division in step 2 as an al-
ternative to MeCab, recall with CRFs differed ap-
proximately le ss than ±1% from recall in Table 3.
On the other hand, when we used MeCab as an al-
ternative to the division in step 2, recall with the
proposed method was significantly lower than in
Table 4.
We cannot compare our performance directly
with the previou s work because of the differences
in data sets. For reference, Murayama et al. (2006)
reported 68.4% recall in the top 30 with the Noisy-
Channel Model. They used 851 abbreviations
corresponding to 748 full form s extracted from
Wikipedia. Saikou et al. (2008) reported 72.5%
recall in the top 30 with CRFs. They used 51 ab-
breviations collected by WoZ16 as te st data and
781 abbreviations that appeared in Wikipedia as
training data.
</bodyText>
<subsectionHeader confidence="0.999242">
4.3 Combination of two methods
</subsectionHeader>
<bodyText confidence="0.968485842105263">
Table 4 show s that the baseline method i s better for
the alphabetical words, name sof universities, and
kanji words, whereas the proposed method i s bet-
ter for others. However the classification on Table
1 i s made by hand. Here, we automatically clas-
sified them into the following case A and B based
on the conditions according to type sof character
after merging the six data set sin Table 1. Then,
we applied the method with CRF s to the case A
and the proposed method to the case B.
Case A i s when an original term i s (1) an alpha-
betical term with more than two words, (2) a kanji
term in which other characters do not constitute, or
(3) a term of (1) or (2) with numeral s or symbols.
Case B i s when an original term doe snot fulfill the
condition sof the case A.
The total number of abbreviations was 3114
(1921 in the case A and 1103 in the case B). Ta-
16Wizard-of-Oz
</bodyText>
<page confidence="0.998468">
68
</page>
<bodyText confidence="0.9999756">
ble 5 shows the number of abbreviations in each
case for each data set. The total performance was
measured by calculating weighted average for two
recall scores, that is, in the case of A and B mea-
sured under a ten-fold cross-validation in the top
30. As a result, recall was 9 7.1% and 76.9% in
the case A and B respectively, and the total re-
call was 89.4%. Additionally, we conducted an
experiment in which the method with CRFs was
applied to all the abbreviations as a baseline. The
recall was 87.0% measured under a ten-fold cross-
validation in the top 30. The results show that it
is better to apply different methods according to
types of character than to apply one method to the
entire data set.
</bodyText>
<sectionHeader confidence="0.997916" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99996385">
In this paper, we proposed a method for generating
Japanese abbreviations from their full forms with
LLM. As a result of experiments, the proposed
method was confirmed to be effective for TV pro-
gram titles. It achieved 78.8% recall in the top
30, and improved 10.5% from a baseline method
using CRFs that achieved 68.3% recall. We also
described difficulties in generating Japanese ab-
breviations by examining six data sets classified
according to types of character and semantic cate-
gories. Consequently, we showed that the baseline
method is suited to a term containing a single type
of character such as alphabetical words and kanji
words, whereas the proposed method is suited to
a term containing multiple types of character. In
the future, we will apply the proposed method to
Japanese abbreviations generated with transliter-
ation between English and Japanese17. We also
plan to narrow down the top ranked abbreviation
candidates by using the search results on the Web.
</bodyText>
<sectionHeader confidence="0.997098" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996819375">
Eytan Adar. 2004. SaRAD: A Simple and Robust Ab-
breviation Dictionary. Bioinformatics, 20(4): 527-
533.
Masanori Enoki, Mika Koho, Kenko Ota, and Masuzo
Yanagida. 2007. Automatic Generation Abbrivi-
ated Forms of Japanese Expressions and its Appri-
cations to Speech Recognition (in Japanese). IPSJ
SIG Notes, 313-318.
</reference>
<footnote confidence="0.94818425">
17For example, &amp;quot;ミュージックステーション (music sta-
tion)&amp;quot; is abbreviated as &amp;quot;M ステ (emu sute)&amp;quot;. It is created
by using a substring &amp;quot;M&amp;quot; of &amp;quot;Music&amp;quot; translated from &amp;quot;ミュー
ジック&amp;quot; and a substring &amp;quot;ステ&amp;quot; of &amp;quot;ステーション&amp;quot;.
</footnote>
<reference confidence="0.991594307692307">
Toru Hisamitsu and Yoshiki Niwa. 2001. Extract-
ing useful terms from parenthetical expression by
combining simple rules and statistical measures: A
comparative evaluation of bigram statistics. Di-
dier Bourigault and Christian Jacquemin and Marie-
Claude L&apos;Homme editors. Recent Advances in Com-
putational Terminology, 209-224.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. Proceedings of the ICML-2001, 282-
289.
Christopher D. Manning and Hinrich Schutze. 1999.
The MIT Press. Foundations of statistical natural
language processing.
Norifumi Murayama and Manabu Okumura. 2006.
Automatic Generation of Abbreviations with Noisy-
channel model (in Japanese). NLP2006, 763-766.
Norifumi Murayama and Manabu Okumura. 2008.
Statistical Model for Japanese Abbreviations. Pro-
ceedings of the PRICAI-08, 260-272.
David Nadeau and Peter D. Turney. 2005. A super-
vised learning approach to acronym identification.
Proceedings of the AI&apos;2005, 10 pages.
Naoaki Okazaki and Sophia Ananiadou. 2006. Build-
ing an abbreviation dictionary using a term recogni-
tion approach. Bioinformatics, 22(24):3089-309 5.
Naoaki Okazaki, Sophia Ananiadou, and Jun&apos;ichi Tsu-
jii. 2008(1). A Discriminative Alignment Model for
Abbreviation Recognition. Proceedings of the Col-
ing 2008, 6 57-664.
Naoaki Okazaki, Mitsuru Ishizuka, and Jun&apos;ichi Tsujii.
2008(2). A Discriminative Approach to Japanese
Abbreviation Extraction. Proceedings of the IJC-
NLP 2008, 889-894.
Youngja Park and Roy J. Byrd. 2001. Hybrid Text
Mining for Finding Abbreviations and Their Defini-
tions. Proceedings of the EMNLP-2001, 126-133.
Masahiro Saikou, Kiyokazu Miki, and Hiroaki Hattori.
2008. Automatic Generation of Abbreviations with
Probabilistic Models (in Japanese). The Acoustical
Society of Japan, 237-238.
Hiroyuki Sakai and Shigeru Masuyama. 2005. Im-
provement of the Method for Acquiring Knowl-
edge from a Single Corpus on Correspondences be-
tween Abbreviations and Their Original words (in
Japanese). Journal of Natural Language Process-
ing, 12(4):207-231.
Ariel S. Schwartz and Marti A. Hearst. 2003. A Sim-
ple Algorithm for Identifying Abbreviation Defini-
tions in Biomedical Text. Proceedings of the PSB-
2003, 4 51-462.
</reference>
<page confidence="0.999097">
69
</page>
<table confidence="0.993943727272727">
Case A % Case B % Total
Alpha 917 (98.8) 11 (1.2) 928
Kata. 0 (0) 245 (100) 245
Kanji 363 (91.0) 36 (9.0) 399
Univ. 513 (98.1) 10 (1.9) 523
TV1 27 (8.3) 299 (91.7) 326
TV2 49 (8.1) 554 (91.9) 603
Total 1869 (61.8) 1155 (38.2) 3024
(Rec@30) - - 87.0%
CRF 97.1% 76.9% 89.4%
CRF/LLM
</table>
<tableCaption confidence="0.999606">
Table 5: The number of abbreviations in case A and B for the six data sets, and recall in the top 30.
</tableCaption>
<figure confidence="0.742686461538462">
&lt; A IphabetieaI W ords &gt; $EEU.
-DSDQ (OHFWURQLFV DQG ,QIRUPDWLRQ 7HFKQRORJ\ ,QGXVWULHV $VV-(,7$
1LSSRQ 7HOHSKRQH DQG 7HOHJUDSK FRUSRUDWLRQ 177
)UHHGRP 2I 0RELOH PXOWLPHGLD $FFHVV )20$
&lt; K atakana W ords &gt; $EEU.
࣮࢜ࢺ࣐ࢳࢵࢡࢺࣛࣥࢫ࣑ࢵࢩࣙࣥ (2RWRPDFKLNNX WRUDQVXPLVVK࣮࢜ࢺ࣐(2RWRPD)
(DXWRPDWLF WUDQVPLVVLRQ)
ࢫ࣮ࣃ࣮ࢥࣥࣆ࣮ࣗࢱ࣮ (6XXSDD FRQS\XXWDD) ࢫࣃࢥࣥ(6XSD FRQ)
(VXSHU FRPSXWHU)
࢔࣓ࣜ࢝ࣥࣇࢵࢺ࣮࣎ࣝ ($PHULFDQ IXWWR ERRUX) ࢔࣓ࣇࢺ ($PH IXWR)
($PHULFDQ IRRWEDOO)
&lt; KanjiW ords &gt; $EEU.
⚾ⓗ⊂༨ࡢ⚗Ṇཬࡧබṇྲྀᘬࡢ☜ಖ࡟㛵ࡍࡿἲᚊ ⊂⚗ἲ(&apos;RNNLQ KRX)
(6KLWHNL GRNXVHQ QR NLQVKL R\REL NRXVHL WRULKLNL QR NDNXKR QL NDQVXUX KRXULWVX)
($FW RQ 3URKLELWLRQ RI 3ULYDWH 0RQRSROL]DWLRQ DQG 0DLQWHQDQFH RI )DLU 7UDGH)
඲᪥ᮏẸ୺་⒪ᶵ㛵㐃ྜ఍ (=HQ QLKRQ PLQVKX LU\RX NLNDQ UHQJẸ་㐃(0LQLUHQ)
(-DSDQ )HGHUDWLRQ RI &apos;HPRFUDWLF 0HGLFDO ,QVWLWXWLRQV)
኱ᏛධᏛ㈨᱁᳨ᐃ ( &apos;DLJDNX Q\XXJDNX VLNDNX NHQWHL) ኱᳨(&apos;DL NHQ)
(WKH 8QLYHUVLW\ (QWUDQFH 4XDOLILFDWLRQ ([DPLQDWLRQ)
&lt; Name ofU niversity &gt; $EEU.
᪥ᮏ་⛉኱Ꮫ (1LKRQ LND GDLJDNX) ᪥་኱(1LFKLLGDL�, ᪥་(1LFKLL)
(1LSSRQ 0HGLFDO 6FKRRO)
ྡྂᒇၟ⛉኱Ꮫ (1DJR\D VKRXND GDLJDNX) ྡၟ኱(0HLVKRXGDL), ྡၟ(0HLVKRX)
(1DJR\D 8QLYHUVLW\ RI &amp;RPPHUFH &amp; %XVLQHVV)
࠾ⲔࡢỈዪᏊ኱Ꮫ (2FKDQRPL]X M\RVKL GDLJDNX) ࠾Ⲕዪ(2FKDM\R), ࠾Ⲕ኱(2FKDGDL)
(2FKDQRPL]X 8QLYHUVLW\)
&lt; Nam a of TV prog. 1 &gt; $EEU.
&lt; Nam a of TV prog. 2 &gt;
࠾ࡋࡷࢀᕤᡣ (2VKDUH NRXERX) ࠾ࡋࡷࢀ(2VKDUH)
($ QLIW\ FUDIW FHQWHU)
ࢸࣞ㐟ࡧࣃࣇ࢛࣮əᡯ⹡ࡢ✰ (7HUH DVREL SDIRR PDQ]DL WRUDQR ࢸࣞ㐟ࡧࣃࣇ࢛࣮(7HUH DVREL SDIRR)
3࠿᭶ࢺࣆࢵࢡⱥ఍ヰ (6DQ-NDJHWVX WRSLNNX HLNDLZD) ࢺࣆⱥ(7RSL HL)
($Q (QJOLVK FRQYHUVDWLRQ SURJUDP IRFXVLQJ RQ RQH WKHPH SHU HYHU\ WKUHH PRQWKV)
ᮅࡣࣅࢱ࣑ࣥ ($VD ZD ELWDPLQ) ᮅࣅࢱ($VD ELWD)
ࢲ࢘ࣥࢱ࢘ࣥࡢ࢞࢟ࡢ౑࠸ࡸ࠶ࡽ࡬ࢇ࡛ ࢞࢟౑(*DNL WVXND), ࢞࢟౑࠸(*DNL WVX
(&apos;RZQWRZQ QR JDNL QR WVXNDL \D DUDKHQ GH) ࢞࢟(*DNL), ࢞࢟ࡢ౑࠸(*DNL QR WVXND
(&apos;RZQWRZQ&apos;V -,&apos;P 1RW $Q (UUDQG %R\!-)
Ỉ᭙࡝࠺࡛ࡋࡻ࠺ (6XL\RX GRXGHVKRX) ࡝࠺࡛ࡋࡻ࠺(&apos;RXGHVKRX), Ỉ࡝࠺(6XL G
(+RZ GR \RX OLNH :HGQHVGD\&amp;quot;)
</figure>
<figureCaption confidence="0.999409">
Figure 2: Example of data sets.
</figureCaption>
<page confidence="0.992452">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335994">
<note confidence="0.5238445">Abbreviation Generation for Japanese Multi-Word Expressions 1 Komukai-Toshiba, Saiwai-ku, Kawasaki, 212-8582,</note>
<email confidence="0.931088">hiroko.fujii,</email>
<abstract confidence="0.999534578947368">This paper proposes a novel method for generating Japanese abbreviations from their full forms with the Log-Linear Model (LLM) in order to take advantage of characteristic patterns of Japanese abbreviation. Our experimental results show that the method is effective for TV program titles that contain colloquial expressions. proposed method achieved recall for the top 30 candidates, whereas a baseline method using Conditional Ran- Fields (CRFs) achieved recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eytan Adar</author>
</authors>
<title>SaRAD: A Simple and Robust Abbreviation Dictionary.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>527--533</pages>
<contexts>
<context position="1236" citStr="Adar, 2004" startWordPosition="180" endWordPosition="181"> The proposed method achieved 78.8% recall for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms. 1 Introduction Much research has been done on abbreviation extraction to detect terms having the same meaning. However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwart2 and Hearst, 2003; Adar, 2004; Sakai and Masuyama, 2005; Nadeau and Turney, 2005; Oka2aki and Ananiadou, 2006; Oka2aki et al., 2008(1); Oka2aki et al., 2008(2)) aimed at extracting abbreviations of organi2ation names and technical terms from well-written documents such as news articles and techincal papers. Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction. These terms can take several grammatical forms: a noun phrase, a sentence fragment, and even a</context>
</contexts>
<marker>Adar, 2004</marker>
<rawString>Eytan Adar. 2004. SaRAD: A Simple and Robust Abbreviation Dictionary. Bioinformatics, 20(4): 527-533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masanori Enoki</author>
<author>Mika Koho</author>
<author>Kenko Ota</author>
<author>Masuzo Yanagida</author>
</authors>
<title>Automatic Generation Abbriviated Forms of Japanese Expressions and its Apprications to Speech Recognition (in Japanese).</title>
<date>2007</date>
<journal>IPSJ SIG Notes,</journal>
<pages>313--318</pages>
<contexts>
<context position="6961" citStr="Enoki et al., 2007" startWordPosition="1063" endWordPosition="1066">in short form because of space limitations. In this process, we used program titles in TV schedules in newspapers as short forms and EPG 7 data as long forms. When a title in the schedule is written with short form of the title with the same date, time, and channel as EPG data, we recognized that it is an abbreviation and the other is its full form. We extracted 603 abbreviations. 2.2 Characteristics In this paper, we focus on abbreviations that lack some characters compared with the full forms. The followings are well-known characteristics of Japanese abbreviations (Sakai and Masuyama, 2005; Enoki et al., 2007; Murayama and Okumura, 2008).Abbreviations are created according to rules: (1) retain the beginning of a word and omit the rest (truncation); (2) divide an original term into base words, retaining several substrings from some of them, and combine them (contraction). In particular, four-mora$ katakana abbreviations are often created by combining two-mora as in the case of the katakana words in Figure 2. Also, the length of an abbreviation in kanji tends to be two or three letters as in the case of the kanji words in Figure 2. Moreover, if an original term consists of katakana with the specific</context>
</contexts>
<marker>Enoki, Koho, Ota, Yanagida, 2007</marker>
<rawString>Masanori Enoki, Mika Koho, Kenko Ota, and Masuzo Yanagida. 2007. Automatic Generation Abbriviated Forms of Japanese Expressions and its Apprications to Speech Recognition (in Japanese). IPSJ SIG Notes, 313-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Hisamitsu</author>
<author>Yoshiki Niwa</author>
</authors>
<title>Extracting useful terms from parenthetical expression by combining simple rules and statistical measures: A comparative evaluation of bigram statistics.</title>
<date>2001</date>
<booktitle>Didier Bourigault and Christian Jacquemin and MarieClaude L&apos;Homme editors. Recent Advances in Computational Terminology,</booktitle>
<pages>209--224</pages>
<contexts>
<context position="1176" citStr="Hisamitsu and Niwa, 2001" startWordPosition="168" endWordPosition="171">od is effective for TV program titles that contain colloquial expressions. The proposed method achieved 78.8% recall for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms. 1 Introduction Much research has been done on abbreviation extraction to detect terms having the same meaning. However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwart2 and Hearst, 2003; Adar, 2004; Sakai and Masuyama, 2005; Nadeau and Turney, 2005; Oka2aki and Ananiadou, 2006; Oka2aki et al., 2008(1); Oka2aki et al., 2008(2)) aimed at extracting abbreviations of organi2ation names and technical terms from well-written documents such as news articles and techincal papers. Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction. These terms can take several gramm</context>
</contexts>
<marker>Hisamitsu, Niwa, 2001</marker>
<rawString>Toru Hisamitsu and Yoshiki Niwa. 2001. Extracting useful terms from parenthetical expression by combining simple rules and statistical measures: A comparative evaluation of bigram statistics. Didier Bourigault and Christian Jacquemin and MarieClaude L&apos;Homme editors. Recent Advances in Computational Terminology, 209-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the ICML-2001,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="17410" citStr="Lafferty et al., 2001" startWordPosition="2909" endWordPosition="2912">. We also assign 1o to a set of zero features, i.e. tp0, tl0, e0, w0, ab0, enum0. Then, let L denote a set merged 11 and 1o. 2) Training and Test First, we obtain the above-mentioned feature set L with a training data set. Next, these features are assigned to all abbreviation candidates generated from the training data set in step 2. Then, a parameter aj (j = 1, • • • , ILI) of the Log-Linear Model is calculated by using Amis. Finally, the probabilities of all abbreviation candidates generated from a test data in step 2 are calculated by the Formula (2). 4 Evaluation 4.1 Baseline Method CRFs (Lafferty et al., 2001) are Log-Linear Models, which are often used for the labeling or parsing of sequential data and are widely applied for many NLP tasks. Some researchers already used CRFs for abbreviation extraction (Okazaki et al., 2008(1)) or generation (Saikou et al., 2008). Therefore, we evaluate a method using CRFs as a baseline. We formalize the abbreviation generation task as a sequence labeling problem in which each letter contained in an original term is to be used in its abbreviation13(Fig. 1). We also designed features attached to each character: morpheme word containing the letter, reading of the mo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the ICML-2001, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>The MIT Press. Foundations of statistical natural language processing.</title>
<date>1999</date>
<contexts>
<context position="13147" citStr="Manning and Schutze, 1999" startWordPosition="2117" endWordPosition="2120"> example, in the case of &amp;quot;Viva/Viva/V6&amp;quot;, all rules are used for &amp;quot;Viva&amp;quot;, &amp;quot;Viva&amp;quot;, and &amp;quot;V6&amp;quot; .Then, if 3), 3), 2) are used for each base word, we get a candidate &amp;quot;VVV6&amp;quot; .With the rules, we can get all candidates combining substrings at the beginning of each word because we used the stop conditions of character length of less than four in step 1 .However, note that we use mora instead of character in the case of phonographic characters .Also, we eliminate duplicative candidates. 3.3 Step3: Ranking Abbreviations LLM is a probabilistic model widely used as a maximum entropy model for many NLP tasks (Manning and Schutze, 1999) .We use standard LLM to rank the abbreviations. Consider a set of observations x for each sample of an object or event with y .Log-Linear Model gives a probability p(yIx; A) of an event by representing an event y as features fj(xl, yk). 1 p(y�x; A) = Z(x, A) exp(EjAjfj(x, y)) (1) Here, Aj(j = 1, ..., M) or aj is a model parameter, and it represents the weight of a feature fj(xl, yk) .Also, regularization term Z(x, A) is calculated as follows: Z(x, A) = EY Ey(x) exp(EjAjfj(x, y,)) Note that Y (x) represents a set of output y corresponding to x .The numerator of the Formula (1) is the same as t</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schutze. 1999. The MIT Press. Foundations of statistical natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norifumi Murayama</author>
<author>Manabu Okumura</author>
</authors>
<date>2006</date>
<booktitle>Automatic Generation of Abbreviations with Noisychannel model (in Japanese). NLP2006,</booktitle>
<pages>763--766</pages>
<marker>Murayama, Okumura, 2006</marker>
<rawString>Norifumi Murayama and Manabu Okumura. 2006. Automatic Generation of Abbreviations with Noisychannel model (in Japanese). NLP2006, 763-766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norifumi Murayama</author>
<author>Manabu Okumura</author>
</authors>
<title>Statistical Model for Japanese Abbreviations.</title>
<date>2008</date>
<booktitle>Proceedings of the PRICAI-08,</booktitle>
<pages>260--272</pages>
<contexts>
<context position="6990" citStr="Murayama and Okumura, 2008" startWordPosition="1067" endWordPosition="1071">e of space limitations. In this process, we used program titles in TV schedules in newspapers as short forms and EPG 7 data as long forms. When a title in the schedule is written with short form of the title with the same date, time, and channel as EPG data, we recognized that it is an abbreviation and the other is its full form. We extracted 603 abbreviations. 2.2 Characteristics In this paper, we focus on abbreviations that lack some characters compared with the full forms. The followings are well-known characteristics of Japanese abbreviations (Sakai and Masuyama, 2005; Enoki et al., 2007; Murayama and Okumura, 2008).Abbreviations are created according to rules: (1) retain the beginning of a word and omit the rest (truncation); (2) divide an original term into base words, retaining several substrings from some of them, and combine them (contraction). In particular, four-mora$ katakana abbreviations are often created by combining two-mora as in the case of the katakana words in Figure 2. Also, the length of an abbreviation in kanji tends to be two or three letters as in the case of the kanji words in Figure 2. Moreover, if an original term consists of katakana with the specific characters such as sokuon 9 </context>
</contexts>
<marker>Murayama, Okumura, 2008</marker>
<rawString>Norifumi Murayama and Manabu Okumura. 2008. Statistical Model for Japanese Abbreviations. Proceedings of the PRICAI-08, 260-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Peter D Turney</author>
</authors>
<title>A supervised learning approach to acronym identification.</title>
<date>2005</date>
<booktitle>Proceedings of the AI&apos;2005,</booktitle>
<pages>10</pages>
<contexts>
<context position="1287" citStr="Nadeau and Turney, 2005" startWordPosition="187" endWordPosition="190">call for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms. 1 Introduction Much research has been done on abbreviation extraction to detect terms having the same meaning. However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwart2 and Hearst, 2003; Adar, 2004; Sakai and Masuyama, 2005; Nadeau and Turney, 2005; Oka2aki and Ananiadou, 2006; Oka2aki et al., 2008(1); Oka2aki et al., 2008(2)) aimed at extracting abbreviations of organi2ation names and technical terms from well-written documents such as news articles and techincal papers. Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction. These terms can take several grammatical forms: a noun phrase, a sentence fragment, and even a sentence. Also, many of these expressions contain </context>
</contexts>
<marker>Nadeau, Turney, 2005</marker>
<rawString>David Nadeau and Peter D. Turney. 2005. A supervised learning approach to acronym identification. Proceedings of the AI&apos;2005, 10 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Building an abbreviation dictionary using a term recognition approach.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<pages>22--24</pages>
<marker>Okazaki, Ananiadou, 2006</marker>
<rawString>Naoaki Okazaki and Sophia Ananiadou. 2006. Building an abbreviation dictionary using a term recognition approach. Bioinformatics, 22(24):3089-309 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
<author>Sophia Ananiadou</author>
<author>Jun&apos;ichi Tsujii</author>
</authors>
<title>A Discriminative Alignment Model for Abbreviation Recognition.</title>
<date>2008</date>
<booktitle>Proceedings of the Coling</booktitle>
<volume>6</volume>
<pages>57--664</pages>
<contexts>
<context position="17629" citStr="Okazaki et al., 2008" startWordPosition="2946" endWordPosition="2949">. Next, these features are assigned to all abbreviation candidates generated from the training data set in step 2. Then, a parameter aj (j = 1, • • • , ILI) of the Log-Linear Model is calculated by using Amis. Finally, the probabilities of all abbreviation candidates generated from a test data in step 2 are calculated by the Formula (2). 4 Evaluation 4.1 Baseline Method CRFs (Lafferty et al., 2001) are Log-Linear Models, which are often used for the labeling or parsing of sequential data and are widely applied for many NLP tasks. Some researchers already used CRFs for abbreviation extraction (Okazaki et al., 2008(1)) or generation (Saikou et al., 2008). Therefore, we evaluate a method using CRFs as a baseline. We formalize the abbreviation generation task as a sequence labeling problem in which each letter contained in an original term is to be used in its abbreviation13(Fig. 1). We also designed features attached to each character: morpheme word containing the letter, reading of the morpheme word, 13In (Saikou et al., 2008), they formalized the abbreviation generation task as a sequence labeling problem in which each mora contained in a term is to be used in its abbreviation. To avoid reading estimat</context>
</contexts>
<marker>Okazaki, Ananiadou, Tsujii, 2008</marker>
<rawString>Naoaki Okazaki, Sophia Ananiadou, and Jun&apos;ichi Tsujii. 2008(1). A Discriminative Alignment Model for Abbreviation Recognition. Proceedings of the Coling 2008, 6 57-664.</rawString>
</citation>
<citation valid="true">
<title>A Discriminative Approach to Japanese Abbreviation Extraction.</title>
<date>2008</date>
<booktitle>Proceedings of the IJCNLP</booktitle>
<pages>889--894</pages>
<contexts>
<context position="22721" citStr="(2008)" startWordPosition="3855" endWordPosition="3855">d the division in step 2 as an alternative to MeCab, recall with CRFs differed approximately le ss than ±1% from recall in Table 3. On the other hand, when we used MeCab as an alternative to the division in step 2, recall with the proposed method was significantly lower than in Table 4. We cannot compare our performance directly with the previou s work because of the differences in data sets. For reference, Murayama et al. (2006) reported 68.4% recall in the top 30 with the NoisyChannel Model. They used 851 abbreviations corresponding to 748 full form s extracted from Wikipedia. Saikou et al. (2008) reported 72.5% recall in the top 30 with CRFs. They used 51 abbreviations collected by WoZ16 as te st data and 781 abbreviations that appeared in Wikipedia as training data. 4.3 Combination of two methods Table 4 show s that the baseline method i s better for the alphabetical words, name sof universities, and kanji words, whereas the proposed method i s better for others. However the classification on Table 1 i s made by hand. Here, we automatically classified them into the following case A and B based on the conditions according to type sof character after merging the six data set sin Table </context>
</contexts>
<marker>2008</marker>
<rawString>Naoaki Okazaki, Mitsuru Ishizuka, and Jun&apos;ichi Tsujii. 2008(2). A Discriminative Approach to Japanese Abbreviation Extraction. Proceedings of the IJCNLP 2008, 889-894.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngja Park</author>
<author>Roy J Byrd</author>
</authors>
<title>Hybrid Text Mining for Finding Abbreviations and Their Definitions.</title>
<date>2001</date>
<booktitle>Proceedings of the EMNLP-2001,</booktitle>
<pages>126--133</pages>
<contexts>
<context position="1197" citStr="Park and Byrd, 2001" startWordPosition="172" endWordPosition="175">gram titles that contain colloquial expressions. The proposed method achieved 78.8% recall for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms. 1 Introduction Much research has been done on abbreviation extraction to detect terms having the same meaning. However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwart2 and Hearst, 2003; Adar, 2004; Sakai and Masuyama, 2005; Nadeau and Turney, 2005; Oka2aki and Ananiadou, 2006; Oka2aki et al., 2008(1); Oka2aki et al., 2008(2)) aimed at extracting abbreviations of organi2ation names and technical terms from well-written documents such as news articles and techincal papers. Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction. These terms can take several grammatical forms: a noun </context>
</contexts>
<marker>Park, Byrd, 2001</marker>
<rawString>Youngja Park and Roy J. Byrd. 2001. Hybrid Text Mining for Finding Abbreviations and Their Definitions. Proceedings of the EMNLP-2001, 126-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiro Saikou</author>
<author>Kiyokazu Miki</author>
<author>Hiroaki Hattori</author>
</authors>
<date>2008</date>
<booktitle>Automatic Generation of Abbreviations with Probabilistic Models (in Japanese). The Acoustical Society of Japan,</booktitle>
<pages>237--238</pages>
<contexts>
<context position="17669" citStr="Saikou et al., 2008" startWordPosition="2952" endWordPosition="2955">ll abbreviation candidates generated from the training data set in step 2. Then, a parameter aj (j = 1, • • • , ILI) of the Log-Linear Model is calculated by using Amis. Finally, the probabilities of all abbreviation candidates generated from a test data in step 2 are calculated by the Formula (2). 4 Evaluation 4.1 Baseline Method CRFs (Lafferty et al., 2001) are Log-Linear Models, which are often used for the labeling or parsing of sequential data and are widely applied for many NLP tasks. Some researchers already used CRFs for abbreviation extraction (Okazaki et al., 2008(1)) or generation (Saikou et al., 2008). Therefore, we evaluate a method using CRFs as a baseline. We formalize the abbreviation generation task as a sequence labeling problem in which each letter contained in an original term is to be used in its abbreviation13(Fig. 1). We also designed features attached to each character: morpheme word containing the letter, reading of the morpheme word, 13In (Saikou et al., 2008), they formalized the abbreviation generation task as a sequence labeling problem in which each mora contained in a term is to be used in its abbreviation. To avoid reading estimation, we generate abbreviations by abbrev</context>
<context position="22721" citStr="Saikou et al. (2008)" startWordPosition="3852" endWordPosition="3855">r. When we used the division in step 2 as an alternative to MeCab, recall with CRFs differed approximately le ss than ±1% from recall in Table 3. On the other hand, when we used MeCab as an alternative to the division in step 2, recall with the proposed method was significantly lower than in Table 4. We cannot compare our performance directly with the previou s work because of the differences in data sets. For reference, Murayama et al. (2006) reported 68.4% recall in the top 30 with the NoisyChannel Model. They used 851 abbreviations corresponding to 748 full form s extracted from Wikipedia. Saikou et al. (2008) reported 72.5% recall in the top 30 with CRFs. They used 51 abbreviations collected by WoZ16 as te st data and 781 abbreviations that appeared in Wikipedia as training data. 4.3 Combination of two methods Table 4 show s that the baseline method i s better for the alphabetical words, name sof universities, and kanji words, whereas the proposed method i s better for others. However the classification on Table 1 i s made by hand. Here, we automatically classified them into the following case A and B based on the conditions according to type sof character after merging the six data set sin Table </context>
</contexts>
<marker>Saikou, Miki, Hattori, 2008</marker>
<rawString>Masahiro Saikou, Kiyokazu Miki, and Hiroaki Hattori. 2008. Automatic Generation of Abbreviations with Probabilistic Models (in Japanese). The Acoustical Society of Japan, 237-238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Sakai</author>
<author>Shigeru Masuyama</author>
</authors>
<title>Improvement of the Method for Acquiring Knowledge from a Single Corpus on Correspondences between Abbreviations and Their Original words (in Japanese).</title>
<date>2005</date>
<journal>Journal of Natural Language Processing,</journal>
<pages>12--4</pages>
<contexts>
<context position="1262" citStr="Sakai and Masuyama, 2005" startWordPosition="182" endWordPosition="186">d method achieved 78.8% recall for the top 30 candidates, whereas a baseline method using Conditional Random Fields (CRFs) achieved 68.3% recall. Moreover, from the results of experiments using six data sets classified according to types of character and semantic categories, we show that each performance of the above two methods depends on the types of the full forms. 1 Introduction Much research has been done on abbreviation extraction to detect terms having the same meaning. However, most previous studies (Hisamitsu and Niwa, 2001; Park and Byrd, 2001; Schwart2 and Hearst, 2003; Adar, 2004; Sakai and Masuyama, 2005; Nadeau and Turney, 2005; Oka2aki and Ananiadou, 2006; Oka2aki et al., 2008(1); Oka2aki et al., 2008(2)) aimed at extracting abbreviations of organi2ation names and technical terms from well-written documents such as news articles and techincal papers. Many Japanese terms indicating individual TV programs, songs, comics, novels, and so on, are multi-word expressions and have the characteristics distinct from terms treated in most previous studies on abbreviation extraction. These terms can take several grammatical forms: a noun phrase, a sentence fragment, and even a sentence. Also, many of t</context>
<context position="6941" citStr="Sakai and Masuyama, 2005" startWordPosition="1059" endWordPosition="1062">s in TV schedules written in short form because of space limitations. In this process, we used program titles in TV schedules in newspapers as short forms and EPG 7 data as long forms. When a title in the schedule is written with short form of the title with the same date, time, and channel as EPG data, we recognized that it is an abbreviation and the other is its full form. We extracted 603 abbreviations. 2.2 Characteristics In this paper, we focus on abbreviations that lack some characters compared with the full forms. The followings are well-known characteristics of Japanese abbreviations (Sakai and Masuyama, 2005; Enoki et al., 2007; Murayama and Okumura, 2008).Abbreviations are created according to rules: (1) retain the beginning of a word and omit the rest (truncation); (2) divide an original term into base words, retaining several substrings from some of them, and combine them (contraction). In particular, four-mora$ katakana abbreviations are often created by combining two-mora as in the case of the katakana words in Figure 2. Also, the length of an abbreviation in kanji tends to be two or three letters as in the case of the kanji words in Figure 2. Moreover, if an original term consists of kataka</context>
</contexts>
<marker>Sakai, Masuyama, 2005</marker>
<rawString>Hiroyuki Sakai and Shigeru Masuyama. 2005. Improvement of the Method for Acquiring Knowledge from a Single Corpus on Correspondences between Abbreviations and Their Original words (in Japanese). Journal of Natural Language Processing, 12(4):207-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariel S Schwartz</author>
<author>Marti A Hearst</author>
</authors>
<title>A Simple Algorithm for Identifying Abbreviation Definitions in Biomedical Text.</title>
<date>2003</date>
<booktitle>Proceedings of the PSB2003,</booktitle>
<volume>4</volume>
<pages>51--462</pages>
<marker>Schwartz, Hearst, 2003</marker>
<rawString>Ariel S. Schwartz and Marti A. Hearst. 2003. A Simple Algorithm for Identifying Abbreviation Definitions in Biomedical Text. Proceedings of the PSB2003, 4 51-462.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>