<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.98797">
A DEFINITE CLAUSE VERSION
OF CATEGORIAL GRAMMAR
</title>
<author confidence="0.985239">
Remo Pareschi*,
</author>
<affiliation confidence="0.873162625">
Department of Computer and Information Science,
University of Pennsylvania,
200 S. 331d St., Philadelphia, PA 191041 and
Department of Artificial Intelligence and
Centre for Cognitive Science,
University of Edinburgh,
2 Buccleuch Place,
Edinburgh EH8 9LW, Scotland
</affiliation>
<email confidence="0.991589">
remo@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.99257" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.980428">
We introduce a first-order version of Catego-
rial Grammar, based on the idea of encoding syn-
tactic types as definite clauses. Thus, we drop
all explicit requirements of adjacency between
combinable constituents, and we capture word-
order constraints simply by allowing subformu-
lae of complex types to share variables ranging
over string positions. We are in this way able
to account for constructions involving discontin-
uous constituents. Such constructions are difficult
to handle in the more traditional version of Cate-
gorial Grammar, which is based on propositional
types and on the requirement of strict string ad-
jacency between combinable constituents.
We show then how, for this formalism, parsing
can be efficiently implemented as theorem proving.
Our approach to encoding types-as definite clauses
presupposes a modification of standard Horn logic
syntax to allow internal implications in definite
clauses. This modification is needed to account for
the types of higher-order functions and, as a con-
sequence, standard Prolog-like Horn logic theorem
proving is not powerful enough. We tackle this
•I am indebted to Dale Miller for help and advice. I
am also grateful to Aravind Joshi, Mark Steedman, David
Weir, Bob Frank, Mitch Marcus and Yves Schabes for com-
ments and discussions. Thanks are due to Elsa Gunter and
Amy Felty for advice on typesetting. Parts of this research
were supported by: a Sloan foundation grant to the Cog-
nitive Science Program, Univ. of Pennsylvania; and NSF
grants MCS-8219196-CER, IRI-10413 A02, ARO grants
DAA29-84-K-0061, DAA29-84-9-0027 and DARPA grant
N00014-85-K0018 to CIS, Univ. of Pennsylvania.
t Address for correspondence
problem by adopting an intuitionistic treatment
of implication, which has already been proposed
elsewhere as an extension of Prolog for implement-
ing hypothetical reasoning and modular logic pro-
gramming.
</bodyText>
<sectionHeader confidence="0.998964" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999509">
Classical Categorial Grammar (CG) [I] is an ap-
proach to natural language syntax where all lin-
guistic information is encoded in the lexicon, via
the assignment of syntactic types to lexical items.
Sudi syntactic types can be viewed as expressions
of an implicational calculus of propositions, where
atomic propositions correspond to atomic types,
and implicational propositions account for com-
plex types. A string is grammatical if and only
if its syntactic type can be logically derived from
the types of its words, assuming certain inference
rules.
In classical CG, a common way of encoding
word-order constraints is by having two symmet-
ric forms of &amp;quot;directional&amp;quot; implication, usually in-
dicated with the forward slash / and the backward
slash \, constraining the antecedent of a complex
type to be, respectively, right- or left-adjacent. A
word, or a string of words, associated with a right-
(left-) oriented type can then be thought of as a
right- (left-) oriented function looking for an ar-
gument of the type specified in the antecedent. A
convention more or less generally followed by lin-
guists working in CG is to have the antecedent and
the consequent of an implication respectively on
</bodyText>
<page confidence="0.988045">
270
</page>
<bodyText confidence="0.97139375">
the right and on the left of the connective. Thus,
the type-assignment (1) says that the ditransitive
verb put is a function taking a right-adjacent ar-
gument of type NP, to return a function taking a
right-adjacent argument of type PP, to return a
function taking a left-adjacent argument of type
NP, to finally return an expression of the atomic
type S.
</bodyText>
<listItem confidence="0.418036">
(1) put: ((S\NP)/PP)/NP
</listItem>
<bodyText confidence="0.999501647058823">
The Definite Clause Grammar (DCG) framework
[14] (see also [13]), where phrase-structure gram-
mars can be encoded as sets of definite clauses
(which are themselves a subset of Horn clauses),
and the formalization of some aspects of it in [15],
suggests a more expressive alternative to encode
word-order constraints in CG. Such an alterna-
tive eliminates all notions of directionality from
the logical connectives, and any explicit require-
ment of adjacency between functions and argu-
ments, and replaces propositions with first-order
formulae. Thus, atomic types are viewed as atomic
formulae obtained from two-place predicates over
string positions represented as integers, the first
and the second argument corresponding, respec-
tively, to the left and right end of a given string.
Therefore, the set of all sentences of length j
generated from a certain lexicon corresponds to
the type S(0, j). Constraints over the order of
constituents are enforced by sharing integer in-
dices across subformulae inside complex (func-
tional) types.
This first-order version of CG can be viewed as a
logical reconstruction of some of the ideas behind
the recent trend of Categorial Unification Gram-
mars [5, 18, 20]&apos;. A strongly analogous develop-
ment characterizes the systems of type-assignment
for the formal languages of Combinatory Logic and
Lambda Calculus, leading from propositional type
systems to the &amp;quot;formulae-as-types&amp;quot; slogan which is
behind the current research in type theory [2]. In
this paper, we show how syntactic types can be en-
coded using an extended version of standard Horn
logic syntax.
</bodyText>
<sectionHeader confidence="0.83796" genericHeader="method">
2 Definite Clauses with In-
ternal Implications
</sectionHeader>
<bodyText confidence="0.991677857142857">
Let A and —■ be logical connectives for conjunc-
tion and implication, and let V and 3 be the univer-
&apos;Indeed, Uszkoreit 1181 mentions the possibility of en-
coding order constraints among constituents via variables
ranging over string positions in the DCG style.
sal and existential quantifiers. Let A be a syntactic
variable ranging over the set of atoms, i. e. the set
of atomic first-order formulae, and let D and G be
syntactic variables ranging, respectively, over the
set of definite clauses and the set of goal clauses.
We introduce the notions of definite clause and
of goal clause via the two following mutually re-
cursive definitions for the corresponding syntactic
variables D and G:
</bodyText>
<listItem confidence="0.9971435">
• D := A I G A IVxD I Di A D2
• G:=AIGiAG213xGID—*G
</listItem>
<bodyText confidence="0.999909642857143">
We call ground a clause not containing variables.
We refer to the part of a non-atomic definite clause
coming on the left of the implication connective
as to the body of the clause, and to the one on
the right as to the head. With respect to standard
Horn logic syntax, the main novelty in the defini-
tions above is that we permit implications in goals
and in the bodies of definite clauses. Extended
Horn logic syntax of this kind has been proposed
to implement hypothetical reasoning [3] and mod-
ules [7] in logic programming. We shall first make
clear the use of this extension for the purpose of
linguistic description, and we shall then illustrate
its operational meaning.
</bodyText>
<sectionHeader confidence="0.996288" genericHeader="method">
3 First-order
</sectionHeader>
<subsectionHeader confidence="0.9516555">
Categorial Grammar
3.1 Definite Clauses as Types
</subsectionHeader>
<bodyText confidence="0.977016272727273">
We take CONN (for &amp;quot;connects&amp;quot;) to be a three-
place predicate defined over lexical items and pairs
of integers, such that CONN(item,i,j) holds if
and only if and only if i = j — 1, with the in-
tuitive meaning that item lies between the two
consecutive string positions i and j. Then, a
most direct way to translate in first-order logic
the type-assignment (1) is by the type-assignment
(2), where, in the formula corresponding to the as-
signed type, the non-directional implication con-
nective replaces the slashes.
</bodyText>
<equation confidence="0.8894186">
(2) put : VzVyVzVw[CONN(put, y — 1,y) —■
(NP(y, z) --■
(PP(z,w)
(NP(x,y — 1) —■
5.(0,w))))]
</equation>
<page confidence="0.976697">
271
</page>
<bodyText confidence="0.9975905">
A definite clause equivalent of the formula in (2)
is given by the type-assignment (3)2.
</bodyText>
<listItem confidence="0.74389675">
(3) put: VxVyVzVw[CONN(put, y — 1, y) A
NP(y, z) A
PP(z , w) A
NP(x, y — 1) —+ S(x,w)]
</listItem>
<bodyText confidence="0.99578975">
Observe that the predicate CONNwill need also
to be part of types assigned to &amp;quot;non-functional&amp;quot;
lexical items. For example, we can have for the
noun-phrase Mary the type-assignment (4).
</bodyText>
<listItem confidence="0.9131665">
(4) Mary: Vy[CONN(Mary, y — 1, y)
NP(y — 1, y)]
</listItem>
<subsectionHeader confidence="0.9994415">
3.2 Higher-order Types and Inter-
nal Implications
</subsectionHeader>
<bodyText confidence="0.99984525">
Propositional CG makes crucial use of func-
tions of higher-order type. For example, the type-
assignment (5) makes the relative pronoun which
into a function taking a right-oriented function
from noun-phrases to sentences and returning a
relative clause3. This kind of type-assignment has
been used by several linguists to provide attractive
accounts of certain cases of extraction [16, 17, 10].
</bodyText>
<listItem confidence="0.705033">
(5) which : REL/(S/ NP)
</listItem>
<bodyText confidence="0.9999206">
In our definite clause version of CG, a similar
assignment, exemplified by (6), is possible, since
implications are allowed in the, body of clauses.
Notice that in (6) the noun-phrase needed to fill
the extraction site is &amp;quot;virtual&amp;quot;, having null length.
</bodyText>
<listItem confidence="0.697372">
(6) which : VvVy[CONN(which, v — 1, v) A
</listItem>
<equation confidence="0.983986">
(NP(Y) Y) S(V, Y)) --
REL(v — 1, y)]
</equation>
<bodyText confidence="0.98348025">
2 See [2] for a pleasant formal characterization of first-
order definite clauses as type declarations.
3For simplicity sake, we treat here relative clauses as
constituents of atomic type. But in reality relative clauses
are noun modifiers, that is, functions from nouns to nouns.
Therefore, the propositional and the first-order atomic type
for relative clauses in the examples below should be thought
of as shorthands for corresponding complex types.
</bodyText>
<subsectionHeader confidence="0.99924">
3.3 Arithmetic Predicates
</subsectionHeader>
<bodyText confidence="0.9999927">
The fact that we quantify over integers allows
us to use arithmetic predicates to determine sub-
sets of indices over which certain variables must
range. This use of arithmetic predicates charac-
terizes also Rounds&apos; ILFP notation [15], which ap-
pears in many ways interestingly related to the
framework proposed here. We show here below
how this capability can be exploited to account
for a case of extraction which is particularly prob-
lematic for bidirectional propositional CG.
</bodyText>
<subsectionHeader confidence="0.637164">
3.3.1 Non-peripheral Extraction
</subsectionHeader>
<bodyText confidence="0.997340970588235">
Both the propositional type (5) and the first-
order type (6) are good enough to describe the
kind of constituent needed by a relative pronoun
in the following right-oriented case of peripheral
extraction, where the extraction site is located at
one end of the sentence. (We indicate the extrac-
tion site with an upward-looking arrow.)
which [ I shall put a book on
However, a case of non-peripheral extraction,
where the extraction site is in the middle, such
as
which [ I shall put T on the table ]
is difficult to describe in bidirectional proposi-
tional CG, where all functions must take left- or
right-adjacent arguments. For instance, a solution
like the one proposed in [17] involves permuting
the arguments of a given function. Such an opera-
tion needs to be rather cumbersomely constrained
in an explicit way to cases of extraction, lest it
should wildly overgenerate. Another solution, pro-
posed in [10], is also cumbersome and counterintu-
itive, in that involves the assignment of multiple
types to wh-expressions, one for each site where
extraction can take place.
On the other hand, the greater expressive power
of first-order logic allows us to elegantly general-
ize the type-assignment (6) to the type-assignment
(7). In fact, in (7) the variable identifying the ex-
traction site ranges over the set of integers in be-
tween the indices corresponding, respectively, to
the left and right end of the sentence on which
the relative pronoun operates. Therefore, such a
sentence can have an extraction site anywhere be-
tween its string boundaries.
</bodyText>
<page confidence="0.900534">
272
</page>
<equation confidence="0.9586995">
(7) which : VvVyVw[CONN(which, v — 1, v) A
(NP(y , y) S(v w)) A
v&lt;yAy&lt;w---.
REL(v — 1, w)
</equation>
<bodyText confidence="0.999338045454546">
Non-peripheral extraction is but one example of
a class of discontinuous constituents, that is, con-
stituents where the function-argument relation is
not determined in terms of left- or right-adjacency,
since they have two or more parts disconnected
by intervening lexical material, or by internal ex-
traction sites. Extraposition phenomena, gap-
ping constructions in coordinate structures, and
the distribution of adverbials offer other problem-
atic examples of English discontinuous construc-
tions for which this first-order framework seems
to promise well. A much larger batch of simi-
lar phenomena is offered by languages with freer
word order than English, for which, as pointed
out in [5, 18], classical CG suffers from an even
clearer lack of expressive power. Indeed, Joshi [4]
proposes within the TAG framework an attractive
general solution to word-order variations phenom-
ena in terms of linear precedence relations among
constituents. Such a solution suggests a similar
approach for further work to be pursued within
the framework presented here.
</bodyText>
<sectionHeader confidence="0.994612" genericHeader="method">
4 Theorem Proving
</sectionHeader>
<bodyText confidence="0.99999393220339">
In propositional CG, the problem of determin-
ing the type of a string from the types of its
words has been addressed either by defining cer-
tain &amp;quot;combinatory&amp;quot; rules which then determine a
rewrite relation between sequences of types, or by
viewing the type of a string as a logical conse-
quence of the types of its words. The first al-
ternative has been explored mainly in Combina-
tory Grammar [16, 17], where, beside the rewrite
rule of functional application, which was already
in the initial formulation of CG in [1], there are
also the rules of functional composition and type
raising, which are used to account for extraction
and coordination phenomena. This approach of-
fers a psychologically attractive model of parsing,
based on the idea of incremental processing, but
causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost
exponential proliferation of the possible derivation
paths for identical analyses of a given string. In
fact, although a rule like functional composition
is specifically needed for cases of extraction and
coordination, in principle nothing prevents its use
to analyze strings not characterized by such phe-
nomena, which would be analyzable in terms of
functional application alone. Tentative solutions
of this problem have been recently discussed in
[12, 19].
The second alternative has been undertaken in
the late fifties by Lambek [6] who defined a deci-
sion procedure for bidirectional propositional CG
in terms of a Gentzen-style sequent system. Lam-
bek&apos;s implicational calculus of syntactic types has
recently enjoyed renewed interest in the works of
van Benthem, Moortgat and other scholars. This
approach can account for a range of syntactic phe-
nomena similar to that of Combinatory Grammar,
and in fact many of the rewrite rules of Combi-
natory Grammar can be derived as theorems in
the calculus. However, analyses of cases of extrac-
tion and coordination are here obtained via infer-
ences over the internal implications in the types of
higher-order functions. Thus, extraction and coor-
dination can be handled in an expectation-driven
fashion, and, as a consequence, there is no problem
of spuriously ambiguous derivations.
Our approach here is close in spirit to Lambek&apos;s
enterprise, since we also make use of a Gentzen
system capable of handling the internal implica-
tions in the types of higher-order functions, but
at the same time differs radically from it, since
we do not need to have a &amp;quot;specialized&amp;quot; proposi-
tional logic, with directional connectives and adja-
cency requirements. Indeed, the expressive power
of standard first-order logic completely eliminates
the need for this kind of specialization, and at the
same time provides the ability to account for con-
structions which, as shown in section 3.3.1, are
problematic for an (albeit specialized) proposi-
tional framework.
</bodyText>
<subsectionHeader confidence="0.9189115">
4.1 An Intuitionistic Extension of
Prolog
</subsectionHeader>
<bodyText confidence="0.999983916666667">
The inference system we are going to introduce
below has been proposed in [7] as an extension of
Prolog suitable for modular logic programming. A
similar extension has been proposed in [3] to im-
plement hypotethical reasoning in logic program-
ming. We are thus dealing with what can be con-
sidered the specification of a general purpose logic
programming language. The encoding of a par-
ticular linguistic formalism is but one other appli-
cation of such a language, which Miller [7] shows
to be sound and complete for intuitionistic logic,
and to have a well defined semantics in terms of
</bodyText>
<page confidence="0.996182">
273
</page>
<bodyText confidence="0.854074">
Kripke models.
</bodyText>
<subsubsectionHeader confidence="0.811685">
4.1.1 Logic Programs
</subsubsectionHeader>
<bodyText confidence="0.978551">
We take a logic program or, simply, a program
</bodyText>
<listItem confidence="0.977078">
P to be any set of definite clauses. We formally
represent the fact that a goal clause G is logically
derivable from a program P with a sequent of the
form P G, where P and G are, respectively, the
antecedent and the succedent of the sequent. If P
is a program then we take its substitution closure
[P] to be the smallest set such that
• P C [7:1
• if D1 A D2 E pi then DI E VI and D2 E
• if VxD E [2] then [xlt]D E [P) for all terms I,
</listItem>
<bodyText confidence="0.8134985">
where [zit] denotes the result of substituting
I for free occurrences of t in D
</bodyText>
<subsubsectionHeader confidence="0.975668">
4.1.2 Proof Rules
</subsubsectionHeader>
<bodyText confidence="0.999117">
We introduce now the following proof rules,
which define the notion of proof for our logic pro-
gramming language:
</bodyText>
<equation confidence="0.996127909090909">
(I) P G if G E [P]
P G
(II) if G A E [P]
(III)
P Gi A G2
P
(IV) [xlt]G
P 3xG
V) P U {D} G
(
P D G
</equation>
<bodyText confidence="0.999626">
In the inference figures for rules (II) - (V), the
sequent(s) appearing above the horizontal line are
the upper seguent(s), while the sequent appearing
below is the lower sequent. A proof for a sequent
P G is a tree whose nodes are labeled with
sequents such that (i) the root node is labeled with
P G, (ii) the internal nodes are instances of one
of proof rules (II) - (V) and (iii) the leaf nodes are
labeled with sequents representing proof rule (I).
The height of a proof is the length of the longest
path from the root to some leaf. The size of a
proof is the number of nodes in it.
Thus, proof rules (I)-(V) provide the abstract
specification of a first-order theorem prover which
can then be implemented in terms of depth-first
search, backtracking and unification like a Prolog
interpreter. (An example of such an implemen-
tation, as a metainterpreter on top of Lambda-
Prolog, is given in [9].) Observe however that
an important difference of such a theorem prover
from a standard Prolog interpreter is in the wider
distribution of &amp;quot;logical&amp;quot; variables, which, in the
logic programming tradition, stand for existen-
tially quantified variables within goals. Such vari-
ables can get instantiated in the course of a Prolog
proof, thus providing the procedural ability to re-
turn specific values as output of the computation.
Logical variables play the same role in the pro-
gramming language we are considering here; more-
over, they can also occur in program clauses, since
subformulae of goal clauses can be added to pro-
grams via proof rule (V).
</bodyText>
<subsectionHeader confidence="0.996844">
4.2 How Strings Define Programs
</subsectionHeader>
<bodyText confidence="0.999286666666667">
Let a be a string al ... an of words from a lex-
icon L. Then a defines a program Pa =
such that
</bodyText>
<listItem confidence="0.999748">
• ra = {CONN(ai, i — 1, i) I 1 &lt; i &lt; n}
• Ac, = {Dlai:D ELandl&lt;i&lt;n}
</listItem>
<bodyText confidence="0.999909153846154">
Thus, ra just contains ground atoms encoding
the position of words in a. Aa contains instead all
the types assigned in the lexicon to words in a. We
assume arithmetic operators for addition, subtrac-
tion, multiplication and integer division, and we
assume that any program Pa works together with
an infinite set of axioms A defining the compari-
son predicates over ground arithmetic expressions
&lt;, &lt;, &gt;, &gt;. (Prolog&apos;s evaluation mechanism treats
arithmetic expressions in a similar way.) Then,
under this approach a string a is of type Ga if and
only if there is a proof for the sequent Pal.JA Ga
according to rules (I) - (V).
</bodyText>
<subsectionHeader confidence="0.999864">
4.3 An Example
</subsectionHeader>
<bodyText confidence="0.816400777777778">
We give here an example of a proof which deter-
mines a corresponding type-assignment. Consider
the string
whom John loves
Such a sentence determines a program P with
the following set 1&apos; of ground atoms:
{CONN(whom, 0, 1),
CONN(John, 1,2),
CONN(loves, 2, 3)}
</bodyText>
<page confidence="0.996134">
274
</page>
<bodyText confidence="0.999976">
We assume lexical type assignments such that
the remaining set of clauses A is as follows:
</bodyText>
<equation confidence="0.921337428571429">
{VxVz[CONN(whom, a — 1, x) A
(NP(y, y) S(x, y))
REL(x — 1, y)],
V x[CONN(John, x — 1, x) NP(x — 1,x)],
VxVyVz[CONN(loves, y — 1, y) A
NP(y, z) A NP(x , y — 1) —■
S(x , z)J}
</equation>
<bodyText confidence="0.999939285714286">
The clause assigned to the relative pronoun
whom corresponds to the type of a higher-order
function, and contains an implication in its body.
Figure 1 shows a proof tree for such a type-
assignment. The tree, which is represented as
growing up from its root, has size 11, and height
8.
</bodyText>
<sectionHeader confidence="0.984468" genericHeader="conclusions">
5 Structural Rules
</sectionHeader>
<bodyText confidence="0.9914745">
We now briefly examine the interaction of struc-
tural rules with parsing. In intuitionistic sequent
systems, structural rules define ways of subtract-
ing, adding, and reordering hypotheses in sequents
during proofs. We have the three following struc-
tural rules:
</bodyText>
<listItem confidence="0.988854666666667">
• Interchange, which allows to use hypotheses
in any order
• Contraction, which allows to use a hypothesis
more than once
• Thinning, which says that not all hypotheses
need to be used
</listItem>
<subsectionHeader confidence="0.9920395">
5.1 Programs as Unordered Sets of
Hypotheses
</subsectionHeader>
<bodyText confidence="0.999939947368421">
All of the structural rules above are implicit in
proof rules (I)-(V), and they are all needed to ob-
tain intuitionistic soundness and completeness as
in [7]. By contrast, Lambek&apos;s propositional calcu-
lus does not have any of the structural rules; for
instance, Interchange is not admitted, since the
hypotheses deriving the type of a given string must
also account for the positions of the words to which
they have been assigned as types, and must obey
the strict string adjacency requirement between
functions and arguments of classical CG. Thus,
Lambek&apos;s calculus must assume ordered lists of
hypotheses, so as to account for word-order con-
straints. Under our approach, word-order con-
straints are obtained declaratively, via sharing of
string positions, and there is no strict adjacency
requirement. In proof-theoretical terms, this di-
rectly translates in viewing programs as unordered
sets of hypotheses.
</bodyText>
<subsectionHeader confidence="0.9786555">
5.2 Trading Contraction against
Decidability
</subsectionHeader>
<bodyText confidence="0.99995864516129">
The logic defined by rules (I)-(V) is in general
undecidable, but it becomes decidable as soon as
Contraction is disallowed. In fact, if a given hy-
pothesis can be used at most once, then clearly the
number of internal nodes in a proof tree for a se-
quent P = G is at most equal to the total number
of occurrences of A and 3 in P G, since these
are the logical constants for which proof rules with
corresponding inference figures have been defined.
Hence, no proof tree can contain infinite branches
and decidability follows.
Now, it seems a plausible conjecture that the
programs directly defined by input strings as in
Section 4.2 never need Contraction. In fact, each
time we use a hypothesis in the proof, either we
consume a corresponding word in the input string,
or we consume a &amp;quot;virtual&amp;quot; constituent correspond-
ing to a step of hypothesis introduction deter-
mined by rule (V) for implications. (Construc-
tions like parasitic gaps can be accounted for by as-
sociating specific lexical items with clauses which
determine the simultaneous introduction of gaps of
the same type.) If this conjecture can be formally
confirmed, then we could automate our formalism
via a metainterpreter based on rules (I)-(V), but
implemented in such a way that clauses are re-
moved from programs as soon as they are used.
Being based on a decidable fragment of logic, such
a metainterpreter would not be affected by the
kind of infinite loops normally characterizing DCG
parsing.
</bodyText>
<subsectionHeader confidence="0.678396">
5.3 Thinning and Vacuous Abstrac-
tion
</subsectionHeader>
<bodyText confidence="0.999816">
Thinning can cause problems of overgeneration,
as hypotheses introduced via rule (V) may end up
as being never used, since other hypotheses can be
used instead. For instance, the type assignment
</bodyText>
<equation confidence="0.860652733333333">
(7) which : VvVyVw[CONN(which, v — 1, v) A
(NP(y, y) S(v, w)) A
v&lt;yAy&lt;w
275
Vu {NP(3, 3)} CONN(John, 1, 2) (II
U INP(3,3)) NP(1,2) u {NP(3,3)) NP(3,3)
(III)
P u {NP(3, 3)} CONN(loves, 2, 3) P u { NP(3, 3)) NP(1, 2) A NP(3, 3)
(III)
VU {NP(3,3)} CONN(loves, 2, 3) A NP(1, 2) A NP(3, 3) (II)
Pu INP(3,3)) S(1,3) (V)
CONN(whom, 0,1) P NP(3, 3) S(1, 3) (III)
P CONN(whom, 0, 1) A (NP(3, 3) --. S(1, 3))
(II)
REL(0, 3)
</equation>
<figureCaption confidence="0.993818">
Figure 1: Type derivation for whom John loves
</figureCaption>
<bodyText confidence="0.990753435897436">
REL(v — 1 , w)
can be used to account for the well-formedness of
both
which [ I shall put a book on
and
which [ I shall put on the table ]
but will also accept the ungrammatical
which [ I shall put a book on the table ]
In fact, as we do not have to use all the hy-
potheses, in this last case the virtual noun-phrase
corresponding to the extraction site is added to
the program but is never used. Notice that our
conjecture in section 4.4.2 was that Contraction
is not needed to prove the theorems correspond-
ing to the types of grammatical strings; by con-
trast, Thinning givcs us more theorems than we
want. As a consequence, eliminating Thinning
would compromise the proof-theoretic properties
of (I)-(V) with respect to intuitionistic logic, and
the corresponding Kripke models semantics of our
programming language. &apos;
There is however a formally well defined way to
account for the ungrammaticality of the example
above without changing the logical properties of
our inference system. We can encode proofs as
terms of Lambda Calculus and then filter certain
kinds of proof terms. In particular, a hypothesis
introduction, determined by rule (V), corresponds
to a step of )-abstraction, while a hypothesis elim-
ination, determined by one of rules (I)-(II), cor-
responds to a step of functional application and
.A-contraction. Hypotheses which are introduced
but never eliminated result in corresponding cases
of vacuous abstraction. Thus, the three examples
above have the three following Lambda encodings
of the proof of the sentence for which an extraction
site is hypothesized, where the last ungrammatical
example corresponds to a case of vacuous abstrac-
tion:
</bodyText>
<listItem confidence="0.999500666666667">
• Ax putaa book], [on x], I)
• Ax put(x, [on the table], I)
• Ax putaa book], [on the table], I)
</listItem>
<bodyText confidence="0.999947956521739">
Constraints for filtering proof terms character-
ized by vacuous abstraction can be defined in
a straightforward manner, particularly if we are
working with a metainterpreter implemented on
top of a language based on Lambda terms, such as
Lambda-Prolog [8, 9]. Beside the desire to main-
tain certain well defined proof-theoretic and se-
mantic properties of our inference system, there
are other reasons for using this strategy instead
of disallowing Thinning. Indeed, our target here
seems specifically to be the elimination of vacuous
Lambda abstraction. Absence of vacuous abstrac-
tion has been proposed by Steedman [17] as a uni-
versal property of human languages. Morrill and
Carpenter [11] show that other well-formedness
constraints formulated in different grammatical
theories such as GPSG, LFG and GB reduce to
this same property. Moreover, Thinning gives us
a straightforward way to account for situations of
lexical ambiguity, where the program defined by a
certain input string can in fact contain hypothe-
ses which are not needed to derive the type of the
string.
</bodyText>
<sectionHeader confidence="0.999548" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995415">
[1] Bar-Hillel, Yehoshua. 1953.
A Quasi-arithmetical Notation for Syntactic
Description. Language. 29. pp47-58.
[2] Huet, Gerard 1986. Formal Structures for
Computation and Deduction. Unpublished
lecture notes. Carnegie-Mellon University.
</reference>
<page confidence="0.975627">
276
</page>
<reference confidence="0.999979881578948">
[3] Gabbay, D. M., and U. Reyle. 1984. N-Prolog:
An Extension of Prolog with Hypothetical Im-
plications. I The Journal of Logic Program-
ming. 1. pp319-355.
[4] Joshi, Aravind. 1987. Word-order Variation
in Natural Language Generation. In Proceed-
ings of the National Conference on Artificial
Intelligence (AAAI 87), Seattle.
[5] Karttunen, Lauri. 1986. Radical Lexicalism.
Report No. CSLI-86-68. CSLI, Stanford Uni-
versity.
[6] Lambek, Joachim. 1958. The Mathematics of
Sentence Structure. American Mathematical
Monthly. 65. pp363-386.
[7] Miller, Dale. 1987. A Logical Analysis of Mod-
ules in Logic Programming. To appear in the
Journal of Logic Programming.
[8] Miller; Dale and Gopalan Nadathur. 1986.
Some Uses of Higher-order Logic in Com-
putational Linguistics. In Proceedings of the
24th Annual Meeting of the Association for
Computational Linguistics, Columbia Uni-
versity.
[9] Miller, Dale and Gopalan Nadathur. 1987. A
Logic Programming Approach to Manipulat-
ing Formulas and Programs. Paper presented
at the IEEE Fourth Symposium on Logic Pro-
gramming, San Francisco.
[10] Moortgat, Michael. 1987. Lambek Theorem
Proving. Paper presented at the ZWO work-
shop Categorial Grammar: Its Current State.
June 4-5 1987, ITLI Amsterdam.
[11] Morrill, Glyn and Bob Carpenter 1987.
Cornpositionality, Implication.al Logic and
Theories of Grammar. Research Paper
EUCCS/RP-11, University of Edinburgh,
Centre for Cognitive Science.
[12] Pareschi, Remo and Mark J. Steedman. 1987.
A Lazy Way to Chart-parse with Categorial
Grammars. In Proceedings of the 25th An-
nual Meeting of the Association for Compu-
tational Linguistics, Stanford University.
[13] Pereira, Fernando C. N. and Stuart M.
Shieber. 1987. Prolog and Natural Language
Analysis. CSLI Lectures Notes No. 10. CSLI,
Stanford University.
[14] Pereira, Fernando C. N. and David II. D.
Warren. 1980. Definite Clauses for Language
Analysis. Artificial Intelligence. 13. pp231-
278.
[15] Rounds, William C. 1987. LFP: A Logic for
Linguistic Descriptions and an Analysis of Its
Complexity. Technical Report No. 9. The Uni-
versity of Michigan. To appear in Computa-
tional Linguistics.
[16] Steedman, Mark J. 1985. Dependency and
Coordination in the Grammar of Dutch and
English. Language, 61, pp523-568
[17] Steedman, Mark J. 1987. Combinatory Gram-
mar and Parasitic Gaps. To appear in Natu-
ral Language and Linguistic Theory.
[18] Uszkoreit, Hans. 1986. Categorial Unification
Grammar. In Proceedings of the 11th Inter-
national Conference of Computational Lin-
guistics, Bonn.
[19] Wittenburg, Kent. 1987. Predictive Combina-
tors for the Efficient Parsing of Combinatory
Grammars. In Proceedings of the 25th An-
nual Meeting of the Association for Compu-
tational Linguistics, Stanford University.
[20] Zeevat, H., Klein, E., and J. Calder. 1987. An
Introduction to Unification Categorial Gram-
mar. In N. Haddock et al. (eds.), Edinburgh
Working Papers in Cognitive Science, 1: Cat-
egorial Grammar, Unification Grammar, and
Parsing.
</reference>
<page confidence="0.997265">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.231675">
<title confidence="0.9889805">A DEFINITE CLAUSE VERSION OF CATEGORIAL GRAMMAR</title>
<author confidence="0.996813">Remo Pareschi</author>
<affiliation confidence="0.9998455">Department of Computer and Information Science, University of Pennsylvania,</affiliation>
<address confidence="0.544886">S. St., Philadelphia, PA 191041 and</address>
<affiliation confidence="0.9813085">Department of Artificial Intelligence and Centre for Cognitive Science, University of Edinburgh, 2 Buccleuch Place,</affiliation>
<address confidence="0.997488">Edinburgh EH8 9LW, Scotland</address>
<email confidence="0.99987">remo@linc.cis.upenn.edu</email>
<abstract confidence="0.96692025">We introduce a first-order version of Categorial Grammar, based on the idea of encoding syntactic types as definite clauses. Thus, we drop all explicit requirements of adjacency between combinable constituents, and we capture wordorder constraints simply by allowing subformulae of complex types to share variables ranging over string positions. We are in this way able to account for constructions involving discontinuous constituents. Such constructions are difficult to handle in the more traditional version of Categorial Grammar, which is based on propositional types and on the requirement of strict string adjacency between combinable constituents. We show then how, for this formalism, parsing can be efficiently implemented as theorem proving. Our approach to encoding types-as definite clauses presupposes a modification of standard Horn logic syntax to allow internal implications in definite clauses. This modification is needed to account for the types of higher-order functions and, as a consequence, standard Prolog-like Horn logic theorem proving is not powerful enough. We tackle this •I am indebted to Dale Miller for help and advice. I am also grateful to Aravind Joshi, Mark Steedman, David Weir, Bob Frank, Mitch Marcus and Yves Schabes for comments and discussions. Thanks are due to Elsa Gunter and for advice on typesetting. Parts of this research were supported by: a Sloan foundation grant to the Cognitive Science Program, Univ. of Pennsylvania; and NSF grants MCS-8219196-CER, IRI-10413 A02, ARO grants DAA29-84-K-0061, DAA29-84-9-0027 and DARPA grant N00014-85-K0018 to CIS, Univ. of Pennsylvania. t Address for correspondence problem by adopting an intuitionistic treatment of implication, which has already been proposed elsewhere as an extension of Prolog for implementing hypothetical reasoning and modular logic programming.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
</authors>
<title>A Quasi-arithmetical Notation for Syntactic Description.</title>
<date>1953</date>
<journal>Language.</journal>
<volume>29</volume>
<pages>47--58</pages>
<contexts>
<context position="12956" citStr="[1]" startWordPosition="2102" endWordPosition="2102">ach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences of types, or by viewing the type of a string as a logical consequence of the types of its words. The first alternative has been explored mainly in Combinatory Grammar [16, 17], where, beside the rewrite rule of functional application, which was already in the initial formulation of CG in [1], there are also the rules of functional composition and type raising, which are used to account for extraction and coordination phenomena. This approach offers a psychologically attractive model of parsing, based on the idea of incremental processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed for cases of extraction and coordination, in principle nothing prevents its use to analyze strings not characterized b</context>
</contexts>
<marker>[1]</marker>
<rawString>Bar-Hillel, Yehoshua. 1953. A Quasi-arithmetical Notation for Syntactic Description. Language. 29. pp47-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Huet</author>
</authors>
<title>Formal Structures for Computation and Deduction. Unpublished lecture notes.</title>
<date>1986</date>
<institution>Carnegie-Mellon University.</institution>
<contexts>
<context position="5292" citStr="[2]" startWordPosition="821" endWordPosition="821">nds to the type S(0, j). Constraints over the order of constituents are enforced by sharing integer indices across subformulae inside complex (functional) types. This first-order version of CG can be viewed as a logical reconstruction of some of the ideas behind the recent trend of Categorial Unification Grammars [5, 18, 20]&apos;. A strongly analogous development characterizes the systems of type-assignment for the formal languages of Combinatory Logic and Lambda Calculus, leading from propositional type systems to the &amp;quot;formulae-as-types&amp;quot; slogan which is behind the current research in type theory [2]. In this paper, we show how syntactic types can be encoded using an extended version of standard Horn logic syntax. 2 Definite Clauses with Internal Implications Let A and —■ be logical connectives for conjunction and implication, and let V and 3 be the univer&apos;Indeed, Uszkoreit 1181 mentions the possibility of encoding order constraints among constituents via variables ranging over string positions in the DCG style. sal and existential quantifiers. Let A be a syntactic variable ranging over the set of atoms, i. e. the set of atomic first-order formulae, and let D and G be syntactic variables </context>
<context position="8756" citStr="[2]" startWordPosition="1423" endWordPosition="1423"> taking a right-oriented function from noun-phrases to sentences and returning a relative clause3. This kind of type-assignment has been used by several linguists to provide attractive accounts of certain cases of extraction [16, 17, 10]. (5) which : REL/(S/ NP) In our definite clause version of CG, a similar assignment, exemplified by (6), is possible, since implications are allowed in the, body of clauses. Notice that in (6) the noun-phrase needed to fill the extraction site is &amp;quot;virtual&amp;quot;, having null length. (6) which : VvVy[CONN(which, v — 1, v) A (NP(Y) Y) S(V, Y)) -- REL(v — 1, y)] 2 See [2] for a pleasant formal characterization of firstorder definite clauses as type declarations. 3For simplicity sake, we treat here relative clauses as constituents of atomic type. But in reality relative clauses are noun modifiers, that is, functions from nouns to nouns. Therefore, the propositional and the first-order atomic type for relative clauses in the examples below should be thought of as shorthands for corresponding complex types. 3.3 Arithmetic Predicates The fact that we quantify over integers allows us to use arithmetic predicates to determine subsets of indices over which certain va</context>
<context position="16358" citStr="[2]" startWordPosition="2669" endWordPosition="2669">iller [7] shows to be sound and complete for intuitionistic logic, and to have a well defined semantics in terms of 273 Kripke models. 4.1.1 Logic Programs We take a logic program or, simply, a program P to be any set of definite clauses. We formally represent the fact that a goal clause G is logically derivable from a program P with a sequent of the form P G, where P and G are, respectively, the antecedent and the succedent of the sequent. If P is a program then we take its substitution closure [P] to be the smallest set such that • P C [7:1 • if D1 A D2 E pi then DI E VI and D2 E • if VxD E [2] then [xlt]D E [P) for all terms I, where [zit] denotes the result of substituting I for free occurrences of t in D 4.1.2 Proof Rules We introduce now the following proof rules, which define the notion of proof for our logic programming language: (I) P G if G E [P] P G (II) if G A E [P] (III) P Gi A G2 P (IV) [xlt]G P 3xG V) P U {D} G ( P D G In the inference figures for rules (II) - (V), the sequent(s) appearing above the horizontal line are the upper seguent(s), while the sequent appearing below is the lower sequent. A proof for a sequent P G is a tree whose nodes are labeled with sequents s</context>
</contexts>
<marker>[2]</marker>
<rawString>Huet, Gerard 1986. Formal Structures for Computation and Deduction. Unpublished lecture notes. Carnegie-Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Gabbay</author>
<author>U Reyle</author>
</authors>
<title>N-Prolog: An Extension of Prolog with Hypothetical Implications.</title>
<date>1984</date>
<journal>I The Journal of Logic Programming.</journal>
<volume>1</volume>
<pages>319--355</pages>
<contexts>
<context position="6684" citStr="[3]" startWordPosition="1067" endWordPosition="1067">definitions for the corresponding syntactic variables D and G: • D := A I G A IVxD I Di A D2 • G:=AIGiAG213xGID—*G We call ground a clause not containing variables. We refer to the part of a non-atomic definite clause coming on the left of the implication connective as to the body of the clause, and to the one on the right as to the head. With respect to standard Horn logic syntax, the main novelty in the definitions above is that we permit implications in goals and in the bodies of definite clauses. Extended Horn logic syntax of this kind has been proposed to implement hypothetical reasoning [3] and modules [7] in logic programming. We shall first make clear the use of this extension for the purpose of linguistic description, and we shall then illustrate its operational meaning. 3 First-order Categorial Grammar 3.1 Definite Clauses as Types We take CONN (for &amp;quot;connects&amp;quot;) to be a threeplace predicate defined over lexical items and pairs of integers, such that CONN(item,i,j) holds if and only if and only if i = j — 1, with the intuitive meaning that item lies between the two consecutive string positions i and j. Then, a most direct way to translate in first-order logic the type-assignme</context>
<context position="15475" citStr="[3]" startWordPosition="2497" endWordPosition="2497">opositional logic, with directional connectives and adjacency requirements. Indeed, the expressive power of standard first-order logic completely eliminates the need for this kind of specialization, and at the same time provides the ability to account for constructions which, as shown in section 3.3.1, are problematic for an (albeit specialized) propositional framework. 4.1 An Intuitionistic Extension of Prolog The inference system we are going to introduce below has been proposed in [7] as an extension of Prolog suitable for modular logic programming. A similar extension has been proposed in [3] to implement hypotethical reasoning in logic programming. We are thus dealing with what can be considered the specification of a general purpose logic programming language. The encoding of a particular linguistic formalism is but one other application of such a language, which Miller [7] shows to be sound and complete for intuitionistic logic, and to have a well defined semantics in terms of 273 Kripke models. 4.1.1 Logic Programs We take a logic program or, simply, a program P to be any set of definite clauses. We formally represent the fact that a goal clause G is logically derivable from a</context>
</contexts>
<marker>[3]</marker>
<rawString>Gabbay, D. M., and U. Reyle. 1984. N-Prolog: An Extension of Prolog with Hypothetical Implications. I The Journal of Logic Programming. 1. pp319-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
</authors>
<title>Word-order Variation in Natural Language Generation.</title>
<date>1987</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI 87),</booktitle>
<location>Seattle.</location>
<contexts>
<context position="12152" citStr="[4]" startWordPosition="1968" endWordPosition="1968">left- or right-adjacency, since they have two or more parts disconnected by intervening lexical material, or by internal extraction sites. Extraposition phenomena, gapping constructions in coordinate structures, and the distribution of adverbials offer other problematic examples of English discontinuous constructions for which this first-order framework seems to promise well. A much larger batch of similar phenomena is offered by languages with freer word order than English, for which, as pointed out in [5, 18], classical CG suffers from an even clearer lack of expressive power. Indeed, Joshi [4] proposes within the TAG framework an attractive general solution to word-order variations phenomena in terms of linear precedence relations among constituents. Such a solution suggests a similar approach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences of types, or by viewing the type of a string as a logical consequence of the types of it</context>
</contexts>
<marker>[4]</marker>
<rawString>Joshi, Aravind. 1987. Word-order Variation in Natural Language Generation. In Proceedings of the National Conference on Artificial Intelligence (AAAI 87), Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<date>1986</date>
<tech>Radical Lexicalism. Report No. CSLI-86-68.</tech>
<institution>CSLI, Stanford University.</institution>
<contexts>
<context position="5015" citStr="[5, 18, 20]" startWordPosition="780" endWordPosition="782">mulae obtained from two-place predicates over string positions represented as integers, the first and the second argument corresponding, respectively, to the left and right end of a given string. Therefore, the set of all sentences of length j generated from a certain lexicon corresponds to the type S(0, j). Constraints over the order of constituents are enforced by sharing integer indices across subformulae inside complex (functional) types. This first-order version of CG can be viewed as a logical reconstruction of some of the ideas behind the recent trend of Categorial Unification Grammars [5, 18, 20]&apos;. A strongly analogous development characterizes the systems of type-assignment for the formal languages of Combinatory Logic and Lambda Calculus, leading from propositional type systems to the &amp;quot;formulae-as-types&amp;quot; slogan which is behind the current research in type theory [2]. In this paper, we show how syntactic types can be encoded using an extended version of standard Horn logic syntax. 2 Definite Clauses with Internal Implications Let A and —■ be logical connectives for conjunction and implication, and let V and 3 be the univer&apos;Indeed, Uszkoreit 1181 mentions the possibility of encoding o</context>
<context position="12065" citStr="[5, 18]" startWordPosition="1953" endWordPosition="1954">, that is, constituents where the function-argument relation is not determined in terms of left- or right-adjacency, since they have two or more parts disconnected by intervening lexical material, or by internal extraction sites. Extraposition phenomena, gapping constructions in coordinate structures, and the distribution of adverbials offer other problematic examples of English discontinuous constructions for which this first-order framework seems to promise well. A much larger batch of similar phenomena is offered by languages with freer word order than English, for which, as pointed out in [5, 18], classical CG suffers from an even clearer lack of expressive power. Indeed, Joshi [4] proposes within the TAG framework an attractive general solution to word-order variations phenomena in terms of linear precedence relations among constituents. Such a solution suggests a similar approach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences o</context>
</contexts>
<marker>[5]</marker>
<rawString>Karttunen, Lauri. 1986. Radical Lexicalism. Report No. CSLI-86-68. CSLI, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<date>1958</date>
<journal>The Mathematics of Sentence Structure. American Mathematical Monthly.</journal>
<volume>65</volume>
<pages>363--386</pages>
<contexts>
<context position="13796" citStr="[6]" startWordPosition="2229" endWordPosition="2229">tal processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed for cases of extraction and coordination, in principle nothing prevents its use to analyze strings not characterized by such phenomena, which would be analyzable in terms of functional application alone. Tentative solutions of this problem have been recently discussed in [12, 19]. The second alternative has been undertaken in the late fifties by Lambek [6] who defined a decision procedure for bidirectional propositional CG in terms of a Gentzen-style sequent system. Lambek&apos;s implicational calculus of syntactic types has recently enjoyed renewed interest in the works of van Benthem, Moortgat and other scholars. This approach can account for a range of syntactic phenomena similar to that of Combinatory Grammar, and in fact many of the rewrite rules of Combinatory Grammar can be derived as theorems in the calculus. However, analyses of cases of extraction and coordination are here obtained via inferences over the internal implications in the types</context>
</contexts>
<marker>[6]</marker>
<rawString>Lambek, Joachim. 1958. The Mathematics of Sentence Structure. American Mathematical Monthly. 65. pp363-386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Miller</author>
</authors>
<title>A Logical Analysis of Modules in Logic Programming.</title>
<date>1987</date>
<journal>the Journal of Logic Programming.</journal>
<note>To appear in</note>
<contexts>
<context position="6700" citStr="[7]" startWordPosition="1071" endWordPosition="1071">the corresponding syntactic variables D and G: • D := A I G A IVxD I Di A D2 • G:=AIGiAG213xGID—*G We call ground a clause not containing variables. We refer to the part of a non-atomic definite clause coming on the left of the implication connective as to the body of the clause, and to the one on the right as to the head. With respect to standard Horn logic syntax, the main novelty in the definitions above is that we permit implications in goals and in the bodies of definite clauses. Extended Horn logic syntax of this kind has been proposed to implement hypothetical reasoning [3] and modules [7] in logic programming. We shall first make clear the use of this extension for the purpose of linguistic description, and we shall then illustrate its operational meaning. 3 First-order Categorial Grammar 3.1 Definite Clauses as Types We take CONN (for &amp;quot;connects&amp;quot;) to be a threeplace predicate defined over lexical items and pairs of integers, such that CONN(item,i,j) holds if and only if and only if i = j — 1, with the intuitive meaning that item lies between the two consecutive string positions i and j. Then, a most direct way to translate in first-order logic the type-assignment (1) is by the</context>
<context position="15364" citStr="[7]" startWordPosition="2479" endWordPosition="2479">rder functions, but at the same time differs radically from it, since we do not need to have a &amp;quot;specialized&amp;quot; propositional logic, with directional connectives and adjacency requirements. Indeed, the expressive power of standard first-order logic completely eliminates the need for this kind of specialization, and at the same time provides the ability to account for constructions which, as shown in section 3.3.1, are problematic for an (albeit specialized) propositional framework. 4.1 An Intuitionistic Extension of Prolog The inference system we are going to introduce below has been proposed in [7] as an extension of Prolog suitable for modular logic programming. A similar extension has been proposed in [3] to implement hypotethical reasoning in logic programming. We are thus dealing with what can be considered the specification of a general purpose logic programming language. The encoding of a particular linguistic formalism is but one other application of such a language, which Miller [7] shows to be sound and complete for intuitionistic logic, and to have a well defined semantics in terms of 273 Kripke models. 4.1.1 Logic Programs We take a logic program or, simply, a program P to be</context>
<context position="20555" citStr="[7]" startWordPosition="3435" endWordPosition="3435">tructural rules with parsing. In intuitionistic sequent systems, structural rules define ways of subtracting, adding, and reordering hypotheses in sequents during proofs. We have the three following structural rules: • Interchange, which allows to use hypotheses in any order • Contraction, which allows to use a hypothesis more than once • Thinning, which says that not all hypotheses need to be used 5.1 Programs as Unordered Sets of Hypotheses All of the structural rules above are implicit in proof rules (I)-(V), and they are all needed to obtain intuitionistic soundness and completeness as in [7]. By contrast, Lambek&apos;s propositional calculus does not have any of the structural rules; for instance, Interchange is not admitted, since the hypotheses deriving the type of a given string must also account for the positions of the words to which they have been assigned as types, and must obey the strict string adjacency requirement between functions and arguments of classical CG. Thus, Lambek&apos;s calculus must assume ordered lists of hypotheses, so as to account for word-order constraints. Under our approach, word-order constraints are obtained declaratively, via sharing of string positions, a</context>
</contexts>
<marker>[7]</marker>
<rawString>Miller, Dale. 1987. A Logical Analysis of Modules in Logic Programming. To appear in the Journal of Logic Programming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Miller</author>
<author>Gopalan Nadathur</author>
</authors>
<title>Some Uses of Higher-order Logic in Computational Linguistics.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association</booktitle>
<institution>for Computational Linguistics, Columbia University.</institution>
<contexts>
<context position="25520" citStr="[8, 9]" startWordPosition="4275" endWordPosition="4276">vacuous abstraction. Thus, the three examples above have the three following Lambda encodings of the proof of the sentence for which an extraction site is hypothesized, where the last ungrammatical example corresponds to a case of vacuous abstraction: • Ax putaa book], [on x], I) • Ax put(x, [on the table], I) • Ax putaa book], [on the table], I) Constraints for filtering proof terms characterized by vacuous abstraction can be defined in a straightforward manner, particularly if we are working with a metainterpreter implemented on top of a language based on Lambda terms, such as Lambda-Prolog [8, 9]. Beside the desire to maintain certain well defined proof-theoretic and semantic properties of our inference system, there are other reasons for using this strategy instead of disallowing Thinning. Indeed, our target here seems specifically to be the elimination of vacuous Lambda abstraction. Absence of vacuous abstraction has been proposed by Steedman [17] as a universal property of human languages. Morrill and Carpenter [11] show that other well-formedness constraints formulated in different grammatical theories such as GPSG, LFG and GB reduce to this same property. Moreover, Thinning gives</context>
</contexts>
<marker>[8]</marker>
<rawString>Miller; Dale and Gopalan Nadathur. 1986. Some Uses of Higher-order Logic in Computational Linguistics. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Miller</author>
<author>Gopalan Nadathur</author>
</authors>
<title>A Logic Programming Approach to Manipulating Formulas and Programs.</title>
<date>1987</date>
<booktitle>Paper presented at the IEEE Fourth Symposium on Logic Programming,</booktitle>
<location>San Francisco.</location>
<contexts>
<context position="17601" citStr="[9]" startWordPosition="2910" endWordPosition="2910">th P G, (ii) the internal nodes are instances of one of proof rules (II) - (V) and (iii) the leaf nodes are labeled with sequents representing proof rule (I). The height of a proof is the length of the longest path from the root to some leaf. The size of a proof is the number of nodes in it. Thus, proof rules (I)-(V) provide the abstract specification of a first-order theorem prover which can then be implemented in terms of depth-first search, backtracking and unification like a Prolog interpreter. (An example of such an implementation, as a metainterpreter on top of LambdaProlog, is given in [9].) Observe however that an important difference of such a theorem prover from a standard Prolog interpreter is in the wider distribution of &amp;quot;logical&amp;quot; variables, which, in the logic programming tradition, stand for existentially quantified variables within goals. Such variables can get instantiated in the course of a Prolog proof, thus providing the procedural ability to return specific values as output of the computation. Logical variables play the same role in the programming language we are considering here; moreover, they can also occur in program clauses, since subformulae of goal clauses </context>
<context position="25520" citStr="[8, 9]" startWordPosition="4275" endWordPosition="4276">vacuous abstraction. Thus, the three examples above have the three following Lambda encodings of the proof of the sentence for which an extraction site is hypothesized, where the last ungrammatical example corresponds to a case of vacuous abstraction: • Ax putaa book], [on x], I) • Ax put(x, [on the table], I) • Ax putaa book], [on the table], I) Constraints for filtering proof terms characterized by vacuous abstraction can be defined in a straightforward manner, particularly if we are working with a metainterpreter implemented on top of a language based on Lambda terms, such as Lambda-Prolog [8, 9]. Beside the desire to maintain certain well defined proof-theoretic and semantic properties of our inference system, there are other reasons for using this strategy instead of disallowing Thinning. Indeed, our target here seems specifically to be the elimination of vacuous Lambda abstraction. Absence of vacuous abstraction has been proposed by Steedman [17] as a universal property of human languages. Morrill and Carpenter [11] show that other well-formedness constraints formulated in different grammatical theories such as GPSG, LFG and GB reduce to this same property. Moreover, Thinning gives</context>
</contexts>
<marker>[9]</marker>
<rawString>Miller, Dale and Gopalan Nadathur. 1987. A Logic Programming Approach to Manipulating Formulas and Programs. Paper presented at the IEEE Fourth Symposium on Logic Programming, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Moortgat</author>
</authors>
<title>Lambek Theorem Proving. Paper presented at the ZWO workshop Categorial Grammar: Its Current State.</title>
<date>1987</date>
<location>ITLI Amsterdam.</location>
<contexts>
<context position="8390" citStr="[16, 17, 10]" startWordPosition="1354" endWordPosition="1356">ypes assigned to &amp;quot;non-functional&amp;quot; lexical items. For example, we can have for the noun-phrase Mary the type-assignment (4). (4) Mary: Vy[CONN(Mary, y — 1, y) NP(y — 1, y)] 3.2 Higher-order Types and Internal Implications Propositional CG makes crucial use of functions of higher-order type. For example, the typeassignment (5) makes the relative pronoun which into a function taking a right-oriented function from noun-phrases to sentences and returning a relative clause3. This kind of type-assignment has been used by several linguists to provide attractive accounts of certain cases of extraction [16, 17, 10]. (5) which : REL/(S/ NP) In our definite clause version of CG, a similar assignment, exemplified by (6), is possible, since implications are allowed in the, body of clauses. Notice that in (6) the noun-phrase needed to fill the extraction site is &amp;quot;virtual&amp;quot;, having null length. (6) which : VvVy[CONN(which, v — 1, v) A (NP(Y) Y) S(V, Y)) -- REL(v — 1, y)] 2 See [2] for a pleasant formal characterization of firstorder definite clauses as type declarations. 3For simplicity sake, we treat here relative clauses as constituents of atomic type. But in reality relative clauses are noun modifiers, that</context>
<context position="10639" citStr="[10]" startWordPosition="1725" endWordPosition="1725">ith an upward-looking arrow.) which [ I shall put a book on However, a case of non-peripheral extraction, where the extraction site is in the middle, such as which [ I shall put T on the table ] is difficult to describe in bidirectional propositional CG, where all functions must take left- or right-adjacent arguments. For instance, a solution like the one proposed in [17] involves permuting the arguments of a given function. Such an operation needs to be rather cumbersomely constrained in an explicit way to cases of extraction, lest it should wildly overgenerate. Another solution, proposed in [10], is also cumbersome and counterintuitive, in that involves the assignment of multiple types to wh-expressions, one for each site where extraction can take place. On the other hand, the greater expressive power of first-order logic allows us to elegantly generalize the type-assignment (6) to the type-assignment (7). In fact, in (7) the variable identifying the extraction site ranges over the set of integers in between the indices corresponding, respectively, to the left and right end of the sentence on which the relative pronoun operates. Therefore, such a sentence can have an extraction site </context>
</contexts>
<marker>[10]</marker>
<rawString>Moortgat, Michael. 1987. Lambek Theorem Proving. Paper presented at the ZWO workshop Categorial Grammar: Its Current State. June 4-5 1987, ITLI Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glyn Morrill</author>
<author>Bob Carpenter</author>
</authors>
<date>1987</date>
<booktitle>Cornpositionality, Implication.al Logic and Theories of Grammar. Research Paper EUCCS/RP-11,</booktitle>
<institution>University of Edinburgh, Centre for Cognitive Science.</institution>
<marker>[11]</marker>
<rawString>Morrill, Glyn and Bob Carpenter 1987. Cornpositionality, Implication.al Logic and Theories of Grammar. Research Paper EUCCS/RP-11, University of Edinburgh, Centre for Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remo Pareschi</author>
<author>Mark J Steedman</author>
</authors>
<title>A Lazy Way to Chart-parse with Categorial Grammars.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association</booktitle>
<institution>for Computational Linguistics, Stanford University.</institution>
<contexts>
<context position="13718" citStr="[12, 19]" startWordPosition="2215" endWordPosition="2216">offers a psychologically attractive model of parsing, based on the idea of incremental processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed for cases of extraction and coordination, in principle nothing prevents its use to analyze strings not characterized by such phenomena, which would be analyzable in terms of functional application alone. Tentative solutions of this problem have been recently discussed in [12, 19]. The second alternative has been undertaken in the late fifties by Lambek [6] who defined a decision procedure for bidirectional propositional CG in terms of a Gentzen-style sequent system. Lambek&apos;s implicational calculus of syntactic types has recently enjoyed renewed interest in the works of van Benthem, Moortgat and other scholars. This approach can account for a range of syntactic phenomena similar to that of Combinatory Grammar, and in fact many of the rewrite rules of Combinatory Grammar can be derived as theorems in the calculus. However, analyses of cases of extraction and coordinatio</context>
</contexts>
<marker>[12]</marker>
<rawString>Pareschi, Remo and Mark J. Steedman. 1987. A Lazy Way to Chart-parse with Categorial Grammars. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>Prolog and Natural Language Analysis.</title>
<date>1987</date>
<journal>CSLI Lectures Notes No.</journal>
<volume>10</volume>
<institution>CSLI, Stanford University.</institution>
<contexts>
<context position="3883" citStr="[13]" startWordPosition="604" endWordPosition="604">edent. A convention more or less generally followed by linguists working in CG is to have the antecedent and the consequent of an implication respectively on 270 the right and on the left of the connective. Thus, the type-assignment (1) says that the ditransitive verb put is a function taking a right-adjacent argument of type NP, to return a function taking a right-adjacent argument of type PP, to return a function taking a left-adjacent argument of type NP, to finally return an expression of the atomic type S. (1) put: ((S\NP)/PP)/NP The Definite Clause Grammar (DCG) framework [14] (see also [13]), where phrase-structure grammars can be encoded as sets of definite clauses (which are themselves a subset of Horn clauses), and the formalization of some aspects of it in [15], suggests a more expressive alternative to encode word-order constraints in CG. Such an alternative eliminates all notions of directionality from the logical connectives, and any explicit requirement of adjacency between functions and arguments, and replaces propositions with first-order formulae. Thus, atomic types are viewed as atomic formulae obtained from two-place predicates over string positions represented as i</context>
</contexts>
<marker>[13]</marker>
<rawString>Pereira, Fernando C. N. and Stuart M. Shieber. 1987. Prolog and Natural Language Analysis. CSLI Lectures Notes No. 10. CSLI, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Warren</author>
</authors>
<title>Definite Clauses for Language Analysis.</title>
<date>1980</date>
<journal>Artificial Intelligence.</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="3868" citStr="[14]" startWordPosition="601" endWordPosition="601">ed in the antecedent. A convention more or less generally followed by linguists working in CG is to have the antecedent and the consequent of an implication respectively on 270 the right and on the left of the connective. Thus, the type-assignment (1) says that the ditransitive verb put is a function taking a right-adjacent argument of type NP, to return a function taking a right-adjacent argument of type PP, to return a function taking a left-adjacent argument of type NP, to finally return an expression of the atomic type S. (1) put: ((S\NP)/PP)/NP The Definite Clause Grammar (DCG) framework [14] (see also [13]), where phrase-structure grammars can be encoded as sets of definite clauses (which are themselves a subset of Horn clauses), and the formalization of some aspects of it in [15], suggests a more expressive alternative to encode word-order constraints in CG. Such an alternative eliminates all notions of directionality from the logical connectives, and any explicit requirement of adjacency between functions and arguments, and replaces propositions with first-order formulae. Thus, atomic types are viewed as atomic formulae obtained from two-place predicates over string positions r</context>
</contexts>
<marker>[14]</marker>
<rawString>Pereira, Fernando C. N. and David II. D. Warren. 1980. Definite Clauses for Language Analysis. Artificial Intelligence. 13. pp231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
</authors>
<title>LFP: A Logic for Linguistic Descriptions and an Analysis of Its Complexity.</title>
<date>1987</date>
<tech>Technical Report No. 9.</tech>
<institution>The University of Michigan.</institution>
<note>To appear in Computational Linguistics.</note>
<contexts>
<context position="4061" citStr="[15]" startWordPosition="634" endWordPosition="634"> the left of the connective. Thus, the type-assignment (1) says that the ditransitive verb put is a function taking a right-adjacent argument of type NP, to return a function taking a right-adjacent argument of type PP, to return a function taking a left-adjacent argument of type NP, to finally return an expression of the atomic type S. (1) put: ((S\NP)/PP)/NP The Definite Clause Grammar (DCG) framework [14] (see also [13]), where phrase-structure grammars can be encoded as sets of definite clauses (which are themselves a subset of Horn clauses), and the formalization of some aspects of it in [15], suggests a more expressive alternative to encode word-order constraints in CG. Such an alternative eliminates all notions of directionality from the logical connectives, and any explicit requirement of adjacency between functions and arguments, and replaces propositions with first-order formulae. Thus, atomic types are viewed as atomic formulae obtained from two-place predicates over string positions represented as integers, the first and the second argument corresponding, respectively, to the left and right end of a given string. Therefore, the set of all sentences of length j generated fro</context>
<context position="9455" citStr="[15]" startWordPosition="1529" endWordPosition="1529">or simplicity sake, we treat here relative clauses as constituents of atomic type. But in reality relative clauses are noun modifiers, that is, functions from nouns to nouns. Therefore, the propositional and the first-order atomic type for relative clauses in the examples below should be thought of as shorthands for corresponding complex types. 3.3 Arithmetic Predicates The fact that we quantify over integers allows us to use arithmetic predicates to determine subsets of indices over which certain variables must range. This use of arithmetic predicates characterizes also Rounds&apos; ILFP notation [15], which appears in many ways interestingly related to the framework proposed here. We show here below how this capability can be exploited to account for a case of extraction which is particularly problematic for bidirectional propositional CG. 3.3.1 Non-peripheral Extraction Both the propositional type (5) and the firstorder type (6) are good enough to describe the kind of constituent needed by a relative pronoun in the following right-oriented case of peripheral extraction, where the extraction site is located at one end of the sentence. (We indicate the extraction site with an upward-lookin</context>
</contexts>
<marker>[15]</marker>
<rawString>Rounds, William C. 1987. LFP: A Logic for Linguistic Descriptions and an Analysis of Its Complexity. Technical Report No. 9. The University of Michigan. To appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J Steedman</author>
</authors>
<title>Dependency and Coordination in the</title>
<date>1985</date>
<journal>Grammar of Dutch and English. Language,</journal>
<volume>61</volume>
<pages>523--568</pages>
<contexts>
<context position="8390" citStr="[16, 17, 10]" startWordPosition="1354" endWordPosition="1356">ypes assigned to &amp;quot;non-functional&amp;quot; lexical items. For example, we can have for the noun-phrase Mary the type-assignment (4). (4) Mary: Vy[CONN(Mary, y — 1, y) NP(y — 1, y)] 3.2 Higher-order Types and Internal Implications Propositional CG makes crucial use of functions of higher-order type. For example, the typeassignment (5) makes the relative pronoun which into a function taking a right-oriented function from noun-phrases to sentences and returning a relative clause3. This kind of type-assignment has been used by several linguists to provide attractive accounts of certain cases of extraction [16, 17, 10]. (5) which : REL/(S/ NP) In our definite clause version of CG, a similar assignment, exemplified by (6), is possible, since implications are allowed in the, body of clauses. Notice that in (6) the noun-phrase needed to fill the extraction site is &amp;quot;virtual&amp;quot;, having null length. (6) which : VvVy[CONN(which, v — 1, v) A (NP(Y) Y) S(V, Y)) -- REL(v — 1, y)] 2 See [2] for a pleasant formal characterization of firstorder definite clauses as type declarations. 3For simplicity sake, we treat here relative clauses as constituents of atomic type. But in reality relative clauses are noun modifiers, that</context>
<context position="12839" citStr="[16, 17]" startWordPosition="2082" endWordPosition="2083"> variations phenomena in terms of linear precedence relations among constituents. Such a solution suggests a similar approach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences of types, or by viewing the type of a string as a logical consequence of the types of its words. The first alternative has been explored mainly in Combinatory Grammar [16, 17], where, beside the rewrite rule of functional application, which was already in the initial formulation of CG in [1], there are also the rules of functional composition and type raising, which are used to account for extraction and coordination phenomena. This approach offers a psychologically attractive model of parsing, based on the idea of incremental processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed f</context>
</contexts>
<marker>[16]</marker>
<rawString>Steedman, Mark J. 1985. Dependency and Coordination in the Grammar of Dutch and English. Language, 61, pp523-568</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J Steedman</author>
</authors>
<title>Combinatory Grammar and Parasitic Gaps. To appear in Natural Language and Linguistic Theory.</title>
<date>1987</date>
<contexts>
<context position="8390" citStr="[16, 17, 10]" startWordPosition="1354" endWordPosition="1356">ypes assigned to &amp;quot;non-functional&amp;quot; lexical items. For example, we can have for the noun-phrase Mary the type-assignment (4). (4) Mary: Vy[CONN(Mary, y — 1, y) NP(y — 1, y)] 3.2 Higher-order Types and Internal Implications Propositional CG makes crucial use of functions of higher-order type. For example, the typeassignment (5) makes the relative pronoun which into a function taking a right-oriented function from noun-phrases to sentences and returning a relative clause3. This kind of type-assignment has been used by several linguists to provide attractive accounts of certain cases of extraction [16, 17, 10]. (5) which : REL/(S/ NP) In our definite clause version of CG, a similar assignment, exemplified by (6), is possible, since implications are allowed in the, body of clauses. Notice that in (6) the noun-phrase needed to fill the extraction site is &amp;quot;virtual&amp;quot;, having null length. (6) which : VvVy[CONN(which, v — 1, v) A (NP(Y) Y) S(V, Y)) -- REL(v — 1, y)] 2 See [2] for a pleasant formal characterization of firstorder definite clauses as type declarations. 3For simplicity sake, we treat here relative clauses as constituents of atomic type. But in reality relative clauses are noun modifiers, that</context>
<context position="10409" citStr="[17]" startWordPosition="1688" endWordPosition="1688">gh to describe the kind of constituent needed by a relative pronoun in the following right-oriented case of peripheral extraction, where the extraction site is located at one end of the sentence. (We indicate the extraction site with an upward-looking arrow.) which [ I shall put a book on However, a case of non-peripheral extraction, where the extraction site is in the middle, such as which [ I shall put T on the table ] is difficult to describe in bidirectional propositional CG, where all functions must take left- or right-adjacent arguments. For instance, a solution like the one proposed in [17] involves permuting the arguments of a given function. Such an operation needs to be rather cumbersomely constrained in an explicit way to cases of extraction, lest it should wildly overgenerate. Another solution, proposed in [10], is also cumbersome and counterintuitive, in that involves the assignment of multiple types to wh-expressions, one for each site where extraction can take place. On the other hand, the greater expressive power of first-order logic allows us to elegantly generalize the type-assignment (6) to the type-assignment (7). In fact, in (7) the variable identifying the extract</context>
<context position="12839" citStr="[16, 17]" startWordPosition="2082" endWordPosition="2083"> variations phenomena in terms of linear precedence relations among constituents. Such a solution suggests a similar approach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences of types, or by viewing the type of a string as a logical consequence of the types of its words. The first alternative has been explored mainly in Combinatory Grammar [16, 17], where, beside the rewrite rule of functional application, which was already in the initial formulation of CG in [1], there are also the rules of functional composition and type raising, which are used to account for extraction and coordination phenomena. This approach offers a psychologically attractive model of parsing, based on the idea of incremental processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed f</context>
</contexts>
<marker>[17]</marker>
<rawString>Steedman, Mark J. 1987. Combinatory Grammar and Parasitic Gaps. To appear in Natural Language and Linguistic Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Uszkoreit</author>
</authors>
<title>Categorial Unification Grammar.</title>
<date>1986</date>
<booktitle>In Proceedings of the 11th International Conference of Computational Linguistics,</booktitle>
<location>Bonn.</location>
<contexts>
<context position="5015" citStr="[5, 18, 20]" startWordPosition="780" endWordPosition="782">mulae obtained from two-place predicates over string positions represented as integers, the first and the second argument corresponding, respectively, to the left and right end of a given string. Therefore, the set of all sentences of length j generated from a certain lexicon corresponds to the type S(0, j). Constraints over the order of constituents are enforced by sharing integer indices across subformulae inside complex (functional) types. This first-order version of CG can be viewed as a logical reconstruction of some of the ideas behind the recent trend of Categorial Unification Grammars [5, 18, 20]&apos;. A strongly analogous development characterizes the systems of type-assignment for the formal languages of Combinatory Logic and Lambda Calculus, leading from propositional type systems to the &amp;quot;formulae-as-types&amp;quot; slogan which is behind the current research in type theory [2]. In this paper, we show how syntactic types can be encoded using an extended version of standard Horn logic syntax. 2 Definite Clauses with Internal Implications Let A and —■ be logical connectives for conjunction and implication, and let V and 3 be the univer&apos;Indeed, Uszkoreit 1181 mentions the possibility of encoding o</context>
<context position="12065" citStr="[5, 18]" startWordPosition="1953" endWordPosition="1954">, that is, constituents where the function-argument relation is not determined in terms of left- or right-adjacency, since they have two or more parts disconnected by intervening lexical material, or by internal extraction sites. Extraposition phenomena, gapping constructions in coordinate structures, and the distribution of adverbials offer other problematic examples of English discontinuous constructions for which this first-order framework seems to promise well. A much larger batch of similar phenomena is offered by languages with freer word order than English, for which, as pointed out in [5, 18], classical CG suffers from an even clearer lack of expressive power. Indeed, Joshi [4] proposes within the TAG framework an attractive general solution to word-order variations phenomena in terms of linear precedence relations among constituents. Such a solution suggests a similar approach for further work to be pursued within the framework presented here. 4 Theorem Proving In propositional CG, the problem of determining the type of a string from the types of its words has been addressed either by defining certain &amp;quot;combinatory&amp;quot; rules which then determine a rewrite relation between sequences o</context>
</contexts>
<marker>[18]</marker>
<rawString>Uszkoreit, Hans. 1986. Categorial Unification Grammar. In Proceedings of the 11th International Conference of Computational Linguistics, Bonn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kent Wittenburg</author>
</authors>
<title>Predictive Combinators for the Efficient Parsing of Combinatory Grammars.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association</booktitle>
<institution>for Computational Linguistics, Stanford University.</institution>
<contexts>
<context position="13718" citStr="[12, 19]" startWordPosition="2215" endWordPosition="2216">offers a psychologically attractive model of parsing, based on the idea of incremental processing, but causes &amp;quot;spurious ambiguity&amp;quot;, that is, an almost exponential proliferation of the possible derivation paths for identical analyses of a given string. In fact, although a rule like functional composition is specifically needed for cases of extraction and coordination, in principle nothing prevents its use to analyze strings not characterized by such phenomena, which would be analyzable in terms of functional application alone. Tentative solutions of this problem have been recently discussed in [12, 19]. The second alternative has been undertaken in the late fifties by Lambek [6] who defined a decision procedure for bidirectional propositional CG in terms of a Gentzen-style sequent system. Lambek&apos;s implicational calculus of syntactic types has recently enjoyed renewed interest in the works of van Benthem, Moortgat and other scholars. This approach can account for a range of syntactic phenomena similar to that of Combinatory Grammar, and in fact many of the rewrite rules of Combinatory Grammar can be derived as theorems in the calculus. However, analyses of cases of extraction and coordinatio</context>
</contexts>
<marker>[19]</marker>
<rawString>Wittenburg, Kent. 1987. Predictive Combinators for the Efficient Parsing of Combinatory Grammars. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zeevat</author>
<author>E Klein</author>
<author>J Calder</author>
</authors>
<title>An Introduction to Unification Categorial Grammar. In</title>
<date>1987</date>
<booktitle>Edinburgh Working Papers in Cognitive Science, 1: Categorial Grammar, Unification Grammar, and Parsing.</booktitle>
<editor>N. Haddock et al. (eds.),</editor>
<contexts>
<context position="5015" citStr="[5, 18, 20]" startWordPosition="780" endWordPosition="782">mulae obtained from two-place predicates over string positions represented as integers, the first and the second argument corresponding, respectively, to the left and right end of a given string. Therefore, the set of all sentences of length j generated from a certain lexicon corresponds to the type S(0, j). Constraints over the order of constituents are enforced by sharing integer indices across subformulae inside complex (functional) types. This first-order version of CG can be viewed as a logical reconstruction of some of the ideas behind the recent trend of Categorial Unification Grammars [5, 18, 20]&apos;. A strongly analogous development characterizes the systems of type-assignment for the formal languages of Combinatory Logic and Lambda Calculus, leading from propositional type systems to the &amp;quot;formulae-as-types&amp;quot; slogan which is behind the current research in type theory [2]. In this paper, we show how syntactic types can be encoded using an extended version of standard Horn logic syntax. 2 Definite Clauses with Internal Implications Let A and —■ be logical connectives for conjunction and implication, and let V and 3 be the univer&apos;Indeed, Uszkoreit 1181 mentions the possibility of encoding o</context>
</contexts>
<marker>[20]</marker>
<rawString>Zeevat, H., Klein, E., and J. Calder. 1987. An Introduction to Unification Categorial Grammar. In N. Haddock et al. (eds.), Edinburgh Working Papers in Cognitive Science, 1: Categorial Grammar, Unification Grammar, and Parsing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>