<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010046">
<title confidence="0.997228">
Trigger-Pair Predictors in Parsing and Tagging
</title>
<author confidence="0.96618">
Ezra Black, Andrew Finch, Hideki Kashioka
</author>
<affiliation confidence="0.475539">
ATR Interpreting Telecommunications
</affiliation>
<address confidence="0.821242666666667">
Laboratories
2-2 Hikaridai, Seika-cho
Soraku-gun, Kyoto, Japan 619-02
</address>
<email confidence="0.999613">
fblack,finch,kashiokal@atr.itl.co.jp
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999982461538462">
In this article, we apply to natural language
parsing and tagging the device of trigger-
pair predictors, previously employed exclu-
sively within the field of language mod-
elling for speech recognition. Given the
task of predicting the correct rule to as-
sociate with a parse-tree node, or the cor-
rect tag to associate with a word of text,
and assuming a particular class of pars-
ing or tagging model, we quantify the in-
formation gain realized by taking account
of rule or tag trigger-pair predictors, i.e.
pairs consisting of a &amp;quot;triggering&amp;quot; rule or
tag which has already occurred in the docu-
ment being processed, together with a spe-
cific &amp;quot;triggered&amp;quot; rule or tag whose proba-
bility of occurrence within the current sen-
tence we wish to estimate. This informa-
tion gain is shown to be substantial. Fur-
ther, by utilizing trigger pairs taken from
the same general sort of document as is be-
ing processed (e.g. same subject matter or
same discourse type)—as opposed to pre-
dictors derived from a comprehensive gen-
eral set of English texts—we can signifi-
cantly increase this information gain.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999284">
If a person or device wished to predict which words
or grammatical constructions were about to occur in
some document, intuitively one of the most helpful
things to know would seem to be which words and
constructions occurred within the last half-dozen or
dozen sentences of the document. Other things be-
ing equal, a text that has so far been larded with,
say, mountaineering terms, is a good bet to continue
featuring them. An author with the habit of ending
sentences with adverbial clauses of confirmation, e.g.
&amp;quot;as we all know&amp;quot;, will probably keep up that habit
as the discourse progresses.
Within the field of language modelling for speech
recognition, maintaining a cache of words that have
occurred so far within a document, and using this
information to alter probabilities of occurrence of
particular choices for the word being predicted, has
proved a winning strategy (Kuhn et al., 1990). Mod-
els using trigger pairs of words, i.e. pairs consist-
ing of a &amp;quot;triggering&amp;quot; word which has already oc-
curred in the document being processed, plus a spe-
cific &amp;quot;triggered&amp;quot; word whose probability of occur-
rence as the next word of the document needs to
be estimated, have yielded perplexity&apos; reductions
of 29-38% over the baseline trigram model, for a
5-million-word Wall Street Journal training corpus
(Rosenfeld, 1996).
This paper introduces the idea of using trigger-
pair techniques to assist in the prediction of rule
and tag occurrences, within the context of natural-
language parsing and tagging. Given the task of
predicting the correct rule to associate with a parse-
tree node, or the correct tag to associate with a word
of text, and assuming a particular class of parsing
or tagging model, we quantify the information gain
realized by taking account of rule or tag trigger-pair
predictors, i.e. pairs consisting of a &amp;quot;triggering&amp;quot; rule
or tag which has already occurred in the document
being processed, plus a specific &amp;quot;triggered&amp;quot; rule or
tag whose probability of occurrence within the cur-
rent sentence we wish to estimate.
In what follows, Section 2 provides a basic
overview of trigger-pair models. Section 3 de-
scribes the experiments we have performed, which
to a large extent parallel successful modelling ex-
periments within the field of language modelling for
speech recognition. In the first experiment, we inves-
tigate the use of trigger pairs to predict both rules
and tags over our full corpus of around a million
words. The subsequent experiments investigate the
</bodyText>
<footnote confidence="0.917757">
1See Section 2.
</footnote>
<page confidence="0.997862">
131
</page>
<bodyText confidence="0.999891571428571">
additional information gains accruing from trigger—
pair modelling when we know what sort of document
is being parsed or tagged. We present our exper-
imental results in Section 4, and discuss them in
Section 5. In Section 6, we present some example
trigger pairs; and we conclude, with a glance at pro-
jected future research, in Section 7.
</bodyText>
<sectionHeader confidence="0.98673" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999849227272728">
Trigger—pair modelling research has been pursued
within the field of language modelling for speech
recognition over the last decade (Beeferman et al.,
1997; Della Pietra et al., 1992; Kupiec, 1989; Lau,
1994; Lau et al., 1993; Rosenfeld, 1996).
Fundamentally, the idea is a simple one: if you
have recently seen a word in a document, then it is
more likely to occur again, or, more generally, the
prior occurrence of a word in a document affects the
probability of occurrence of itself and other words.
More formally, from an information—theoretic
viewpoint, we can interpret the process as the rela-
tionship between two dependent random variables.
Let the outcome (from the alphabet of outcomes
Ay) of a random variable Y be observed and used
to predict a random variable X (with alphabet Ax).
The probability distribution of X, in our case, is de-
pendent on the outcome of Y.
The average amount of information necessary to
specify an outcome of X (measured in bits) is called
its entropy H(X) and can also be viewed as a mea-
sure of the average ambiguity of its outcome:2
</bodyText>
<equation confidence="0.962162">
H(X) = E —P(x) log, P(x) (1)
xEAx
</equation>
<bodyText confidence="0.9999608">
The mutual information between X and Y is a
measure of entropy (ambiguity) reduction of X from
the observation of the outcome of Y. This is the
entropy of X minus its a posteriori entropy, having
observed the outcome of Y.
</bodyText>
<equation confidence="0.981676">
/(X; Y) = H(X)— H(XIY)
E E P(x, y) log, P(x,y)
P(x)P(y) (2)
rEAx yEAy
</equation>
<bodyText confidence="0.9985242">
The dependency information between a word and
its history may be captured by the trigger pair.3
A trigger pair is an ordered pair of words t and
w. Knowledge that the trigger word t has occurred
within some window of words in the history, changes
</bodyText>
<footnote confidence="0.992066">
2A more intuitive view of entropy is provided through
perplexity (Jelinek et al., 1977) which is a measure of the
number of choices, on average, there are for a random
variable. It is defined to be: 211(x).
3For a thorough description of trigger-based mod-
elling, see (Rosenfeld, 1996).
</footnote>
<bodyText confidence="0.9998214">
the probability estimate that word w will occur sub-
sequently.
Selection of these triggers can be performed by
calculating the average mutual information between
word pairs over a training corpus. In this case, the
alphabet Ax = {w, t7}, the presence or absence of
word w; similarly, Ay = {t, t}, the presence or ab-
sence of the triggering word in the history.
This is a measure of the effect that the knowl-
edge of the occurrence of the triggering word t has
on the occurence of word w, in terms of the entropy
(and therefore perplexity) reduction it will provide.
Clearly, in the absence of other context (i.e. in the
case of the a priori distribition of X), this infor-
mation will be additional. However, once &amp;quot;related
contextual information is included (for example by
building a trigram model, or, using other triggers for
the same word), this is no longer strictly true.
Once the trigger pairs are chosen, they may be
used to form constraint functions to be used in
a maximum—entropy model, alongside other con-
straints. Models of this form are extremely versa-
tile, allowing the combination of short— and long—
range information. To construct such a model, one
transforms the trigger pairs into constraint functions
</bodyText>
<equation confidence="0.99144075">
f (1 , w):
1 if t E history and
f(t,w) = next word = w (3)
0 otherwise
</equation>
<bodyText confidence="0.999803315789474">
The expected values of these functions are then
used to constrain the model, usually in combination
of with other constraints such as similar functions
embodying uni—, bi— and trigram probability esti-
mates.
(Beeferman et al., 1997) models more accurately
the effect of distance between triggering and trig-
gered word, showing that for non—self—triggers,&apos; the
triggering effect decays exponentially with distance.
For self—triggers,&apos; the effect is the same except that
the triggering effect is lessened within a short range
of the word. Using a model of these distance effects,
they are able to improve the performance of a trigger
model.
We are unaware of any work on the use of trigger
pairs in parsing or tagging. In fact, we have not
found any previous research in which extrasentential
data of any sort are applied to the problem of parsing
or tagging.
</bodyText>
<sectionHeader confidence="0.986415" genericHeader="method">
3 The Experiments
</sectionHeader>
<subsectionHeader confidence="0.991461">
3.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.90057875">
In order to investigate the utility of using long—
range trigger information in tagging and parsing
4i.e. words which trigger words other than themselves
&apos;i.e. words which trigger themselves
</bodyText>
<page confidence="0.991059">
132
</page>
<bodyText confidence="0.936495152542373">
tasks. we adopt the simple mutual-information ap-
proach used in (Rosenfeld, 1996). We carry over
into the domain of tags and rules an experiment from
Rosenfeld&apos;s paper the details of which we outline be-
low.
The idea is to measure the information con-
tributed (in bits, or, equivalently in terms of per-
plexity reduction) by using the triggers. Using this
technique requires special care to ensure that infor-
mation &amp;quot;added&amp;quot; by the triggers is indeed additional
information.
For this reason, in all our experiments we use the
unigram model as our base model and we allow only
one trigger for each tag (or rule) token.&apos; We derive
these unigram probabilities from the training cor-
pus and then calculate the total mutual information
gained by using the trigger pairs, again with respect
to the training corpus.
When using trigger pairs, one usually restricts the
trigger to occur within a certain window defined by
its distance to the triggered token. In our experi-
ments, the window starts at the sentence prior to
that containing the token and extends back W (the
window size) sentences. The choice to use sentences
as the unit of distance is motivated by our intention
to incorporate triggers of this form into a probabilis-
tic treebank-based parser and tagger, such as (Black
et al.. 1998; Black et al., 1997; Brill, 1994: Collins,
1996: Jelinek et al., 1994; Nlagerman, 1995; Ratna-
parkhi, 1997). All such parsers and taggers of which
we are aware use only intrasentential information in
predicting parses or tags, and we wish to remove
this information, as far as possible, from our results
&apos; . The window was not allowed to cross a docu-
ment boundary. The perplexity of the task before
taking the trigger-pair information into account for
tags was 224.0 and for rules was 57.0.
The characteristics of the training corpus we em-
ploy are given in Table 1. The corpus, a subsets
of the ATR/Lancaster General-English Treebank
(Black et al., 1996), consists of a sequence of sen-
tences which have been tagged and parsed by hu-
man experts in terms of the ATR English Gram-
mar, a broad-coverage grammar of English with a
high level of analytic detail (Black et al., 1996; Black
et al., 1997). For instance, the tagset is both seman-
6By rule assignment, we mean the task of assigning
a rule—name to a node in a parse tree, given that the
constituent boundaries have already been defined.
&apos; This is not completely possible, since correlations,
even if slight, will exist between intra— and extrasenten-
tial information
8specifically, a roughly-900,000—word subset of the
full ATR/Lancaster General—English Treebank (about
1.05 million words), from which all 150,000 words were
excluded that were treebanked by the two least accurate
ATR/Lancaster treebankers (expected hand—parsing er-
ror rate 32%, versus less than 10% overall for the three
remaining treebankers)
</bodyText>
<table confidence="0.982732">
1868 documents
80299 sentences
904431 words (tag instances)
1622664 constituents (rule instances)
1873 tags utilized
907 rules utilized
11.3 words per sentence, on average
</table>
<tableCaption confidence="0.9375265">
Table 1: Characteristics of Training Set (Subset of
ATR/Lancaster General-English Treebank)
</tableCaption>
<bodyText confidence="0.999531878048781">
tic and syntactic, and includes around 2000 different
tags, which classify nouns, verbs, adjectives and ad-
verbs via over 100 semantic categories. As examples
of the level of syntactic detail, exhaustive syntactic
and semantic analysis is performed on all nominal
compounds; and the full range of attachment sites
is available within the Grammar for sentential and
phrasal modifiers, and are used precisely in the Tree-
bank. The Treebank actually consists of a set of doc-
uments, from a variety of sources. Crucially for our
experiments (see below), the idea9 informing the se-
lection of (the roughly 2000) documents for inclusion
in the Treebank was to pack into it the maximum
degree of document variation along many different
scales—document length, subject area, style, point
of view, etc.—but without establishing a single, pre-
determined classification of the included documents.
In the first experiment, we examine the effective-
ness of using trigger pairs over the entire training
corpus. At the same time we investigate the ef-
fect of varying the window size. In additional ex-
periments, we observe the effect of partitioning our
training dataset into a few relatively homogeneous
subsets, on the hypothesis that this will decrease
perplexity. It seems reasonable that in different text
varieties, different sets of trigger pairs will be useful,
and that tokens which do not have effective triggers
within one text variety may have them in another.1°
To investigate the utility of partitioning the
dataset, we construct a separate set of trigger pairs
for each class. These triggers are only active for their
respective class and are independent of each other.
Their total mutual information is compared to that
derived in exactly the same way from a random par-
tition of our corpus into the same number of classes,
each comprised of the same number of documents.
Our training data partitions naturally into four
subsets, shown in Table 2 as Partitioning 1
(&amp;quot;Source&amp;quot;). Partitioning 2, &amp;quot;List Structure&amp;quot;, puts
all documents which contain at least some HTML-
like &amp;quot;List&amp;quot; markup (e.g. LI (=List Item))11 in one
</bodyText>
<footnote confidence="0.9555002">
9see (Black et al., 1996)
&amp;quot;Related work in topic—specific trigram modelling
(Lau, 1994) has led to a reduction in perplexity.
&amp;quot;All documents in our training set are marked up in
HTML—like annotation.
</footnote>
<page confidence="0.996549">
133
</page>
<bodyText confidence="0.989246733333333">
Total mutual information (bits)
subset, and all other documents in the other sub-
set. By merging Partitionings 1 and 2 we obtain
Partitioning 3, &amp;quot;Source Plus List Structure&amp;quot;. Parti-
tioning 4 is &amp;quot;Source Plus Document Type&amp;quot;, and con-
tains 9 subsets, e.g. &amp;quot;Letters; diaries&amp;quot; (subset 8) and
&amp;quot;Novels; stories; fables&amp;quot; (subset 7). With 13 subsets,
Partitioning 5, &amp;quot;Source Plus Domain&amp;quot;, includes e.g.
&amp;quot;Social Sciences&amp;quot; (subset 9) and Recreation (subset
1). Partitionings 4 and 5 were effected by actual
inspection of each document, or at least of its title
and/or summary, by one of the authors. The reason
we included Source within most partitionings was
to determine the extent to which information gains
were additive.12
</bodyText>
<sectionHeader confidence="0.998817" genericHeader="method">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.988872">
4.1 Window Size
</subsectionHeader>
<bodyText confidence="0.999947916666667">
Figure 1 shows the effect of varying the window
size from 1 to 500 for both rule and tag tokens. The
optimal window size for tags was approximately 12
sentences (about 135 words) and for rules it was ap-
proximately 6 sentences (about 68 words). These
values were used for all subsequent experiments. It
is interesting to note that the curves are of simi-
lar shape for both rules and tags and that the op-
timal value is not the largest window size. Related
effects for words are reported in (Lau, 1994; Beefer-
man et al., 1997). In the latter paper, an exponential
model of distance is used to penalize large distances
between triggering word and triggered word. The
variable window used here can be seen as a simple
alternative to this.
One explanation for this effect in our data is, in
the case of tags, that topic changes occur in docu-
ments. In the case of rules, the effect would seem
to indicate a short span of relatively intense stylistic
carryover in text. For instance, it may be much more
important, in predicting rules typical of list struc-
ture, to know that similar rules occurred a few sen-
tences ago, than to know that they occurred dozens
of sentences back in the document.
</bodyText>
<subsectionHeader confidence="0.978218">
4.2 Class-Specific Triggers
</subsectionHeader>
<bodyText confidence="0.999976125">
Table 3 shows the improvement in perplexity over
the base (unigram) tag and rule models for both the
randomly—split and the hand—partitioned training
sets. In every case, the meaningful split yielded sig-
nificantly more information than the random split.
(Of course, the results for randomly—split training
sets are roughly the same as for the unpartitioned
training set (Figure 1)).
</bodyText>
<footnote confidence="0.8620855">
&apos;For instance, compare the results for Partitionings
1, 2, and 3 in this regard.
</footnote>
<figure confidence="0.997581111111111">
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0 50 100 150 200 250 300 350 400 450 500
Window size (sentences)
</figure>
<figureCaption confidence="0.9655995">
Figure 1: Mutual information gain varying window
size
</figureCaption>
<sectionHeader confidence="0.99301" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999934368421053">
The main result of this paper is to show that
analogous to the case of words in language mod-
elling, a significant amount of extrasentential infor-
mation can be extracted from the long—range his-
tory of a document, using trigger pairs for tags and
rules. Although some redundancy of information is
inevitable, we have taken care to exclude as much
information as possible that is already available to
(intrasentential—data—based, i.e. all known) parsers
and taggers.
Quantitatively, the studies of (Rosenfeld, 1996)
yielded a total mutual information gain of 0.38 bits,
using Wall Street Journal data, with one trigger per
word. In a parallel experiment, using the same tech-
nique, but on the ATR/Lancaster corpus, the total
mutual information of the triggers for tags was 0.41
bits. This figure increases to 0.52 bits when tags fur-
ther away than 135 tags (the approximate equivalent
in words to the optimal window size in sentences) are
excluded from the history. For the remainder of our
experiments, we do not use as part of the history
the tags/rules from the sentence containing the to-
ken to be predicted. This is motivated by our wish
to exclude the intrasentential information which is
already available to parsers and taggers.
In the case of tags, using the optimal window size,
the gain was 0.31 bits, and for rules the information
gain was 0.12 bits. Although these figures are not
as large as for the case where intrasentential infor-
mation is incorporated, they are sufficiently close to
encourage us to exploit this information in our mod-
els.
For the case of words, the evidence shows that
triggers derived in the same manner as the trig-
gers in our experiments, can provide a substantial
amount of new information when used in combina-
tion with sophisticated language models. For ex-
ample, (Rosenfeld, 1996) used a maximum—entropy
</bodyText>
<figure confidence="0.867172">
tags —
rules
...
</figure>
<page confidence="0.99103">
134
</page>
<table confidence="0.997715294117647">
Part. 1: Source Tart. 4: Source + Doc Type Part. 5: Source + Domain Sents
Class Name Sents Class Name Sents Class Name
1: Assoc. Press, WSJ 8851 1: Legislative 5626 1: Recreation 3545
2: Canadian Hansards 5002 (incl. Srce.2) 2: Business 2055
3: General English 23105 2: Transcripts 44287 3: Science, Techn. 4018
4: Travel-domain dialgs 43341 (incl. Srce.4) 4: Humanities 2224
Part. 2: List Structure 3: News 8614 5: Daily Living 896
Class Name Sents (incl. most Srce.1) 6: Health, Education 1649
1: COntains lists 14147 4: Polemical essays 5160 7: Government, Polit. 1768
2: Contains no lists 66152 5: Reports; FAQs; 11440 8: Travel 2667
Part. 3: Source + List Structure listings 9: Social Sciences 3617
Class Name Sents 6: Idiom examples 666 10: Idiom examp. sents 666
1: Assoc. Press, WSJ 8851 7: Novels; stories; 741 11: Canadian Hansards 5002
2: Canadian Hansards 5002 fables 12: Assoc. Press, WSJ 8851
3: Contains lists (Gen.) 11998 8: Letters; diaries 1997 13: Travel dialgs 43341
4: Contains no lists (Gen.) 11117 9: Legal cases; 1768
5: Travel-domain dialogues 43341 constitutions
</table>
<tableCaption confidence="0.868894">
Table 2: Training Set Partitions
</tableCaption>
<table confidence="0.999964">
Partitioning Perplexity reduction for tags Perplexity reduction for rules
Meaningful partition Random Meaningful partition Random
1: Source 28.40% 16.66% 15.44% 6.30%
2: List Structure 20.39% 18.71% 10.55% 7.46%
3: Source Plus List Structure 28.74% 17.12% 15.61% 6.50%
4: Source Plus Document Type 30.11% 18.15% 16.20% 6.82%
5: Source Plus Domain 31.55% 19.39% 16.60% 7.34%
</table>
<tableCaption confidence="0.944983">
Table 3: Perplexity reduction using class-specific triggers to predict tags and rules
</tableCaption>
<table confidence="0.997204222222222">
# Triggering Tag Triggered Tag I.e. Words Like These: Trigger Words Like These:
1 NP1LOCNM NP1STATENM Hill, County, Bay, Lake Utah, Maine, Alaska
2 JJSYSTEM NP1ORG national, federal, political Party, Council, Department
3 IIDESPITE CFYET despite yet (conjunction)
4 PN1PERSON LEBUT22 everyone, one, anybody (not) only, (not) just
5 ... MPRICE 8452,983,000, 810,000, 819.95
6 IIAT(SF) MPHONE22 at (sent.-final, +/-&amp;quot;:&amp;quot;) 913-3434 (follows area code)
7 IIFROM(SF) MZIP from (sent.-final, +/-&amp;quot;:&amp;quot;) 22314-1698 (postal zipcode)
8 NNUNUM NNEMONEY 25%, 12&amp;quot;, 9.4m3 profit, price, cost
</table>
<tableCaption confidence="0.965845">
Table 4: Selected Tag Trigger-Pairs, ATR/Lancaster General-English Treebank
</tableCaption>
<table confidence="0.618019727272727">
# A Construction Like This: Triggers A Construction Like This:
la Interrupter Phrase -&gt; * Or - Sentence -&gt; Interrupter P+Phrasal (Non-S)
lb Example: *, - Example: * DIG. AM/FM TUNER
2a VP -&gt; Verb+Interrupter Phrase+Obj/Compl Interrupter Phrase -&gt; ,+Interrupter+,
2b Example: starring—surprise, surprise—men Example: , according to participants ,
3a Noun Phrase -&gt; Simple Noun Phrase+Num Num -&gt; Num +PrepP with Numerical Obj
3b Example: Lows around 50 Example: (Snow level) 6000 to 7000
4a Verb Phrase -&gt; Adverb Phrase+Verb Phrase Auxiliary VP -&gt; Model/Auxilliary Verb+Not
4b Example: just need to understand it Example: do not
5a Question -&gt; Be+NP+Object/Complement Quoted Phrasal -&gt; &amp;quot;+Phrasal Constit+&amp;quot;
5b Example: Is it possible? Example: &amp;quot;Mutual funds are back.&amp;quot;
</table>
<tableCaption confidence="0.97485">
Table 5: Selected Rule Trigger-Pairs, ATR/Lancaster General-English Treebank
</tableCaption>
<page confidence="0.948832">
135
</page>
<table confidence="0.981419846153846">
# Triggering Tag Triggered Tag I.e. Words Like These: Trigger Words Like These:
1 VVNSEND NP1STATENM shipped, distributed Utah, Maine, Alaska
2 NP1LOCNM NP1STATENM Hill, County, Bay, Lake Utah, Maine, Alaska
For training-set document class Recreation (1) vs. for unpartitioned training set (2)
4 VVOALTER NN2SUBSTANCE II inhibit, affect, modify tumors, drugs, agents
For JJPHYS-ATT NN2SUBSTANCE II fragile, brown, choppy pines, apples, chemicals
training-set document class Health And Education (3) vs. for unpartitioned training set (4)
5 I NN1TIME [ NN2MONEY period, future, decade 1 expenses, fees, taxes
6 I NP1POSTFRMNM I NN2MONEY Inc., Associates, Co. I loans, damages, charges
For training-set document class Business (5) vs. for unpartitioned training set (6)
7 I DD1 I DDQ this, that, another, each which
8 I DDQ I DDQ which which
For training-set document class Travel Dialogues (7) vs. for unpartitioned training set (8)
</table>
<tableCaption confidence="0.9857995">
Table 6: Selected Tag Trigger-Pairs, ATR/Lancaster General-English Treebank: Contrasting Trigger-Pairs
Arising From Partitioned vs. Unpartitioned Training Sets
</tableCaption>
<bodyText confidence="0.999870391304348">
model trained on 5 million words, with only trigger,
uni-, bi- and trigram constraints, to measure the
test-set perpexity reduction with respect to a &amp;quot;com-
pact&amp;quot; backoff trigram model, a well-respected model
in the language-modelling field. When the top six
triggers for each word were used, test-set perplex-
ity was reduced by 25%. Furthermore, when a more
sophisticated version of this model13 was applied in
conjunction with the SPHINX II speech recognition
system (Huang et al., 1993), a 10-14% reduction in
word error rate resulted (Rosenfeld, 1996). We see
no reason why this effect should not carry over to tag
and rule tokens, and are optimistic that long-range
trigger information can be used in both parsing and
tagging to improve performance.
For words (Rosenfeld, 1996), self-triggers—words
which triggered themselves—were the most frequent
kind of triggers (68% of all word triggers were self-
triggers). This is also the case for tags and rules. For
tags, 76.8% were self-triggers, and for rules, 96.5%
were self-triggers. As in the case of words, the set
of self-triggers provides the most useful predictive
information.
</bodyText>
<sectionHeader confidence="0.992949" genericHeader="method">
6 Some Examples
</sectionHeader>
<bodyText confidence="0.999975">
We will now explicate a few of the example trig-
ger pairs in Tables 4-6. Table 4 Item 5, for instance,
captures the common practice of using a sequence of
points, e.g. , to separate each item of a (price)
list and the price of that item. Items 6 and 7 are
similar cases (e.g. &amp;quot;contact/call (someone) at:&amp;quot; +
phone number; &amp;quot;available from:&amp;quot; + source, typically
including address, hence zipcode). These correla-
tions typically occur within listings, and, crucially
</bodyText>
<footnote confidence="0.9730005">
13trained on 38 million words, and also employing
distance-2 N-gram constraints, a unigram cache and a
conditional bigram cache (this model reduced perplexity
over the baseline trigram model by 32%)
</footnote>
<bodyText confidence="0.998956">
for their usefulness as triggers, typically occur many
at a time.
When triggers are drawn from a relatively homo-
geneous set of documents, correlations emerge which
seem to reflect the character of the text type in-
volved. So in Table 6 Item 5, the proverbial equa-
tion of time and money emerges as more central to
Business and Commerce texts than the different but
equally sensible linkup, within our overall training
set, between business corporations and money.
Turning to rule triggers, Table 5 Item 1 is more
or less a syntactic analog of the tag examples Ta-
ble 4 Items 5-7, just discussed. What seems to be
captured is that a particular style of listing things,
e.g. * + listed item, characterizes a document as a
whole (if it contains lists); further, listed items are
not always of the same phrasal type, but are prone
to vary syntactically. The same document that con-
tains the list item &amp;quot;* DIG. AM/FM TUNER&amp;quot;, for
instance, which is based on a Noun Phrase, soon af-
terwards includes &amp;quot;* WEATHER PROOF&amp;quot; and &amp;quot;*
ULTRA COMPACT&amp;quot;, which are based on Adjective
Phrases.
Finally, as in the case of the tag trigger examples
of Table 6, text-type-particular correlations emerge
when rule triggers are drawn from a relatively ho-
mogeneous set of documents. A trigger pair of con-
structions specific to Class 1 of the Source partition-
ing, which contains only Associated Press newswire
and Wall Street Journal articles, is the following: A
sentence containing both a quoted remark and an
attribution of that remark to a particular source,
triggers a sentence containing simply a quoted re-
mark, without attribution. (E.g. &amp;quot;The King was in
trouble,&amp;quot; Wall wrote. triggers &amp;quot;This increased the
King&apos;s bitterness.&amp;quot;.) This correlation is essentially
absent in other text types.
</bodyText>
<page confidence="0.998292">
136
</page>
<sectionHeader confidence="0.997379" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999819730769231">
In this paper, we have shown that, as in the case of
words, there is a substantial amount of information
outside the sentence which could be used to sup-
plement tagging and parsing models. We have also
shown that knowledge of the type of document being
processed greatly increases the usefulness of triggers.
If this information is known, or can be predicted ac-
curately from the history of a given document being
processed, then model interpolation techniques (Je-
linek et al., 1980) could be employed, we anticipate,
to exploit this to useful effect.
Future research will concentrate on incorporating
trigger-pair information, and extrasentential infor-
mation more generally, into more sophisticated mod-
els of parsing and tagging. An obvious first extention
to this work, for the case of tags, will be, following
(Rosenfeld, 1996), to incorporate the triggers into a
maximum-entropy model using trigger pairs in ad-
dition to unigram, bigram and trigram constraints.
Later we intend to incorporate trigger information
into a probabilistic English parser/tagger which is
able to ask complex, detailed questions about the
contents of a sentence. From the results presented
here we are optimistic that the additional, extrasen-
tential information provided by trigger pairs will
benefit such parsing and tagging systems.
</bodyText>
<sectionHeader confidence="0.998571" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998421268292683">
D. Beeferman, A. Berger, and J. Lafferty. 1997. A
Model of Lexical Attraction and Repulsion. In
Proceedings of the ACL-EACL&apos;97 Joint Confer-
ence, Madrid.
E. Black, S. Eubank, H. Kashioka, J. Saia. 1998.
Reinventing Part-of-Speech Tagging. Journal of
Natural Language Processing (Japan), 5:1.
E. Black, S. Eubank, H. Kashioka. 1997. Probabilis-
tic Parsing of Unrestricted English Text, With A
Highly-Detailed Grammar. In Proceedings, Fifth
Workshop on Very Large Corpora, Beijing/Hong
Kong.
E. Black, S. Eubank, H. Kashioka, R. Garside, G.
Leech, and D. Magerman. 1996. Beyond skeleton
parsing: producing a comprehensive large-scale
general-English treebank with full grammatical
analysis. In Proceedings of the 16th Annual Con-
ference on Computational Linguistics, pages 107-
112. Copenhagen.
E. Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. In Proceedings
of the Twelfth National Conference on Artificial
Intelligence, pages 722-727, Seattle, Washington.
American Association for Artificial Intelligence.
M. Collins. 1996, A new statistical parser based
on bigram lexical dependencies. In Proceedings of
the 34th Annual Meeting of the Association for
Computational Lan guistics, Santa Cruz.
Della Pietra, V. Della Pietra, R. Mercer, S.
Roukos. 1992. Adaptive language modeling us-
ing minimum discriminant information. Proceed-
ings of the International Conference on Acoustics,
Speech and Signal Processing, 1:633-636.
Huang, F. Alleva, H.-W. Hon, M.-Y. Hwang, K.-
F. Lee, and R. Rosenfeld. 1993. The SPHINX-II
speech recognition system: an overview. Com-
puter Speech and Language, 2:137-148.
Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker.
1977. Perplexity—a measure of difficulty of
speech recognition tasks. In Proceedings of the
94th Meeting of the Acoustic Society of America,
Miami Beach, FL.
Jelinek and R. Mercer. 1980. Interpolated esti-
mation of Markov source parameters from sparse
data. In Pattern Recognition In Practice, E. S.
Gelsema and N. L. Kanal, eds., pages 381-402,
Amsterdam: North Holland.
Jelinek, J. Lafferty, D. Magerman, R. Mercer, A.
Ratnaparkhi, S. Roukos. 1994. Decision tree pars-
ing using a hidden derivation model. In Proceed-
ings of the ARPA Workshop on Human Language
Technology, pages 260-265, Plainsboro, New Jer-
sey. Advanced Research Projects Agency.
. Kuhn, R. De Mori. 1990. A Cache-Based
Natural Language Model for Speech Recognition.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 12(6):570-583.
Kupiec. 1989. Probabilistic models of short and
long distance word dependencies in running text.
In Proceedings of the DARPA Workshop on Speech
and Natural Language, pages 290-295.
. Lau, R,. Rosenfeld, S. Roukos. 1993. Trigger-
based language models: a maximum entropy ap-
proach. Proceedings of the International Confer-
ence on Acoustics, Speech and Signal Processing,
11:45-48.
. Lau. 1994. Adaptive Statistical Language Mod-
elling. Master&apos;s Thesis, Massachusetts Institute
of Technology, MA.
. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In 33rd Annual Meeting of the
Association for Computational Linguistics, pages
276-283, Cambridge, Massachusetts. Association
for Computational Linguistics.
. Ratnaparkhi. 1997. A Linear Observed Time
Statistical Parser Based on Maximum Entropy
Models. In Proceedings, Second Conference on
Empirical Methods in Natural Language Process-
ing, Providence, RI.
. Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modelling. Com-
puter Speech and Language, 10:187-228.
</reference>
<page confidence="0.997706">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889561">
<title confidence="0.999827">Trigger-Pair Predictors in Parsing and Tagging</title>
<author confidence="0.997661">Ezra Black</author>
<author confidence="0.997661">Andrew Finch</author>
<author confidence="0.997661">Hideki Kashioka</author>
<affiliation confidence="0.977944">ATR Interpreting Telecommunications Laboratories</affiliation>
<address confidence="0.968729">2-2 Hikaridai, Seika-cho Soraku-gun, Kyoto, Japan 619-02</address>
<email confidence="0.988942">fblack,finch,kashiokal@atr.itl.co.jp</email>
<abstract confidence="0.999362148148148">In this article, we apply to natural language parsing and tagging the device of triggerpair predictors, previously employed exclusively within the field of language modelling for speech recognition. Given the task of predicting the correct rule to associate with a parse-tree node, or the correct tag to associate with a word of text, and assuming a particular class of parsing or tagging model, we quantify the information gain realized by taking account of rule or tag trigger-pair predictors, i.e. pairs consisting of a &amp;quot;triggering&amp;quot; rule or tag which has already occurred in the document being processed, together with a specific &amp;quot;triggered&amp;quot; rule or tag whose probability of occurrence within the current sentence we wish to estimate. This information gain is shown to be substantial. Further, by utilizing trigger pairs taken from the same general sort of document as is being processed (e.g. same subject matter or same discourse type)—as opposed to predictors derived from a comprehensive general set of English texts—we can significantly increase this information gain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>A Model of Lexical Attraction and Repulsion.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL-EACL&apos;97 Joint Conference,</booktitle>
<location>Madrid.</location>
<contexts>
<context position="4332" citStr="Beeferman et al., 1997" startWordPosition="701" endWordPosition="704">tags over our full corpus of around a million words. The subsequent experiments investigate the 1See Section 2. 131 additional information gains accruing from trigger— pair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger—pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information—theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes Ay) of a random variable Y be observed and used to predict a</context>
<context position="7627" citStr="Beeferman et al., 1997" startWordPosition="1282" endWordPosition="1285">are chosen, they may be used to form constraint functions to be used in a maximum—entropy model, alongside other constraints. Models of this form are extremely versatile, allowing the combination of short— and long— range information. To construct such a model, one transforms the trigger pairs into constraint functions f (1 , w): 1 if t E history and f(t,w) = next word = w (3) 0 otherwise The expected values of these functions are then used to constrain the model, usually in combination of with other constraints such as similar functions embodying uni—, bi— and trigram probability estimates. (Beeferman et al., 1997) models more accurately the effect of distance between triggering and triggered word, showing that for non—self—triggers,&apos; the triggering effect decays exponentially with distance. For self—triggers,&apos; the effect is the same except that the triggering effect is lessened within a short range of the word. Using a model of these distance effects, they are able to improve the performance of a trigger model. We are unaware of any work on the use of trigger pairs in parsing or tagging. In fact, we have not found any previous research in which extrasentential data of any sort are applied to the proble</context>
<context position="15180" citStr="Beeferman et al., 1997" startWordPosition="2517" endWordPosition="2521">ermine the extent to which information gains were additive.12 4 Experimental Results 4.1 Window Size Figure 1 shows the effect of varying the window size from 1 to 500 for both rule and tag tokens. The optimal window size for tags was approximately 12 sentences (about 135 words) and for rules it was approximately 6 sentences (about 68 words). These values were used for all subsequent experiments. It is interesting to note that the curves are of similar shape for both rules and tags and that the optimal value is not the largest window size. Related effects for words are reported in (Lau, 1994; Beeferman et al., 1997). In the latter paper, an exponential model of distance is used to penalize large distances between triggering word and triggered word. The variable window used here can be seen as a simple alternative to this. One explanation for this effect in our data is, in the case of tags, that topic changes occur in documents. In the case of rules, the effect would seem to indicate a short span of relatively intense stylistic carryover in text. For instance, it may be much more important, in predicting rules typical of list structure, to know that similar rules occurred a few sentences ago, than to know</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1997. A Model of Lexical Attraction and Repulsion. In Proceedings of the ACL-EACL&apos;97 Joint Conference, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Eubank</author>
<author>H Kashioka</author>
<author>J Saia</author>
</authors>
<title>Reinventing Part-of-Speech Tagging.</title>
<date>1998</date>
<journal>Journal of Natural Language Processing (Japan),</journal>
<volume>5</volume>
<marker>Black, Eubank, Kashioka, Saia, 1998</marker>
<rawString>E. Black, S. Eubank, H. Kashioka, J. Saia. 1998. Reinventing Part-of-Speech Tagging. Journal of Natural Language Processing (Japan), 5:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Eubank</author>
<author>H Kashioka</author>
</authors>
<title>Probabilistic Parsing of Unrestricted English Text, With A Highly-Detailed Grammar.</title>
<date>1997</date>
<booktitle>In Proceedings, Fifth Workshop on Very Large Corpora, Beijing/Hong Kong.</booktitle>
<contexts>
<context position="9790" citStr="Black et al., 1997" startWordPosition="1643" endWordPosition="1646">n calculate the total mutual information gained by using the trigger pairs, again with respect to the training corpus. When using trigger pairs, one usually restricts the trigger to occur within a certain window defined by its distance to the triggered token. In our experiments, the window starts at the sentence prior to that containing the token and extends back W (the window size) sentences. The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistic treebank-based parser and tagger, such as (Black et al.. 1998; Black et al., 1997; Brill, 1994: Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-Eng</context>
</contexts>
<marker>Black, Eubank, Kashioka, 1997</marker>
<rawString>E. Black, S. Eubank, H. Kashioka. 1997. Probabilistic Parsing of Unrestricted English Text, With A Highly-Detailed Grammar. In Proceedings, Fifth Workshop on Very Large Corpora, Beijing/Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Eubank</author>
<author>H Kashioka</author>
<author>R Garside</author>
<author>G Leech</author>
<author>D Magerman</author>
</authors>
<title>Beyond skeleton parsing: producing a comprehensive large-scale general-English treebank with full grammatical analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Annual Conference on Computational Linguistics,</booktitle>
<pages>107--112</pages>
<location>Copenhagen.</location>
<contexts>
<context position="10424" citStr="Black et al., 1996" startWordPosition="1750" endWordPosition="1753">Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-English Treebank (Black et al., 1996), consists of a sequence of sentences which have been tagged and parsed by human experts in terms of the ATR English Grammar, a broad-coverage grammar of English with a high level of analytic detail (Black et al., 1996; Black et al., 1997). For instance, the tagset is both seman6By rule assignment, we mean the task of assigning a rule—name to a node in a parse tree, given that the constituent boundaries have already been defined. &apos; This is not completely possible, since correlations, even if slight, will exist between intra— and extrasentential information 8specifically, a roughly-900,000—word</context>
<context position="13726" citStr="Black et al., 1996" startWordPosition="2273" endWordPosition="2276">construct a separate set of trigger pairs for each class. These triggers are only active for their respective class and are independent of each other. Their total mutual information is compared to that derived in exactly the same way from a random partition of our corpus into the same number of classes, each comprised of the same number of documents. Our training data partitions naturally into four subsets, shown in Table 2 as Partitioning 1 (&amp;quot;Source&amp;quot;). Partitioning 2, &amp;quot;List Structure&amp;quot;, puts all documents which contain at least some HTMLlike &amp;quot;List&amp;quot; markup (e.g. LI (=List Item))11 in one 9see (Black et al., 1996) &amp;quot;Related work in topic—specific trigram modelling (Lau, 1994) has led to a reduction in perplexity. &amp;quot;All documents in our training set are marked up in HTML—like annotation. 133 Total mutual information (bits) subset, and all other documents in the other subset. By merging Partitionings 1 and 2 we obtain Partitioning 3, &amp;quot;Source Plus List Structure&amp;quot;. Partitioning 4 is &amp;quot;Source Plus Document Type&amp;quot;, and contains 9 subsets, e.g. &amp;quot;Letters; diaries&amp;quot; (subset 8) and &amp;quot;Novels; stories; fables&amp;quot; (subset 7). With 13 subsets, Partitioning 5, &amp;quot;Source Plus Domain&amp;quot;, includes e.g. &amp;quot;Social Sciences&amp;quot; (subset 9) a</context>
</contexts>
<marker>Black, Eubank, Kashioka, Garside, Leech, Magerman, 1996</marker>
<rawString>E. Black, S. Eubank, H. Kashioka, R. Garside, G. Leech, and D. Magerman. 1996. Beyond skeleton parsing: producing a comprehensive large-scale general-English treebank with full grammatical analysis. In Proceedings of the 16th Annual Conference on Computational Linguistics, pages 107-112. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in TransformationBased Part of Speech Tagging.</title>
<date>1994</date>
<journal>Artificial Intelligence.</journal>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<pages>722--727</pages>
<publisher>American Association for</publisher>
<location>Seattle, Washington.</location>
<contexts>
<context position="9803" citStr="Brill, 1994" startWordPosition="1647" endWordPosition="1648">l mutual information gained by using the trigger pairs, again with respect to the training corpus. When using trigger pairs, one usually restricts the trigger to occur within a certain window defined by its distance to the triggered token. In our experiments, the window starts at the sentence prior to that containing the token and extends back W (the window size) sentences. The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistic treebank-based parser and tagger, such as (Black et al.. 1998; Black et al., 1997; Brill, 1994: Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-English Treebank</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>E. Brill. 1994. Some Advances in TransformationBased Part of Speech Tagging. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 722-727, Seattle, Washington. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Lan guistics,</booktitle>
<location>Santa Cruz.</location>
<contexts>
<context position="9818" citStr="Collins, 1996" startWordPosition="1649" endWordPosition="1650">rmation gained by using the trigger pairs, again with respect to the training corpus. When using trigger pairs, one usually restricts the trigger to occur within a certain window defined by its distance to the triggered token. In our experiments, the window starts at the sentence prior to that containing the token and extends back W (the window size) sentences. The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistic treebank-based parser and tagger, such as (Black et al.. 1998; Black et al., 1997; Brill, 1994: Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-English Treebank (Black et al.,</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996, A new statistical parser based on bigram lexical dependencies. In Proceedings of the 34th Annual Meeting of the Association for Computational Lan guistics, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
<author>S Roukos</author>
</authors>
<title>Adaptive language modeling using minimum discriminant information.</title>
<date>1992</date>
<booktitle>Proceedings of the International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>1--633</pages>
<contexts>
<context position="4359" citStr="Pietra et al., 1992" startWordPosition="706" endWordPosition="709">round a million words. The subsequent experiments investigate the 1See Section 2. 131 additional information gains accruing from trigger— pair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger—pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information—theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes Ay) of a random variable Y be observed and used to predict a random variable X (with al</context>
</contexts>
<marker>Pietra, Pietra, Mercer, Roukos, 1992</marker>
<rawString>Della Pietra, V. Della Pietra, R. Mercer, S. Roukos. 1992. Adaptive language modeling using minimum discriminant information. Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 1:633-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Alleva Huang</author>
<author>H-W Hon</author>
<author>M-Y Hwang</author>
<author>K-F Lee</author>
<author>R Rosenfeld</author>
</authors>
<title>The SPHINX-II speech recognition system: an overview. Computer Speech and Language,</title>
<date>1993</date>
<pages>2--137</pages>
<contexts>
<context position="23012" citStr="Huang et al., 1993" startWordPosition="3757" endWordPosition="3760">ger-Pairs, ATR/Lancaster General-English Treebank: Contrasting Trigger-Pairs Arising From Partitioned vs. Unpartitioned Training Sets model trained on 5 million words, with only trigger, uni-, bi- and trigram constraints, to measure the test-set perpexity reduction with respect to a &amp;quot;compact&amp;quot; backoff trigram model, a well-respected model in the language-modelling field. When the top six triggers for each word were used, test-set perplexity was reduced by 25%. Furthermore, when a more sophisticated version of this model13 was applied in conjunction with the SPHINX II speech recognition system (Huang et al., 1993), a 10-14% reduction in word error rate resulted (Rosenfeld, 1996). We see no reason why this effect should not carry over to tag and rule tokens, and are optimistic that long-range trigger information can be used in both parsing and tagging to improve performance. For words (Rosenfeld, 1996), self-triggers—words which triggered themselves—were the most frequent kind of triggers (68% of all word triggers were selftriggers). This is also the case for tags and rules. For tags, 76.8% were self-triggers, and for rules, 96.5% were self-triggers. As in the case of words, the set of self-triggers pro</context>
</contexts>
<marker>Huang, Hon, Hwang, Lee, Rosenfeld, 1993</marker>
<rawString>Huang, F. Alleva, H.-W. Hon, M.-Y. Hwang, K.-F. Lee, and R. Rosenfeld. 1993. The SPHINX-II speech recognition system: an overview. Computer Speech and Language, 2:137-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Mercer Jelinek</author>
<author>L R Bahl</author>
<author>J K Baker</author>
</authors>
<title>Perplexity—a measure of difficulty of speech recognition tasks.</title>
<date>1977</date>
<booktitle>In Proceedings of the 94th Meeting of the Acoustic Society of America,</booktitle>
<location>Miami Beach, FL.</location>
<contexts>
<context position="5909" citStr="Jelinek et al., 1977" startWordPosition="985" endWordPosition="988">tual information between X and Y is a measure of entropy (ambiguity) reduction of X from the observation of the outcome of Y. This is the entropy of X minus its a posteriori entropy, having observed the outcome of Y. /(X; Y) = H(X)— H(XIY) E E P(x, y) log, P(x,y) P(x)P(y) (2) rEAx yEAy The dependency information between a word and its history may be captured by the trigger pair.3 A trigger pair is an ordered pair of words t and w. Knowledge that the trigger word t has occurred within some window of words in the history, changes 2A more intuitive view of entropy is provided through perplexity (Jelinek et al., 1977) which is a measure of the number of choices, on average, there are for a random variable. It is defined to be: 211(x). 3For a thorough description of trigger-based modelling, see (Rosenfeld, 1996). the probability estimate that word w will occur subsequently. Selection of these triggers can be performed by calculating the average mutual information between word pairs over a training corpus. In this case, the alphabet Ax = {w, t7}, the presence or absence of word w; similarly, Ay = {t, t}, the presence or absence of the triggering word in the history. This is a measure of the effect that the k</context>
</contexts>
<marker>Jelinek, Bahl, Baker, 1977</marker>
<rawString>Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker. 1977. Perplexity—a measure of difficulty of speech recognition tasks. In Proceedings of the 94th Meeting of the Acoustic Society of America, Miami Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jelinek</author>
<author>R Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition</title>
<date>1980</date>
<pages>381--402</pages>
<editor>In Practice, E. S. Gelsema and N. L. Kanal, eds.,</editor>
<publisher>North Holland.</publisher>
<location>Amsterdam:</location>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek and R. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Pattern Recognition In Practice, E. S. Gelsema and N. L. Kanal, eds., pages 381-402, Amsterdam: North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty Jelinek</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>A Ratnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision tree parsing using a hidden derivation model.</title>
<date>1994</date>
<journal>Advanced Research Projects Agency.</journal>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>260--265</pages>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="9840" citStr="Jelinek et al., 1994" startWordPosition="1651" endWordPosition="1654">by using the trigger pairs, again with respect to the training corpus. When using trigger pairs, one usually restricts the trigger to occur within a certain window defined by its distance to the triggered token. In our experiments, the window starts at the sentence prior to that containing the token and extends back W (the window size) sentences. The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistic treebank-based parser and tagger, such as (Black et al.. 1998; Black et al., 1997; Brill, 1994: Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-English Treebank (Black et al., 1996), consists of a </context>
</contexts>
<marker>Jelinek, Magerman, Mercer, Ratnaparkhi, Roukos, 1994</marker>
<rawString>Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Ratnaparkhi, S. Roukos. 1994. Decision tree parsing using a hidden derivation model. In Proceedings of the ARPA Workshop on Human Language Technology, pages 260-265, Plainsboro, New Jersey. Advanced Research Projects Agency.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Mori Kuhn</author>
</authors>
<title>A Cache-Based Natural Language Model for Speech Recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>12--6</pages>
<marker>Kuhn, 1990</marker>
<rawString>. Kuhn, R. De Mori. 1990. A Cache-Based Natural Language Model for Speech Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570-583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kupiec</author>
</authors>
<title>Probabilistic models of short and long distance word dependencies in running text.</title>
<date>1989</date>
<booktitle>In Proceedings of the DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>290--295</pages>
<contexts>
<context position="4373" citStr="Kupiec, 1989" startWordPosition="710" endWordPosition="711">. The subsequent experiments investigate the 1See Section 2. 131 additional information gains accruing from trigger— pair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger—pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information—theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes Ay) of a random variable Y be observed and used to predict a random variable X (with alphabet Ax). Th</context>
</contexts>
<marker>Kupiec, 1989</marker>
<rawString>Kupiec. 1989. Probabilistic models of short and long distance word dependencies in running text. In Proceedings of the DARPA Workshop on Speech and Natural Language, pages 290-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
</authors>
<title>Triggerbased language models: a maximum entropy approach.</title>
<date>1993</date>
<booktitle>Proceedings of the International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>11--45</pages>
<marker>Lau, 1993</marker>
<rawString>. Lau, R,. Rosenfeld, S. Roukos. 1993. Triggerbased language models: a maximum entropy approach. Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 11:45-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lau</author>
</authors>
<title>Adaptive Statistical Language Modelling. Master&apos;s Thesis,</title>
<date>1994</date>
<institution>Massachusetts Institute of Technology, MA.</institution>
<contexts>
<context position="4384" citStr="Lau, 1994" startWordPosition="712" endWordPosition="713">nt experiments investigate the 1See Section 2. 131 additional information gains accruing from trigger— pair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger—pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information—theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes Ay) of a random variable Y be observed and used to predict a random variable X (with alphabet Ax). The probabili</context>
<context position="13788" citStr="Lau, 1994" startWordPosition="2283" endWordPosition="2284">s are only active for their respective class and are independent of each other. Their total mutual information is compared to that derived in exactly the same way from a random partition of our corpus into the same number of classes, each comprised of the same number of documents. Our training data partitions naturally into four subsets, shown in Table 2 as Partitioning 1 (&amp;quot;Source&amp;quot;). Partitioning 2, &amp;quot;List Structure&amp;quot;, puts all documents which contain at least some HTMLlike &amp;quot;List&amp;quot; markup (e.g. LI (=List Item))11 in one 9see (Black et al., 1996) &amp;quot;Related work in topic—specific trigram modelling (Lau, 1994) has led to a reduction in perplexity. &amp;quot;All documents in our training set are marked up in HTML—like annotation. 133 Total mutual information (bits) subset, and all other documents in the other subset. By merging Partitionings 1 and 2 we obtain Partitioning 3, &amp;quot;Source Plus List Structure&amp;quot;. Partitioning 4 is &amp;quot;Source Plus Document Type&amp;quot;, and contains 9 subsets, e.g. &amp;quot;Letters; diaries&amp;quot; (subset 8) and &amp;quot;Novels; stories; fables&amp;quot; (subset 7). With 13 subsets, Partitioning 5, &amp;quot;Source Plus Domain&amp;quot;, includes e.g. &amp;quot;Social Sciences&amp;quot; (subset 9) and Recreation (subset 1). Partitionings 4 and 5 were effected </context>
<context position="15155" citStr="Lau, 1994" startWordPosition="2515" endWordPosition="2516"> was to determine the extent to which information gains were additive.12 4 Experimental Results 4.1 Window Size Figure 1 shows the effect of varying the window size from 1 to 500 for both rule and tag tokens. The optimal window size for tags was approximately 12 sentences (about 135 words) and for rules it was approximately 6 sentences (about 68 words). These values were used for all subsequent experiments. It is interesting to note that the curves are of similar shape for both rules and tags and that the optimal value is not the largest window size. Related effects for words are reported in (Lau, 1994; Beeferman et al., 1997). In the latter paper, an exponential model of distance is used to penalize large distances between triggering word and triggered word. The variable window used here can be seen as a simple alternative to this. One explanation for this effect in our data is, in the case of tags, that topic changes occur in documents. In the case of rules, the effect would seem to indicate a short span of relatively intense stylistic carryover in text. For instance, it may be much more important, in predicting rules typical of list structure, to know that similar rules occurred a few se</context>
</contexts>
<marker>Lau, 1994</marker>
<rawString>. Lau. 1994. Adaptive Statistical Language Modelling. Master&apos;s Thesis, Massachusetts Institute of Technology, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>276--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, Massachusetts.</location>
<marker>Magerman, 1995</marker>
<rawString>. Magerman. 1995. Statistical decision-tree models for parsing. In 33rd Annual Meeting of the Association for Computational Linguistics, pages 276-283, Cambridge, Massachusetts. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ratnaparkhi</author>
</authors>
<title>A Linear Observed Time Statistical Parser Based on Maximum Entropy Models.</title>
<date>1997</date>
<booktitle>In Proceedings, Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Providence, RI.</location>
<contexts>
<context position="9877" citStr="Ratnaparkhi, 1997" startWordPosition="1657" endWordPosition="1659">respect to the training corpus. When using trigger pairs, one usually restricts the trigger to occur within a certain window defined by its distance to the triggered token. In our experiments, the window starts at the sentence prior to that containing the token and extends back W (the window size) sentences. The choice to use sentences as the unit of distance is motivated by our intention to incorporate triggers of this form into a probabilistic treebank-based parser and tagger, such as (Black et al.. 1998; Black et al., 1997; Brill, 1994: Collins, 1996: Jelinek et al., 1994; Nlagerman, 1995; Ratnaparkhi, 1997). All such parsers and taggers of which we are aware use only intrasentential information in predicting parses or tags, and we wish to remove this information, as far as possible, from our results &apos; . The window was not allowed to cross a document boundary. The perplexity of the task before taking the trigger-pair information into account for tags was 224.0 and for rules was 57.0. The characteristics of the training corpus we employ are given in Table 1. The corpus, a subsets of the ATR/Lancaster General-English Treebank (Black et al., 1996), consists of a sequence of sentences which have been</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>. Ratnaparkhi. 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models. In Proceedings, Second Conference on Empirical Methods in Natural Language Processing, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modelling.</title>
<date>1996</date>
<journal>Computer Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="2648" citStr="Rosenfeld, 1996" startWordPosition="426" endWordPosition="427">curred so far within a document, and using this information to alter probabilities of occurrence of particular choices for the word being predicted, has proved a winning strategy (Kuhn et al., 1990). Models using trigger pairs of words, i.e. pairs consisting of a &amp;quot;triggering&amp;quot; word which has already occurred in the document being processed, plus a specific &amp;quot;triggered&amp;quot; word whose probability of occurrence as the next word of the document needs to be estimated, have yielded perplexity&apos; reductions of 29-38% over the baseline trigram model, for a 5-million-word Wall Street Journal training corpus (Rosenfeld, 1996). This paper introduces the idea of using triggerpair techniques to assist in the prediction of rule and tag occurrences, within the context of naturallanguage parsing and tagging. Given the task of predicting the correct rule to associate with a parsetree node, or the correct tag to associate with a word of text, and assuming a particular class of parsing or tagging model, we quantify the information gain realized by taking account of rule or tag trigger-pair predictors, i.e. pairs consisting of a &amp;quot;triggering&amp;quot; rule or tag which has already occurred in the document being processed, plus a spec</context>
<context position="4420" citStr="Rosenfeld, 1996" startWordPosition="718" endWordPosition="719">e 1See Section 2. 131 additional information gains accruing from trigger— pair modelling when we know what sort of document is being parsed or tagged. We present our experimental results in Section 4, and discuss them in Section 5. In Section 6, we present some example trigger pairs; and we conclude, with a glance at projected future research, in Section 7. 2 Background Trigger—pair modelling research has been pursued within the field of language modelling for speech recognition over the last decade (Beeferman et al., 1997; Della Pietra et al., 1992; Kupiec, 1989; Lau, 1994; Lau et al., 1993; Rosenfeld, 1996). Fundamentally, the idea is a simple one: if you have recently seen a word in a document, then it is more likely to occur again, or, more generally, the prior occurrence of a word in a document affects the probability of occurrence of itself and other words. More formally, from an information—theoretic viewpoint, we can interpret the process as the relationship between two dependent random variables. Let the outcome (from the alphabet of outcomes Ay) of a random variable Y be observed and used to predict a random variable X (with alphabet Ax). The probability distribution of X, in our case, i</context>
<context position="6106" citStr="Rosenfeld, 1996" startWordPosition="1021" endWordPosition="1022"> outcome of Y. /(X; Y) = H(X)— H(XIY) E E P(x, y) log, P(x,y) P(x)P(y) (2) rEAx yEAy The dependency information between a word and its history may be captured by the trigger pair.3 A trigger pair is an ordered pair of words t and w. Knowledge that the trigger word t has occurred within some window of words in the history, changes 2A more intuitive view of entropy is provided through perplexity (Jelinek et al., 1977) which is a measure of the number of choices, on average, there are for a random variable. It is defined to be: 211(x). 3For a thorough description of trigger-based modelling, see (Rosenfeld, 1996). the probability estimate that word w will occur subsequently. Selection of these triggers can be performed by calculating the average mutual information between word pairs over a training corpus. In this case, the alphabet Ax = {w, t7}, the presence or absence of word w; similarly, Ay = {t, t}, the presence or absence of the triggering word in the history. This is a measure of the effect that the knowledge of the occurrence of the triggering word t has on the occurence of word w, in terms of the entropy (and therefore perplexity) reduction it will provide. Clearly, in the absence of other co</context>
<context position="8569" citStr="Rosenfeld, 1996" startWordPosition="1436" endWordPosition="1437"> distance effects, they are able to improve the performance of a trigger model. We are unaware of any work on the use of trigger pairs in parsing or tagging. In fact, we have not found any previous research in which extrasentential data of any sort are applied to the problem of parsing or tagging. 3 The Experiments 3.1 Experimental Design In order to investigate the utility of using long— range trigger information in tagging and parsing 4i.e. words which trigger words other than themselves &apos;i.e. words which trigger themselves 132 tasks. we adopt the simple mutual-information approach used in (Rosenfeld, 1996). We carry over into the domain of tags and rules an experiment from Rosenfeld&apos;s paper the details of which we outline below. The idea is to measure the information contributed (in bits, or, equivalently in terms of perplexity reduction) by using the triggers. Using this technique requires special care to ensure that information &amp;quot;added&amp;quot; by the triggers is indeed additional information. For this reason, in all our experiments we use the unigram model as our base model and we allow only one trigger for each tag (or rule) token.&apos; We derive these unigram probabilities from the training corpus and </context>
<context position="17010" citStr="Rosenfeld, 1996" startWordPosition="2825" endWordPosition="2826"> 400 450 500 Window size (sentences) Figure 1: Mutual information gain varying window size 5 Discussion The main result of this paper is to show that analogous to the case of words in language modelling, a significant amount of extrasentential information can be extracted from the long—range history of a document, using trigger pairs for tags and rules. Although some redundancy of information is inevitable, we have taken care to exclude as much information as possible that is already available to (intrasentential—data—based, i.e. all known) parsers and taggers. Quantitatively, the studies of (Rosenfeld, 1996) yielded a total mutual information gain of 0.38 bits, using Wall Street Journal data, with one trigger per word. In a parallel experiment, using the same technique, but on the ATR/Lancaster corpus, the total mutual information of the triggers for tags was 0.41 bits. This figure increases to 0.52 bits when tags further away than 135 tags (the approximate equivalent in words to the optimal window size in sentences) are excluded from the history. For the remainder of our experiments, we do not use as part of the history the tags/rules from the sentence containing the token to be predicted. This </context>
<context position="18311" citStr="Rosenfeld, 1996" startWordPosition="3047" endWordPosition="3048">available to parsers and taggers. In the case of tags, using the optimal window size, the gain was 0.31 bits, and for rules the information gain was 0.12 bits. Although these figures are not as large as for the case where intrasentential information is incorporated, they are sufficiently close to encourage us to exploit this information in our models. For the case of words, the evidence shows that triggers derived in the same manner as the triggers in our experiments, can provide a substantial amount of new information when used in combination with sophisticated language models. For example, (Rosenfeld, 1996) used a maximum—entropy tags — rules ... 134 Part. 1: Source Tart. 4: Source + Doc Type Part. 5: Source + Domain Sents Class Name Sents Class Name Sents Class Name 1: Assoc. Press, WSJ 8851 1: Legislative 5626 1: Recreation 3545 2: Canadian Hansards 5002 (incl. Srce.2) 2: Business 2055 3: General English 23105 2: Transcripts 44287 3: Science, Techn. 4018 4: Travel-domain dialgs 43341 (incl. Srce.4) 4: Humanities 2224 Part. 2: List Structure 3: News 8614 5: Daily Living 896 Class Name Sents (incl. most Srce.1) 6: Health, Education 1649 1: COntains lists 14147 4: Polemical essays 5160 7: Governm</context>
<context position="23078" citStr="Rosenfeld, 1996" startWordPosition="3769" endWordPosition="3770">r-Pairs Arising From Partitioned vs. Unpartitioned Training Sets model trained on 5 million words, with only trigger, uni-, bi- and trigram constraints, to measure the test-set perpexity reduction with respect to a &amp;quot;compact&amp;quot; backoff trigram model, a well-respected model in the language-modelling field. When the top six triggers for each word were used, test-set perplexity was reduced by 25%. Furthermore, when a more sophisticated version of this model13 was applied in conjunction with the SPHINX II speech recognition system (Huang et al., 1993), a 10-14% reduction in word error rate resulted (Rosenfeld, 1996). We see no reason why this effect should not carry over to tag and rule tokens, and are optimistic that long-range trigger information can be used in both parsing and tagging to improve performance. For words (Rosenfeld, 1996), self-triggers—words which triggered themselves—were the most frequent kind of triggers (68% of all word triggers were selftriggers). This is also the case for tags and rules. For tags, 76.8% were self-triggers, and for rules, 96.5% were self-triggers. As in the case of words, the set of self-triggers provides the most useful predictive information. 6 Some Examples We w</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modelling. Computer Speech and Language, 10:187-228.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>