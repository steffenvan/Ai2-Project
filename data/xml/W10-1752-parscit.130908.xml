<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000246">
<title confidence="0.965139">
Normalized Compression Distance Based Measures for
MetricsMATR 2010
</title>
<author confidence="0.993073">
Marcus Dobrinkat and Jaakko V¨ayrynen and Tero Tapiovaara
</author>
<affiliation confidence="0.998062">
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
</affiliation>
<address confidence="0.947162">
P.O. Box 15400, FI-00076 Aalto, Finland
</address>
<email confidence="0.995986">
{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fi
</email>
<author confidence="0.983393">
Kimmo Kettunen
</author>
<affiliation confidence="0.999498">
Kymenlaakso University of Applied Sciences
</affiliation>
<address confidence="0.929431">
P.O. Box 9, FI-48401 Kotka, Finland
</address>
<email confidence="0.995151">
Kimmo.kettunen@kyamk.fi
</email>
<sectionHeader confidence="0.997316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996365">
We present the MT-NCD and MT-mNCD
machine translation evaluation metrics
as submission to the machine transla-
tion evaluation shared task (MetricsMATR
2010). The metrics are based on nor-
malized compression distance (NCD), a
general information theoretic measure of
string similarity, and evaluated against hu-
man judgments from the WMT08 shared
task. The experiments show that 1)
our metric improves correlation to hu-
man judgments by using flexible match-
ing, 2) segment replication is effective,
and 3) our NCD-inspired method for mul-
tiple references indicates improved results.
Generally, the proposed MT-NCD and
MT-mNCD methods correlate competi-
tively with human judgments compared to
commonly used machine translations eval-
uation metrics, for instance, BLEU.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999908980769231">
The quality of automatic machine translation
(MT) evaluation metrics plays an important role
in the development of MT systems. Human eval-
uation would no longer be necessary if automatic
MT metrics correlated perfectly with manual judg-
ments. Besides high correlation with human judg-
ments of translation quality, a good metric should
be language independent, fast to compute and sen-
sitive enough to reliably detect small improve-
ments in MT systems.
Recently there have been some experiments
with normalized compression distance (NCD) as a
method for automatic evaluation of machine trans-
lation. NCD is a general string similarity measure
that has been useful for clustering in various tasks
(Cilibrasi and Vitanyi, 2005).
Parker (2008) introduced BADGER, a machine
translation evaluation metric that uses NCD to-
gether with a language independent word normal-
ization method. Kettunen (2009) independently
applied NCD to the direct evaluation of transla-
tions. He showed with a small corpus of three lan-
guage pairs that the scores of NCD and METEOR
(v0.6) from translations of 10–12 MT systems
were highly correlated.
V¨ayrynen et al. (2010) have extended the work
by showing that NCD can be used to rank transla-
tions of different MT systems so that the ranking
order correlates with human rankings at the same
level as BLEU (Papineni et al., 2001). For trans-
lations into English, NCD had an overall system-
level correlation of 0.66 whereas the best method,
ULC had an overall correlation of 0.76, and BLEU
had an overall correlation of 0.65. NCD presents
a viable alternative to the de facto standard BLEU.
Both metrics are language independent, simple
and efficient to compute. However, NCD is a
general measure of similarity that has been ap-
plied in many domains. More advanced meth-
ods achieve better correlation with human judg-
ments, but typically use additional language spe-
cific linguistic resources. Dobrinkat et al. (2010)
experimented with relaxed word matching, adding
language specific resources to NCD. The metric
called mNCD, which works similarly to mBLEU
(Agarwal and Lavie, 2008), showed improved cor-
relation to human judgments in English, the only
language where a METEOR synonym module was
used.
The motivation for this challenge submission is
to evaluate the MT-NCD and MT-mNCD metric
performance in an open competition with state-of-
</bodyText>
<page confidence="0.990518">
343
</page>
<note confidence="0.4536065">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 343–348,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999139833333333">
the-art MT evaluation metrics. Our experiments
and submission build on NCD and mNCD. We ex-
pand NCD to handle multiple references and re-
port experimental results for replicating segments
as a preprocessing step that improves the NCD as
an MT evaluation metric.
</bodyText>
<sectionHeader confidence="0.936623" genericHeader="method">
2 NCD-based NIT evaluation metrics
</sectionHeader>
<bodyText confidence="0.99928075">
NCD-based MT evaluation metrics build on the
idea that a string x is similar to another string y,
when both share common substrings. When de-
scribing y, common substrings do not have to be
repeated, but can be referenced to x. This is done
when compressing the concatenation of x and y,
which results in smaller output when more infor-
mation of y is already included in x.
</bodyText>
<subsectionHeader confidence="0.918595">
2.1 Normalized Compression Distance
</subsectionHeader>
<bodyText confidence="0.999984">
The normalized compression distance, as defined
by Cilibrasi and Vitanyi (2005) is given in Equa-
tion 1, in which C(x) is the length of the compres-
sion of x and C(x, y) is the length of the compres-
sion of the concatenation of x and y.
</bodyText>
<equation confidence="0.9995875">
C(x, y) − min {C(x), C(y)}
NCD(x, y) = max {C(x), C(y)} (1)
</equation>
<bodyText confidence="0.999860263157895">
NCD computes the distance as a score closer to
one for very different strings and closer to zero for
more similar strings. Most MT evaluation met-
rics are defined as similarity measures in contrast
to NCD, which is a distance measure. For eas-
ier comparison with other MT evaluation metrics,
we define the NCD based MT evaluation similar-
ity metric MT-NCD as 1 − NCD.
NCD is a practically usable form of the uncom-
putable normalized information distance (NID), a
general metric for the similarity of two objects.
NID is based on the notion of Kolmogorov com-
plexity K(x), a theoretical measure for the algo-
rithmic information content of a string x. It is de-
fined as the shortest universal Turing machine that
prints x and stops (Solomonoff, 1964). NCD ap-
proximates NID by the use of a compressor C(x)
that presents a computable approximation of the
Kolmogorov complexity K(x).
</bodyText>
<subsectionHeader confidence="0.997444">
2.2 NCD with multiple references
</subsectionHeader>
<bodyText confidence="0.978038357142857">
Most ideas can be described with in different
ways, therefore using only one reference transla-
tion for the evaluation of a candidate sentence is
not ideal and the exploitation of knowledge in sev-
eral different reference translations is helpful for
automatic MT evaluation.
One simple way for handling multiple refer-
ences is to evaluate against each reference indi-
vidually and select the maximum score. Although
this works, it is clearly not optimal. We developed
the NCDm metric, which is inspired by NCD. It
considers all references simultaneously and the
quality of a translation t against multiple refer-
ences R = {r1, ... , rm} is assessed as
</bodyText>
<equation confidence="0.9678135">
max{C(t|R), min
rER
max{C(t), min
rER
</equation>
<bodyText confidence="0.999869571428571">
where C(x|y) = C(x, y) − C(y) approximates
conditional algorithmic information with the com-
pressor C. The NCDm similarity metric with a
single reference (m = 1) is equal to NCD in Equa-
tion 1. Again, we define MT-NCDm as 1−NCDm.
Figure 1 shows how both, the MT-NCDm and
the BLEU metric change with a different num-
ber of references when the translation is varied
from correct to a random sequence of words. The
scores are computed with 249 sentences from the
LDC2010E28Dev data set using the first reference
as the correct translation. A higher score with mul-
tiple references against the correct translation indi-
cates that the measure is able to take into account
information from multiple references at the same
time.
The words in the candidate translation are re-
placed with probability p with a word randomly
selected with uniform probability from a lexicon
created from all reference translations. This simu-
lates partially correct translations. The words are
changed in a simple way without deletions, inser-
tions or word order permutations. The MT-NCDm
score increases with more than one reference
translation and random changes to the sentence re-
duce the score roughly proportional to the number
of changed words. With BLEU, the score is af-
fected more by a small number of changes.
</bodyText>
<subsectionHeader confidence="0.987389">
2.3 mNCD
</subsectionHeader>
<bodyText confidence="0.999901">
One enhancement to the basic NCD as auto-
matic evaluation metric is mNCD (Dobrinkat et
al., 2010), which provides relaxed word matching
based on the flexible matching modules of ME-
TEOR (Agarwal and Lavie, 2008).
What mNCD does is that it changes the ref-
erence sentence to be more similar to the candi-
</bodyText>
<equation confidence="0.979515333333333">
C(r|t}
NCDm(t, R) =
C(r)} (2)
</equation>
<page confidence="0.993969">
344
</page>
<table confidence="0.9736853">
MT−NCDm with 3 references
MT−NCDm with 2 references
MT−NCDm with 1 reference
BLEU with 3 references
BLEU with 2 references
BLEU with 1 reference
MT−NCDm
BLEU
0.0 0.2 0.4 0.6 0.8 1.0
word change probability (p)
</table>
<figureCaption confidence="0.98485675">
Figure 1: The MT-NCDm and BLEU scores with
a different number of multiple references against
correct translation with random word change
probability (p).
</figureCaption>
<bodyText confidence="0.976661066666667">
date, given that some of the words are synonyms
or share the same stem. Subsequent analysis using
any n-gram based automatic analysis should result
in a larger similarity score in the hope that this re-
flects more than just the surface similarity between
the candidate and the reference.
Given suitable Wordnet resources, mNCD
should alleviate the problem of translation vari-
ability especially in absence of multiple refer-
ence translations. Our submission uses the de-
fault METEOR exact stem synonym mod-
ules, which provide synonyms only for English.
We base our submission metric on the MT-NCD
metric and therefore define MT-mNCD as 1 −
mNCD.
</bodyText>
<sectionHeader confidence="0.996181" genericHeader="method">
3 NIT Evaluation System Description
</sectionHeader>
<subsectionHeader confidence="0.995479">
3.1 System Parameters
</subsectionHeader>
<bodyText confidence="0.9992834">
The system parameters for the submission metrics
include how candidates and references are prepro-
cessed, the choice of compressor for the NCD it-
self, as well as the granularity of how large seg-
ments are evaluated by NCD and how they are
combined into a final score.
Partly due to time constraints we decided not to
introduce language specific parameters, therefore
we chose those parameter values that perform well
in overall and are simple to compute.
</bodyText>
<subsectionHeader confidence="0.837822">
3.1.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999882294117647">
Character casing For MT-NCD, we did ex-
periments without preprocessing and with lower-
casing candidates and references. On average over
all tasks for language pairs into English, lower-
casing consistently decreased the RANK correla-
tion scores but increased the CONST correlation
scores. No consistent effect could be found for the
language pairs from English. In our submission
metrics we use no preprocessing.
For MT-mNCD the used METEOR matching
module lower-cases the adapted words by default.
After adapting a synonym in a reference, we tried
to keep the casing as it was in the candidate, which
we called real-casing. We use no real-casing for
our submitted MT-mNCD metric as this did not
improve results consistently over all task into En-
glish.
</bodyText>
<subsubsectionHeader confidence="0.519437">
Segment Replication Compression algorithms
</subsubsectionHeader>
<bodyText confidence="0.999935238095238">
may not work optimally with short strings, which
would deteriorate the approximation of Kol-
mogorov complexity. Our hypothesis was that
a replication of a string (&amp;quot;abc&amp;quot;) multiple times
(3 x &amp;quot;abc&amp;quot; = &amp;quot;abcabcabc&amp;quot;) could help the com-
pression algorithm to produce a better estimate of
the algorithmic information. This was tested in
the MT evaluation framework, and correlation be-
tween MT-NCD and human judgments improved
when the segments were replicated two times.
Further replication did not produce improvements.
Results for the MT-NCD metric with replica-
tions one, two and three times are shown in Ta-
ble 1. The results are averages over all used lan-
guages. With two compared to one replication, the
details for each language show that RANK corre-
lation is improved for the target languages English
and French, but degrades for German and Spanish.
CONST andYES/NO correlation improve for all
languages except German. We did not use repli-
cation in our submissions.
</bodyText>
<subsectionHeader confidence="0.944022">
3.1.2 Block size
</subsectionHeader>
<bodyText confidence="0.934789666666667">
The block size parameter governs the number of
joined segments that are compared with NCD as a
single string. On one extreme, with block size one,
</bodyText>
<figure confidence="0.4744485">
0.0 0.1 0.2 0.3 0.4 0.5 0.6
MT evaluation metric score
</figure>
<page confidence="0.742147">
345
</page>
<table confidence="0.999678">
MT-NCD rep 1 .61 .71 .73 .68
MT-NCD rep 2 .62 .73 .75 .70
MT-NCD rep 3 .61 .72 .74 .69
</table>
<tableCaption confidence="0.998727">
Table 1: Effect of the replication factor on
</tableCaption>
<bodyText confidence="0.993974739130435">
MT-NCD correlation scores for the bz2 compres-
sor with block size one as average over all lan-
guages.
each segment is evaluated separately and the seg-
ment scores are aggregated to a document score.
This is similar to how other MT metrics, for ex-
ample, BLEU, work. The other extreme is to join
all segments together, with block size equal to the
number of segments, and evaluate it as a single
string, which is similar to document comparison.
For block aggregation we experimented with arith-
metic and geometric mean and obtained very sim-
ilar results. We selected arithmetic mean for the
submission metrics.
Figure 2 shows the block size effect on the cor-
relation between MT-NCD and human judgments
for different target languages. Except for Spanish,
our experiments indicate that the block size value
has little effect. Therefore, and given how other
evaluation metrics work, we chose a block size of
one for our submission metrics. We noticed incon-
sistencies with Spanish in other settings as well
and will investigate these issues further.
</bodyText>
<subsectionHeader confidence="0.600828">
3.1.3 Compressor
</subsectionHeader>
<bodyText confidence="0.999995916666667">
There are several universal compressors that can
be utilized with NCD, for instance, zlib/gzip, bz2
and PPMZ, which represent different approaches
to compression. In terms of compression rate,
PPMZ is the best of the mentioned methods, but
it is considerably slower to compute compared to
the other methods. In terms of correlation with hu-
man judgments, NCD using bz2 performs slightly
worse than using PPMZ. Given much shorter com-
pression times for bz2 with very little correlation
performance degradation, our choice for the sub-
mission is the more standard bz2 compressor.
</bodyText>
<subsectionHeader confidence="0.50948">
3.1.4 Segment Interleaving
</subsectionHeader>
<bodyText confidence="0.924082666666667">
Computation of NCD between longer texts (e.g.
documents) may exceed the internal compressor
window size that is present in some compression
</bodyText>
<figure confidence="0.316201">
block size in lines
</figure>
<figureCaption confidence="0.933991">
Figure 2: Effect of the block size on the correlation
</figureCaption>
<bodyText confidence="0.970932222222222">
of MT-NCD to human judgments for the system
level evaluation.
algorithms (Cebrian et al., 2005). In this case,
only a part of the texts to be compared are visible
at any time to the compressor and similarities to
the text outside the window will be missed. One
solution for the MT evaluation task is to use uti-
lize the known parallel segments of candidate and
reference translations. The two segment lists can
be interleaved so that the corresponding segments
are always adjacent and the compression window
size is not exceeded for matching segments.
For our submission, we chose a block size of
one, therefore every segment is evaluated individ-
ually. As a result, segment interleaving does not
have any effect. Segment interleaving is affective
in the block size evaluation and results shown in
Figure 2.
</bodyText>
<subsectionHeader confidence="0.999812">
3.2 Evaluation Experiments
</subsectionHeader>
<bodyText confidence="0.999977333333333">
We chose parameters and evaluated our metrics
using the WMT08 part of the MetricsMATR 2010
development data, which contains human judg-
ments of the 2008 ACL Workshop on Statistical
Machine Translation (Callison-Burch et al., 2008)
for translations from a total of 30 MT systems be-
tween English and five other European languages.
There are human evaluations and several auto-
matic evaluations for the translations, divided into
several tasks defined by the language pair and the
domain of the translated sentences. For each of
these tasks, the WMT08 data contains about 2 000
</bodyText>
<figure confidence="0.983838230769231">
2 5 10 20 50 100 500 2000 5000
y
correlation with human judgements
0.0 0.2 0.4 0.6 0.8 1.0
into fr
into en
into es
into de
fr
en
es
de
RANK CONST YES/NO TOTAL
</figure>
<page confidence="0.997852">
346
</page>
<bodyText confidence="0.999963785714286">
reference sentences (segments) plus their aligned
translations for 12 to 17 different translation sys-
tems, depending on the language pair.
The human judgments include three categories
which contain evaluations for at most one segment
at a time, not whole documents. In the RANK
category, humans had to rank the output of five
MT systems according to quality. The CONST
category contains rankings for short phrases (con-
stituents), and the YES/NO category contains bi-
nary answers to judge if a short phrase is an ac-
ceptable translation or not.
We report RANK, CONST and YES/NO system
level correlations to human judgments as results of
our metrics for French, Spanish and German both
from and to English. The English–Spanish news
task was left out as most metrics had negative cor-
relation with human judgments.
The evaluation methodology used in Callison-
Burch et al. (2008) allows us to measure how each
MT evaluation metric correlates with human judg-
ments on the system level, in which all translations
from each MT system are aggregated into a single
score. The system rankings based on the scores
are compared to human judgments.
Spearman’s rank correlation coefficient p was
calculated between each MT metric and human
judgment category using the simplified equation:
</bodyText>
<equation confidence="0.831761">
p = 1 − n(nEZ di)(3)
</equation>
<bodyText confidence="0.999995571428571">
where for each system i, di is the difference be-
tween the rank derived from annotators’ input and
the rank obtained from the metric. From the anno-
tators’ input, the n MT systems were ranked based
on the number of times each system’s output was
selected as the best translation divided by the num-
ber of times each system was part of a judgment.
</bodyText>
<subsectionHeader confidence="0.793794">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999502842105263">
The results for WMT08 data for our submitted
metrics are shown in Table 2 and are sorted by the
RANK category separately for language pairs from
English and into English.
For tasks into English, the correlations show
that MT-mNCD improves over the MT-NCD met-
ric in all categories. Also the flexible match-
ing seems to work better for NCD-based metrics
than for BLEU, where mBLEU only improves
the CONST correlation scores. For tasks from
English, MT-mNCD shows slightly higher cor-
relation compared to MT-NCD, except for the
YES/NO category. The standard BLEU correla-
tion score is best of the shown evaluation met-
rics. Relaxed matching using mBLEU does not
improve BLEU’s RANK correlation scores here
either, but CONST and YES/NO correlation per-
forms better relative to BLEU than MT-mNCD
compared to MT-NCD.
</bodyText>
<table confidence="0.999455625">
MT-mNCD .61 .74 .75 .70
MT-NCD .57 .69 .71 .66
mBLEU .50 .76 .70 .65
BLEU .50 .72 .74 .65
BLEU .68 .79 .79 .75
MT-mNCD .67 .76 .74 .72
MT-NCD .65 .73 .75 .71
mBLEU .63 .81 .81 .75
</table>
<tableCaption confidence="0.996614">
Table 2: Average system-level correlations for the
</tableCaption>
<bodyText confidence="0.825108333333333">
WMT08 data sorted by RANK into English and
from English for our submitted metrics MT-NCD
and MT-mNCD and for BLEU and mBLEU
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999947826086957">
In our submissions, we applied MT-NCD and
MT-mNCD metrics and extended the NCD MT
evaluation metric to handle multiple references.
The reported experiment indicate a possible im-
provement for the multiple references.
We showed that a replication of segments as a
preprocessing step improves the correlation to hu-
man judgments. The string replication might alle-
viate problems in the compressor for short strings
and thus could provide better estimates of the al-
gorithmic information.
The results of our experiments show that re-
laxed matching in MT-mNCD works well with
proper synonym dictionaries, but is less effective
for tasks from English, which only use stemming.
MT-mNCD and MT-NCD are reasonably sim-
ple to compute and utilize standard and widely
used resources, such as the bz2 compression al-
gorithm and WordNet. The metrics perform com-
parable to the de facto standard BLEU. Improve-
ments with language dependent resources, in par-
ticular relaxed matching using synonym dictionar-
ies proved to be useful.
</bodyText>
<figure confidence="0.86081">
RANK CONST YES/NO TOTAL
INTO EN
FROM EN
</figure>
<page confidence="0.9903">
347
</page>
<sectionHeader confidence="0.99784" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989347826087">
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In StatMT ’08: Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 115–118, Morristown, NJ, USA. Association
for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
StatMT ’08: Proceedings of the Third Workshop
on Statistical Machine Translation, pages 70–106,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Manuel Cebrian, Manuel Alfonseca, and Alfonso Or-
tega. 2005. Common pitfalls using the normalized
compression distance: What to watch out for in a
compressor. Communications in Information and
Systems, 5(4):367–384.
Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering
by compression. IEEE Transactions on Information
Theory, 51:1523–1545.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko J.
V¨ayrynen, and Kimmo Kettunen. 2010. Evaluating
machine translations using mNCD. In Proceedings
of the ACL-2010 (to appear), Uppsala, Sweden.
Kimmo Kettunen. 2009. Packing it all up in search for
a language independent MT quality measure tool. In
Proceedings of LTC-09, 4th Language and Technol-
ogy Conference, pages 280–284, Poznan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center.
Steven Parker. 2008. BADGER: A new machine trans-
lation metric. In Metrics for Machine Translation
Challenge 2008, Waikiki, Hawai’i, October. AMTA.
Ray Solomonoff. 1964. Formal theory of inductive
inference. Part I. Information and Control, 7(1):1–
22.
Jaakko J. V¨ayrynen, Tero Tapiovaara, Kimmo Ket-
tunen, and Marcus Dobrinkat. 2010. Normalized
compression distance as an automatic MT evalua-
tion metric. In Proceedings of MT 25 years on. To
appear.
</reference>
<page confidence="0.998053">
348
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100582">
<title confidence="0.993705">Normalized Compression Distance Based Measures</title>
<address confidence="0.44076">MetricsMATR 2010</address>
<author confidence="0.963534">Dobrinkat V¨ayrynen</author>
<affiliation confidence="0.7708496">Adaptive Informatics Research Aalto University School of Science and P.O. Box 15400, FI-00076 Aalto, Kimmo Kymenlaakso University of Applied</affiliation>
<address confidence="0.953905">P.O. Box 9, FI-48401 Kotka,</address>
<email confidence="0.956258">Kimmo.kettunen@kyamk.fi</email>
<abstract confidence="0.989476">We present the MT-NCD and MT-mNCD machine translation evaluation metrics as submission to the machine translation evaluation shared task (MetricsMATR 2010). The metrics are based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and evaluated against human judgments from the WMT08 shared task. The experiments show that 1) our metric improves correlation to human judgments by using flexible matching, 2) segment replication is effective, and 3) our NCD-inspired method for multiple references indicates improved results. Generally, the proposed MT-NCD and MT-mNCD methods correlate competitively with human judgments compared to commonly used machine translations evaluation metrics, for instance, BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhaya Agarwal</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>115--118</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3303" citStr="Agarwal and Lavie, 2008" startWordPosition="491" endWordPosition="494">erall correlation of 0.76, and BLEU had an overall correlation of 0.65. NCD presents a viable alternative to the de facto standard BLEU. Both metrics are language independent, simple and efficient to compute. However, NCD is a general measure of similarity that has been applied in many domains. More advanced methods achieve better correlation with human judgments, but typically use additional language specific linguistic resources. Dobrinkat et al. (2010) experimented with relaxed word matching, adding language specific resources to NCD. The metric called mNCD, which works similarly to mBLEU (Agarwal and Lavie, 2008), showed improved correlation to human judgments in English, the only language where a METEOR synonym module was used. The motivation for this challenge submission is to evaluate the MT-NCD and MT-mNCD metric performance in an open competition with state-of343 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 343–348, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics the-art MT evaluation metrics. Our experiments and submission build on NCD and mNCD. We expand NCD to handle multiple references and report experimenta</context>
<context position="7839" citStr="Agarwal and Lavie, 2008" startWordPosition="1254" endWordPosition="1257">erence translations. This simulates partially correct translations. The words are changed in a simple way without deletions, insertions or word order permutations. The MT-NCDm score increases with more than one reference translation and random changes to the sentence reduce the score roughly proportional to the number of changed words. With BLEU, the score is affected more by a small number of changes. 2.3 mNCD One enhancement to the basic NCD as automatic evaluation metric is mNCD (Dobrinkat et al., 2010), which provides relaxed word matching based on the flexible matching modules of METEOR (Agarwal and Lavie, 2008). What mNCD does is that it changes the reference sentence to be more similar to the candiC(r|t} NCDm(t, R) = C(r)} (2) 344 MT−NCDm with 3 references MT−NCDm with 2 references MT−NCDm with 1 reference BLEU with 3 references BLEU with 2 references BLEU with 1 reference MT−NCDm BLEU 0.0 0.2 0.4 0.6 0.8 1.0 word change probability (p) Figure 1: The MT-NCDm and BLEU scores with a different number of multiple references against correct translation with random word change probability (p). date, given that some of the words are synonyms or share the same stem. Subsequent analysis using any n-gram bas</context>
</contexts>
<marker>Agarwal, Lavie, 2008</marker>
<rawString>Abhaya Agarwal and Alon Lavie. 2008. METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 115–118, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14540" citStr="Callison-Burch et al., 2008" startWordPosition="2354" endWordPosition="2357">he corresponding segments are always adjacent and the compression window size is not exceeded for matching segments. For our submission, we chose a block size of one, therefore every segment is evaluated individually. As a result, segment interleaving does not have any effect. Segment interleaving is affective in the block size evaluation and results shown in Figure 2. 3.2 Evaluation Experiments We chose parameters and evaluated our metrics using the WMT08 part of the MetricsMATR 2010 development data, which contains human judgments of the 2008 ACL Workshop on Statistical Machine Translation (Callison-Burch et al., 2008) for translations from a total of 30 MT systems between English and five other European languages. There are human evaluations and several automatic evaluations for the translations, divided into several tasks defined by the language pair and the domain of the translated sentences. For each of these tasks, the WMT08 data contains about 2 000 2 5 10 20 50 100 500 2000 5000 y correlation with human judgements 0.0 0.2 0.4 0.6 0.8 1.0 into fr into en into es into de fr en es de RANK CONST YES/NO TOTAL 346 reference sentences (segments) plus their aligned translations for 12 to 17 different transla</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 70–106, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Cebrian</author>
<author>Manuel Alfonseca</author>
<author>Alfonso Ortega</author>
</authors>
<title>Common pitfalls using the normalized compression distance: What to watch out for in a compressor.</title>
<date>2005</date>
<journal>Communications in Information and Systems,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="13573" citStr="Cebrian et al., 2005" startWordPosition="2196" endWordPosition="2199">he other methods. In terms of correlation with human judgments, NCD using bz2 performs slightly worse than using PPMZ. Given much shorter compression times for bz2 with very little correlation performance degradation, our choice for the submission is the more standard bz2 compressor. 3.1.4 Segment Interleaving Computation of NCD between longer texts (e.g. documents) may exceed the internal compressor window size that is present in some compression block size in lines Figure 2: Effect of the block size on the correlation of MT-NCD to human judgments for the system level evaluation. algorithms (Cebrian et al., 2005). In this case, only a part of the texts to be compared are visible at any time to the compressor and similarities to the text outside the window will be missed. One solution for the MT evaluation task is to use utilize the known parallel segments of candidate and reference translations. The two segment lists can be interleaved so that the corresponding segments are always adjacent and the compression window size is not exceeded for matching segments. For our submission, we chose a block size of one, therefore every segment is evaluated individually. As a result, segment interleaving does not </context>
</contexts>
<marker>Cebrian, Alfonseca, Ortega, 2005</marker>
<rawString>Manuel Cebrian, Manuel Alfonseca, and Alfonso Ortega. 2005. Common pitfalls using the normalized compression distance: What to watch out for in a compressor. Communications in Information and Systems, 5(4):367–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul Vitanyi</author>
</authors>
<title>Clustering by compression.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>51--1523</pages>
<contexts>
<context position="1933" citStr="Cilibrasi and Vitanyi, 2005" startWordPosition="269" endWordPosition="272">le in the development of MT systems. Human evaluation would no longer be necessary if automatic MT metrics correlated perfectly with manual judgments. Besides high correlation with human judgments of translation quality, a good metric should be language independent, fast to compute and sensitive enough to reliably detect small improvements in MT systems. Recently there have been some experiments with normalized compression distance (NCD) as a method for automatic evaluation of machine translation. NCD is a general string similarity measure that has been useful for clustering in various tasks (Cilibrasi and Vitanyi, 2005). Parker (2008) introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method. Kettunen (2009) independently applied NCD to the direct evaluation of translations. He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10–12 MT systems were highly correlated. V¨ayrynen et al. (2010) have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU</context>
<context position="4533" citStr="Cilibrasi and Vitanyi (2005)" startWordPosition="686" endWordPosition="689">results for replicating segments as a preprocessing step that improves the NCD as an MT evaluation metric. 2 NCD-based NIT evaluation metrics NCD-based MT evaluation metrics build on the idea that a string x is similar to another string y, when both share common substrings. When describing y, common substrings do not have to be repeated, but can be referenced to x. This is done when compressing the concatenation of x and y, which results in smaller output when more information of y is already included in x. 2.1 Normalized Compression Distance The normalized compression distance, as defined by Cilibrasi and Vitanyi (2005) is given in Equation 1, in which C(x) is the length of the compression of x and C(x, y) is the length of the compression of the concatenation of x and y. C(x, y) − min {C(x), C(y)} NCD(x, y) = max {C(x), C(y)} (1) NCD computes the distance as a score closer to one for very different strings and closer to zero for more similar strings. Most MT evaluation metrics are defined as similarity measures in contrast to NCD, which is a distance measure. For easier comparison with other MT evaluation metrics, we define the NCD based MT evaluation similarity metric MT-NCD as 1 − NCD. NCD is a practically</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2005</marker>
<rawString>Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering by compression. IEEE Transactions on Information Theory, 51:1523–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcus Dobrinkat</author>
<author>Tero Tapiovaara</author>
<author>Jaakko J V¨ayrynen</author>
<author>Kimmo Kettunen</author>
</authors>
<title>Evaluating machine translations using mNCD.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL-2010 (to appear),</booktitle>
<location>Uppsala,</location>
<marker>Dobrinkat, Tapiovaara, V¨ayrynen, Kettunen, 2010</marker>
<rawString>Marcus Dobrinkat, Tero Tapiovaara, Jaakko J. V¨ayrynen, and Kimmo Kettunen. 2010. Evaluating machine translations using mNCD. In Proceedings of the ACL-2010 (to appear), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Kettunen</author>
</authors>
<title>Packing it all up in search for a language independent MT quality measure tool.</title>
<date>2009</date>
<booktitle>In Proceedings of LTC-09, 4th Language and Technology Conference,</booktitle>
<pages>280--284</pages>
<location>Poznan.</location>
<contexts>
<context position="2101" citStr="Kettunen (2009)" startWordPosition="295" endWordPosition="296">human judgments of translation quality, a good metric should be language independent, fast to compute and sensitive enough to reliably detect small improvements in MT systems. Recently there have been some experiments with normalized compression distance (NCD) as a method for automatic evaluation of machine translation. NCD is a general string similarity measure that has been useful for clustering in various tasks (Cilibrasi and Vitanyi, 2005). Parker (2008) introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method. Kettunen (2009) independently applied NCD to the direct evaluation of translations. He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10–12 MT systems were highly correlated. V¨ayrynen et al. (2010) have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU (Papineni et al., 2001). For translations into English, NCD had an overall systemlevel correlation of 0.66 whereas the best method, ULC had an overall correlation of 0</context>
</contexts>
<marker>Kettunen, 2009</marker>
<rawString>Kimmo Kettunen. 2009. Packing it all up in search for a language independent MT quality measure tool. In Proceedings of LTC-09, 4th Language and Technology Conference, pages 280–284, Poznan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="2557" citStr="Papineni et al., 2001" startWordPosition="373" endWordPosition="376">Parker (2008) introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method. Kettunen (2009) independently applied NCD to the direct evaluation of translations. He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10–12 MT systems were highly correlated. V¨ayrynen et al. (2010) have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU (Papineni et al., 2001). For translations into English, NCD had an overall systemlevel correlation of 0.66 whereas the best method, ULC had an overall correlation of 0.76, and BLEU had an overall correlation of 0.65. NCD presents a viable alternative to the de facto standard BLEU. Both metrics are language independent, simple and efficient to compute. However, NCD is a general measure of similarity that has been applied in many domains. More advanced methods achieve better correlation with human judgments, but typically use additional language specific linguistic resources. Dobrinkat et al. (2010) experimented with </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Parker</author>
</authors>
<title>BADGER: A new machine translation metric.</title>
<date>2008</date>
<booktitle>In Metrics for Machine Translation Challenge</booktitle>
<publisher>AMTA.</publisher>
<location>Waikiki, Hawai’i,</location>
<contexts>
<context position="1948" citStr="Parker (2008)" startWordPosition="273" endWordPosition="274">stems. Human evaluation would no longer be necessary if automatic MT metrics correlated perfectly with manual judgments. Besides high correlation with human judgments of translation quality, a good metric should be language independent, fast to compute and sensitive enough to reliably detect small improvements in MT systems. Recently there have been some experiments with normalized compression distance (NCD) as a method for automatic evaluation of machine translation. NCD is a general string similarity measure that has been useful for clustering in various tasks (Cilibrasi and Vitanyi, 2005). Parker (2008) introduced BADGER, a machine translation evaluation metric that uses NCD together with a language independent word normalization method. Kettunen (2009) independently applied NCD to the direct evaluation of translations. He showed with a small corpus of three language pairs that the scores of NCD and METEOR (v0.6) from translations of 10–12 MT systems were highly correlated. V¨ayrynen et al. (2010) have extended the work by showing that NCD can be used to rank translations of different MT systems so that the ranking order correlates with human rankings at the same level as BLEU (Papineni et a</context>
</contexts>
<marker>Parker, 2008</marker>
<rawString>Steven Parker. 2008. BADGER: A new machine translation metric. In Metrics for Machine Translation Challenge 2008, Waikiki, Hawai’i, October. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Solomonoff</author>
</authors>
<title>Formal theory of inductive inference.</title>
<date>1964</date>
<journal>Part I. Information and Control,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>22</pages>
<contexts>
<context position="5489" citStr="Solomonoff, 1964" startWordPosition="865" endWordPosition="866">ost MT evaluation metrics are defined as similarity measures in contrast to NCD, which is a distance measure. For easier comparison with other MT evaluation metrics, we define the NCD based MT evaluation similarity metric MT-NCD as 1 − NCD. NCD is a practically usable form of the uncomputable normalized information distance (NID), a general metric for the similarity of two objects. NID is based on the notion of Kolmogorov complexity K(x), a theoretical measure for the algorithmic information content of a string x. It is defined as the shortest universal Turing machine that prints x and stops (Solomonoff, 1964). NCD approximates NID by the use of a compressor C(x) that presents a computable approximation of the Kolmogorov complexity K(x). 2.2 NCD with multiple references Most ideas can be described with in different ways, therefore using only one reference translation for the evaluation of a candidate sentence is not ideal and the exploitation of knowledge in several different reference translations is helpful for automatic MT evaluation. One simple way for handling multiple references is to evaluate against each reference individually and select the maximum score. Although this works, it is clearly</context>
</contexts>
<marker>Solomonoff, 1964</marker>
<rawString>Ray Solomonoff. 1964. Formal theory of inductive inference. Part I. Information and Control, 7(1):1– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaakko J V¨ayrynen</author>
<author>Tero Tapiovaara</author>
<author>Kimmo Kettunen</author>
<author>Marcus Dobrinkat</author>
</authors>
<title>Normalized compression distance as an automatic MT evaluation metric.</title>
<date>2010</date>
<booktitle>In Proceedings of MT 25</booktitle>
<note>years on. To appear.</note>
<marker>V¨ayrynen, Tapiovaara, Kettunen, Dobrinkat, 2010</marker>
<rawString>Jaakko J. V¨ayrynen, Tero Tapiovaara, Kimmo Kettunen, and Marcus Dobrinkat. 2010. Normalized compression distance as an automatic MT evaluation metric. In Proceedings of MT 25 years on. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>