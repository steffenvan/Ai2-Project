<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.983189">
Semi-Supervised Semantic Role Labeling
</title>
<author confidence="0.997152">
Hagen F¨urstenau
</author>
<affiliation confidence="0.840819333333333">
Dept. of Computational Linguistics
Saarland University
Saarbr¨ucken, Germany
</affiliation>
<email confidence="0.979606">
hagenf@coli.uni-saarland.de
</email>
<author confidence="0.981176">
Mirella Lapata
</author>
<affiliation confidence="0.920777666666667">
School of Informatics
University of Edinburgh
Edinburgh, UK
</affiliation>
<email confidence="0.994532">
mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.997348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99954552173913">
Large scale annotated corpora are pre-
requisite to developing high-performance
semantic role labeling systems. Unfor-
tunately, such corpora are expensive to
produce, limited in size, and may not be
representative. Our work aims to reduce
the annotation effort involved in creat-
ing resources for semantic role labeling
via semi-supervised learning. Our algo-
rithm augments a small number of man-
ually labeled instances with unlabeled ex-
amples whose roles are inferred automat-
ically via annotation projection. We for-
mulate the projection task as a generaliza-
tion of the linear assignment problem. We
seek to find a role assignment in the un-
labeled data such that the argument sim-
ilarity between the labeled and unlabeled
instances is maximized. Experimental re-
sults on semantic role labeling show that
the automatic annotations produced by our
method improve performance over using
hand-labeled instances alone.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996955685714286">
Recent years have seen a growing interest in the
task of automatically identifying and labeling the
semantic roles conveyed by sentential constituents
(Gildea and Jurafsky, 2002). This is partly due to
its relevance for applications ranging from infor-
mation extraction (Surdeanu et al., 2003; Mos-
chitti et al., 2003) to question answering (Shen and
Lapata, 2007), paraphrase identification (Pad´o and
Erk, 2005), and the modeling of textual entailment
relations (Tatu and Moldovan, 2005). Resources
like FrameNet (Fillmore et al., 2003) and Prop-
Bank (Palmer et al., 2005) have also facilitated the
development of semantic role labeling methods by
providing high-quality annotations for use in train-
ing. Semantic role labelers are commonly devel-
oped using a supervised learning paradigm1 where
a classifier learns to predict role labels based on
features extracted from annotated training data.
Examples of the annotations provided in
FrameNet are given in (1). Here, the meaning of
predicates (usually verbs, nouns, or adjectives) is
conveyed by frames, schematic representations of
situations. Semantic roles (or frame elements) are
defined for each frame and correspond to salient
entities present in the situation evoked by the pred-
icate (or frame evoking element). Predicates with
similar semantics instantiate the same frame and
are attested with the same roles. In our exam-
ple, the frame Cause harm has three core semantic
roles, Agent, Victim, and Body part and can be in-
stantiated with verbs such as punch, crush, slap,
and injure. The frame may also be attested with
non-core (peripheral) roles that are more generic
and often shared across frames (see the roles De-
gree, Reason, and Means, in (1c) and (1d)).
</bodyText>
<listItem confidence="0.886976">
(1) a. [Lee]Agent punched [John]Victim
[in the eye]Body part.
b. [A falling rock]Cause crushed [my
ankle]Body part.
c. [She]Agent slapped [him]Victim
[hard]Degree [for his change of
mood]Reason.
d. [Rachel]Agent injured [her
friend]Victim [by closing the car
door on his left hand]Means.
</listItem>
<bodyText confidence="0.998315">
The English FrameNet (version 1.3) contains
502 frames covering 5,866 lexical entries. It also
comes with a set of manually annotated exam-
ple sentences, taken mostly from the British Na-
tional Corpus. These annotations are often used
</bodyText>
<footnote confidence="0.952805">
1The approaches are too numerous to list; we refer the
interested reader to the proceedings of the SemEval-2007
shared task (Baker et al., 2007) for an overview of the state-
of-the-art.
</footnote>
<note confidence="0.961211">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220–228,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996917">
220
</page>
<bodyText confidence="0.99996785483871">
as training data for semantic role labeling sys-
tems. However, the applicability of these sys-
tems is limited to those words for which labeled
data exists, and their accuracy is strongly corre-
lated with the amount of labeled data available.
Despite the substantial annotation effort involved
in the creation of FrameNet (spanning approxi-
mately twelve years), the number of annotated in-
stances varies greatly across lexical items. For in-
stance, FrameNet contains annotations for 2,113
verbs; of these 12.3% have five or less annotated
examples. The average number of annotations per
verb is 29.2. Labeled data is thus scarce for indi-
vidual predicates within FrameNet’s target domain
and would presumably be even scarcer across do-
mains. The problem is more severe for languages
other than English, where training data on the
scale of FrameNet is virtually non-existent. Al-
though FrameNets are being constructed for Ger-
man, Spanish, and Japanese, these resources are
substantially smaller than their English counter-
part and of limited value for modeling purposes.
One simple solution, albeit expensive and time-
consuming, is to manually create more annota-
tions. A better alternative may be to begin with
an initial small set of labeled examples and aug-
ment it with unlabeled data sufficiently similar to
the original labeled set. Suppose we have man-
ual annotations for sentence (1a). We shall try and
find in an unlabeled corpus other sentences that
are both structurally and semantically similar. For
instance, we may think that Bill will punch me in
the face and I punched her hard in the head re-
semble our initial sentence and are thus good ex-
amples to add to our database. Now, in order to
use these new sentences as training data we must
somehow infer their semantic roles. We can prob-
ably guess that constituents in the same syntactic
position must have the same semantic role, espe-
cially if they refer to the same concept (e.g., “body
parts”) and thus label in the face and in the head
with the role Body part. Analogously, Bill and
I would be labeled as Agent and me and her as
Victim.
In this paper we formalize the method sketched
above in order to expand a small number of
FrameNet-style semantic role annotations with
large amounts of unlabeled data. We adopt a learn-
ing strategy where annotations are projected from
labeled onto unlabeled instances via maximizing
a similarity function measuring syntactic and se-
mantic compatibility. We formalize the annotation
projection problem as a generalization of the linear
assignment problem and solve it efficiently using
the simplex algorithm. We evaluate our algorithm
by comparing the performance of a semantic role
labeler trained on the annotations produced by our
method and on a smaller dataset consisting solely
of hand-labeled instances. Results in several ex-
perimental settings show that the automatic anno-
tations, despite being noisy, bring significant per-
formance improvements.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955184210527">
The lack of annotated data presents an obstacle
to developing many natural language applications,
especially when these are not in English. It is
therefore not surprising that previous efforts to re-
duce the need for semantic role annotation have
focused primarily on non-English languages.
Annotation projection is a popular framework
for transferring frame semantic annotations from
one language to another by exploiting the transla-
tional and structural equivalences present in par-
allel corpora. The idea here is to leverage the ex-
isting English FrameNet and rely on word or con-
stituent alignments to automatically create an an-
notated corpus in a new language. Pad´o and Lap-
ata (2006) transfer semantic role annotations from
English onto German and Johansson and Nugues
(2006) from English onto Swedish. A different
strategy is presented in Fung and Chen (2004),
where English FrameNet entries are mapped to
concepts listed in HowNet, an on-line ontology
for Chinese, without consulting a parallel corpus.
Then, Chinese sentences with predicates instan-
tiating these concepts are found in a monolin-
gual corpus and their arguments are labeled with
FrameNet roles.
Other work attempts to alleviate the data re-
quirements for semantic role labeling either by re-
lying on unsupervised learning or by extending ex-
isting resources through the use of unlabeled data.
Swier and Stevenson (2004) present an unsuper-
vised method for labeling the arguments of verbs
with their semantic roles. Given a verb instance,
their method first selects a frame from VerbNet, a
semantic role resource akin to FrameNet and Prop-
Bank, and labels each argument slot with sets of
possible roles. The algorithm proceeds iteratively
by first making initial unambiguous role assign-
ments, and then successively updating a probabil-
</bodyText>
<page confidence="0.996156">
221
</page>
<bodyText confidence="0.999959761904762">
ity model on which future assignments are based.
Being unsupervised, their approach requires no
manual effort other than creating the frame dic-
tionary. Unfortunately, existing resources do not
have exhaustive coverage and a large number of
verbs may be assigned no semantic role informa-
tion since they are not in the dictionary in the
first place. Pennacchiotti et al. (2008) address
precisely this problem by augmenting FrameNet
with new lexical units if they are similar to an ex-
isting frame (their notion of similarity combines
distributional and WordNet-based measures). In
a similar vein, Gordon and Swanson (2007) at-
tempt to increase the coverage of PropBank. Their
approach leverages existing annotations to handle
novel verbs. Rather than annotating new sentences
that contain novel verbs, they find syntactically
similar verbs and use their annotations as surro-
gate training data.
Our own work aims to reduce but not entirely
eliminate the annotation effort involved in creating
training data for semantic role labeling. We thus
assume that a small number of manual annotations
is initially available. Our algorithm augments
these with unlabeled examples whose roles are in-
ferred automatically. We apply our method in a
monolingual setting, and thus do not project an-
notations between languages but within the same
language. In contrast to Pennacchiotti et al. (2008)
and Gordon and Swanson (2007), we do not aim
to handle novel verbs, although this would be a
natural extension of our method. Given a verb
and a few labeled instances exemplifying its roles,
we wish to find more instances of the same verb
in an unlabeled corpus so as to improve the per-
formance of a hypothetical semantic role labeler
without having to annotate more data manually.
Although the use of semi-supervised learning is
widespread in many natural language tasks, rang-
ing from parsing to word sense disambiguation, its
application to FrameNet-style semantic role label-
ing is, to our knowledge, novel.
</bodyText>
<sectionHeader confidence="0.971882" genericHeader="method">
3 Semi-Supervised Learning Method
</sectionHeader>
<bodyText confidence="0.999981142857143">
Our method assumes that we have access to a
small seed corpus that has been manually anno-
tated. This represents a relatively typical situation
where some annotation has taken place but not on
a scale that is sufficient for high-performance su-
pervised learning. For each sentence in the seed
corpus we select a number of similar sentences
</bodyText>
<figure confidence="0.973843">
Fluidic motion
DET
V
our
</figure>
<figureCaption confidence="0.999612">
Figure 1: Labeled dependency graph with seman-
</figureCaption>
<bodyText confidence="0.991075944444444">
tic role annotations for the frame evoking ele-
ment (FEE) course in the sentence We can feel the
blood coursing through our veins again. The frame
is Fluidic motion, and its roles are Fluid and Path.
Directed edges (without dashes) represent depen-
dency relations between words, edge labels denote
types of grammatical relations (e.g., SUBJ, AUX).
from an unlabeled expansion corpus. These are
automatically annotated by projecting relevant se-
mantic role information from the labeled sentence.
The similarity between two sentences is opera-
tionalized by measuring whether their arguments
have a similar structure and whether they express
related meanings. The seed corpus is then en-
larged with the k most similar unlabeled sentences
to form the expanded corpus. In what follows we
describe in more detail how we measure similarity
and project annotations.
</bodyText>
<subsectionHeader confidence="0.997318">
3.1 Extracting Predicate-Argument
Structures
</subsectionHeader>
<bodyText confidence="0.999912866666667">
Our method operates over labeled dependency
graphs. We show an example in Figure 1 for
the sentence We can feel the blood coursing
through our veins again. We represent verbs
(i.e., frame evoking elements) in the seed and
unlabeled corpora by their predicate-argument
structure. Specifically, we record the direct de-
pendents of the predicate course (e.g., blood
or again in Figure 1) and their grammatical
roles (e.g., SUBJ, MOD). Prepositional nodes
are collapsed, i.e., we record the preposition’s
object and a composite grammatical role (like
IOBJ THROUGH, where IOBJ stands for “preposi-
tional object” and THROUGH for the preposition
itself). In addition to direct dependents, we also
</bodyText>
<figure confidence="0.983534321428571">
� � �
Fluid � � �
/
SUBJ
�������
IOBJ
�
�
�
FEE
�
�
�
A
again
��������
�MOD
� |� �
SUBJu����������� ����
{����������
AUX
we can
feel
����
� � XCOMP
�
�
�
�
DOBJ
/
/
blood
through
�
�
�
�
Path
course
the
DOBJ
l
vein
DET
V
222
Lemma GramRole SemRole
blood SUBJ Fluid
vein IOBJ THROUGH Path
again MOD —
function sim(σ) defined as:
(A - syn(gli, guσ(i)) + sem(wli, wσ
u(i)) − B)
�
iEM,
</figure>
<tableCaption confidence="0.78281">
Table 1: Predicate-argument structure for the verb
course in Figure 1.
</tableCaption>
<bodyText confidence="0.998914285714286">
consider nodes coordinated with the predicate as
arguments. Finally, for each argument node we
record the semantic roles it carries, if any. All sur-
face word forms are lemmatized. An example of
the argument structure information we obtain for
the predicate course (see Figure 1) is shown in Ta-
ble 1.
We obtain information about grammatical roles
from the output of RASP (Briscoe et al., 2006),
a broad-coverage dependency parser. However,
there is nothing inherent in our method that re-
stricts us to this particular parser. Any other
parser with broadly similar dependency output
could serve our purposes.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Measuring Similarity
</subsectionHeader>
<bodyText confidence="0.999150515151515">
For each frame evoking verb in the seed corpus our
method creates a labeled predicate-argument re-
presentation. It also extracts all sentences from the
unlabeled corpus containing the same verb. Not
all of these sentences will be suitable instances
for adding to our training data. For example, the
same verb may evoke a different frame with dif-
ferent roles and argument structure. We therefore
must select sentences which resemble the seed an-
notations. Our hypothesis is that verbs appearing
in similar syntactic and semantic contexts will be-
have similarly in the way they relate to their argu-
ments.
Estimating the similarity between two predi-
cate argument structures amounts to finding the
highest-scoring alignment between them. More
formally, given a labeled predicate-argument
structure pl with m arguments and an unla-
beled predicate-argument structure pu with n ar-
guments, we consider (and score) all possible
alignments between these arguments. A (partial)
alignment can be viewed as an injective function
σ : Mσ —* {1,...,n} where Mσ C {1, . . . , m}.
In other words, an argument i of pl is aligned to
argument σ(i) of pu if i E Mσ. Note that this al-
lows for unaligned arguments on both sides.
We score each alignment σ using a similarity
where syn(gli,guσ(i)) denotes the syntactic similar-
ity between grammatical roles gli and guσ(i) and
sem(wli, wuσ(i)) the semantic similarity between
head words wli and wuσ(i).
Our goal is to find an alignment such
that the similarity function is maximized:
</bodyText>
<equation confidence="0.9562885">
σ* := arg max sim(σ). This optimization
σ
</equation>
<bodyText confidence="0.975183">
problem is a generalized version of the linear
assignment problem (Dantzig, 1963). It can be
straightforwardly expressed as a linear program-
ming problem by associating each alignment σ
with a set of binary indicator variables xij:
</bodyText>
<equation confidence="0.820175833333333">
�1 if i E Mσ ∧ σ(i) = j
xij :=
0 otherwise
The similarity objective function then becomes:
( �
A - syn(gl i, gu j ) + sem(wl i, wu j ) − B xij
</equation>
<bodyText confidence="0.9941665">
subject to the following constraints ensuring that σ
is an injective function on some Mσ:
</bodyText>
<equation confidence="0.8440408">
n
xij &lt; 1 for all i = 1,...,m
j=1
�m xij &lt; 1 for all j = 1, ... , n
i=1
</equation>
<bodyText confidence="0.951153722222222">
Figure 2 graphically illustrates the alignment
projection problem. Here, we wish to project
semantic role information from the seed blood
coursing through our veins again onto the un-
labeled sentence Adrenalin was still coursing
through her veins. The predicate course has three
arguments in the labeled sentence and four in the
unlabeled sentence (represented as rectangles in
the figure). There are 73 possible alignments in
this example. In general, for any m and n argu-
ments, where m &lt; n, the number of alignments
is Ek %m!n!
k)!k! . Each alignment is scored
by taking the sum of the similarity scores of the in-
dividual alignment pairs (e.g., between blood and
be, vein and still). In this example, the highest
scoring alignment is between blood and adrenalin,
vein and vein, and again and still, whereas be is
</bodyText>
<equation confidence="0.986959">
�m n
i=1 j=1
</equation>
<page confidence="0.993057">
223
</page>
<bodyText confidence="0.999894675">
left unaligned (see the non-dotted edges in Fig-
ure 2). Note that only vein and blood carry seman-
tic roles (i.e., Fluid and Path) which are projected
onto adrenalin and vein, respectively.
Finding the best alignment crucially depends
on estimating the syntactic and semantic similar-
ity between arguments. We define the syntactic
measure on the grammatical relations produced
by RASP. Specifically, we set syn(gli,guσ(i)) to 1
if the relations are identical, to a &lt; 1 if the rela-
tions are of the same type but different subtype2
and to 0 otherwise. To avoid systematic errors,
syntactic similarity is also set to 0 if the predicates
differ in voice. We measure the semantic similar-
ity sem(wli, wuσ(i)) with a semantic space model.
The meaning of each word is represented by a vec-
tor of its co-occurrences with neighboring words.
The cosine of the angle of the vectors represent-
ing wl and wu quantifies their similarity (Section 4
describes the specific model we used in our exper-
iments in more detail).
The parameter A counterbalances the impor-
tance of syntactic and semantic information, while
the parameter B can be interpreted as the lowest
similarity value for which an alignment between
two arguments is possible. An optimal align-
ment Q* cannot link arguments i0 of pl and j0
of pu, if A · syn(gli0, guj0) + sem(wli0, wuj0) &lt; B
(i.e., either i0 V Mσ* or Q*(i0) =� j0). This
is because for an alignment Q with Q(i0) = j0
we can construct a better alignment Q0, which is
identical to Q on all i =� i0, but leaves i0 un-
aligned (i.e., i0 V Mσ0). By eliminating a neg-
ative term from the scoring function, it follows
that sim(Q0) &gt; sim(Q). Therefore, an alignment Q
satisfying Q(i0) = j0 cannot be optimal and con-
versely the optimal alignment Q* can never link
two arguments with each other if the sum of their
weighted syntactic and semantic similarity scores
is below B.
</bodyText>
<subsectionHeader confidence="0.999449">
3.3 Projecting Annotations
</subsectionHeader>
<bodyText confidence="0.999949428571429">
Once we obtain the best alignment Q* between pl
and pu, we can simply transfer the role of each
role-bearing argument i of pl to the aligned argu-
ment Q*(i) of pu, resulting in a labeling of pu.
To increase the accuracy of our method we dis-
card projections if they fail to transfer all roles
of the labeled to the unlabeled dependency graph.
</bodyText>
<footnote confidence="0.93009">
2This concerns fine-grained distinctions made by the
parser, e.g., the underlying grammatical roles in passive con-
structions.
</footnote>
<figureCaption confidence="0.8423955">
Figure 2: Alignments between the argument
structures representing the clauses blood coursing
</figureCaption>
<bodyText confidence="0.960359642857143">
through our veins again and Adrenalin was still
coursing through her veins; non-dotted lines illus-
trate the highest scoring alignment.
This can either be the case if pl does not cover all
roles annotated on the graph (i.e., there are role-
bearing nodes which we do not recognize as argu-
ments of the frame evoking verb) or if there are
unaligned role-bearing arguments (i.e., i V Mσ*
for a role-bearing argument i of pl).
The remaining projections form our expan-
sion corpus. For each seed instance we select
the k most similar neighbors to add to our training
data. The parameter k controls the trade-off be-
tween annotation confidence and expansion size.
</bodyText>
<sectionHeader confidence="0.999152" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999927047619048">
In this section we discuss our experimental setup
for assessing the usefulness of the method pre-
sented above. We give details on our training pro-
cedure and parameter estimation, describe the se-
mantic labeler we used in our experiments and ex-
plain how its output was evaluated.
Corpora Our seed corpus was taken from
FrameNet. The latter contains approximately
2,000 verb entries out of which we randomly se-
lected a sample of 100. We next extracted all an-
notated sentences for each of these verbs. These
sentences formed our gold standard corpus, 20%
of which was reserved as test data. We used
the remaining 80% as seeds for training purposes.
We generated seed corpora of various sizes by
randomly reducing the number of annotation in-
stances per verb to a maximum of n. An addi-
tional (non-overlapping) random sample of 100
verbs was used as development set for tuning the
parameters for our method. We gathered unla-
beled sentences from the BNC.
</bodyText>
<figure confidence="0.950985833333333">
Fluid _ _ _�� blood
SUBJ
Path _ _ _�� vein
IOBJ THROUGH
again
MOD
adrenalin
SUBJ
be
AUX
still
MOD
</figure>
<equation confidence="0.775336777777778">
�� �� ��
�� �� ��
� �� ��
1 1 1 1 1 1 1 1 1
�� �1� ��
1 1 1 1 1 1 1 1 1 1 1 1
1 �
vein
IOBJ THROUGH
</equation>
<page confidence="0.995851">
224
</page>
<bodyText confidence="0.999969882978724">
The seed and unlabeled corpora were parsed
with RASP (Briscoe et al., 2006). The FrameNet
annotations in the seed corpus were converted
into dependency graphs (see Figure 1) using the
method described in F¨urstenau (2008). Briefly,
the method works by matching nodes in the de-
pendency graph with role bearing substrings in
FrameNet. It first finds the node in the graph
which most closely matches the frame evoking
element in FrameNet. Next, individual graph
nodes are compared against labeled substrings in
FrameNet to transfer all roles onto their closest
matching graph nodes.
Parameter Estimation The similarity function
described in Section 3.2 has three free parameters.
These are the weight A which determines the rel-
ative importance of syntactic and semantic infor-
mation, the parameter B which determines when
two arguments cannot be aligned and the syntactic
score a for almost identical grammatical roles. We
optimized these parameters on the development
set using Powell’s direction set method (Brent,
1973) with F1 as our loss function. The optimal
values for A, B and a were 1.76, 0.41 and 0.67,
respectively.
Our similarity function is further parametrized
in using a semantic space model to compute the
similarity between two words. Considerable lat-
itude is allowed in specifying the parameters of
vector-based models. These involve the defi-
nition of the linguistic context over which co-
occurrences are collected, the number of com-
ponents used (e.g., the k most frequent words
in a corpus), and their values (e.g., as raw co-
occurrence frequencies or ratios of probabilities).
We created a vector-based model from a lem-
matized version of the BNC. Following previ-
ous work (Bullinaria and Levy, 2007), we opti-
mized the parameters of our model on a word-
based semantic similarity task. The task involves
examining the degree of linear relationship be-
tween the human judgments for two individual
words and vector-based similarity values. We ex-
perimented with a variety of dimensions (ranging
from 50 to 500,000), vector component definitions
(e.g., pointwise mutual information or log likeli-
hood ratio) and similarity measures (e.g., cosine or
confusion probability). We used WordSim353, a
benchmark dataset (Finkelstein et al., 2002), con-
sisting of relatedness judgments (on a scale of 0
to 10) for 353 word pairs.
We obtained best results with a model using a
context window of five words on either side of the
target word, the cosine measure, and 2,000 vec-
tor dimensions. The latter were the most com-
mon context words (excluding a stop list of func-
tion words). Their values were set to the ratio of
the probability of the context word given the tar-
get word to the probability of the context word
overall. This configuration gave high correlations
with the WordSim353 similarity judgments using
the cosine measure.
Solving the Linear Program A variety of algo-
rithms have been developed for solving the linear
assignment problem efficiently. In our study, we
used the simplex algorithm (Dantzig, 1963). We
generate and solve an LP of every unlabeled sen-
tence we wish to annotate.
Semantic role labeler We evaluated our method
on a semantic role labeling task. Specifically, we
compared the performance of a generic seman-
tic role labeler trained on the seed corpus and
a larger corpus expanded with annotations pro-
duced by our method. Our semantic role labeler
followed closely the implementation of Johans-
son and Nugues (2008). We extracted features
from dependency parses corresponding to those
routinely used in the semantic role labeling liter-
ature (see Baker et al. (2007) for an overview).
SVM classifiers were trained to identify the argu-
ments and label them with appropriate roles. For
the latter we performed multi-class classification
following the one-versus-one method3 (Friedman,
1996). For the experiments reported in this paper
we used the LIBLINEAR library (Fan et al., 2008).
The misclassification penalty C was set to 0.1.
To evaluate against the test set, we linearized
the resulting dependency graphs in order to obtain
labeled role bracketings like those in example (1)
and measured labeled precision, labeled recall and
labeled F1. (Since our focus is on role labeling and
not frame prediction, we let our role labeler make
use of gold standard frame annotations, i.e., label-
ing of frame evoking elements with frame names.)
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.998502333333333">
The evaluation of our method was motivated by
three questions: (1) How do different training set
sizes affect semantic role labeling performance?
</bodyText>
<footnote confidence="0.8819945">
3Given n classes the one-versus-one method builds
n(n − 1)/2 classifiers.
</footnote>
<page confidence="0.990565">
225
</page>
<table confidence="0.999945375">
TrainSet Size Prec (%) Rec (%) F1 (%)
0-NN 849 35.5 42.0 38.5
1-NN 1205 36.4 43.3 39.5
2-NN 1549 38.1 44.1 40.9*
3-NN 1883 37.9 43.7 40.6*
4-NN 2204 38.0 43.9 40.7*
5-NN 2514 37.4 43.9 40.4*
self train 1609 34.0 41.0 37.1
</table>
<tableCaption confidence="0.7020725">
Table 2: Semantic role labeling performance using
different amounts of training data; the seeds are
expanded with their k nearest neighbors; *: F1 is
significantly different from 0-NN (p &lt; 0.05).
</tableCaption>
<bodyText confidence="0.99989868627451">
Training size varies depending on the number of
unlabeled sentences added to the seed corpus. The
quality of these sentences also varies depending
on their similarity to the seed sentences. So,
we would like to assess whether there is a trade-
off between annotation quality and training size.
(2) How does the size of the seed corpus influence
role labeling performance? Here, we are interested
to find out what is the least amount of manual
annotation possible for our method to have some
positive impact. (3) And finally, what are the an-
notation savings our method brings?
Table 2 shows the performance of our semantic
role labeler when trained on corpora of different
sizes. The seed corpus was reduced to at most 10
instances per verb. Each row in the table corre-
sponds to adding the k nearest neighbors of these
instances to the training data. When trained solely
on the seed corpus the semantic role labeler yields
a (labeled) F1 of 38.5%, (labeled) recall is 42.0%
and (labeled) precision is 35.5% (see row 0-NN
in the table). All subsequent expansions yield
improved precision and recall. In all cases ex-
cept k = 1 the improvement is statistically signif-
icant (p &lt; 0.05). We performed significance test-
ing on F1 using stratified shuffling (Noreen, 1989),
an instance of assumption-free approximative ran-
domization testing. As can be seen, the optimal
trade-off between the size of the training corpus
and annotation quality is reached with two nearest
neighbors. This corresponds roughly to doubling
the number of training instances. (Due to the re-
strictions mentioned in Section 3.3 a 2-NN expan-
sion does not triple the number of instances.)
We also compared our results against a self-
training procedure (see last row in Table 2). Here,
we randomly selected unlabeled sentences corre-
sponding in number to a 2-NN expansion, labeled
them with our role labeler, added them to the train-
ing set, and retrained. Self-training resulted in per-
formance inferior to the baseline of adding no un-
labeled data at all (see the first row in Table 2).
Performance decreased even more with the addi-
tion of more self-labeled instances. These results
indicate that the similarity function is crucial to the
success of our method.
An example of the annotations our method pro-
duces is given below. Sentence (2a) is the seed.
Sentences (2b)–(2e) are its most similar neighbors.
The sentences are presented in decreasing order of
similarity.
</bodyText>
<listItem confidence="0.98560425">
(2) a. [He]Theme stared and came
[slowly]Manner [towards me]Goal.
b. [He]Theme had heard the shooting
and come [rapidly]Manner [back to-
wards the house]Goal.
c. Without answering, [she]Theme left
the room and came [slowly]Manner
[down the stairs]Goal.
d. [Then]Manner [he]Theme won’t come
[to Salisbury]Goal.
e. Does [he]Theme always come round
[in the morning]Goal [then]Manner?
</listItem>
<bodyText confidence="0.999988130434783">
As we can see, sentences (2b) and (2c) accu-
rately identify the semantic roles of the verb come
evoking the frame Arriving. In (2b) He is la-
beled as Theme, rapidly as Manner, and towards
the house as Goal. Analogously, in (2c) she is
the Theme, slowly is Manner and down the stairs
is Goal. The quality of the annotations decreases
with less similar instances. In (2d) then is marked
erroneously as Manner, whereas in (2e) only the
Theme role is identified correctly.
To answer our second question, we varied the
size of the training corpus by varying the num-
ber of seeds per verb. For these experiments we
fixed k = 2. Table 3 shows the performance of the
semantic role labeler when the seed corpus has one
annotation per verb, five annotations per verb, and
so on. (The results for 10 annotations are repeated
from Table 2). With 1, 5 or 10 instances per verb
our method significantly improves labeling perfor-
mance. We observe improvements in F1 of 1.5%,
2.1%, and 2.4% respectively when adding the 2
most similar neighbors to these training corpora.
Our method also improves F1 when a 20 seeds
</bodyText>
<page confidence="0.994926">
226
</page>
<table confidence="0.999959454545455">
TrainSet Size Prec (%) Rec (%) F1 (%)
&lt; 1 seed 95 24.9 31.3 27.7
+ 2-NN 170 26.4 32.6 29.2*
&lt; 5 seeds 450 29.7 38.4 33.5
+ 2-NN 844 31.8 40.4 35.6*
&lt; 10 seeds 849 35.5 42.0 38.5
+ 2-NN 1549 38.1 44.1 40.9*
&lt; 20 seeds 1414 38.7 46.1 42.1
+ 2-NN 2600 40.5 46.7 43.4
all seeds 2323 38.3 47.0 42.2
+ 2-NN 4387 39.5 46.7 42.8
</table>
<tableCaption confidence="0.998225">
Table 3: Semantic role labeling performance us-
</tableCaption>
<bodyText confidence="0.99911621875">
ing different numbers of seed instances per verb in
the training corpus; the seeds are expanded with
their k = 2 nearest neighbors; *: F1 is signifi-
cantly different from seed corpus (p &lt; 0.05).
corpus or all available seeds are used, however the
difference is not statistically significant.
The results in Table 3 also allow us to draw
some conclusions regarding the relative quality
of manual and automatic annotation. Expand-
ing a seed corpus with 10 instances per verb im-
proves F1 from 38.5% to 40.9%. We can com-
pare this to the labeler’s performance when trained
solely on the 20 seeds corpus (without any ex-
pansion). The latter has approximately the same
size as the expanded 10 seeds corpus. Interest-
ingly, F1 on this exclusively hand-annotated cor-
pus is only 1.2% better than on the expanded cor-
pus. So, using our expansion method on a 10 seeds
corpus performs almost as well as using twice as
many manual annotations. Even in the case of the
5 seeds corpus, where there is limited informa-
tion for our method to expand from, we achieve
an improvement from 33.5% to 35.6%, compared
to 38.5% for manual annotation of about the same
number of instances. In sum, while additional
manual annotation is naturally more effective for
improving the quality of the training data, we can
achieve substantial proportions of these improve-
ments by automatic expansion alone. This is a
promising result suggesting that it is possible to
reduce annotation costs without drastically sacri-
ficing quality.
</bodyText>
<sectionHeader confidence="0.999709" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999944465116279">
This paper presents a novel method for reducing
the annotation effort involved in creating resources
for semantic role labeling. Our strategy is to ex-
pand a manually annotated corpus by projecting
semantic role information from labeled onto un-
labeled instances. We formulate the projection
problem as an instance of the linear assignment
problem. We seek to find role assignments that
maximize the similarity between labeled and un-
labeled instances. Similarity is measured in terms
of structural and semantic compatibility between
argument structures.
Our method improves semantic role labeling
performance in several experimental conditions. It
is especially effective when a small number of an-
notations is available for each verb. This is typi-
cally the case when creating frame semantic cor-
pora for new languages or new domains. Our ex-
periments show that expanding such corpora with
our method can yield almost the same relative im-
provement as using exclusively manual annota-
tion.
In the future we plan to extend our method
in order to handle novel verbs that are not at-
tested in the seed corpus. Another direction con-
cerns the systematic modeling of diathesis alter-
nations (Levin, 1993). These are currently only
captured implicitly by our method (when the se-
mantic similarity overrides syntactic dissimilar-
ity). Ideally, we would like to be able to system-
atically identify changes in the realization of the
argument structure of a given predicate. Although
our study focused solely on FrameNet annotations,
we believe it can be adapted to related annotation
schemes, such as PropBank. An interesting ques-
tion is whether the improvements obtained by our
method carry over to other role labeling frame-
works.
Acknowledgments The authors acknowledge
the support of DFG (IRTG 715) and EPSRC
(grant GR/T04540/01). We are grateful to
Richard Johansson for his help with the re-
implementation of his semantic role labeler.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999136875">
Collin F. Baker, Michael Ellsworth, and Katrin Erk.
2007. SemEval-2007 Task 19: Frame Semantic
Structure Extraction. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 99–104, Prague, Czech Republic.
R. P. Brent. 1973. Algorithms for Minimization with-
out Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
</reference>
<page confidence="0.967797">
227
</page>
<reference confidence="0.999767108108108">
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The Second Release of the RASP System. In Pro-
ceedings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, pages 77–80, Sydney, Australia.
J. A. Bullinaria and J. P. Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510–526.
George B. Dantzig. 1963. Linear Programming and
Extensions. Princeton University Press, Princeton,
NJ, USA.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classification. Journal of Machine Learning
Research, 9:1871–1874.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R. L. Petruck. 2003. Background to
FrameNet. International Journal of Lexicography,
16:235–250.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Jerome H. Friedman. 1996. Another approach to poly-
chotomous classification. Technical report, Depart-
ment of Statistics, Stanford University.
Pascale Fung and Benfeng Chen. 2004. BiFrameNet:
Bilingual frame semantics resources construction
by cross-lingual induction. In Proceedings of the
20th International Conference on Computational
Linguistics, pages 931–935, Geneva, Switzerland.
Hagen F¨urstenau. 2008. Enriching frame semantic re-
sources with dependency graphs. In Proceedings of
the 6th Language Resources and Evaluation Confer-
ence, Marrakech, Morocco.
Daniel Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguis-
tics, 28:3:245–288.
Andrew Gordon and Reid Swanson. 2007. General-
izing semantic role annotations across syntactically
similar verbs. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 192–199, Prague, Czech Republic.
Richard Johansson and Pierre Nugues. 2006. A
FrameNet-based semantic role labeler for Swedish.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 436–443, Syd-
ney, Australia.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role la-
beling. In Proceedings of the 22nd International
Conference on Computational Linguistics, pages
393–400, Manchester, UK.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Alessandro Moschitti, Paul Morarescu, and Sanda
Harabagiu. 2003. Open-domain information extrac-
tion via automatic semantic labeling. In Proceed-
ings of FLAIRS 2003, pages 397–401, St. Augustine,
FL.
E. Noreen. 1989. Computer-intensive Methods for
Testing Hypotheses: An Introduction. John Wiley
and Sons Inc.
Sebastian Pad´o and Katrin Erk. 2005. To cause
or not to cause: Cross-lingual semantic matching
for paraphrase modelling. In Proceedings of the
EUROLAN Workshop on Cross-Linguistic Knowl-
edge Induction, pages 23–30, Cluj-Napoca, Roma-
nia.
Sebastian Pad´o and Mirella Lapata. 2006. Optimal
constituent alignment with edge covers for seman-
tic projection. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics, pages 1161–1168, Sydney,
Australia.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The Proposition Bank: An annotated corpus of se-
mantic roles. Computational Linguistics, 31(1):71–
106.
Marco Pennacchiotti, Diego De Cao, Roberto Basili,
Danilo Croce, and Michael Roth. 2008. Automatic
induction of FrameNet lexical units. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 457–465, Honolulu,
Hawaii.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceed-
ings of the joint Conference on Empirical Methods
in Natural Language Processing and Conference on
Computational Natural Language Learning, pages
12–21, Prague, Czech Republic.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 8–15, Sap-
poro, Japan.
Robert S. Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labelling. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 95–102. Bacelona,
Spain.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of the joint Human Language Technology
Conference and Conference on Empirical Methods
in Natural Language Processing, pages 371–378,
Vancouver, BC.
</reference>
<page confidence="0.997657">
228
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.799934">
<title confidence="0.999933">Semi-Supervised Semantic Role Labeling</title>
<author confidence="0.999681">Hagen F¨urstenau</author>
<affiliation confidence="0.999908">Dept. of Computational Linguistics Saarland University</affiliation>
<address confidence="0.976478">Saarbr¨ucken, Germany</address>
<email confidence="0.997838">hagenf@coli.uni-saarland.de</email>
<author confidence="0.995601">Mirella Lapata</author>
<affiliation confidence="0.999969">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.839184">Edinburgh, UK</address>
<email confidence="0.997459">mlap@inf.ed.ac.uk</email>
<abstract confidence="0.99938175">Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems. Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative. Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning. Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 19: Frame Semantic Structure Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>99--104</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3567" citStr="Baker et al., 2007" startWordPosition="535" endWordPosition="538"> eye]Body part. b. [A falling rock]Cause crushed [my ankle]Body part. c. [She]Agent slapped [him]Victim [hard]Degree [for his change of mood]Reason. d. [Rachel]Agent injured [her friend]Victim [by closing the car door on his left hand]Means. The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries. It also comes with a set of manually annotated example sentences, taken mostly from the British National Corpus. These annotations are often used 1The approaches are too numerous to list; we refer the interested reader to the proceedings of the SemEval-2007 shared task (Baker et al., 2007) for an overview of the stateof-the-art. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 220–228, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 220 as training data for semantic role labeling systems. However, the applicability of these systems is limited to those words for which labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available. Despite the substantial annotation effort involved in the creation of FrameNet (spanning approximately twelve years), the number of annotate</context>
<context position="24419" citStr="Baker et al. (2007)" startWordPosition="4015" endWordPosition="4018">e used the simplex algorithm (Dantzig, 1963). We generate and solve an LP of every unlabeled sentence we wish to annotate. Semantic role labeler We evaluated our method on a semantic role labeling task. Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996). For the experiments reported in this paper we used the LIBLINEAR library (Fan et al., 2008). The misclassification penalty C was set to 0.1. To evaluate against the test set, we linearized the resulting dependency graphs in order to obtain labeled role bracketings like those in example (1) and measured labeled precision, labeled recall and labeled F1. (Since our focus is on </context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin F. Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 99–104, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R P Brent</author>
</authors>
<title>Algorithms for Minimization without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="21851" citStr="Brent, 1973" startWordPosition="3596" endWordPosition="3597"> evoking element in FrameNet. Next, individual graph nodes are compared against labeled substrings in FrameNet to transfer all roles onto their closest matching graph nodes. Parameter Estimation The similarity function described in Section 3.2 has three free parameters. These are the weight A which determines the relative importance of syntactic and semantic information, the parameter B which determines when two arguments cannot be aligned and the syntactic score a for almost identical grammatical roles. We optimized these parameters on the development set using Powell’s direction set method (Brent, 1973) with F1 as our loss function. The optimal values for A, B and a were 1.76, 0.41 and 0.67, respectively. Our similarity function is further parametrized in using a semantic space model to compute the similarity between two words. Considerable latitude is allowed in specifying the parameters of vector-based models. These involve the definition of the linguistic context over which cooccurrences are collected, the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw cooccurrence frequencies or ratios of probabilities). We created a vector-based m</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>R. P. Brent. 1973. Algorithms for Minimization without Derivatives. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The Second Release of the RASP System.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<pages>77--80</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="13403" citStr="Briscoe et al., 2006" startWordPosition="2130" endWordPosition="2133"> Lemma GramRole SemRole blood SUBJ Fluid vein IOBJ THROUGH Path again MOD — function sim(σ) defined as: (A - syn(gli, guσ(i)) + sem(wli, wσ u(i)) − B) � iEM, Table 1: Predicate-argument structure for the verb course in Figure 1. consider nodes coordinated with the predicate as arguments. Finally, for each argument node we record the semantic roles it carries, if any. All surface word forms are lemmatized. An example of the argument structure information we obtain for the predicate course (see Figure 1) is shown in Table 1. We obtain information about grammatical roles from the output of RASP (Briscoe et al., 2006), a broad-coverage dependency parser. However, there is nothing inherent in our method that restricts us to this particular parser. Any other parser with broadly similar dependency output could serve our purposes. 3.2 Measuring Similarity For each frame evoking verb in the seed corpus our method creates a labeled predicate-argument representation. It also extracts all sentences from the unlabeled corpus containing the same verb. Not all of these sentences will be suitable instances for adding to our training data. For example, the same verb may evoke a different frame with different roles and </context>
<context position="20908" citStr="Briscoe et al., 2006" startWordPosition="3449" endWordPosition="3452">ining 80% as seeds for training purposes. We generated seed corpora of various sizes by randomly reducing the number of annotation instances per verb to a maximum of n. An additional (non-overlapping) random sample of 100 verbs was used as development set for tuning the parameters for our method. We gathered unlabeled sentences from the BNC. Fluid _ _ _�� blood SUBJ Path _ _ _�� vein IOBJ THROUGH again MOD adrenalin SUBJ be AUX still MOD �� �� �� �� �� �� � �� �� 1 1 1 1 1 1 1 1 1 �� �1� �� 1 1 1 1 1 1 1 1 1 1 1 1 1 � vein IOBJ THROUGH 224 The seed and unlabeled corpora were parsed with RASP (Briscoe et al., 2006). The FrameNet annotations in the seed corpus were converted into dependency graphs (see Figure 1) using the method described in F¨urstenau (2008). Briefly, the method works by matching nodes in the dependency graph with role bearing substrings in FrameNet. It first finds the node in the graph which most closely matches the frame evoking element in FrameNet. Next, individual graph nodes are compared against labeled substrings in FrameNet to transfer all roles onto their closest matching graph nodes. Parameter Estimation The similarity function described in Section 3.2 has three free parameters</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The Second Release of the RASP System. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77–80, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Bullinaria</author>
<author>J P Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="22545" citStr="Bullinaria and Levy, 2007" startWordPosition="3709" endWordPosition="3712">e 1.76, 0.41 and 0.67, respectively. Our similarity function is further parametrized in using a semantic space model to compute the similarity between two words. Considerable latitude is allowed in specifying the parameters of vector-based models. These involve the definition of the linguistic context over which cooccurrences are collected, the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw cooccurrence frequencies or ratios of probabilities). We created a vector-based model from a lemmatized version of the BNC. Following previous work (Bullinaria and Levy, 2007), we optimized the parameters of our model on a wordbased semantic similarity task. The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>J. A. Bullinaria and J. P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George B Dantzig</author>
</authors>
<title>Linear Programming and Extensions.</title>
<date>1963</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ, USA.</location>
<contexts>
<context position="15268" citStr="Dantzig, 1963" startWordPosition="2435" endWordPosition="2436">σ : Mσ —* {1,...,n} where Mσ C {1, . . . , m}. In other words, an argument i of pl is aligned to argument σ(i) of pu if i E Mσ. Note that this allows for unaligned arguments on both sides. We score each alignment σ using a similarity where syn(gli,guσ(i)) denotes the syntactic similarity between grammatical roles gli and guσ(i) and sem(wli, wuσ(i)) the semantic similarity between head words wli and wuσ(i). Our goal is to find an alignment such that the similarity function is maximized: σ* := arg max sim(σ). This optimization σ problem is a generalized version of the linear assignment problem (Dantzig, 1963). It can be straightforwardly expressed as a linear programming problem by associating each alignment σ with a set of binary indicator variables xij: �1 if i E Mσ ∧ σ(i) = j xij := 0 otherwise The similarity objective function then becomes: ( � A - syn(gl i, gu j ) + sem(wl i, wu j ) − B xij subject to the following constraints ensuring that σ is an injective function on some Mσ: n xij &lt; 1 for all i = 1,...,m j=1 �m xij &lt; 1 for all j = 1, ... , n i=1 Figure 2 graphically illustrates the alignment projection problem. Here, we wish to project semantic role information from the seed blood coursin</context>
<context position="23844" citStr="Dantzig, 1963" startWordPosition="3923" endWordPosition="3924">ds on either side of the target word, the cosine measure, and 2,000 vector dimensions. The latter were the most common context words (excluding a stop list of function words). Their values were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure. Solving the Linear Program A variety of algorithms have been developed for solving the linear assignment problem efficiently. In our study, we used the simplex algorithm (Dantzig, 1963). We generate and solve an LP of every unlabeled sentence we wish to annotate. Semantic role labeler We evaluated our method on a semantic role labeling task. Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM cl</context>
</contexts>
<marker>Dantzig, 1963</marker>
<rawString>George B. Dantzig. 1963. Linear Programming and Extensions. Princeton University Press, Princeton, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-E Fan</author>
<author>K-W Chang</author>
<author>C-J Hsieh</author>
<author>X-R Wang</author>
<author>C-J Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="24733" citStr="Fan et al., 2008" startWordPosition="4062" endWordPosition="4065">ger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996). For the experiments reported in this paper we used the LIBLINEAR library (Fan et al., 2008). The misclassification penalty C was set to 0.1. To evaluate against the test set, we linearized the resulting dependency graphs in order to obtain labeled role bracketings like those in example (1) and measured labeled precision, labeled recall and labeled F1. (Since our focus is on role labeling and not frame prediction, we let our role labeler make use of gold standard frame annotations, i.e., labeling of frame evoking elements with frame names.) 5 Results The evaluation of our method was motivated by three questions: (1) How do different training set sizes affect semantic role labeling pe</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Christopher R Johnson</author>
<author>Miriam R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<pages>16--235</pages>
<contexts>
<context position="1720" citStr="Fillmore et al., 2003" startWordPosition="244" endWordPosition="247">ur method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annotations provided in FrameNet are given in (1). Here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations. Semantic roles (or frame elements) are de</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>Charles J. Fillmore, Christopher R. Johnson, and Miriam R. L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16:235–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="23076" citStr="Finkelstein et al., 2002" startWordPosition="3789" endWordPosition="3792">odel from a lemmatized version of the BNC. Following previous work (Bullinaria and Levy, 2007), we optimized the parameters of our model on a wordbased semantic similarity task. The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values. We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability). We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs. We obtained best results with a model using a context window of five words on either side of the target word, the cosine measure, and 2,000 vector dimensions. The latter were the most common context words (excluding a stop list of function words). Their values were set to the ratio of the probability of the context word given the target word to the probability of the context word overall. This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure. Solving the Linea</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome H Friedman</author>
</authors>
<title>Another approach to polychotomous classification.</title>
<date>1996</date>
<tech>Technical report,</tech>
<institution>Department of Statistics, Stanford University.</institution>
<contexts>
<context position="24640" citStr="Friedman, 1996" startWordPosition="4048" endWordPosition="4049">red the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996). For the experiments reported in this paper we used the LIBLINEAR library (Fan et al., 2008). The misclassification penalty C was set to 0.1. To evaluate against the test set, we linearized the resulting dependency graphs in order to obtain labeled role bracketings like those in example (1) and measured labeled precision, labeled recall and labeled F1. (Since our focus is on role labeling and not frame prediction, we let our role labeler make use of gold standard frame annotations, i.e., labeling of frame evoking elements with frame names.) 5 Results The evaluation of our method was motivated</context>
</contexts>
<marker>Friedman, 1996</marker>
<rawString>Jerome H. Friedman. 1996. Another approach to polychotomous classification. Technical report, Department of Statistics, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Benfeng Chen</author>
</authors>
<title>BiFrameNet: Bilingual frame semantics resources construction by cross-lingual induction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>931--935</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7605" citStr="Fung and Chen (2004)" startWordPosition="1186" endWordPosition="1189">ave focused primarily on non-English languages. Annotation projection is a popular framework for transferring frame semantic annotations from one language to another by exploiting the translational and structural equivalences present in parallel corpora. The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language. Pad´o and Lapata (2006) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data. Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their</context>
</contexts>
<marker>Fung, Chen, 2004</marker>
<rawString>Pascale Fung and Benfeng Chen. 2004. BiFrameNet: Bilingual frame semantics resources construction by cross-lingual induction. In Proceedings of the 20th International Conference on Computational Linguistics, pages 931–935, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen F¨urstenau</author>
</authors>
<title>Enriching frame semantic resources with dependency graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Language Resources and Evaluation Conference,</booktitle>
<location>Marrakech, Morocco.</location>
<marker>F¨urstenau, 2008</marker>
<rawString>Hagen F¨urstenau. 2008. Enriching frame semantic resources with dependency graphs. In Proceedings of the 6th Language Resources and Evaluation Conference, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Dan Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="1362" citStr="Gildea and Jurafsky, 2002" startWordPosition="190" endWordPosition="193">lly via annotation projection. We formulate the projection task as a generalization of the linear assignment problem. We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning p</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Dan Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:3:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gordon</author>
<author>Reid Swanson</author>
</authors>
<title>Generalizing semantic role annotations across syntactically similar verbs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>192--199</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9162" citStr="Gordon and Swanson (2007)" startWordPosition="1430" endWordPosition="1433">il221 ity model on which future assignments are based. Being unsupervised, their approach requires no manual effort other than creating the frame dictionary. Unfortunately, existing resources do not have exhaustive coverage and a large number of verbs may be assigned no semantic role information since they are not in the dictionary in the first place. Pennacchiotti et al. (2008) address precisely this problem by augmenting FrameNet with new lexical units if they are similar to an existing frame (their notion of similarity combines distributional and WordNet-based measures). In a similar vein, Gordon and Swanson (2007) attempt to increase the coverage of PropBank. Their approach leverages existing annotations to handle novel verbs. Rather than annotating new sentences that contain novel verbs, they find syntactically similar verbs and use their annotations as surrogate training data. Our own work aims to reduce but not entirely eliminate the annotation effort involved in creating training data for semantic role labeling. We thus assume that a small number of manual annotations is initially available. Our algorithm augments these with unlabeled examples whose roles are inferred automatically. We apply our me</context>
</contexts>
<marker>Gordon, Swanson, 2007</marker>
<rawString>Andrew Gordon and Reid Swanson. 2007. Generalizing semantic role annotations across syntactically similar verbs. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 192–199, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>A FrameNet-based semantic role labeler for Swedish.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>436--443</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="7520" citStr="Johansson and Nugues (2006)" startWordPosition="1172" endWordPosition="1175">efore not surprising that previous efforts to reduce the need for semantic role annotation have focused primarily on non-English languages. Annotation projection is a popular framework for transferring frame semantic annotations from one language to another by exploiting the translational and structural equivalences present in parallel corpora. The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language. Pad´o and Lapata (2006) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data. Swier and Stevenson</context>
</contexts>
<marker>Johansson, Nugues, 2006</marker>
<rawString>Richard Johansson and Pierre Nugues. 2006. A FrameNet-based semantic role labeler for Swedish. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 436–443, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>The effect of syntactic representation on semantic role labeling.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>393--400</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="24269" citStr="Johansson and Nugues (2008)" startWordPosition="3991" endWordPosition="3995"> cosine measure. Solving the Linear Program A variety of algorithms have been developed for solving the linear assignment problem efficiently. In our study, we used the simplex algorithm (Dantzig, 1963). We generate and solve an LP of every unlabeled sentence we wish to annotate. Semantic role labeler We evaluated our method on a semantic role labeling task. Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method. Our semantic role labeler followed closely the implementation of Johansson and Nugues (2008). We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview). SVM classifiers were trained to identify the arguments and label them with appropriate roles. For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996). For the experiments reported in this paper we used the LIBLINEAR library (Fan et al., 2008). The misclassification penalty C was set to 0.1. To evaluate against the test set, we linearized the resulting dependency graphs in ord</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. The effect of syntactic representation on semantic role labeling. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 393–400, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="32809" citStr="Levin, 1993" startWordPosition="5426" endWordPosition="5427">tic role labeling performance in several experimental conditions. It is especially effective when a small number of annotations is available for each verb. This is typically the case when creating frame semantic corpora for new languages or new domains. Our experiments show that expanding such corpora with our method can yield almost the same relative improvement as using exclusively manual annotation. In the future we plan to extend our method in order to handle novel verbs that are not attested in the seed corpus. Another direction concerns the systematic modeling of diathesis alternations (Levin, 1993). These are currently only captured implicitly by our method (when the semantic similarity overrides syntactic dissimilarity). Ideally, we would like to be able to systematically identify changes in the realization of the argument structure of a given predicate. Although our study focused solely on FrameNet annotations, we believe it can be adapted to related annotation schemes, such as PropBank. An interesting question is whether the improvements obtained by our method carry over to other role labeling frameworks. Acknowledgments The authors acknowledge the support of DFG (IRTG 715) and EPSRC</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Paul Morarescu</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Open-domain information extraction via automatic semantic labeling.</title>
<date>2003</date>
<booktitle>In Proceedings of FLAIRS 2003,</booktitle>
<pages>397--401</pages>
<location>St. Augustine, FL.</location>
<contexts>
<context position="1500" citStr="Moschitti et al., 2003" startWordPosition="212" endWordPosition="216">assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annota</context>
</contexts>
<marker>Moschitti, Morarescu, Harabagiu, 2003</marker>
<rawString>Alessandro Moschitti, Paul Morarescu, and Sanda Harabagiu. 2003. Open-domain information extraction via automatic semantic labeling. In Proceedings of FLAIRS 2003, pages 397–401, St. Augustine, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Noreen</author>
</authors>
<title>Computer-intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley and Sons Inc.</publisher>
<contexts>
<context position="27100" citStr="Noreen, 1989" startWordPosition="4464" endWordPosition="4465">n corpora of different sizes. The seed corpus was reduced to at most 10 instances per verb. Each row in the table corresponds to adding the k nearest neighbors of these instances to the training data. When trained solely on the seed corpus the semantic role labeler yields a (labeled) F1 of 38.5%, (labeled) recall is 42.0% and (labeled) precision is 35.5% (see row 0-NN in the table). All subsequent expansions yield improved precision and recall. In all cases except k = 1 the improvement is statistically significant (p &lt; 0.05). We performed significance testing on F1 using stratified shuffling (Noreen, 1989), an instance of assumption-free approximative randomization testing. As can be seen, the optimal trade-off between the size of the training corpus and annotation quality is reached with two nearest neighbors. This corresponds roughly to doubling the number of training instances. (Due to the restrictions mentioned in Section 3.3 a 2-NN expansion does not triple the number of instances.) We also compared our results against a selftraining procedure (see last row in Table 2). Here, we randomly selected unlabeled sentences corresponding in number to a 2-NN expansion, labeled them with our role la</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. Noreen. 1989. Computer-intensive Methods for Testing Hypotheses: An Introduction. John Wiley and Sons Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Katrin Erk</author>
</authors>
<title>To cause or not to cause: Cross-lingual semantic matching for paraphrase modelling.</title>
<date>2005</date>
<booktitle>In Proceedings of the EUROLAN Workshop on Cross-Linguistic Knowledge Induction,</booktitle>
<pages>23--30</pages>
<location>Cluj-Napoca, Romania.</location>
<marker>Pad´o, Erk, 2005</marker>
<rawString>Sebastian Pad´o and Katrin Erk. 2005. To cause or not to cause: Cross-lingual semantic matching for paraphrase modelling. In Proceedings of the EUROLAN Workshop on Cross-Linguistic Knowledge Induction, pages 23–30, Cluj-Napoca, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Optimal constituent alignment with edge covers for semantic projection.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1161--1168</pages>
<location>Sydney, Australia.</location>
<marker>Pad´o, Lapata, 2006</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2006. Optimal constituent alignment with edge covers for semantic projection. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1161–1168, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>106</pages>
<contexts>
<context position="1755" citStr="Palmer et al., 2005" startWordPosition="251" endWordPosition="254">ing hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annotations provided in FrameNet are given in (1). Here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations. Semantic roles (or frame elements) are defined for each frame and correspond</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71– 106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Diego De Cao</author>
<author>Roberto Basili</author>
<author>Danilo Croce</author>
<author>Michael Roth</author>
</authors>
<title>Automatic induction of FrameNet lexical units.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>457--465</pages>
<location>Honolulu, Hawaii.</location>
<marker>Pennacchiotti, De Cao, Basili, Croce, Roth, 2008</marker>
<rawString>Marco Pennacchiotti, Diego De Cao, Roberto Basili, Danilo Croce, and Michael Roth. 2008. Automatic induction of FrameNet lexical units. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 457–465, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1546" citStr="Shen and Lapata, 2007" startWordPosition="220" endWordPosition="223">argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annotations provided in FrameNet are given in (1). H</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of the joint Conference on Empirical Methods in Natural Language Processing and Conference on Computational Natural Language Learning, pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>8--15</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1475" citStr="Surdeanu et al., 2003" startWordPosition="208" endWordPosition="211">We seek to find a role assignment in the unlabeled data such that the argument similarity between the labeled and unlabeled instances is maximized. Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training dat</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 8–15, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Swier</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Unsupervised semantic role labelling.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>95--102</pages>
<location>Bacelona,</location>
<contexts>
<context position="8127" citStr="Swier and Stevenson (2004)" startWordPosition="1267" endWordPosition="1270">n and Nugues (2006) from English onto Swedish. A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus. Then, Chinese sentences with predicates instantiating these concepts are found in a monolingual corpus and their arguments are labeled with FrameNet roles. Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data. Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their semantic roles. Given a verb instance, their method first selects a frame from VerbNet, a semantic role resource akin to FrameNet and PropBank, and labels each argument slot with sets of possible roles. The algorithm proceeds iteratively by first making initial unambiguous role assignments, and then successively updating a probabil221 ity model on which future assignments are based. Being unsupervised, their approach requires no manual effort other than creating the frame dictionary. Unfortunately, existing resource</context>
</contexts>
<marker>Swier, Stevenson, 2004</marker>
<rawString>Robert S. Swier and Suzanne Stevenson. 2004. Unsupervised semantic role labelling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 95–102. Bacelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Dan Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of the joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>371--378</pages>
<location>Vancouver, BC.</location>
<contexts>
<context position="1671" citStr="Tatu and Moldovan, 2005" startWordPosition="237" endWordPosition="240">g show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone. 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Moschitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005). Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training. Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data. Examples of the annotations provided in FrameNet are given in (1). Here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situ</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>Marta Tatu and Dan Moldovan. 2005. A semantic approach to recognizing textual entailment. In Proceedings of the joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 371–378, Vancouver, BC.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>