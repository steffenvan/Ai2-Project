<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010086">
<title confidence="0.9910835">
Unsupervised Construction of a Multilingual WordNet
from Parallel Corpora
</title>
<author confidence="0.964943">
Dimitar Kazakov and Ahmad R. Shahid
</author>
<affiliation confidence="0.998006">
Department of Computer Science
University of York
</affiliation>
<address confidence="0.896846">
Heslington, York YO10 5DD, UK
</address>
<email confidence="0.989917">
kazakov |ahmad@cs.york.ac.uk
</email>
<sectionHeader confidence="0.995327" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997707">
This paper outlines an approach to the unsu-
pervised construction from unannotated paral-
lel corpora of a lexical semantic resource akin to
WordNet. The paper also describes how this re-
source can be used to add lexical semantic tags
to the text corpus at hand. Finally, we discuss
the possibility to add some of the predicates typi-
cal for WordNet to its automatically constructed
multilingual version, and the ways in which the
success of this approach can be measured.
</bodyText>
<sectionHeader confidence="0.99263" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.90924">
Parallel corpora, WordNet, unsupervised learning
</keyword>
<sectionHeader confidence="0.99802" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960188679245">
Lexical ambiguity is inherent and widespread in all
languages; it emerges spontaneously in computer sim-
ulations of language evolution [23], and its origin prob-
ably stems in the interplay between geographic divi-
sions and interaction between communities, diachronic
linguistic changes, and evolutionary pressures on the
cost of communication. Two challenges arise when
dealing with lexical ambiguity: firstly, to define the el-
ementary semantic concepts employed in a given lan-
guage, and, secondly, given one or more utterances, to
identify the semantic concepts to which the words in
those utterances refer. Throughout history, numerous
attempts have been made to address both challenges
through the construction of artificial languages with
unambiguous semantics (e.g., see Ecos detailed and
entertaining review [3]). Arguably, there are two pos-
sible ways of defining a semantic concept, either by
trying to map it onto some sensory experience (lead-
ing to a philosophical discussion about the extent to
which they are shared and the notion of qualia), or by
describing it through other concepts, in a way that is
mutually recursive and unbounded (cf. Peirces Sign
Theory and the notion of infinite semiosis).
The last twenty five years saw academic and com-
mercial efforts directed towards the creation of large
repositories combining the description of semantic
concepts, their relationship, and, possibly, common
knowledge statements expressed in those terms. This
includes, among others, the Cycorp Cyc project [11]
and the lexical semantic database WordNet [14]. Both
approaches use a number of predicates to make state-
ments, such as “concept A is an instance of concept
B” or “concept A is a specific case of concept B” (in
other terms, all instances of concept A form a subset of
the instances of concept B). WordNet also employs the
notion of synsets, defining a semantic concept through
the list of words (synonyms) sharing that meaning,
e.g., {mercury, quicksilver, Hg}. The original version
of WordNet developed in Princeton covered the En-
glish language, but this effort has been replicated for
other languages [25]. All these databases are monolin-
gual; they are also handcrafted, and while very com-
prehensive in many aspects, it is difficult to assume
they could be kept abreast of the new developments,
including the use of newly coined words, and giving
new meanings to the old ones.
The dawn and rapid growth of dynamic online en-
cyclopedic resources with shared authorship, such as
Wikipedia, in the last decade, have begun to draw at-
tention as a potential source of semantic concepts and
ontologies [7]. Recently, there have also been efforts to
use the likes of Wikipedia to the existing hand-crafted
resources [13].
</bodyText>
<sectionHeader confidence="0.974288" genericHeader="method">
2 Multilingual Synsets
</sectionHeader>
<bodyText confidence="0.999949181818182">
The synsets in WordNet clarify a concept (or, from an-
other point of view, narrow down the sense of a word)
by paraphrasing it repeatedly, using other words of
the same language. This approach is based on the
fact that words rarely share all their meanings: {step,
pace} specifies a different meaning from {step, stair}.
The same result, however, can be achieved through the
use of words of different languages that share at least
one sense and therefore can be seen as translations of
each other. So, {EN: bank, FR: banque} could refer
to a financial institution or a collection of a particular
kind (e.g., a blood bank), as these words share both
meanings, but eliminates the concept of ‘river bank’
that the English word alone might denote. Increasing
the number of languages could gradually remove all
ambiguity, as in the case of {EN: bank, FR: banque,
NL: bank}. Insofar these lists of words specify a sin-
gle semantic concept, they can be seen as synsets of a
WordNet that makes use of words of several languages,
rather than just one. The greater the number of trans-
lations in this multilingual WordNet, the clearer the
meaning, yet, one might object, the fewer the number
</bodyText>
<page confidence="0.904203">
9
</page>
<bodyText confidence="0.986804372093023">
NLP Methods and Corpora in Translation, Lexicography, and Language Learning 2009 – Borovets, Bulgaria, pages 9–12
of such polyglots, who could benefit from such trans-
lations. However, these multilingual synsets can also
be useful in a monolingual context, as unique indices
that distinguish the individual meanings of a word.
For instance, if the English word bank is translated
variously as {EN: bank, FR: banque}, and {EN: bank,
FR: rive} one does not need to understand French to
suspect that ‘bank’ may have (at least) two different
meanings. The greater the number of languages in
which the two synsets differ, the stronger the intuition
that they indicate different meanings of the word in
the pivotal language.
Synsets, whether monolingual or multilingual, can
be used to tag the lexical items in a corpus with their
intended meaning (see Table 1). The benefits of such
lexical semantic tags are evident. Focussing now on
the novel case of multilingual synsets, one can consider
the two separate questions of how to produce a collec-
tion of such synsets, and how to annotate the lexical
items in a text with them. Kazakov and Shahid [22]
present an interesting study in that direction, where
the titles of ‘equivalent’ Wikipedia pages are collected
for a number of preselected languages on the assump-
tion that they represent an accurate translation of each
other (see Fig.1).
The authors repeat the same exercise for a num-
ber of specific domains, and also with the names of
Wikipedia categories. The latter case demonstrates
the potential to use Wikipedia not only to collect mul-
tilingual synsets, but also to add a hierarchical rela-
tionship between them, as this information is explicitly
present there. A number of other researchers have in
fact used Wikipedia to extract ontologies [6], [7], but
in all cases this was done for a single language.
Semantically disambiguated corpora, including
those using WordNet synsets as semantic tags, are
valuable assets [10], [9], but creating them requires
a considerable effort. Here we propose an alterna-
tive approach, where a text is automatically annotated
with lexical semantic tags in the form of multilingual
synsets, provided the text forms part of a multilingual,
parallel corpus.
</bodyText>
<tableCaption confidence="0.984248333333333">
Table 1: Examples of lexical semantic annotation
using standard English WordNet synsets (above) and
multilingual synsets (below).
</tableCaption>
<bodyText confidence="0.920096636363636">
A darkened outlook for Germany’s banks:SS1
The banks:SS2 of the river Nile
SS1={bank, depository financial institution}
Gloss=“a financial institution that accepts deposits
and lends money”
SS2={bank}
Gloss=“sloping land”
A darkened outlook for Germany’s banks:mSS1
The banks:mSS2 of the river Nile
mSS1={EN:bank, FR:banque, CZ:banka}
mSS2={EN:bank, FR:rive, CZ:bfeh}
</bodyText>
<tableCaption confidence="0.8251405">
Table 2: Examples of variation between synsets due to
the use of different word forms (above) and synonyms
</tableCaption>
<equation confidence="0.389442">
(below).
EN FR CZ ...
</equation>
<figureCaption confidence="0.68322975">
work travail price ...
works travaux price ...
work travail price ...
work bouleau price ...
</figureCaption>
<sectionHeader confidence="0.9546855" genericHeader="method">
3 Annotating Parallel Corpora
with Lexical Semantics
</sectionHeader>
<bodyText confidence="0.999975326530612">
In our approach the multilingual synsets become the
sense tags and the parallel corpora are tagged with the
corresponding tags (see Fig.2).
The idea is as simple as it is elegant: assuming we
have a word-aligned parallel corpus with n languages,
annotate each word with a lexical semantic tag con-
sisting of the n-tuple of aligned words. As a result, all
occurrences of a given word in the text for language G
are considered as having the same sense, provided they
correspond to (are tagged with) the same multilingual
synset.
Two great advantages of this scheme are that it
is completely unsupervised, and the fact that, unlike
manually tagged corpora using WordNet, all words in
the corpus are guaranteed to have a corresponding
multilingual synset. Since we are only interested in
words with their own semantics, a stop list can be used
to remove the words of the closed lexicon before the
rest are semantically tagged. Also the need for word
alignment should not be an issue, at least in princi-
ple, as there are standard tools, such as GIZA++ [16]
serving that purpose.
The approach as described so far needs to deal with
two main issues, both related to the fact that the sim-
ple listing of n-tuples as synsets may produce many
more synsets than the real number of concepts to
which the words in the text refer. The first issue stems
from the fact that a lexeme (word entry) corresponds
to several word forms in most languages, so, for in-
stance, the word forms {EN: work} and {EN: works}
will correspond to two different synsets (Table 2, top),
even if they are used with the same meaning. The
second of the above mentioned issues is related to the
use of synonyms in one language, whereas the transla-
tion in another makes use of the same word (lexeme)
(Table 2, bottom).
From this point of view, we could consider the orig-
inal n-tuples as proto-synsets, and then strife to rec-
ognize the variation due to the use of different word
forms and synonyms, and eliminate it, if possible, by
grouping these proto-synsets into genuine synsets cor-
responding to different concepts. As much of the ap-
peal of the whole approach stems from its unsuper-
vised nature, we shall also consider unsupervised ways
of solving this specific problem. For several languages,
there are detailed, explicit models of their word mor-
phology [19], [20], [17], which makes mapping word
forms onto the list of lexemes they may represent a
straightforward task.
</bodyText>
<page confidence="0.995155">
10
</page>
<figureCaption confidence="0.99854">
Fig. 1: Wikipedia page titles seen as multilingual synsets.
</figureCaption>
<bodyText confidence="0.999952620689655">
Using morpho-lexical analyzers for the languages in
the corpus will produce a list of lexical entries for each
language, which can be narrowed down through the
use of conventional lexicons listing the possible pairs
of lexical entries between given two languages. In this
way, the word form ‘works’ will be mapped onto the
lexemes work, n., works, n., and work, v., but in the
context of the French travail, only the first lexeme will
be retained, as the other two would not form a trans-
lation pair in an English-French lexicon.
One could also consider doing away with any mod-
els of morphology, assuming complete ignorance in
this respect, and employing unsupervised learning of
word morphology [8], [4]. In their latest form, these
approaches can identify word form paradigms, which
would allow all forms of a lexical entry to be mapped
consistently onto it.
It is also possible to automate the process of identi-
fying synonyms among the words of a given language.
For instance, Crouch’s approach [2] is based on the
discrimination value model [21]. Other approaches in-
clude Bayesian Networks [18], Hierarchical Clustering
[24], and link co-occurrences [15], etc. Such automated
approaches have certain advantages over the manually
generated thesauri, both in terms of cost and time of
development, and also in the level of detail, with the
latter often being too fine grained, and reflecting us-
ages that are not common and irrelevant in practice
[12].
</bodyText>
<sectionHeader confidence="0.63234" genericHeader="method">
4 Extracting a Fully-Fledged
Multilingual WordNet
</sectionHeader>
<bodyText confidence="0.999986714285714">
So far, we have described a procedure that extracts
multilingual synsets from a parallel corpus and reduces
spurious ambiguity by merging synsets corresponding
to multiple word forms of the same lexeme, resp. by
combining those only varying in the use of synonyms
of a given language. Extraction of hierarchical seman-
tic relationships between words in a corpus has been
studied for almost two decades [5], and the same pro-
cedures can be applied here, leading to the acquisition
of a lexical resource akin to WordNet, which also con-
tains some of the predicates (e.g., hyponym/2, resp.
hypernym/2). Such semantic hierarchies can then be
used to tag the text corpus with synsets of a certain
level of granularity, depending on the task at hand.
</bodyText>
<sectionHeader confidence="0.987376" genericHeader="conclusions">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999957466666667">
The evaluation of this approach could be twofold: di-
rectly, using an already semantically annotated gold
standard, and indirectly, through the measured ben-
efits of lexical semantic tagging in other tasks. The
limitations of the direct approach are given by the
lack of semantically annotated parallel corpora, how-
ever, there is at least one such corpus at the time of
writing, namely, multi-Semcor [1]. Indirectly, the po-
tential benefits of tagging text with such multilingual
synsets can be measured in tasks, such as document
clustering, where the lexical semantic tags can be used
to discriminate between various word senses. Any im-
provement in the within-clusters and between-clusters
quality measures would indicate relative (and measur-
able) success.
</bodyText>
<sectionHeader confidence="0.998141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999536266666667">
[1] L. Bentivogli, E. Pianta, and M. Ranieri. Multisemcor: an En-
glish Italian aligned corpus with a shared inventory of senses.
In Proceedings of the Meaning Workshop, page 90, Trento,
Italy, February 2005.
[2] C. J. Crouch. A Cluster-Based Approach to Thesaurus Con-
struction. In Proceedings of ACM SIGIR-88, page 309320,
1988.
[3] U. Eco. La recherche de la langue parfaite dans la culture
europeenne. Seuil, Paris, 1994.
[4] J. Goldsmith. Unsupervised acquisition of the morhology of a
natural language. Computational Linguistics, 27(2):153–198,
2001.
[5] M. A. Hearst. Automatic acquisition of hyponyms from large
text corpora. In Proceedings of the 14th International Con-
ference on Computational Linguistics (COLING), 1992.
</reference>
<page confidence="0.99881">
11
</page>
<figureCaption confidence="0.852469">
Fig. 2: Assignment of sense tags in aligned documents.
</figureCaption>
<reference confidence="0.999956208955224">
[6] M. Hepp, D. Bachlechner, and K. Siorpaes. Harvesting wiki
consensus – using wikipedia entries as ontology. In Proc.
ESWC-06 Workshop on Semantic Wikis, pages 132–46, 2006.
[7] A. Herbelot and A. Copestake. Acquiring ontological relation-
ships from wikipedia using RMRS. In Proc. ISWC-06 Work-
shop on Web Content Mining with Human Language, 2006.
[8] D. Kazakov. Unsupervised learning of nave morphology with
genetic algorithms. In W. van den Bosch and A. Weijters, edi-
tors, in Workshop Notes of the ECML/MLnet Workshop on
Empirical Learning of Natural Language Processing Tasks,
pages 105–112, Prague, Czech Republic, April 1997.
[9] D. Kazakov. Combining LAPIS and WordNet for the learning
of LR parsers with optimal semantic constraints. In S. Dze-
roski and P. Flach, editors, The Ninth International Work-
shop ILP-99, volume 1634 of LNAI, Bled, Slovenia, 1999.
Springer-Verlag.
[10] S. Landes, C. Leacock, and R. Tengi. WordNet: An Elec-
tronic Lexical Database, chapter Building semantic concor-
dances. MIT Press, Cambridge, Mass., 1998.
[11] D. Lenat. Cyc: A large-scale investment in knowledge infras-
tructure. Communications of the ACM, 38(11), 1995.
[12] D. Lin. Automatic Rretrieval and Clustering of Similar
Words. In Proceedings of the 17th international conference
on Computational linguistics, pages 768–774, Montreal, Que-
bec, Canada, August 1998.
[13] O. Medelyan and C. Legg. Integrating Cyc and Wikipedia:
Folksonomy meets rigorously defined common-sense. In Pro-
ceedings of the WikiAI Workshop at AAAI-2008, Chicago,
2008.
[14] G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. Five papers on WordNet. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. MIT Press, May
1998.
[15] K. Nakayama, T. Hara, and S. Nishio. Wikipedia mining for an
association web thesaurus construction. In In Proceedings of
IEEE International Conference on Web Information Systems
Engineering, pages 322–334, 2007.
[16] F. J. Och and H. Ney. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19–51, 2003.
[17] K. Oflazer. Two-level description of turkish morphology. In
EACL, 1993.
[18] Y. C. Park and K.-S. Choi. Automatic thesaurus construction
using Bayesian networks. Information Processing and Man-
agement: an International Journal, 32(5):543–553, Septem-
ber 1996.
[19] E. Paskaleva. A formal procedure for Bulgarian word form
generation. In COLING, pages 217–221, 1982.
[20] G. D. Ritchie, G. J. Russell, A. W. Black, and S. G. Pul-
man. Computational Morphology: Practical Mechanism for
the English Lexicon. MIT Press, 1991.
[21] G. Salton, C. Yang, and C. Yu. A Theory of Term Importance
in Automatic Text Analysis. Journal of the American Society
for Information Science, 26(1):33–44, 1975.
[22] A. R. Shahid and D. Kazakov. Automatic Multilingual Lexicon
Generation using Wikipedia as a Resource. In Proc. of the
Intl. Conf. on Agents and Artificial Intelligence (ICAART),
Porto, Portugal, January 2009.
[23] L. Steels. Emergent adaptive lexicons. In P. Maes, M. Mataric,
J.-A. Meyer, J. Pollack, and S. W. Wilson, editors, Fourth In-
ternational Conference on Simulation of Adaptive Behavior.
The MIT Press/Bradford Books, 1996.
[24] T. Tokunaga, M. Iwayama, and H. Tanaka. Automatic the-
saurus construction based-on grammatical relations. In In Pro-
ceedings of the 14th International Joint Conference on Arti-
ficial Intelligence, pages 1308–1313, 1995.
[25] P. Vossen, editor. EuroWordNet. Kluwer, 1998.
</reference>
<page confidence="0.99846">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425278">
<title confidence="0.9758935">Unsupervised Construction of a Multilingual from Parallel Corpora</title>
<author confidence="0.98189">Dimitar Kazakov</author>
<author confidence="0.98189">R Ahmad</author>
<affiliation confidence="0.998097">Department of Computer University of</affiliation>
<address confidence="0.733569">Heslington, York YO10 5DD,</address>
<abstract confidence="0.999908909090909">This paper outlines an approach to the unsupervised construction from unannotated parallel corpora of a lexical semantic resource akin to WordNet. The paper also describes how this resource can be used to add lexical semantic tags to the text corpus at hand. Finally, we discuss the possibility to add some of the predicates typical for WordNet to its automatically constructed multilingual version, and the ways in which the success of this approach can be measured.</abstract>
<keyword confidence="0.996262">Keywords</keyword>
<intro confidence="0.614663">Parallel corpora, WordNet, unsupervised learning</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>E Pianta</author>
<author>M Ranieri</author>
</authors>
<title>Multisemcor: an English Italian aligned corpus with a shared inventory of senses.</title>
<date>2005</date>
<booktitle>In Proceedings of the Meaning Workshop, page 90,</booktitle>
<location>Trento, Italy,</location>
<marker>[1]</marker>
<rawString>L. Bentivogli, E. Pianta, and M. Ranieri. Multisemcor: an English Italian aligned corpus with a shared inventory of senses. In Proceedings of the Meaning Workshop, page 90, Trento, Italy, February 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Crouch</author>
</authors>
<title>A Cluster-Based Approach to Thesaurus Construction.</title>
<date>1988</date>
<booktitle>In Proceedings of ACM SIGIR-88,</booktitle>
<pages>309320</pages>
<contexts>
<context position="11209" citStr="[2]" startWordPosition="1827" endWordPosition="1827">h travail, only the first lexeme will be retained, as the other two would not form a translation pair in an English-French lexicon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus a</context>
</contexts>
<marker>[2]</marker>
<rawString>C. J. Crouch. A Cluster-Based Approach to Thesaurus Construction. In Proceedings of ACM SIGIR-88, page 309320, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Eco</author>
</authors>
<title>La recherche de la langue parfaite dans la culture europeenne. Seuil,</title>
<date>1994</date>
<location>Paris,</location>
<contexts>
<context position="1583" citStr="[3]" startWordPosition="234" endWordPosition="234">ions and interaction between communities, diachronic linguistic changes, and evolutionary pressures on the cost of communication. Two challenges arise when dealing with lexical ambiguity: firstly, to define the elementary semantic concepts employed in a given language, and, secondly, given one or more utterances, to identify the semantic concepts to which the words in those utterances refer. Throughout history, numerous attempts have been made to address both challenges through the construction of artificial languages with unambiguous semantics (e.g., see Ecos detailed and entertaining review [3]). Arguably, there are two possible ways of defining a semantic concept, either by trying to map it onto some sensory experience (leading to a philosophical discussion about the extent to which they are shared and the notion of qualia), or by describing it through other concepts, in a way that is mutually recursive and unbounded (cf. Peirces Sign Theory and the notion of infinite semiosis). The last twenty five years saw academic and commercial efforts directed towards the creation of large repositories combining the description of semantic concepts, their relationship, and, possibly, common k</context>
</contexts>
<marker>[3]</marker>
<rawString>U. Eco. La recherche de la langue parfaite dans la culture europeenne. Seuil, Paris, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised acquisition of the morhology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="10912" citStr="[4]" startWordPosition="1777" endWordPosition="1777">s for each language, which can be narrowed down through the use of conventional lexicons listing the possible pairs of lexical entries between given two languages. In this way, the word form ‘works’ will be mapped onto the lexemes work, n., works, n., and work, v., but in the context of the French travail, only the first lexeme will be retained, as the other two would not form a translation pair in an English-French lexicon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and </context>
</contexts>
<marker>[4]</marker>
<rawString>J. Goldsmith. Unsupervised acquisition of the morhology of a natural language. Computational Linguistics, 27(2):153–198, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING),</booktitle>
<contexts>
<context position="12116" citStr="[5]" startWordPosition="1969" endWordPosition="1969">lso in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets corresponding to multiple word forms of the same lexeme, resp. by combining those only varying in the use of synonyms of a given language. Extraction of hierarchical semantic relationships between words in a corpus has been studied for almost two decades [5], and the same procedures can be applied here, leading to the acquisition of a lexical resource akin to WordNet, which also contains some of the predicates (e.g., hyponym/2, resp. hypernym/2). Such semantic hierarchies can then be used to tag the text corpus with synsets of a certain level of granularity, depending on the task at hand. 5 Evaluation The evaluation of this approach could be twofold: directly, using an already semantically annotated gold standard, and indirectly, through the measured benefits of lexical semantic tagging in other tasks. The limitations of the direct approach are g</context>
</contexts>
<marker>[5]</marker>
<rawString>M. A. Hearst. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING), 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hepp</author>
<author>D Bachlechner</author>
<author>K Siorpaes</author>
</authors>
<title>Harvesting wiki consensus – using wikipedia entries as ontology.</title>
<date>2006</date>
<booktitle>In Proc. ESWC-06 Workshop on Semantic Wikis,</booktitle>
<pages>132--46</pages>
<contexts>
<context position="6498" citStr="[6]" startWordPosition="1051" endWordPosition="1051"> where the titles of ‘equivalent’ Wikipedia pages are collected for a number of preselected languages on the assumption that they represent an accurate translation of each other (see Fig.1). The authors repeat the same exercise for a number of specific domains, and also with the names of Wikipedia categories. The latter case demonstrates the potential to use Wikipedia not only to collect multilingual synsets, but also to add a hierarchical relationship between them, as this information is explicitly present there. A number of other researchers have in fact used Wikipedia to extract ontologies [6], [7], but in all cases this was done for a single language. Semantically disambiguated corpora, including those using WordNet synsets as semantic tags, are valuable assets [10], [9], but creating them requires a considerable effort. Here we propose an alternative approach, where a text is automatically annotated with lexical semantic tags in the form of multilingual synsets, provided the text forms part of a multilingual, parallel corpus. Table 1: Examples of lexical semantic annotation using standard English WordNet synsets (above) and multilingual synsets (below). A darkened outlook for Ger</context>
</contexts>
<marker>[6]</marker>
<rawString>M. Hepp, D. Bachlechner, and K. Siorpaes. Harvesting wiki consensus – using wikipedia entries as ontology. In Proc. ESWC-06 Workshop on Semantic Wikis, pages 132–46, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Herbelot</author>
<author>A Copestake</author>
</authors>
<title>Acquiring ontological relationships from wikipedia using RMRS.</title>
<date>2006</date>
<booktitle>In Proc. ISWC-06 Workshop on Web Content Mining with Human Language,</booktitle>
<contexts>
<context position="3384" citStr="[7]" startWordPosition="531" endWordPosition="531">veloped in Princeton covered the English language, but this effort has been replicated for other languages [25]. All these databases are monolingual; they are also handcrafted, and while very comprehensive in many aspects, it is difficult to assume they could be kept abreast of the new developments, including the use of newly coined words, and giving new meanings to the old ones. The dawn and rapid growth of dynamic online encyclopedic resources with shared authorship, such as Wikipedia, in the last decade, have begun to draw attention as a potential source of semantic concepts and ontologies [7]. Recently, there have also been efforts to use the likes of Wikipedia to the existing hand-crafted resources [13]. 2 Multilingual Synsets The synsets in WordNet clarify a concept (or, from another point of view, narrow down the sense of a word) by paraphrasing it repeatedly, using other words of the same language. This approach is based on the fact that words rarely share all their meanings: {step, pace} specifies a different meaning from {step, stair}. The same result, however, can be achieved through the use of words of different languages that share at least one sense and therefore can be </context>
<context position="6503" citStr="[7]" startWordPosition="1052" endWordPosition="1052">e the titles of ‘equivalent’ Wikipedia pages are collected for a number of preselected languages on the assumption that they represent an accurate translation of each other (see Fig.1). The authors repeat the same exercise for a number of specific domains, and also with the names of Wikipedia categories. The latter case demonstrates the potential to use Wikipedia not only to collect multilingual synsets, but also to add a hierarchical relationship between them, as this information is explicitly present there. A number of other researchers have in fact used Wikipedia to extract ontologies [6], [7], but in all cases this was done for a single language. Semantically disambiguated corpora, including those using WordNet synsets as semantic tags, are valuable assets [10], [9], but creating them requires a considerable effort. Here we propose an alternative approach, where a text is automatically annotated with lexical semantic tags in the form of multilingual synsets, provided the text forms part of a multilingual, parallel corpus. Table 1: Examples of lexical semantic annotation using standard English WordNet synsets (above) and multilingual synsets (below). A darkened outlook for Germany’</context>
</contexts>
<marker>[7]</marker>
<rawString>A. Herbelot and A. Copestake. Acquiring ontological relationships from wikipedia using RMRS. In Proc. ISWC-06 Workshop on Web Content Mining with Human Language, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kazakov</author>
</authors>
<title>Unsupervised learning of nave morphology with genetic algorithms.</title>
<date>1997</date>
<booktitle>in Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks,</booktitle>
<pages>105--112</pages>
<editor>In W. van den Bosch and A. Weijters, editors,</editor>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="10907" citStr="[8]" startWordPosition="1776" endWordPosition="1776">ntries for each language, which can be narrowed down through the use of conventional lexicons listing the possible pairs of lexical entries between given two languages. In this way, the word form ‘works’ will be mapped onto the lexemes work, n., works, n., and work, v., but in the context of the French travail, only the first lexeme will be retained, as the other two would not form a translation pair in an English-French lexicon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development,</context>
</contexts>
<marker>[8]</marker>
<rawString>D. Kazakov. Unsupervised learning of nave morphology with genetic algorithms. In W. van den Bosch and A. Weijters, editors, in Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, pages 105–112, Prague, Czech Republic, April 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kazakov</author>
</authors>
<title>Combining LAPIS and WordNet for the learning of LR parsers with optimal semantic constraints.</title>
<date>1999</date>
<booktitle>The Ninth International Workshop ILP-99, volume 1634 of LNAI,</booktitle>
<editor>In S. Dzeroski and P. Flach, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<location>Bled, Slovenia,</location>
<contexts>
<context position="6680" citStr="[9]" startWordPosition="1079" endWordPosition="1079"> Fig.1). The authors repeat the same exercise for a number of specific domains, and also with the names of Wikipedia categories. The latter case demonstrates the potential to use Wikipedia not only to collect multilingual synsets, but also to add a hierarchical relationship between them, as this information is explicitly present there. A number of other researchers have in fact used Wikipedia to extract ontologies [6], [7], but in all cases this was done for a single language. Semantically disambiguated corpora, including those using WordNet synsets as semantic tags, are valuable assets [10], [9], but creating them requires a considerable effort. Here we propose an alternative approach, where a text is automatically annotated with lexical semantic tags in the form of multilingual synsets, provided the text forms part of a multilingual, parallel corpus. Table 1: Examples of lexical semantic annotation using standard English WordNet synsets (above) and multilingual synsets (below). A darkened outlook for Germany’s banks:SS1 The banks:SS2 of the river Nile SS1={bank, depository financial institution} Gloss=“a financial institution that accepts deposits and lends money” SS2={bank} Gloss=“</context>
</contexts>
<marker>[9]</marker>
<rawString>D. Kazakov. Combining LAPIS and WordNet for the learning of LR parsers with optimal semantic constraints. In S. Dzeroski and P. Flach, editors, The Ninth International Workshop ILP-99, volume 1634 of LNAI, Bled, Slovenia, 1999. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Tengi</author>
</authors>
<title>WordNet: An Electronic Lexical Database, chapter Building semantic concordances.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="6675" citStr="[10]" startWordPosition="1078" endWordPosition="1078">r (see Fig.1). The authors repeat the same exercise for a number of specific domains, and also with the names of Wikipedia categories. The latter case demonstrates the potential to use Wikipedia not only to collect multilingual synsets, but also to add a hierarchical relationship between them, as this information is explicitly present there. A number of other researchers have in fact used Wikipedia to extract ontologies [6], [7], but in all cases this was done for a single language. Semantically disambiguated corpora, including those using WordNet synsets as semantic tags, are valuable assets [10], [9], but creating them requires a considerable effort. Here we propose an alternative approach, where a text is automatically annotated with lexical semantic tags in the form of multilingual synsets, provided the text forms part of a multilingual, parallel corpus. Table 1: Examples of lexical semantic annotation using standard English WordNet synsets (above) and multilingual synsets (below). A darkened outlook for Germany’s banks:SS1 The banks:SS2 of the river Nile SS1={bank, depository financial institution} Gloss=“a financial institution that accepts deposits and lends money” SS2={bank} Gl</context>
</contexts>
<marker>[10]</marker>
<rawString>S. Landes, C. Leacock, and R. Tengi. WordNet: An Electronic Lexical Database, chapter Building semantic concordances. MIT Press, Cambridge, Mass., 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>Cyc: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2285" citStr="[11]" startWordPosition="345" endWordPosition="345">t onto some sensory experience (leading to a philosophical discussion about the extent to which they are shared and the notion of qualia), or by describing it through other concepts, in a way that is mutually recursive and unbounded (cf. Peirces Sign Theory and the notion of infinite semiosis). The last twenty five years saw academic and commercial efforts directed towards the creation of large repositories combining the description of semantic concepts, their relationship, and, possibly, common knowledge statements expressed in those terms. This includes, among others, the Cycorp Cyc project [11] and the lexical semantic database WordNet [14]. Both approaches use a number of predicates to make statements, such as “concept A is an instance of concept B” or “concept A is a specific case of concept B” (in other terms, all instances of concept A form a subset of the instances of concept B). WordNet also employs the notion of synsets, defining a semantic concept through the list of words (synonyms) sharing that meaning, e.g., {mercury, quicksilver, Hg}. The original version of WordNet developed in Princeton covered the English language, but this effort has been replicated for other languag</context>
</contexts>
<marker>[11]</marker>
<rawString>D. Lenat. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11), 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Rretrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>768--774</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="11660" citStr="[12]" startWordPosition="1898" endWordPosition="1898">tly onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets corresponding to multiple word forms of the same lexeme, resp. by combining those only varying in the use of synonyms of a given language. Extraction of hierarchical semantic relationships between words in a corpus has been studied for almost two decades [5], and the same procedures can be applied here, leading to the acquisition of a lexical resource akin to WordNet, which also contains some of the</context>
</contexts>
<marker>[12]</marker>
<rawString>D. Lin. Automatic Rretrieval and Clustering of Similar Words. In Proceedings of the 17th international conference on Computational linguistics, pages 768–774, Montreal, Quebec, Canada, August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Medelyan</author>
<author>C Legg</author>
</authors>
<title>Integrating Cyc and Wikipedia: Folksonomy meets rigorously defined common-sense.</title>
<date>2008</date>
<booktitle>In Proceedings of the WikiAI Workshop at AAAI-2008,</booktitle>
<location>Chicago,</location>
<contexts>
<context position="3498" citStr="[13]" startWordPosition="549" endWordPosition="549">All these databases are monolingual; they are also handcrafted, and while very comprehensive in many aspects, it is difficult to assume they could be kept abreast of the new developments, including the use of newly coined words, and giving new meanings to the old ones. The dawn and rapid growth of dynamic online encyclopedic resources with shared authorship, such as Wikipedia, in the last decade, have begun to draw attention as a potential source of semantic concepts and ontologies [7]. Recently, there have also been efforts to use the likes of Wikipedia to the existing hand-crafted resources [13]. 2 Multilingual Synsets The synsets in WordNet clarify a concept (or, from another point of view, narrow down the sense of a word) by paraphrasing it repeatedly, using other words of the same language. This approach is based on the fact that words rarely share all their meanings: {step, pace} specifies a different meaning from {step, stair}. The same result, however, can be achieved through the use of words of different languages that share at least one sense and therefore can be seen as translations of each other. So, {EN: bank, FR: banque} could refer to a financial institution or a collect</context>
</contexts>
<marker>[13]</marker>
<rawString>O. Medelyan and C. Legg. Integrating Cyc and Wikipedia: Folksonomy meets rigorously defined common-sense. In Proceedings of the WikiAI Workshop at AAAI-2008, Chicago, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database.</booktitle>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="2332" citStr="[14]" startWordPosition="352" endWordPosition="352">ilosophical discussion about the extent to which they are shared and the notion of qualia), or by describing it through other concepts, in a way that is mutually recursive and unbounded (cf. Peirces Sign Theory and the notion of infinite semiosis). The last twenty five years saw academic and commercial efforts directed towards the creation of large repositories combining the description of semantic concepts, their relationship, and, possibly, common knowledge statements expressed in those terms. This includes, among others, the Cycorp Cyc project [11] and the lexical semantic database WordNet [14]. Both approaches use a number of predicates to make statements, such as “concept A is an instance of concept B” or “concept A is a specific case of concept B” (in other terms, all instances of concept A form a subset of the instances of concept B). WordNet also employs the notion of synsets, defining a semantic concept through the list of words (synonyms) sharing that meaning, e.g., {mercury, quicksilver, Hg}. The original version of WordNet developed in Princeton covered the English language, but this effort has been replicated for other languages [25]. All these databases are monolingual; t</context>
</contexts>
<marker>[14]</marker>
<rawString>G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. Five papers on WordNet. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press, May 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nakayama</author>
<author>T Hara</author>
<author>S Nishio</author>
</authors>
<title>Wikipedia mining for an association web thesaurus construction. In</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE International Conference on Web Information Systems Engineering,</booktitle>
<pages>322--334</pages>
<contexts>
<context position="11366" citStr="[15]" startWordPosition="1849" endWordPosition="1849">doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets corresponding to multiple word forms of the same lexeme, resp. by combining those only varying in the use of</context>
</contexts>
<marker>[15]</marker>
<rawString>K. Nakayama, T. Hara, and S. Nishio. Wikipedia mining for an association web thesaurus construction. In In Proceedings of IEEE International Conference on Web Information Systems Engineering, pages 322–334, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8765" citStr="[16]" startWordPosition="1409" endWordPosition="1409">se, provided they correspond to (are tagged with) the same multilingual synset. Two great advantages of this scheme are that it is completely unsupervised, and the fact that, unlike manually tagged corpora using WordNet, all words in the corpus are guaranteed to have a corresponding multilingual synset. Since we are only interested in words with their own semantics, a stop list can be used to remove the words of the closed lexicon before the rest are semantically tagged. Also the need for word alignment should not be an issue, at least in principle, as there are standard tools, such as GIZA++ [16] serving that purpose. The approach as described so far needs to deal with two main issues, both related to the fact that the simple listing of n-tuples as synsets may produce many more synsets than the real number of concepts to which the words in the text refer. The first issue stems from the fact that a lexeme (word entry) corresponds to several word forms in most languages, so, for instance, the word forms {EN: work} and {EN: works} will correspond to two different synsets (Table 2, top), even if they are used with the same meaning. The second of the above mentioned issues is related to th</context>
</contexts>
<marker>[16]</marker>
<rawString>F. J. Och and H. Ney. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
</authors>
<title>Two-level description of turkish morphology.</title>
<date>1993</date>
<booktitle>In EACL,</booktitle>
<contexts>
<context position="10045" citStr="[17]" startWordPosition="1633" endWordPosition="1633">use of the same word (lexeme) (Table 2, bottom). From this point of view, we could consider the original n-tuples as proto-synsets, and then strife to recognize the variation due to the use of different word forms and synonyms, and eliminate it, if possible, by grouping these proto-synsets into genuine synsets corresponding to different concepts. As much of the appeal of the whole approach stems from its unsupervised nature, we shall also consider unsupervised ways of solving this specific problem. For several languages, there are detailed, explicit models of their word morphology [19], [20], [17], which makes mapping word forms onto the list of lexemes they may represent a straightforward task. 10 Fig. 1: Wikipedia page titles seen as multilingual synsets. Using morpho-lexical analyzers for the languages in the corpus will produce a list of lexical entries for each language, which can be narrowed down through the use of conventional lexicons listing the possible pairs of lexical entries between given two languages. In this way, the word form ‘works’ will be mapped onto the lexemes work, n., works, n., and work, v., but in the context of the French travail, only the first lexeme will b</context>
</contexts>
<marker>[17]</marker>
<rawString>K. Oflazer. Two-level description of turkish morphology. In EACL, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y C Park</author>
<author>K-S Choi</author>
</authors>
<title>Automatic thesaurus construction using Bayesian networks.</title>
<date>1996</date>
<booktitle>Information Processing and Management: an International Journal,</booktitle>
<volume>32</volume>
<issue>5</issue>
<contexts>
<context position="11306" citStr="[18]" startWordPosition="1842" endWordPosition="1842"> pair in an English-French lexicon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets corresponding to multiple word forms of the same</context>
</contexts>
<marker>[18]</marker>
<rawString>Y. C. Park and K.-S. Choi. Automatic thesaurus construction using Bayesian networks. Information Processing and Management: an International Journal, 32(5):543–553, September 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Paskaleva</author>
</authors>
<title>A formal procedure for Bulgarian word form generation.</title>
<date>1982</date>
<booktitle>In COLING,</booktitle>
<pages>217--221</pages>
<contexts>
<context position="10033" citStr="[19]" startWordPosition="1631" endWordPosition="1631">other makes use of the same word (lexeme) (Table 2, bottom). From this point of view, we could consider the original n-tuples as proto-synsets, and then strife to recognize the variation due to the use of different word forms and synonyms, and eliminate it, if possible, by grouping these proto-synsets into genuine synsets corresponding to different concepts. As much of the appeal of the whole approach stems from its unsupervised nature, we shall also consider unsupervised ways of solving this specific problem. For several languages, there are detailed, explicit models of their word morphology [19], [20], [17], which makes mapping word forms onto the list of lexemes they may represent a straightforward task. 10 Fig. 1: Wikipedia page titles seen as multilingual synsets. Using morpho-lexical analyzers for the languages in the corpus will produce a list of lexical entries for each language, which can be narrowed down through the use of conventional lexicons listing the possible pairs of lexical entries between given two languages. In this way, the word form ‘works’ will be mapped onto the lexemes work, n., works, n., and work, v., but in the context of the French travail, only the first l</context>
</contexts>
<marker>[19]</marker>
<rawString>E. Paskaleva. A formal procedure for Bulgarian word form generation. In COLING, pages 217–221, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Ritchie</author>
<author>G J Russell</author>
<author>A W Black</author>
<author>S G Pulman</author>
</authors>
<title>Computational Morphology: Practical Mechanism for the English Lexicon.</title>
<date>1991</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="10039" citStr="[20]" startWordPosition="1632" endWordPosition="1632">makes use of the same word (lexeme) (Table 2, bottom). From this point of view, we could consider the original n-tuples as proto-synsets, and then strife to recognize the variation due to the use of different word forms and synonyms, and eliminate it, if possible, by grouping these proto-synsets into genuine synsets corresponding to different concepts. As much of the appeal of the whole approach stems from its unsupervised nature, we shall also consider unsupervised ways of solving this specific problem. For several languages, there are detailed, explicit models of their word morphology [19], [20], [17], which makes mapping word forms onto the list of lexemes they may represent a straightforward task. 10 Fig. 1: Wikipedia page titles seen as multilingual synsets. Using morpho-lexical analyzers for the languages in the corpus will produce a list of lexical entries for each language, which can be narrowed down through the use of conventional lexicons listing the possible pairs of lexical entries between given two languages. In this way, the word form ‘works’ will be mapped onto the lexemes work, n., works, n., and work, v., but in the context of the French travail, only the first lexeme </context>
</contexts>
<marker>[20]</marker>
<rawString>G. D. Ritchie, G. J. Russell, A. W. Black, and S. G. Pulman. Computational Morphology: Practical Mechanism for the English Lexicon. MIT Press, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Yang</author>
<author>C Yu</author>
</authors>
<title>A Theory of Term Importance in Automatic Text Analysis.</title>
<date>1975</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="11257" citStr="[21]" startWordPosition="1835" endWordPosition="1835">ed, as the other two would not form a translation pair in an English-French lexicon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets</context>
</contexts>
<marker>[21]</marker>
<rawString>G. Salton, C. Yang, and C. Yu. A Theory of Term Importance in Automatic Text Analysis. Journal of the American Society for Information Science, 26(1):33–44, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Shahid</author>
<author>D Kazakov</author>
</authors>
<title>Automatic Multilingual Lexicon Generation using Wikipedia as a Resource.</title>
<date>2009</date>
<booktitle>In Proc. of the Intl. Conf. on Agents and Artificial Intelligence (ICAART),</booktitle>
<location>Porto, Portugal,</location>
<contexts>
<context position="5847" citStr="[22]" startWordPosition="946" endWordPosition="946">anings. The greater the number of languages in which the two synsets differ, the stronger the intuition that they indicate different meanings of the word in the pivotal language. Synsets, whether monolingual or multilingual, can be used to tag the lexical items in a corpus with their intended meaning (see Table 1). The benefits of such lexical semantic tags are evident. Focussing now on the novel case of multilingual synsets, one can consider the two separate questions of how to produce a collection of such synsets, and how to annotate the lexical items in a text with them. Kazakov and Shahid [22] present an interesting study in that direction, where the titles of ‘equivalent’ Wikipedia pages are collected for a number of preselected languages on the assumption that they represent an accurate translation of each other (see Fig.1). The authors repeat the same exercise for a number of specific domains, and also with the names of Wikipedia categories. The latter case demonstrates the potential to use Wikipedia not only to collect multilingual synsets, but also to add a hierarchical relationship between them, as this information is explicitly present there. A number of other researchers ha</context>
</contexts>
<marker>[22]</marker>
<rawString>A. R. Shahid and D. Kazakov. Automatic Multilingual Lexicon Generation using Wikipedia as a Resource. In Proc. of the Intl. Conf. on Agents and Artificial Intelligence (ICAART), Porto, Portugal, January 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Steels</author>
</authors>
<title>Emergent adaptive lexicons. In</title>
<date>1996</date>
<booktitle>Fourth International Conference on Simulation of Adaptive Behavior. The MIT</booktitle>
<editor>P. Maes, M. Mataric, J.-A. Meyer, J. Pollack, and S. W. Wilson, editors,</editor>
<publisher>Press/Bradford Books,</publisher>
<contexts>
<context position="907" citStr="[23]" startWordPosition="136" endWordPosition="136">otated parallel corpora of a lexical semantic resource akin to WordNet. The paper also describes how this resource can be used to add lexical semantic tags to the text corpus at hand. Finally, we discuss the possibility to add some of the predicates typical for WordNet to its automatically constructed multilingual version, and the ways in which the success of this approach can be measured. Keywords Parallel corpora, WordNet, unsupervised learning 1 Introduction Lexical ambiguity is inherent and widespread in all languages; it emerges spontaneously in computer simulations of language evolution [23], and its origin probably stems in the interplay between geographic divisions and interaction between communities, diachronic linguistic changes, and evolutionary pressures on the cost of communication. Two challenges arise when dealing with lexical ambiguity: firstly, to define the elementary semantic concepts employed in a given language, and, secondly, given one or more utterances, to identify the semantic concepts to which the words in those utterances refer. Throughout history, numerous attempts have been made to address both challenges through the construction of artificial languages wit</context>
</contexts>
<marker>[23]</marker>
<rawString>L. Steels. Emergent adaptive lexicons. In P. Maes, M. Mataric, J.-A. Meyer, J. Pollack, and S. W. Wilson, editors, Fourth International Conference on Simulation of Adaptive Behavior. The MIT Press/Bradford Books, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tokunaga</author>
<author>M Iwayama</author>
<author>H Tanaka</author>
</authors>
<title>Automatic thesaurus construction based-on grammatical relations. In</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1308--1313</pages>
<contexts>
<context position="11336" citStr="[24]" startWordPosition="1845" endWordPosition="1845">icon. One could also consider doing away with any models of morphology, assuming complete ignorance in this respect, and employing unsupervised learning of word morphology [8], [4]. In their latest form, these approaches can identify word form paradigms, which would allow all forms of a lexical entry to be mapped consistently onto it. It is also possible to automate the process of identifying synonyms among the words of a given language. For instance, Crouch’s approach [2] is based on the discrimination value model [21]. Other approaches include Bayesian Networks [18], Hierarchical Clustering [24], and link co-occurrences [15], etc. Such automated approaches have certain advantages over the manually generated thesauri, both in terms of cost and time of development, and also in the level of detail, with the latter often being too fine grained, and reflecting usages that are not common and irrelevant in practice [12]. 4 Extracting a Fully-Fledged Multilingual WordNet So far, we have described a procedure that extracts multilingual synsets from a parallel corpus and reduces spurious ambiguity by merging synsets corresponding to multiple word forms of the same lexeme, resp. by combining th</context>
</contexts>
<marker>[24]</marker>
<rawString>T. Tokunaga, M. Iwayama, and H. Tanaka. Automatic thesaurus construction based-on grammatical relations. In In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 1308–1313, 1995.</rawString>
</citation>
<citation valid="false">
<date>1998</date>
<editor>P. Vossen, editor.</editor>
<publisher>EuroWordNet. Kluwer,</publisher>
<contexts>
<context position="2892" citStr="[25]" startWordPosition="447" endWordPosition="447">d the lexical semantic database WordNet [14]. Both approaches use a number of predicates to make statements, such as “concept A is an instance of concept B” or “concept A is a specific case of concept B” (in other terms, all instances of concept A form a subset of the instances of concept B). WordNet also employs the notion of synsets, defining a semantic concept through the list of words (synonyms) sharing that meaning, e.g., {mercury, quicksilver, Hg}. The original version of WordNet developed in Princeton covered the English language, but this effort has been replicated for other languages [25]. All these databases are monolingual; they are also handcrafted, and while very comprehensive in many aspects, it is difficult to assume they could be kept abreast of the new developments, including the use of newly coined words, and giving new meanings to the old ones. The dawn and rapid growth of dynamic online encyclopedic resources with shared authorship, such as Wikipedia, in the last decade, have begun to draw attention as a potential source of semantic concepts and ontologies [7]. Recently, there have also been efforts to use the likes of Wikipedia to the existing hand-crafted resource</context>
</contexts>
<marker>[25]</marker>
<rawString>P. Vossen, editor. EuroWordNet. Kluwer, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>