<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.456704">
Knowledge-Based Question Answering as Machine Translation
</title>
<author confidence="0.474099">
Junwei Bao† , Nan Duan$ , Ming Zhou$ , Tiejun Zhao††Harbin Institute of Technology
</author>
<affiliation confidence="0.537962">
$Microsoft Research
</affiliation>
<email confidence="0.806825">
baojunwei001@gmail.com
{nanduan, mingzhou}@microsoft.com
tjzhao@hit.edu.cn
</email>
<sectionHeader confidence="0.994537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945884615385">
A typical knowledge-based question an-
swering (KB-QA) system faces two chal-
lenges: one is to transform natural lan-
guage questions into their meaning repre-
sentations (MRs); the other is to retrieve
answers from knowledge bases (KBs) us-
ing generated MRs. Unlike previous meth-
ods which treat them in a cascaded man-
ner, we present a translation-based ap-
proach to solve these two tasks in one u-
nified framework. We translate questions
to answers based on CYK parsing. An-
swers as translations of the span covered
by each CYK cell are obtained by a ques-
tion translation method, which first gener-
ates formal triple queries as MRs for the
span based on question patterns and re-
lation expressions, and then retrieves an-
swers from a given KB based on triple
queries generated. A linear model is de-
fined over derivations, and minimum er-
ror rate training is used to tune feature
weights based on a set of question-answer
pairs. Compared to a KB-QA system us-
ing a state-of-the-art semantic parser, our
method achieves better results.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976013888888889">
Knowledge-based question answering (KB-QA)
computes answers to natural language (NL) ques-
tions based on existing knowledge bases (KBs).
Most previous systems tackle this task in a cas-
caded manner: First, the input question is trans-
formed into its meaning representation (MR) by
an independent semantic parser (Zettlemoyer and
Collins, 2005; Mooney, 2007; Artzi and Zettle-
moyer, 2011; Liang et al., 2011; Cai and Yates,
</bodyText>
<footnote confidence="0.6346655">
∗This work was finished while the author was visiting Mi-
crosoft Research Asia.
</footnote>
<bodyText confidence="0.992863975609756">
2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski
et al., 2013; Berant et al., 2013); Then, the answer-
s are retrieved from existing KBs using generated
MRs as queries.
Unlike existing KB-QA systems which treat se-
mantic parsing and answer retrieval as two cas-
caded tasks, this paper presents a unified frame-
work that can integrate semantic parsing into the
question answering procedure directly. Borrow-
ing ideas from machine translation (MT), we treat
the QA task as a translation procedure. Like MT,
CYK parsing is used to parse each input question,
and answers of the span covered by each CYK cel-
l are considered the translations of that cell; un-
like MT, which uses offline-generated translation
tables to translate source phrases into target trans-
lations, a semantic parsing-based question trans-
lation method is used to translate each span into
its answers on-the-fly, based on question patterns
and relation expressions. The final answers can be
obtained from the root cell. Derivations generated
during such a translation procedure are modeled
by a linear model, and minimum error rate train-
ing (MERT) (Och, 2003) is used to tune feature
weights based on a set of question-answer pairs.
Figure 1 shows an example: the question direc-
tor of movie starred by Tom Hanks is translated to
one of its answers Robert Zemeckis by three main
steps: (i) translate director of to director of; (ii)
translate movie starred by Tom Hanks to one of it-
s answers Forrest Gump; (iii) translate director of
Forrest Gump to a final answer Robert Zemeckis.
Note that the updated question covered by Cell[0,
6] is obtained by combining the answers to ques-
tion spans covered by Cell[0, 1] and Cell[2, 6].
The contributions of this work are two-fold: (1)
We propose a translation-based KB-QA method
that integrates semantic parsing and QA in one
unified framework. The benefit of our method
is that we don’t need to explicitly generate com-
plete semantic structures for input questions. Be-
</bodyText>
<page confidence="0.950691">
967
</page>
<note confidence="0.91944">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 967–976,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999789">
Figure 1: Translation-based KB-QA example
</figureCaption>
<bodyText confidence="0.999859285714286">
sides which, answers generated during the transla-
tion procedure help significantly with search space
pruning. (2) We propose a robust method to trans-
form single-relation questions into formal triple
queries as their MRs, which trades off between
transformation accuracy and recall using question
patterns and relation expressions respectively.
</bodyText>
<sectionHeader confidence="0.995607" genericHeader="method">
2 Translation-Based KB-QA
</sectionHeader>
<subsectionHeader confidence="0.630151">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.9755038">
Formally, given a knowledge base KB and an N-
L question Q, our KB-QA method generates a set
of formal triples-answer pairs {hD, Ai} as deriva-
tions, which are scored and ranked by the distribu-
tion P(hD, Ai|KB, Q) defined as follows:
</bodyText>
<note confidence="0.721664">
Hanks, Film.Actor.Film, Forrest Gumpi62 and
hForrest Gump, Film.Film.Director, Robert
Zemeckisi60 are three ordered formal triples
corresponding to the three translation steps in
</note>
<figureCaption confidence="0.488461666666667">
Figure 1. We define the task of transforming
question spans into formal triples as question
translation. A denotes one final answer of Q.
</figureCaption>
<listItem confidence="0.9950545">
• hi(·) denotes the ith feature function.
• Ai denotes the feature weight of hi(·).
</listItem>
<bodyText confidence="0.949462375">
According to the above description, our KB-
QA method can be decomposed into four tasks as:
(1) search space generation for H(Q); (2) ques-
tion translation for transforming question spans in-
to their corresponding formal triples; (3) feature
design for hi(·); and (4) feature weight tuning for
{Ai}. We present details of these four tasks in the
following subsections one-by-one.
</bodyText>
<subsectionHeader confidence="0.99854">
2.2 Search Space Generation
</subsectionHeader>
<bodyText confidence="0.999752333333333">
We first present our translation-based KB-QA
method in Algorithm 1, which is used to generate
H(Q) for each input NL question Q.
</bodyText>
<figure confidence="0.94476846875">
Algorithm 1: Translation-based KB-QA
1 for l = 1 to |Q |do
for all i,js.t. j − i = l do
(iii) director of Forrest Gump ⟹ Robert Zemeckis
director of movie starred by Tom Hanks
Cell[0, 6]
Cell[0, 1]
Cell[2, 6]
(ii) movie starred by Tom Hanks ⟹ Forrest Gump
(i) director of ⟹ director of
H(Qj i ) =∅;
T = QTrans(Qji, KB);
foreach formal triple t ∈ T do
create a new derivation d;
d.A = t.eobj;
d.D = {t};
update the model score of d;
insert d to H(Qji );
end
end
2
3
4
5
6
7
8
9
10
11
12
exp{EMi=1 Ai · hi(hD, Ai, KB, Q)}
</figure>
<listItem confidence="0.90566575">
F(D&apos;,A&apos;)∈H(Q) exp{EMi=1 Ai · hi(hD&apos;, A&apos;i, KB, Q)}
• KB denotes a knowledge base1 that stores a
set of assertions. Each assertion t ∈ KB is in
the form of {eID
</listItem>
<equation confidence="0.9072775">
sbj, p, eID
obj}, where p denotes
a predicate, eID
sbj and eID
</equation>
<bodyText confidence="0.819404">
obj denote the subject
and object entities of t, with unique IDs2.
</bodyText>
<listItem confidence="0.881133">
• H(Q) denotes the search space {hD, Ai}. D
is composed of a set of ordered formal triples
{t1, ..., tn}. Each triple t = {esbj, p, eobj}ji ∈
D denotes an assertion in KB, where i and
j denotes the beginning and end indexes of
the question span from which t is trans-
formed. The order of triples in D denotes
the order of translation steps from Q to A.
E.g., hdirector of, Null, director ofi10,
</listItem>
<footnote confidence="0.9963126">
1We use a large scale knowledge base in this paper, which
contains 2.3B entities, 5.5K predicates, and 18B assertions. A
16-machine cluster is used to host and serve the whole data.
2Each KB entity has a unique ID. For the sake of conve-
nience, we omit the ID information in the rest of the paper.
</footnote>
<figure confidence="0.71119475">
hTom
end
end
end
end
13 end
14 for l = 1 to |Q |do
for all i,js.t. j − i = l do
for all m s.t. i ≤ m &lt; j do
for dl ∈ H(Qmi ) and dr ∈ H(Qjm+1) do
Qupdate = dl.A + dr.A;
T = QTrans(Qupdate, KB);
foreach formal triple t ∈ T do
create a new derivation d;
d.A = t.eobj;
d.D = dl.D U dr.D U{t};
update the model score of d;
insert d to H(Qji );
15
16
17
18
19
20
</figure>
<page confidence="0.856237083333333">
21
22
23
24
25
26
27
28
29
30 end
31 returnH(Q).
968
</page>
<bodyText confidence="0.999965952380952">
The first half (from Line 1 to Line 13) gen-
erates a formal triple set T for each unary span
Qji E Q, using the question translation method
QTrans(Qji,KB) (Line 4), which takes Qji as the
input. Each triple t E T returned is in the form of
{esbj,p, eobj}, where esbj’s mention occurs in Qji,
p is a predicate that denotes the meaning expressed
by the context of esbj in Qji, eobj is an answer of
Qji based on esbj, p and KB. We describe the im-
plementation detail of QTrans(·) in Section 2.3.
The second half (from Line 14 to Line 31) first
updates the content of each bigger span Qji by con-
catenating the answers to its any two consecutive
smaller spans covered by Qji (Line 18). Then,
QTrans(Qji,KB) is called to generate triples for
the updated span (Line 19). The above operations
are equivalent to answering a simplified question,
which is obtained by replacing the answerable
spans in the original question with their corre-
sponding answers. The search space H(Q) for the
entire question Q is returned at last (Line 31).
</bodyText>
<subsectionHeader confidence="0.99976">
2.3 Question Translation
</subsectionHeader>
<bodyText confidence="0.999949545454546">
The purpose of question translation is to translate
a span Q to a set of formal triples T. Each triple
t E T is in the form of {esbj, p, eobj}, where esbj’s
mention3 occurs in Q, p is a predicate that denotes
the meaning expressed by the context of esbj in
Q, eobj is an answer to Q retrieved from KB us-
ing a triple query q = {esbj, p, ?}. Note that if
no predicate p or answer eobj can be generated,
{Q, Null, Q} will be returned as a special triple,
which sets eobj to be Q itself, and p to be Null.
This makes sure the un-answerable spans can be
passed on to the higher-level operations.
Question translation assumes each span Q is a
single-relation question (Fader et al., 2013). Such
assumption simplifies the efforts of semantic pars-
ing to the minimum question units, while leaving
the capability of handling multiple-relation ques-
tions (Figure 1 gives one such example) to the out-
er CYK-parsing based translation procedure. Two
question translation methods are presented in the
rest of this subsection, which are based on ques-
tion patterns and relation expressions respectively.
</bodyText>
<subsectionHeader confidence="0.839985">
2.3.1 Question Pattern-based Translation
</subsectionHeader>
<bodyText confidence="0.99928">
A question pattern QP includes a pattern string
QPpattern, which is composed of words and a slot
</bodyText>
<footnote confidence="0.911749">
3For simplicity, a cleaned entity dictionary dumped from
the entire KB is used to detect entity mentions in Q.
</footnote>
<equation confidence="0.8487372">
Algorithm 2: QP-based Question Translation
1 T = ∅;
2 foreach entity mention eQ ∈ Q do
Qpattern = replace eQ in Q with [Slot];
foreach question pattern QP do
if Qpattern == QPpattern then
E = Disambiguate(eQ, QPpredicate);
foreach e ∈ E do
create a new triple query q;
q = {e, QPpredicate, ?};
{Ai} = AnswerRetrieve(q, KB);
foreach A ∈ {Ai} do
create a new formal triple t;
t = {q.esbj, q.p, A};
t.score = 1.0;
</equation>
<figure confidence="0.812753714285714">
insert t to T;
end
end
end
end
20 end
21 return T.
</figure>
<bodyText confidence="0.99989646875">
symbol [Slot], and a KB predicate QPpredicate,
which denotes the meaning expressed by the con-
text words in QPpattern.
Algorithm 2 shows how to generate formal
triples for a span Q based on question pattern-
s (QP-based question translation). For each en-
tity mention eQ E Q, we replace it with [Slot]
and obtain a pattern string Qpattern (Line 3). If
Qpattern can match one QPpattern, then we con-
struct a triple query q (Line 9) using QPpredicate
as its predicate and one of the KB entities re-
turned by Disambiguate(eQ, QPpredicate) as it-
s subject entity (Line 6). Here, the objective of
Disambiguate(eQ, QPpredicate) is to output a set
of disambiguated KB entities E in KB. The name
of each entity returned equals the input entity
mention eQ and occurs in some assertions where
QPpredicate are the predicates. The underlying
idea is to use the context (predicate) information to
help entity disambiguation. The answers of q are
returned by AnswerRetrieve(q,KB) based on q
and KB (Line 10), each of which is used to con-
struct a formal triple and added to T for Q (from
Line 11 to Line 16). Figure 2 gives an example.
Question patterns are collected as follows: First,
SW queries, which begin with What, Where, Who,
When, or Which, are selected from a large scale
query log of a commercial search engine; Then, a
cleaned entity dictionary is used to annotate each
query by replacing all entity mentions it contains
with the symbol [Slot]. Only high-frequent query
patterns which contain one [Slot] are maintained;
</bodyText>
<figure confidence="0.982765073170732">
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
969
Algorithm 3: RE-based Question Translation
1 T = O;
2 foreach entity mention eQ ∈ Q do
foreach e ∈ KB s.t. e.name==eQ do
foreach predicate p ∈ KB related to e do
score = Sim(eQ, Q, REp);
3
4
5
𝓠 : who is the director of Forrest Gump
𝓠𝓟𝒑𝒂𝒕𝒕𝒆𝒓𝒏 : who is the director of [Slot]
𝓠𝓟𝒑𝒓𝒆𝒅𝒊𝒄𝒂𝒕𝒆 : Film.Film.Director
if score &gt; 0 then
create a new triple query q;
q = {e, p, ?};
{Az} = AnswerRetrieve(q, KB);
foreach A ∈ {Az} do
𝒒 : &lt;Forrest Gump, Film.Film.Director, ?&gt; 6
𝒕 : &lt;Forrest Gump, Film.Film.Director, Robert Zemeckis&gt; 9
7
KB
8
10
</figure>
<figureCaption confidence="0.658571">
Figure 2: QP-based question translation example
</figureCaption>
<figure confidence="0.746345">
13
14
11
12
create a new formal triple t;
t = {q.e.bj, q.p, A};
t.score = score;
insert t to T;
</figure>
<bodyText confidence="0.999913">
Lastly, annotators try to manually label the most-
frequent 50,000 query patterns with their corre-
sponding predicates, and 4,764 question patterns
with single labeled predicates are obtained.
From experiments (Table 3 in Section 4.3) we
can see that, question pattern based question trans-
lation can achieve high end-to-end accuracy. But
as human efforts are needed in the mining proce-
dure, this method cannot be extended to large scale
very easily. Besides, different users often type the
questions with the same meaning in different NL
expressions. For example, although the question
Forrest Gump was directed by which moviemaker
means the same as the question Q in Figure 2, no
question pattern can cover it. We need to find an
alternative way to alleviate such coverage issue.
</bodyText>
<subsubsectionHeader confidence="0.516439">
2.3.2 Relation Expression-based Translation
</subsubsectionHeader>
<bodyText confidence="0.999613363636364">
Aiming to alleviate the coverage issue occurring in
QP-based method, an alternative relation expres-
sion (RE) -based method is proposed, and will be
used when the QP-based method fails.
We define REp as a relation expression set for
a given KB predicate p ∈ KB. Each relation ex-
pression RE ∈ REp includes an expression string
REexpression, which must contain at least one con-
tent word, and a weight REweight, which denotes
the confidence that REexpression can represent p’s
meaning in NL. For example, is the director of
is one relation expression string for the predicate
Film.Film.Director, which means it is usually used
to express this relation (predicate) in NL.
Algorithm 3 shows how to generate triples for
a question Q based on relation expressions. For
each possible entity mention eQ ∈ Q and a K-
B predicate p ∈ KB that is related to a KB enti-
ty e whose name equals eQ, Sim(eQ, Q, REp) is
computed (Line 5) based on the similarity between
question context and REp, which measures how
likely Q can be transformed into a triple query
</bodyText>
<figure confidence="0.538365571428571">
end
end
end
end
19 end
20 sort T based on the score of each t ∈ T;
21 return T.
</figure>
<bodyText confidence="0.988187333333333">
q = {e, p, ?}. If this score is larger than 0, which
means there are overlaps between Q’s context and
REp, then q will be used as the triple query of Q,
and a set of formal triples will be generated based
on q and KB (from Line 7 to Line 15). The compu-
tation of Sim(eQ, Q, REp) is defined as follows:
</bodyText>
<table confidence="0.579531666666667">
� P(Wn|REp)}
1
ten&apos;  |Q |− n + 1 {ωn∈Q,ω eQ=φ
</table>
<bodyText confidence="0.9995028">
where n is the n-gram order which ranges from 1
to 5, Wn is an n-gram occurring in Q without over-
lapping with eQ and containing at least one con-
tent word, P(Wn|REp) is the posterior probability
which is computed by:
</bodyText>
<equation confidence="0.983567">
= �` Count(Wn, REp)
P(Wn |REp) �w0nEREp
Count (W&apos;, REp)
</equation>
<bodyText confidence="0.935274">
Count(W, REp) denotes the weighted sum of
times that W occurs in REp:
</bodyText>
<equation confidence="0.7846275">
Count(W, REp) = � {#ω(RE) · REweight}
RE∈REp
</equation>
<bodyText confidence="0.99991975">
where #ω(RE) denotes the number of times that
W occurs in REexpression, and REweight is decided
by the relation expression extraction component.
Figure 3 gives an example, where n-grams with
rectangles are the ones that occur in both Q’s con-
text and the relation expression set of a given pred-
icate p = Film.Film.Director. Unlike the QP-
based method which needs a perfect match, the
</bodyText>
<figure confidence="0.633652272727273">
15
16
17
18
970
Q : Forrest Gump was directed by which moviemaker
JUMn.�iMn.Director: is directed by
was directed and written by
is the moviemaker of
was famous as the director of
...
</figure>
<figureCaption confidence="0.999786">
Figure 3: RE-based question translation example
</figureCaption>
<bodyText confidence="0.9955666">
RE-based method allows fuzzy matching between
Q and REp, and records this (Line 13) in generat-
ed triples, which is used as features later.
Relation expressions are mined as follows: Giv-
en a set of KB assertions with an identical predi-
cate p, we first extract all sentences from English
Wiki pages4, each of which contains at least one
pair of entities occurring in one assertion. Then,
we extract the shortest path between paired entities
in the dependency tree of each sentence as an RE
candidate for the given predicate. The intuition is
that any sentence containing such entity pairs oc-
cur in an assertion is likely to express the predi-
cate of that assertion in some way. Last, all rela-
tion expressions extracted are filtered by heuristic
rules, i.e., the frequency must be larger than 4, the
length must be shorter than 10, and then weighted
by the pattern scoring methods proposed in (Ger-
ber and Ngomo, 2011; Gerber and Ngomo, 2012).
For each predicate, we only keep the relation ex-
pressions whose pattern scores are larger than a
pre-defined threshold. Figure 4 gives one relation
expression extraction example. The statistics and
overall quality of the relation expressions are list-
ed in Section 4.1.
</bodyText>
<subsubsectionHeader confidence="0.613428">
2.3.3 Question Decomposition
</subsubsectionHeader>
<bodyText confidence="0.999899862068966">
Sometimes, a question may provide multiple con-
straints to its answers. movie starred by Tom Han-
ks in 1994 is one such question. All the films as
the answers of this question should satisfy the fol-
lowing two constraints: (1) starred by Tom Hanks;
and (2) released in 1994. It is easy to see that such
questions cannot be translated to single triples.
We propose a dependency tree-based method to
handle such multiple-constraint questions by (i)
decomposing the original question into a set of
sub-questions using syntax-based patterns; and (ii)
intersecting the answers of all sub-questions as the
final answers of the original question. Note, ques-
tion decomposition only operates on the original
question and question spans covered by complete
dependency subtrees. Four syntax-based patterns
(Figure 5) are used for question decomposition. If
a question matches any one of these patterns, then
sub-questions are generated by collecting the path-
s between no and each ni(i &gt; 0) in the pattern,
where each n denotes a complete subtree with a
noun, number, or question word as its root node,
the symbol ∗ above prep∗ denotes this preposition
can be skipped in matching. For the question men-
tioned at the beginning, its two sub-questions gen-
erated are movie starred by Tom Hanks and movie
starred in 1994, as its dependency form matches
pattern (a). Similar ideas are used in IBM Wat-
son (Kalyanpur et al., 2012) as well.
</bodyText>
<figure confidence="0.970725925">
n2
(a)
n0
verb
verb
n1
prep*
prep
n0
n0
q : &lt;Forrest Gump, Film.Film.Director, ?&gt;
KB
t : &lt;Forrest Gump, Film.Film.Director, Robert Zemeckis&gt;
n2 n1 and
(b)
verb
verb n0 prep* and verb
Paired entity of a
KB predicate
𝑝=Film.Film.Director
{Forrest Gump, Robert Zemeckis}
{Titanic, James Cameron}
{The Dark Knight Rises, Christopher Nolan}
...
prep*
prep*
...
prep*
n1
n2
nk
n1
Passage retrieval Robert Zemeckis is the director of Forrest Gump (c) (d)
from Wiki pages James Cameron is the moviemaker of Titanic
The Dark Knight Rises is directed by Christopher Nolan
Relation expression
weighting
is the director of   0.25
is the moviemaker of   0.23
is directed by   0.20
</figure>
<figureCaption confidence="0.997566666666667">
Figure 5: Four syntax-based patterns for question
decomposition
Figure 4: RE extraction example
</figureCaption>
<footnote confidence="0.978625">
4http://en.wikipedia.org/wiki/Wikipedia:Database download
</footnote>
<bodyText confidence="0.99748275">
As dependency parsing is not perfect, we gen-
erate single triples for such questions without con-
sidering constraints as well, and add them to the
search space for competition. hsyntax constraint(·)
</bodyText>
<page confidence="0.97741">
971
</page>
<bodyText confidence="0.897313714285714">
h
is used to boost triples that are converted from sub-
questions generated by question decomposition.
The more constraints an answer satisfies, the bet-
ter. Obviously, current patterns used can’t cover
all cases but most-common ones. We leave a more
general pattern mining method for future work.
</bodyText>
<subsectionHeader confidence="0.981815">
2.4 Feature Design
</subsectionHeader>
<bodyText confidence="0.989691333333333">
The objective of our KB-QA system is to seek the
derivation h ˆD, ˆAi that maximizes the probability
P(hD, Ai|KB, Q) described in Section 2.1 as:
</bodyText>
<equation confidence="0.986147">
ˆD, ˆAi = argmax P(hD, Ai|KB, Q)
(D,A)EW(Q)
M
= argmax Ai · hi(hD, Ai, KB, Q)
(D,A)EW(Q) i=1
</equation>
<bodyText confidence="0.9989995">
We now introduce the feature sets {hi(·)} that are
used in the above linear model:
</bodyText>
<listItem confidence="0.991306868421053">
• hquestion word(·), which counts the number of
original question words occurring in A. It pe-
nalizes those partially answered questions.
• hspan(·), which counts the number of spans
in Q that are converted to formal triples. It
controls the granularity of the spans used in
question translation.
• hsyntax subtree(·), which counts the number
of spans in Q that are (1) converted to formal
triples, whose predicates are not Null, and
(2) covered by complete dependency subtrees
at the same time. The underlying intuition
is that, dependency subtrees of Q should be
treated as units for question translation.
• hsyntax constraint(·), which counts the num-
ber of triples in D that are converted from
sub-questions generated by the question de-
composition component.
• htriple(·), which counts the number of triples
in D, whose predicates are not Null.
• htripleweight(·), which sums the scores of all
triples {ti} in D as EtiED ti.score.
• hQPcount(·), which counts the number of
triples in D that are generated by QP-based
question translation method.
• hREcount(·), which counts the number of
triples in D that are generated by RE-based
question translation method.
• hstaticranksbj(·), which sums the static rank
scores of all subject entities in D’s triple set
as EtiED ti.esbj.static rank.
• hstaticrankobj(·), which sums the static rank
scores of all object entities in D’s triple set as
E
tiED ti.eobj.static rank.
• hconfidenceobj(·), which sums the confidence
scores of all object entities in D’s triple set as
EtED t.eobj.confidence.
</listItem>
<bodyText confidence="0.9925335">
For each assertion {esbj, p, eobj} stored in KB,
esbj.static rank and eobj.static rank denote the
static rank scores5 for esbj and eobj respectively;
eobj.confidence rank represents the probability
p(eobj|esbj, p). These three scores are used as fea-
tures to rank answers generated in QA procedure.
</bodyText>
<subsectionHeader confidence="0.818103">
2.5 Feature Weight Tuning
</subsectionHeader>
<bodyText confidence="0.995564">
Given a set of question-answer pairs {Qi, Aref
</bodyText>
<equation confidence="0.596753">
i }
</equation>
<bodyText confidence="0.896610133333333">
as the development (dev) set, we use the minimum
error rate training (MERT) (Och, 2003) algorithm
to tune the feature weights AMi in our proposed
model. The training criterion is to seek the feature
weights that can minimize the accumulated errors
of the top-1 answer of questions in the dev set:
Err(Aref
i , ˆAi; AM1 )
N is the number of questions in the dev set, Aref
i
is the correct answers as references of the ith ques-
tion in the dev set, ˆAi is the top-1 answer candi-
date of the ith question in the dev set based on
feature weights AM1 , Err(·) is the error function
which is defined as:
</bodyText>
<equation confidence="0.976996333333333">
Err(Aref
i , ˆAi; AM1 ) = 1 − δ(Aref
i , ˆAi)
</equation>
<bodyText confidence="0.981586">
where δ(Aref
i , ˆAi) is an indicator function which
equals 1 when ˆAi is included in the reference set
Aref
i , and 0 otherwise.
</bodyText>
<sectionHeader confidence="0.954051" genericHeader="method">
3 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.996993">
Our work intersects with two research directions:
semantic parsing and question answering.
Some previous works on semantic pars-
ing (Zelle and Mooney, 1996; Zettlemoyer and
Collins, 2005; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2007; Wong and Mooney,
</bodyText>
<footnote confidence="0.851287">
5The static rank score of an entity represents a general
indicator of the overall quality of that entity.
</footnote>
<equation confidence="0.9961736">
ˆAM1 = argmin
λM
1
N
i=1
</equation>
<page confidence="0.992761">
972
</page>
<bodyText confidence="0.999662131147541">
2007; Kwiatkowski et al., 2010; Kwiatkowski
et al., 2011) require manually annotated logical
forms as supervision, and are hard to extend result-
ing parsers from limited domains, such as GEO,
JOBS and ATIS, to open domains. Recent work-
s (Clarke and Lapata, 2010; Liang et al., 2013)
have alleviated such issues using question-answer
pairs as weak supervision, but still with the short-
coming of using limited lexical triggers to link NL
phrases to predicates. Poon (2013) has proposed
an unsupervised method by adopting grounded-
learning to leverage the database for indirect su-
pervision. But transformation from NL questions
to MRs heavily depends on dependency parsing
results. Besides, the KB used (ATIS) is limited as
well. Kwiatkowski et al. (2013) use Wiktionary
and a limited manual lexicon to map POS tags to
a set of predefined CCG lexical categories, which
aims to reduce the need for learning lexicon from
training data. But it still needs human efforts to de-
fine lexical categories, which usually can not cover
all the semantic phenomena.
Berant et al. (2013) have not only enlarged the
KB used for Freebase (Google, 2013), but also
used a bigger lexicon trigger set extracted by the
open IE method (Lin et al., 2012) for NL phrases
to predicates linking. In comparison, our method
has further advantages: (1) Question answering
and semantic parsing are performed in an join-
t way under a unified framework; (2) A robust
method is proposed to map NL questions to their
formal triple queries, which trades off the mapping
quality by using question patterns and relation ex-
pressions in a cascaded way; and (3) We use do-
main independent feature set which allowing us to
use a relatively small number of question-answer
pairs to tune model parameters.
Fader et al. (2013) map questions to formal
(triple) queries over a large scale, open-domain
database of facts extracted from a raw corpus by
ReVerb (Fader et al., 2011). Compared to their
work, our method gains an improvement in two
aspects: (1) Instead of using facts extracted us-
ing the open IE method, we leverage a large scale,
high-quality knowledge base; (2) We can han-
dle multiple-relation questions, instead of single-
relation queries only, based on our translation
based KB-QA framework.
Espana-Bonet and Comas (2012) have proposed
an MT-based method for factoid QA. But MT in
there work means to translate questions into n-
best translations, which are used for finding simi-
lar sentences in the document collection that prob-
ably contain answers. Echihabi and Marcu (2003)
have developed a noisy-channel model for QA,
which explains how a sentence containing an an-
swer to a given question can be rewritten into that
question through a sequence of stochastic opera-
tions. Compared to the above two MT-motivated
QA work, our method uses MT methodology to
translate questions to answers directly.
</bodyText>
<sectionHeader confidence="0.992476" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998836">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999960166666667">
Following Berant et al. (2013), we use the same
subset of WEBQUESTIONS (3,778 questions) as
the development set (Dev) for weight tuning in
MERT, and use the other part of WEBQUES-
TIONS (2,032 questions) as the test set (Test). Ta-
ble 1 shows the statistics of this data set.
</bodyText>
<table confidence="0.4830285">
Data Set # Questions # Words
WEBQUESTIONS 5,810 6.7
</table>
<tableCaption confidence="0.881887">
Table 1: Statistics of evaluation set. # Questions is
</tableCaption>
<bodyText confidence="0.985410636363636">
the number of questions in a data set, # Words is
the averaged word count of a question.
Table 2 shows the statistics of question patterns
and relation expressions used in our KB-QA sys-
tem. As all question patterns are collected with hu-
man involvement as we discussed in Section 2.3.1,
the quality is very high (98%). We also sample
1,000 instances from the whole relation expression
set and manually label their quality. The accuracy
is around 89%. These two resources can cover 566
head predicates in our KB.
</bodyText>
<table confidence="0.999082">
# Entries Accuracy
Question Patterns 4,764 98%
Relation Expressions 133,445 89%
</table>
<tableCaption confidence="0.9683405">
Table 2: Statistics of question patterns and relation
expressions.
</tableCaption>
<subsectionHeader confidence="0.974174">
4.2 KB-QA Systems
</subsectionHeader>
<bodyText confidence="0.999747857142857">
Since Berant et al. (2013) is one of the latest
work which has reported QA results based on a
large scale, general domain knowledge base (Free-
base), we consider their evaluation result on WE-
BQUESTIONS as our baseline.
Our KB-QA system generates the k-best deriva-
tions for each question span, where k is set to 20.
</bodyText>
<page confidence="0.997845">
973
</page>
<bodyText confidence="0.9999776">
The answers with the highest model scores are
considered the best answers for evaluation. For
evaluation, we follow Berant et al. (2013) to al-
low partial credit and score an answer using the F1
measure, comparing the predicted set of entities to
the annotated set of entities.
One difference between these two systems is the
KB used. Since Freebase is completely contained
by our KB, we disallow all entities which are not
included by Freebase. By doing so, our KB pro-
vides the same knowledge as Freebase does, which
means we do not gain any extra advantage by us-
ing a larger KB. But we still allow ourselves to
use the static rank scores and confidence scores of
entities as features, as we described in Section 2.4.
</bodyText>
<subsectionHeader confidence="0.999531">
4.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.9978555">
We first show the overall evaluation results of our
KB-QA system and compare them with baseline’s
results on Dev and Test. Note that we do not re-
implement the baseline system, but just list their
evaluation numbers reported in the paper. Com-
parison results are listed in Table 3.
</bodyText>
<table confidence="0.999791333333333">
Dev (Accuracy) Test (Accuracy)
Baseline 32.9% 31.4%
Our Method 42.5% (+9.6%) 37.5% (+6.1%)
</table>
<tableCaption confidence="0.999171">
Table 3: Accuracy on evaluation sets. Accuracy is
</tableCaption>
<bodyText confidence="0.9989522">
defined as the number of correctly answered ques-
tions divided by the total number of questions.
Table 3 shows our KB-QA method outperforms
baseline on both Dev and Test. We think the po-
tential reasons of this improvement include:
</bodyText>
<listItem confidence="0.7320566">
• Different methods are used to map NL phras-
es to KB predicates. Berant et al. (2013)
have used a lexicon extracted from a subset
of ReVerb triples (Lin et al., 2012), which
is similar to the relation expression set used
</listItem>
<bodyText confidence="0.977720666666667">
in question translation. But as our relation
expressions are extracted by an in-house ex-
tractor, we can record their extraction-related
statistics as extra information, and use them
as features to measure the mapping quality.
Besides, as a portion of entities in our KB
are extracted from Wiki, we know the one-
to-one correspondence between such entities
and Wiki pages, and use this information in
relation expression extraction for entity dis-
ambiguation. A lower disambiguation error
rate results in better relation expressions.
</bodyText>
<listItem confidence="0.8417735">
• Question patterns are used to map NL context
to KB predicates. Context can be either con-
</listItem>
<bodyText confidence="0.9991918">
tinuous or discontinues phrases. Although
the size of this set is limited, they can actually
cover head questions/queries6 very well. The
underlying intuition of using patterns is that
those high-frequent questions/queries should
and can be treated and solved in the QA task,
by involving human effort at a relative small
price but with very impressive accuracy.
In order to figure out the impacts of question
patterns and relation expressions, another exper-
iment (Table 4) is designed to evaluate their in-
dependent influences, where 2Ponly and R£only
denote the results of KB-QA systems which only
allow question patterns and relation expressions in
question translation respectively.
</bodyText>
<table confidence="0.981287333333333">
Settings Test (Accuracy) Test (Precision)
QPonly 11.8% 97.5%
REonly 32.5% 73.2%
</table>
<tableCaption confidence="0.963871">
Table 4: Impacts of question patterns and relation
</tableCaption>
<bodyText confidence="0.993482538461538">
expressions. Precision is defined as the num-
ber of correctly answered questions divided by the
number of questions with non-empty answers gen-
erated by our KB-QA system.
From Table 4 we can see that the accuracy of
R£only on Test (32.5%) is slightly better than
baseline’s result (31.4%). We think this improve-
ment comes from two aspects: (1) The quality of
the relation expressions is better than the quality
of the lexicon entries used in the baseline; and
(2) We use the extraction-related statistics of re-
lation expressions as features, which brings more
information to measure the confidence of map-
ping between NL phrases and KB predicates, and
makes the model to be more flexible. Meanwhile,
2Ponly perform worse (11.8%) than R£only, due
to coverage issue. But by comparing the precision-
s of these two settings, we find 2Ponly (97.5%)
outperforms R£only (73.2%) significantly, due to
its high quality. This means how to extract high-
quality question patterns is worth to be studied for
the question answering task.
As the performance of our KB-QA system re-
lies heavily on the k-best beam approximation, we
evaluate the impact of the beam size and list the
comparison results in Figure 6. We can see that as
</bodyText>
<footnote confidence="0.9992595">
6Head questions/queries mean the questions/queries with
high frequency and clear patterns.
</footnote>
<page confidence="0.996577">
974
</page>
<bodyText confidence="0.999825625">
we increase k incrementally, the accuracy increase
at the same time. However, a larger k (e.g. 200)
cannot bring significant improvements comparing
to a smaller one (e.g., 20), but using a large k has
a tremendous impact on system efficiency. So we
choose k = 20 as the optimal value in above ex-
periments, which trades off between accuracy and
efficiency.
</bodyText>
<sectionHeader confidence="0.793184" genericHeader="method">
Accuracy on Test
</sectionHeader>
<subsectionHeader confidence="0.779499">
0.45
</subsectionHeader>
<bodyText confidence="0.999912272727273">
Nelson and 2012, all the others are non-content
words.
Besides, ambiguous entries contained in rela-
tion expression sets of different predicates can
bring mapping errors as well. For the follow-
ing question who did Steve Spurrier play pro
football for? as an example, since the unigram
play exists in both Film.Film.Actor and Ameri-
can Football.Player.Current Team ’s relation ex-
pression sets, we made a wrong prediction, which
led to wrong answers.
</bodyText>
<figure confidence="0.9789">
0.4
0.35
0.3
</figure>
<subsectionHeader confidence="0.551963">
4.4.3 Specific Questions
</subsectionHeader>
<bodyText confidence="0.999812333333333">
Sometimes, we cannot give exact answers to
superlative questions like what is the first book
Sherlock Holmes appeared in?. For this example,
we can give all book names where Sherlock
Holmes appeared in, but we cannot rank them
based on their publication date , as we cannot
learn the alignment between the constraint word
first occurred in the question and the predicate
Book.Written Work.Date Of First Publication
from training data automatically. Although we
have followed some work (Poon, 2013; Liang
et al., 2013) to handle such special linguistic
phenomena by defining some specific operators,
it is still hard to cover all unseen cases. We leave
this to future work as an independent topic.
</bodyText>
<figure confidence="0.99948725">
0.25
0.2
0.15
0.1
0.05
0
5 20 50 100 200
Accuracy
</figure>
<figureCaption confidence="0.999978">
Figure 6: Impacts of beam size on accuracy.
</figureCaption>
<bodyText confidence="0.999782222222222">
Actually, the size of our system’s search space
is much smaller than the one of the semantic parser
used in the baseline.This is due to the fact that, if
triple queries generated by the question translation
component cannot derive any answer from KB, we
will discard such triple queries directly during the
QA procedure. We can see that using a small k
can achieve better results than baseline, where the
beam size is set to be 200.
</bodyText>
<subsectionHeader confidence="0.535316">
4.4 Error Analysis
4.4.1 Entity Detection
</subsectionHeader>
<bodyText confidence="0.999976769230769">
Since named entity recognizers trained on Penn
TreeBank usually perform poorly on web queries,
We instead use a simple string-match method to
detect entity mentions in the question using a
cleaned entity dictionary dumped from our KB.
One problem of doing so is the entity detection
issue. For example, in the question who was Es-
ther’s husband ?, we cannot detect Esther as an
entity, as it is just part of an entity name. We need
an ad-hoc entity detection component to handle
such issues, especially for a web scenario, where
users often type entity names in their partial or ab-
breviation forms.
</bodyText>
<subsubsectionHeader confidence="0.736724">
4.4.2 Predicate Mapping
</subsubsectionHeader>
<bodyText confidence="0.999755">
Some questions lack sufficient evidences to detec-
t predicates. where is Byron Nelson 2012 ? is an
example. Since each relation expression must con-
tain at least one content word, this question cannot
match any relation expression. Except for Byron
</bodyText>
<sectionHeader confidence="0.99401" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999979761904762">
This paper presents a translation-based KB-QA
method that integrates semantic parsing and QA
in one unified framework. Comparing to the base-
line system using an independent semantic parser
with state-of-the-art performance, we achieve bet-
ter results on a general domain evaluation set.
Several directions can be further explored in the
future: (i) We plan to design a method that can
extract question patterns automatically, using ex-
isting labeled question patterns and KB as weak
supervision. As we discussed in the experiment
part, how to mine high-quality question patterns is
worth further study for the QA task; (ii) We plan
to integrate an ad-hoc NER into our KB-QA sys-
tem to alleviate the entity detection issue; (iii) In
fact, our proposed QA framework can be general-
ized to other intelligence besides knowledge bases
as well. Any method that can generate answers to
questions, such as the Web-based QA approach,
can be integrated into this framework, by using
them in the question translation component.
</bodyText>
<page confidence="0.998347">
975
</page>
<sectionHeader confidence="0.993833" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997294">
Yoav Artzi and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. In
EMNLP, pages 421–432.
Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettle-
moyer. 2013. Semantic parsing with combinatory
categorial grammars. In ACL (Tutorial Abstracts),
page 2.
Jonathan Berant, Andrew Chou, Roy Frostig, and Per-
cy Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, pages 1533–
1544.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In ACL, pages 423–433.
James Clarke and Mirella Lapata. 2010. Discourse
constraints for document compression. Computa-
tional Linguistics, 36(3):411–441.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In
ACL.
Cristina Espana-Bonet and Pere R. Comas. 2012. Full
machine translation for factoid question answering.
In EACL, pages 20–29.
Anthony Fader, Stephen Soderland, and Oren Etzion-
i. 2011. Identifying relations for open information
extraction. In EMNLP, pages 1535–1545.
Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL, pages 1608–1618.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011.
Bootstrapping the linked data web. In ISWC.
Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012.
Extracting multilingual natural-language patterns
for rdf predicates. In ESWC.
Google. 2013. Freebase. In http://www.freebase.com.
Aditya Kalyanpur, Siddharth Patwardhan, Branimir
Boguraev, Adam Lally, and Jennifer Chu-Carroll.
2012. Fact-based question decomposition in deep-
qa. IBM Journal of Research and Development,
56(3):13.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In EMNLP, pages 1223–1233.
Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Gold-
water, and Mark Steedman. 2011. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In EMNLP, pages 1512–1523.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and
Luke S. Zettlemoyer. 2013. Scaling seman-
tic parsers with on-the-fly ontology matching. In
EMNLP, pages 1545–1556.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In ACL, pages 590–599.
Percy Liang, Michael I. Jordan, and Dan Klein. 2013.
Learning dependency-based compositional seman-
tics. Computational Linguistics, 39(2):389–446.
Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity
linking at web scale. In AKBC-WEKEX, pages 84–
88.
Raymond J. Mooney. 2007. Learning for semantic
parsing. In CICLing, pages 311–324.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, pages 160–
167.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL, pages 933–943.
Yuk Wah Wong and Raymond J. Mooney. 2006.
Learning for semantic parsing with statistical ma-
chine translation. In HLT-NAACL.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In ACL.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In AAAI/IAAI, Vol. 2, pages 1050–
1055.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI, pages 658–666.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed ccg grammars for parsing to
logical form. In EMNLP-CoNLL, pages 678–687.
</reference>
<page confidence="0.998821">
976
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308549">
<title confidence="0.7777655">Knowledge-Based Question Answering as Machine Translation , Ming Tiejun Institute of</title>
<email confidence="0.613303">tjzhao@hit.edu.cn</email>
<abstract confidence="0.99981362962963">A typical knowledge-based question answering (KB-QA) system faces two challenges: one is to transform natural language questions into their meaning representations (MRs); the other is to retrieve answers from knowledge bases (KBs) using generated MRs. Unlike previous methods which treat them in a cascaded manner, we present a translation-based approach to solve these two tasks in one unified framework. We translate questions to answers based on CYK parsing. Answers as translations of the span covered by each CYK cell are obtained by a question translation method, which first generates formal triple queries as MRs for the span based on question patterns and relation expressions, and then retrieves answers from a given KB based on triple queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>421--432</pages>
<contexts>
<context position="1661" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="255" endWordPosition="259">d over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation proc</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi and Luke S. Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In EMNLP, pages 421–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Nicholas FitzGerald</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Semantic parsing with combinatory categorial grammars.</title>
<date>2013</date>
<booktitle>In ACL (Tutorial Abstracts),</booktitle>
<pages>2</pages>
<contexts>
<context position="1813" citStr="Artzi et al., 2013" startWordPosition="283" endWordPosition="286">tate-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of th</context>
</contexts>
<marker>Artzi, FitzGerald, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi, Nicholas FitzGerald, and Luke S. Zettlemoyer. 2013. Semantic parsing with combinatory categorial grammars. In ACL (Tutorial Abstracts), page 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1533--1544</pages>
<contexts>
<context position="1861" citStr="Berant et al., 2013" startWordPosition="291" endWordPosition="294">hieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated</context>
<context position="24299" citStr="Berant et al. (2013)" startWordPosition="4206" endWordPosition="4209">edicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing</context>
<context position="26144" citStr="Berant et al. (2013)" startWordPosition="4512" endWordPosition="4515">sed an MT-based method for factoid QA. But MT in there work means to translate questions into nbest translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 4 Experiment 4.1 Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statistics of this data set. Data Set # Questions # Words WEBQUESTIONS 5,810 6.7 Table 1: Statistics of evaluation set. # Questions is the number of questions in a data set, # Words is the averaged word count of a question. Table 2 shows the statistics of question patterns and relation expressions used in our KB-QA system. As all question patterns are collected with human involvement</context>
<context position="27623" citStr="Berant et al. (2013)" startWordPosition="4765" endWordPosition="4768"> # Entries Accuracy Question Patterns 4,764 98% Relation Expressions 133,445 89% Table 2: Statistics of question patterns and relation expressions. 4.2 KB-QA Systems Since Berant et al. (2013) is one of the latest work which has reported QA results based on a large scale, general domain knowledge base (Freebase), we consider their evaluation result on WEBQUESTIONS as our baseline. Our KB-QA system generates the k-best derivations for each question span, where k is set to 20. 973 The answers with the highest model scores are considered the best answers for evaluation. For evaluation, we follow Berant et al. (2013) to allow partial credit and score an answer using the F1 measure, comparing the predicted set of entities to the annotated set of entities. One difference between these two systems is the KB used. Since Freebase is completely contained by our KB, we disallow all entities which are not included by Freebase. By doing so, our KB provides the same knowledge as Freebase does, which means we do not gain any extra advantage by using a larger KB. But we still allow ourselves to use the static rank scores and confidence scores of entities as features, as we described in Section 2.4. 4.3 Evaluation Res</context>
<context position="28964" citStr="Berant et al. (2013)" startWordPosition="4999" endWordPosition="5002"> Test. Note that we do not reimplement the baseline system, but just list their evaluation numbers reported in the paper. Comparison results are listed in Table 3. Dev (Accuracy) Test (Accuracy) Baseline 32.9% 31.4% Our Method 42.5% (+9.6%) 37.5% (+6.1%) Table 3: Accuracy on evaluation sets. Accuracy is defined as the number of correctly answered questions divided by the total number of questions. Table 3 shows our KB-QA method outperforms baseline on both Dev and Test. We think the potential reasons of this improvement include: • Different methods are used to map NL phrases to KB predicates. Berant et al. (2013) have used a lexicon extracted from a subset of ReVerb triples (Lin et al., 2012), which is similar to the relation expression set used in question translation. But as our relation expressions are extracted by an in-house extractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality. Besides, as a portion of entities in our KB are extracted from Wiki, we know the oneto-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity disambiguation. A lower </context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP, pages 1533– 1544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>423--433</pages>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In ACL, pages 423–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Discourse constraints for document compression.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="23491" citStr="Clarke and Lapata, 2010" startWordPosition="4075" endWordPosition="4078">esearch directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, wh</context>
</contexts>
<marker>Clarke, Lapata, 2010</marker>
<rawString>James Clarke and Mirella Lapata. 2010. Discourse constraints for document compression. Computational Linguistics, 36(3):411–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25766" citStr="Echihabi and Marcu (2003)" startWordPosition="4450" endWordPosition="4453">r et al., 2011). Compared to their work, our method gains an improvement in two aspects: (1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of singlerelation queries only, based on our translation based KB-QA framework. Espana-Bonet and Comas (2012) have proposed an MT-based method for factoid QA. But MT in there work means to translate questions into nbest translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 4 Experiment 4.1 Data Sets Following Berant et al. (2013), we use the same subset of WEBQUESTIONS (3,778 questions) as the development set (Dev) for weight tuning in MERT, and use the other part of WEBQUESTIONS (2,032 questions) as the test set (Test). Table 1 shows the statisti</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A noisy-channel approach to question answering. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Espana-Bonet</author>
<author>Pere R Comas</author>
</authors>
<title>Full machine translation for factoid question answering.</title>
<date>2012</date>
<booktitle>In EACL,</booktitle>
<pages>20--29</pages>
<contexts>
<context position="25513" citStr="Espana-Bonet and Comas (2012)" startWordPosition="4408" endWordPosition="4411"> which allowing us to use a relatively small number of question-answer pairs to tune model parameters. Fader et al. (2013) map questions to formal (triple) queries over a large scale, open-domain database of facts extracted from a raw corpus by ReVerb (Fader et al., 2011). Compared to their work, our method gains an improvement in two aspects: (1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of singlerelation queries only, based on our translation based KB-QA framework. Espana-Bonet and Comas (2012) have proposed an MT-based method for factoid QA. But MT in there work means to translate questions into nbest translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. Compared to the above two MT-motivated QA work, our method uses MT methodology to translate questions to answers directly. 4 Experiment 4.1 Data Sets</context>
</contexts>
<marker>Espana-Bonet, Comas, 2012</marker>
<rawString>Cristina Espana-Bonet and Pere R. Comas. 2012. Full machine translation for factoid question answering. In EACL, pages 20–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="25156" citStr="Fader et al., 2011" startWordPosition="4352" endWordPosition="4355">antages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. Fader et al. (2013) map questions to formal (triple) queries over a large scale, open-domain database of facts extracted from a raw corpus by ReVerb (Fader et al., 2011). Compared to their work, our method gains an improvement in two aspects: (1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of singlerelation queries only, based on our translation based KB-QA framework. Espana-Bonet and Comas (2012) have proposed an MT-based method for factoid QA. But MT in there work means to translate questions into nbest translations, which are used for finding similar sentences in the document collection that probably contain answers. Echihabi and Ma</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In EMNLP, pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke S Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-driven learning for open question answering.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1608--1618</pages>
<contexts>
<context position="9176" citStr="Fader et al., 2013" startWordPosition="1604" endWordPosition="1607"> a set of formal triples T. Each triple t E T is in the form of {esbj, p, eobj}, where esbj’s mention3 occurs in Q, p is a predicate that denotes the meaning expressed by the context of esbj in Q, eobj is an answer to Q retrieved from KB using a triple query q = {esbj, p, ?}. Note that if no predicate p or answer eobj can be generated, {Q, Null, Q} will be returned as a special triple, which sets eobj to be Q itself, and p to be Null. This makes sure the un-answerable spans can be passed on to the higher-level operations. Question translation assumes each span Q is a single-relation question (Fader et al., 2013). Such assumption simplifies the efforts of semantic parsing to the minimum question units, while leaving the capability of handling multiple-relation questions (Figure 1 gives one such example) to the outer CYK-parsing based translation procedure. Two question translation methods are presented in the rest of this subsection, which are based on question patterns and relation expressions respectively. 2.3.1 Question Pattern-based Translation A question pattern QP includes a pattern string QPpattern, which is composed of words and a slot 3For simplicity, a cleaned entity dictionary dumped from t</context>
<context position="25006" citStr="Fader et al. (2013)" startWordPosition="4327" endWordPosition="4330">lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. Fader et al. (2013) map questions to formal (triple) queries over a large scale, open-domain database of facts extracted from a raw corpus by ReVerb (Fader et al., 2011). Compared to their work, our method gains an improvement in two aspects: (1) Instead of using facts extracted using the open IE method, we leverage a large scale, high-quality knowledge base; (2) We can handle multiple-relation questions, instead of singlerelation queries only, based on our translation based KB-QA framework. Espana-Bonet and Comas (2012) have proposed an MT-based method for factoid QA. But MT in there work means to translate que</context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke S. Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In ACL, pages 1608–1618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gerber</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
</authors>
<title>Bootstrapping the linked data web.</title>
<date>2011</date>
<booktitle>In ISWC.</booktitle>
<contexts>
<context position="16726" citStr="Gerber and Ngomo, 2011" startWordPosition="2947" endWordPosition="2951"> pages4, each of which contains at least one pair of entities occurring in one assertion. Then, we extract the shortest path between paired entities in the dependency tree of each sentence as an RE candidate for the given predicate. The intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way. Last, all relation expressions extracted are filtered by heuristic rules, i.e., the frequency must be larger than 4, the length must be shorter than 10, and then weighted by the pattern scoring methods proposed in (Gerber and Ngomo, 2011; Gerber and Ngomo, 2012). For each predicate, we only keep the relation expressions whose pattern scores are larger than a pre-defined threshold. Figure 4 gives one relation expression extraction example. The statistics and overall quality of the relation expressions are listed in Section 4.1. 2.3.3 Question Decomposition Sometimes, a question may provide multiple constraints to its answers. movie starred by Tom Hanks in 1994 is one such question. All the films as the answers of this question should satisfy the following two constraints: (1) starred by Tom Hanks; and (2) released in 1994. It </context>
</contexts>
<marker>Gerber, Ngomo, 2011</marker>
<rawString>Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2011. Bootstrapping the linked data web. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gerber</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
</authors>
<title>Extracting multilingual natural-language patterns for rdf predicates.</title>
<date>2012</date>
<booktitle>In ESWC. Google.</booktitle>
<note>Freebase. In http://www.freebase.com.</note>
<contexts>
<context position="16751" citStr="Gerber and Ngomo, 2012" startWordPosition="2952" endWordPosition="2955">ontains at least one pair of entities occurring in one assertion. Then, we extract the shortest path between paired entities in the dependency tree of each sentence as an RE candidate for the given predicate. The intuition is that any sentence containing such entity pairs occur in an assertion is likely to express the predicate of that assertion in some way. Last, all relation expressions extracted are filtered by heuristic rules, i.e., the frequency must be larger than 4, the length must be shorter than 10, and then weighted by the pattern scoring methods proposed in (Gerber and Ngomo, 2011; Gerber and Ngomo, 2012). For each predicate, we only keep the relation expressions whose pattern scores are larger than a pre-defined threshold. Figure 4 gives one relation expression extraction example. The statistics and overall quality of the relation expressions are listed in Section 4.1. 2.3.3 Question Decomposition Sometimes, a question may provide multiple constraints to its answers. movie starred by Tom Hanks in 1994 is one such question. All the films as the answers of this question should satisfy the following two constraints: (1) starred by Tom Hanks; and (2) released in 1994. It is easy to see that such </context>
</contexts>
<marker>Gerber, Ngomo, 2012</marker>
<rawString>Daniel Gerber and Axel-Cyrille Ngonga Ngomo. 2012. Extracting multilingual natural-language patterns for rdf predicates. In ESWC. Google. 2013. Freebase. In http://www.freebase.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditya Kalyanpur</author>
<author>Siddharth Patwardhan</author>
<author>Branimir Boguraev</author>
<author>Adam Lally</author>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>Fact-based question decomposition in deepqa.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>56</volume>
<issue>3</issue>
<contexts>
<context position="18457" citStr="Kalyanpur et al., 2012" startWordPosition="3231" endWordPosition="3234">patterns (Figure 5) are used for question decomposition. If a question matches any one of these patterns, then sub-questions are generated by collecting the paths between no and each ni(i &gt; 0) in the pattern, where each n denotes a complete subtree with a noun, number, or question word as its root node, the symbol ∗ above prep∗ denotes this preposition can be skipped in matching. For the question mentioned at the beginning, its two sub-questions generated are movie starred by Tom Hanks and movie starred in 1994, as its dependency form matches pattern (a). Similar ideas are used in IBM Watson (Kalyanpur et al., 2012) as well. n2 (a) n0 verb verb n1 prep* prep n0 n0 q : &lt;Forrest Gump, Film.Film.Director, ?&gt; KB t : &lt;Forrest Gump, Film.Film.Director, Robert Zemeckis&gt; n2 n1 and (b) verb verb n0 prep* and verb Paired entity of a KB predicate 𝑝=Film.Film.Director {Forrest Gump, Robert Zemeckis} {Titanic, James Cameron} {The Dark Knight Rises, Christopher Nolan} ... prep* prep* ... prep* n1 n2 nk n1 Passage retrieval Robert Zemeckis is the director of Forrest Gump (c) (d) from Wiki pages James Cameron is the moviemaker of Titanic The Dark Knight Rises is directed by Christopher Nolan Relation expression weightin</context>
</contexts>
<marker>Kalyanpur, Patwardhan, Boguraev, Lally, Chu-Carroll, 2012</marker>
<rawString>Aditya Kalyanpur, Siddharth Patwardhan, Branimir Boguraev, Adam Lally, and Jennifer Chu-Carroll. 2012. Fact-based question decomposition in deepqa. IBM Journal of Research and Development, 56(3):13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke S Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="23261" citStr="Kwiatkowski et al., 2010" startWordPosition="4037" endWordPosition="4040">f i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In EMNLP, pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke S Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in ccg grammar induction for semantic parsing. In</title>
<date>2011</date>
<booktitle>EMNLP,</booktitle>
<pages>1512--1523</pages>
<contexts>
<context position="23288" citStr="Kwiatkowski et al., 2011" startWordPosition="4041" endWordPosition="4044">ref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependen</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke S. Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in ccg grammar induction for semantic parsing. In EMNLP, pages 1512–1523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke S Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1545--1556</pages>
<contexts>
<context position="1839" citStr="Kwiatkowski et al., 2013" startWordPosition="287" endWordPosition="290">ntic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which </context>
<context position="23981" citStr="Kwiatkowski et al. (2013)" startWordPosition="4151" endWordPosition="4154">rd to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke S. Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In EMNLP, pages 1545–1556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>590--599</pages>
<contexts>
<context position="1681" citStr="Liang et al., 2011" startWordPosition="260" endWordPosition="263">um error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In ACL, pages 590–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>2</issue>
<contexts>
<context position="23512" citStr="Liang et al., 2013" startWordPosition="4079" endWordPosition="4082">tic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce th</context>
<context position="33223" citStr="Liang et al., 2013" startWordPosition="5685" endWordPosition="5688">n sets, we made a wrong prediction, which led to wrong answers. 0.4 0.35 0.3 4.4.3 Specific Questions Sometimes, we cannot give exact answers to superlative questions like what is the first book Sherlock Holmes appeared in?. For this example, we can give all book names where Sherlock Holmes appeared in, but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book.Written Work.Date Of First Publication from training data automatically. Although we have followed some work (Poon, 2013; Liang et al., 2013) to handle such special linguistic phenomena by defining some specific operators, it is still hard to cover all unseen cases. We leave this to future work as an independent topic. 0.25 0.2 0.15 0.1 0.05 0 5 20 50 100 200 Accuracy Figure 6: Impacts of beam size on accuracy. Actually, the size of our system’s search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during the QA procedure. We ca</context>
</contexts>
<marker>Liang, Jordan, Klein, 2013</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2013. Learning dependency-based compositional semantics. Computational Linguistics, 39(2):389–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Entity linking at web scale.</title>
<date>2012</date>
<booktitle>In AKBC-WEKEX,</booktitle>
<pages>84--88</pages>
<contexts>
<context position="24457" citStr="Lin et al., 2012" startWordPosition="4235" endWordPosition="4238"> NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. Berant et al. (2013) have not only enlarged the KB used for Freebase (Google, 2013), but also used a bigger lexicon trigger set extracted by the open IE method (Lin et al., 2012) for NL phrases to predicates linking. In comparison, our method has further advantages: (1) Question answering and semantic parsing are performed in an joint way under a unified framework; (2) A robust method is proposed to map NL questions to their formal triple queries, which trades off the mapping quality by using question patterns and relation expressions in a cascaded way; and (3) We use domain independent feature set which allowing us to use a relatively small number of question-answer pairs to tune model parameters. Fader et al. (2013) map questions to formal (triple) queries over a la</context>
<context position="29045" citStr="Lin et al., 2012" startWordPosition="5014" endWordPosition="5017">uation numbers reported in the paper. Comparison results are listed in Table 3. Dev (Accuracy) Test (Accuracy) Baseline 32.9% 31.4% Our Method 42.5% (+9.6%) 37.5% (+6.1%) Table 3: Accuracy on evaluation sets. Accuracy is defined as the number of correctly answered questions divided by the total number of questions. Table 3 shows our KB-QA method outperforms baseline on both Dev and Test. We think the potential reasons of this improvement include: • Different methods are used to map NL phrases to KB predicates. Berant et al. (2013) have used a lexicon extracted from a subset of ReVerb triples (Lin et al., 2012), which is similar to the relation expression set used in question translation. But as our relation expressions are extracted by an in-house extractor, we can record their extraction-related statistics as extra information, and use them as features to measure the mapping quality. Besides, as a portion of entities in our KB are extracted from Wiki, we know the oneto-one correspondence between such entities and Wiki pages, and use this information in relation expression extraction for entity disambiguation. A lower disambiguation error rate results in better relation expressions. • Question patt</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. Entity linking at web scale. In AKBC-WEKEX, pages 84– 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing. In CICLing,</title>
<date>2007</date>
<pages>311--324</pages>
<contexts>
<context position="1632" citStr="Mooney, 2007" startWordPosition="253" endWordPosition="254">odel is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the </context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In CICLing, pages 311–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2894" citStr="Och, 2003" startWordPosition="459" endWordPosition="460">sing is used to parse each input question, and answers of the span covered by each CYK cell are considered the translations of that cell; unlike MT, which uses offline-generated translation tables to translate source phrases into target translations, a semantic parsing-based question translation method is used to translate each span into its answers on-the-fly, based on question patterns and relation expressions. The final answers can be obtained from the root cell. Derivations generated during such a translation procedure are modeled by a linear model, and minimum error rate training (MERT) (Och, 2003) is used to tune feature weights based on a set of question-answer pairs. Figure 1 shows an example: the question director of movie starred by Tom Hanks is translated to one of its answers Robert Zemeckis by three main steps: (i) translate director of to director of; (ii) translate movie starred by Tom Hanks to one of its answers Forrest Gump; (iii) translate director of Forrest Gump to a final answer Robert Zemeckis. Note that the updated question covered by Cell[0, 6] is obtained by combining the answers to question spans covered by Cell[0, 1] and Cell[2, 6]. The contributions of this work a</context>
<context position="22121" citStr="Och, 2003" startWordPosition="3826" endWordPosition="3827">tiED ti.eobj.static rank. • hconfidenceobj(·), which sums the confidence scores of all object entities in D’s triple set as EtED t.eobj.confidence. For each assertion {esbj, p, eobj} stored in KB, esbj.static rank and eobj.static rank denote the static rank scores5 for esbj and eobj respectively; eobj.confidence rank represents the probability p(eobj|esbj, p). These three scores are used as features to rank answers generated in QA procedure. 2.5 Feature Weight Tuning Given a set of question-answer pairs {Qi, Aref i } as the development (dev) set, we use the minimum error rate training (MERT) (Och, 2003) algorithm to tune the feature weights AMi in our proposed model. The training criterion is to seek the feature weights that can minimize the accumulated errors of the top-1 answer of questions in the dev set: Err(Aref i , ˆAi; AM1 ) N is the number of questions in the dev set, Aref i is the correct answers as references of the ith question in the dev set, ˆAi is the top-1 answer candidate of the ith question in the dev set based on feature weights AM1 , Err(·) is the error function which is defined as: Err(Aref i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator functio</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
</authors>
<title>Grounded unsupervised semantic parsing.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>933--943</pages>
<contexts>
<context position="1793" citStr="Poon, 2013" startWordPosition="281" endWordPosition="282">em using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT), we treat the QA task as a translation procedure. Like MT, CYK parsing is used to parse each input question, and answers of the span covered by each CYK cell are considered th</context>
<context position="23700" citStr="Poon (2013)" startWordPosition="4110" endWordPosition="4111">ooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to predicates. Poon (2013) has proposed an unsupervised method by adopting groundedlearning to leverage the database for indirect supervision. But transformation from NL questions to MRs heavily depends on dependency parsing results. Besides, the KB used (ATIS) is limited as well. Kwiatkowski et al. (2013) use Wiktionary and a limited manual lexicon to map POS tags to a set of predefined CCG lexical categories, which aims to reduce the need for learning lexicon from training data. But it still needs human efforts to define lexical categories, which usually can not cover all the semantic phenomena. Berant et al. (2013) </context>
<context position="33202" citStr="Poon, 2013" startWordPosition="5683" endWordPosition="5684">on expression sets, we made a wrong prediction, which led to wrong answers. 0.4 0.35 0.3 4.4.3 Specific Questions Sometimes, we cannot give exact answers to superlative questions like what is the first book Sherlock Holmes appeared in?. For this example, we can give all book names where Sherlock Holmes appeared in, but we cannot rank them based on their publication date , as we cannot learn the alignment between the constraint word first occurred in the question and the predicate Book.Written Work.Date Of First Publication from training data automatically. Although we have followed some work (Poon, 2013; Liang et al., 2013) to handle such special linguistic phenomena by defining some specific operators, it is still hard to cover all unseen cases. We leave this to future work as an independent topic. 0.25 0.2 0.15 0.1 0.05 0 5 20 50 100 200 Accuracy Figure 6: Impacts of beam size on accuracy. Actually, the size of our system’s search space is much smaller than the one of the semantic parser used in the baseline.This is due to the fact that, if triple queries generated by the question translation component cannot derive any answer from KB, we will discard such triple queries directly during th</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>Hoifung Poon. 2013. Grounded unsupervised semantic parsing. In ACL, pages 933–943.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="23046" citStr="Wong and Mooney, 2006" startWordPosition="3998" endWordPosition="4001">wers as references of the ith question in the dev set, ˆAi is the top-1 answer candidate of the ith question in the dev set based on feature weights AM1 , Err(·) is the error function which is defined as: Err(Aref i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical </context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<volume>2</volume>
<pages>1050--1055</pages>
<contexts>
<context position="22992" citStr="Zelle and Mooney, 1996" startWordPosition="3990" endWordPosition="3993"> of questions in the dev set, Aref i is the correct answers as references of the ith question in the dev set, ˆAi is the top-1 answer candidate of the ith question in the dev set based on feature weights AM1 , Err(·) is the error function which is defined as: Err(Aref i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, bu</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In AAAI/IAAI, Vol. 2, pages 1050– 1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="1618" citStr="Zettlemoyer and Collins, 2005" startWordPosition="249" endWordPosition="252">e queries generated. A linear model is defined over derivations, and minimum error rate training is used to tune feature weights based on a set of question-answer pairs. Compared to a KB-QA system using a state-of-the-art semantic parser, our method achieves better results. 1 Introduction Knowledge-based question answering (KB-QA) computes answers to natural language (NL) questions based on existing knowledge bases (KBs). Most previous systems tackle this task in a cascaded manner: First, the input question is transformed into its meaning representation (MR) by an independent semantic parser (Zettlemoyer and Collins, 2005; Mooney, 2007; Artzi and Zettlemoyer, 2011; Liang et al., 2011; Cai and Yates, ∗This work was finished while the author was visiting Microsoft Research Asia. 2013; Poon, 2013; Artzi et al., 2013; Kwiatkowski et al., 2013; Berant et al., 2013); Then, the answers are retrieved from existing KBs using generated MRs as queries. Unlike existing KB-QA systems which treat semantic parsing and answer retrieval as two cascaded tasks, this paper presents a unified framework that can integrate semantic parsing into the question answering procedure directly. Borrowing ideas from machine translation (MT),</context>
<context position="23023" citStr="Zettlemoyer and Collins, 2005" startWordPosition="3994" endWordPosition="3997"> set, Aref i is the correct answers as references of the ith question in the dev set, ˆAi is the top-1 answer candidate of the ith question in the dev set based on feature weights AM1 , Err(·) is the error function which is defined as: Err(Aref i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI, pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed ccg grammars for parsing to logical form. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>678--687</pages>
<contexts>
<context position="23077" citStr="Zettlemoyer and Collins, 2007" startWordPosition="4002" endWordPosition="4006">he ith question in the dev set, ˆAi is the top-1 answer candidate of the ith question in the dev set based on feature weights AM1 , Err(·) is the error function which is defined as: Err(Aref i , ˆAi; AM1 ) = 1 − δ(Aref i , ˆAi) where δ(Aref i , ˆAi) is an indicator function which equals 1 when ˆAi is included in the reference set Aref i , and 0 otherwise. 3 Comparison with Previous Work Our work intersects with two research directions: semantic parsing and question answering. Some previous works on semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Zettlemoyer and Collins, 2007; Wong and Mooney, 5The static rank score of an entity represents a general indicator of the overall quality of that entity. ˆAM1 = argmin λM 1 N i=1 972 2007; Kwiatkowski et al., 2010; Kwiatkowski et al., 2011) require manually annotated logical forms as supervision, and are hard to extend resulting parsers from limited domains, such as GEO, JOBS and ATIS, to open domains. Recent works (Clarke and Lapata, 2010; Liang et al., 2013) have alleviated such issues using question-answer pairs as weak supervision, but still with the shortcoming of using limited lexical triggers to link NL phrases to </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2007. Online learning of relaxed ccg grammars for parsing to logical form. In EMNLP-CoNLL, pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>