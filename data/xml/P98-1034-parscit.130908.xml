<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000197">
<title confidence="0.9978995">
Error-Driven Pruning of Treebank Grammars
for Base Noun Phrase Identification
</title>
<author confidence="0.963895">
Claire Cardie and David Pierce
</author>
<affiliation confidence="0.9954235">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.592908">
Ithaca, NY 14853
</address>
<email confidence="0.733282">
cardie, pierceacs.cornell.edu
</email>
<sectionHeader confidence="0.966551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995821875">
Finding simple, non-recursive, base noun phrases is
an important subtask for many natural language
processing applications. While previous empirical
methods for base NP identification have been rather
complex, this paper instead proposes a very simple
algorithm that is tailored to the relative simplicity
of the task. In particular, we present a corpus-based
approach for finding base NPs by matching part-of-
speech tag sequences. The training phase of the al-
gorithm is based on two successful techniques: first
the base NP grammar is read from a &amp;quot;treebank&amp;quot; cor-
pus; then the grammar is improved by selecting rules
with high &amp;quot;benefit&amp;quot; scores. Using this simple algo-
rithm with a naive heuristic for matching rules, we
achieve surprising accuracy in an evaluation on the
Penn Treebank Wall Street Journal.
</bodyText>
<sectionHeader confidence="0.995545" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997392517241379">
Finding base noun phrases is a sensible first step
for many natural language processing (NLP) tasks:
Accurate identification of base noun phrases is ar-
guably the most critical component of any partial
parser; in addition, information retrieval systems
rely on base noun phrases as the main source of
multi-word indexing terms; furthermore, the psy-
cholinguistic studies of Gee and Grosjean (1983) in-
dicate that text chunks like base noun phrases play
an important role in human language processing. In
this work we define base NPs to be simple, nonre-
cursive noun phrases — noun phrases that do not
contain other noun phrase descendants. The brack-
eted portions of Figure 1, for example, show the base
NPs in one sentence from the Penn Treebank Wall
Street Journal (WSJ) corpus (Marcus et al., 1993).
Thus, the string the sunny confines of resort towns
like Boca Raton and Hot Springs is too complex to
be a base NP; instead, it contains four simpler noun
phrases, each of which is considered a base NP: the
sunny confines, resort towns, Boca Raton, and Hot
Springs.
Previous empirical research has addressed the
problem of base NP identification. Several algo-
rithms identify &amp;quot;terminological phrases&amp;quot; — certain
When [it] is [time] for [their biannual powwow] ,
[the nation] &apos;s [manufacturing titans] typically
jet off to [the sunny confines] of [resort towns]
like [Boca Raton] and [Hot Springs].
</bodyText>
<figureCaption confidence="0.994594">
Figure 1: Base NP Examples
</figureCaption>
<bodyText confidence="0.999982583333333">
base noun phrases with initial determiners and mod-
ifiers removed: Justeson &amp; Katz (1995) look for
repeated phrases; Bourigault (1992) uses a hand-
crafted noun phrase grammar in conjunction with
heuristics for finding maximal length noun phrases;
Voutilainen&apos;s NPTool (1993) uses a handcrafted lex-
icon and constraint grammar to find terminological
noun phrases that include phrase-final prepositional
phrases. Church&apos;s PARTS program (1988), on the
other hand, uses a probabilistic model automati-
cally trained on the Brown corpus to locate core
noun phrases as well as to assign parts of speech.
More recently, Ramshaw &amp; Marcus (In press) ap-
ply transformation-based learning (Brill, 1995) to
the problem. Unfortunately, it is difficult to directly
compare approaches. Each method uses a slightly
different definition of base NP. Each is evaluated on
a different corpus. Most approaches have been eval-
uated by hand on a small test set rather than by au-
tomatic comparison to a large test corpus annotated
by an impartial third party. A notable exception is
the Ramshaw &amp; Marcus work, which evaluates their
transformation-based learning approach on a base
NP corpus derived from the Penn Treebank WSJ,
and achieves precision and recall levels of approxi-
mately 93%.
This paper presents a new algorithm for identi-
fying base NPs in an arbitrary text. Like some of
the earlier work on base NP identification, ours is
a trainable, corpus-based algorithm. In contrast to
other corpus-based approaches, however, we hypoth-
esized that the relatively simple nature of base NPs
would permit their accurate identification using cor-
respondingly simple methods. Assume, for example,
that we use the annotated text of Figure 1 as our
training corpus. To identify base NPs in an unseen
</bodyText>
<page confidence="0.997257">
218
</page>
<bodyText confidence="0.999878704545455">
text, we could simply search for all occurrences of the
base NPs seen during training — it, time, their bian-
nual powwow, .. . , Hot Springs — and mark them
as base NPs in the new text. However, this method
would certainly suffer from data sparseness. Instead,
we use a similar approach, but back off from lexical
items to parts of speech: we identify as a base NP
any string having the same part-of-speech tag se-
quence as a base NP from the training corpus. The
training phase of the algorithm employs two previ-
ously successful techniques: like Charniak&apos;s (1996)
statistical parser, our initial base NP grammar is
read from a &amp;quot;treebank&amp;quot; corpus; then the grammar
is improved by selecting rules with high &amp;quot;benefit&amp;quot;
scores. Our benefit measure is identical to that used
in transformation-based learning to select an ordered
set of useful transformations (Brill, 1995).
Using this simple algorithm with a naive heuristic
for matching rules, we achieve surprising accuracy
in an evaluation on two base NP corpora of varying
complexity, both derived from the Penn Treebank
WSJ. The first base NP corpus is that used in the
Ramshaw &amp; Marcus work. The second espouses a
slightly simpler definition of base NP that conforms
to the base NPs used in our Empire sentence ana-
lyzer. These simpler phrases appear to be a good
starting point for partial parsers that purposely de-
lay all complex attachment decisions to later phases
of processing.
Overall results for the approach are promising.
For the Empire corpus, our base NP finder achieves
94% precision and recall; for the Ramshaw &amp; Marcus
corpus, it obtains 91% precision and recall, which is
2% less than the best published results. Ramshaw
&amp; Marcus, however, provide the learning algorithm
with word-level information in addition to the part-
of-speech information used in our base NP finder.
By controlling for this disparity in available knowl-
edge sources, we find that our base NP algorithm
performs comparably, achieving slightly worse preci-
sion (-1.1%) and slightly better recall (+0.2%) than
the Ramshaw &amp; Marcus approach. Moreover, our
approach offers many important advantages that
make it appropriate for many NLP tasks:
</bodyText>
<listItem confidence="0.974923133333333">
• Training is exceedingly simple.
• The base NP bracketer is very fast, operating
in time linear in the length of the text.
• The accuracy of the treebank approach is good
for applications that require or prefer fairly sim-
ple base NPs.
• The learned grammar is easily modified for use
with corpora that differ from the training texts.
Rules can be selectively added to or deleted
from the grammar without worrying about or-
dering effects.
• Finally, our benefit-based training phase offers
a simple, general approach for extracting gram-
mars other than noun phrase grammars from
annotated text.
</listItem>
<bodyText confidence="0.999450681818182">
Note also that the treebank approach to base NP
identification obtains good results in spite of a very
simple algorithm for &amp;quot;parsing&amp;quot; base NPs. This is ex-
tremely encouraging, and our evaluation suggests at
least two areas for immediate improvement. First,
by replacing the naive match heuristic with a proba-
bilistic base NP parser that incorporates lexical pref-
erences, we would expect a nontrivial increase in re-
call and precision. Second, many of the remaining
base NP errors tend to follow simple patterns; these
might be corrected using localized, learnable repair
rules.
The remainder of the paper describes the specifics
of the approach and its evaluation. The next section
presents the training and application phases of the
treebank approach to base NP identification in more
detail. Section 3 describes our general approach for
pruning the base NP grammar as well as two instan-
tiations of that approach. The evaluation and a dis-
cussion of the results appear in Section 4, along with
techniques for reducing training time and an initial
investigation into the use of local repair heuristics.
</bodyText>
<sectionHeader confidence="0.941588" genericHeader="method">
2 The Treebartk Approach
</sectionHeader>
<bodyText confidence="0.995839111111111">
Figure 2 depicts the treebank approach to base NP
identification. For training, the algorithm requires
a corpus that has been annotated with base NPs.
More specifically, we assume that the training corpus
is a sequence of words wi, w2,..., along with a set of
base NP annotations b(i1,j1), ..., where b(,j)
indicates that the NP brackets words i through j:
[Nr, wi, , wj]. The goal of the training phase is to
create a base NP grammar from this training corpus:
</bodyText>
<listItem confidence="0.99135575">
1. Using any available part-of-speech tagger, as-
sign a part-of-speech tag ti to each word wi in
the training corpus.
2. Extract from each base noun phrase bo,j) in the
training corpus its sequence of part-of-speech
tags ti, , tj to form base NP rules, one rule
per base NP.
3. Remove any duplicate rules.
</listItem>
<bodyText confidence="0.909123">
The resulting &amp;quot;grammar&amp;quot; can then be used to iden-
tify base NPs in a novel text.
</bodyText>
<listItem confidence="0.987950333333333">
1. Assign part-of-speech tags t1, t2,... to the input
words wi, w2, • • •
2. Proceed through the tagged text from left
to right, at each point matching the NP
rules against the remaining part-of-speech tags
in the text.
</listItem>
<page confidence="0.991998">
219
</page>
<figure confidence="0.998422818181818">
Training Phase
Application Phase
NP Rules
&lt;PRP&gt;
&lt;NN&gt;
&lt;PRP$ JJ NN&gt;
&lt;DT NN&gt;
&lt;VBG NNS&gt;
&lt;DT JJ NNS&gt;
&lt;NN NNS&gt;
&lt;NNP NNP&gt;
</figure>
<subsectionHeader confidence="0.855118">
Novel Text
</subsectionHeader>
<bodyText confidence="0.99890125">
Not this year. National Association of Manufacturers settled
on the Hoosier capital of Indianapolis for its next meeting.
And the city decided to treat its guests more like royalty or
rock stars than factory owners.
</bodyText>
<subsectionHeader confidence="0.859084">
Tagged Text
</subsectionHeader>
<construct confidence="0.677421285714286">
Not/RB this/DT year/NN J. National/NNP
Association/NNP of/IN Manufacturers/NNP settled/VBD
on/IN the/DT Hoosier/NNP capital/NN of/IN
Indianapolis/NNP for/IN its/PRPS next/JJ meeting/NN J.
And/CC the/DT city/NN decided/VBD to/TO treat/VB
its/PRPS guests/NNS more/JR like/IN royalty/NN or/CC
rock/NN stars/NNS than/IN factory/NN owners/NNS J.
</construct>
<subsectionHeader confidence="0.587761">
NP Bracketed Text
</subsectionHeader>
<bodyText confidence="0.947961333333333">
Not [this year). /National Association) of /Manufacturers!
settled on the Hoosier capital) of [Indianapolis) for I its next
meeting). And the city) decided to treat its guests more
like !royalty) or frock stars/ than [factory owners).
Training Corpus
When [it/ is [time/ for /their biannual powwow) .
the nation] &apos;s [manufacturing titans/ typically jet
off to the sunny confines/ of /resort towns) like
[Boca Raton) and [Hot Springs!.
</bodyText>
<figure confidence="0.948187">
Tagged Text
When/WRB [it/PRP/ is/VBZ )time/NN) for/IN itheir/PRP$
biannual/1J powwow/NN) J. )the/DT nation/NM &apos;s/POS
)manufacturing/VBG titans/NNS] typically/RB jet/VBP
off/RP to/TO )the/DT sunnyal confines/NNS] of/IN
)resort/NN towns/NNS I like/IN )Boca/NNP Raton/NNP)
and/CC Hot/NNP Springs/NNPI .
</figure>
<figureCaption confidence="0.999929">
Figure 2: The Treebank Approach to Base NP Identification
</figureCaption>
<bodyText confidence="0.999359857142857">
3. If there are multiple rules that match beginning
at ti, use the longest matching rule R. Add the
new base noun phrase t(i,i+IRi_i) to the set of
base NPs. Continue matching at ti+tRi.
With the rules stored in an appropriate data struc-
ture, this greedy &amp;quot;parsing&amp;quot; of base NPs is very fast.
In our implementation, for example, we store the
rules in a decision tree, which permits base NP iden-
tification in time linear in the length of the tagged
input text when using the longest match heuristic.
Unfortunately, there is an obvious problem with
the algorithm described above. There will be many
unhelpful rules in the rule set extracted from the
training corpus. These &amp;quot;bad&amp;quot; rules arise from four
sources: bracketing errors in the corpus; tagging er-
rors; unusual or irregular linguistic constructs (such
as parenthetical expressions); and inherent ambigu-
ities in the base NPs — in spite of their simplicity.
For example, the rule (VBG NNS), which was ex-
tracted from manufacturing/VBG titans/NNS in the
example text, is ambiguous, and will cause erroneous
bracketing in sentences such as The execs squeezed
in a few meetings before [boarding/VBG buses/NNS]
again. In order to have a viable mechanism for iden-
tifying base NPs using this algorithm, the grammar
must be improved by removing problematic rules.
The next section presents two such methods for au-
tomatically pruning the base NP grammar.
</bodyText>
<sectionHeader confidence="0.878208" genericHeader="method">
3 Pruning the Base NP Grammar
</sectionHeader>
<bodyText confidence="0.999916666666667">
As described above, our goal is to use the base NP
corpus to extract and select a set of noun phrase
rules that can be used to accurately identify base
NPs in novel text. Our general pruning procedure is
shown in Figure 3. First, we divide the base NP cor-
pus into two parts: a training corpus and a pruning
corpus. The initial base NP grammar is extracted
from the training corpus as described in Section 2.
Next, the pruning corpus is used to evaluate the set
of rules and produce a ranking of the rules in terms
of their utility in identifying base NPs. More specif-
ically, we use the rule set and the longest match
heuristic to find all base NPs in the pruning corpus.
Performance of the rule set is measured in terms of
labeled precision (P):
</bodyText>
<listItem confidence="0.403622666666667">
# of correct proposed NPs
p _
# of proposed NPs
</listItem>
<bodyText confidence="0.99906525">
We then assign to each rule a score that denotes
the &amp;quot;net benefit&amp;quot; achieved by using the rule during
NP parsing of the improvement corpus. The ben-
efit of rule r is given by Br = Cr — Er where Cr
</bodyText>
<page confidence="0.97126">
220
</page>
<figure confidence="0.9921086">
Training
Corpus
Extract Rules
Improved
Rule Set
</figure>
<figureCaption confidence="0.999944">
Figure 3: Pruning the Base NP Grammar
</figureCaption>
<bodyText confidence="0.952294638888889">
is the number of NPs correctly identified by r, and
Er is the number of precision errors for which r is
responsible.1 A rule is considered responsible for an
error if it was the first rule to bracket part of a refer-
ence NP, i.e., an NP in the base NP training corpus.
Thus, rules that form erroneous bracketings are not
penalized if another rule previously bracketed part
of the same reference NP.
For example, suppose the fragment containing
base NPs Boca Raton, Hot Springs, and Palm Beach
is bracketed as shown below.
resort towns like
[NP, Boca/NNP Raton/NNP , Hot/NNP]
[NP, Springs/NNP], and
[NP, Palm/NNP Beach/NNP1
Rule (NNP NNP , NNP) brackets NPi; (NNP)
brackets NP2; and (NNP NNP) brackets NP3. Rule
(NNP NNP , NNP) incorrectly identifies Boca Ra-
ton , Hot as a noun phrase, so its score is —1. Rule
(NNP) incorrectly identifies Springs, but it is not
held responsible for the error because of the previ-
ous error by (NNP NNP , NNP) on the same original
NP Hot Springs: so its score is 0. Finally, rule (NNP
NNP) receives a score of 1 for correctly identifying
Palm Beach as a base NP.
The benefit scores from evaluation on the pruning
corpus are used to rank the rules in the grammar.
With such a ranking, we can improve the rule set
by discarding the worst rules. Thus far, we have
investigated two iterative approaches for discarding
rules, a thresholding approach and an incremental
approach. We describe each, in turn, in the subsec-
tions below.
&apos;This same benefit measure is also used in the R&amp;M study,
but it is used to rank transformations rather than to rank NP
rules.
</bodyText>
<subsectionHeader confidence="0.996854">
3.1 Threshold Pruning
</subsectionHeader>
<bodyText confidence="0.999992888888889">
Given a ranking on the rule set, the threshold algo-
rithm simply discards rules whose score is less than
a predefined threshold R. For all of our experiments,
we set R = 1 to select rules that propose more cor-
rect bracketings than incorrect. The process of eval-
uating, ranking, and discarding rules is repeated un-
til no rules have a score less than R. For our evalua-
tion on the WSJ corpus, this typically requires only
four to five iterations.
</bodyText>
<subsectionHeader confidence="0.996688">
3.2 Incremental Pruning
</subsectionHeader>
<bodyText confidence="0.99998945">
Thresholding provides a very coarse mechanism for
pruning the NP grammar. In particular, because
of interactions between the rules during bracketing,
thresholding discards rules whose score might in-
crease in the absence of other rules that are also be-
ing discarded. Consider, for example, the Boca Ra-
ton fragments given earlier. In the absence of (NNP
NNP , NNP), the rule (NNP NNP) would have re-
ceived a score of three for correctly identifying all
three NPs.
As a result, we explored a more fine-grained
method of discarding rules: Each iteration of incre-
mental pruning discards the N worst rules, rather
than all rules whose rank is less than some thresh-
old. In all of our experiments, we set N = 10. As
with thresholding, the process of evaluating, rank-
ing, and discarding rules is repeated, this time until
precision of the current rule set on the pruning cor-
pus begins to drop. The rule set that maximized
precision becomes the final rule set.
</bodyText>
<subsectionHeader confidence="0.999648">
3.3 Human Review
</subsectionHeader>
<bodyText confidence="0.999799347826087">
In the experiments below, we compare the thresh-
olding and incremental methods for pruning the NP
grammar to a rule set that was pruned by hand.
When the training corpus is large, exhaustive re-
view of the extracted rules is not practical. This
is the case for our initial rule set, culled from the
WSJ corpus, which contains approximately 4500
base NP rules. Rather than identifying and dis-
carding individual problematic rules, our reviewer
identified problematic classes of rules that could be
removed from the grammar automatically. In partic-
ular, the goal of the human reviewer was to discard
rules that introduced ambiguity or corresponded to
overly complex base NPs. Within our partial parsing
framework, these NPs are better identified by more
informed components of the NLP system. Our re-
viewer identified the following classes of rules as pos-
sibly troublesome: rules that contain a preposition,
period, or colon; rules that contain WH tags; rules
that begin/end with a verb or adverb; rules that con-
tain pronouns with any other tags; rules that contain
misplaced commas or quotes; rules that end with
adjectives. Rules covered under any of these classes
</bodyText>
<figure confidence="0.986779">
Initial Rule Set
Pruning
Corpus
F.Valtlate MACS
Ranked Rule Set
Discard Rules
Final Rule Set
</figure>
<page confidence="0.995636">
221
</page>
<bodyText confidence="0.9997375">
were omitted from the human-pruned rule sets used
in the experiments of Section 4.
</bodyText>
<sectionHeader confidence="0.996534" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999951592592593">
To evaluate the treebank approach to base NP iden-
tification, we created two base NP corpora. Each
is derived from the Penn Treebank WSJ. The first
corpus attempts to duplicate the base NPs used the
Ramshaw &amp; Marcus (R&amp;M) study. The second cor-
pus contains slightly less complicated base NPs —
base NPs that are better suited for use with our
sentence analyzer, Empire.&apos; By evaluating on both
corpora, we can measure the effect of noun phrase
complexity on the treebank approach to base NP
identification. In particular, we hypothesize that the
treebank approach will be most appropriate when
the base NPs are sufficiently simple.
For all experiments, we derived the training, prun-
ing, and testing sets from the 25 sections of Wall
Street Journal distributed with the Penn Treebank
II. All experiments employ 5-fold cross validation.
More specifically, in each of five runs, a different fold
is used for testing the final, pruned rule set; three of
the remaining folds comprise the training corpus (to
create the initial rule set); and the final partition is
the pruning corpus (to prune bad rules from the ini-
tial rule set). All results are averages across the five
folds. Performance is measured in terms of precision
and recall. Precision was described earlier — it is a
standard measure of accuracy. Recall, on the other
hand, is an attempt to measure coverage:
</bodyText>
<listItem confidence="0.622958333333333">
# of correct proposed NPs
# of proposed NPs
# of correct proposed NPs
</listItem>
<footnote confidence="0.786575857142857">
2Very briefly, the Empire sentence analyzer relies on par-
tial parsing to find simple constituents like base NPs and
verb groups. Machine learning algorithms then operate on
the output of the partial parser to perform all attachment de-
cisions. The ultimate output of the parser is a semantic case
frame representation of the functional structure of the input
sentence.
</footnote>
<table confidence="0.98273625">
R&amp;M (1998) R&amp;M (1998) Treebank
with without Approach
lexical templates lexical templates
93.1P/93.5R 90.5P/90.7R 89.4P/90.9R
</table>
<tableCaption confidence="0.715970666666667">
Table 2: Comparison of Treebank Approach with
Ramshaw &amp; Marcus (1998) both With and Without
Lexical Templates, on the R&amp;M Corpus
</tableCaption>
<bodyText confidence="0.999764090909091">
Throughout the table, we see the effects of base
NP complexity — the base NPs of the R&amp;M cor-
pus are substantially more difficult for our approach
to identify than the simpler NPs of the Empire cor-
pus. For the R&amp;M corpus, we lag the best pub-
lished results (93.1P/93.5R) by approximately 3%.
This straightforward comparison, however, is not en-
tirely appropriate. Ramshaw &amp; Marcus allow their
learning algorithm to access word-level information
in addition to part-of-speech tags. The treebank ap-
proach, on the other hand, makes use only of part-of-
speech tags. Table 2 compares Ramshaw &amp; Marcus&apos;
(In press) results with and without lexical knowl-
edge. The first column reports their performance
when using lexical templates; the second when lexi-
cal templates are not used; the third again shows the
treebank approach using incremental pruning. The
treebank approach and the R&amp;M approach without
lecial templates are shown to perform comparably
(-1.1P/+0.2R). Lexicalization of our base NP finder
will be addressed in Section 4.1.
Finally, note the relatively small difference be-
tween the threshold and incremental pruning meth-
ods in Table 1. For some applications, this minor
drop in performance may be worth the decrease in
training time. Another effective technique to speed
up training is motivated by Charniak&apos;s (1996) ob-
servation that the benefit of using rules that only
occurred once in training is marginal. By discard-
ing these rules before pruning, we reduce the size of
the initial grammar — and the time for incremental
pruning — by 60%, with a performance drop of only
-0.3P/-0.1R.
</bodyText>
<subsectionHeader confidence="0.980722">
4.1 Errors and Local Repair Heuristics
</subsectionHeader>
<bodyText confidence="0.999993">
It is informative to consider the kinds of errors
made by the treebank approach to bracketing. In
particular, the errors may indicate options for incor-
porating lexical information into the base NP finder.
Given the increases in performance achieved by
Ramshaw &amp; Marcus by including word-level cues, we
would hope to see similar improvements by exploit-
ing lexical information in the treebank approach.
For each corpus we examined the first 100 or so
errors and found that certain linguistic constructs
consistently cause trouble. (In the examples that
follow, the bracketing shown is the error.)
</bodyText>
<equation confidence="0.999494">
P =
R =
</equation>
<bodyText confidence="0.9817768">
# of NPs in the annotated text
Table 1 summarizes the performance of the tree-
bank approach to base NP identification on the
R&amp;M and Empire corpora using the initial and
pruned rule sets. The first column of results shows
the performance of the initial, unpruned base NP
grammar. The next two columns show the perfor-
mance of the automatically pruned rule sets. The
final column indicates the performance of rule sets
that had been pruned using the handcrafted pruning
heuristics. As expected, the initial rule set performs
quite poorly. Both automated approaches provide
significant increases in both recall and precision. In
addition, they outperform the rule set pruned using
handcrafted pruning heuristics.
</bodyText>
<page confidence="0.992789">
222
</page>
<table confidence="0.997888">
Base NP Initial Threshold Incremental Human
Corpus Rule Set Pruning Pruning Review
Empire 23.OP/46.5R 91.2P/93.1R 92.7P/93.7R 90.3P/90.5R
R&amp;M 19.OP/36.1R 87.2P/90.0R 89.4P/90.9R 81.6P/85.0R
</table>
<tableCaption confidence="0.8759175">
Table 1: Evaluation of the Treebank Approach Using the Mitre Part-of-Speech Tagger (P = precision; R =
recall)
</tableCaption>
<table confidence="0.9998155">
Base NP Threshold Threshold Incremental Incremental
Corpus Improvement + Local Repair Improvement + Local Repair
Empire 91.2P/93.1R 92.8P/93.7R 92.7P/93.7R 93.7P/94.0R
R&amp;M 87.2P/90.0R 89.2P/90.6R 89.4P/90.9R 90.7P/91.1R
</table>
<tableCaption confidence="0.999553">
Table 3: Effect of Local Repair Heuristics
</tableCaption>
<listItem confidence="0.96404808">
• Conjunctions. Conjunctions were a major prob-
lem in the R&amp;M corpus. For the Empire
corpus, conjunctions of adjectives proved dif-
ficult: [record/N1V] [third-quarter/JJ and/CC
nine-month/JJ results/NNS].
• Gerunds. Even though the most difficult VBG
constructions such as manufacturing titans were
removed from the Empire corpus, there were
others that the bracketer did not handle, like
[chief] operating [officer]. Like conjunctions,
gerunds posed a major difficulty in the R&amp;M
corpus.
• NPs Containing Punctuation. Predictably, the
bracketer has difficulty with NPs containing pe-
riods, quotation marks, hyphens, and parenthe-
ses.
• Adverbial Noun Phrases. Especially temporal
NPs such as last month in at [83.6%] of [capacity
last month].
• Appositives. These are juxtaposed NPs such as
of [colleague Michael Madden] that the brack-
eter mistakes for a single NP.
• Quantified NPs. NPs that look like PPs are
a problem: at/IN [least/JJS] [the/DT right/JJ
jobs/NNS]; about/IN [25/CD million/CD].
</listItem>
<bodyText confidence="0.994624981132076">
Many errors appear to stem from four underly-
ing causes. First, close to 20% can be attributed
to errors in the Treebank and in the Base NP cor-
pus, bringing the effective performance of the algo-
rithm to 94.2P/95.9R and 91.5P/92.7R for the Em-
pire and R&amp;M corpora, respectively. For example,
neither corpus includes WH-phrases as base NPs.
When the bracketer correctly recognizes these NPs,
they are counted as errors. Part-of-speech tagging
errors are a second cause. Third, many NPs are
missed by the bracketer because it lacks the appro-
priate rule. For example, household products busi-
ness is bracketed as [household/NN products/NNS]
[business/NM. Fourth, idiomatic and specialized ex-
pressions, especially time, date, money, and numeric
phrases, also account for a substantial portion of the
errors.
These last two categories of errors can often be de-
tected because they produce either recognizable pat-
terns or unlikely linguistic constructs. Consecutive
NPs, for example, usually denote bracketing errors,
as in [household/NN products/NNS] [business/NM.
Merging consecutive NPs in the correct contexts
would fix many such errors. Idiomatic and special-
ized expressions might be corrected by similarly local
repair heuristics. Typical examples might include
changing [effective/JJ Monday/NNP] to effective
[Monday]; changing [the/DT balance/NN due/JJ] to
[the balance] due; and changing were/VBP In&apos;t/RB
the/DT only/RB losers/NNS] to were n&apos;t [the only
losers].
Given these observations, we implemented three
local repair heuristics. The first merges consecutive
NPs unless either might be a time expression. The
second identifies two simple date expressions. The
third looks for quantifiers preceding of NP. The first
heuristic, for example, merges [household products]
[business] to form [household products business], but
leaves increased [15 %) [last Friday] untouched. The
second heuristic merges [June 5] , [1995] into [June
5, 1995]; and [June] , [1995] into [June, 1995]. The
third finds examples like some of [the companies] and
produces [some] of [the companies]. These heuristics
represent an initial exploration into the effectiveness
of employing lexical information in a post-processing
phase rather than during grammar induction and
bracketing. While we are investigating the latter
in current work, local repair heuristics have the ad-
vantage of keeping the training and bracketing algo-
rithms both simple and fast.
The effect of these heuristics on recall and preci-
sion is shown in Table 3. We see consistent improve-
ments for both corpora and both pruning methods,
</bodyText>
<page confidence="0.995372">
223
</page>
<bodyText confidence="0.999986230769231">
achieving approximately 94P/R for the Empire cor-
pus and approximately 91P/R for the R&amp;M corpus.
Note that these are the final results reported in the
introduction and conclusion. Although these experi-
ments represent only an initial investigation into the
usefulness of local repair heuristics, we are very en-
couraged by the results. The heuristics uniformly
boost precision without harming recall; they help
the R&amp;M corpus even though they were designed in
response to errors in the Empire corpus. In addi-
tion, these three heuristics alone recover 1/2 to 1/3
of the improvements we can expect to obtain from
lexicalization based on the R&amp;M results.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999877235294118">
This paper presented a new method for identifying
base NPs. Our treebank approach uses the simple
technique of matching part-of-speech tag sequences,
with the intention of capturing the simplicity of the
corresponding syntactic structure. It employs two
existing corpus-based techniques: the initial noun
phrase grammar is extracted directly from an an-
notated corpus; and a benefit score calculated from
errors on an improvement corpus selects the best
subset of rules via a coarse- or fine-grained pruning
algorithm.
The overall results are surprisingly good, espe-
cially considering the simplicity of the method. It
achieves 94% precision and recall on simple base
NPs. It achieves 91% precision and recall on the
more complex NPs of the Rainshaw &amp; Marcus cor-
pus. We believe, however, that the base NP finder
can be improved further. First, the longest-match
heuristic of the noun phrase bracketer could be re-
placed by more sophisticated parsing methods that
account for lexical preferences. Rule application, for
example, could be disambiguated statistically using
distributions induced during training. We are cur-
rently investigating such extensions. One approach
closely related to ours — weighted finite-state trans-
ducers (e.g. (Pereira and Riley, 1997)) — might pro-
vide a principled way to do this. We could then
consider applying our error-driven pruning strategy
to rules encoded as transducers. Second, we have
only recently begun to explore the use of local re-
pair heuristics. While initial results are promising,
the full impact of such heuristics on overall perfor-
mance can be determined only if they are system-
atically learned and tested using available training
data. Future work will concentrate on the corpus-
based acquisition of local repair heuristics.
In conclusion, the treebank approach to base NPs
provides an accurate and fast bracketing method,
running in time linear in the length of the tagged
text. The approach is simple to understand, im-
plement, and train. The learned grammar is easily
modified for use with new corpora, as rules can be
added or deleted with minimal interaction problems.
Finally, the approach provides a general framework
for developing other treebank grammars (e.g., for
subject/verb/object identification) in addition to
these for base NPs.
Acknowledgments. This work was supported in
part by NSF Grants IRI-9624639 and GER-9454149.
We thank Mitre for providing their part-of-speech tag-
ger.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991769047619">
D. Bourigault. 1992. Surface Grammatical Anal-
ysis for the Extraction of Terminological Noun
Phrases. In Proceedings, COLING-92, pages 977-
981.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Process-
ing: A Case Study in Part-of-Speech Tagging.
Computational Linguistics, 21(4):543-565.
E. Charniak. 1996. Treebank Grammars. In Pro-
ceedings of the Thirteenth National Conference on
Artificial Intelligence, pages 1031-1036, Portland,
OR. AAAI Press / MIT Press.
K. Church. 1988. A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text. In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing, pages 136-143. Associ-
ation for Computational Linguistics.
J. P. Gee and F. Grosjean. 1983. Performance struc-
tures: A psycholinguistic and linguistic appraisal.
Cognitive Psychology, 15:411-458.
John S. Justeson and Slava M. Katz. 1995. Techni-
cal Terminology: Some Linguistic Properties and
an Algorithm for Identification in Text. Natural
Language Engineering, 1:9-27.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Lin-
guistics, 19(2):313-330.
Fernando C. N. Pereira and Michael D. Riley. 1997.
Speech Recognition by Composition of Weighted
Finite Automata. In Emmanuel Roche and Yves
Schabes, editors, Finite-State Language Process-
ing. MIT Press.
Lance A. Ramshaw and Mitchell P. Marcus. In
press. Text chunking using transformation-based
learning. In Natural Language Processing Using
Very Large Corpora. Kluwer. Originally appeared
in WVLC95,82-94.
A. Voutilainen. 1993. NPTool, A Detector of En-
glish Noun Phrases. In Proceedings of the Work-
shop on Very Large Corpora, pages 48-57. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.998712">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.473225">
<title confidence="0.998747">Error-Driven Pruning of Treebank Grammars for Base Noun Phrase Identification</title>
<author confidence="0.99973">Claire Cardie</author>
<author confidence="0.99973">David Pierce</author>
<affiliation confidence="0.9998425">Department of Computer Science Cornell University</affiliation>
<address confidence="0.999973">Ithaca, NY 14853</address>
<email confidence="0.999576">cardie,pierceacs.cornell.edu</email>
<abstract confidence="0.999395625">Finding simple, non-recursive, base noun phrases is an important subtask for many natural language processing applications. While previous empirical methods for base NP identification have been rather complex, this paper instead proposes a very simple algorithm that is tailored to the relative simplicity of the task. In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences. The training phase of the algorithm is based on two successful techniques: first the base NP grammar is read from a &amp;quot;treebank&amp;quot; corpus; then the grammar is improved by selecting rules with high &amp;quot;benefit&amp;quot; scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the</abstract>
<note confidence="0.481594">Penn Treebank Wall Street Journal.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases.</title>
<date>1992</date>
<booktitle>In Proceedings, COLING-92,</booktitle>
<pages>977--981</pages>
<contexts>
<context position="2587" citStr="Bourigault (1992)" startWordPosition="407" endWordPosition="408">ur simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs. Previous empirical research has addressed the problem of base NP identification. Several algorithms identify &amp;quot;terminological phrases&amp;quot; — certain When [it] is [time] for [their biannual powwow] , [the nation] &apos;s [manufacturing titans] typically jet off to [the sunny confines] of [resort towns] like [Boca Raton] and [Hot Springs]. Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: Justeson &amp; Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainen&apos;s NPTool (1993) uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases. Church&apos;s PARTS program (1988), on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech. More recently, Ramshaw &amp; Marcus (In press) apply transformation-based learning (Brill, 1995) to the problem. Unfortunately, it is difficult </context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>D. Bourigault. 1992. Surface Grammatical Analysis for the Extraction of Terminological Noun Phrases. In Proceedings, COLING-92, pages 977-981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="3139" citStr="Brill, 1995" startWordPosition="490" endWordPosition="491">n &amp; Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainen&apos;s NPTool (1993) uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases. Church&apos;s PARTS program (1988), on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech. More recently, Ramshaw &amp; Marcus (In press) apply transformation-based learning (Brill, 1995) to the problem. Unfortunately, it is difficult to directly compare approaches. Each method uses a slightly different definition of base NP. Each is evaluated on a different corpus. Most approaches have been evaluated by hand on a small test set rather than by automatic comparison to a large test corpus annotated by an impartial third party. A notable exception is the Ramshaw &amp; Marcus work, which evaluates their transformation-based learning approach on a base NP corpus derived from the Penn Treebank WSJ, and achieves precision and recall levels of approximately 93%. This paper presents a new </context>
<context position="5086" citStr="Brill, 1995" startWordPosition="814" endWordPosition="815">nstead, we use a similar approach, but back off from lexical items to parts of speech: we identify as a base NP any string having the same part-of-speech tag sequence as a base NP from the training corpus. The training phase of the algorithm employs two previously successful techniques: like Charniak&apos;s (1996) statistical parser, our initial base NP grammar is read from a &amp;quot;treebank&amp;quot; corpus; then the grammar is improved by selecting rules with high &amp;quot;benefit&amp;quot; scores. Our benefit measure is identical to that used in transformation-based learning to select an ordered set of useful transformations (Brill, 1995). Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on two base NP corpora of varying complexity, both derived from the Penn Treebank WSJ. The first base NP corpus is that used in the Ramshaw &amp; Marcus work. The second espouses a slightly simpler definition of base NP that conforms to the base NPs used in our Empire sentence analyzer. These simpler phrases appear to be a good starting point for partial parsers that purposely delay all complex attachment decisions to later phases of processing. Overall results for the approach </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Treebank Grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<publisher>AAAI Press / MIT Press.</publisher>
<location>Portland, OR.</location>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Treebank Grammars. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1031-1036, Portland, OR. AAAI Press / MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Church, 1988</marker>
<rawString>K. Church. 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Gee</author>
<author>F Grosjean</author>
</authors>
<title>Performance structures: A psycholinguistic and linguistic appraisal.</title>
<date>1983</date>
<journal>Cognitive Psychology,</journal>
<pages>15--411</pages>
<contexts>
<context position="1428" citStr="Gee and Grosjean (1983)" startWordPosition="212" endWordPosition="215">oved by selecting rules with high &amp;quot;benefit&amp;quot; scores. Using this simple algorithm with a naive heuristic for matching rules, we achieve surprising accuracy in an evaluation on the Penn Treebank Wall Street Journal. 1 Introduction Finding base noun phrases is a sensible first step for many natural language processing (NLP) tasks: Accurate identification of base noun phrases is arguably the most critical component of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of Gee and Grosjean (1983) indicate that text chunks like base noun phrases play an important role in human language processing. In this work we define base NPs to be simple, nonrecursive noun phrases — noun phrases that do not contain other noun phrase descendants. The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a bas</context>
</contexts>
<marker>Gee, Grosjean, 1983</marker>
<rawString>J. P. Gee and F. Grosjean. 1983. Performance structures: A psycholinguistic and linguistic appraisal. Cognitive Psychology, 15:411-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Justeson</author>
<author>Slava M Katz</author>
</authors>
<title>Technical Terminology: Some Linguistic Properties and an Algorithm for Identification in Text. Natural Language Engineering,</title>
<date>1995</date>
<contexts>
<context position="2542" citStr="Justeson &amp; Katz (1995)" startWordPosition="399" endWordPosition="402">o complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs. Previous empirical research has addressed the problem of base NP identification. Several algorithms identify &amp;quot;terminological phrases&amp;quot; — certain When [it] is [time] for [their biannual powwow] , [the nation] &apos;s [manufacturing titans] typically jet off to [the sunny confines] of [resort towns] like [Boca Raton] and [Hot Springs]. Figure 1: Base NP Examples base noun phrases with initial determiners and modifiers removed: Justeson &amp; Katz (1995) look for repeated phrases; Bourigault (1992) uses a handcrafted noun phrase grammar in conjunction with heuristics for finding maximal length noun phrases; Voutilainen&apos;s NPTool (1993) uses a handcrafted lexicon and constraint grammar to find terminological noun phrases that include phrase-final prepositional phrases. Church&apos;s PARTS program (1988), on the other hand, uses a probabilistic model automatically trained on the Brown corpus to locate core noun phrases as well as to assign parts of speech. More recently, Ramshaw &amp; Marcus (In press) apply transformation-based learning (Brill, 1995) to</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>John S. Justeson and Slava M. Katz. 1995. Technical Terminology: Some Linguistic Properties and an Algorithm for Identification in Text. Natural Language Engineering, 1:9-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="1829" citStr="Marcus et al., 1993" startWordPosition="283" endWordPosition="286">component of any partial parser; in addition, information retrieval systems rely on base noun phrases as the main source of multi-word indexing terms; furthermore, the psycholinguistic studies of Gee and Grosjean (1983) indicate that text chunks like base noun phrases play an important role in human language processing. In this work we define base NPs to be simple, nonrecursive noun phrases — noun phrases that do not contain other noun phrase descendants. The bracketed portions of Figure 1, for example, show the base NPs in one sentence from the Penn Treebank Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Thus, the string the sunny confines of resort towns like Boca Raton and Hot Springs is too complex to be a base NP; instead, it contains four simpler noun phrases, each of which is considered a base NP: the sunny confines, resort towns, Boca Raton, and Hot Springs. Previous empirical research has addressed the problem of base NP identification. Several algorithms identify &amp;quot;terminological phrases&amp;quot; — certain When [it] is [time] for [their biannual powwow] , [the nation] &apos;s [manufacturing titans] typically jet off to [the sunny confines] of [resort towns] like [Boca Raton] and [Hot Springs]. Fi</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Michael D Riley</author>
</authors>
<title>Speech Recognition by Composition of Weighted Finite Automata.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="28651" citStr="Pereira and Riley, 1997" startWordPosition="4635" endWordPosition="4638">ision and recall on simple base NPs. It achieves 91% precision and recall on the more complex NPs of the Rainshaw &amp; Marcus corpus. We believe, however, that the base NP finder can be improved further. First, the longest-match heuristic of the noun phrase bracketer could be replaced by more sophisticated parsing methods that account for lexical preferences. Rule application, for example, could be disambiguated statistically using distributions induced during training. We are currently investigating such extensions. One approach closely related to ours — weighted finite-state transducers (e.g. (Pereira and Riley, 1997)) — might provide a principled way to do this. We could then consider applying our error-driven pruning strategy to rules encoded as transducers. Second, we have only recently begun to explore the use of local repair heuristics. While initial results are promising, the full impact of such heuristics on overall performance can be determined only if they are systematically learned and tested using available training data. Future work will concentrate on the corpusbased acquisition of local repair heuristics. In conclusion, the treebank approach to base NPs provides an accurate and fast bracketin</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando C. N. Pereira and Michael D. Riley. 1997. Speech Recognition by Composition of Weighted Finite Automata. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>In press. Text chunking using transformation-based learning.</title>
<booktitle>In Natural Language Processing Using Very Large Corpora. Kluwer. Originally appeared in</booktitle>
<pages>95--82</pages>
<marker>Ramshaw, Marcus, </marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. In press. Text chunking using transformation-based learning. In Natural Language Processing Using Very Large Corpora. Kluwer. Originally appeared in WVLC95,82-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>NPTool, A Detector of English Noun Phrases.</title>
<date>1993</date>
<booktitle>In Proceedings of the Workshop on Very Large Corpora,</booktitle>
<pages>48--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Voutilainen, 1993</marker>
<rawString>A. Voutilainen. 1993. NPTool, A Detector of English Noun Phrases. In Proceedings of the Workshop on Very Large Corpora, pages 48-57. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>