<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.96746">
Transition-based Neural Constituent Parsing
</title>
<author confidence="0.84008">
Taro Watanabe∗ and Eiichiro Sumita
</author>
<affiliation confidence="0.854767">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.815542">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 JAPAN
</address>
<email confidence="0.984328">
tarow@google.com,eiichiro.sumita@nict.go.jp
</email>
<sectionHeader confidence="0.994503" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914">
Constituent parsing is typically modeled
by a chart-based algorithm under prob-
abilistic context-free grammars or by a
transition-based algorithm with rich fea-
tures. Previous models rely heavily on
richer syntactic information through lex-
icalizing rules, splitting categories, or
memorizing long histories. However en-
riched models incur numerous parameters
and sparsity issues, and are insufficient for
capturing various syntactic phenomena.
We propose a neural network structure that
explicitly models the unbounded history of
actions performed on the stack and queue
employed in transition-based parsing, in
addition to the representations of partially
parsed tree structure. Our transition-based
neural constituent parsing achieves perfor-
mance comparable to the state-of-the-art
parsers, demonstrating F1 score of 90.68%
for English and 84.33% for Chinese, with-
out reranking, feature templates or addi-
tional data to train model parameters.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996696611111111">
A popular parsing algorithm is a cubic time chart-
based dynamic programming algorithm that uses
probabilistic context-free grammars (PCFGs).
However, PCFGs learned from treebanks are too
coarse to represent the syntactic structures of texts.
To address this problem, various contexts are in-
corporated into the grammars through lexicaliza-
tion (Collins, 2003; Charniak, 2000) or cate-
gory splitting either manually (Klein and Man-
ning, 2003) or automatically (Matsuzaki et al.,
2005; Petrov et al., 2006). Recently a rich feature
set was introduced to capture the lexical contexts
∗The first author is now affiliated with Google, Japan.
in each span without extra annotations in gram-
mars (Hall et al., 2014).
Alternatively, transition-based algorithms run in
linear time by taking a series of shift-reduce ac-
tions with richer lexicalized features considering
histories; however, the accuracies did not match
with the state-of-the-art methods until recently
(Sagae and Lavie, 2005; Zhang and Clark, 2009).
Zhu et al. (2013) show that the use of better transi-
tion actions considering unaries and a set of non-
local features can compete with the accuracies of
chart-based parsing. The features employed in a
transition-based algorithm usually require part of
speech (POS) annotation in the input, but the de-
layed feature technique allows joint POS inference
(Wang and Xue, 2014).
In both frameworks, the richer models require
that more parameters be estimated during train-
ing which can easily result in the data sparseness
problems. Furthermore, the enriched models are
still insufficient to capture various syntactic rela-
tions in texts due to the limited contexts repre-
sented in latent annotations or non-local features.
Recently Socher et al. (2013) introduced composi-
tional vector grammar (CVG) to address the above
limitations. However, they employ reranking over
a forest generated by a baseline parser for efficient
search, because CVG is built on cubic time chart-
based parsing.
In this paper, we propose a neural network-
based parser — transition-based neural con-
stituent parsing (TNCP) — which can guarantee
efficient search naturally. TNCP explicitly models
the actions performed on the stack and queue em-
ployed in transition-based parsing. More specif-
ically, the queue is modeled by recurrent neural
network (RNN) or Elman network (Elman, 1990)
in backward direction (Henderson, 2004). The
stack structure is also modeled similarly to RNNs,
and its top item is updated using the previously
constructed hidden representations saved in the
</bodyText>
<page confidence="0.968006">
1169
</page>
<note confidence="0.977450333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999900578947368">
stack. The representations from both the stack and
queue are combined with the representations prop-
agated from the partially parsed tree structure in-
spired by the recursive neural networks of CVGs.
Parameters are estimated efficiently by a variant
of max-violation (Huang et al., 2012) which con-
siders the worst mistakes found during search and
updates parameters based on the expected mistake.
Under similar settings, TCNP performs compa-
rably to state-of-the-art parsers. Experimental re-
sults obtained using the Wall Street Journal corpus
of the English Penn Treebank achieved a labeled
F1 score of 90.68%, and the result for the Penn
Chinese Treebank was 84.33%. Our parser per-
forms no reranking with computationally expen-
sive models, employs no templates for feature en-
gineering, and requires no additional monolingual
data for reliable parameter estimation. The source
code and models will be made public1.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906620689655">
Our study is largely inspired by recursive neural
networks for parsing, first pioneered by Costa et
al. (2003), in which parsing is treated as a ranking
problem of finding phrasal attachment. Such net-
work structures have been used successfully as a
reranker for k-best parses from a baseline parser
(Menchetti et al., 2005) or parse forests (Socher et
al., 2013), and have achieved gains on large data.
Stenetorp (2013) showed that the recursive neu-
ral networks are comparable to the state-of-the-art
system with a rich feature set under dependency
parsing. Our model is not a reranking model, but
a discriminative parsing model, which incorpo-
rates the representations of stacks and queues em-
ployed in the transition-based parsing framework,
in addition to the representations of the tree struc-
tures. The use of representations outside of the
partial parsed trees is very similar to the recently
proposed inside-outside recursive neural networks
(Le and Zuidema, 2014) which can assign proba-
bilities in a top-down manner, in the same way as
PCFGs.
Henderson (2003) was the first to demonstrate
the successful use of neural networks to represent
derivation histories under large-scale parsing ex-
periments. He employed synchrony networks, i.e.,
feed-forward style networks, to assign a probabil-
ity for each step in the left-corner parsing condi-
tioning on all parsing steps. Henderson (2004)
</bodyText>
<footnote confidence="0.936771">
1http://github.com/tarowatanabe/trance
</footnote>
<bodyText confidence="0.999942222222222">
later employed a discriminative model and showed
further gains by conditioning on the representa-
tion of the future input in addition to the history
of parsing steps. Similar feed-forward style net-
works are successfully applied for transition-based
dependency parsing in which limited contexts are
considered in the feature representation (Chen and
Manning, 2014). Our model is very similar in that
the score of each action is computed by condition-
ing on all previous actions and future input in the
queue.
The use of neural networks for transition-based
shift-reduce parsing was first presented by May-
berry and Miikkulainen (1999) in which the stack
representation was treated as a hidden state of an
RNN. In their study, the hidden state is updated
recurrently by either a shift or reduce action, and
its corresponding parse tree is decoded recursively
from the hidden state (Berg, 1992) using recursive
auto-associative memories (Pollack, 1990). We
apply the idea of representing a stack in a contin-
uous vector; however, our method differs in that
it memorizes all hidden states pushed to the stack
and performs push/pop operations. In this man-
ner, we can represent the local contexts saved in
the stack explicitly and use them to construct new
hidden states.
</bodyText>
<sectionHeader confidence="0.990187" genericHeader="method">
3 Transition-based Constituent Parsing
</sectionHeader>
<bodyText confidence="0.998207904761905">
Our transition-based parser is based on a study by
Zhu et al. (2013), which adopts the shift-reduce
parsing of Sagae and Lavie (2005) and Zhang and
Clark (2009). However, our parser differs in that
we do not differentiate left or right head words.
In addition, POS tags are jointly induced during
parsing in the same manner as Wang and Xue
(2014). Given an input sentence w0, · · · , wn−1,
the transition-based parser employs a stack of par-
tially constructed constituent tree structures and a
queue of input words. In each step, a transition
action is applied to a state (i, f, 5), where i is the
next input word position in the queue wz, f is a flag
indicating the completion of parsing, i.e., whether
the ROOT of a constituent tree covering all the
input words is reached, and 5 represents a stack of
tree elements, s0, s1, · · · .
The parser consists of five actions:
shift-X consumes the next input word, wz, from
the queue and pushes a non-terminal symbol
(or a POS label) as a tree of X —* wz.
</bodyText>
<page confidence="0.624298">
1170
</page>
<equation confidence="0.847820888888889">
axiom 0 : (0, false, (eps)) : 0
goal (2 + u)n : (n, true, S) : p
unary-X j + 1 : (i, false, S|X) : p + pun
j : (n, false, S) : p
finish
j + 1 : (n, true, S) : p + pfi
j : (n, true, S) : p
idle
j + 1 : (n, true, S) : p + pid
</equation>
<figureCaption confidence="0.967036">
Figure 1: Deduction system for shift-reduce pars-
ing, where j is a step size and p is a score.
</figureCaption>
<bodyText confidence="0.999638136363637">
reduce-X pops the top two items s0 and s1 out of
the stack and combines them as a partial tree
with the constituent label X as its root, and
with s0 and s1 as right and left antecedents,
respectively (X —* s1s0). The newly created
tree is then pushed into the stack.
unary-X is similar to reduce-X; however, it con-
sumes only the top most item s0 from the
stack and pushes a new tree of X —* s0.
finish indicates the completion of parsing, i.e.,
reaching the ROOT.
idle preserves completion until the goal is
reached.
The whole procedure is summarized as a deduc-
tion system in Figure 1. We employ beam search
which starts from an axiom consisting of a stack
with a special symbol (eps), and ends when we
reach a goal item (Zhang and Clark, 2009). A set
of agenda B = B0, B1, · · · maintains the k-best
states for each step j at Bj, which is first initial-
ized by inserting the axiom in B0. Then, at each
step j = 0, 1, · · · , every state in the agenda Bj is
extended by applying one of the actions and the
new states are inserted into the agenda Bj+1 for
the next step, which retains only the k-best states.
We limit the maximum number of consecutive
unary actions to u (Sagae and Lavie, 2005; Zhang
and Clark, 2009) and the maximum number of
unary actions in a single derivation to uxn. Thus,
the process is repeated until we reach the final step
of (2+u)n, which keeps the completed states. The
idle action is inspired by the padding method of
Zhu et al. (2013), such that the states in an agenda
are comparable in terms of score even if differ-
ences exist in the number of unary actions. Un-
like Zhu et al. (2013) we do not terminate parsing
even if all the states in an agenda are completed
(f = true).
The score of a state is computed by summing
the scores of all the actions leading to the state. In
Figure 1, psh, pre, pun, pfi and pid are the scores
of shift-X, reduce-X, unary-X, finish and idle ac-
tions, respectively, which are computed on the ba-
sis of the history of actions.
</bodyText>
<sectionHeader confidence="0.997881" genericHeader="method">
4 Neural Constituent Parsing
</sectionHeader>
<bodyText confidence="0.998946666666667">
The score of a state is defined formally as the total
score of transition actions, or a (partial) derivation
d = d0, d1, · · · leading to the state as follows:
</bodyText>
<equation confidence="0.994144333333333">
p(dj|dj−1
0 ).
(1)
</equation>
<bodyText confidence="0.9999234">
Note that the score of each action is dependent on
all previous actions. In previous studies, the score
is computed by a linear model, i.e., a weighted
sum of feature values derived from limited histo-
ries, such as those that consider two adjacent con-
stituent trees in a stack (Sagae and Lavie, 2005;
Zhang and Clark, 2009; Zhu et al., 2013). Our
method employs an RNN or Elman network (El-
man, 1990) to represent an unlimited stack and
queue history.
Formally, we use an m-dimensional vector for
each hidden state unless otherwise stated. Here, let
xi E Rm�×1 be an m0-dimensional vector repre-
senting the input word wi and the dimension may
not match with the hidden state size m. qi E Rm×1
denotes the hidden state for the input word wi in a
queue. Following the RNN in backward direction
(Henderson, 2004), the hidden state for each word
wi is computed right-to-left, qn−1 to q0, beginning
from a constant qn:
</bodyText>
<equation confidence="0.812831">
qi = τ (Hquqi+1 + Wquxi + bqu) , (2)
</equation>
<bodyText confidence="0.954389333333333">
where Hqu E Rm×m, Wqu E Rm×m�, bqu E
Rm×1 and τ(x) is hard-tanh applied element-
wise2.
</bodyText>
<equation confidence="0.9974112">
2τ(x) = −1 for x &lt; 1, 1 for x &gt; 1 otherwise x.
j : (i, false, S) : p
shift-X
j + 1 : (i + 1, false, S|X) : p + psh
j : (i, false, S|s1|s0) : p
reduce-X
j + 1 : (i, false, S|X) : p + pre
j : (i, false, S|s0) : p
p(d) = |d|−1
j=0
</equation>
<page confidence="0.80867">
1171
</page>
<figure confidence="0.496828066666667">
next stack queue
2 1 0
hj+1 hj+1 hj+1
Qsh
Hsh
h1j h0 xi xi+1
j Wsh
input
(a) shift-X action
next stack next stack
2 1 0 2 1 0
hj+1 hj+1 hj+1 hj+1 hj+1 hj+1
h3j h2j h1j h0j h2j h1j h0j
current stack current stack
(b) reduce-X action (c) unary-X action
</figure>
<figureCaption confidence="0.7135342">
Figure 2: Example neural network for constituent
parsing. The thick arrows indicate the context of
tree structures, and the gray arrows represent in-
teractions from the stack and queue. The dotted
arrows denote popped states.
</figureCaption>
<bodyText confidence="0.999332333333333">
Shift: Now, let hlj E Rm×1 represent a hidden
state associated with the lth stack item for the jth
action. We define the score of a shift action:
</bodyText>
<equation confidence="0.994236666666667">
h0j+1 = τ (HXshh0j + QXshqi + WXshxi + bX � (3)
sh
ρ(dj = shift-X|dj−1
</equation>
<bodyText confidence="0.9892948">
0 ) = VshX h0j+1 + vXsh (4)
where HXsh E Rm×m, QXshE Rm×m, WshX E
Rm×m&apos; and bXsh E Rm×1. Figure 2(a) shows the
network structure for Equation 3. HXsh represents
an RNN-style architecture that propagates the pre-
vious context in the stack. QXsh can reflect the
queue context qi, or the future input sequence from
wi through wn−1, while WshX directly expresses
the leaf of a tree structure using the shifted input
word representation xi for wi. The hidden state
h0j+1 is used to compute the score of a derivation
ρ(dj|dj−1
0 ) in Equation 4, which is based on the
matrix VshX E R1×m and the bias term vXsh E R.
Note that hlj+1 = hl−1
j for l = 1, 2, · · · because
the stack is updated by the newly created partial
tree label X associated with the new hidden state
h0j+1.
Inspired by CVG (Socher et al., 2013), we dif-
ferentiate the matrices for each non-terminal (or
POS) label X rather than using shared parameters.
However, our model differs in that the parameters
are untied on the basis of the left hand side of a
rule, rather than the right hand side, because our
model assigns a score discriminatively for each ac-
tion with the left hand side label X unlike a gen-
erative model derived from PCFGs.
Reduce: Similarly, the score for a reduce action
is obtained as follows:
</bodyText>
<equation confidence="0.9850122">
h0j = τ (T-Txh2 + Qeg + WeXho:l]+be+-rre ri r)
(5)
ρ(dj = reduce-X|dj−1
0 ) = V X
re h0j+1 + vXre, (6)
</equation>
<bodyText confidence="0.99676468">
where HX re E Rm×m, QXre E Rm×m, WreX E
Rm×2m, bXre E Rm×1, and h[l:l&apos;] denotes the verti-
cal matrix concatenation of hidden states from hl
to hl&apos;.
Note that the reduce-X action pops top two
items in the stack that correspond to the two hid-
den states of h[0:1] jas represented by Figure 2(b).
By pushing a newly created tree with the con-
stituent X, its corresponding hidden state h0j+1 is
pushed to the stack with each remaining hidden
state hlj+1 = hl+1
j for l = 1, 2, · · · . The hid-
den state of the top stack item h0j is a represen-
tation of the right antecedent of a newly created
binary tree with h0j+1 as a root, while the hidden
state of the next top stack item h1j corresponds
to the left antecedent of the binary tree. Thus,
the two hidden states capture the recursive neural
network-like structure (Costa et al., 2003), while
h2j = h1j+1 represents the RNN-like linear history
in the stack.
Unary: In the same manner as the reduce action,
the unary action is defined by simply reducing a
single item from a stack and by pushing a new item
(Figure 2(c)):
</bodyText>
<equation confidence="0.96940725">
o X 1 X X 0 X
ho = τ (Hunhj + Qunqi + Wunhj +bun) (7)
ρ(dj = unary-X|dj−1
0 ) = VunX h0j+1 + vXun, (8)
</equation>
<bodyText confidence="0.996366888888889">
where HXun E Rm×m, QX un E Rm×m, W un X
E
Rm×m and bXun E Rm×1. Note that hlj+1 = hlj
for l = 1, 2, · · · , because only the top item is up-
dated in the stack by creating a partial tree with h0 j
together with the stack history h1j.
In summary, the number of model parameters
for the three actions is 9xm2+mxm0+6xm+3
for each non-terminal label X. The scores for a
</bodyText>
<figure confidence="0.998022416666667">
queue
Qun
Wun
Hun
qi qi+1
Qre
queue
qi qi+1
Wre
Hre
qi qi+1
current stack
</figure>
<page confidence="0.989951">
1172
</page>
<bodyText confidence="0.999878">
finish action and an idle action are defined analo-
gous to the unary-X action with special labels for
X, (finish) and (idle), respectively3.
</bodyText>
<sectionHeader confidence="0.961352" genericHeader="method">
5 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999763636363637">
Let θ = {HXsh, QXsh, } E RM bean M-
dimensional vector of all model parameters. The
parameters are initialized randomly by following
Glorot and Bengio (2010), in which the random
value range is determined by the size of the in-
put/output layers. The bias parameters are initial-
ized to zeros.
We employ a variant of max-violation (Huang
et al., 2012) as our training objective, in which pa-
rameters are updated based on the worst mistake
found during search, rather than the first mistake
as performed in the early update perceptron al-
gorithm (Collins and Roark, 2004). Specifically,
given a training instance (w, y) where w is an in-
put sentence and y is its gold derivation, i.e., a se-
quence of actions representing the gold parse tree
for w, we seek for the step j* where the difference
of the scores is the largest:
10 can be intuitively considered an expected mis-
take suffered at the maximum violated step j*,
which is measured by the Viterbi violation in
Equation 9. Note that if we replace E ˜Bj* [ρθ] with
maxdEBj* ρθ(d) in Equation 10, it is exactly the
same as the max-violation objective (Huang et al.,
2012)5.
To minimize the loss function, we use a di-
agonal version of AdaDec (Senior et al., 2013)
— a variant of diagonal AdaGrad (Duchi et al.,
2011) — under mini-batch settings. Given the
sub-gradient gt E RM of Equation 10 at time t
computed by the back-propagation through struc-
ture (Goller and K¨uchler, 1996), we maintain ad-
ditional parameters Gt E RM:
</bodyText>
<equation confidence="0.974867">
Gt +- γGt−1 + gt O gt, (14)
</equation>
<bodyText confidence="0.99981725">
where O is the Hadamard product (or the element-
wise product). θt−1 is updated using the element
specific learning rate ηt E RM derived from Gt
and a constant η0 &gt; 0:
</bodyText>
<equation confidence="0.9912275">
1
ηt +- η0 (Gt + E)− 2 (15)
S ρθ (yj0) − a B ρθ (d)1 . (9)
θt−1 +- θt−1 − ηt O gt (16)
2
1211θ − θt−211122 + λη�tabs(θ).
j* = arg min
j
θt +- arg min
θ
</equation>
<bodyText confidence="0.992733333333333">
Then, we define the following hinge-loss function:
wherein we consider the subset of sub-derivations
˜Bj* C Bj* consisting of those scored higher than
</bodyText>
<equation confidence="0.985091714285714">
ρθ(yj*
0 ):
˜Bj* = {d E Bj* I ρθ (d) &gt; ρθ(yj0)} (11)
exp(ρθ(d))
pθ(d) = (12)
Ed&apos;E ˜Bj* exp(ρθ(d&apos;))
pθ(d)ρθ(d). (13)
</equation>
<bodyText confidence="0.99143675">
Unlike Huang et al. (2012) and inspired by Tamura
et al. (2014), we consider all incorrect sub-
derivations found in ˜Bj* through the expected
score E ˜Bj* [ρθ]4. The loss function in Equation
</bodyText>
<footnote confidence="0.9229196">
3Since h1j and qn are constants for the finish and idle ac-
tions, we enforce HXun = 0 and QXun = 0 for those special
actions.
4We can use all the sub-derivations in Bj*; however, our
preliminary studies indicated that the use of ˜Bj* was better.
</footnote>
<equation confidence="0.843658">
(17)
</equation>
<bodyText confidence="0.997510681818182">
Compared with AdaGrad, the squared sum of the
sub-gradients decays over time using a constant
0 &lt; γ &lt; 1 in Equation 14. The learning
rate in Equation 15 is computed element-wise and
bounded by a constant E &gt; 0, and if we set E &gt; η20,
it is always decayed6. In our preliminary stud-
ies, AdaGrad eventually becomes very conserva-
tive to update parameters when training longer it-
erations. AdaDec fixes the problem by ignoring
older histories of sub-gradients in G, which is re-
flected in the learning rate η. In each update, we
employ `1 regularization through FOBOS (Duchi
and Singer, 2009) using a hyperparameter λ &gt; 0
to control the fitness in Equation 16 and 17. For
testing, we found that taking the average of the pa-
rameters over period T1 ET θt under training +1
iterations T was very effective as demonstrated by
Hashimoto et al. (2013).
Parameter estimation is performed in parallel
by distributing training instances asynchronously
5Or, setting pθ(d*) = 1 for the Viterbi derivation d* =
arg maxdEB�* ρθ(d) and zero otherwise.
</bodyText>
<footnote confidence="0.5599355">
6Note that AdaGrad is a special case of AdaDec with γ =
1 and E=0.
</footnote>
<equation confidence="0.99323725">
L(w, y; B, θ) = max{0,1 − ρθ(yI0 + E ˜Bj* [ρθ] } ,
(10)
�E ˜Bj* [ρθ] =
dE ˜Bj*
</equation>
<page confidence="0.878873">
1173
</page>
<bodyText confidence="0.999955083333333">
in each shard and by updating locally copied pa-
rameters using the sub-gradients computed from
the distributed mini-batches (Dean et al., 2012).
The sub-gradients are broadcast asynchronously
to other shards to reflect the updates in one shard.
Unlike Dean et al. (2012), we do not keep a cen-
tral storage for model parameters; the replicated
parameters are synchronized in each iteration by
choosing the model parameters from one of the
shards with respect to the minimum of i1 norm7.
Note that we synchronize θ, but G is maintained
as shard local parameters.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.946516">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.999949466666667">
We conducted experiments for transition-based
neural constituent parsing (TNCP) for two lan-
guages — English and Chinese. English data were
derived from the Wall Street Journal (WSJ) of the
Penn Treebank (Marcus et al., 1993), from which
sections 2-21 were used for training, 22 for de-
velopment and 23 for testing. Chinese data were
extracted from the Penn Chinese Treebank (CTB)
(Xue et al., 2005); articles 001-270 and 440-
1151 were used for training, 301-325 for develop-
ment, and 271-300 for testing. Inspired by jack-
knifing (Collins and Koo, 2005), we reassigned
POS tags for training data using the Stanford tag-
ger (Toutanova et al., 2003)8. The treebank trees
were normalized by removing empty nodes and
unary rules with X over X (or X → X), then
binarized in a left-branched manner.
The possible actions taken for our shift-reduce
parsing, e.g., X → w in shift-X, were learned
from the normalized treebank trees. The words
that occurred twice or less were handled differ-
ently in order to consider OOVs for testing: They
were simply mapped to a special token (unk) when
looking up their corresponding word representa-
tion vector. Similarly, when assigning possible
POS tags in shift actions, they fell back to their
corresponding “word signature” in the same man-
ner as the Berkeley parser9. A maximum number
of consecutive unary actions was set to u = 3 for
WSJ and u = 4 for CTB, as determined by the
</bodyText>
<footnote confidence="0.990180285714286">
7We also tried averaging among shards. However we ob-
served no gains likely because we performed averaging for
testing.
8http://nlp.stanford.edu/software/
tagger.shtml
9https://code.google.com/p/
berkeleyparser/
</footnote>
<table confidence="0.999853555555556">
rep. size 32 64 128 256 512 1024
WSJ-32 89.91 90.15 90.48 90.70 90.75 90.87
64 90.37 90.73 90.81 90.62 90.71 91.11
CTB-32 79.25 81.59 82.80 82.68 84.17 85.12
64 84.04 83.29 82.92 85.12 85.24 85.77
WSJ-32 89.03 89.49 89.75 90.45 90.37 90.01
64 89.74 90.16 90.48 90.06 89.91 90.68
CTB-32 75.19 78.29 80.46 81.87 83.16 82.64
64 80.11 81.35 81.67 82.91 83.76 84.33
</table>
<tableCaption confidence="0.999764">
Table 1: Comparison of various state/word rep-
</tableCaption>
<bodyText confidence="0.993213882352941">
resentation dimension size measured by labeled
F1(%). “-32” denotes the hidden state size m =
32. The numbers in bold indicate the best results
for each hidden state dimension.
treebanks.
Parameter estimation was performed on 16
cores of a Xeon E5-2680 2.7GHz CPU. It took
approximately one day for 100 training iterations
with m = 32 and m&apos; = 128 under a mini-
batch size of 4 and a beam size of 32. Dou-
bling either one of m or m&apos; incurred approxi-
mately double training time. We chose the fol-
lowing hyperparameters by tuning toward the de-
velopment data in our preliminary experiments10:
η0 = 10−2, ry = 0.9, 6 = 1. The choice of A from
{10−5,10−6, 10−7} and the number of training it-
erations were very important for different training
objectives and models in order to avoid overfitting.
Thus, they were determined by the performance
on the development data for each different train-
ing objective and/or network configuration, e.g.,
the dimension for a hidden state. The word rep-
resentations were initialized by a tool developed
in-house for an RNN language model (Mikolov et
al., 2010) trained by noise contrastive estimation
(Mnih and Teh, 2012). Note that the word repre-
sentations for initialization were learned from the
given training data, not from additional unanno-
tated data as done by Chen and Manning (2014).
Testing was performed using a beam size of 64
with a Xeon X5550 2.67GHz CPU. All results
were measured by the labeled bracketing metric
PARSEVAL (Black et al., 1991) using EVALB11
after debinarization.
</bodyText>
<subsectionHeader confidence="0.773747">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.993613">
Table 1 shows the impact of dimensions on
the parsing performance. We varied the hid-
</bodyText>
<footnote confidence="0.735395">
10We confirmed that this hyperparameter setting was ap-
propriate for different models experimented in Section 6.2
through our preliminary studies.
11http://nlp.cs.nyu.edu/evalb/
</footnote>
<figure confidence="0.797330285714286">
CTev
test
1174
90.89 91.11
dev
dev
84.94 85.77
90.21 90.68
test
test
82.62 84.33
loss Viterbi expected
WSJ
CTB
WSJ
CTB
model tree +stack +queue
WSJ 77.70 90.54 91.11
CTB 69.74 84.70 85.77
WSJ 76.48 90.00 90.68
CTB 66.03 82.85 84.33
</figure>
<tableCaption confidence="0.94853225">
Table 2: Comparison of network structures mea-
sured by labeled F1(%).
Table 3: Comparison of loss functions measured
by labeled F1(%).
</tableCaption>
<bodyText confidence="0.995699619047619">
den vector size m = 132,641 and the word
representation (embedding) vector size m&apos; =
132, 64,128, 256, 512, 1024112. As can be seen,
the greater word representation dimensions are
generally helpful for both WSJ and CTB on the
closed development data (dev), which may match
with our intuition that the richer syntactic and se-
mantic knowledge representation for each word is
required for parsing. However, overfitting was ob-
served when using a 32-dimension hidden vector
in both tasks, i.e., drops of performance on the
open test data (test) when m&apos; = 1024, probably
caused by the limited generalization capability in
the smaller hidden state size. In the rest of this pa-
per, we show the results with m = 64 and m&apos; =
1024 as determined by the performance on the
development data, wherein we achieved 91.11%
and 85.77% labeled F1 for WSJ and CTB, respec-
tively. The total number of parameters were ap-
proximately 28.3M and 22.0M for WSJ and CTB,
respectively, among which 17.8M and 13.4M were
occupied for word representations, respectively.
Table 2 differentiated the network structure.
The tree model computes the new hidden state
h0j+1 using only the recursively constructed net-
work by ignoring parameters from the stack and
queue, e.g., by enforcing HXsh = 0 and QXsh = 0
in Equation 3, which is essentially similar to the
CVG approach (Socher et al., 2013). Adding the
context from the stack in +stack boosts the per-
formance significantly. Further gains are observed
when the queue context +queue is incorporated in
the model. These results clearly indicate that ex-
plicit representations of the stack and queue are
very important when applying a recursive neural
network model for transition-based parsing.
We then compared the expected mistake with
the Viterbi mistake (Huang et al., 2012) as our
training objective by replacing E ˜B,* [pθ] with
maxd∈B,* pθ(d) in Equation 10. Table 3 shows
that the use of the expected mistake (expected)
as a loss function is significantly better than that
</bodyText>
<footnote confidence="0.40008">
12We experimented larger dimensions in Appendix A.
</footnote>
<figure confidence="0.8211255">
0 10 20 30 40 50 60 70 80 90 100
iterations
</figure>
<figureCaption confidence="0.994714">
Figure 3: Plots for training iterations and labeled
F1(%) on WSJ.
</figureCaption>
<bodyText confidence="0.999872380952381">
of the Viterbi mistake (Viterbi) by considering all
the incorrect sub-derivations at maximum violated
steps during search. Figure 3 and 4 plot the train-
ing curves for WSJ and CTB, respectively. The
plots clearly demonstrate that the use of the ex-
pected mistake is faster in convergence and stabler
in learning when compared with that of the Viterbi
mistake13.
Next, we compare our parser, TNCP, with other
parsers listed in Table 4 for WSJ and Table 5 for
CTB on the test data. The Collins parser (Collins,
1997) and the Berkeley parser (Petrov and Klein,
2007) are chart-based parsers with rich states, ei-
ther through lexicalization or latent annotation.
SSN is a left-corner parser (Henderson, 2004), and
CVG is a compositional vector grammar-based
parser (Socher et al., 2013)14. Both parsers rely on
neural networks to represent rich contexts, similar
to our work; however they differ in that they es-
sentially perform reranking from either the k-best
parses or parse forests15. The word representa-
</bodyText>
<footnote confidence="0.708806111111111">
13The labeled F1 on those plots are slightly different from
EVALB in that all the syntactic labels are considered when
computing bracket matching. Further, the scores on the train-
ing data are approximation since they were obtained as a by-
product of online learning.
14http://nlp.stanford.edu/software/
lex-parser.shtml
15Strictly speaking, SSN can work as a standalone parser;
Table 4 shows the result after reranking (Henderson, 2004).
</footnote>
<figure confidence="0.984592533333333">
100
F1(%) 95
90
85
80
75
70
65
expected (train)
Viterbi (train)
expected (dev)
Viterbi (dev)
1175
0 10 20 30 40 50 60 70 80 90 100
iterations
</figure>
<figureCaption confidence="0.960149">
Figure 4: Plots for training iterations and labeled
F1(%) on CTB.
</figureCaption>
<table confidence="0.9801115">
parser test
Collins (Collins, 1997) 87.8
Berkeley (Petrov and Klein, 2007) 90.1
SSN (Henderson, 2004) 90.1
ZPar (Zhu et al., 2013) 90.4
CVG (Socher et al., 2013) 90.4
Charniak-R (Charniak and Johnson, 2005) 91.0
This work: TNCP 90.7
</table>
<tableCaption confidence="0.8240515">
Table 4: Comparison of different parsers on the
WSJ test data measured by labeled F1(%).
</tableCaption>
<bodyText confidence="0.9966732">
tion in CVG was learned from large monolingual
data (Turian et al., 2010), but our parser learns
word representation from only the provided train-
ing data. Charniak-R is a discriminative rerank-
ing parser with non-local features (Charniak and
Johnson, 2005). ZPar is a transition-based shift-
reduce parser (Zhu et al., 2013)16 that influences
the deduction system in Figure 1, but differs in that
scores are computed by a large number of features
and POS tagging is performed separately. The re-
sults shown in Table 4 and 5 come from the feature
set without extra data, i.e., semi-supervised fea-
tures. Joint is the joint POS tagging and transition-
based parsing with non-local features (Wang and
Xue, 2014). Similar to ZPar, we present the result
without cluster features learned from extra unan-
notated data.
Finally, we measured the speed for parsing by
varying beam size and hidden dimension (Table
6). When testing, we applied a pre-computation
technique for layers involving word representation
vectors (Devlin et al., 2014), i.e., Wqu in Equation
2 and WshX in Equation 3. Thus, the parsing speed
was influenced by only the hidden state size m. It
is clear that the enlarged beam size improves per-
</bodyText>
<footnote confidence="0.84889">
16http://sourceforge.net/projects/zpar/
</footnote>
<table confidence="0.894102">
parser test
ZPar (Zhu et al., 2013) 83.2
Berkeley (Petrov and Klein, 2007) 83.3
Joint (Wang and Xue, 2014) 84.9
This work: TNCP 84.3
</table>
<tableCaption confidence="0.956323">
Table 5: Comparison of different parsers on the
CTB test data measured by labeled F1(%).
</tableCaption>
<table confidence="0.999898">
beam 32 64 128
WSJ-32 15.42/89.95 7.90/90.01 3.97/90.04
64 7.31/90.56 3.56/90.68 1.76/90.73
CTB-32 13.67/82.35 6.95/82.64 3.68/82.84
64 6.15/84.12 3.11/84.33 1.53/83.83
</table>
<tableCaption confidence="0.67477">
Table 6: Comparison of parsing speed by varying
beam size and hidden dimension; each cell shows
the number of sentences per second/labeled F1(%)
measured on the test data.
</tableCaption>
<bodyText confidence="0.9990727">
formance by trading off run time in most cases.
Note that Berkeley, CVG and ZPar took 4.74, 1.54
and 37.92 sentences/sec, respectively, with WSJ.
Although it is more difficult to compare with other
parsers, our parser implemented in C++ is on par
with Java implementations of Berkeley and CVG.
The large run time difference with the C++ imple-
mented ZPar may come from the network compu-
tation and joint POS inference in our model which
impact parsing speed significantly.
</bodyText>
<subsectionHeader confidence="0.990617">
6.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999982357142857">
To assess parser error types, we used the tool pro-
posed by Kummerfeld et al. (2012)17. The average
number of errors per sentence is listed in Table 7
for each error type on the WSJ test data. Gener-
ally, our parser results in errors that are compara-
ble to the state-of-the-art parsers; however, greater
reductions are observed for various attachments
errors. One of the largest gains comes from the
clause attachment, i.e., 0.12 reduction in average
errors from Berkeley and 0.05 from CVG. The av-
erage number of errors is also reduced by 0.09
from Berkeley and 0.06 from CVG for the PP at-
tachment. We also observed large reductions in
coordination and unary rule errors.
</bodyText>
<sectionHeader confidence="0.992157" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9998855">
We have introduced transition-based neural con-
stituent parsing — a neural network architecture
that encodes each state explicitly — as a con-
tinuous vector by considering the recurrent se-
</bodyText>
<footnote confidence="0.529211">
17https://code.google.com/p/
berkeley-parser-analyser/
</footnote>
<figure confidence="0.97827525">
expected (train)
Viterbi (train)
expected (dev)
Viterbi (dev)
F1(%) 100
95
90
85
80
75
70
65
</figure>
<page confidence="0.965908">
1176
</page>
<table confidence="0.999659818181818">
error type Berkeley CVG TNCP
PP Attach 0.82 0.79 0.73
Clause Attach 0.50 0.43 0.38
Diff Label 0.29 0.29 0.29
Mod Attach 0.27 0.27 0.27
NP Attach 0.37 0.31 0.32
Co-ord 0.38 0.32 0.29
1-Word Span 0.28 0.31 0.30
Unary 0.24 0.22 0.18
NP Int 0.18 0.19 0.20
Other 0.41 0.41 0.45
</table>
<tableCaption confidence="0.997754">
Table 7: Comparison of different parsers on the
</tableCaption>
<bodyText confidence="0.99235615">
WSJ test data measured by average number of er-
rors per sentence; the numbers in bold indicate the
least errors in each error type.
quences of the stack and queue in the transition-
based parsing framework in addition to recursively
constructed partial trees. Our parser works in
a standalone fashion without reranking and does
not rely on an external POS tagger or additional
monolingual data for reliable estimates of syntac-
tic and/or semantic representations of words. The
parser achieves performance that is comparable to
state-of-the-art systems.
In the future, we plan to apply our neural net-
work structure to dependency parsing. We are also
interested in using long short-term memory neu-
ral networks (Hochreiter and Schmidhuber, 1997)
to better model the locality of propagated infor-
mation from the stack and queue. The parameter
estimation under semi-supervised setting will be
investigated further.
</bodyText>
<sectionHeader confidence="0.997296" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999811">
We would like to thank Lemao Liu for suggestions
while drafting this paper. We are also grateful for
various comments from anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.998135545454546">
George Berg. 1992. A connectionist parser with recur-
sive sentence structure and lexical disambiguation.
In Proc. of AAAI ’92, pages 32–37.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Phil Harrison, Don Hin-
dle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
Procedure for quantitatively comparing the syntac-
tic coverage of english grammars. In Proc. of the
Workshop on Speech and Natural Language, pages
306–311, Stroudsburg, PA, USA.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL 2005, pages 173–180,
Ann Arbor, Michigan, June.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of NAACL 2000, pages
132–139, Stroudsburg, PA, USA.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proc. of EMNLP 2014, pages 740–750,
Doha, Qatar, October.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–70, March.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proc. of
ACL 2004, pages 111–118, Barcelona, Spain, July.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proc. of ACL ’97,
pages 16–23, Madrid, Spain, July.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Lin-
guistics, 29(4):589–637, December.
Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo,
and Giovanni Soda. 2003. Towards incremental
parsing of natural language using recursive neural
networks. Applied Intelligence, 19(1-2):9–25, May.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,
Matthieu Devin, Mark Mao, Marc’aurelio Ranzato,
Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le,
and Andrew Y. Ng. 2012. Large scale distributed
deep networks. In Advances in Neural Information
Processing Systems 25, pages 1223–1231. Curran
Associates, Inc.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In Proc. of ACL 2014,
pages 1370–1380, Baltimore, Maryland, June.
John Duchi and Yoram Singer. 2009. Efficient online
and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10:2899–
2934, December.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159, July.
Jeffrey L. Elman. 1990. Finding structure in time.
Cognitive Science, 14(2):179–211.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In Proc. of the Thirteenth International
Conference on Artificial Intelligence and Statistics
(AISTATS-10), volume 9, pages 249–256.
</reference>
<page confidence="0.95012">
1177
</page>
<reference confidence="0.998677653846155">
Christoph Goller and Andreas K¨uchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Proc. of
IEEE International Conference on Neural Networks,
1996, volume 1, pages 347–352 vol.1, Jun.
David Hall, Greg Durrett, and Dan Klein. 2014. Less
grammar, more features. In Proc. of ACL 2014,
pages 228–237, Baltimore, Maryland, June.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple cus-
tomization of recursive neural networks for seman-
tic relation classification. In Proc. of EMNLP 2013,
pages 1372–1376, Seattle, Washington, USA, Octo-
ber.
James Henderson. 2003. Inducing history represen-
tations for broad coverage statistical parsing. In
Proc. of HLT-NAACL 2003, pages 24–31, Strouds-
burg, PA, USA.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proc. of ACL
2004, pages 95–102, Barcelona, Spain, July.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural Computation,
9(8):1735–1780, November.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
of NAACL-HLT 2012, pages 142–151, Montr´eal,
Canada, June.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proc. of ACL 2003,
pages 423–430, Sapporo, Japan, July.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of error
types in parser output. In Proc. of EMNLP-CoNLL
2012, pages 1048–1059, Jeju Island, Korea, July.
Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proc. of EMNLP 2014, pages
729–739, Doha, Qatar, October.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330, June.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ofACL 2005, pages 75–82, Ann Arbor, Michi-
gan, June.
Marshall R. Mayberry and Risto Miikkulainen. 1999.
Sardsrn: A neural network shift-reduce parser. In
Proc. of IJCAI ’99, pages 820–827, San Francisco,
CA, USA.
Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and
Massimiliano Pontil. 2005. Wide coverage natural
language processing using kernel methods and neu-
ral networks for structured data. Pattern Recogni-
tion Letters, 26(12):1896–1906, September.
Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan
ˇCernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Proc.
of INTERSPEECH 2010, pages 1045–1048.
Andriy Mnih and Yee W. Teh. 2012. A fast and simple
algorithm for training neural probabilistic language
models. In John Langford and Joelle Pineau, edi-
tors, Proc. of ICML-2012, pages 1751–1758, New
York, NY, USA.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of NAACL-HLT
2007, pages 404–411, Rochester, New York, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL
2006, pages 433–440, Sydney, Australia, July.
Jordan B. Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1-2):77–105,
November.
Kenji Sagae and Alon Lavie. 2005. A classifier-
based parser with linear run-time complexity. In
Proc. of the Ninth International Workshop on Pars-
ing Technology, pages 125–132, Vancouver, British
Columbia, October.
Andrew Senior, Georg Heigold, Marc’Aurelio Ran-
zato, and Ke Yang. 2013. An empirical study of
learning rates in deep neural networks for speech
recognition. In Proc. of ICASSP 2013, pages 6724–
6728, May.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with compo-
sitional vector grammars. In Proc. of ACL 2013,
pages 455–465, Sofia, Bulgaria, August.
Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In Proc.
of Deep Learning Workshop at the 2013 Conference
on Neural Information Processing Systems (NIPS),
Lake Tahoe, Nevada, USA, December.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2014. Recurrent neural networks for word align-
ment model. In Proc. of ACL 2014, pages 1470–
1480, Baltimore, Maryland, June.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
of-speech tagging with a cyclic dependency net-
work. In Proc. of HLT-NAACL 2003, pages 173–
180, Stroudsburg, PA, USA.
</reference>
<page confidence="0.9864">
1178
</page>
<table confidence="0.999070666666667">
rep. size 32 64 128 256 512 1024 2048 4096
WSJ-32 89.91 90.15 90.48 90.70 90.75 90.87 91.11 91.04
64 90.37 90.73 90.81 90.62 90.71 91.11 91.34 91.36
CTB-32 79.25 81.59 82.80 82.68 84.17 85.12 85.61 85.76
64 84.04 83.29 82.92 85.12 85.24 85.77 86.28 86.94
WSJ-32 89.03 89.49 89.75 90.45 90.37 90.01 90.33 90.40
64 89.74 90.16 90.48 90.06 89.91 90.68 91.05 90.94
CTB-32 75.19 78.29 80.46 81.87 83.16 82.64 83.13 83.67
64 80.11 81.35 81.67 82.91 83.76 84.33 83.76 84.38
</table>
<tableCaption confidence="0.759590666666667">
Table 8: Comparison of various state/word representation dimension size measured by labeled F1(%).
“-32” denotes the hidden state size m = 32. The numbers in bold indicate the best results for each hidden
state dimension.
</tableCaption>
<reference confidence="0.998916857142857">
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proc. of
ACL 2010, pages 384–394, Uppsala, Sweden, July.
Zhiguo Wang and Nianwen Xue. 2014. Joint pos tag-
ging and transition-based constituent parsing in chi-
nese with non-local features. In Proc. of ACL 2014,
pages 733–742, Baltimore, Maryland, June.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207–238, June.
Yue Zhang and Stephen Clark. 2009. Transition-based
parsing of the chinese treebank using a global dis-
criminative model. In Proc. of the 11th International
Conference on Parsing Technologies (IWPT’09),
pages 162–171, Paris, France, October.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and accurate shift-
reduce constituent parsing. In Proc. of ACL 2013,
pages 434–443, Sofia, Bulgaria, August.
</reference>
<sectionHeader confidence="0.854006" genericHeader="method">
A Additional Results
</sectionHeader>
<bodyText confidence="0.999849">
We conducted additional experiments by enlarg-
ing the word representation vector size m&apos; in Ta-
ble 8. In general, we observed further gains with
richer word representation, but suffered overfit-
ting effects when setting m&apos; = 4096. The re-
sults with m = 64 and m&apos; = 4096 achieved
the best performance on the development data,
91.36% and 86.94% labeled F1 for WSJ and CTB,
respectively, wherein we observed the accuracies
of 90.94% and 84.38% on the test data, respec-
tively. Note that it took approximately one week
to train the model when m&apos; = 4096 under WSJ,
which was impractical to analyze the results fur-
ther, e.g. comparison with other training objec-
tives.
</bodyText>
<figure confidence="0.646339">
CTev
test
</figure>
<page confidence="0.970471">
1179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.613659">
<title confidence="0.993043">Transition-based Neural Constituent Parsing</title>
<affiliation confidence="0.926452">National Institute of Information and Communications</affiliation>
<address confidence="0.906102">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289</address>
<abstract confidence="0.994246166666667">Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features. Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories. However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena. We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure. Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>George Berg</author>
</authors>
<title>A connectionist parser with recursive sentence structure and lexical disambiguation.</title>
<date>1992</date>
<booktitle>In Proc. of AAAI ’92,</booktitle>
<pages>32--37</pages>
<contexts>
<context position="7261" citStr="Berg, 1992" startWordPosition="1083" endWordPosition="1084">imited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser diffe</context>
</contexts>
<marker>Berg, 1992</marker>
<rawString>George Berg. 1992. A connectionist parser with recursive sentence structure and lexical disambiguation. In Proc. of AAAI ’92, pages 32–37.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ezra Black</author>
<author>Steve Abney</author>
<author>Dan Flickinger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>Phil Harrison</author>
<author>Don Hindle</author>
<author>Robert Ingria</author>
<author>Fred Jelinek</author>
<author>Judith Klavans</author>
<author>Mark Liberman</author>
<author>Mitchell Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>Procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In Proc. of the Workshop on Speech and Natural Language,</booktitle>
<pages>306--311</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="24089" citStr="Black et al., 1991" startWordPosition="4211" endWordPosition="4214">ch different training objective and/or network configuration, e.g., the dimension for a hidden state. The word representations were initialized by a tool developed in-house for an RNN language model (Mikolov et al., 2010) trained by noise contrastive estimation (Mnih and Teh, 2012). Note that the word representations for initialization were learned from the given training data, not from additional unannotated data as done by Chen and Manning (2014). Testing was performed using a beam size of 64 with a Xeon X5550 2.67GHz CPU. All results were measured by the labeled bracketing metric PARSEVAL (Black et al., 1991) using EVALB11 after debinarization. 6.2 Results Table 1 shows the impact of dimensions on the parsing performance. We varied the hid10We confirmed that this hyperparameter setting was appropriate for different models experimented in Section 6.2 through our preliminary studies. 11http://nlp.cs.nyu.edu/evalb/ CTev test 1174 90.89 91.11 dev dev 84.94 85.77 90.21 90.68 test test 82.62 84.33 loss Viterbi expected WSJ CTB WSJ CTB model tree +stack +queue WSJ 77.70 90.54 91.11 CTB 69.74 84.70 85.77 WSJ 76.48 90.00 90.68 CTB 66.03 82.85 84.33 Table 2: Comparison of network structures measured by labe</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra Black, Steve Abney, Dan Flickinger, Claudia Gdaniec, Ralph Grishman, Phil Harrison, Don Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. Procedure for quantitatively comparing the syntactic coverage of english grammars. In Proc. of the Workshop on Speech and Natural Language, pages 306–311, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="28771" citStr="Charniak and Johnson, 2005" startWordPosition="4979" endWordPosition="4982">tained as a byproduct of online learning. 14http://nlp.stanford.edu/software/ lex-parser.shtml 15Strictly speaking, SSN can work as a standalone parser; Table 4 shows the result after reranking (Henderson, 2004). 100 F1(%) 95 90 85 80 75 70 65 expected (train) Viterbi (train) expected (dev) Viterbi (dev) 1175 0 10 20 30 40 50 60 70 80 90 100 iterations Figure 4: Plots for training iterations and labeled F1(%) on CTB. parser test Collins (Collins, 1997) 87.8 Berkeley (Petrov and Klein, 2007) 90.1 SSN (Henderson, 2004) 90.1 ZPar (Zhu et al., 2013) 90.4 CVG (Socher et al., 2013) 90.4 Charniak-R (Charniak and Johnson, 2005) 91.0 This work: TNCP 90.7 Table 4: Comparison of different parsers on the WSJ test data measured by labeled F1(%). tion in CVG was learned from large monolingual data (Turian et al., 2010), but our parser learns word representation from only the provided training data. Charniak-R is a discriminative reranking parser with non-local features (Charniak and Johnson, 2005). ZPar is a transition-based shiftreduce parser (Zhu et al., 2013)16 that influences the deduction system in Figure 1, but differs in that scores are computed by a large number of features and POS tagging is performed separately.</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proc. of ACL 2005, pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>132--139</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1585" citStr="Charniak, 2000" startWordPosition="208" endWordPosition="209">nt parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 1 Introduction A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 20</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proc. of NAACL 2000, pages 132–139, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP 2014,</booktitle>
<pages>740--750</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="6735" citStr="Chen and Manning, 2014" startWordPosition="992" endWordPosition="995">nder large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for transition-based dependency parsing in which limited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the i</context>
<context position="23922" citStr="Chen and Manning (2014)" startWordPosition="4182" endWordPosition="4185">ere very important for different training objectives and models in order to avoid overfitting. Thus, they were determined by the performance on the development data for each different training objective and/or network configuration, e.g., the dimension for a hidden state. The word representations were initialized by a tool developed in-house for an RNN language model (Mikolov et al., 2010) trained by noise contrastive estimation (Mnih and Teh, 2012). Note that the word representations for initialization were learned from the given training data, not from additional unannotated data as done by Chen and Manning (2014). Testing was performed using a beam size of 64 with a Xeon X5550 2.67GHz CPU. All results were measured by the labeled bracketing metric PARSEVAL (Black et al., 1991) using EVALB11 after debinarization. 6.2 Results Table 1 shows the impact of dimensions on the parsing performance. We varied the hid10We confirmed that this hyperparameter setting was appropriate for different models experimented in Section 6.2 through our preliminary studies. 11http://nlp.cs.nyu.edu/evalb/ CTev test 1174 90.89 91.11 dev dev 84.94 85.77 90.21 90.68 test test 82.62 84.33 loss Viterbi expected WSJ CTB WSJ CTB mode</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proc. of EMNLP 2014, pages 740–750, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="21134" citStr="Collins and Koo, 2005" startWordPosition="3713" endWordPosition="3716"> G is maintained as shard local parameters. 6 Experiments 6.1 Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 440- 1151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8. The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token (unk) when looking up their corresponding word representation vector. Similarly, when </context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proc. of ACL 2004,</booktitle>
<pages>111--118</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="16858" citStr="Collins and Roark, 2004" startWordPosition="2939" endWordPosition="2942">, (finish) and (idle), respectively3. 5 Parameter Estimation Let θ = {HXsh, QXsh, } E RM bean Mdimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j* where the difference of the scores is the largest: 10 can be intuitively considered an expected mistake suffered at the maximum violated step j*, which is measured by the Viterbi violation in Equation 9. Note that if we replace E ˜Bj* [ρθ] with maxdEBj* ρθ(d) in Equation 10, it is exactly the same as the max-violation objective (Huang et al., 2012)5. To minimize the loss function, we use a diagona</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In Proc. of ACL 2004, pages 111–118, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of ACL ’97,</booktitle>
<pages>16--23</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="27431" citStr="Collins, 1997" startWordPosition="4768" endWordPosition="4769">30 40 50 60 70 80 90 100 iterations Figure 3: Plots for training iterations and labeled F1(%) on WSJ. of the Viterbi mistake (Viterbi) by considering all the incorrect sub-derivations at maximum violated steps during search. Figure 3 and 4 plot the training curves for WSJ and CTB, respectively. The plots clearly demonstrate that the use of the expected mistake is faster in convergence and stabler in learning when compared with that of the Viterbi mistake13. Next, we compare our parser, TNCP, with other parsers listed in Table 4 for WSJ and Table 5 for CTB on the test data. The Collins parser (Collins, 1997) and the Berkeley parser (Petrov and Klein, 2007) are chart-based parsers with rich states, either through lexicalization or latent annotation. SSN is a left-corner parser (Henderson, 2004), and CVG is a compositional vector grammar-based parser (Socher et al., 2013)14. Both parsers rely on neural networks to represent rich contexts, similar to our work; however they differ in that they essentially perform reranking from either the k-best parses or parse forests15. The word representa13The labeled F1 on those plots are slightly different from EVALB in that all the syntactic labels are consider</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of ACL ’97, pages 16–23, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="1568" citStr="Collins, 2003" startWordPosition="206" endWordPosition="207">eural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 1 Introduction A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sa</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Vincenzo Lombardo</author>
<author>Giovanni Soda</author>
</authors>
<title>Towards incremental parsing of natural language using recursive neural networks.</title>
<date>2003</date>
<journal>Applied Intelligence,</journal>
<pages>19--1</pages>
<contexts>
<context position="5055" citStr="Costa et al. (2003)" startWordPosition="738" endWordPosition="741">, TCNP performs comparably to state-of-the-art parsers. Experimental results obtained using the Wall Street Journal corpus of the English Penn Treebank achieved a labeled F1 score of 90.68%, and the result for the Penn Chinese Treebank was 84.33%. Our parser performs no reranking with computationally expensive models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation. The source code and models will be made public1. 2 Related Work Our study is largely inspired by recursive neural networks for parsing, first pioneered by Costa et al. (2003), in which parsing is treated as a ranking problem of finding phrasal attachment. Such network structures have been used successfully as a reranker for k-best parses from a baseline parser (Menchetti et al., 2005) or parse forests (Socher et al., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the tran</context>
<context position="15350" citStr="Costa et al., 2003" startWordPosition="2650" endWordPosition="2653"> that correspond to the two hidden states of h[0:1] jas represented by Figure 2(b). By pushing a newly created tree with the constituent X, its corresponding hidden state h0j+1 is pushed to the stack with each remaining hidden state hlj+1 = hl+1 j for l = 1, 2, · · · . The hidden state of the top stack item h0j is a representation of the right antecedent of a newly created binary tree with h0j+1 as a root, while the hidden state of the next top stack item h1j corresponds to the left antecedent of the binary tree. Thus, the two hidden states capture the recursive neural network-like structure (Costa et al., 2003), while h2j = h1j+1 represents the RNN-like linear history in the stack. Unary: In the same manner as the reduce action, the unary action is defined by simply reducing a single item from a stack and by pushing a new item (Figure 2(c)): o X 1 X X 0 X ho = τ (Hunhj + Qunqi + Wunhj +bun) (7) ρ(dj = unary-X|dj−1 0 ) = VunX h0j+1 + vXun, (8) where HXun E Rm×m, QX un E Rm×m, W un X E Rm×m and bXun E Rm×1. Note that hlj+1 = hlj for l = 1, 2, · · · , because only the top item is updated in the stack by creating a partial tree with h0 j together with the stack history h1j. In summary, the number of mod</context>
</contexts>
<marker>Costa, Frasconi, Lombardo, Soda, 2003</marker>
<rawString>Fabrizio Costa, Paolo Frasconi, Vincenzo Lombardo, and Giovanni Soda. 2003. Towards incremental parsing of natural language using recursive neural networks. Applied Intelligence, 19(1-2):9–25, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Greg Corrado</author>
<author>Rajat Monga</author>
<author>Kai Chen</author>
<author>Matthieu Devin</author>
<author>Mark Mao</author>
<author>Marc’aurelio Ranzato</author>
<author>Andrew Senior</author>
<author>Paul Tucker</author>
<author>Ke Yang</author>
<author>Quoc V Le</author>
<author>Andrew Y Ng</author>
</authors>
<title>Large scale distributed deep networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1223--1231</pages>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="20139" citStr="Dean et al., 2012" startWordPosition="3549" endWordPosition="3552"> of the parameters over period T1 ET θt under training +1 iterations T was very effective as demonstrated by Hashimoto et al. (2013). Parameter estimation is performed in parallel by distributing training instances asynchronously 5Or, setting pθ(d*) = 1 for the Viterbi derivation d* = arg maxdEB�* ρθ(d) and zero otherwise. 6Note that AdaGrad is a special case of AdaDec with γ = 1 and E=0. L(w, y; B, θ) = max{0,1 − ρθ(yI0 + E ˜Bj* [ρθ] } , (10) �E ˜Bj* [ρθ] = dE ˜Bj* 1173 in each shard and by updating locally copied parameters using the sub-gradients computed from the distributed mini-batches (Dean et al., 2012). The sub-gradients are broadcast asynchronously to other shards to reflect the updates in one shard. Unlike Dean et al. (2012), we do not keep a central storage for model parameters; the replicated parameters are synchronized in each iteration by choosing the model parameters from one of the shards with respect to the minimum of i1 norm7. Note that we synchronize θ, but G is maintained as shard local parameters. 6 Experiments 6.1 Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the </context>
</contexts>
<marker>Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato, Senior, Tucker, Yang, Le, Ng, 2012</marker>
<rawString>Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. 2012. Large scale distributed deep networks. In Advances in Neural Information Processing Systems 25, pages 1223–1231. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL 2014,</booktitle>
<pages>1370--1380</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="29910" citStr="Devlin et al., 2014" startWordPosition="5164" endWordPosition="5167">e computed by a large number of features and POS tagging is performed separately. The results shown in Table 4 and 5 come from the feature set without extra data, i.e., semi-supervised features. Joint is the joint POS tagging and transitionbased parsing with non-local features (Wang and Xue, 2014). Similar to ZPar, we present the result without cluster features learned from extra unannotated data. Finally, we measured the speed for parsing by varying beam size and hidden dimension (Table 6). When testing, we applied a pre-computation technique for layers involving word representation vectors (Devlin et al., 2014), i.e., Wqu in Equation 2 and WshX in Equation 3. Thus, the parsing speed was influenced by only the hidden state size m. It is clear that the enlarged beam size improves per16http://sourceforge.net/projects/zpar/ parser test ZPar (Zhu et al., 2013) 83.2 Berkeley (Petrov and Klein, 2007) 83.3 Joint (Wang and Xue, 2014) 84.9 This work: TNCP 84.3 Table 5: Comparison of different parsers on the CTB test data measured by labeled F1(%). beam 32 64 128 WSJ-32 15.42/89.95 7.90/90.01 3.97/90.04 64 7.31/90.56 3.56/90.68 1.76/90.73 CTB-32 13.67/82.35 6.95/82.64 3.68/82.84 64 6.15/84.12 3.11/84.33 1.53/8</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In Proc. of ACL 2014, pages 1370–1380, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient online and batch learning using forward backward splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>10</volume>
<pages>2934</pages>
<contexts>
<context position="19400" citStr="Duchi and Singer, 2009" startWordPosition="3415" endWordPosition="3418">that the use of ˜Bj* was better. (17) Compared with AdaGrad, the squared sum of the sub-gradients decays over time using a constant 0 &lt; γ &lt; 1 in Equation 14. The learning rate in Equation 15 is computed element-wise and bounded by a constant E &gt; 0, and if we set E &gt; η20, it is always decayed6. In our preliminary studies, AdaGrad eventually becomes very conservative to update parameters when training longer iterations. AdaDec fixes the problem by ignoring older histories of sub-gradients in G, which is reflected in the learning rate η. In each update, we employ `1 regularization through FOBOS (Duchi and Singer, 2009) using a hyperparameter λ &gt; 0 to control the fitness in Equation 16 and 17. For testing, we found that taking the average of the parameters over period T1 ET θt under training +1 iterations T was very effective as demonstrated by Hashimoto et al. (2013). Parameter estimation is performed in parallel by distributing training instances asynchronously 5Or, setting pθ(d*) = 1 for the Viterbi derivation d* = arg maxdEB�* ρθ(d) and zero otherwise. 6Note that AdaGrad is a special case of AdaDec with γ = 1 and E=0. L(w, y; B, θ) = max{0,1 − ρθ(yI0 + E ˜Bj* [ρθ] } , (10) �E ˜Bj* [ρθ] = dE ˜Bj* 1173 in </context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient online and batch learning using forward backward splitting. Journal of Machine Learning Research, 10:2899– 2934, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="17552" citStr="Duchi et al., 2011" startWordPosition="3067" endWordPosition="3070">ce and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j* where the difference of the scores is the largest: 10 can be intuitively considered an expected mistake suffered at the maximum violated step j*, which is measured by the Viterbi violation in Equation 9. Note that if we replace E ˜Bj* [ρθ] with maxdEBj* ρθ(d) in Equation 10, it is exactly the same as the max-violation objective (Huang et al., 2012)5. To minimize the loss function, we use a diagonal version of AdaDec (Senior et al., 2013) — a variant of diagonal AdaGrad (Duchi et al., 2011) — under mini-batch settings. Given the sub-gradient gt E RM of Equation 10 at time t computed by the back-propagation through structure (Goller and K¨uchler, 1996), we maintain additional parameters Gt E RM: Gt +- γGt−1 + gt O gt, (14) where O is the Hadamard product (or the elementwise product). θt−1 is updated using the element specific learning rate ηt E RM derived from Gt and a constant η0 &gt; 0: 1 ηt +- η0 (Gt + E)− 2 (15) S ρθ (yj0) − a B ρθ (d)1 . (9) θt−1 +- θt−1 − ηt O gt (16) 2 1211θ − θt−211122 + λη�tabs(θ). j* = arg min j θt +- arg min θ Then, we define the following hinge-loss func</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="3557" citStr="Elman, 1990" startWordPosition="515" endWordPosition="516">al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 1990) in backward direction (Henderson, 2004). The stack structure is also modeled similarly to RNNs, and its top item is updated using the previously constructed hidden representations saved in the 1169 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics stack. The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree s</context>
<context position="11587" citStr="Elman, 1990" startWordPosition="1916" endWordPosition="1918">ions. 4 Neural Constituent Parsing The score of a state is defined formally as the total score of transition actions, or a (partial) derivation d = d0, d1, · · · leading to the state as follows: p(dj|dj−1 0 ). (1) Note that the score of each action is dependent on all previous actions. In previous studies, the score is computed by a linear model, i.e., a weighted sum of feature values derived from limited histories, such as those that consider two adjacent constituent trees in a stack (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). Our method employs an RNN or Elman network (Elman, 1990) to represent an unlimited stack and queue history. Formally, we use an m-dimensional vector for each hidden state unless otherwise stated. Here, let xi E Rm�×1 be an m0-dimensional vector representing the input word wi and the dimension may not match with the hidden state size m. qi E Rm×1 denotes the hidden state for the input word wi in a queue. Following the RNN in backward direction (Henderson, 2004), the hidden state for each word wi is computed right-to-left, qn−1 to q0, beginning from a constant qn: qi = τ (Hquqi+1 + Wquxi + bqu) , (2) where Hqu E Rm×m, Wqu E Rm×m�, bqu E Rm×1 and τ(x)</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time. Cognitive Science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In Proc. of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10),</booktitle>
<volume>9</volume>
<pages>249--256</pages>
<contexts>
<context position="16450" citStr="Glorot and Bengio (2010)" startWordPosition="2869" endWordPosition="2872">updated in the stack by creating a partial tree with h0 j together with the stack history h1j. In summary, the number of model parameters for the three actions is 9xm2+mxm0+6xm+3 for each non-terminal label X. The scores for a queue Qun Wun Hun qi qi+1 Qre queue qi qi+1 Wre Hre qi qi+1 current stack 1172 finish action and an idle action are defined analogous to the unary-X action with special labels for X, (finish) and (idle), respectively3. 5 Parameter Estimation Let θ = {HXsh, QXsh, } E RM bean Mdimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the </context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proc. of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-10), volume 9, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas K¨uchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Proc. of IEEE International Conference on Neural Networks,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<marker>Goller, K¨uchler, 1996</marker>
<rawString>Christoph Goller and Andreas K¨uchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Proc. of IEEE International Conference on Neural Networks, 1996, volume 1, pages 347–352 vol.1, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hall</author>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Less grammar, more features.</title>
<date>2014</date>
<booktitle>In Proc. of ACL 2014,</booktitle>
<pages>228--237</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1915" citStr="Hall et al., 2014" startWordPosition="261" endWordPosition="264"> that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delaye</context>
</contexts>
<marker>Hall, Durrett, Klein, 2014</marker>
<rawString>David Hall, Greg Durrett, and Dan Klein. 2014. Less grammar, more features. In Proc. of ACL 2014, pages 228–237, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple customization of recursive neural networks for semantic relation classification.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP 2013,</booktitle>
<pages>1372--1376</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="19653" citStr="Hashimoto et al. (2013)" startWordPosition="3462" endWordPosition="3465">if we set E &gt; η20, it is always decayed6. In our preliminary studies, AdaGrad eventually becomes very conservative to update parameters when training longer iterations. AdaDec fixes the problem by ignoring older histories of sub-gradients in G, which is reflected in the learning rate η. In each update, we employ `1 regularization through FOBOS (Duchi and Singer, 2009) using a hyperparameter λ &gt; 0 to control the fitness in Equation 16 and 17. For testing, we found that taking the average of the parameters over period T1 ET θt under training +1 iterations T was very effective as demonstrated by Hashimoto et al. (2013). Parameter estimation is performed in parallel by distributing training instances asynchronously 5Or, setting pθ(d*) = 1 for the Viterbi derivation d* = arg maxdEB�* ρθ(d) and zero otherwise. 6Note that AdaGrad is a special case of AdaDec with γ = 1 and E=0. L(w, y; B, θ) = max{0,1 − ρθ(yI0 + E ˜Bj* [ρθ] } , (10) �E ˜Bj* [ρθ] = dE ˜Bj* 1173 in each shard and by updating locally copied parameters using the sub-gradients computed from the distributed mini-batches (Dean et al., 2012). The sub-gradients are broadcast asynchronously to other shards to reflect the updates in one shard. Unlike Dean </context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization of recursive neural networks for semantic relation classification. In Proc. of EMNLP 2013, pages 1372–1376, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL</booktitle>
<pages>24--31</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6009" citStr="Henderson (2003)" startWordPosition="892" endWordPosition="893">ural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for trans</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proc. of HLT-NAACL 2003, pages 24–31, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proc. of ACL 2004,</booktitle>
<pages>95--102</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3597" citStr="Henderson, 2004" startWordPosition="520" endWordPosition="521">vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 1990) in backward direction (Henderson, 2004). The stack structure is also modeled similarly to RNNs, and its top item is updated using the previously constructed hidden representations saved in the 1169 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics stack. The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree structure inspired by the recursive neura</context>
<context position="6333" citStr="Henderson (2004)" startWordPosition="940" endWordPosition="941">ions of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for transition-based dependency parsing in which limited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-</context>
<context position="11995" citStr="Henderson, 2004" startWordPosition="1988" endWordPosition="1989">ited histories, such as those that consider two adjacent constituent trees in a stack (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). Our method employs an RNN or Elman network (Elman, 1990) to represent an unlimited stack and queue history. Formally, we use an m-dimensional vector for each hidden state unless otherwise stated. Here, let xi E Rm�×1 be an m0-dimensional vector representing the input word wi and the dimension may not match with the hidden state size m. qi E Rm×1 denotes the hidden state for the input word wi in a queue. Following the RNN in backward direction (Henderson, 2004), the hidden state for each word wi is computed right-to-left, qn−1 to q0, beginning from a constant qn: qi = τ (Hquqi+1 + Wquxi + bqu) , (2) where Hqu E Rm×m, Wqu E Rm×m�, bqu E Rm×1 and τ(x) is hard-tanh applied elementwise2. 2τ(x) = −1 for x &lt; 1, 1 for x &gt; 1 otherwise x. j : (i, false, S) : p shift-X j + 1 : (i + 1, false, S|X) : p + psh j : (i, false, S|s1|s0) : p reduce-X j + 1 : (i, false, S|X) : p + pre j : (i, false, S|s0) : p p(d) = |d|−1 j=0 1171 next stack queue 2 1 0 hj+1 hj+1 hj+1 Qsh Hsh h1j h0 xi xi+1 j Wsh input (a) shift-X action next stack next stack 2 1 0 2 1 0 hj+1 hj+1 hj+</context>
<context position="27620" citStr="Henderson, 2004" startWordPosition="4796" endWordPosition="4797">maximum violated steps during search. Figure 3 and 4 plot the training curves for WSJ and CTB, respectively. The plots clearly demonstrate that the use of the expected mistake is faster in convergence and stabler in learning when compared with that of the Viterbi mistake13. Next, we compare our parser, TNCP, with other parsers listed in Table 4 for WSJ and Table 5 for CTB on the test data. The Collins parser (Collins, 1997) and the Berkeley parser (Petrov and Klein, 2007) are chart-based parsers with rich states, either through lexicalization or latent annotation. SSN is a left-corner parser (Henderson, 2004), and CVG is a compositional vector grammar-based parser (Socher et al., 2013)14. Both parsers rely on neural networks to represent rich contexts, similar to our work; however they differ in that they essentially perform reranking from either the k-best parses or parse forests15. The word representa13The labeled F1 on those plots are slightly different from EVALB in that all the syntactic labels are considered when computing bracket matching. Further, the scores on the training data are approximation since they were obtained as a byproduct of online learning. 14http://nlp.stanford.edu/software</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proc. of ACL 2004, pages 95–102, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<volume>9</volume>
<issue>8</issue>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Suphan Fayong</author>
<author>Yang Guo</author>
</authors>
<title>Structured perceptron with inexact search.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL-HLT 2012,</booktitle>
<pages>142--151</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4304" citStr="Huang et al., 2012" startWordPosition="621" endWordPosition="624">ng the previously constructed hidden representations saved in the 1169 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1169–1179, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics stack. The representations from both the stack and queue are combined with the representations propagated from the partially parsed tree structure inspired by the recursive neural networks of CVGs. Parameters are estimated efficiently by a variant of max-violation (Huang et al., 2012) which considers the worst mistakes found during search and updates parameters based on the expected mistake. Under similar settings, TCNP performs comparably to state-of-the-art parsers. Experimental results obtained using the Wall Street Journal corpus of the English Penn Treebank achieved a labeled F1 score of 90.68%, and the result for the Penn Chinese Treebank was 84.33%. Our parser performs no reranking with computationally expensive models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation. The source code and mod</context>
<context position="16641" citStr="Huang et al., 2012" startWordPosition="2903" endWordPosition="2906">inal label X. The scores for a queue Qun Wun Hun qi qi+1 Qre queue qi qi+1 Wre Hre qi qi+1 current stack 1172 finish action and an idle action are defined analogous to the unary-X action with special labels for X, (finish) and (idle), respectively3. 5 Parameter Estimation Let θ = {HXsh, QXsh, } E RM bean Mdimensional vector of all model parameters. The parameters are initialized randomly by following Glorot and Bengio (2010), in which the random value range is determined by the size of the input/output layers. The bias parameters are initialized to zeros. We employ a variant of max-violation (Huang et al., 2012) as our training objective, in which parameters are updated based on the worst mistake found during search, rather than the first mistake as performed in the early update perceptron algorithm (Collins and Roark, 2004). Specifically, given a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j* where the difference of the scores is the largest: 10 can be intuitively considered an expected mistake suffered at the maximum violated step j*, which is measured by the Viterbi viol</context>
<context position="18400" citStr="Huang et al. (2012)" startWordPosition="3236" endWordPosition="3239">t, (14) where O is the Hadamard product (or the elementwise product). θt−1 is updated using the element specific learning rate ηt E RM derived from Gt and a constant η0 &gt; 0: 1 ηt +- η0 (Gt + E)− 2 (15) S ρθ (yj0) − a B ρθ (d)1 . (9) θt−1 +- θt−1 − ηt O gt (16) 2 1211θ − θt−211122 + λη�tabs(θ). j* = arg min j θt +- arg min θ Then, we define the following hinge-loss function: wherein we consider the subset of sub-derivations ˜Bj* C Bj* consisting of those scored higher than ρθ(yj* 0 ): ˜Bj* = {d E Bj* I ρθ (d) &gt; ρθ(yj0)} (11) exp(ρθ(d)) pθ(d) = (12) Ed&apos;E ˜Bj* exp(ρθ(d&apos;)) pθ(d)ρθ(d). (13) Unlike Huang et al. (2012) and inspired by Tamura et al. (2014), we consider all incorrect subderivations found in ˜Bj* through the expected score E ˜Bj* [ρθ]4. The loss function in Equation 3Since h1j and qn are constants for the finish and idle actions, we enforce HXun = 0 and QXun = 0 for those special actions. 4We can use all the sub-derivations in Bj*; however, our preliminary studies indicated that the use of ˜Bj* was better. (17) Compared with AdaGrad, the squared sum of the sub-gradients decays over time using a constant 0 &lt; γ &lt; 1 in Equation 14. The learning rate in Equation 15 is computed element-wise and bou</context>
<context position="26555" citStr="Huang et al., 2012" startWordPosition="4613" endWordPosition="4616">d network by ignoring parameters from the stack and queue, e.g., by enforcing HXsh = 0 and QXsh = 0 in Equation 3, which is essentially similar to the CVG approach (Socher et al., 2013). Adding the context from the stack in +stack boosts the performance significantly. Further gains are observed when the queue context +queue is incorporated in the model. These results clearly indicate that explicit representations of the stack and queue are very important when applying a recursive neural network model for transition-based parsing. We then compared the expected mistake with the Viterbi mistake (Huang et al., 2012) as our training objective by replacing E ˜B,* [pθ] with maxd∈B,* pθ(d) in Equation 10. Table 3 shows that the use of the expected mistake (expected) as a loss function is significantly better than that 12We experimented larger dimensions in Appendix A. 0 10 20 30 40 50 60 70 80 90 100 iterations Figure 3: Plots for training iterations and labeled F1(%) on WSJ. of the Viterbi mistake (Viterbi) by considering all the incorrect sub-derivations at maximum violated steps during search. Figure 3 and 4 plot the training curves for WSJ and CTB, respectively. The plots clearly demonstrate that the use</context>
</contexts>
<marker>Huang, Fayong, Guo, 2012</marker>
<rawString>Liang Huang, Suphan Fayong, and Yang Guo. 2012. Structured perceptron with inexact search. In Proc. of NAACL-HLT 2012, pages 142–151, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1649" citStr="Klein and Manning, 2003" startWordPosition="216" endWordPosition="220">-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 1 Introduction A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proc. of ACL 2003, pages 423–430, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser showdown at the wall street corral: An empirical investigation of error types in parser output.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL 2012,</booktitle>
<pages>1048--1059</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="31260" citStr="Kummerfeld et al. (2012)" startWordPosition="5383" endWordPosition="5386">er second/labeled F1(%) measured on the test data. formance by trading off run time in most cases. Note that Berkeley, CVG and ZPar took 4.74, 1.54 and 37.92 sentences/sec, respectively, with WSJ. Although it is more difficult to compare with other parsers, our parser implemented in C++ is on par with Java implementations of Berkeley and CVG. The large run time difference with the C++ implemented ZPar may come from the network computation and joint POS inference in our model which impact parsing speed significantly. 6.3 Error Analysis To assess parser error types, we used the tool proposed by Kummerfeld et al. (2012)17. The average number of errors per sentence is listed in Table 7 for each error type on the WSJ test data. Generally, our parser results in errors that are comparable to the state-of-the-art parsers; however, greater reductions are observed for various attachments errors. One of the largest gains comes from the clause attachment, i.e., 0.12 reduction in average errors from Berkeley and 0.05 from CVG. The average number of errors is also reduced by 0.09 from Berkeley and 0.06 from CVG for the PP attachment. We also observed large reductions in coordination and unary rule errors. 7 Conclusion </context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser showdown at the wall street corral: An empirical investigation of error types in parser output. In Proc. of EMNLP-CoNLL 2012, pages 1048–1059, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP 2014,</booktitle>
<pages>729--739</pages>
<location>Doha, Qatar,</location>
<contexts>
<context position="5913" citStr="Zuidema, 2014" startWordPosition="875" endWordPosition="876">., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manner, in the same way as PCFGs. Henderson (2003) was the first to demonstrate the successful use of neural networks to represent derivation histories under large-scale parsing experiments. He employed synchrony networks, i.e., feed-forward style networks, to assign a probability for each step in the left-corner parsing conditioning on all parsing steps. Henderson (2004) 1http://github.com/tarowatanabe/trance later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the </context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proc. of EMNLP 2014, pages 729–739, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="20807" citStr="Marcus et al., 1993" startWordPosition="3658" endWordPosition="3661"> to other shards to reflect the updates in one shard. Unlike Dean et al. (2012), we do not keep a central storage for model parameters; the replicated parameters are synchronized in each iteration by choosing the model parameters from one of the shards with respect to the minimum of i1 norm7. Note that we synchronize θ, but G is maintained as shard local parameters. 6 Experiments 6.1 Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 440- 1151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8. The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proc. ofACL 2005,</booktitle>
<pages>75--82</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1690" citStr="Matsuzaki et al., 2005" startWordPosition="223" endWordPosition="226"> of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 1 Introduction A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic CFG with latent annotations. In Proc. ofACL 2005, pages 75–82, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marshall R Mayberry</author>
<author>Risto Miikkulainen</author>
</authors>
<title>Sardsrn: A neural network shift-reduce parser.</title>
<date>1999</date>
<booktitle>In Proc. of IJCAI ’99,</booktitle>
<pages>820--827</pages>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="7003" citStr="Mayberry and Miikkulainen (1999)" startWordPosition="1036" endWordPosition="1040">e later employed a discriminative model and showed further gains by conditioning on the representation of the future input in addition to the history of parsing steps. Similar feed-forward style networks are successfully applied for transition-based dependency parsing in which limited contexts are considered in the feature representation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them </context>
</contexts>
<marker>Mayberry, Miikkulainen, 1999</marker>
<rawString>Marshall R. Mayberry and Risto Miikkulainen. 1999. Sardsrn: A neural network shift-reduce parser. In Proc. of IJCAI ’99, pages 820–827, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sauro Menchetti</author>
<author>Fabrizio Costa</author>
<author>Paolo Frasconi</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Wide coverage natural language processing using kernel methods and neural networks for structured data.</title>
<date>2005</date>
<journal>Pattern Recognition Letters,</journal>
<volume>26</volume>
<issue>12</issue>
<contexts>
<context position="5268" citStr="Menchetti et al., 2005" startWordPosition="773" endWordPosition="776"> the Penn Chinese Treebank was 84.33%. Our parser performs no reranking with computationally expensive models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation. The source code and models will be made public1. 2 Related Work Our study is largely inspired by recursive neural networks for parsing, first pioneered by Costa et al. (2003), in which parsing is treated as a ranking problem of finding phrasal attachment. Such network structures have been used successfully as a reranker for k-best parses from a baseline parser (Menchetti et al., 2005) or parse forests (Socher et al., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside rec</context>
</contexts>
<marker>Menchetti, Costa, Frasconi, Pontil, 2005</marker>
<rawString>Sauro Menchetti, Fabrizio Costa, Paolo Frasconi, and Massimiliano Pontil. 2005. Wide coverage natural language processing using kernel methods and neural networks for structured data. Pattern Recognition Letters, 26(12):1896–1906, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
<author>Martin Karafi´at</author>
<author>Luk´aˇs Burget</author>
<author>Jan ˇCernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proc. of INTERSPEECH</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, ˇCernock´y, Khudanpur, 2010</marker>
<rawString>Tom´aˇs Mikolov, Martin Karafi´at, Luk´aˇs Burget, Jan ˇCernock´y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proc. of INTERSPEECH 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee W Teh</author>
</authors>
<title>A fast and simple algorithm for training neural probabilistic language models.</title>
<date>2012</date>
<booktitle>In John Langford and Joelle Pineau, editors, Proc. of ICML-2012,</booktitle>
<pages>1751--1758</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="23752" citStr="Mnih and Teh, 2012" startWordPosition="4154" endWordPosition="4157">ward the development data in our preliminary experiments10: η0 = 10−2, ry = 0.9, 6 = 1. The choice of A from {10−5,10−6, 10−7} and the number of training iterations were very important for different training objectives and models in order to avoid overfitting. Thus, they were determined by the performance on the development data for each different training objective and/or network configuration, e.g., the dimension for a hidden state. The word representations were initialized by a tool developed in-house for an RNN language model (Mikolov et al., 2010) trained by noise contrastive estimation (Mnih and Teh, 2012). Note that the word representations for initialization were learned from the given training data, not from additional unannotated data as done by Chen and Manning (2014). Testing was performed using a beam size of 64 with a Xeon X5550 2.67GHz CPU. All results were measured by the labeled bracketing metric PARSEVAL (Black et al., 1991) using EVALB11 after debinarization. 6.2 Results Table 1 shows the impact of dimensions on the parsing performance. We varied the hid10We confirmed that this hyperparameter setting was appropriate for different models experimented in Section 6.2 through our preli</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee W. Teh. 2012. A fast and simple algorithm for training neural probabilistic language models. In John Langford and Joelle Pineau, editors, Proc. of ICML-2012, pages 1751–1758, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="27480" citStr="Petrov and Klein, 2007" startWordPosition="4774" endWordPosition="4777">re 3: Plots for training iterations and labeled F1(%) on WSJ. of the Viterbi mistake (Viterbi) by considering all the incorrect sub-derivations at maximum violated steps during search. Figure 3 and 4 plot the training curves for WSJ and CTB, respectively. The plots clearly demonstrate that the use of the expected mistake is faster in convergence and stabler in learning when compared with that of the Viterbi mistake13. Next, we compare our parser, TNCP, with other parsers listed in Table 4 for WSJ and Table 5 for CTB on the test data. The Collins parser (Collins, 1997) and the Berkeley parser (Petrov and Klein, 2007) are chart-based parsers with rich states, either through lexicalization or latent annotation. SSN is a left-corner parser (Henderson, 2004), and CVG is a compositional vector grammar-based parser (Socher et al., 2013)14. Both parsers rely on neural networks to represent rich contexts, similar to our work; however they differ in that they essentially perform reranking from either the k-best parses or parse forests15. The word representa13The labeled F1 on those plots are slightly different from EVALB in that all the syntactic labels are considered when computing bracket matching. Further, the </context>
<context position="30198" citStr="Petrov and Klein, 2007" startWordPosition="5211" endWordPosition="5214">nd Xue, 2014). Similar to ZPar, we present the result without cluster features learned from extra unannotated data. Finally, we measured the speed for parsing by varying beam size and hidden dimension (Table 6). When testing, we applied a pre-computation technique for layers involving word representation vectors (Devlin et al., 2014), i.e., Wqu in Equation 2 and WshX in Equation 3. Thus, the parsing speed was influenced by only the hidden state size m. It is clear that the enlarged beam size improves per16http://sourceforge.net/projects/zpar/ parser test ZPar (Zhu et al., 2013) 83.2 Berkeley (Petrov and Klein, 2007) 83.3 Joint (Wang and Xue, 2014) 84.9 This work: TNCP 84.3 Table 5: Comparison of different parsers on the CTB test data measured by labeled F1(%). beam 32 64 128 WSJ-32 15.42/89.95 7.90/90.01 3.97/90.04 64 7.31/90.56 3.56/90.68 1.76/90.73 CTB-32 13.67/82.35 6.95/82.64 3.68/82.84 64 6.15/84.12 3.11/84.33 1.53/83.83 Table 6: Comparison of parsing speed by varying beam size and hidden dimension; each cell shows the number of sentences per second/labeled F1(%) measured on the test data. formance by trading off run time in most cases. Note that Berkeley, CVG and ZPar took 4.74, 1.54 and 37.92 sent</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proc. of NAACL-HLT 2007, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1712" citStr="Petrov et al., 2006" startWordPosition="227" endWordPosition="230">nd 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters. 1 Introduction A popular parsing algorithm is a cubic time chartbased dynamic programming algorithm that uses probabilistic context-free grammars (PCFGs). However, PCFGs learned from treebanks are too coarse to represent the syntactic structures of texts. To address this problem, various contexts are incorporated into the grammars through lexicalization (Collins, 2003; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of n</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of COLING-ACL 2006, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="7319" citStr="Pollack, 1990" startWordPosition="1089" endWordPosition="1090">ation (Chen and Manning, 2014). Our model is very similar in that the score of each action is computed by conditioning on all previous actions and future input in the queue. The use of neural networks for transition-based shift-reduce parsing was first presented by Mayberry and Miikkulainen (1999) in which the stack representation was treated as a hidden state of an RNN. In their study, the hidden state is updated recurrently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser differs in that we do not differentiate left or right head word</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B. Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1-2):77–105, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A classifierbased parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proc. of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>125--132</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="2187" citStr="Sagae and Lavie, 2005" startWordPosition="298" endWordPosition="301">03; Charniak, 2000) or category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insuffi</context>
<context position="7807" citStr="Sagae and Lavie (2005)" startWordPosition="1169" endWordPosition="1172">ponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser differs in that we do not differentiate left or right head words. In addition, POS tags are jointly induced during parsing in the same manner as Wang and Xue (2014). Given an input sentence w0, · · · , wn−1, the transition-based parser employs a stack of partially constructed constituent tree structures and a queue of input words. In each step, a transition action is applied to a state (i, f, 5), where i is the next input word position in the queue wz, f is a flag indicating the completion of parsing, i.e., whether the ROOT of a constituent tree</context>
<context position="10188" citStr="Sagae and Lavie, 2005" startWordPosition="1653" endWordPosition="1656">em in Figure 1. We employ beam search which starts from an axiom consisting of a stack with a special symbol (eps), and ends when we reach a goal item (Zhang and Clark, 2009). A set of agenda B = B0, B1, · · · maintains the k-best states for each step j at Bj, which is first initialized by inserting the axiom in B0. Then, at each step j = 0, 1, · · · , every state in the agenda Bj is extended by applying one of the actions and the new states are inserted into the agenda Bj+1 for the next step, which retains only the k-best states. We limit the maximum number of consecutive unary actions to u (Sagae and Lavie, 2005; Zhang and Clark, 2009) and the maximum number of unary actions in a single derivation to uxn. Thus, the process is repeated until we reach the final step of (2+u)n, which keeps the completed states. The idle action is inspired by the padding method of Zhu et al. (2013), such that the states in an agenda are comparable in terms of score even if differences exist in the number of unary actions. Unlike Zhu et al. (2013) we do not terminate parsing even if all the states in an agenda are completed (f = true). The score of a state is computed by summing the scores of all the actions leading to th</context>
<context position="11487" citStr="Sagae and Lavie, 2005" startWordPosition="1896" endWordPosition="1899">duce-X, unary-X, finish and idle actions, respectively, which are computed on the basis of the history of actions. 4 Neural Constituent Parsing The score of a state is defined formally as the total score of transition actions, or a (partial) derivation d = d0, d1, · · · leading to the state as follows: p(dj|dj−1 0 ). (1) Note that the score of each action is dependent on all previous actions. In previous studies, the score is computed by a linear model, i.e., a weighted sum of feature values derived from limited histories, such as those that consider two adjacent constituent trees in a stack (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). Our method employs an RNN or Elman network (Elman, 1990) to represent an unlimited stack and queue history. Formally, we use an m-dimensional vector for each hidden state unless otherwise stated. Here, let xi E Rm�×1 be an m0-dimensional vector representing the input word wi and the dimension may not match with the hidden state size m. qi E Rm×1 denotes the hidden state for the input word wi in a queue. Following the RNN in backward direction (Henderson, 2004), the hidden state for each word wi is computed right-to-left, qn−1 to q0, beginning from a </context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Kenji Sagae and Alon Lavie. 2005. A classifierbased parser with linear run-time complexity. In Proc. of the Ninth International Workshop on Parsing Technology, pages 125–132, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Senior</author>
<author>Georg Heigold</author>
<author>Marc’Aurelio Ranzato</author>
<author>Ke Yang</author>
</authors>
<title>An empirical study of learning rates in deep neural networks for speech recognition.</title>
<date>2013</date>
<booktitle>In Proc. of ICASSP 2013,</booktitle>
<pages>6724--6728</pages>
<contexts>
<context position="17499" citStr="Senior et al., 2013" startWordPosition="3057" endWordPosition="3060"> a training instance (w, y) where w is an input sentence and y is its gold derivation, i.e., a sequence of actions representing the gold parse tree for w, we seek for the step j* where the difference of the scores is the largest: 10 can be intuitively considered an expected mistake suffered at the maximum violated step j*, which is measured by the Viterbi violation in Equation 9. Note that if we replace E ˜Bj* [ρθ] with maxdEBj* ρθ(d) in Equation 10, it is exactly the same as the max-violation objective (Huang et al., 2012)5. To minimize the loss function, we use a diagonal version of AdaDec (Senior et al., 2013) — a variant of diagonal AdaGrad (Duchi et al., 2011) — under mini-batch settings. Given the sub-gradient gt E RM of Equation 10 at time t computed by the back-propagation through structure (Goller and K¨uchler, 1996), we maintain additional parameters Gt E RM: Gt +- γGt−1 + gt O gt, (14) where O is the Hadamard product (or the elementwise product). θt−1 is updated using the element specific learning rate ηt E RM derived from Gt and a constant η0 &gt; 0: 1 ηt +- η0 (Gt + E)− 2 (15) S ρθ (yj0) − a B ρθ (d)1 . (9) θt−1 +- θt−1 − ηt O gt (16) 2 1211θ − θt−211122 + λη�tabs(θ). j* = arg min j θt +- ar</context>
</contexts>
<marker>Senior, Heigold, Ranzato, Yang, 2013</marker>
<rawString>Andrew Senior, Georg Heigold, Marc’Aurelio Ranzato, and Ke Yang. 2013. An empirical study of learning rates in deep neural networks for speech recognition. In Proc. of ICASSP 2013, pages 6724– 6728, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Ng Andrew Y</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proc. of ACL 2013,</booktitle>
<pages>455--465</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2955" citStr="Socher et al. (2013)" startWordPosition="420" endWordPosition="423">mpete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased parsing. In this paper, we propose a neural networkbased parser — transition-based neural constituent parsing (TNCP) — which can guarantee efficient search naturally. TNCP explicitly models the actions performed on the stack and queue employed in transition-based parsing. More specifically, the queue is modeled by recurrent neural network (RNN) or Elman network (Elman, 199</context>
<context position="5307" citStr="Socher et al., 2013" startWordPosition="780" endWordPosition="783"> parser performs no reranking with computationally expensive models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation. The source code and models will be made public1. 2 Related Work Our study is largely inspired by recursive neural networks for parsing, first pioneered by Costa et al. (2003), in which parsing is treated as a ranking problem of finding phrasal attachment. Such network structures have been used successfully as a reranker for k-best parses from a baseline parser (Menchetti et al., 2005) or parse forests (Socher et al., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema,</context>
<context position="13954" citStr="Socher et al., 2013" startWordPosition="2385" endWordPosition="2388">e that propagates the previous context in the stack. QXsh can reflect the queue context qi, or the future input sequence from wi through wn−1, while WshX directly expresses the leaf of a tree structure using the shifted input word representation xi for wi. The hidden state h0j+1 is used to compute the score of a derivation ρ(dj|dj−1 0 ) in Equation 4, which is based on the matrix VshX E R1×m and the bias term vXsh E R. Note that hlj+1 = hl−1 j for l = 1, 2, · · · because the stack is updated by the newly created partial tree label X associated with the new hidden state h0j+1. Inspired by CVG (Socher et al., 2013), we differentiate the matrices for each non-terminal (or POS) label X rather than using shared parameters. However, our model differs in that the parameters are untied on the basis of the left hand side of a rule, rather than the right hand side, because our model assigns a score discriminatively for each action with the left hand side label X unlike a generative model derived from PCFGs. Reduce: Similarly, the score for a reduce action is obtained as follows: h0j = τ (T-Txh2 + Qeg + WeXho:l]+be+-rre ri r) (5) ρ(dj = reduce-X|dj−1 0 ) = V X re h0j+1 + vXre, (6) where HX re E Rm×m, QXre E Rm×m</context>
<context position="26121" citStr="Socher et al., 2013" startWordPosition="4546" endWordPosition="4549"> the performance on the development data, wherein we achieved 91.11% and 85.77% labeled F1 for WSJ and CTB, respectively. The total number of parameters were approximately 28.3M and 22.0M for WSJ and CTB, respectively, among which 17.8M and 13.4M were occupied for word representations, respectively. Table 2 differentiated the network structure. The tree model computes the new hidden state h0j+1 using only the recursively constructed network by ignoring parameters from the stack and queue, e.g., by enforcing HXsh = 0 and QXsh = 0 in Equation 3, which is essentially similar to the CVG approach (Socher et al., 2013). Adding the context from the stack in +stack boosts the performance significantly. Further gains are observed when the queue context +queue is incorporated in the model. These results clearly indicate that explicit representations of the stack and queue are very important when applying a recursive neural network model for transition-based parsing. We then compared the expected mistake with the Viterbi mistake (Huang et al., 2012) as our training objective by replacing E ˜B,* [pθ] with maxd∈B,* pθ(d) in Equation 10. Table 3 shows that the use of the expected mistake (expected) as a loss functi</context>
<context position="27698" citStr="Socher et al., 2013" startWordPosition="4806" endWordPosition="4809">ves for WSJ and CTB, respectively. The plots clearly demonstrate that the use of the expected mistake is faster in convergence and stabler in learning when compared with that of the Viterbi mistake13. Next, we compare our parser, TNCP, with other parsers listed in Table 4 for WSJ and Table 5 for CTB on the test data. The Collins parser (Collins, 1997) and the Berkeley parser (Petrov and Klein, 2007) are chart-based parsers with rich states, either through lexicalization or latent annotation. SSN is a left-corner parser (Henderson, 2004), and CVG is a compositional vector grammar-based parser (Socher et al., 2013)14. Both parsers rely on neural networks to represent rich contexts, similar to our work; however they differ in that they essentially perform reranking from either the k-best parses or parse forests15. The word representa13The labeled F1 on those plots are slightly different from EVALB in that all the syntactic labels are considered when computing bracket matching. Further, the scores on the training data are approximation since they were obtained as a byproduct of online learning. 14http://nlp.stanford.edu/software/ lex-parser.shtml 15Strictly speaking, SSN can work as a standalone parser; T</context>
</contexts>
<marker>Socher, Bauer, Manning, Y, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Ng Andrew Y. 2013. Parsing with compositional vector grammars. In Proc. of ACL 2013, pages 455–465, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Transition-based dependency parsing using recursive neural networks.</title>
<date>2013</date>
<booktitle>In Proc. of Deep Learning Workshop at the 2013 Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,</booktitle>
<location>Nevada, USA,</location>
<contexts>
<context position="5364" citStr="Stenetorp (2013)" startWordPosition="791" endWordPosition="792">models, employs no templates for feature engineering, and requires no additional monolingual data for reliable parameter estimation. The source code and models will be made public1. 2 Related Work Our study is largely inspired by recursive neural networks for parsing, first pioneered by Costa et al. (2003), in which parsing is treated as a ranking problem of finding phrasal attachment. Such network structures have been used successfully as a reranker for k-best parses from a baseline parser (Menchetti et al., 2005) or parse forests (Socher et al., 2013), and have achieved gains on large data. Stenetorp (2013) showed that the recursive neural networks are comparable to the state-of-the-art system with a rich feature set under dependency parsing. Our model is not a reranking model, but a discriminative parsing model, which incorporates the representations of stacks and queues employed in the transition-based parsing framework, in addition to the representations of the tree structures. The use of representations outside of the partial parsed trees is very similar to the recently proposed inside-outside recursive neural networks (Le and Zuidema, 2014) which can assign probabilities in a top-down manne</context>
</contexts>
<marker>Stenetorp, 2013</marker>
<rawString>Pontus Stenetorp. 2013. Transition-based dependency parsing using recursive neural networks. In Proc. of Deep Learning Workshop at the 2013 Conference on Neural Information Processing Systems (NIPS), Lake Tahoe, Nevada, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Recurrent neural networks for word alignment model.</title>
<date>2014</date>
<booktitle>In Proc. of ACL 2014,</booktitle>
<pages>1470--1480</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="18437" citStr="Tamura et al. (2014)" startWordPosition="3243" endWordPosition="3246">uct (or the elementwise product). θt−1 is updated using the element specific learning rate ηt E RM derived from Gt and a constant η0 &gt; 0: 1 ηt +- η0 (Gt + E)− 2 (15) S ρθ (yj0) − a B ρθ (d)1 . (9) θt−1 +- θt−1 − ηt O gt (16) 2 1211θ − θt−211122 + λη�tabs(θ). j* = arg min j θt +- arg min θ Then, we define the following hinge-loss function: wherein we consider the subset of sub-derivations ˜Bj* C Bj* consisting of those scored higher than ρθ(yj* 0 ): ˜Bj* = {d E Bj* I ρθ (d) &gt; ρθ(yj0)} (11) exp(ρθ(d)) pθ(d) = (12) Ed&apos;E ˜Bj* exp(ρθ(d&apos;)) pθ(d)ρθ(d). (13) Unlike Huang et al. (2012) and inspired by Tamura et al. (2014), we consider all incorrect subderivations found in ˜Bj* through the expected score E ˜Bj* [ρθ]4. The loss function in Equation 3Since h1j and qn are constants for the finish and idle actions, we enforce HXun = 0 and QXun = 0 for those special actions. 4We can use all the sub-derivations in Bj*; however, our preliminary studies indicated that the use of ˜Bj* was better. (17) Compared with AdaGrad, the squared sum of the sub-gradients decays over time using a constant 0 &lt; γ &lt; 1 in Equation 14. The learning rate in Equation 15 is computed element-wise and bounded by a constant E &gt; 0, and if we s</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2014</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2014. Recurrent neural networks for word alignment model. In Proc. of ACL 2014, pages 1470– 1480, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich partof-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>Proc. of HLT-NAACL</booktitle>
<pages>173--180</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="21227" citStr="Toutanova et al., 2003" startWordPosition="3729" endWordPosition="3732">nts for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 440- 1151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8. The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to consider OOVs for testing: They were simply mapped to a special token (unk) when looking up their corresponding word representation vector. Similarly, when assigning possible POS tags in shift actions, they fell back to their corresponding “word sig</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich partof-speech tagging with a cyclic dependency network. In Proc. of HLT-NAACL 2003, pages 173– 180, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL 2010,</booktitle>
<pages>384--394</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="28960" citStr="Turian et al., 2010" startWordPosition="5012" endWordPosition="5015">enderson, 2004). 100 F1(%) 95 90 85 80 75 70 65 expected (train) Viterbi (train) expected (dev) Viterbi (dev) 1175 0 10 20 30 40 50 60 70 80 90 100 iterations Figure 4: Plots for training iterations and labeled F1(%) on CTB. parser test Collins (Collins, 1997) 87.8 Berkeley (Petrov and Klein, 2007) 90.1 SSN (Henderson, 2004) 90.1 ZPar (Zhu et al., 2013) 90.4 CVG (Socher et al., 2013) 90.4 Charniak-R (Charniak and Johnson, 2005) 91.0 This work: TNCP 90.7 Table 4: Comparison of different parsers on the WSJ test data measured by labeled F1(%). tion in CVG was learned from large monolingual data (Turian et al., 2010), but our parser learns word representation from only the provided training data. Charniak-R is a discriminative reranking parser with non-local features (Charniak and Johnson, 2005). ZPar is a transition-based shiftreduce parser (Zhu et al., 2013)16 that influences the deduction system in Figure 1, but differs in that scores are computed by a large number of features and POS tagging is performed separately. The results shown in Table 4 and 5 come from the feature set without extra data, i.e., semi-supervised features. Joint is the joint POS tagging and transitionbased parsing with non-local f</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proc. of ACL 2010, pages 384–394, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiguo Wang</author>
<author>Nianwen Xue</author>
</authors>
<title>Joint pos tagging and transition-based constituent parsing in chinese with non-local features.</title>
<date>2014</date>
<booktitle>In Proc. of ACL 2014,</booktitle>
<pages>733--742</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2582" citStr="Wang and Xue, 2014" startWordPosition="363" endWordPosition="366"> in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relations in texts due to the limited contexts represented in latent annotations or non-local features. Recently Socher et al. (2013) introduced compositional vector grammar (CVG) to address the above limitations. However, they employ reranking over a forest generated by a baseline parser for efficient search, because CVG is built on cubic time chartbased pa</context>
<context position="8020" citStr="Wang and Xue (2014)" startWordPosition="1207" endWordPosition="1210">ethod differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser differs in that we do not differentiate left or right head words. In addition, POS tags are jointly induced during parsing in the same manner as Wang and Xue (2014). Given an input sentence w0, · · · , wn−1, the transition-based parser employs a stack of partially constructed constituent tree structures and a queue of input words. In each step, a transition action is applied to a state (i, f, 5), where i is the next input word position in the queue wz, f is a flag indicating the completion of parsing, i.e., whether the ROOT of a constituent tree covering all the input words is reached, and 5 represents a stack of tree elements, s0, s1, · · · . The parser consists of five actions: shift-X consumes the next input word, wz, from the queue and pushes a non-t</context>
<context position="29588" citStr="Wang and Xue, 2014" startWordPosition="5115" endWordPosition="5118"> parser learns word representation from only the provided training data. Charniak-R is a discriminative reranking parser with non-local features (Charniak and Johnson, 2005). ZPar is a transition-based shiftreduce parser (Zhu et al., 2013)16 that influences the deduction system in Figure 1, but differs in that scores are computed by a large number of features and POS tagging is performed separately. The results shown in Table 4 and 5 come from the feature set without extra data, i.e., semi-supervised features. Joint is the joint POS tagging and transitionbased parsing with non-local features (Wang and Xue, 2014). Similar to ZPar, we present the result without cluster features learned from extra unannotated data. Finally, we measured the speed for parsing by varying beam size and hidden dimension (Table 6). When testing, we applied a pre-computation technique for layers involving word representation vectors (Devlin et al., 2014), i.e., Wqu in Equation 2 and WshX in Equation 3. Thus, the parsing speed was influenced by only the hidden state size m. It is clear that the enlarged beam size improves per16http://sourceforge.net/projects/zpar/ parser test ZPar (Zhu et al., 2013) 83.2 Berkeley (Petrov and Kl</context>
</contexts>
<marker>Wang, Xue, 2014</marker>
<rawString>Zhiguo Wang and Nianwen Xue. 2014. Joint pos tagging and transition-based constituent parsing in chinese with non-local features. In Proc. of ACL 2014, pages 733–742, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-dong Chiou</author>
<author>Marta Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="20980" citStr="Xue et al., 2005" startWordPosition="3688" endWordPosition="3691">ed in each iteration by choosing the model parameters from one of the shards with respect to the minimum of i1 norm7. Note that we synchronize θ, but G is maintained as shard local parameters. 6 Experiments 6.1 Settings We conducted experiments for transition-based neural constituent parsing (TNCP) for two languages — English and Chinese. English data were derived from the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993), from which sections 2-21 were used for training, 22 for development and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al., 2005); articles 001-270 and 440- 1151 were used for training, 301-325 for development, and 271-300 for testing. Inspired by jackknifing (Collins and Koo, 2005), we reassigned POS tags for training data using the Stanford tagger (Toutanova et al., 2003)8. The treebank trees were normalized by removing empty nodes and unary rules with X over X (or X → X), then binarized in a left-branched manner. The possible actions taken for our shift-reduce parsing, e.g., X → w in shift-X, were learned from the normalized treebank trees. The words that occurred twice or less were handled differently in order to co</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(2):207–238, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Transition-based parsing of the chinese treebank using a global discriminative model.</title>
<date>2009</date>
<booktitle>In Proc. of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>162--171</pages>
<location>Paris, France,</location>
<contexts>
<context position="2211" citStr="Zhang and Clark, 2009" startWordPosition="302" endWordPosition="305">category splitting either manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various</context>
<context position="7834" citStr="Zhang and Clark (2009)" startWordPosition="1174" endWordPosition="1177">ed recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser differs in that we do not differentiate left or right head words. In addition, POS tags are jointly induced during parsing in the same manner as Wang and Xue (2014). Given an input sentence w0, · · · , wn−1, the transition-based parser employs a stack of partially constructed constituent tree structures and a queue of input words. In each step, a transition action is applied to a state (i, f, 5), where i is the next input word position in the queue wz, f is a flag indicating the completion of parsing, i.e., whether the ROOT of a constituent tree covering all the input wor</context>
<context position="9741" citStr="Zhang and Clark, 2009" startWordPosition="1560" endWordPosition="1563">X as its root, and with s0 and s1 as right and left antecedents, respectively (X —* s1s0). The newly created tree is then pushed into the stack. unary-X is similar to reduce-X; however, it consumes only the top most item s0 from the stack and pushes a new tree of X —* s0. finish indicates the completion of parsing, i.e., reaching the ROOT. idle preserves completion until the goal is reached. The whole procedure is summarized as a deduction system in Figure 1. We employ beam search which starts from an axiom consisting of a stack with a special symbol (eps), and ends when we reach a goal item (Zhang and Clark, 2009). A set of agenda B = B0, B1, · · · maintains the k-best states for each step j at Bj, which is first initialized by inserting the axiom in B0. Then, at each step j = 0, 1, · · · , every state in the agenda Bj is extended by applying one of the actions and the new states are inserted into the agenda Bj+1 for the next step, which retains only the k-best states. We limit the maximum number of consecutive unary actions to u (Sagae and Lavie, 2005; Zhang and Clark, 2009) and the maximum number of unary actions in a single derivation to uxn. Thus, the process is repeated until we reach the final st</context>
<context position="11510" citStr="Zhang and Clark, 2009" startWordPosition="1900" endWordPosition="1903"> and idle actions, respectively, which are computed on the basis of the history of actions. 4 Neural Constituent Parsing The score of a state is defined formally as the total score of transition actions, or a (partial) derivation d = d0, d1, · · · leading to the state as follows: p(dj|dj−1 0 ). (1) Note that the score of each action is dependent on all previous actions. In previous studies, the score is computed by a linear model, i.e., a weighted sum of feature values derived from limited histories, such as those that consider two adjacent constituent trees in a stack (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013). Our method employs an RNN or Elman network (Elman, 1990) to represent an unlimited stack and queue history. Formally, we use an m-dimensional vector for each hidden state unless otherwise stated. Here, let xi E Rm�×1 be an m0-dimensional vector representing the input word wi and the dimension may not match with the hidden state size m. qi E Rm×1 denotes the hidden state for the input word wi in a queue. Following the RNN in backward direction (Henderson, 2004), the hidden state for each word wi is computed right-to-left, qn−1 to q0, beginning from a constant qn: qi = τ (Hq</context>
</contexts>
<marker>Zhang, Clark, 2009</marker>
<rawString>Yue Zhang and Stephen Clark. 2009. Transition-based parsing of the chinese treebank using a global discriminative model. In Proc. of the 11th International Conference on Parsing Technologies (IWPT’09), pages 162–171, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and accurate shiftreduce constituent parsing.</title>
<date>2013</date>
<booktitle>In Proc. of ACL 2013,</booktitle>
<pages>434--443</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2230" citStr="Zhu et al. (2013)" startWordPosition="306" endWordPosition="309">r manually (Klein and Manning, 2003) or automatically (Matsuzaki et al., 2005; Petrov et al., 2006). Recently a rich feature set was introduced to capture the lexical contexts ∗The first author is now affiliated with Google, Japan. in each span without extra annotations in grammars (Hall et al., 2014). Alternatively, transition-based algorithms run in linear time by taking a series of shift-reduce actions with richer lexicalized features considering histories; however, the accuracies did not match with the state-of-the-art methods until recently (Sagae and Lavie, 2005; Zhang and Clark, 2009). Zhu et al. (2013) show that the use of better transition actions considering unaries and a set of nonlocal features can compete with the accuracies of chart-based parsing. The features employed in a transition-based algorithm usually require part of speech (POS) annotation in the input, but the delayed feature technique allows joint POS inference (Wang and Xue, 2014). In both frameworks, the richer models require that more parameters be estimated during training which can easily result in the data sparseness problems. Furthermore, the enriched models are still insufficient to capture various syntactic relation</context>
<context position="7742" citStr="Zhu et al. (2013)" startWordPosition="1159" endWordPosition="1162">currently by either a shift or reduce action, and its corresponding parse tree is decoded recursively from the hidden state (Berg, 1992) using recursive auto-associative memories (Pollack, 1990). We apply the idea of representing a stack in a continuous vector; however, our method differs in that it memorizes all hidden states pushed to the stack and performs push/pop operations. In this manner, we can represent the local contexts saved in the stack explicitly and use them to construct new hidden states. 3 Transition-based Constituent Parsing Our transition-based parser is based on a study by Zhu et al. (2013), which adopts the shift-reduce parsing of Sagae and Lavie (2005) and Zhang and Clark (2009). However, our parser differs in that we do not differentiate left or right head words. In addition, POS tags are jointly induced during parsing in the same manner as Wang and Xue (2014). Given an input sentence w0, · · · , wn−1, the transition-based parser employs a stack of partially constructed constituent tree structures and a queue of input words. In each step, a transition action is applied to a state (i, f, 5), where i is the next input word position in the queue wz, f is a flag indicating the co</context>
<context position="10459" citStr="Zhu et al. (2013)" startWordPosition="1702" endWordPosition="1705"> initialized by inserting the axiom in B0. Then, at each step j = 0, 1, · · · , every state in the agenda Bj is extended by applying one of the actions and the new states are inserted into the agenda Bj+1 for the next step, which retains only the k-best states. We limit the maximum number of consecutive unary actions to u (Sagae and Lavie, 2005; Zhang and Clark, 2009) and the maximum number of unary actions in a single derivation to uxn. Thus, the process is repeated until we reach the final step of (2+u)n, which keeps the completed states. The idle action is inspired by the padding method of Zhu et al. (2013), such that the states in an agenda are comparable in terms of score even if differences exist in the number of unary actions. Unlike Zhu et al. (2013) we do not terminate parsing even if all the states in an agenda are completed (f = true). The score of a state is computed by summing the scores of all the actions leading to the state. In Figure 1, psh, pre, pun, pfi and pid are the scores of shift-X, reduce-X, unary-X, finish and idle actions, respectively, which are computed on the basis of the history of actions. 4 Neural Constituent Parsing The score of a state is defined formally as the t</context>
<context position="28695" citStr="Zhu et al., 2013" startWordPosition="4967" endWordPosition="4970">e scores on the training data are approximation since they were obtained as a byproduct of online learning. 14http://nlp.stanford.edu/software/ lex-parser.shtml 15Strictly speaking, SSN can work as a standalone parser; Table 4 shows the result after reranking (Henderson, 2004). 100 F1(%) 95 90 85 80 75 70 65 expected (train) Viterbi (train) expected (dev) Viterbi (dev) 1175 0 10 20 30 40 50 60 70 80 90 100 iterations Figure 4: Plots for training iterations and labeled F1(%) on CTB. parser test Collins (Collins, 1997) 87.8 Berkeley (Petrov and Klein, 2007) 90.1 SSN (Henderson, 2004) 90.1 ZPar (Zhu et al., 2013) 90.4 CVG (Socher et al., 2013) 90.4 Charniak-R (Charniak and Johnson, 2005) 91.0 This work: TNCP 90.7 Table 4: Comparison of different parsers on the WSJ test data measured by labeled F1(%). tion in CVG was learned from large monolingual data (Turian et al., 2010), but our parser learns word representation from only the provided training data. Charniak-R is a discriminative reranking parser with non-local features (Charniak and Johnson, 2005). ZPar is a transition-based shiftreduce parser (Zhu et al., 2013)16 that influences the deduction system in Figure 1, but differs in that scores are com</context>
<context position="30159" citStr="Zhu et al., 2013" startWordPosition="5205" endWordPosition="5208">g with non-local features (Wang and Xue, 2014). Similar to ZPar, we present the result without cluster features learned from extra unannotated data. Finally, we measured the speed for parsing by varying beam size and hidden dimension (Table 6). When testing, we applied a pre-computation technique for layers involving word representation vectors (Devlin et al., 2014), i.e., Wqu in Equation 2 and WshX in Equation 3. Thus, the parsing speed was influenced by only the hidden state size m. It is clear that the enlarged beam size improves per16http://sourceforge.net/projects/zpar/ parser test ZPar (Zhu et al., 2013) 83.2 Berkeley (Petrov and Klein, 2007) 83.3 Joint (Wang and Xue, 2014) 84.9 This work: TNCP 84.3 Table 5: Comparison of different parsers on the CTB test data measured by labeled F1(%). beam 32 64 128 WSJ-32 15.42/89.95 7.90/90.01 3.97/90.04 64 7.31/90.56 3.56/90.68 1.76/90.73 CTB-32 13.67/82.35 6.95/82.64 3.68/82.84 64 6.15/84.12 3.11/84.33 1.53/83.83 Table 6: Comparison of parsing speed by varying beam size and hidden dimension; each cell shows the number of sentences per second/labeled F1(%) measured on the test data. formance by trading off run time in most cases. Note that Berkeley, CVG </context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and accurate shiftreduce constituent parsing. In Proc. of ACL 2013, pages 434–443, Sofia, Bulgaria, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>