<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011949">
<title confidence="0.998269">
Semi-supervised Training of a Statistical Parser
from Unlabeled Partially-bracketed Data
</title>
<author confidence="0.995295">
Rebecca Watson and Ted Briscoe John Carroll
</author>
<affiliation confidence="0.992909">
Computer Laboratory Department of Informatics
University of Cambridge, UK University of Sussex, UK
</affiliation>
<email confidence="0.993263">
FirstName.LastName@cl.cam.ac.uk J.A.Carroll@sussex.ac.uk
</email>
<sectionHeader confidence="0.993671" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99799325">
We compare the accuracy of a statisti-
cal parse ranking model trained from a
fully-annotated portion of the Susanne
treebank with one trained from unla-
beled partially-bracketed sentences de-
rived from this treebank and from the
Penn Treebank. We demonstrate that
confidence-based semi-supervised tech-
niques similar to self-training outperform
expectation maximization when both are
constrained by partial bracketing. Both
methods based on partially-bracketed
training data outperform the fully su-
pervised technique, and both can, in
principle, be applied to any statistical
parser whose output is consistent with
such partial-bracketing. We also explore
tuning the model to a different domain
and the effect of in-domain data in the
semi-supervised training processes.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994635625">
Extant statistical parsers require extensive and
detailed treebanks, as many of their lexical and
structural parameters are estimated in a fully-
supervised fashion from treebank derivations.
Collins (1999) is a detailed exposition of one
such ongoing line of research which utilizes the
Wall Street Journal (WSJ) sections of the Penn
Treebank (PTB). However, there are disadvan-
tages to this approach. Firstly, treebanks are ex-
pensive to create manually. Secondly, the richer
the annotation required, the harder it is to adapt
the treebank to train parsers which make differ-
ent assumptions about the structure of syntac-
tic analyses. For example, Hockenmeier (2003)
trains a statistical parser based on Combinatory
Categorial Grammar (CCG) on the WSJ PTB,
but first maps the treebank to CCG derivations
semi-automatically. Thirdly, many (lexical) pa-
rameter estimates do not generalize well be-
tween domains. For instance, Gildea (2001) re-
ports that WSJ-derived bilexical parameters in
Collins’ (1999) Model 1 parser contribute about
1% to parse selection accuracy when test data
is in the same domain, but yield no improve-
ment for test data selected from the Brown Cor-
pus. Tadayoshi et al. (2005) adapt a statistical
parser trained on the WSJ PTB to the biomed-
ical domain by retraining on the Genia Corpus,
augmented with manually corrected derivations
in the same format. To make statistical parsing
more viable for a range of applications, we need
to make more effective and flexible use of extant
training data and minimize the cost of annota-
tion for new data created to tune a system to a
new domain.
Unsupervised methods for training parsers
have been relatively unsuccessful to date, in-
cluding expectation maximization (EM) such as
the inside-outside algorithm (IOA) over PCFGs
(Baker, 1979; Prescher, 2001). However, Pereira
and Schabes (1992) adapted the IOA to apply
over semi-supervised data (unlabeled bracket-
ings) extracted from the PTB. They constrain
the training data (parses) considered within the
IOA to those consistent with the constituent
boundaries defined by the bracketing. One ad-
vantage of this approach is that, although less
information is derived from the treebank, it gen-
</bodyText>
<page confidence="0.980237">
23
</page>
<note confidence="0.946966">
Proceedings of the 10th Conference on Parsing Technologies, pages 23–32,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
eralizes better to parsers which make different
representational assumptions, and it is easier,
as Pereira and Schabes did, to map unlabeled
bracketings to a format more consistent with
</note>
<bodyText confidence="0.994584666666667">
the target grammar. Another is that the cost
of annotation with unlabeled brackets should be
lower than that of developing a representation-
ally richer treebank. More recently, both Riezler
et al. (2002) and Clark and Curran (2004) have
successfully trained maximum entropy parsing
models utilizing all derivations in the model con-
sistent with the annotation of the WSJ PTB,
weighting counts by the normalized probability
of the associated derivation. In this paper, we
extend this line of investigation by utilizing only
unlabeled and partial bracketing.
We compare the performance of a statisti-
cal parsing model trained from a detailed tree-
bank with that of the same model trained with
semi-supervised techniques that require only un-
labeled partially-bracketed data. We contrast
an IOA-based EM method for training a PGLR
parser (Inui et al., 1997), similar to the method
applied by Pereira and Schabes to PCFGs, to a
range of confidence-based semi-supervised meth-
ods described below. The IOA is a generaliza-
tion of the Baum-Welch or Forward-Backward
algorithm, another instance of EM, which can be
used to train Hidden Markov Models (HMMs).
Elworthy (1994) and Merialdo (1994) demon-
strated that Baum-Welch does not necessarily
improve the performance of an HMM part-of-
speech tagger when deployed in an unsuper-
vised or semi-supervised setting. These some-
what negative results, in contrast to those of
Pereira and Schabes (1992), suggest that EM
techniques require fairly determinate training
data to yield useful models. Another motiva-
tion to explore alternative non-iterative meth-
ods is that the derivation space over partially-
bracketed data can remain large (&gt;1K) while
the confidence-based methods we explore have a
total processing overhead equivalent to one iter-
ation of an IOA-based EM algorithm.
As we utilize an initial model to annotate ad-
ditional training data, our methods are closely
related to self-training methods described in the
literature (e.g. McClosky et al. 2006, Bacchi-
ani et al. 2006). However these methods have
been applied to fully-annotated training data
to create the initial model, which is then used
to annotate further training data derived from
unannotated text. Instead, we train entirely
from partially-bracketed data, starting from the
small proportion of ‘unambiguous’ data whereby
a single parse is consistent with the annota-
tion. Therefore, our methods are better de-
scribed as semi-supervised and the main focus
of this work is the flexible re-use of existing
treebanks to train a wider variety of statistical
parsing models. While many statistical parsers
extract a context-free grammar in parallel with
a statistical parse selection model, we demon-
strate that existing treebanks can be utilized to
train parsers that deploy grammars that make
other representational assumptions. As a result,
our methods can be applied by a range of parsers
to minimize the manual effort required to train
a parser or adapt to a new domain.
§2 gives details of the parsing system that are
relevant to this work. §3 and §4 describe our
data and evaluation schemes, respectively. §5
describes our semi-supervised training methods.
§6 explores the problem of tuning a parser to a
new domain. Finally, §7 gives conclusions and
future work.
</bodyText>
<sectionHeader confidence="0.909854" genericHeader="introduction">
2 The Parsing System
</sectionHeader>
<bodyText confidence="0.973470571428572">
Sentences are automatically preprocessed in a
series of modular pipelined steps, including to-
kenization, part of speech (POS) tagging, and
morphological analysis, before being passed to
the statistical parser. The parser utilizes a man-
ually written feature-based unification grammar
over POS tag sequences.
</bodyText>
<subsectionHeader confidence="0.997289">
2.1 The Parse Selection Model
</subsectionHeader>
<bodyText confidence="0.931220555555555">
A context-free ‘backbone’ is automatically de-
rived from the unification grammars and a gen-
eralized or non-deterministic LALR(1) table is
&apos;This backbone is determined by compiling out the
values of prespecified attributes. For example, if we com-
pile out the attribute PLURAL which has 2 possible val-
ues (plural or not) we will create 2 CFG rules for each
rule with categories that contain PLURAL. Therefore,
no information is lost during this process.
</bodyText>
<page confidence="0.997931">
24
</page>
<bodyText confidence="0.99965590625">
constructed from this backbone (Tomita, 1987).
The residue of features not incorporated into
the backbone are unified on each reduce action
and if unification fails the associated derivation
paths also fail. The parser creates a packed
parse forest represented as a graph-structured
stack.2 The parse selection model ranks com-
plete derivations in the parse forest by com-
puting the product of the probabilities of the
(shift/reduce) parse actions (given LR state and
lookahead item) which created each derivation
(Inui et al., 1997).
Estimating action probabilities, consists of
a) recording an action history for the correct
derivation in the parse forest (for each sen-
tence in a treebank), b) computing the fre-
quency of each action over all action histories
and c) normalizing these frequencies to deter-
mine probability distributions over conflicting
(i.e. shift/reduce or reduce/reduce) actions.
Inui et al. (1997) describe the probability
model utilized in the system where a transition
is represented by the probability of moving from
one stack state, σi−1, (an instance of the graph
structured stack) to another, σi. They estimate
this probability using the stack-top state si−1,
next input symbol li and next action ai. This
probability is conditioned on the type of state
si−1. Ss and Sr are mutually exclusive sets
of states which represent those states reached
after shift or reduce actions, respectively. The
probability of an action is estimated as:
</bodyText>
<equation confidence="0.9948965">
� P (li, ai|si−1) si−1 ∈ Ss �
P(li, ai, σi|σi−1) ≈ P(ai|si−1, li) si−1 ∈ Sr
</equation>
<bodyText confidence="0.759801214285714">
Therefore, normalization is performed over all
lookaheads for a state or over each lookahead
for the state depending on whether the state is
a member of Ss or Sr, respectively (hereafter
the I function). In addition, Laplace estimation
can be used to ensure that all actions in the
2The parse forest is an instance of a feature forest as
defined by Miyao and Tsujii (2002). We will use the term
‘node’ herein to refer to an element in a derivation tree
or in the parse forest that corresponds to a (sub-)analysis
whose label is the mother’s label in the corresponding CF
‘backbone’ rule.
table are assigned a non-zero probability (the
IL function).
</bodyText>
<sectionHeader confidence="0.948046" genericHeader="method">
3 Training Data
</sectionHeader>
<bodyText confidence="0.9998000625">
The treebanks we use in this work are in one of
two possible formats. In either case, a treebank
T consists of a set of sentences. Each sentence
t is a pair (s, M), where s is the automatically
preprocessed set of POS tagged tokens (see §2)
and M is either a fully annotated derivation, A,
or an unlabeled bracketing U. This bracketing
may be partial in the sense that it may be com-
patible with more than one derivation produced
by a given parser. Although occasionally the
bracketing is itself complete but alternative non-
terminal labeling causes indeterminacy, most of-
ten the `flatter&apos; bracketing available from ex-
tant treebanks is compatible with several alter-
native ‘deeper’ mostly binary-branching deriva-
tions output by a parser.
</bodyText>
<subsectionHeader confidence="0.999853">
3.1 Derivation Consistency
</subsectionHeader>
<bodyText confidence="0.999737666666667">
Given t = (s, A), there will exist a single deriva-
tion in the parse forest that is compatible (cor-
rect). In this case, equality between the deriva-
tion tree and the treebank annotation A iden-
tifies the correct derivation. Following Pereira
and Schabes (1992) given t = (s, U), a node’s
span in the parse forest is valid if it does not
overlap with any span outlined in U, and hence,
a derivation is correct if the span of every node
in the derivation is valid in U. That is, if no
crossing brackets are present in the derivation.
Thus, given t = (s, U), there will often be more
than one derivation compatible with the partial
bracketing.
Given the correct nodes in the parse forest
or in derivations, we can then extract the cor-
responding action histories and estimate action
probabilities as described in §2.1. In this way,
partial bracketing is used to constrain the set of
derivations considered in training to those that
are compatible with this bracketing.
</bodyText>
<subsectionHeader confidence="0.999377">
3.2 The Susanne Treebank and
Baseline Training Data
</subsectionHeader>
<bodyText confidence="0.9390085">
The Susanne Treebank (Sampson, 1995) is uti-
lized to create fully annotated training data.
</bodyText>
<page confidence="0.988469">
25
</page>
<bodyText confidence="0.999982722222222">
This treebank contains detailed syntactic deriva-
tions represented as trees, but the node label-
ing is incompatible with the system grammar.
We extracted sentences from Susanne and auto-
matically preprocessed them. A few multiwords
are retokenized, and the sentences are retagged
using the POS tagger, and the bracketing de-
terministically modified to more closely match
that of the grammar, resulting in a bracketed
corpus of 6674 sentences. We will refer to this
bracketed treebank as S, henceforth.
A fully-annotated and system compatible
treebank of 3543 sentences from S was also
created. We will refer to this annotated tree-
bank, used for fully supervised training, as B.
The system parser was applied to construct
a parse forest of analyses which are compati-
ble with the bracketing. For 1258 sentences,
the grammar writer interactively selected cor-
rect (sub)analyses within this set until a sin-
gle analysis remained. The remaining 2285 sen-
tences were automatically parsed and all consis-
tent derivations were returned. Since B contains
more than one possible derivation for roughly
two thirds of the data the 1258 sentences (paired
with a single tree) were repeated twice so that
counts from these trees were weighted more
highly. The level of reweighting was determined
experimentally using some held out data from
S. The baseline supervised model against which
we compare in this work is defined by the func-
tion IL(B) as described in §2.1. The costs of
deriving the fully-annotated treebank are high
as interactive manual disambiguation takes an
average of ten minutes per sentence, even given
the partial bracketing derived from Susanne.
</bodyText>
<subsectionHeader confidence="0.999501">
3.3 The WSJ PTB Training Data
</subsectionHeader>
<bodyText confidence="0.999567916666667">
The Wall Street Journal (WSJ) sections of the
Penn Treebank (PTB) are employed as both
training and test data by many researchers in
the field of statistical parsing. The annotated
corpus implicitly defines a grammar by provid-
ing a labeled bracketing over words annotated
with POS tags. We extracted the unlabeled
bracketing from the de facto standard training
sections (2-21 inclusive).3 We will refer to the
resulting corpus as W and the combination (con-
catenation) of the partially-bracketed corpora S
and W as SW.
</bodyText>
<subsectionHeader confidence="0.989396">
3.4 The DepBank Test Data
</subsectionHeader>
<bodyText confidence="0.9999737">
King et al. (2003) describe the development
of the PARC 700 Dependency Bank, a gold-
standard set of relational dependencies for 700
sentences (from the PTB) drawn at random
from section 23 of the WSJ (the de facto stan-
dard test set for statistical parsing). In all the
evaluations reported in this paper we test our
parser over a gold-standard set of relational de-
pendencies compatible with our parser output
derived (Briscoe and Carroll, 2006) from the
PARC 700 Dependency Bank (DepBank, hence-
forth).
The Susanne Corpus is a (balanced) subset of
the Brown Corpus which consists of 15 broad
categories of American English texts. All but
one category (reportage text) is drawn from dif-
ferent domains than the WSJ. We therefore, fol-
lowing Gildea (2001) and others, consider S, and
also the baseline training data, B, as out-of-
domain training data.
</bodyText>
<sectionHeader confidence="0.9869" genericHeader="method">
4 The Evaluation Scheme
</sectionHeader>
<bodyText confidence="0.9999876">
The parser’s output is evaluated using a rela-
tional dependency evaluation scheme (Carroll,
et al., 1998; Lin, 1998) with standard measures:
precision, recall and F1. Relations are organized
into a hierarchy with the root node specifying an
unlabeled dependency. The microaveraged pre-
cision, recall and F1 scores are calculated from
the counts for all relations in the hierarchy which
subsume the parser output. The microaveraged
F1 score for the baseline system using this eval-
uation scheme is 75.61%, which – over similar
sets of relational dependencies – is broadly com-
parable to recent evaluation results published by
King and collaborators with their state-of-the-
art parsing system (Briscoe et al., 2006).
</bodyText>
<footnote confidence="0.97163075">
3The pipeline is the same as that used for creating S
though we do not automatically map the bracketing to
be more consistent with the system grammar, instead,
we simply removed unary brackets.
</footnote>
<page confidence="0.992088">
26
</page>
<subsectionHeader confidence="0.988578">
4.1 Wilcoxon Signed Ranks Test
</subsectionHeader>
<bodyText confidence="0.997327307692308">
The Wilcoxon Signed Ranks (Wilcoxon, hence-
forth) test is a non-parametric test for statistical
significance that is appropriate when there is one
data sample and several measures. For example,
to compare the accuracy of two parsers over the
same data set. As the number of samples (sen-
tences) is large we use the normal approximation
for z. Siegel and Castellan (1988) describe and
motivate this test. We use a 0.05 level of sig-
nificance, and provide z-value probabilities for
significant results reported below. These results
are computed over microaveraged F1 scores for
each sentence in DepBank.
</bodyText>
<sectionHeader confidence="0.990635" genericHeader="method">
5 Training from Unlabeled
Bracketings
</sectionHeader>
<bodyText confidence="0.999927814814815">
We parsed all the bracketed training data us-
ing the baseline model to obtain up to 1K top-
ranked derivations and found that a significant
proportion of the sentences of the potential set
available for training had only a single deriva-
tion compatible with their unlabeled bracket-
ing. We refer to these sets as the unambiguous
training data (&apos;y) and will refer to the remaining
sentences (for which more than one derivation
was compatible with their unlabeled bracketing)
as the ambiguous training data (a). The avail-
ability of significant quantities of unambiguous
training data that can be found automatically
suggests that we may be able to dispense with
the costly reannotation step required to gener-
ate the fully supervised training corpus, B.
Table 1 illustrates the split of the corpora into
mutually exclusive sets &apos;y, a, `no match’ and
`timeout&apos;. The latter two sets are not utilized
during training and refer to sentences for which
all parses were inconsistent with the bracketing
and those for which no parses were found due
to time and memory limitations (self-imposed)
on the system.4 As our grammar is different
from that implicit in the WSJ PTB there is a
high proportion of sentences where no parses
were consistent with the unmodified PTB brack-
</bodyText>
<footnote confidence="0.945786333333333">
4As there are time and memory restrictions during
parsing, the SW results are not equal to the sum of those
from S and W analysis.
</footnote>
<table confidence="0.967632">
Corpus  |-y   ||α  |No Match Timeout
S 1097 4138 1322 191
W 6334 15152 15749 1094
SW 7409 19248 16946 1475
</table>
<tableCaption confidence="0.999864">
Table 1: Corpus split for S, W and SW.
</tableCaption>
<bodyText confidence="0.997828090909091">
eting. However, a preliminary investigation of
no matches didn’t yield any clear patterns of
inconsistency that we could quickly ameliorate
by simple modifications of the PTB bracketing.
We leave for the future a more extensive investi-
gation of these cases which, in principle, would
allow us to make more use of this training data.
An alternative approach that we have also ex-
plored is to utilize a similar bootstrapping ap-
proach with data partially-annotated for gram-
matical relations (Watson and Briscoe, 2007).
</bodyText>
<subsectionHeader confidence="0.896342">
5.1 Confidence-Based Approaches
</subsectionHeader>
<bodyText confidence="0.95219675862069">
We use &apos;y to build an initial model. We then
utilize this initial model to derive derivations
(compatible with the unlabeled partial brack-
eting) for a from which we select additional
training data. We employ two types of selection
methods. First, we select the top-ranked deriva-
tion only and weight actions which resulted in
this derivation equally with those of the initial
model (C1). This method is similar to `Viterbi
training’ of HMMs though we do not weight
the corresponding actions using the top parse’s
probability. Secondly, we select more than one
derivation, placing an appropriate weight on
the corresponding action histories based on the
initial model’s confidence in the derivation. We
consider three such models, in which we weight
transitions corresponding to each derivation
ranked r with probability p in the set of size n
either using 1�, 1r or p itself to weight counts.5
For example, given a treebank T with sentences
t = (s, U), function P to return the set of
parses consistent with U given t and function A
that returns the set of actions given a parse p,
then the frequency count of action ak in Cr is
5In §2.1 we calculate action probabilities based on fre-
quency counts where we perform a weighted sum over
action histories and each history has a weight of 1. We
extend this scheme to include weights that differ between
action histories corresponding to each derivation.
</bodyText>
<page confidence="0.990066">
27
</page>
<bodyText confidence="0.905844">
determined as follows:
</bodyText>
<equation confidence="0.991233">
 ||T |1 |P(ti) |1
ak |= �i= �j=1,akEA(pij) j
</equation>
<bodyText confidence="0.99996328125">
These methods all perform normalization over
the resulting action histories using the training
function IL and will be referred to as C, Cr
and Cp, respectively. C,,, is a ‘uniform’ model
which weights counts only by degree of ambi-
guity and makes no use of ranking information.
Cr weights counts by derivation rank, and Cp
is simpler than and different to one iteration of
EM as outside probabilities are not utilized. All
of the semi-supervised functions described here
take two arguments: an initial model and the
data to train over, respectively.
Models derived from unambiguous training
data, &apos;y, alone are relatively accurate, achiev-
ing indistinguishable performance to that of the
baseline system given either W or SW as train-
ing data. We utilize these models as initial mod-
els and train over different corpora with each of
the confidence-based models. Table 2 gives re-
sults for all models. Results statistically signifi-
cant compared to the baseline system are shown
in bold print (better) or italic print (worse).
These methods show promise, often yielding sys-
tems whose performance is significantly better
than the baseline system. Method Cr achieved
the best performance in this experiment and re-
mained consistently better in those reported be-
low. Throughout the different approaches a do-
main effect can be seen, models utilizing just S
are worse, although the best performing models
benefit from the use of both S and W as training
data (i.e. SW).
</bodyText>
<subsectionHeader confidence="0.993692">
5.2 EM
</subsectionHeader>
<bodyText confidence="0.9997959">
Our EM model differs from that of Pereira and
Schabes as a PGLR parser adds context over
a PCFG so that a single rule can be applied
in several different states containing reduce ac-
tions. Therefore, the summation and normaliza-
tion performed for a CFG rule within IOA is in-
stead applied within such contexts. We can ap-
ply I (our PGLR normalization function with-
out Laplace smoothing) to perform the required
steps if we output the action history with the
</bodyText>
<table confidence="0.999785470588235">
Model Prec Rec F1 P(z)$
Baseline 77.05 74.22 75.61 -
IL(ry(S)) 76.02 73.40 74.69 0.0294
C1(IL(ry(S)), α(S)) 77.05 74.22 75.61 0.4960
Cn(IL(ry(S)), α(S)) 77.51 74.80 76.13 0.0655
C�(IL(ry(S)), α(S)) 77.73 74.98 76.33 0.0154
Cp(IL(ry(S)),α(S)) 76.45 73.91 75.16 0.2090
IL(ry(W)) 77.01 74.31 75.64 0.1038
C1(IL(ry(W)),α(W)) 76.90 74.23 75.55 0.2546
Cn(IL(ry(W)),α(W)) 77.85 75.07 76.43 0.0017
C�(IL(ry(W)), α(W)) 77.88 75.04 76.43 0.0011
Cp(IL(ry(W)),α(W)) 77.40 74.75 76.05 0.1335
IL(ry(SW)) 77.09 74.35 75.70 0.1003
C1(IL(ry(SW)),α(SW)) 76.86 74.21 75.51 0.2483
Cn(IL(ry(SW)),α(SW)) 77.88 75.05 76.44 0.0048
C�(IL(ry(SW)),α(SW)) 78.01 75.13 76.54 0.0007
Cp(IL(ry(SW)),α(SW)) 77.54 74.95 76.23 0.0618
</table>
<tableCaption confidence="0.996164">
Table 2: Performance of all models on DepBank.
</tableCaption>
<bodyText confidence="0.988540290322581">
$represents the statistical significance of the sys-
tem against the baseline model.
corresponding normalized inside-outside weight
for each node (Watson et al., 2005).
We perform EM starting from two initial mod-
els; either a uniform probability model, IL(), or
from models derived from unambiguous train-
ing data, &apos;y. Figure 1 shows the cross entropy
decreasing monotonically from iteration 2 (as
guaranteed by the EM method) for different cor-
pora and initial models. Some models show an
initial increase in cross-entropy from iteration 1
to iteration 2, because the models are initial-
ized from a subset of the data which is used to
perform maximisation. Cross-entropy increases,
by definition, as we incorporate ambiguous data
with more than one consistent derivation.
Performance over DepBank can be seen in
Figures 2, 3, and 4 for each dataset S, W and
SW, respectively. Comparing the Cr and EM
lines in each of Figures 2, 3, and 4, it is evident
that Cr outperforms EM across all datasets, re-
gardless of the initial model applied. In most
cases, these results are significant, even when
we manually select the best model (iteration)
for EM.
The graphs of EM performance from itera-
tion 1 illustrate the same ‘classical’ and ‘initial’
patterns observed by Elworthy (1994). When
EM is initialized from a relatively poor model,
such as that built from S (Figure 2), a ‘classical’
</bodyText>
<page confidence="0.996608">
28
</page>
<figure confidence="0.99925">
0 2 4 6 8 10 12 14 16
Iteration Number
</figure>
<figureCaption confidence="0.9786555">
Figure 1: Cross Entropy Convergence for vari-
ous training data and models, with EM.
</figureCaption>
<bodyText confidence="0.9996605">
pattern emerges with relatively steady improve-
ment from iteration 1 until performance asymp-
totes. However, when the starting point is better
(Figures 3 and 4), the ‘initial’ pattern emerges
in which the best performance is reached after a
single iteration.
</bodyText>
<sectionHeader confidence="0.682041" genericHeader="method">
6 Tuning to a New Domain
</sectionHeader>
<bodyText confidence="0.999908444444444">
When building NLP applications we would want
to be able to tune a parser to a new domain
with minimal manual effort. To obtain training
data in a new domain, annotating a corpus with
partial-bracketing information is much cheaper
than full annotation. To investigate whether
such data would be of value, we considered W
to be the corpus over which we were tuning and
applied the best performing model trained over
S, Cr(IL(-y(S)), α(S)), as our initial model. Fig-
ure 5 illustrates the performance of Cr compared
to EM.
Tuning using Cr was not significantly differ-
ent from the model built directly from the entire
data set with Cr, achieving 76.57% as opposed
to 76.54% F1 (see Table 2). By contrast, EM
performs better given all the data from the be-
ginning rather than tuning to the new domain.
</bodyText>
<figure confidence="0.998038">
0 2 4 6 8 10 12 14 16
Iteration Number
</figure>
<figureCaption confidence="0.975491">
Figure 2: Performance over S for Cr and EM.
</figureCaption>
<figure confidence="0.993777125">
76.6
76.4
76.2
r
r r r r
76
F1 75.8
❜
75.6
Baseline
C*(IL(γ(W )), α(W ))
EM(IL(), W)
EM(IL(γ(W)),W)
75
0 2 4 6 8 10 12 14 16
Iteration Number
</figure>
<figureCaption confidence="0.994131">
Figure 3: Performance over W for Cr and EM.
</figureCaption>
<figure confidence="0.991313446428571">
O?
�
❝
r
EM(IL(), S
EM(IL(γ(S)), S
EM(IL(), W
EM(IL(γ(W)),W
EM(IL(), SW
EM(IL(γ(SW)), SW
r
❝
?
�
O
r
F1
76.5
75.5
74.5
76
75
74
❜
❜ ❜
r
r r
❜
❜ ❜ ❜ ❜ ❜ ❜ ❜ ❜ ❜ ❜ ❜❜
r
r
Baseline
C*(IL(γ(S)), α(S))
EM(IL(), S)
EM(IL(γ(S)), S)
❜
r
1.9
1.8
1.7
1.6
H(C,G)
1.5
1.4
1.3
1.2
75.4
75.2
❜r
r
❜
r
29
F1
0 2 4 6 8 10 12 14 16
Iteration Number
</figure>
<figureCaption confidence="0.997933">
Figure 4: Performance over SW for Cr and EM.
</figureCaption>
<figure confidence="0.9991285">
0 2 4 6 8 10 12 14 16
Iteration Number
</figure>
<figureCaption confidence="0.9489115">
Figure 5: Tuning over the WSJ PTB (W) from
Susanne Corpus (S).
</figureCaption>
<figure confidence="0.998185744680851">
76.8
76.6
76.4
76.2
75.8
75.6
75.4
75.2
76
75
❜
❜
r r ❜ r
❜
Baseline
Cr(IL(7(SW)), α(SW))
EM(IL(), SW)
EM(IL(7(SW)), SW)
❜
r
F1
76.5
75.5
74.5
77
76
75
74
❝r
❜
❝
❜
r
EM(Cr(IL(7(S)), α(S)), SW)
❝❜
EM(Cr(IL(7(S)), α(S)), W)
Baseline
Cr(IL(7(SW)), α(SW))
Cr(Cr(IL(7(S)), α(S)), W)
EM(IL(7(SW)), SW)
❝❜
r
r
r r
❝
❜
r
</figure>
<bodyText confidence="0.998917923076923">
Cr generally outperforms EM, though it is worth
noting the behavior of EM given only the tun-
ing data (W) rather than the data from both do-
mains (SW). In this case, the graph illustrates a
combination of Elworthy&apos;s ‘initial’ and ‘classical’
patterns. The steep drop in performance (down
to 69.93% F1) after the first iteration is proba-
bly due to loss of information from S. However,
this run also eventually converges to similar per-
formance, suggesting that the information in S
is effectively disregarded as it forms only a small
portion of SW, and that these runs effectively
converge to a local maximum over W.
Bacchiani et al. (2006), working in a similar
framework, explore weighting the contribution
(frequency counts) of the in-domain and out-of-
domain training datasets and demonstrate that
this can have beneficial effects. Furthermore,
they also tried unsupervised tuning to the in-
domain corpus by weighting parses for it by
their normalized probability. This method is
similar to our Cp method. However, when we
tried unsupervised tuning using the WSJ and
an initial model built from S in conjunction with
our confidence-based methods, performance de-
graded significantly.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99997336">
We have presented several semi-supervised
confidence-based training methods which have
significantly improved performance over an ex-
tant (more supervised) method, while also re-
ducing the manual effort required to create
training or tuning data. We have shown
that given a medium-sized unlabeled partially
bracketed corpus, the confidence-based models
achieve superior results to those achieved with
EM applied to the same PGLR parse selection
model. Indeed, a bracketed corpus provides flex-
ibility as existing treebanks can be utilized de-
spite the incompatibility between the system
grammar and the underlying grammar of the
treebank. Mapping an incompatible annotated
treebank to a compatible partially-bracketed
corpus is relatively easy compared to mapping
to a compatible fully-annotated corpus.
An immediate benefit of this work is that
(re)training parsers with incrementally-modified
grammars based on different linguistic frame-
works should be much more straightforward –
see, for example Oepen et al. (2002) for a good
discussion of the problem. Furthermore, it sug-
gests that it may be possible to usefully tune
</bodyText>
<page confidence="0.993802">
30
</page>
<bodyText confidence="0.999177147058824">
a parser to a new domain with less annotation
effort.
Our findings support those of Elworthy (1994)
and Merialdo (1994) for POS tagging and sug-
gest that EM is not always the most suit-
able semi-supervised training method (espe-
cially when some in-domain training data is
available). The confidence-based methods were
successful because the level of noise introduced
did not outweigh the benefit of incorporating
all derivations compatible with the bracketing
in which the derivations contained a high pro-
portion of correct constituents. These findings
may not hold if the level of bracketing available
does not adequately constrain the parses consid-
ered – see Hwa (1999) for a related investigation
with EM.
In future work we intend to further investigate
the problem of tuning to a new domain, given
that minimal manual effort is a major prior-
ity. We hope to develop methods which required
no manual annotation, for example, high preci-
sion automatic partial bracketing using phrase
chunking and/or named entity recognition tech-
niques might yield enough information to sup-
port the training methods developed here.
Finally, further experiments on weighting the
contribution of each dataset might be beneficial.
For instance, Bacchiani et al. (2006) demon-
strate imrpovements in parsing accuracy with
unsupervised adaptation from unannotated data
and explore the effect of different weighting of
counts derived from the supervised and unsu-
pervised data.
</bodyText>
<sectionHeader confidence="0.994952" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999297375">
The first author is funded by the Overseas Re-
search Students Awards Scheme, and the Poyn-
ton Scholarship awarded by the Cambridge Aus-
tralia Trust in collaboration with the Cam-
bridge Commonwealth Trust. Development of
the RASP system was and is supported by the
EPSRC (grants GR/N36462, GR/N36493 and
GR/T19919).
</bodyText>
<sectionHeader confidence="0.996355" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999502511627907">
Bacchiani, M., Riley, M., Roark, B. and R.
Sproat (2006) ‘MAP adaptation of stochas-
tic grammars’, Computer Speech and Lan-
guage, vol.20.1, pp.41–68.
Baker, J. K. (1979) ‘Trainable grammars for
speech recognition’ in Klatt, D. and Wolf,
J. (eds.), Speech Communications Papers for
the 97th Meeting of the Acoustical Society of
America, MIT, Cambridge, Massachusetts,
pp. 557–550.
Briscoe, E.J., J. Carroll and R. Watson (2006)
‘The Second Release of the RASP System’,
Proceedings of ACL-Coling&apos;06, Sydney, Aus-
tralia.
Carroll, J., Briscoe, T. and Sanfilippo, A. (1998)
‘Parser evaluation: a survey and a new
proposal’, Proceedings of LREC, Granada,
pp. 447–454.
Clark, S. and J. Curran (2004) ‘Parsing the WSJ
Using CCG and Log-Linear Models’, Pro-
ceedings of 42nd Meeting of the Association
for Computational Linguistics, Barcelona,
pp. 103–110.
Collins, M. (1999) Head-driven Statistical Mod-
els for Natural Language Parsing, PhD Dis-
sertation, University of Pennsylvania.
Elworthy, D. (1994) ‘Does Baum-Welch Re-
estimation Help Taggers?&apos;, Proceedings of
ANLP, Stuttgart, Germany, pp. 53–58.
Gildea, D. (2001) ‘Corpus variation and parser
performance’, Proceedings of EMNLP, Pitts-
burgh, PA.
Hockenmaier, J. (2003) Data and models for sta-
tistical parsing with Combinatory Categorial
Grammar, PhD Dissertation, The Univer-
sity of Edinburgh.
Hwa, R. (1999) ‘Supervised grammar induction
using training data with limited constituent
information’, Proceedings of ACL, College
Park, Maryland, pp. 73–79.
Inui, K., V. Sornlertlamvanich, H. Tanaka and
T. Tokunaga (1997) `A new formalization
of probabilistic GLR parsing’, Proceedings
</reference>
<page confidence="0.997676">
31
</page>
<reference confidence="0.999735737704918">
of IWPT, MIT, Cambridge, Massachusetts,
pp. 123–134.
King, T.H., R. Crouch, S. Riezler, M. Dalrymple
and R. Kaplan (2003) ‘The PARC700 De-
pendency Bank’, Proceedings of LINC, Bu-
dapest.
Lin, D. (1998) ‘Dependency-based evaluation
of MINIPAR&apos;, Proceedings of Workshop at
LREC&apos;98 on The Evaluation of Parsing Sys-
tems, Granada, Spain.
McClosky, D., Charniak, E. and M. Johnson
(2006) `Effective self-training for parsing’,
Proceedings of HLT-NAACL, New York.
Merialdo, B. (1994) ‘Tagging English Text with
a Probabilistic Model’, Computational Lin-
guistics, vol.20.2, pp.155–171.
Miyao, Y. and J. Tsujii (2002) ‘Maximum En-
tropy Estimation for Feature Forests’, Pro-
ceedings of HLT, San Diego, California.
Oepen, S., K. Toutanova, S. Shieber, C. Man-
ning, D. Flickinger, and T. Brants (2002)
‘The LinGO Redwoods Treebank: Motiva-
tion and preliminary applications’, Proceed-
ings of COLING, Taipei, Taiwan.
Pereira, F and Y. Schabes (1992) ‘Inside-
Outside Reestimation From Partially
Bracketed Corpora’, Proceedings of ACL,
Delaware.
Prescher, D. (2001) ‘Inside-outside estimation
meets dynamic EM&apos;, Proceedings of 7th
Int. Workshop on Parsing Technologies
(IWPT01), Beijing, China.
Riezler, S., T. King, R. Kaplan, R. Crouch,
J. Maxwell III and M. Johnson (2002)
‘Parsing the Wall Street Journal using a
Lexical-Functional Grammar and Discrimi-
native Estimation Techniques’, Proceedings
of 40th Annual Meeting of the Association
for Computational Linguistics, Philadelphia,
pp. 271–278.
Sampson, G. (1995) English for the Computer,
Oxford University Press, Oxford, UK.
Siegel S. and N. J. Castellan (1988) Nonpara-
metric Statistics for the Behavioural Sci-
ences, 2nd edition, McGraw-Hill.
Tadayoshi, H., Y. Miyao and J. Tsujii (2005)
‘Adapting a probabilistic disambiguation
model of an HPSG parser to a new domain’,
Proceedings of IJCNLP, Jeju Island, Korea.
Tomita, M. (1987) `An Efficient Augmented
Context-Free Parsing Algorithm’, Computa-
tional Linguistics, vol.13(1–2), pp.31–46.
Watson, R. and E.J. Briscoe (2007) ‘Adapting
the RASP system for the CoNLL07 domain-
adaptation task’, Proceedings of EMNLP-
CoNLL-07, Prague.
Watson, R., J. Carroll and E.J. Briscoe (2005)
`Efficient extraction of grammatical rela-
tions’, Proceedings of 9th Int. Workshop on
Parsing Technologies (IWPT&apos;05), Vancou-
ver, Ca..
</reference>
<page confidence="0.999297">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476446">
<title confidence="0.9888345">Semi-supervised Training of a Statistical from Unlabeled Partially-bracketed Data</title>
<author confidence="0.997273">Rebecca Watson</author>
<author confidence="0.997273">Ted Briscoe John Carroll</author>
<affiliation confidence="0.99223">Computer Laboratory Department of Informatics University of Cambridge, UK University of Sussex, UK</affiliation>
<email confidence="0.497947">FirstName.LastName@cl.cam.ac.ukJ.A.Carroll@sussex.ac.uk</email>
<abstract confidence="0.999760904761905">We compare the accuracy of a statistical parse ranking model trained from a fully-annotated portion of the Susanne treebank with one trained from unlabeled partially-bracketed sentences derived from this treebank and from the Penn Treebank. We demonstrate that techniques similar to self-training outperform expectation maximization when both are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a different domain and the effect of in-domain data in the semi-supervised training processes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Bacchiani</author>
<author>M Riley</author>
<author>B Roark</author>
<author>R Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars’,</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<pages>41--68</pages>
<contexts>
<context position="5629" citStr="Bacchiani et al. 2006" startWordPosition="848" endWordPosition="852">ast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm. As we utilize an initial model to annotate additional training data, our methods are closely related to self-training methods described in the literature (e.g. McClosky et al. 2006, Bacchiani et al. 2006). However these methods have been applied to fully-annotated training data to create the initial model, which is then used to annotate further training data derived from unannotated text. Instead, we train entirely from partially-bracketed data, starting from the small proportion of ‘unambiguous’ data whereby a single parse is consistent with the annotation. Therefore, our methods are better described as semi-supervised and the main focus of this work is the flexible re-use of existing treebanks to train a wider variety of statistical parsing models. While many statistical parsers extract a co</context>
<context position="26914" citStr="Bacchiani et al. (2006)" startWordPosition="4415" endWordPosition="4418">outperforms EM, though it is worth noting the behavior of EM given only the tuning data (W) rather than the data from both domains (SW). In this case, the graph illustrates a combination of Elworthy&apos;s ‘initial’ and ‘classical’ patterns. The steep drop in performance (down to 69.93% F1) after the first iteration is probably due to loss of information from S. However, this run also eventually converges to similar performance, suggesting that the information in S is effectively disregarded as it forms only a small portion of SW, and that these runs effectively converge to a local maximum over W. Bacchiani et al. (2006), working in a similar framework, explore weighting the contribution (frequency counts) of the in-domain and out-ofdomain training datasets and demonstrate that this can have beneficial effects. Furthermore, they also tried unsupervised tuning to the indomain corpus by weighting parses for it by their normalized probability. This method is similar to our Cp method. However, when we tried unsupervised tuning using the WSJ and an initial model built from S in conjunction with our confidence-based methods, performance degraded significantly. 7 Conclusions We have presented several semi-supervised</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Bacchiani, M., Riley, M., Roark, B. and R. Sproat (2006) ‘MAP adaptation of stochastic grammars’, Computer Speech and Language, vol.20.1, pp.41–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition’</title>
<date>1979</date>
<booktitle>Speech Communications Papers for the 97th Meeting of the Acoustical Society of America, MIT,</booktitle>
<pages>557--550</pages>
<editor>in Klatt, D. and Wolf, J. (eds.),</editor>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="2872" citStr="Baker, 1979" startWordPosition="427" endWordPosition="428"> al. (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain. Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics eralizes better to parsers which make</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, J. K. (1979) ‘Trainable grammars for speech recognition’ in Klatt, D. and Wolf, J. (eds.), Speech Communications Papers for the 97th Meeting of the Acoustical Society of America, MIT, Cambridge, Massachusetts, pp. 557–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<date>2006</date>
<booktitle>The Second Release of the RASP System’, Proceedings of ACL-Coling&apos;06,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="15558" citStr="Briscoe et al., 2006" startWordPosition="2467" endWordPosition="2470">ll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1. Relations are organized into a hierarchy with the root node specifying an unlabeled dependency. The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output. The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which – over similar sets of relational dependencies – is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006). 3The pipeline is the same as that used for creating S though we do not automatically map the bracketing to be more consistent with the system grammar, instead, we simply removed unary brackets. 26 4.1 Wilcoxon Signed Ranks Test The Wilcoxon Signed Ranks (Wilcoxon, henceforth) test is a non-parametric test for statistical significance that is appropriate when there is one data sample and several measures. For example, to compare the accuracy of two parsers over the same data set. As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) d</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Briscoe, E.J., J. Carroll and R. Watson (2006) ‘The Second Release of the RASP System’, Proceedings of ACL-Coling&apos;06, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal’,</title>
<date>1998</date>
<booktitle>Proceedings of LREC, Granada,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="14953" citStr="Carroll, et al., 1998" startWordPosition="2373" endWordPosition="2376">elational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth). The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts. All but one category (reportage text) is drawn from different domains than the WSJ. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data. 4 The Evaluation Scheme The parser’s output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1. Relations are organized into a hierarchy with the root node specifying an unlabeled dependency. The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output. The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which – over similar sets of relational dependencies – is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., </context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, J., Briscoe, T. and Sanfilippo, A. (1998) ‘Parser evaluation: a survey and a new proposal’, Proceedings of LREC, Granada, pp. 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J Curran</author>
</authors>
<title>Parsing the WSJ Using CCG and Log-Linear Models’,</title>
<date>2004</date>
<booktitle>Proceedings of 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>103--110</pages>
<location>Barcelona,</location>
<contexts>
<context position="3851" citStr="Clark and Curran (2004)" startWordPosition="572" endWordPosition="575">nformation is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser </context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, S. and J. Curran (2004) ‘Parsing the WSJ Using CCG and Log-Linear Models’, Proceedings of 42nd Meeting of the Association for Computational Linguistics, Barcelona, pp. 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing,</title>
<date>1999</date>
<institution>PhD Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="1284" citStr="Collins (1999)" startWordPosition="171" endWordPosition="172">oth are constrained by partial bracketing. Both methods based on partially-bracketed training data outperform the fully supervised technique, and both can, in principle, be applied to any statistical parser whose output is consistent with such partial-bracketing. We also explore tuning the model to a different domain and the effect of in-domain data in the semi-supervised training processes. 1 Introduction Extant statistical parsers require extensive and detailed treebanks, as many of their lexical and structural parameters are estimated in a fullysupervised fashion from treebank derivations. Collins (1999) is a detailed exposition of one such ongoing line of research which utilizes the Wall Street Journal (WSJ) sections of the Penn Treebank (PTB). However, there are disadvantages to this approach. Firstly, treebanks are expensive to create manually. Secondly, the richer the annotation required, the harder it is to adapt the treebank to train parsers which make different assumptions about the structure of syntactic analyses. For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. (1999) Head-driven Statistical Models for Natural Language Parsing, PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch Reestimation Help Taggers?&apos;,</title>
<date>1994</date>
<booktitle>Proceedings of ANLP,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany,</location>
<contexts>
<context position="4779" citStr="Elworthy (1994)" startWordPosition="719" endWordPosition="720">bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent t</context>
<context position="23908" citStr="Elworthy (1994)" startWordPosition="3836" endWordPosition="3837">ss-entropy increases, by definition, as we incorporate ambiguous data with more than one consistent derivation. Performance over DepBank can be seen in Figures 2, 3, and 4 for each dataset S, W and SW, respectively. Comparing the Cr and EM lines in each of Figures 2, 3, and 4, it is evident that Cr outperforms EM across all datasets, regardless of the initial model applied. In most cases, these results are significant, even when we manually select the best model (iteration) for EM. The graphs of EM performance from iteration 1 illustrate the same ‘classical’ and ‘initial’ patterns observed by Elworthy (1994). When EM is initialized from a relatively poor model, such as that built from S (Figure 2), a ‘classical’ 28 0 2 4 6 8 10 12 14 16 Iteration Number Figure 1: Cross Entropy Convergence for various training data and models, with EM. pattern emerges with relatively steady improvement from iteration 1 until performance asymptotes. However, when the starting point is better (Figures 3 and 4), the ‘initial’ pattern emerges in which the best performance is reached after a single iteration. 6 Tuning to a New Domain When building NLP applications we would want to be able to tune a parser to a new doma</context>
<context position="28695" citStr="Elworthy (1994)" startWordPosition="4682" endWordPosition="4683">ng grammar of the treebank. Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus. An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward – see, for example Oepen et al. (2002) for a good discussion of the problem. Furthermore, it suggests that it may be possible to usefully tune 30 a parser to a new domain with less annotation effort. Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. These findings may not hold if the level of bracketing available does not adequately constrain the parses considered – see Hwa (1999) for a related investigation wi</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, D. (1994) ‘Does Baum-Welch Reestimation Help Taggers?&apos;, Proceedings of ANLP, Stuttgart, Germany, pp. 53–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance’,</title>
<date>2001</date>
<booktitle>Proceedings of EMNLP,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="2011" citStr="Gildea (2001)" startWordPosition="284" endWordPosition="285">ns of the Penn Treebank (PTB). However, there are disadvantages to this approach. Firstly, treebanks are expensive to create manually. Secondly, the richer the annotation required, the harder it is to adapt the treebank to train parsers which make different assumptions about the structure of syntactic analyses. For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically. Thirdly, many (lexical) parameter estimates do not generalize well between domains. For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus. Tadayoshi et al. (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost</context>
<context position="14730" citStr="Gildea (2001)" startWordPosition="2339" endWordPosition="2340">nces (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing). In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth). The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts. All but one category (reportage text) is drawn from different domains than the WSJ. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data. 4 The Evaluation Scheme The parser’s output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1. Relations are organized into a hierarchy with the root node specifying an unlabeled dependency. The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output. The microaveraged F1 score for the baseline system using this evaluat</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, D. (2001) ‘Corpus variation and parser performance’, Proceedings of EMNLP, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
</authors>
<title>Data and models for statistical parsing with Combinatory Categorial Grammar,</title>
<date>2003</date>
<institution>PhD Dissertation, The University of Edinburgh.</institution>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, J. (2003) Data and models for statistical parsing with Combinatory Categorial Grammar, PhD Dissertation, The University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Supervised grammar induction using training data with limited constituent information’,</title>
<date>1999</date>
<booktitle>Proceedings of ACL, College</booktitle>
<pages>73--79</pages>
<location>Park, Maryland,</location>
<contexts>
<context position="29264" citStr="Hwa (1999)" startWordPosition="4771" endWordPosition="4772">findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. These findings may not hold if the level of bracketing available does not adequately constrain the parses considered – see Hwa (1999) for a related investigation with EM. In future work we intend to further investigate the problem of tuning to a new domain, given that minimal manual effort is a major priority. We hope to develop methods which required no manual annotation, for example, high precision automatic partial bracketing using phrase chunking and/or named entity recognition techniques might yield enough information to support the training methods developed here. Finally, further experiments on weighting the contribution of each dataset might be beneficial. For instance, Bacchiani et al. (2006) demonstrate imrpovemen</context>
</contexts>
<marker>Hwa, 1999</marker>
<rawString>Hwa, R. (1999) ‘Supervised grammar induction using training data with limited constituent information’, Proceedings of ACL, College Park, Maryland, pp. 73–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>A new formalization of probabilistic GLR parsing’,</title>
<date>1997</date>
<booktitle>Proceedings of IWPT, MIT,</booktitle>
<pages>123--134</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="4470" citStr="Inui et al., 1997" startWordPosition="668" endWordPosition="671">have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM tec</context>
<context position="8224" citStr="Inui et al., 1997" startWordPosition="1255" endWordPosition="1258">egories that contain PLURAL. Therefore, no information is lost during this process. 24 constructed from this backbone (Tomita, 1987). The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail. The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997). Estimating action probabilities, consists of a) recording an action history for the correct derivation in the parse forest (for each sentence in a treebank), b) computing the frequency of each action over all action histories and c) normalizing these frequencies to determine probability distributions over conflicting (i.e. shift/reduce or reduce/reduce) actions. Inui et al. (1997) describe the probability model utilized in the system where a transition is represented by the probability of moving from one stack state, σi−1, (an instance of the graph structured stack) to another, σi. They esti</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>Inui, K., V. Sornlertlamvanich, H. Tanaka and T. Tokunaga (1997) `A new formalization of probabilistic GLR parsing’, Proceedings of IWPT, MIT, Cambridge, Massachusetts, pp. 123–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H King</author>
<author>R Crouch</author>
<author>S Riezler</author>
<author>M Dalrymple</author>
<author>R Kaplan</author>
</authors>
<date>2003</date>
<booktitle>The PARC700 Dependency Bank’, Proceedings of LINC,</booktitle>
<location>Budapest.</location>
<contexts>
<context position="13999" citStr="King et al. (2003)" startWordPosition="2215" endWordPosition="2218">ing derived from Susanne. 3.3 The WSJ PTB Training Data The Wall Street Journal (WSJ) sections of the Penn Treebank (PTB) are employed as both training and test data by many researchers in the field of statistical parsing. The annotated corpus implicitly defines a grammar by providing a labeled bracketing over words annotated with POS tags. We extracted the unlabeled bracketing from the de facto standard training sections (2-21 inclusive).3 We will refer to the resulting corpus as W and the combination (concatenation) of the partially-bracketed corpora S and W as SW. 3.4 The DepBank Test Data King et al. (2003) describe the development of the PARC 700 Dependency Bank, a goldstandard set of relational dependencies for 700 sentences (from the PTB) drawn at random from section 23 of the WSJ (the de facto standard test set for statistical parsing). In all the evaluations reported in this paper we test our parser over a gold-standard set of relational dependencies compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth). The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American Engli</context>
</contexts>
<marker>King, Crouch, Riezler, Dalrymple, Kaplan, 2003</marker>
<rawString>King, T.H., R. Crouch, S. Riezler, M. Dalrymple and R. Kaplan (2003) ‘The PARC700 Dependency Bank’, Proceedings of LINC, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR&apos;,</title>
<date>1998</date>
<booktitle>Proceedings of Workshop at LREC&apos;98 on The Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="14965" citStr="Lin, 1998" startWordPosition="2377" endWordPosition="2378">compatible with our parser output derived (Briscoe and Carroll, 2006) from the PARC 700 Dependency Bank (DepBank, henceforth). The Susanne Corpus is a (balanced) subset of the Brown Corpus which consists of 15 broad categories of American English texts. All but one category (reportage text) is drawn from different domains than the WSJ. We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-ofdomain training data. 4 The Evaluation Scheme The parser’s output is evaluated using a relational dependency evaluation scheme (Carroll, et al., 1998; Lin, 1998) with standard measures: precision, recall and F1. Relations are organized into a hierarchy with the root node specifying an unlabeled dependency. The microaveraged precision, recall and F1 scores are calculated from the counts for all relations in the hierarchy which subsume the parser output. The microaveraged F1 score for the baseline system using this evaluation scheme is 75.61%, which – over similar sets of relational dependencies – is broadly comparable to recent evaluation results published by King and collaborators with their state-of-theart parsing system (Briscoe et al., 2006). 3The </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998) ‘Dependency-based evaluation of MINIPAR&apos;, Proceedings of Workshop at LREC&apos;98 on The Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing’,</title>
<date>2006</date>
<booktitle>Proceedings of HLT-NAACL,</booktitle>
<location>New York.</location>
<contexts>
<context position="5605" citStr="McClosky et al. 2006" startWordPosition="844" endWordPosition="847">tive results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm. As we utilize an initial model to annotate additional training data, our methods are closely related to self-training methods described in the literature (e.g. McClosky et al. 2006, Bacchiani et al. 2006). However these methods have been applied to fully-annotated training data to create the initial model, which is then used to annotate further training data derived from unannotated text. Instead, we train entirely from partially-bracketed data, starting from the small proportion of ‘unambiguous’ data whereby a single parse is consistent with the annotation. Therefore, our methods are better described as semi-supervised and the main focus of this work is the flexible re-use of existing treebanks to train a wider variety of statistical parsing models. While many statisti</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>McClosky, D., Charniak, E. and M. Johnson (2006) `Effective self-training for parsing’, Proceedings of HLT-NAACL, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model’,</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>155--171</pages>
<contexts>
<context position="4799" citStr="Merialdo (1994)" startWordPosition="722" endWordPosition="723">re the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of a</context>
<context position="28715" citStr="Merialdo (1994)" startWordPosition="4685" endWordPosition="4686">eebank. Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus. An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward – see, for example Oepen et al. (2002) for a good discussion of the problem. Furthermore, it suggests that it may be possible to usefully tune 30 a parser to a new domain with less annotation effort. Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained a high proportion of correct constituents. These findings may not hold if the level of bracketing available does not adequately constrain the parses considered – see Hwa (1999) for a related investigation with EM. In future wor</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, B. (1994) ‘Tagging English Text with a Probabilistic Model’, Computational Linguistics, vol.20.2, pp.155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Maximum Entropy Estimation for Feature Forests’,</title>
<date>2002</date>
<booktitle>Proceedings of HLT,</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="9602" citStr="Miyao and Tsujii (2002)" startWordPosition="1483" endWordPosition="1486">s and Sr are mutually exclusive sets of states which represent those states reached after shift or reduce actions, respectively. The probability of an action is estimated as: � P (li, ai|si−1) si−1 ∈ Ss � P(li, ai, σi|σi−1) ≈ P(ai|si−1, li) si−1 ∈ Sr Therefore, normalization is performed over all lookaheads for a state or over each lookahead for the state depending on whether the state is a member of Ss or Sr, respectively (hereafter the I function). In addition, Laplace estimation can be used to ensure that all actions in the 2The parse forest is an instance of a feature forest as defined by Miyao and Tsujii (2002). We will use the term ‘node’ herein to refer to an element in a derivation tree or in the parse forest that corresponds to a (sub-)analysis whose label is the mother’s label in the corresponding CF ‘backbone’ rule. table are assigned a non-zero probability (the IL function). 3 Training Data The treebanks we use in this work are in one of two possible formats. In either case, a treebank T consists of a set of sentences. Each sentence t is a pair (s, M), where s is the automatically preprocessed set of POS tagged tokens (see §2) and M is either a fully annotated derivation, A, or an unlabeled b</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Miyao, Y. and J. Tsujii (2002) ‘Maximum Entropy Estimation for Feature Forests’, Proceedings of HLT, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>K Toutanova</author>
<author>S Shieber</author>
<author>C Manning</author>
<author>D Flickinger</author>
<author>T Brants</author>
</authors>
<title>The LinGO Redwoods Treebank: Motivation and preliminary applications’,</title>
<date>2002</date>
<booktitle>Proceedings of COLING,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="28488" citStr="Oepen et al. (2002)" startWordPosition="4643" endWordPosition="4646">th EM applied to the same PGLR parse selection model. Indeed, a bracketed corpus provides flexibility as existing treebanks can be utilized despite the incompatibility between the system grammar and the underlying grammar of the treebank. Mapping an incompatible annotated treebank to a compatible partially-bracketed corpus is relatively easy compared to mapping to a compatible fully-annotated corpus. An immediate benefit of this work is that (re)training parsers with incrementally-modified grammars based on different linguistic frameworks should be much more straightforward – see, for example Oepen et al. (2002) for a good discussion of the problem. Furthermore, it suggests that it may be possible to usefully tune 30 a parser to a new domain with less annotation effort. Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). The confidence-based methods were successful because the level of noise introduced did not outweigh the benefit of incorporating all derivations compatible with the bracketing in which the derivations contained </context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Oepen, S., K. Toutanova, S. Shieber, C. Manning, D. Flickinger, and T. Brants (2002) ‘The LinGO Redwoods Treebank: Motivation and preliminary applications’, Proceedings of COLING, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>InsideOutside Reestimation From Partially Bracketed Corpora’,</title>
<date>1992</date>
<booktitle>Proceedings of ACL,</booktitle>
<location>Delaware.</location>
<contexts>
<context position="2926" citStr="Pereira and Schabes (1992)" startWordPosition="432" endWordPosition="435">trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain. Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is eas</context>
<context position="5049" citStr="Pereira and Schabes (1992)" startWordPosition="759" endWordPosition="762"> for training a PGLR parser (Inui et al., 1997), similar to the method applied by Pereira and Schabes to PCFGs, to a range of confidence-based semi-supervised methods described below. The IOA is a generalization of the Baum-Welch or Forward-Backward algorithm, another instance of EM, which can be used to train Hidden Markov Models (HMMs). Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-ofspeech tagger when deployed in an unsupervised or semi-supervised setting. These somewhat negative results, in contrast to those of Pereira and Schabes (1992), suggest that EM techniques require fairly determinate training data to yield useful models. Another motivation to explore alternative non-iterative methods is that the derivation space over partiallybracketed data can remain large (&gt;1K) while the confidence-based methods we explore have a total processing overhead equivalent to one iteration of an IOA-based EM algorithm. As we utilize an initial model to annotate additional training data, our methods are closely related to self-training methods described in the literature (e.g. McClosky et al. 2006, Bacchiani et al. 2006). However these meth</context>
<context position="10914" citStr="Pereira and Schabes (1992)" startWordPosition="1706" endWordPosition="1709">h more than one derivation produced by a given parser. Although occasionally the bracketing is itself complete but alternative nonterminal labeling causes indeterminacy, most often the `flatter&apos; bracketing available from extant treebanks is compatible with several alternative ‘deeper’ mostly binary-branching derivations output by a parser. 3.1 Derivation Consistency Given t = (s, A), there will exist a single derivation in the parse forest that is compatible (correct). In this case, equality between the derivation tree and the treebank annotation A identifies the correct derivation. Following Pereira and Schabes (1992) given t = (s, U), a node’s span in the parse forest is valid if it does not overlap with any span outlined in U, and hence, a derivation is correct if the span of every node in the derivation is valid in U. That is, if no crossing brackets are present in the derivation. Thus, given t = (s, U), there will often be more than one derivation compatible with the partial bracketing. Given the correct nodes in the parse forest or in derivations, we can then extract the corresponding action histories and estimate action probabilities as described in §2.1. In this way, partial bracketing is used to co</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Pereira, F and Y. Schabes (1992) ‘InsideOutside Reestimation From Partially Bracketed Corpora’, Proceedings of ACL, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Prescher</author>
</authors>
<title>Inside-outside estimation meets dynamic EM&apos;,</title>
<date>2001</date>
<booktitle>Proceedings of 7th Int. Workshop on Parsing Technologies (IWPT01),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="2889" citStr="Prescher, 2001" startWordPosition="429" endWordPosition="430">dapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain. Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979; Prescher, 2001). However, Pereira and Schabes (1992) adapted the IOA to apply over semi-supervised data (unlabeled bracketings) extracted from the PTB. They constrain the training data (parses) considered within the IOA to those consistent with the constituent boundaries defined by the bracketing. One advantage of this approach is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics eralizes better to parsers which make different repres</context>
</contexts>
<marker>Prescher, 2001</marker>
<rawString>Prescher, D. (2001) ‘Inside-outside estimation meets dynamic EM&apos;, Proceedings of 7th Int. Workshop on Parsing Technologies (IWPT01), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T King</author>
<author>R Kaplan</author>
<author>R Crouch</author>
<author>J Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques’,</title>
<date>2002</date>
<booktitle>Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<location>Philadelphia,</location>
<contexts>
<context position="3823" citStr="Riezler et al. (2002)" startWordPosition="567" endWordPosition="570">h is that, although less information is derived from the treebank, it gen23 Proceedings of the 10th Conference on Parsing Technologies, pages 23–32, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics eralizes better to parsers which make different representational assumptions, and it is easier, as Pereira and Schabes did, to map unlabeled bracketings to a format more consistent with the target grammar. Another is that the cost of annotation with unlabeled brackets should be lower than that of developing a representationally richer treebank. More recently, both Riezler et al. (2002) and Clark and Curran (2004) have successfully trained maximum entropy parsing models utilizing all derivations in the model consistent with the annotation of the WSJ PTB, weighting counts by the normalized probability of the associated derivation. In this paper, we extend this line of investigation by utilizing only unlabeled and partial bracketing. We compare the performance of a statistical parsing model trained from a detailed treebank with that of the same model trained with semi-supervised techniques that require only unlabeled partially-bracketed data. We contrast an IOA-based EM method</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Riezler, S., T. King, R. Kaplan, R. Crouch, J. Maxwell III and M. Johnson (2002) ‘Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques’, Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, pp. 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the Computer,</title>
<date>1995</date>
<publisher>University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="11707" citStr="Sampson, 1995" startWordPosition="1847" endWordPosition="1848">he derivation is valid in U. That is, if no crossing brackets are present in the derivation. Thus, given t = (s, U), there will often be more than one derivation compatible with the partial bracketing. Given the correct nodes in the parse forest or in derivations, we can then extract the corresponding action histories and estimate action probabilities as described in §2.1. In this way, partial bracketing is used to constrain the set of derivations considered in training to those that are compatible with this bracketing. 3.2 The Susanne Treebank and Baseline Training Data The Susanne Treebank (Sampson, 1995) is utilized to create fully annotated training data. 25 This treebank contains detailed syntactic derivations represented as trees, but the node labeling is incompatible with the system grammar. We extracted sentences from Susanne and automatically preprocessed them. A few multiwords are retokenized, and the sentences are retagged using the POS tagger, and the bracketing deterministically modified to more closely match that of the grammar, resulting in a bracketed corpus of 6674 sentences. We will refer to this bracketed treebank as S, henceforth. A fully-annotated and system compatible treeb</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, G. (1995) English for the Computer, Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>N J Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioural Sciences, 2nd edition,</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="16156" citStr="Siegel and Castellan (1988)" startWordPosition="2567" endWordPosition="2570">system (Briscoe et al., 2006). 3The pipeline is the same as that used for creating S though we do not automatically map the bracketing to be more consistent with the system grammar, instead, we simply removed unary brackets. 26 4.1 Wilcoxon Signed Ranks Test The Wilcoxon Signed Ranks (Wilcoxon, henceforth) test is a non-parametric test for statistical significance that is appropriate when there is one data sample and several measures. For example, to compare the accuracy of two parsers over the same data set. As the number of samples (sentences) is large we use the normal approximation for z. Siegel and Castellan (1988) describe and motivate this test. We use a 0.05 level of significance, and provide z-value probabilities for significant results reported below. These results are computed over microaveraged F1 scores for each sentence in DepBank. 5 Training from Unlabeled Bracketings We parsed all the bracketed training data using the baseline model to obtain up to 1K topranked derivations and found that a significant proportion of the sentences of the potential set available for training had only a single derivation compatible with their unlabeled bracketing. We refer to these sets as the unambiguous trainin</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel S. and N. J. Castellan (1988) Nonparametric Statistics for the Behavioural Sciences, 2nd edition, McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tadayoshi</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Adapting a probabilistic disambiguation model of an HPSG parser to a new domain’,</title>
<date>2005</date>
<booktitle>Proceedings of IJCNLP, Jeju Island,</booktitle>
<contexts>
<context position="2272" citStr="Tadayoshi et al. (2005)" startWordPosition="327" endWordPosition="330">ferent assumptions about the structure of syntactic analyses. For example, Hockenmeier (2003) trains a statistical parser based on Combinatory Categorial Grammar (CCG) on the WSJ PTB, but first maps the treebank to CCG derivations semi-automatically. Thirdly, many (lexical) parameter estimates do not generalize well between domains. For instance, Gildea (2001) reports that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute about 1% to parse selection accuracy when test data is in the same domain, but yield no improvement for test data selected from the Brown Corpus. Tadayoshi et al. (2005) adapt a statistical parser trained on the WSJ PTB to the biomedical domain by retraining on the Genia Corpus, augmented with manually corrected derivations in the same format. To make statistical parsing more viable for a range of applications, we need to make more effective and flexible use of extant training data and minimize the cost of annotation for new data created to tune a system to a new domain. Unsupervised methods for training parsers have been relatively unsuccessful to date, including expectation maximization (EM) such as the inside-outside algorithm (IOA) over PCFGs (Baker, 1979</context>
</contexts>
<marker>Tadayoshi, Miyao, Tsujii, 2005</marker>
<rawString>Tadayoshi, H., Y. Miyao and J. Tsujii (2005) ‘Adapting a probabilistic disambiguation model of an HPSG parser to a new domain’, Proceedings of IJCNLP, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient Augmented Context-Free Parsing Algorithm’,</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="7738" citStr="Tomita, 1987" startWordPosition="1181" endWordPosition="1182">ilizes a manually written feature-based unification grammar over POS tag sequences. 2.1 The Parse Selection Model A context-free ‘backbone’ is automatically derived from the unification grammars and a generalized or non-deterministic LALR(1) table is &apos;This backbone is determined by compiling out the values of prespecified attributes. For example, if we compile out the attribute PLURAL which has 2 possible values (plural or not) we will create 2 CFG rules for each rule with categories that contain PLURAL. Therefore, no information is lost during this process. 24 constructed from this backbone (Tomita, 1987). The residue of features not incorporated into the backbone are unified on each reduce action and if unification fails the associated derivation paths also fail. The parser creates a packed parse forest represented as a graph-structured stack.2 The parse selection model ranks complete derivations in the parse forest by computing the product of the probabilities of the (shift/reduce) parse actions (given LR state and lookahead item) which created each derivation (Inui et al., 1997). Estimating action probabilities, consists of a) recording an action history for the correct derivation in the pa</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Tomita, M. (1987) `An Efficient Augmented Context-Free Parsing Algorithm’, Computational Linguistics, vol.13(1–2), pp.31–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
<author>E J Briscoe</author>
</authors>
<title>Adapting the RASP system for the CoNLL07 domainadaptation task’,</title>
<date>2007</date>
<booktitle>Proceedings of EMNLPCoNLL-07,</booktitle>
<location>Prague.</location>
<contexts>
<context position="18469" citStr="Watson and Briscoe, 2007" startWordPosition="2953" endWordPosition="2956">ut S 1097 4138 1322 191 W 6334 15152 15749 1094 SW 7409 19248 16946 1475 Table 1: Corpus split for S, W and SW. eting. However, a preliminary investigation of no matches didn’t yield any clear patterns of inconsistency that we could quickly ameliorate by simple modifications of the PTB bracketing. We leave for the future a more extensive investigation of these cases which, in principle, would allow us to make more use of this training data. An alternative approach that we have also explored is to utilize a similar bootstrapping approach with data partially-annotated for grammatical relations (Watson and Briscoe, 2007). 5.1 Confidence-Based Approaches We use &apos;y to build an initial model. We then utilize this initial model to derive derivations (compatible with the unlabeled partial bracketing) for a from which we select additional training data. We employ two types of selection methods. First, we select the top-ranked derivation only and weight actions which resulted in this derivation equally with those of the initial model (C1). This method is similar to `Viterbi training’ of HMMs though we do not weight the corresponding actions using the top parse’s probability. Secondly, we select more than one derivat</context>
</contexts>
<marker>Watson, Briscoe, 2007</marker>
<rawString>Watson, R. and E.J. Briscoe (2007) ‘Adapting the RASP system for the CoNLL07 domainadaptation task’, Proceedings of EMNLPCoNLL-07, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Watson</author>
<author>J Carroll</author>
<author>E J Briscoe</author>
</authors>
<title>Efficient extraction of grammatical relations’,</title>
<date>2005</date>
<booktitle>Proceedings of 9th Int. Workshop on Parsing Technologies (IWPT&apos;05),</booktitle>
<location>Vancouver, Ca..</location>
<contexts>
<context position="22802" citStr="Watson et al., 2005" startWordPosition="3649" endWordPosition="3652">.64 0.1038 C1(IL(ry(W)),α(W)) 76.90 74.23 75.55 0.2546 Cn(IL(ry(W)),α(W)) 77.85 75.07 76.43 0.0017 C�(IL(ry(W)), α(W)) 77.88 75.04 76.43 0.0011 Cp(IL(ry(W)),α(W)) 77.40 74.75 76.05 0.1335 IL(ry(SW)) 77.09 74.35 75.70 0.1003 C1(IL(ry(SW)),α(SW)) 76.86 74.21 75.51 0.2483 Cn(IL(ry(SW)),α(SW)) 77.88 75.05 76.44 0.0048 C�(IL(ry(SW)),α(SW)) 78.01 75.13 76.54 0.0007 Cp(IL(ry(SW)),α(SW)) 77.54 74.95 76.23 0.0618 Table 2: Performance of all models on DepBank. $represents the statistical significance of the system against the baseline model. corresponding normalized inside-outside weight for each node (Watson et al., 2005). We perform EM starting from two initial models; either a uniform probability model, IL(), or from models derived from unambiguous training data, &apos;y. Figure 1 shows the cross entropy decreasing monotonically from iteration 2 (as guaranteed by the EM method) for different corpora and initial models. Some models show an initial increase in cross-entropy from iteration 1 to iteration 2, because the models are initialized from a subset of the data which is used to perform maximisation. Cross-entropy increases, by definition, as we incorporate ambiguous data with more than one consistent derivatio</context>
</contexts>
<marker>Watson, Carroll, Briscoe, 2005</marker>
<rawString>Watson, R., J. Carroll and E.J. Briscoe (2005) `Efficient extraction of grammatical relations’, Proceedings of 9th Int. Workshop on Parsing Technologies (IWPT&apos;05), Vancouver, Ca..</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>