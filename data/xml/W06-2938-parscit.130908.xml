<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011032">
<title confidence="0.993704">
Dependency Parsing as a Classification Problem
</title>
<author confidence="0.969654">
Deniz Yuret
</author>
<affiliation confidence="0.812679">
Koc¸ University
Istanbul, Turkey
</affiliation>
<email confidence="0.994778">
dyuret@ku.edu.tr
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957222222222">
This paper presents an approach to depen-
dency parsing which can utilize any stan-
dard machine learning (classification) al-
gorithm. A decision list learner was used
in this work. The training data provided
in the form of a treebank is converted to a
format in which each instance represents
information about one word pair, and the
classification indicates the existence, di-
rection, and type of the link between the
words of the pair. Several distinct mod-
els are built to identify the links between
word pairs at different distances. These
models are applied sequentially to give the
dependency parse of a sentence, favoring
shorter links. An analysis of the errors,
attribute selection, and comparison of dif-
ferent languages is presented.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999363545454545">
This paper presents an approach to supervised learn-
ing of dependency relations in a language using stan-
dard machine learning techniques. The treebanks
(Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a
et al., 2003; Kromann, 2003; van der Beek et al.,
2002; Brants et al., 2002; Kawata and Bartels, 2000;
Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Tor-
ruella and MartiAntonin, 2002; Nilsson et al., 2005;
Oflazer et al., 2003; Atalay et al., 2003) provided for
the CoNLL shared task(Buchholz et al., 2006) were
converted to a set of instances each of which con-
sists of the attributes of a candidate word pair with
a classification that indicates the existence, direction
and type of the dependency link between the pair.
An initial model is built to identify dependency
relations between adjacent word pairs using a deci-
sion list learning algorithm. To identify longer dis-
tance relations, the adjacent modifiers are dropped
from the sentence and a second order model is built
based on the word pairs that come into contact. A
total of three models were built using this technique
successively and used for parsing.
All given attributes are considered as candidates
in an attribute selection process before building each
model. In addition, attributes indicating suffixes of
various lengths and character type information were
constructed and used.
To parse a given sentence, the models are applied
sequentially, each one considering candidate word
pairs and adding new links without deleting the ex-
isting links or creating conflicts (cycles or crossings)
with them. Thus, the algorithm can be considered a
bottom-up, multi-pass, deterministic parser. Given
a candidate word pair, the models may output “no
link”, or give a link with a specified direction and
type. Thus labeling is an integrated step. Word
pair candidates that may form cycles or crossings
are never considered, so the parser will only gen-
erate projective structures.
Section 2 gives the details of the learning algo-
rithm. Section 3 describes the first pass model of
links between adjacent words. Section 4 details
the approach for identifying long distance links and
presents the parsing results.
</bodyText>
<page confidence="0.984049">
246
</page>
<note confidence="0.4167415">
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 246–250, New York City, June 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.889577" genericHeader="introduction">
2 The Learning Algorithm
</sectionHeader>
<bodyText confidence="0.998290285714286">
The Greedy Prepend Algorithm (Yuret and Ture,
2006) was used to build decision lists to identify de-
pendency relations. A decision list is an ordered list
of rules where each rule consists of a pattern and a
classification (Rivest, 1987). The first rule whose
pattern matches a given instance is used for its clas-
sification. In our application the pattern specifies the
attributes of the two words to be linked such as parts
of speech and morphological features. The classi-
fication indicates the existence and the type of the
dependency link between the two words.
Table 1 gives a subset of the decision list that iden-
tifies links between adjacent words in German. The
class column indicates the type of the link, the pat-
tern contains attributes of the two candidate words X
and Y, as well as their neighbors (XL1 indicates the
left neighbor of X). For example, given the part of
speech sequence APPR-ART-NN, there would be an
NK link between APPR and ART (matches rule 3), but
there would be no link between ART and NN (rule 1
overrides rule 2).
</bodyText>
<sectionHeader confidence="0.9725596" genericHeader="method">
Rule Class Pattern
1 NONE XL1:postag=APPR
2 L:NK X:postag=ART Y:postag=NN
3 R:NK X:postag=APPR
4 NONE
</sectionHeader>
<tableCaption confidence="0.9647605">
Table 1: A four rule decision list for adjacent word
dependencies in German
</tableCaption>
<bodyText confidence="0.998015285714286">
The average training instance for the depen-
dency problem has over 40 attributes describing the
two candidate words including suffixes of different
lengths, parts of speech and information on neigh-
boring words. Most of this information may be re-
dundant or irrelevant to the problem at hand. The
number of distinct attribute values is on the order
of the number of distinct word-forms in the train-
ing set. GPA was picked for this problem because
it has proven to be fairly efficient and robust in the
presence of irrelevant or redundant attributes in pre-
vious work such as morphological disambiguation
in Turkish (Yuret and Ture, 2006) and protein sec-
ondary structure prediction (Kurt, 2005).
</bodyText>
<sectionHeader confidence="0.942162" genericHeader="method">
3 Dependency of Adjacent Words
</sectionHeader>
<bodyText confidence="0.999800952380952">
We start by looking at adjacent words and try to pre-
dict whether they are linked, and if they are, what
type of link they have. This is a nice subproblem to
study because: (i) It is easily converted to a standard
machine learning problem, thus amenable to com-
mon machine learning techniques and analysis, (ii)
It demonstrates the differences between languages
and the impact of various attributes. The machine
learning algorithm used was GPA (See Section 2)
which builds decision lists.
Table 2 shows the percentage of adjacent tokens
that are linked in the training sets for the languages
studied&apos;. Most languages have approximately half
of the adjacent words linked. German, with 42.15%
is at the low end whereas Arabic and Turkish with
above 60% are at the high end. The differences may
be due to linguistic factors such as the ubiquity of
function words which prefer short distance links, or
it may be an accident of data representation: for ex-
ample each token in the Turkish data represents an
inflectional group, not a whole word.
</bodyText>
<table confidence="0.995215666666667">
Arabic 61.02 Japanese 54.81
Chinese 56.59 Portuguese 50.81
Czech 48.73 Slovene 45.62
Danish 55.93 Spanish 51.28
Dutch 55.54 Swedish 48.26
German 42.15 Turkish 62.60
</table>
<tableCaption confidence="0.999167">
Table 2: Percentage of adjacent tokens linked.
</tableCaption>
<subsectionHeader confidence="0.983881">
3.1 Attributes
</subsectionHeader>
<bodyText confidence="0.999977692307692">
The five attributes provided for each word in the
treebanks were the wordform, the lemma, the
coarse-grained and fine-grained parts of speech, and
a list of syntactic and/or morphological features. In
addition I generated two more attributes for each
word: suffixes of up to n characters (indicated
by suffix[n]), and character type information, i.e.
whether the word contains any punctuation charac-
ters, upper case letters, digits, etc.
Two questions to be answered empirically are: (i)
How much context to include in the description of
each instance, and (ii) Which attributes to use for
each language.
</bodyText>
<subsectionHeader confidence="0.809">
&apos;Including non-scoring tokens
</subsectionHeader>
<page confidence="0.910713">
247
</page>
<bodyText confidence="0.9993481875">
Table 3 shows the impact of using varying
amounts of context in Spanish. I used approximately
10,000 instances for training and 10,000 instances
for testing. Only the postag feature is used for
each word in this experiment. As an example, con-
sider the word sequence w1 ... wiwi+1 ... wn, and
the two words to be linked are wi and wi+1. Con-
text=0 means only information about wi and wi+1
is included, context=1 means we also include wi_1
and wi+2, etc. The table also includes the number
of rules in each decision list. The results are typical
of the experiments performed with other languages
and other attribute combinations: there is a statisti-
cally significant improvement going from context=0
to context=1. Increasing the context size further
does not have a significant effect.
</bodyText>
<table confidence="0.893483">
Context Rules Accuracy
0 161 83.17
1 254 87.31
2 264 87.05
3 137 87.14
</table>
<tableCaption confidence="0.999881">
Table 3: Context size vs. accuracy in Spanish.
</tableCaption>
<bodyText confidence="0.99965740625">
A number of experiments were run to determine
the best attribute combinations for each language.
Table 4 gives a set of results for single attributes in
Spanish. These results are based on 10,000 training
instances and all experiments use context=1. Postag
was naturally the most informative single attribute
on all languages tested, however the second best
or the best combination varied between languages.
Suffix[3] indicates all suffixes up to three characters
in length. The FEATS column was split into its con-
stituent features each of which was treated as a bi-
nary attribute.
There are various reasons for performing at-
tribute selection. Intuitively, including more infor-
mation should be good, so why not use all the at-
tributes? First, not every machine learning algo-
rithm is equally tolerant of redundant or irrelevant
attributes. Naive Bayes gets 81.54% and C4.5 gets
86.32% on the Spanish data with the single postag
attribute using context=1. One reason I chose GPA
was its relative tolerance to redundant or irrelevant
attributes. However, no matter how robust the algo-
rithm, the lack of sufficient training data will pose a
problem: it becomes difficult to distinguish informa-
tive attributes from non-informative ones if the data
is sparse. About half of the languages in this study
had less than 100,000 words of training data. Fi-
nally, studying the contribution of each attribute type
in each language is an interesting research topic in
its own right. The next section will present the best
attribute combinations and the resulting accuracy for
each language.
</bodyText>
<sectionHeader confidence="0.596897" genericHeader="evaluation">
3.2 Results
</sectionHeader>
<table confidence="0.965218769230769">
Language Attributes Accuracy
Arabic ALL 76.87
Chinese postag, cpostag 84.51
Czech postag, lemma 79.25
Danish postag, form 86.96
Dutch postag, feats 85.36
German postag, form 87.97
Japanese postag, suffix[2] 95.56
Portuguese postag, lemma 90.18
Slovene ALL 85.19
Spanish postag, lemma 89.01
Swedish postag, form 83.20
Turkish ALL 85.27
</table>
<tableCaption confidence="0.991688">
Table 5: Adjacent word link accuracy.
</tableCaption>
<table confidence="0.98996625">
Attributes Rules Accuracy
postag 254 87.31
cpostag 154 85.72
suffix[3] 328 77.15
lemma 394 76.78
form 621 75.06
feats 66 71.95
ctype 47 53.40
</table>
<tableCaption confidence="0.996483">
Table 4: Attributes vs. accuracy in Spanish.
</tableCaption>
<bodyText confidence="0.996309888888889">
Table 5 gives the best attribute combinations for
determining adjacent word links for each language
studied. The attribute combinations and the corre-
sponding models were determined using the training
sets, and the accuracy reported is on the test sets.
These attribute combinations were used as part of
the model in the final evaluation. I used context=1
for all the models. Because of time limitations at-
tribute combinations with more than two attributes
</bodyText>
<page confidence="0.994724">
248
</page>
<bodyText confidence="0.999133086956522">
could not be tested and only the first 100,000 train-
ing instances were used. Exceptions are indicated
with “ALL”, where all attributes were used in the
model – these are cases where using all the attributes
outperformed other subsets tried.
For most languages, the adjacent word link accu-
racy is in the 85-90% range. The outliers are Ara-
bic and Czech at the lower end, and Japanese at the
higher end. It is difficult to pinpoint the exact rea-
sons: Japanese has the smallest set of link types,
and Arabic has the greatest percentage of adjacent
word links. Some of the differences between the
languages come from linguistic origins, but many
are due to the idiosyncrasies of our particular data
set: number of parts of speech, types of links, qual-
ity of the treebank, amount of data are all arbitrary
factors that effect the results. One observation is that
the ranking of the languages in Table 5 according to
performance is close to the ranking of the best re-
sults in the CoNLL shared task – the task of linking
adjacent words via machine learning seems to be a
good indicator of the difficulty of the full parsing
problem.
</bodyText>
<sectionHeader confidence="0.969368" genericHeader="conclusions">
4 Long Distance Dependencies
</sectionHeader>
<bodyText confidence="0.998716266666667">
Roughly half of the dependency links are between
non-adjacent words in a sentence. To illustrate how
we can extend the previous section’s approach to
long distance links, consider the phrase “kick the
red ball”. The adjacent word linker can only find
the red-ball link even if it is 100% accurate. How-
ever once that link has been correctly identified, we
can drop the modifier “red” and do a second pass
with the words “kick the ball”. This will identify the
link the-ball, and dropping the modifier again leaves
us with “kick ball”. Thus, doing three passes over
this word sequence will bring all linked words into
contact and allow us to use our adjacent word linker.
Table 6 gives the percentage of the links discovered
in each pass by a perfect model in Spanish.
</bodyText>
<table confidence="0.9664025">
Pass: 1 2 3 4 5
Link%: 51.09 23.56 10.45 5.99 3.65
</table>
<tableCaption confidence="0.997248">
Table 6: Spanish links discovered in multiple passes.
</tableCaption>
<bodyText confidence="0.999966184210526">
We need to elaborate a bit on the operation of
“dropping the modifiers” that lead from one pass to
the next. After the discovery of the red-ball link
in the above example, it is true that “red” can no
longer link with any other words to the right (it can-
not cross its own head), but it can certainly link with
the words to the left. To be safe, in the next pass
we should consider both the-red and the-ball as can-
didate links. In the actual implementation, given a
partial linkage, all “potentially adjacent” word pairs
that do not create cycles or link crossings were con-
sidered as candidate pairs for the next pass.
There are significant differences between the first
pass and the second pass. Some word pairs will
rarely be seen in contact during the first pass (e.g.
“kick ball”). Maybe more importantly, we will
have additional “syntactic” context during the sec-
ond pass, i.e. information about the modifiers dis-
covered in the first pass. All this argues for building
a separate model for the second pass, and maybe for
further passes as well.
In the actual implementation, models for three
passes were built for each language. To create the
training data for the n’th pass, all the links that can
be discovered with (n-1) passes are taken as given,
and all word pairs that are “potentially adjacent”
given this partial linkage are used as training in-
stances. To describe each training instance, I used
the attributes of the two candidate words, their sur-
face neighbors (i.e. the words they are adjacent to
in the actual sentence), and their syntactic neighbors
(i.e. the words they have linked with so far).
To parse a sentence the three passes were run se-
quentially, with the whole sequence repeated twice2.
Each pass adds new links to the existing partial link-
age, but does not remove any existing links. Table 7
gives the labeled and unlabeled attachment score for
the test set of each language using this scheme.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="acknowledgments">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999810285714286">
I used standard machine learning techniques to in-
vestigate the lower bound accuracy and the impact
of various attributes on the subproblem of identify-
ing dependency links between adjacent words. The
technique was then extended to identify long dis-
tance dependencies and used as a parser. The model
gives average results for Turkish and Japanese but
</bodyText>
<footnote confidence="0.739437">
2This counterintuitive procedure was used because it gave
the best results on the training set.
</footnote>
<page confidence="0.991112">
249
</page>
<table confidence="0.999728384615384">
Language LAS UAS
Arabic 52.42 68.82
Chinese 72.72 78.37
Czech 51.86 66.36
Danish 71.56 78.16
Dutch 62.75 66.17
German 63.82 67.71
Japanese 84.35 87.31
Portuguese 70.35 79.46
Slovene 55.06 70.60
Spanish 69.63 73.89
Swedish 65.23 73.25
Turkish 60.31 71.54
</table>
<tableCaption confidence="0.999251">
Table 7: Labeled and unlabeled attachment scores.
</tableCaption>
<bodyText confidence="0.999891375">
generally performs below average. The lack of a
specialized parsing algorithm taking into account
sentence wide constraints and the lack of a prob-
abilistic component in the model are probably to
blame. Nevertheless, the particular decomposition
of the problem and the simplicity of the resulting
models provide some insight into the difficulties as-
sociated with individual languages.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999283954545455">
A. Abeill´e, editor. 2003. Treebanks: Building and Us-
ing Parsed Corpora, volume 20 of Text, Speech and
Language Technology. Kluwer Academic Publishers,
Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. “Flo-
resta sint´a(c)tica”: a treebank for Portuguese. In Proc.
of the Third Intern. Conf. on Language Resources and
Evaluation (LREC), pages 1698–1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annota-
tion process in the Turkish treebank. In Proc. of the 4th
Intern. Workshop on Linguistically Interpreteted Cor-
pora (LINC).
A. B¨ohmov´a, J. Haji&amp;quot;c, E. Haji&amp;quot;cov´a, and B. Hladk´a. 2003.
The PDT: a 3-level annotation scenario. In Abeill´e
(Abeill´e, 2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. of the Tenth Conf. on Com-
putational Natural Language Learning (CoNLL-X).
SIGNLL.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeill´e
(Abeill´e, 2003), chapter 13, pages 231–248.
M. Civit Torruella and Ma A. MartiAntonin. 2002. De-
sign principles for a Spanish treebank. In Proc. of the
First Workshop on Treebanks and Linguistic Theories
(TLT).
S. Dz&amp;quot;eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. &amp;quot;Zabokrtsky, and A. &amp;quot;Zele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Intern.
Conf. on Language Resources and Evaluation (LREC).
J. Haji&amp;quot;c, O. Smr&amp;quot;z, P. Zem´anek, J. &amp;quot;Snaidauf, and E. Be&amp;quot;ska.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110–117.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese treebank in VERBMOBIL. Verbmobil-
Report 240, Seminar f¨ur Sprachwissenschaft, Univer-
sit¨at T¨ubingen.
M. T. Kromann. 2003. The Danish dependency treebank
and the underlying linguistic theory. In Proc. of the
Second Workshop on Treebanks and Linguistic Theo-
ries (TLT).
Volkan Kurt. 2005. Protein structure prediction using
decision lists. Master’s thesis, Koc¸ University.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from an-
tiquity. In Proc. of the NODALIDA Special Session on
Treebanks.
K. Oflazer, B. Say, D. Zeynep Hakkani-T¨ur, and G. T¨ur.
2003. Building a Turkish treebank. In Abeill´e
(Abeill´e, 2003), chapter 15.
Ronald L. Rivest. 1987. Learning decision lists. Ma-
chine Learning, 2:229–246.
L. van der Beek, G. Bouma, R. Malouf, and G. van No-
ord. 2002. The Alpino dependency treebank. In Com-
putational Linguistics in the Netherlands (CLIN).
Deniz Yuret and Ferhan Ture. 2006. Learning mor-
phological disambiguation rules for Turkish. In HLT-
NAACL 06.
</reference>
<page confidence="0.997087">
250
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.546167">
<title confidence="0.984876">Dependency Parsing as a Classification Problem</title>
<author confidence="0.793576">Deniz</author>
<affiliation confidence="0.598923">Istanbul,</affiliation>
<email confidence="0.996098">dyuret@ku.edu.tr</email>
<abstract confidence="0.998426105263158">This paper presents an approach to dependency parsing which can utilize any standard machine learning (classification) algorithm. A decision list learner was used in this work. The training data provided in the form of a treebank is converted to a format in which each instance represents information about one word pair, and the classification indicates the existence, direction, and type of the link between the words of the pair. Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Treebanks: Building and Using Parsed Corpora,</title>
<date>2003</date>
<booktitle>Text, Speech and Language Technology.</booktitle>
<volume>20</volume>
<editor>A. Abeill´e, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<marker>2003</marker>
<rawString>A. Abeill´e, editor. 2003. Treebanks: Building and Using Parsed Corpora, volume 20 of Text, Speech and Language Technology. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Afonso</author>
<author>E Bick</author>
<author>R Haber</author>
<author>D Santos</author>
</authors>
<title>Floresta sint´a(c)tica”: a treebank for Portuguese.</title>
<date>2002</date>
<booktitle>In Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>1698--1703</pages>
<contexts>
<context position="1195" citStr="Afonso et al., 2002" startWordPosition="187" endWordPosition="190">re built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adjacent modifiers are d</context>
</contexts>
<marker>Afonso, Bick, Haber, Santos, 2002</marker>
<rawString>S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. “Floresta sint´a(c)tica”: a treebank for Portuguese. In Proc. of the Third Intern. Conf. on Language Resources and Evaluation (LREC), pages 1698–1703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N B Atalay</author>
<author>K Oflazer</author>
<author>B Say</author>
</authors>
<title>The annotation process in the Turkish treebank.</title>
<date>2003</date>
<booktitle>In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC).</booktitle>
<contexts>
<context position="1325" citStr="Atalay et al., 2003" startWordPosition="209" endWordPosition="212">dency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adjacent modifiers are dropped from the sentence and a second order model is built based on the word pairs that come into contact. A total of three models</context>
</contexts>
<marker>Atalay, Oflazer, Say, 2003</marker>
<rawString>N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation process in the Turkish treebank. In Proc. of the 4th Intern. Workshop on Linguistically Interpreteted Corpora (LINC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B¨ohmov´a</author>
<author>J Hajic</author>
<author>E Hajicov´a</author>
<author>B Hladk´a</author>
</authors>
<title>The PDT: a 3-level annotation scenario.</title>
<date>2003</date>
<booktitle>In Abeill´e (Abeill´e,</booktitle>
<note>chapter 7.</note>
<marker>B¨ohmov´a, Hajic, Hajicov´a, Hladk´a, 2003</marker>
<rawString>A. B¨ohmov´a, J. Haji&amp;quot;c, E. Haji&amp;quot;cov´a, and B. Hladk´a. 2003. The PDT: a 3-level annotation scenario. In Abeill´e (Abeill´e, 2003), chapter 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="1148" citStr="Brants et al., 2002" startWordPosition="179" endWordPosition="182">he words of the pair. Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer d</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER treebank. In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
<author>A Dubey</author>
<author>Y Krymolowski</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL-X). SIGNLL.</booktitle>
<contexts>
<context position="1383" citStr="Buchholz et al., 2006" startWordPosition="218" endWordPosition="221">alysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adjacent modifiers are dropped from the sentence and a second order model is built based on the word pairs that come into contact. A total of three models were built using this technique successively and used for</context>
</contexts>
<marker>Buchholz, Marsi, Dubey, Krymolowski, 2006</marker>
<rawString>S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of the Tenth Conf. on Computational Natural Language Learning (CoNLL-X). SIGNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Chen</author>
<author>C Luo</author>
<author>M Chang</author>
<author>F Chen</author>
<author>C Chen</author>
<author>C Huang</author>
<author>Z Gao</author>
</authors>
<title>Sinica treebank: Design criteria, representational issues and implementation.</title>
<date>2003</date>
<booktitle>In Abeill´e (Abeill´e,</booktitle>
<pages>231--248</pages>
<contexts>
<context position="1061" citStr="Chen et al., 2003" startWordPosition="163" endWordPosition="166">the classification indicates the existence, direction, and type of the link between the words of the pair. Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations bet</context>
</contexts>
<marker>Chen, Luo, Chang, Chen, Chen, Huang, Gao, 2003</marker>
<rawString>K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and Z. Gao. 2003. Sinica treebank: Design criteria, representational issues and implementation. In Abeill´e (Abeill´e, 2003), chapter 13, pages 231–248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Civit Torruella</author>
<author>Ma A MartiAntonin</author>
</authors>
<title>Design principles for a Spanish treebank.</title>
<date>2002</date>
<booktitle>In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="1259" citStr="Torruella and MartiAntonin, 2002" startWordPosition="196" endWordPosition="200">t different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adjacent modifiers are dropped from the sentence and a second order model is built based</context>
</contexts>
<marker>Torruella, MartiAntonin, 2002</marker>
<rawString>M. Civit Torruella and Ma A. MartiAntonin. 2002. Design principles for a Spanish treebank. In Proc. of the First Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dzeroski</author>
<author>T Erjavec</author>
<author>N Ledinek</author>
<author>P Pajas</author>
<author>Z Zabokrtsky</author>
<author>A Zele</author>
</authors>
<title>Towards a Slovene dependency treebank.</title>
<date>2006</date>
<booktitle>In Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).</booktitle>
<marker>Dzeroski, Erjavec, Ledinek, Pajas, Zabokrtsky, Zele, 2006</marker>
<rawString>S. Dz&amp;quot;eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. &amp;quot;Zabokrtsky, and A. &amp;quot;Zele. 2006. Towards a Slovene dependency treebank. In Proc. of the Fifth Intern. Conf. on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajic</author>
<author>O Smrz</author>
<author>P Zem´anek</author>
<author>J Snaidauf</author>
<author>E Beska</author>
</authors>
<title>Prague Arabic dependency treebank: Development in data and tools.</title>
<date>2004</date>
<booktitle>In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools,</booktitle>
<pages>110--117</pages>
<marker>Hajic, Smrz, Zem´anek, Snaidauf, Beska, 2004</marker>
<rawString>J. Haji&amp;quot;c, O. Smr&amp;quot;z, P. Zem´anek, J. &amp;quot;Snaidauf, and E. Be&amp;quot;ska. 2004. Prague Arabic dependency treebank: Development in data and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic Language Resources and Tools, pages 110–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kawata</author>
<author>J Bartels</author>
</authors>
<title>Stylebook for the Japanese treebank in VERBMOBIL. VerbmobilReport 240, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</title>
<date>2000</date>
<contexts>
<context position="1174" citStr="Kawata and Bartels, 2000" startWordPosition="183" endWordPosition="186"> Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adj</context>
</contexts>
<marker>Kawata, Bartels, 2000</marker>
<rawString>Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese treebank in VERBMOBIL. VerbmobilReport 240, Seminar f¨ur Sprachwissenschaft, Universit¨at T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Kromann</author>
</authors>
<title>The Danish dependency treebank and the underlying linguistic theory.</title>
<date>2003</date>
<booktitle>In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</booktitle>
<contexts>
<context position="1100" citStr="Kromann, 2003" startWordPosition="171" endWordPosition="172"> direction, and type of the link between the words of the pair. Several distinct models are built to identify the links between word pairs at different distances. These models are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decisi</context>
</contexts>
<marker>Kromann, 2003</marker>
<rawString>M. T. Kromann. 2003. The Danish dependency treebank and the underlying linguistic theory. In Proc. of the Second Workshop on Treebanks and Linguistic Theories (TLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volkan Kurt</author>
</authors>
<title>Protein structure prediction using decision lists. Master’s thesis,</title>
<date>2005</date>
<institution>Koc¸ University.</institution>
<contexts>
<context position="5164" citStr="Kurt, 2005" startWordPosition="838" endWordPosition="839">s describing the two candidate words including suffixes of different lengths, parts of speech and information on neighboring words. Most of this information may be redundant or irrelevant to the problem at hand. The number of distinct attribute values is on the order of the number of distinct word-forms in the training set. GPA was picked for this problem because it has proven to be fairly efficient and robust in the presence of irrelevant or redundant attributes in previous work such as morphological disambiguation in Turkish (Yuret and Ture, 2006) and protein secondary structure prediction (Kurt, 2005). 3 Dependency of Adjacent Words We start by looking at adjacent words and try to predict whether they are linked, and if they are, what type of link they have. This is a nice subproblem to study because: (i) It is easily converted to a standard machine learning problem, thus amenable to common machine learning techniques and analysis, (ii) It demonstrates the differences between languages and the impact of various attributes. The machine learning algorithm used was GPA (See Section 2) which builds decision lists. Table 2 shows the percentage of adjacent tokens that are linked in the training </context>
</contexts>
<marker>Kurt, 2005</marker>
<rawString>Volkan Kurt. 2005. Protein structure prediction using decision lists. Master’s thesis, Koc¸ University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nilsson</author>
<author>J Hall</author>
<author>J Nivre</author>
</authors>
<title>MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity.</title>
<date>2005</date>
<booktitle>In Proc. of the NODALIDA Special Session on Treebanks.</booktitle>
<contexts>
<context position="1281" citStr="Nilsson et al., 2005" startWordPosition="201" endWordPosition="204">s are applied sequentially to give the dependency parse of a sentence, favoring shorter links. An analysis of the errors, attribute selection, and comparison of different languages is presented. 1 Introduction This paper presents an approach to supervised learning of dependency relations in a language using standard machine learning techniques. The treebanks (Hajiˇc et al., 2004; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003) provided for the CoNLL shared task(Buchholz et al., 2006) were converted to a set of instances each of which consists of the attributes of a candidate word pair with a classification that indicates the existence, direction and type of the dependency link between the pair. An initial model is built to identify dependency relations between adjacent word pairs using a decision list learning algorithm. To identify longer distance relations, the adjacent modifiers are dropped from the sentence and a second order model is built based on the word pairs tha</context>
</contexts>
<marker>Nilsson, Hall, Nivre, 2005</marker>
<rawString>J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER: Reconstructing a Swedish treebank from antiquity. In Proc. of the NODALIDA Special Session on Treebanks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>B Say</author>
<author>D Zeynep Hakkani-T¨ur</author>
<author>G T¨ur</author>
</authors>
<title>Building a Turkish treebank. In Abeill´e (Abeill´e,</title>
<date>2003</date>
<pages>15</pages>
<marker>Oflazer, Say, Hakkani-T¨ur, T¨ur, 2003</marker>
<rawString>K. Oflazer, B. Say, D. Zeynep Hakkani-T¨ur, and G. T¨ur. 2003. Building a Turkish treebank. In Abeill´e (Abeill´e, 2003), chapter 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald L Rivest</author>
</authors>
<title>Learning decision lists.</title>
<date>1987</date>
<booktitle>Machine Learning,</booktitle>
<pages>2--229</pages>
<contexts>
<context position="3487" citStr="Rivest, 1987" startWordPosition="552" endWordPosition="553">n 3 describes the first pass model of links between adjacent words. Section 4 details the approach for identifying long distance links and presents the parsing results. 246 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 246–250, New York City, June 2006. c�2006 Association for Computational Linguistics 2 The Learning Algorithm The Greedy Prepend Algorithm (Yuret and Ture, 2006) was used to build decision lists to identify dependency relations. A decision list is an ordered list of rules where each rule consists of a pattern and a classification (Rivest, 1987). The first rule whose pattern matches a given instance is used for its classification. In our application the pattern specifies the attributes of the two words to be linked such as parts of speech and morphological features. The classification indicates the existence and the type of the dependency link between the two words. Table 1 gives a subset of the decision list that identifies links between adjacent words in German. The class column indicates the type of the link, the pattern contains attributes of the two candidate words X and Y, as well as their neighbors (XL1 indicates the left neig</context>
</contexts>
<marker>Rivest, 1987</marker>
<rawString>Ronald L. Rivest. 1987. Learning decision lists. Machine Learning, 2:229–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Beek</author>
<author>G Bouma</author>
<author>R Malouf</author>
<author>G van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Computational Linguistics in the Netherlands (CLIN).</booktitle>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>L. van der Beek, G. Bouma, R. Malouf, and G. van Noord. 2002. The Alpino dependency treebank. In Computational Linguistics in the Netherlands (CLIN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Ferhan Ture</author>
</authors>
<title>Learning morphological disambiguation rules for Turkish.</title>
<date>2006</date>
<booktitle>In HLTNAACL 06.</booktitle>
<contexts>
<context position="3302" citStr="Yuret and Ture, 2006" startWordPosition="518" endWordPosition="521">ord pair candidates that may form cycles or crossings are never considered, so the parser will only generate projective structures. Section 2 gives the details of the learning algorithm. Section 3 describes the first pass model of links between adjacent words. Section 4 details the approach for identifying long distance links and presents the parsing results. 246 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 246–250, New York City, June 2006. c�2006 Association for Computational Linguistics 2 The Learning Algorithm The Greedy Prepend Algorithm (Yuret and Ture, 2006) was used to build decision lists to identify dependency relations. A decision list is an ordered list of rules where each rule consists of a pattern and a classification (Rivest, 1987). The first rule whose pattern matches a given instance is used for its classification. In our application the pattern specifies the attributes of the two words to be linked such as parts of speech and morphological features. The classification indicates the existence and the type of the dependency link between the two words. Table 1 gives a subset of the decision list that identifies links between adjacent word</context>
<context position="5108" citStr="Yuret and Ture, 2006" startWordPosition="828" endWordPosition="831">training instance for the dependency problem has over 40 attributes describing the two candidate words including suffixes of different lengths, parts of speech and information on neighboring words. Most of this information may be redundant or irrelevant to the problem at hand. The number of distinct attribute values is on the order of the number of distinct word-forms in the training set. GPA was picked for this problem because it has proven to be fairly efficient and robust in the presence of irrelevant or redundant attributes in previous work such as morphological disambiguation in Turkish (Yuret and Ture, 2006) and protein secondary structure prediction (Kurt, 2005). 3 Dependency of Adjacent Words We start by looking at adjacent words and try to predict whether they are linked, and if they are, what type of link they have. This is a nice subproblem to study because: (i) It is easily converted to a standard machine learning problem, thus amenable to common machine learning techniques and analysis, (ii) It demonstrates the differences between languages and the impact of various attributes. The machine learning algorithm used was GPA (See Section 2) which builds decision lists. Table 2 shows the percen</context>
</contexts>
<marker>Yuret, Ture, 2006</marker>
<rawString>Deniz Yuret and Ferhan Ture. 2006. Learning morphological disambiguation rules for Turkish. In HLTNAACL 06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>