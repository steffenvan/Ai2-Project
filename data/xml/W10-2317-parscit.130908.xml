<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000426">
<title confidence="0.997688">
Computing Word Senses by Semantic Mirroring and Spectral Graph
Partitioning
</title>
<author confidence="0.99338">
Martin Fagerlund
</author>
<affiliation confidence="0.979971">
Link¨oping University
</affiliation>
<address confidence="0.564116">
Link¨oping, Sweden
</address>
<email confidence="0.938053">
marfa229@student.liu.se
</email>
<author confidence="0.987898">
Lars Eld´en
</author>
<affiliation confidence="0.99011">
Link¨oping University
</affiliation>
<address confidence="0.631283">
Link¨oping, Sweden
</address>
<email confidence="0.996637">
lars.elden@liu.se
</email>
<sectionHeader confidence="0.994737" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861461538461">
Using the technique of ”semantic mirror-
ing” a graph is obtained that represents
words and their translations from a paral-
lel corpus or a bilingual lexicon. The con-
nectedness of the graph holds information
about the different meanings of words that
occur in the translations. Spectral graph
theory is used to partition the graph, which
leads to a grouping of the words according
to different senses. We also report results
from an evaluation using a small sample of
seed words from a lexicon of Swedish and
English adjectives.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958666666667">
A great deal of linguistic knowledge is encoded
implicitly in bilingual resources such as par-
allel texts and bilingual dictionaries. Dyvik
(1998, 2005) has provided a knowledge discov-
ery method based on the semantic relationship be-
tween words in a source language and words in
a target language, as manifested in parallel texts.
His method is called Semantic mirroring and the
approach utilizes the way that different languages
encode lexical meaning by mirroring source words
and target words back and forth, in order to es-
tablish semantic relations like synonymy and hy-
ponymy. Work in this area is strongly related to
work within Word Sense Disambiguation (WSD)
and the observation that translations are a good
source for detecting such distinctions (Resnik &amp;
Yarowsky 1999, Ide 2000, Diab &amp; Resnik 2002).
A word that has multiple meanings in one lan-
guage is likely to have different translations in
other languages. This means that translations
serve as sense indicators for a particular source
</bodyText>
<note confidence="0.84639575">
Magnus Merkel
Link¨oping University
Link¨oping, Sweden
magnus.merkel@liu.se
</note>
<author confidence="0.804295">
Lars Ahrenberg
</author>
<affiliation confidence="0.637695">
Link¨oping University
Link¨oping, Sweden
</affiliation>
<email confidence="0.951596">
lars.ahrenberg@liu.se
</email>
<bodyText confidence="0.99962925">
word, and make it possible to divide a given word
into different senses.
In this paper we propose a new graph-based ap-
proach to the analysis of semantic mirrors. The
objective is to find a viable way to discover syn-
onyms and group them into different senses. The
method has been applied to a bilingual dictionary
of English and Swedish adjectives.
</bodyText>
<sectionHeader confidence="0.996128" genericHeader="introduction">
2 Preparations
</sectionHeader>
<subsectionHeader confidence="0.999587">
2.1 The Translation Matrix
</subsectionHeader>
<bodyText confidence="0.999926142857143">
In these experiments we have worked with a
English-Swedish lexicon consisting of 14850 En-
glish adjectives, and their corresponding Swedish
translations. Out of the lexicon was created a
translation matrix B, and two lists with all the
words, one for English and one for Swedish. B
is defined as
</bodyText>
<equation confidence="0.970380666666667">
� _ 1, if i — j,
B(i
,i) 0, otherwise.
</equation>
<bodyText confidence="0.9830525">
The relation i — j means that word i translates
to word j.
</bodyText>
<subsectionHeader confidence="0.998358">
2.2 Translation
</subsectionHeader>
<bodyText confidence="0.999984428571429">
Translation is performed as follows. From the
word i to be translated, we create a vector ei, with
a one in position i, and zeros everywhere else.
Then perform the matrix multiplication Bei if it
is a Swedish word to be translated, or BT ei if it is
an English word to be translated. ei has the same
length as the list in which the word i can be found.
</bodyText>
<sectionHeader confidence="0.978906" genericHeader="method">
3 Semantic Mirroring
</sectionHeader>
<bodyText confidence="0.869169">
We start with an English word, called eng11. We
look up its Swedish translations. Then we look up
&apos;Short for english1. We will use swe for Swedish words.
</bodyText>
<page confidence="0.989836">
103
</page>
<bodyText confidence="0.7612182">
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 103–107,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
the English translations of each of those Swedish
words. We have now performed one ”mirror-
operation”. In mathematical notation:
</bodyText>
<equation confidence="0.857694">
-
f = BB T eeng1.
</equation>
<bodyText confidence="0.99996">
The non-zero elements in the vector f represent
English words that are semantically related to
eng1. Dyvik (1998) calls the set of words that we
get after two translations the inverse t-image. But
there is one problem. The original word should not
be here. Therefore, in the last translation, we mod-
ify the matrix B, by replacing the row in B corre-
sponding to eng1, with an all-zero row. Call this
new modified matrix Bmod1. So instead of the ma-
trix multiplication performed above, we start over
with the following one:
</bodyText>
<equation confidence="0.999399">
T
Bmod1B -eeng1. (1)
</equation>
<bodyText confidence="0.999914380952381">
To make it clearer from a linguistic perspective,
consider the following figure2.
The words to the right in the picture above
(eng2,...,eng5) are the words we want to divide
into senses. To do this, we need some kind of
relation between the words. Therefore we con-
tinue to translate, and perform a second ”mirror
operation”. To keep track of what each word in
the inverse t-image translates to, we must first
make a small modification. We have so far done
the operation (1), which gave us a vector, call it
e ∈ ][814ss0×1. The vector e consists of nonzero in-
tegers in the positions corresponding to the words
in the invers t-image, and zeros everywhere else.
We make a new matrix E, with the same number
of rows as e, and the same number of columns as
there are nonzeros in e. Now go through every el-
ement in e, and when finding a nonzero element
in row i, and if it is the j:th nonzero element, then
put a one in position (i, j) in E. The procedure is
illustrated in (2).
</bodyText>
<footnote confidence="0.870145">
2The arrows indicate translation.
</footnote>
<equation confidence="0.997701428571429">
1 � 1 0 0 0
� 0 � � � � � � � 0 0 0 0
2 −→ 0 1 0 0
� � � � � � � 0 0 1 0
1 0 0 0 0
0 0 0 0 1
3 1
</equation>
<bodyText confidence="0.999916428571429">
When doing our second ”mirror operation”, we do
not want to translate through the Swedish words
swe1,...,swe3. We once again modify the matrix
B, this time replacing the columns of B corre-
sponding to the Swedish words swe1,...,swe3, with
zeros. Call this second modified matrix Bmod2.
With the matrix E from (2), we now get:
</bodyText>
<equation confidence="0.991993">
Bmod2BTmod2E (3)
</equation>
<bodyText confidence="0.569053">
We illustrate the operation (3):
</bodyText>
<equation confidence="0.999004">
eng1 swe2 eng1
swe3 5k5eng4
�����
swe7 &gt; eng7
</equation>
<bodyText confidence="0.9669444">
Now we have got the desired relation between
eng2,...eng5. In (3) we keep only the rows corre-
sponding to eng2,...eng5, and get a symmetric ma-
trix A, which can be considered as the adjacency
matrix of a graph. The adjacency matrix and the
graph of our example are illustrated below.
� 2 1 0 0
A= � � � 1 1 0 0
0 0 1 1
0 0 1 2
</bodyText>
<figureCaption confidence="0.999564">
Figure 1: The graph to the matrix in (4).
</figureCaption>
<bodyText confidence="0.999562">
The adjacency matrix should be interpreted in the
following way. The rows and the columns corre-
spond to the words in the inverse t-image. Follow-
ing our example, eng2 corresponds to row 1 and
</bodyText>
<figure confidence="0.761424333333333">
eng2
eng3
eng4
eng5
������������
��
������������
eng2
������������
swe3 ���
�����������
eng1
swe1
swe2
������������
������������
eng3
eng1
eng4
eng5
1 (2)
eng1 ������ swe1 ������
�� swe2 ������
������ ������
</figure>
<equation confidence="0.8655595">
swe3 ��
������
eng2
eng3
������
��swe5 ��
������
swe4 ��
������
������
swe1 eng3
eng6
eng2
������
eng5 ��
������
eng4
swe6
eng5
1 (4)
</equation>
<page confidence="0.97636">
104
</page>
<bodyText confidence="0.999895730769231">
column 1, eng3 corresponds to row 2 and column
2, and so on. The elements on position (i, i) in
A are the vertex weights. The vertex weight as-
sociated with a word, describes how many transla-
tions that word has in the other language, e.g. eng2
translates to swe4 and swe5 that is translated back
to eng2. So the vertex weight for eng2 is 2, as also
can be seen in position (1, 1) in (4). A high vertex
weight tells us that the word has a high number of
translations, and therefore probably a wide mean-
ing.
The elements in the adjacency matrix on posi-
tion (i, j), i =� j are the edge weights. These
weights are associated with two words, and de-
scribe how many words in the other language that
both word i and j are translated to. E.g. eng5
and eng4 are both translated to swe6, and it fol-
lows that the weight, w(eng4,eng5) = 1. If we in-
stead would take eng5 and eng7, we see that they
both translate to swe6 and swe7, so the weight be-
tween those words, w(eng5,eng7) = 2. (But this is
not shown in the adjacency matrix, since eng7 is
not a word in the inverse t-image). A high edge
weight between two words tells us that they share
a high number of translations, and therefore prob-
ably have the same meanings.
</bodyText>
<sectionHeader confidence="0.985416" genericHeader="method">
4 Graph Partitioning
</sectionHeader>
<bodyText confidence="0.999984933333333">
The example illustrated in Figure 1 gave as a re-
sult two graphs that are not connected. Dyvik ar-
gues that in such a case the graphs represent two
groups of words of different senses. In a larger
and more realistic example one is likely to obtain
a graph that is connected, but which can be parti-
tioned into two subgraphs without breaking more
than a small number of edges. Then it is reason-
able to ask whether such a partitioning has a sim-
ilar effect in that it represents a partitioning of the
words into different senses.
We describe the mathematical procedure of par-
titioning a graph into subgraphs, using spectral
graph theory (Chung, 1997). First, define the de-
gree d(i) of a vertex i to be
</bodyText>
<equation confidence="0.993060142857143">
d(i) = � A(i, j).
9
Let D be the diagonal matrix defined by
� d(i), if i = j,
D(i,j) = 0, otherwise.
The Laplacian L is defined as
L = D − A.
</equation>
<bodyText confidence="0.995283">
We define the normalised Laplacian L to be
</bodyText>
<equation confidence="0.998135">
L = D−2 LD−1
1 2 .
</equation>
<bodyText confidence="0.999966526315789">
Now calculate the eigenvalues λ0, ... , λ,1, and
the eigenvectors of L. The smallest eigen-
value, λ0, is always equal to zero, as shown by
Chung (1997). The multiplicity of zero among
the eigenvalues is equal to the number of con-
nected components in the graph, as shown by
Spielman (2009). We will look at the eigenvector
belonging to the second smallest eigenvalue, λ1.
This eigenpair is often referred to as the Fiedler
value and the Fiedler vector. The entries in the
Fiedler vector corresponds to the vertices in the
graph. (We will assume that there is only one
component in the graph. If not, chose the com-
ponent with the largest number of vertices). Sort
the Fiedler vector, and thus sorting the vertices in
the graph. Then make n −1 cuts along the Fiedler
vector, dividing the elements of the vector into two
sets, and for each cut compute the conductance,
ϕ(S), defined as
</bodyText>
<equation confidence="0.680143">
d(S)d(�S), (5)
</equation>
<bodyText confidence="0.9999432">
where d(S) = EicS d(i).  |∂(S, �S)  |is the total
weight of the edges with one end in S and one end
in �S, and V = S + S� is the set of all vertices in
the graph. Another measure used is the sparsity,
sp(S), defined as
</bodyText>
<equation confidence="0.8987795">
(6)
min(d(S), d(S))
</equation>
<bodyText confidence="0.999928">
For details, see (Spielman, 2009). Choose the cut
with the smallest conductance, and in the graph,
delete the edges with one end in S and the other
end in S. The procedure is then carried out until
the conductance, ϕ(S), reaches a tolerance. The
tolerance is decided by human evaluators, per-
forming experiments on test data.
</bodyText>
<sectionHeader confidence="0.996216" genericHeader="method">
5 Example
</sectionHeader>
<bodyText confidence="0.999781">
We start with the word slithery, and after the mir-
roring operation (3) we get three groups of words
in the inverse t-image, shown in Table 1. After
two partitionings of the graph to slithery, using the
method described in section 4, we get five sense
groups, shown in Table 2.
</bodyText>
<equation confidence="0.999535">
ϕ(S) = d(V ) |∂(S,
S) |
sp(S) = |∂(S, S)|
</equation>
<page confidence="0.991839">
105
</page>
<bodyText confidence="0.940166666666667">
smooth slimy saponaceous
slick smooth-faced
lubricious oleaginous
slippery oily slippy
glib greasy
sleek
</bodyText>
<tableCaption confidence="0.9838505">
Table 1: The three groups of words after the mir-
roring operation.
</tableCaption>
<table confidence="0.9925408">
slimy glib oleaginous
smooth-faced slippery oily
smooth lubricious greasy
sleek slick
saponaceous slippy
</table>
<tableCaption confidence="0.957922">
Table 2: The five sense groups of slithery after two
partitionings.
</tableCaption>
<sectionHeader confidence="0.998726" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999774806451613">
A small evaluation was performed using a ran-
dom sample of 10 Swedish adjectives. We gen-
erated sets under four different conditions. For the
first, using conductance (5). For the second, using
sparsity (6). For the third and fourth, we set the
diagonal entries in the adjacency matrix to zero.
These entries tell us very little of how the words
are connected to each other, but they may effect
how the partitioning is made. So for the third, we
used conductance and no vertex weights, and for
the fourth we used sparsity and no vertex weights.
There were only small differences in results due to
the conditions, so we report results only for one of
them, the one using vertex weights and sparsity.
Generated sets, with singletons removed, were
evaluated from two perspectives: consistency and
synonymy with the seed word. For consistency a
three-valued scheme was used: (i) the set forms a
single synset, (ii) at least two thirds of the words
form a single synset, and (iii) none of these. Syn-
onymy with the seed word was judged as either
yes or no.
Two evaluators first judged all sets indepen-
dently and then coordinated their judgements. The
criterion for consistency was that at least one do-
main, such as personality, taste, manner, can be
found where all adjectives in the set are inter-
changeable. Results are shown in Table 3.
Depending on how we count partially consistent
groups this gives a precision in the range 0.57 to
0.78. We have made no attempt to measure recall.
</bodyText>
<table confidence="0.999334428571429">
Count Average Percentage
All groups 58 5.8 100
Consistent 33 3.3 57
groups
2/3 consistency 12 1.2 21
Synonymy 14 1.4 24
with seed word
</table>
<tableCaption confidence="0.890651">
Table 3: Classified output with frequencies from
one type of partition
</tableCaption>
<bodyText confidence="0.996735">
It may be noted that group size varies. There are
often several small groups with just 2 or 3 words,
but sometimes as many as 10-15 words make up a
group. For large groups, even though they are not
fully consistent, the words tend to be drawn from
two or three synsets.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999941285714286">
So far we have performed a relatively limited num-
ber of tests of the method. Those tests indi-
cate that semantic mirroring coupled with spectral
graph partitioning is a useful method for comput-
ing word senses, which can be developed further
using refined graph theoretic and linguistic tech-
niques in conjunction.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="acknowledgments">
8 Future work
</sectionHeader>
<bodyText confidence="0.9999891">
There is room for many more investigations of the
approach outlined in this paper. We would like
to explore the possibility to have a vertex (word)
belong to multiple synsets, instead of having dis-
crete cuts between synsets. In the present solu-
tion a vertex belongs to only one partition of a
graph, making it impossible to having the same
word belong to several synsets. We would also
like to investigate the properties of graphs to see
whether it is possible to automatically measure
how close a seed word is to a particular synset.
Furthermore, more thorough evaluations of larger
data sets would give us more information on how
to combine similar synsets which were generated
from distinct seed words and explore more com-
plex semantic fields. In our future research we will
test the method also on other lexica, and perform
experiments with the different tolerances involved.
We will also perform extensive tests assessing the
results using a panel of human evaluators.
</bodyText>
<page confidence="0.997794">
106
</page>
<sectionHeader confidence="0.993886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999101">
Daniel A. Spielman. 2009. Spectral Graph theory.
Lecture notes.
Daniel A. Spielman, S. -H. Teng. 2006. Spectral par-
titioning works: Planar graphs and finite element
meshes. Elsevier Inc.
Diab, M. Resnik, P. 2002. An Unsupervised Method
for Word Sense Tagging using Parallel Corpora.
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics. 255-262.
Fan R. K. Chung. 1997. Spectral Graph Theory.
American Mathematical Society, Providence, Rhode
Island.
H. Dyvik. 1998. A Translational Basis for Semantics.
In: Stig Johansson and Signe Oksefjell (eds.): Cor-
pora and Crosslinguistic Research: Theory, Method
and Case Studies, pp. 51-86. Rodopi.
H. Dyvik. 2005. Translations as a Semantic Knowl-
edge Source. Proceedings of the Second Baltic Con-
ference on Human Language Technologies, Tallinn.
Nancy Ide. 2000. Cross-lingual sense determination:
Can it work? Computers and the Humanities: Spe-
cial issue on SENSEVAL, 34:223–234.
Philip Resnik , David Yarowsky. Distinguishing sys-
tems and distinguishing senses: new evaluation
methods for Word Sense Disambiguation Natu-
ral Language Engineering, v.5 n.2, p.113-133, June
1999
</reference>
<page confidence="0.998543">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025048">
<title confidence="0.9991835">Computing Word Senses by Semantic Mirroring and Spectral Graph Partitioning</title>
<author confidence="0.823502">Martin</author>
<affiliation confidence="0.414255">Link¨oping Link¨oping,</affiliation>
<email confidence="0.443623">marfa229@student.liu.se</email>
<affiliation confidence="0.318796">Lars Link¨oping Link¨oping,</affiliation>
<email confidence="0.93012">lars.elden@liu.se</email>
<abstract confidence="0.978667285714286">Using the technique of ”semantic mirroring” a graph is obtained that represents words and their translations from a parallel corpus or a bilingual lexicon. The connectedness of the graph holds information about the different meanings of words that occur in the translations. Spectral graph theory is used to partition the graph, which leads to a grouping of the words according to different senses. We also report results from an evaluation using a small sample of seed words from a lexicon of Swedish and English adjectives.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel A Spielman</author>
</authors>
<title>Spectral Graph theory. Lecture notes.</title>
<date>2009</date>
<contexts>
<context position="8987" citStr="Spielman (2009)" startWordPosition="1625" endWordPosition="1626">ioning a graph into subgraphs, using spectral graph theory (Chung, 1997). First, define the degree d(i) of a vertex i to be d(i) = � A(i, j). 9 Let D be the diagonal matrix defined by � d(i), if i = j, D(i,j) = 0, otherwise. The Laplacian L is defined as L = D − A. We define the normalised Laplacian L to be L = D−2 LD−1 1 2 . Now calculate the eigenvalues λ0, ... , λ,1, and the eigenvectors of L. The smallest eigenvalue, λ0, is always equal to zero, as shown by Chung (1997). The multiplicity of zero among the eigenvalues is equal to the number of connected components in the graph, as shown by Spielman (2009). We will look at the eigenvector belonging to the second smallest eigenvalue, λ1. This eigenpair is often referred to as the Fiedler value and the Fiedler vector. The entries in the Fiedler vector corresponds to the vertices in the graph. (We will assume that there is only one component in the graph. If not, chose the component with the largest number of vertices). Sort the Fiedler vector, and thus sorting the vertices in the graph. Then make n −1 cuts along the Fiedler vector, dividing the elements of the vector into two sets, and for each cut compute the conductance, ϕ(S), defined as d(S)d(</context>
</contexts>
<marker>Spielman, 2009</marker>
<rawString>Daniel A. Spielman. 2009. Spectral Graph theory. Lecture notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel A Spielman</author>
<author>S -H Teng</author>
</authors>
<title>Spectral partitioning works: Planar graphs and finite element meshes.</title>
<date>2006</date>
<publisher>Elsevier Inc.</publisher>
<marker>Spielman, Teng, 2006</marker>
<rawString>Daniel A. Spielman, S. -H. Teng. 2006. Spectral partitioning works: Planar graphs and finite element meshes. Elsevier Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Resnik Diab</author>
<author>P</author>
</authors>
<title>An Unsupervised Method for Word Sense Tagging using Parallel Corpora.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>255--262</pages>
<marker>Diab, P, 2002</marker>
<rawString>Diab, M. Resnik, P. 2002. An Unsupervised Method for Word Sense Tagging using Parallel Corpora. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. 255-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan R K Chung</author>
</authors>
<title>Spectral Graph Theory.</title>
<date>1997</date>
<publisher>American Mathematical Society,</publisher>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="8444" citStr="Chung, 1997" startWordPosition="1511" endWordPosition="1512">e 1 gave as a result two graphs that are not connected. Dyvik argues that in such a case the graphs represent two groups of words of different senses. In a larger and more realistic example one is likely to obtain a graph that is connected, but which can be partitioned into two subgraphs without breaking more than a small number of edges. Then it is reasonable to ask whether such a partitioning has a similar effect in that it represents a partitioning of the words into different senses. We describe the mathematical procedure of partitioning a graph into subgraphs, using spectral graph theory (Chung, 1997). First, define the degree d(i) of a vertex i to be d(i) = � A(i, j). 9 Let D be the diagonal matrix defined by � d(i), if i = j, D(i,j) = 0, otherwise. The Laplacian L is defined as L = D − A. We define the normalised Laplacian L to be L = D−2 LD−1 1 2 . Now calculate the eigenvalues λ0, ... , λ,1, and the eigenvectors of L. The smallest eigenvalue, λ0, is always equal to zero, as shown by Chung (1997). The multiplicity of zero among the eigenvalues is equal to the number of connected components in the graph, as shown by Spielman (2009). We will look at the eigenvector belonging to the second</context>
</contexts>
<marker>Chung, 1997</marker>
<rawString>Fan R. K. Chung. 1997. Spectral Graph Theory. American Mathematical Society, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dyvik</author>
</authors>
<title>A Translational Basis for Semantics. In:</title>
<date>1998</date>
<booktitle>Stig Johansson and Signe Oksefjell (eds.): Corpora and Crosslinguistic Research: Theory, Method and Case Studies,</booktitle>
<pages>51--86</pages>
<publisher>Rodopi.</publisher>
<contexts>
<context position="923" citStr="Dyvik (1998" startWordPosition="135" endWordPosition="136">sents words and their translations from a parallel corpus or a bilingual lexicon. The connectedness of the graph holds information about the different meanings of words that occur in the translations. Spectral graph theory is used to partition the graph, which leads to a grouping of the words according to different senses. We also report results from an evaluation using a small sample of seed words from a lexicon of Swedish and English adjectives. 1 Introduction A great deal of linguistic knowledge is encoded implicitly in bilingual resources such as parallel texts and bilingual dictionaries. Dyvik (1998, 2005) has provided a knowledge discovery method based on the semantic relationship between words in a source language and words in a target language, as manifested in parallel texts. His method is called Semantic mirroring and the approach utilizes the way that different languages encode lexical meaning by mirroring source words and target words back and forth, in order to establish semantic relations like synonymy and hyponymy. Work in this area is strongly related to work within Word Sense Disambiguation (WSD) and the observation that translations are a good source for detecting such disti</context>
<context position="3715" citStr="Dyvik (1998)" startWordPosition="601" endWordPosition="602"> We start with an English word, called eng11. We look up its Swedish translations. Then we look up &apos;Short for english1. We will use swe for Swedish words. 103 Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 103–107, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics the English translations of each of those Swedish words. We have now performed one ”mirroroperation”. In mathematical notation: - f = BB T eeng1. The non-zero elements in the vector f represent English words that are semantically related to eng1. Dyvik (1998) calls the set of words that we get after two translations the inverse t-image. But there is one problem. The original word should not be here. Therefore, in the last translation, we modify the matrix B, by replacing the row in B corresponding to eng1, with an all-zero row. Call this new modified matrix Bmod1. So instead of the matrix multiplication performed above, we start over with the following one: T Bmod1B -eeng1. (1) To make it clearer from a linguistic perspective, consider the following figure2. The words to the right in the picture above (eng2,...,eng5) are the words we want to divid</context>
</contexts>
<marker>Dyvik, 1998</marker>
<rawString>H. Dyvik. 1998. A Translational Basis for Semantics. In: Stig Johansson and Signe Oksefjell (eds.): Corpora and Crosslinguistic Research: Theory, Method and Case Studies, pp. 51-86. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dyvik</author>
</authors>
<title>Translations as a Semantic Knowledge Source.</title>
<date>2005</date>
<booktitle>Proceedings of the Second Baltic Conference on Human Language Technologies,</booktitle>
<location>Tallinn.</location>
<marker>Dyvik, 2005</marker>
<rawString>H. Dyvik. 2005. Translations as a Semantic Knowledge Source. Proceedings of the Second Baltic Conference on Human Language Technologies, Tallinn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
</authors>
<title>Cross-lingual sense determination: Can it work? Computers and the Humanities: Special issue on SENSEVAL,</title>
<date>2000</date>
<pages>34--223</pages>
<contexts>
<context position="1564" citStr="Ide 2000" startWordPosition="239" endWordPosition="240">discovery method based on the semantic relationship between words in a source language and words in a target language, as manifested in parallel texts. His method is called Semantic mirroring and the approach utilizes the way that different languages encode lexical meaning by mirroring source words and target words back and forth, in order to establish semantic relations like synonymy and hyponymy. Work in this area is strongly related to work within Word Sense Disambiguation (WSD) and the observation that translations are a good source for detecting such distinctions (Resnik &amp; Yarowsky 1999, Ide 2000, Diab &amp; Resnik 2002). A word that has multiple meanings in one language is likely to have different translations in other languages. This means that translations serve as sense indicators for a particular source Magnus Merkel Link¨oping University Link¨oping, Sweden magnus.merkel@liu.se Lars Ahrenberg Link¨oping University Link¨oping, Sweden lars.ahrenberg@liu.se word, and make it possible to divide a given word into different senses. In this paper we propose a new graph-based approach to the analysis of semantic mirrors. The objective is to find a viable way to discover synonyms and group th</context>
</contexts>
<marker>Ide, 2000</marker>
<rawString>Nancy Ide. 2000. Cross-lingual sense determination: Can it work? Computers and the Humanities: Special issue on SENSEVAL, 34:223–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Distinguishing systems and distinguishing senses: new evaluation methods for Word Sense Disambiguation Natural Language Engineering,</title>
<date>1999</date>
<volume>5</volume>
<pages>113--133</pages>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik , David Yarowsky. Distinguishing systems and distinguishing senses: new evaluation methods for Word Sense Disambiguation Natural Language Engineering, v.5 n.2, p.113-133, June 1999</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>