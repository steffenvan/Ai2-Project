<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003719">
<title confidence="0.97523">
Segregatory Coordination and Ellipsis in Text Generation
</title>
<author confidence="0.997486">
James Shaw
</author>
<affiliation confidence="0.996427">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.966871">
New York, NY 10027, USA
</address>
<email confidence="0.941147">
shawOcs.columbia.edu
</email>
<sectionHeader confidence="0.991391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999206714285714">
In this paper, we provide an account of how
to generate sentences with coordination con-
structions from clause-sized semantic represen-
tations. An algorithm is developed and various
examples from linguistic literature will be used
to demonstrate that the algorithm does its job
well.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932296875">
The linguistic literature has described numer-
ous coordination phenomena (Gleitman, 1965;
Ross, 1967; Neijt, 1979; Quirk et al., 1985; van
Oirsouw, 1987; Steedman, 1990; Pollard and
Sag, 1994; Carpenter, 1998). We will not ad-
dress common problems associated with pars-
ing, such as disambiguation and construction of
syntactic structures from a string. Instead, we
show how to generate sentences with complex
coordinate constructions starting from seman-
tic representations. We have divided the pro-
cess of generating coordination expressions into
two major tasks, identifying recurring elements
in the conjoined semantic structure and delet-
ing redundant elements using syntactic informa-
tion. Using this model, we are able to handle
coordination phenomenon uniformly, including
difficult cases such as non-constituent coordina-
tion.
In this paper, we are specifically interested in
the generation of segregatory coordination con-
structions. In segregatory coordination, the co-
ordination of smaller units is logically equivalent
to coordination of clauses; for example, &amp;quot;John
likes Mary and Nancy&amp;quot; is logically equivalent
to &amp;quot;John likes Mary&amp;quot; and &amp;quot;John likes Nancy&amp;quot;.
Other similar conjunction coordination phe-
nomena, such as combinatory and rhetorical co-
ordination, are treated differently in text gener-
ation systems. Since these constructions cannot
be analyzed as separate clauses, we will define
them here, but will not describe them further
in the paper. In combinatory coordination, the
sentence &amp;quot;Mary and Nancy are sisters.&amp;quot; is not
equivalent to &amp;quot;Mary is a sister.&amp;quot; and &amp;quot;Nancy
is a sister.&amp;quot; The coordinator &amp;quot;and&amp;quot; sometimes
can function as a rhetorical marker as in &amp;quot;The
train sounded the whistle and [then] departed
the station.&amp;quot;1
To illustrate the common usage of coordina-
tion constructions, we will use a system which
generates reports describing how much work
each employee has performed in an imaginary
supermarket human resource department. Gen-
erating a separate sentence for each tuple in the
relational database would result in: &amp;quot;John re-
arranged cereals in Aisle 2 on Monday. John
rearranged candies in Aisle 2 on Tuesday.&amp;quot; A
system capable of generating segregatory coor-
dination construction can produce a shorter sen-
tence: &amp;quot;John rearranged cereals in Aisle 2 on
Monday and candies on Tuesday.&amp;quot;
In the next section, we briefly describe the
architecture of our generation system and the
modules that handle coordination construction.
A comparison with related work in text gener-
ation is presented in Section 3. In Section 4,
we describe the semantic representation used
for coordination. An algorithm for carrying
out segregatory coordination is provided in Sec-
tion 5 with an example. In Section 6, we will
analyze various examples taken from linguistic
literature and describe how they are handled by
the current algorithm.
</bodyText>
<sectionHeader confidence="0.988866" genericHeader="introduction">
2 Generation Architecture
</sectionHeader>
<bodyText confidence="0.9995086">
Traditional text generation systems contain a
strategic and a tactical component. The strate-
gic component determines what to say and the
order in which to say it while the tactical com-
ponent determines how to say it. Even though
</bodyText>
<footnote confidence="0.663862333333333">
&apos;The string enclosed in symbols [ and ] are deleted
from the surface expression, but these concepts exist in
the semantic representation.
</footnote>
<page confidence="0.987134">
1220
</page>
<bodyText confidence="0.999925021276596">
the strategic component must first decide which
clauses potentially might be combined, it does
not have access to lexical and syntactic knowl-
edge to perform clause combining as the tac-
tical component does. We have implemented a
sentence planner, CASPER (Clause Aggregation
in Sentence PlannER), as the first module in
the tactical component to handle clause combin-
ing. The main tasks of the sentence planner are
clause aggregation, sentence boundary determi-
nation and paraphrasing decisions based on con-
text (Wanner and Hovy, 1996; Shaw, 1995).
The output of the sentence planner is an or-
dered list of semantic structures each of which
can be realized as a sentence. A lexical chooser,
based on a lexicon and the preferences speci-
fied from the sentence planner, determines the
lexical items to represent the semantic concepts
in the representation. The lexicalized result is
then transformed into a syntactic structure and
linearized into a string using FUF/SURGE (El-
hadad, 1993; Robin, 1995), a realization compo-
nent based on Functional Unification Grammar
(Halliday, 1994; Kay, 1984).
Though every component in the architecture
contributes to the generation of coordinate con-
structions, most of the coordination actions take
place in the sentence planner and the lexical
chooser. These two modules reflect the two
main tasks of generating coordination conjunc-
tion: the sentence planner identifies recurring
elements among the coordinated propositions,
and the lexical chooser determines which recur-
ring elements to delete. The reason for such a
division is that ellipsis depends on the sequen-
tial order of the recurring elements at surface
level. This information is only available after
syntactic and lexical decisions have been made.
For example, in &amp;quot;On Monday, John rearranged
cereals in Aisle 2 and cookies in Aisle 4.&amp;quot;, the
second time PP is deleted, but in &amp;quot;John rear-
ranged cereals in Aisle 2 and cookies in Aisle
4 on Monday.&amp;quot;, the first time PP is deleted.2
CASPER only marks the elements as recurring
and let the lexical chooser make deletion deci-
sions later. A more detailed description is pro-
vided in Section 5.
</bodyText>
<footnote confidence="0.992414166666667">
2The expanded first example is &amp;quot;On Monday, John
rearranged cereals in Aisle 2 and [on Monday], [John]
[rearranged] cookies in Aisle 4.&amp;quot; The expanded second
example is &amp;quot;John rearranged cereals in Aisle 2 [on Mon-
day] and [John] [rearranged] cookies in Aisle 4 on Mon-
day.&amp;quot;
</footnote>
<sectionHeader confidence="0.999193" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999962763157895">
Because sentences with coordination can ex-
press a lot of information with fewer words,
many text generation systems have imple-
mented the generation of coordination with var-
ious levels of complexities. In earlier systems
such as EPICURE (Dale, 1992), sentences with
conjunction are formed in the strategic compo-
nent as discourse-level optimizations. Current
systems handle aggregations decisions including
coordination and lexical aggregation, such as
transforming propositions into modifiers (adjec-
tives, prepositional phrases, or relative clauses),
in a sentence planner (Scott and de Souza, 1990;
Dalianis and Hovy, 1993; Huang and Fiedler,
1996; Callaway and Lester, 1997; Shaw, 1998).
Though other systems have implemented co-
ordination, their aggregation rules only handle
simple conjunction inside a syntactic structure,
such as subject, object, or predicate. In con-
trast to these localized rules, the staged algo-
rithm used in CASPER is global in the sense that
it tries to find the most concise coordination
structures across all the propositions. In addi-
tion, a simple heuristic was proposed to avoid
generating overly complex and potentially am-
biguous sentences as a result of coordination.
CASPER also systematically handles ellipsis and
coordination in prepositional clauses which were
not addressed before. When multiple proposi-
tions are combined, the sequential order of the
propositions is an interesting issue. (Dalianis
and Hovy, 1993) proposed a domain specific or-
dering, such as preferring a proposition with an
animate subject to appear before a proposition
with an inanimate subject. CASPER sequential-
izes the propositions according to an order that
allows the most concise coordination of propo-
sitions.
</bodyText>
<sectionHeader confidence="0.975029" genericHeader="method">
4 The Semantic Representation
</sectionHeader>
<bodyText confidence="0.999859363636364">
CASPER uses a representation influenced by
Lexical-Functional Grammar (Kaplan and Bres-
nan, 1982) and Semantic Structures (Jackend-
off, 1990). While it would have been natural
to use thematic roles proposed in Functional
Grammar, because our realization component,
FUF/SURGE, uses them, these roles would
add more complexity into the coordination pro-
cess. One major task of generating coordina-
tion expression is identifying identical elements
in the propositions being combined. In Func-
</bodyText>
<page confidence="0.960147">
1221
</page>
<figure confidence="0.9976441">
((pred ((pred c-lose) (type EVENT)
(tense past)))
(argl ((pred c-name) (type THING)
(first-name &amp;quot;John&amp;quot;)))
(arg2 ((pred c-laptop) (type THING)
(specific no)
(mod ((pred c-expensive)
(type ATTRIBUTE)))))
(mod ((pred c-yesterday)
(type TIME))))
</figure>
<figureCaption confidence="0.9862355">
Figure 1: Semantic representation for &amp;quot;John
lost an expensive laptop yesterday.&amp;quot;
</figureCaption>
<bodyText confidence="0.870536">
Al re-stocked milk in Aisle 5 on Monday.
Al re-stocked coffee in Aisle 2 on Monday.
Al re-stocked tea in Aisle 2 on Monday.
Al re-stocked bread in Aisle 3 on Friday.
</bodyText>
<figureCaption confidence="0.989329">
Figure 2: A sample of input semantic represen-
tations in surface form.
</figureCaption>
<bodyText confidence="0.999990814814815">
tional Grammar, different processes have differ-
ent names for their thematic roles (e.g., MEN-
TAL process has role SENSER for agent while
INTENSIVE process has role IDENTIFIED).
As a result, identifying identical elements un-
der various thematic roles requires looking at
the process first in order to figure out which
thematic roles should be checked for redun-
dancy. Compared to Lexical-Functional Gram-
mar which uses the same feature names, the the-
matic roles for Functional Grammar makes the
identifying task more complicated.
In our representation, the roles for each event
or state are PRED, ARG1, ARG2, ARG3, and
MOD. The slot PRED stores the verb concept.
Depending on the concept in PRED, ARG1,
ARG2, and ARG3 can take on different the-
matic roles, such as Actor, Beneficiary, and
Goal in &amp;quot;John gave Mary a red book yester-
day.&amp;quot; respectively. The optional slot MOD
stores modifiers of the PRED. It can have one
or multiple circumstantial elements, including
MANNER, PLACE, or TIME. Inside each argu-
ment slot, it too has a MOD slot to store infor-
mation such as POSSESSOR or ATTRIBUTE.
An example of the semantic representation is
provided in Figure 1.
</bodyText>
<sectionHeader confidence="0.990046" genericHeader="method">
5 Coordination Algorithm
</sectionHeader>
<bodyText confidence="0.988432142857143">
We have divided the algorithm into four stages,
where the first three stages take place in the
sentence planner and the last stage takes place
Al re-stocked coffee in Aisle 2 on Monday.
Al re-stocked tea in Aisle 2 on Monday.
Al re-stocked milk in Aisle 5 on Monday.
Al re-stocked bread in Aisle 3 on Friday.
</bodyText>
<figureCaption confidence="0.996645">
Figure 3: Propositions in surface form after Stage 1.
</figureCaption>
<bodyText confidence="0.9985768">
in the lexical chooser:
Stage 1: group propositions and order them
according to their similarities while satisfy-
ing pragmatic and contextual constraints.
Stage 2: determine recurring elements in the
ordered propositions that will be combined.
Stage 3: create a sentence boundary when the
combined clause reaches pre-determined
thresholds.
Stage 4: determine which recurring elements
are redundant and should be deleted.
In the following sections, we provide detail on
each stage. To illustrate, we use the imaginary
employee report generation system for a human
resource department in a supermarket.
</bodyText>
<subsectionHeader confidence="0.996218">
5.1 Group and Order Propositions
</subsectionHeader>
<bodyText confidence="0.999986793103448">
It is desirable to group together propositions
with similar elements because these elements
are likely to be inferable and thus redundant
at surface level and deleted. There are many
ways to group and order propositions based on
similarities. For the propositions in Figure 2,
the semantic representations have the follow-
ing slots: PRED, ARG1, ARG2, MOD-PLACE,
and MOD-TIME. To identify which slot has the
most similarity among its elements, we calcu-
late the number of distinct elements in each
slot across the propositions, which we call NDE
(number of distinct elements). For the purpose
of generating concise text, the system prefers to
group propositions which result in as many slots
with NDE = 1 as possible. For the propositions
in Figure 2, both NDEs of PRED and ARG1
are 1 because all the actions are &amp;quot;re-stock&amp;quot; and
all the agents are &amp;quot;Al&amp;quot;; the NDE for ARG2 is 4
because it contains 4 distinct elements: &amp;quot;milk&amp;quot;,
&amp;quot;coffee&amp;quot;, &amp;quot;tea&amp;quot;, and &amp;quot;bread&amp;quot;; similarly, the NDE
of MOD-PLACE is 3; the NDE of MOD-TIME
is 2 (&amp;quot;on Monday&amp;quot; and &amp;quot;on Friday&amp;quot;).
The algorithm re-orders the propositions by
sorting the elements in each slots using compar-
ison operators which can determine that Mon-
day is smaller than Tuesday, or Aisle 2 is smaller
than Aisle 4. Starting from the slots with
largest NDE to the lowest, the algorithm re-
</bodyText>
<page confidence="0.956089">
1222
</page>
<table confidence="0.976366222222222">
((pred c-and) (type LIST)
(elts
&amp;quot;(((pred ((pred &amp;quot;re-stocked&amp;quot;) (type EVENT)
(status RECURRING)))
(argi ((pred &amp;quot;Al&amp;quot;) (TYPE THING)
(status RECURRING)))
(arg2 ((pred &amp;quot;tea&amp;quot;) (type THING)))
(mod ((pred &amp;quot;on&amp;quot;) (type TIME)
(argl ((pred &amp;quot;Monday&amp;quot;)
(type TIME-THING))))))
((pred ((pred &amp;quot;re-stocked&amp;quot;) (type EVENT)
(status RECURRING)))
(argl ((pred &amp;quot;Al&amp;quot;) (TYPE THING)
(status RECURRING)))
(arg2 ((pred &amp;quot;milk&amp;quot;) (type THING)))
(mod ((pred &amp;quot;on&amp;quot;) (type TIME)
(argl ((pred &amp;quot;Friday&amp;quot;)
(type TIME-THING)))))))))
</table>
<figureCaption confidence="0.995659">
Figure 4: The simplified semantic representation
for &amp;quot;Al re-stocked tea on Monday and milk on Fri-
</figureCaption>
<bodyText confidence="0.559275125">
day.&amp;quot; Note: ,-,() a list.
orders the propositions based on the elements of
each particular slot. In this case, propositions
will ordered according to their ARG2 first, fol-
lowed by MOD-PLACE, MOD-TIME, ARG1,
and PRED. The sorting process will put similar
propositions adjacent to each other as shown in
Figure 3.
</bodyText>
<subsectionHeader confidence="0.99732">
5.2 Identify Recurring Elements
</subsectionHeader>
<bodyText confidence="0.999643534482759">
The current algorithm makes its decisions in
a sequential order and it combines only two
propositions at any one time. The result propo-
sition is a semantic representation which repre-
sents the result of combining the propositions.
One task of the sentence planner is to find a way
to combine the next proposition in the ordered
propositions into the resulting proposition. In
Stage 2, it is concerned with how many slots
have distinct values and which slots they are.
When multiple adjacent propositions have only
one slot with distinct elements, these proposi-
tions are 1-distinct. A special optimization can
be carried out between the 1-distinct proposi-
tions by conjoining their distinct elements into
a coordinate structure, such as conjoined verbs,
nouns, or adjectives. McCawley (McCawley,
1981) described this phenomenon as Conjunc-
tion Reduction — &amp;quot;whereby conjoined clauses
that differ only in one item can be replaced by
a simple clause that involves conjoining that
item.&amp;quot; In our example, the first and second
propositions are 1-distinct at ARG2, and they
are combined into a semantic structure repre-
senting &amp;quot;Al re-stocked coffee and tea in Aisle
2 on Monday.&amp;quot; If the third proposition is 1-
distinct at ARG2 in respect to the result propo-
sition also, the element &amp;quot;milk&amp;quot; in ARG2 of the
third proposition would be similarly combined.
In the example, it is not. As a result, we can-
not combine the third proposition using only
conjunction within a syntactic structure.
When the next proposition and the result
proposition have more than one distinct slot or
their 1-distinct slot is different from the previ-
ous 1-distinct slot, the two propositions are said
to be multiple-distinct. Our approach in com-
bining multiple-distinct propositions is different
from previous linguistic analysis. Instead of re-
moving recurring entities right away based on
transformation or substitution, the current sys-
tem generates every conjoined multiple-distinct
proposition. During the generation process
of each conjoined clause, the recurring ele-
ments might be prevented from appearing at
the surface level because the lexical chooser pre-
vented the realization component from generat-
ing any string for such redundant elements. Our
multiple-distinct coordination produces what
linguistics describes as ellipsis and gapping.
Figure 4 shows the result combining two propo-
sitions that will result in &amp;quot;Al re-stocked tea on
Monday and milk on Friday.&amp;quot; Some readers
might notice that PRED and ARG1 in both
propositions are marked as RECURRING but
only subsequent recurring elements are deleted
at surface level. The reason will be explained in
Section 5.4.
</bodyText>
<subsectionHeader confidence="0.995145">
5.3 Determine Sentence Boundary
</subsectionHeader>
<bodyText confidence="0.999876">
Unless combining the next proposition into
the result proposition will exceed the pre-
determined parameters for the complexity of a
sentence, the algorithm will keep on combin-
ing more propositions into the result proposi-
tion using 1-distinct or multiple-distinct coor-
dination. In normal cases, the predefined pa-
rameter limits the number of propositions con-
joined by multiple-distinct coordination to two.
In special cases where the same slots across mul-
tiple propositions are multiple-distinct, the pre-
determined limit is ignored. By taking advan-
tage of parallel structures, these propositions
can be combined using multiple-distinct proce-
dures without making the coordinate structure
more difficult to understand. For example, the
sentence &amp;quot;John took aspirin on Monday, peni-
</bodyText>
<page confidence="0.950349">
1223
</page>
<bodyText confidence="0.9997739">
cillin on Tuesday, and Tylenol on Wednesday.&amp;quot;
is long but quite understandable. Similarly,
conjoining a long list of 3-distinct propositions
produces understandable sentences too: &amp;quot;John
played tennis on Monday, drove to school on
Tuesday, and won the lottery on Wednesday.&amp;quot;
These constraints allow CASPER to produce sen-
tences that are complex and contain a lot of in-
formation, but they are also reasonably easy to
understand.
</bodyText>
<subsectionHeader confidence="0.955292">
5.4 Delete Redundant Elements
</subsectionHeader>
<bodyText confidence="0.999255390243902">
Stage 4 handles ellipsis, one of the most dif-
ficult phenomena to handle in syntax. In the
previous stages, elements that occur more than
once among the propositions are marked as RE-
CURRING, but the actual deletion decisions
have not been made because CASPER lacks the
necessary information. The importance of the
surface sequential order can be demonstrated
by the following example. In the sentence &amp;quot;On
Monday, Al re-stocked coffee and [on Monday,]
[Al] removed rotten milk.&amp;quot;, the elements in
MOD-TIME delete forward (i.e. the subsequent
occurrence of the identical constituent disap-
pears). When MOD-TIME elements are real-
ized at the end of the clause, the same elements
in MOD-TIME delete backward (i.e. the an-
tecedent occurrence of the identical constituent
disappears): &amp;quot;Al re-stocked coffee [on Monday,]
and [Al] removed rotten milk on Monday.&amp;quot; Our
deletion algorithm is an extension to the Di-
rectionality Constraint in (Tai, 1969), which
is based on syntactic structure. Instead, our
algorithm uses the sequential order of the re-
curring element for making deletion decisions.
In general, if a slot is realized at the front or
medial of a clause, the recurring elements in
that slot delete forward. In the first example,
MOD-TIME is realized as the front adverbial
while ARG1, &amp;quot;Al&amp;quot;, appears in the middle of the
clause, so elements in both slots delete forward.
On the other hand, if a slot is realized at the end
position of a clause, the recurring elements in
such slot delete backward, as the MOD-TIME
in second example. The extended directionality
constraint also applies to conjoined premodifiers
and postmodifiers as well, as demonstrated by
&amp;quot;in Aisle 3 and [in Aisle] 4&amp;quot;, and &amp;quot;at 3 [PM] and
[at] 9 PM&amp;quot;.
Using the algorithm just described, the result
of the supermarket example is concise and eas-
ily understandable: &amp;quot;Al re-stocked coffee and
</bodyText>
<reference confidence="0.476075">
1. The Base Plan called for one new fiber activa-
tion at CSA 1061 in 1995 Q2.
2. New 150mb_mux multiplexor placements were
projected at CSA 1160 and 1335 in 1995 Q2.
3. New 150mb_mux multiplexors were placed at
CSA 1178 in 1995 Q4 and at CSA 1835 in 1997
Qi.
4. New 150mb_mux multiplexor placements were
projected at CSA 1160, 1335 and 1338 and one
new 200mb_mux multiplexor placement at CSA
1913b in 1995 Q2.
5. At CSA 2113, the Base Plan called for 32
working-pair transfers in 1997 Q1 and four
working-pair transfers in 1997 Q2 and Q3.
</reference>
<figureCaption confidence="0.996533">
Figure 5: Text generated by CASPER.
</figureCaption>
<bodyText confidence="0.99995675">
tea in Aisle 2 and milk in Aisle 5 on Monday.
Al re-stocked bread in Aisle 3 on Friday.&amp;quot; Fur-
ther discourse processing will replace the second
&amp;quot;Al&amp;quot; with a pronoun &amp;quot;he&amp;quot;, and the adverbial
&amp;quot;also&amp;quot; may be inserted too.
CASPER has been used in an upgraded version
of PLANDoc (McKeown et al., 1994), a robust,
deployed system which generates reports for jus-
tifying the cost to the management in telecom-
munications domain. Some of the current out-
put is shown in Figure 5. In the figure, &amp;quot;CSA&amp;quot;
is a location; &amp;quot;Qi&amp;quot; stands for first quarter;
&amp;quot;multiplexor&amp;quot; and &amp;quot;working-pair transfer&amp;quot; are
telecommunications equipment. The first ex-
ample is a typical simple proposition in the do-
main, which consists of PRED, ARG1, ARG2,
MOD-PLACE, and MOD-TIME. The second
example shows 1-distinct coordination at MOD-
PLACE, where the second CSA been deleted.
The third example demonstrates coordination
of two propositions with multiple-distinct in
MOD-PLACE and MOD-TIME. The fourth ex-
ample shows multiple things: the ARG1 became
plural in the first proposition because multi-
ple placements occurred as indicated by sim-
ple conjunction in MOD-PLACE; the gapping
of the PRED &amp;quot;was projected&amp;quot; in the second
clause was based on multiple-distinct coordina-
tion. The last example demonstrates the dele-
tion of MOD-PLACE in the second proposition
because it is located at the front of the clause at
surface level, so MOD-PLACE deletes forward.
</bodyText>
<sectionHeader confidence="0.975265" genericHeader="method">
6 Linguistic Phenomenon
</sectionHeader>
<bodyText confidence="0.9643735">
In this section, we take examples from various
linguistic literature (Quirk et al., 1985; van Oir-
</bodyText>
<page confidence="0.987072">
1224
</page>
<bodyText confidence="0.999361307692308">
souw, 1987) and show how the algorithm devel-
oped in Section 5 generates them. We also show
how the algorithm can generate sentences with
non-constituent coordination, which pose diffi-
culties for most syntactic theories.
Coordination involves elements of equal syn-
tactic status. Linguists have categorized coor-
dination into simple and complex. Simple coor-
dination conjoins single clauses or clause con-
stituents while complex coordination involves
multiple constituents. For example, the coor-
dinate construction in &amp;quot;John finished his work
and [John] went home.&amp;quot; could be viewed as
a single proposition containing two coordinate
VPs. Based on our algorithm, the phenomenon
would be classified as a multiple-distinct coordi-
nation between two clauses with deleted ARG1,
&amp;quot;John&amp;quot;, in the second clause. In our algorithm,
the 1-distinct procedure can generate many sim-
ple coordinations, including coordinate verbs,
nouns, adjectives, PPs, etc. With simple ex-
tensions to the algorithm, clauses with relative
clauses could be combined and coordinated too.
Complex coordinations involving ellipsis and
gapping are much more challenging. In
multiple-distinct coordination, each conjoined
clause is generated, but recurring elements
among the propositions are deleted depending
on the extended directionality constraints men-
tioned in Subsection 5.4. It works because it
takes advantage of the parallel structure at the
surface level.
Van Oirsouw (van Oirsouw, 1987), based on
the literature on coordinate deletion, identified
a number of rules which result in deletion under
identity: Gapping, which deletes medial mate-
rial; Right-Node-Raising (RNR), which deletes
identical right most constituents in a syntactic
tree; VP-deletion (VPD), which deletes iden-
tical verbs and handles post-auxiliary deletion
(Sag, 1976). Conjunction Reduction (CR),
which deletes identical right-most or leftmost
material. He pointed out that these four rules
reduce the length of a coordination by delet-
ing identical material, and they serve no other
purpose. We will describe how our algorithm
handles the examples van Oirsouw used in Fig-
ure 6.
The algorithm described in Section 5 can use
the multiple-distinct procedure to handle all the
cases except VPD. In the gapping example, the
PRED deletes forward. In RNR, ARG2 deletes
</bodyText>
<reference confidence="0.515110375">
Gapping: John ate fish and Bill 0 rice.
RNR: John caught 0, and Mary killed the ra-
bid dog.
VPD: John sleeps, and Peter does 0, too.
CR1: John gave 0 0, and Peter sold a record
to Sue.
CR2: John gave a book to Mary and 0 0 a
record to Sue.
</reference>
<figureCaption confidence="0.826388">
Figure 6: Four coordination rules for identity
</figureCaption>
<bodyText confidence="0.98789090625">
deletion described by van Oirsouw.
backward because it is positioned at the end of
the clause. In CR1, even though the medial slot
ARG2 should delete forward, it deletes back-
ward because it is considered at the end position
of a clause. In this case, once ARG3 (the BEN-
EFICIARY &amp;quot;to Sue&amp;quot;) deletes backward, ARG2
is at the end position of a clause. This pro-
cess does require more intelligent processing in
the lexical chooser, but it is not difficult. In
CR2, it is straight forward to delete forward be-
cause both ARG1 and PRED are medial. The
current algorithm does not address VPD. For
such a sentence, the system would have gener-
ated &amp;quot;John and Peter slept&amp;quot; using 1-distinct.
Non-constituent coordination phenomena,
the coordination of elements that are not of
equal syntactic status, are challenging for syn-
tactic theories. The following non-constituent
coordination can be explained nicely with the
multiple-distinct procedure. In the sentence,
&amp;quot;The spy was in his forties, of average build, and
spoke with a slightly foreign accent.&amp;quot;, the coordi-
nated constituents are VP, PP, and VP. Based
on our analysis, the sentence could be gener-
ated by combining the first two clauses using
the 1-distinct procedure, and the third clause is
combined using the multiple-distinct procedure,
with ARG1 (&amp;quot;the spy&amp;quot;) deleted forward.
The spy was in his forties, [the spy]
[was] of average build, and [the spy]
spoke with a slightly foreign accent.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999510857142857">
By separating the generation of coordination
constructions into two tasks — identifying re-
curring elements and deleting redundant ele-
ments based on the extended directionality con-
straints, we are able to handle many coordi-
nation constructions correctly, including non-
constituent coordinations. Through numerous
</bodyText>
<page confidence="0.96472">
1225
</page>
<bodyText confidence="0.999878142857143">
examples, we have shown how our algorithm can
generate complex coordinate constructions from
clause-sized semantic representations. Both the
representation and the algorithm have been im-
plemented and used in two different text gener-
ation systems (McKeown et al., 1994; McKeown
et al., 1997).
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999299285714286">
This work is supported by DARPA Contract
DAAL01-94-K-0119, the Columbia University
Center for Advanced Technology in High Per-
formance Computing and Communications in
Healthcare (funded by the New York State
Science and Technology Foundation) and NSF
Grants GER-90-2406.
</bodyText>
<sectionHeader confidence="0.998281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996475663157895">
Charles B. Callaway and James C. Lester. 1997.
Dynamically improving explanations: A revision-
based approach to explanation generation. In
Proc. of the 15th IJCA I, pages 952-958, Nagoya,
Japan.
Bob Carpenter. 1998. Distribution, collection and
quantification: A type-logical account. To appear
in Linguistics and Philosophy.
Robert Dale. 1992. Generating Referring Expres-
sions: Constructing Descriptions in a Domain of
Objects and Processes. MIT Press, Cambridge,
MA.
Hercules Dalianis and Eduard Hovy. 1993. Aggre-
gation in natural language generation. In Proc. of
the 4th European Workshop on Natural Language
Generation, Pisa, Italy.
Michael Elhadad. 1993. Using argumentation to
control lexical choice: A functional unification-
based approach. Ph.D. thesis, Columbia Univer-
sity.
Lila R. Gleitman. 1965. Coordinating conjunctions
in English. Language, 41:260-293.
Michael A. K. Halliday. 1994. An Introduction to
Functional Grammar. Edward Arnold, London,
2nd edition.
Xiaoron Huang and Armin Fiedler. 1996. Para-
phrasing and aggregating argumentative text us-
ing text structure. In Proc. of the 8th Interna-
tional Natural Language Generation Workshop,
pages 21-3, Sussex, UK.
Ray Jackendoff. 1990. Semantic Structures. MIT
Press, Cambridge, MA.
Ronald M. Kaplan and Joan Bresnan. 1982.
Lexical-functional grammar: A formal system for
grammatical representation. In Joan Bresnan, ed-
itor, The Mental Representation of Grammatical
Relations, chapter 4. MIT Press.
Martin Kay. 1984. Functional Unification Gram-
mar: A formalism for machine translation. In
Proc. of the 10th COLING and 22nd ACL, pages
75-78.
James D. McCawley. 1981. Everything that linguists
have always wanted to know about logic (but were
ashamed to ask). University of Chicago Press.
Kathleen McKeown, Karen Kuldch, and James
Shaw. 1994. Practical issues in automatic doc-
umentation generation. In Proc. of the 4th ACL
Conference on Applied Natural Language Process-
ing, pages 7-14, Stuttgart.
Kathleen McKeown, Shimei Pan, James Shaw,
Desmond Jordan, and Barry Allen. 1997. Lan-
guage generation for multimedia healthcare brief-
ings. In Proc. of the Fifth ACL Conf. on ANLP,
pages 277-282.
Anneke H. Neijt. 1979. Gapping: a constribution
to Sentence Grammar. Dordrecht: Foris Publica-
tions.
Carl Pollard and Ivan Sag. 1994. Head-
Driven Phrase Structure Grammar. University of
Chicago Press, Chicago.
Randolph Quirk, Sidney Greebaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman Publish-
ers, London.
Jacques Robin. 1995. Revision-Based Generation of
Natural Language Summaries Providing Histori-
cal Background. Ph.D. thesis, Columbia Univer-
sity.
John Robert Ross. 1967. Constraints on variables
in syntax. Ph.D. thesis, MIT.
Ivan A. Sag. 1976. Deletion and Logical Form.
Ph.D. thesis, MIT.
Donia R. Scott and Clarisse S. de Souza. 1990. Get-
ting the message across in RST-based text gener-
ation. In Robert Dale, Chris Mellish, and Michael
Zock, editors, Current Research in Natural Lan-
guage Generation, pages 47-73. Academic Press,
New York.
James Shaw. 1995. Conciseness through aggrega-
tion in text generation. In Proc. of the 33rd ACL
(Student Session), pages 329-331.
James Shaw. 1998. Clause aggregation using lin-
guistic knowledge. In Proc. of the 9th Interna-
tional Workshop on Natural Language Genera-
tion.
Mark Steedman. 1990. Gapping as constituent coor-
dination. Linguistics and Philosophy, 13:207-264.
J. H.-Y. Tai. 1969. Coordination Reduction. Ph.D.
thesis, Indiana University.
Robert van Oirsouw. 1987. The Syntax of Coordi-
nation. Croom Helm, Beckenham.
Leo Wanner and Eduard Hovy. 1996. The Health-
Doc sentence planner. In Proc. of the 8th Inter-
national Natural Language Generation Workshop,
pages 1-10, Sussex, UK.
</reference>
<page confidence="0.991199">
1226
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911525">
<title confidence="0.99984">Segregatory Coordination and Ellipsis in Text Generation</title>
<author confidence="0.99998">James Shaw</author>
<affiliation confidence="0.9999005">Dept. of Computer Science Columbia University</affiliation>
<address confidence="0.999833">New York, NY 10027, USA</address>
<email confidence="0.999872">shawOcs.columbia.edu</email>
<abstract confidence="0.988773">In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations. An algorithm is developed and various examples from linguistic literature will be used to demonstrate that the algorithm does its job well.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Base Plan called for one new fiber activation at CSA 1061 in</title>
<date>1995</date>
<pages>2</pages>
<marker>1995</marker>
<rawString>1. The Base Plan called for one new fiber activation at CSA 1061 in 1995 Q2.</rawString>
</citation>
<citation valid="true">
<title>New 150mb_mux multiplexor placements were projected at CSA 1160 and 1335 in</title>
<date>1995</date>
<pages>2</pages>
<marker>1995</marker>
<rawString>2. New 150mb_mux multiplexor placements were projected at CSA 1160 and 1335 in 1995 Q2.</rawString>
</citation>
<citation valid="true">
<title>New 150mb_mux multiplexors were placed at CSA 1178</title>
<date>1997</date>
<booktitle>in 1995 Q4 and at CSA 1835 in</booktitle>
<note>Qi.</note>
<marker>1997</marker>
<rawString>3. New 150mb_mux multiplexors were placed at CSA 1178 in 1995 Q4 and at CSA 1835 in 1997 Qi.</rawString>
</citation>
<citation valid="true">
<title>New 150mb_mux multiplexor placements were projected at CSA 1160, 1335 and 1338 and one new 200mb_mux multiplexor placement at CSA 1913b in</title>
<date>1995</date>
<pages>2</pages>
<marker>1995</marker>
<rawString>4. New 150mb_mux multiplexor placements were projected at CSA 1160, 1335 and 1338 and one new 200mb_mux multiplexor placement at CSA 1913b in 1995 Q2.</rawString>
</citation>
<citation valid="true">
<title>At CSA 2113, the Base Plan called for 32 working-pair transfers in 1997 Q1 and four working-pair transfers in</title>
<date>1997</date>
<note>Q2 and Q3.</note>
<marker>1997</marker>
<rawString>5. At CSA 2113, the Base Plan called for 32 working-pair transfers in 1997 Q1 and four working-pair transfers in 1997 Q2 and Q3.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gapping John</author>
</authors>
<title>ate fish and Bill 0 rice. RNR: John caught 0, and Mary killed the rabid dog.</title>
<marker>John, </marker>
<rawString>Gapping: John ate fish and Bill 0 rice. RNR: John caught 0, and Mary killed the rabid dog.</rawString>
</citation>
<citation valid="false">
<authors>
<author>VPD John</author>
</authors>
<title>sleeps, and Peter does 0, too. CR1: John gave 0 0, and Peter sold a record to Sue.</title>
<marker>John, </marker>
<rawString>VPD: John sleeps, and Peter does 0, too. CR1: John gave 0 0, and Peter sold a record to Sue.</rawString>
</citation>
<citation valid="false">
<title>CR2: John gave a book to Mary and 0 0 a record to Sue.</title>
<marker></marker>
<rawString>CR2: John gave a book to Mary and 0 0 a record to Sue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
<author>James C Lester</author>
</authors>
<title>Dynamically improving explanations: A revisionbased approach to explanation generation.</title>
<date>1997</date>
<booktitle>In Proc. of the 15th IJCA I,</booktitle>
<pages>952--958</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="6777" citStr="Callaway and Lester, 1997" startWordPosition="1043" endWordPosition="1046"> lot of information with fewer words, many text generation systems have implemented the generation of coordination with various levels of complexities. In earlier systems such as EPICURE (Dale, 1992), sentences with conjunction are formed in the strategic component as discourse-level optimizations. Current systems handle aggregations decisions including coordination and lexical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, object, or predicate. In contrast to these localized rules, the staged algorithm used in CASPER is global in the sense that it tries to find the most concise coordination structures across all the propositions. In addition, a simple heuristic was proposed to avoid generating overly complex and potentially ambiguous sentences as a result of coordination. CASPER also systematically handles ellipsis and coordination in preposition</context>
</contexts>
<marker>Callaway, Lester, 1997</marker>
<rawString>Charles B. Callaway and James C. Lester. 1997. Dynamically improving explanations: A revisionbased approach to explanation generation. In Proc. of the 15th IJCA I, pages 952-958, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Distribution, collection and quantification: A type-logical account.</title>
<date>1998</date>
<note>To appear in Linguistics and Philosophy.</note>
<contexts>
<context position="673" citStr="Carpenter, 1998" startWordPosition="95" endWordPosition="96">James Shaw Dept. of Computer Science Columbia University New York, NY 10027, USA shawOcs.columbia.edu Abstract In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations. An algorithm is developed and various examples from linguistic literature will be used to demonstrate that the algorithm does its job well. 1 Introduction The linguistic literature has described numerous coordination phenomena (Gleitman, 1965; Ross, 1967; Neijt, 1979; Quirk et al., 1985; van Oirsouw, 1987; Steedman, 1990; Pollard and Sag, 1994; Carpenter, 1998). We will not address common problems associated with parsing, such as disambiguation and construction of syntactic structures from a string. Instead, we show how to generate sentences with complex coordinate constructions starting from semantic representations. We have divided the process of generating coordination expressions into two major tasks, identifying recurring elements in the conjoined semantic structure and deleting redundant elements using syntactic information. Using this model, we are able to handle coordination phenomenon uniformly, including difficult cases such as non-constit</context>
</contexts>
<marker>Carpenter, 1998</marker>
<rawString>Bob Carpenter. 1998. Distribution, collection and quantification: A type-logical account. To appear in Linguistics and Philosophy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="6351" citStr="Dale, 1992" startWordPosition="988" endWordPosition="989">letion decisions later. A more detailed description is provided in Section 5. 2The expanded first example is &amp;quot;On Monday, John rearranged cereals in Aisle 2 and [on Monday], [John] [rearranged] cookies in Aisle 4.&amp;quot; The expanded second example is &amp;quot;John rearranged cereals in Aisle 2 [on Monday] and [John] [rearranged] cookies in Aisle 4 on Monday.&amp;quot; 3 Related Work Because sentences with coordination can express a lot of information with fewer words, many text generation systems have implemented the generation of coordination with various levels of complexities. In earlier systems such as EPICURE (Dale, 1992), sentences with conjunction are formed in the strategic component as discourse-level optimizations. Current systems handle aggregations decisions including coordination and lexical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, objec</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hercules Dalianis</author>
<author>Eduard Hovy</author>
</authors>
<title>Aggregation in natural language generation.</title>
<date>1993</date>
<booktitle>In Proc. of the 4th European Workshop on Natural Language Generation,</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="6725" citStr="Dalianis and Hovy, 1993" startWordPosition="1035" endWordPosition="1038"> Because sentences with coordination can express a lot of information with fewer words, many text generation systems have implemented the generation of coordination with various levels of complexities. In earlier systems such as EPICURE (Dale, 1992), sentences with conjunction are formed in the strategic component as discourse-level optimizations. Current systems handle aggregations decisions including coordination and lexical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, object, or predicate. In contrast to these localized rules, the staged algorithm used in CASPER is global in the sense that it tries to find the most concise coordination structures across all the propositions. In addition, a simple heuristic was proposed to avoid generating overly complex and potentially ambiguous sentences as a result of coordination. CASPER also systematica</context>
</contexts>
<marker>Dalianis, Hovy, 1993</marker>
<rawString>Hercules Dalianis and Eduard Hovy. 1993. Aggregation in natural language generation. In Proc. of the 4th European Workshop on Natural Language Generation, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>Using argumentation to control lexical choice: A functional unificationbased approach.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="4671" citStr="Elhadad, 1993" startWordPosition="715" endWordPosition="717">ng. The main tasks of the sentence planner are clause aggregation, sentence boundary determination and paraphrasing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture contributes to the generation of coordinate constructions, most of the coordination actions take place in the sentence planner and the lexical chooser. These two modules reflect the two main tasks of generating coordination conjunction: the sentence planner identifies recurring elements among the coordinated propositions, and the lexical chooser determines which recurring elements to delete. The reason for such a division is that ellipsis depen</context>
</contexts>
<marker>Elhadad, 1993</marker>
<rawString>Michael Elhadad. 1993. Using argumentation to control lexical choice: A functional unificationbased approach. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lila R Gleitman</author>
</authors>
<date>1965</date>
<booktitle>Coordinating conjunctions in English. Language,</booktitle>
<pages>41--260</pages>
<marker>Gleitman, 1965</marker>
<rawString>Lila R. Gleitman. 1965. Coordinating conjunctions in English. Language, 41:260-293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar. Edward Arnold, London, 2nd edition.</title>
<date>1994</date>
<contexts>
<context position="4766" citStr="Halliday, 1994" startWordPosition="729" endWordPosition="730">tion and paraphrasing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture contributes to the generation of coordinate constructions, most of the coordination actions take place in the sentence planner and the lexical chooser. These two modules reflect the two main tasks of generating coordination conjunction: the sentence planner identifies recurring elements among the coordinated propositions, and the lexical chooser determines which recurring elements to delete. The reason for such a division is that ellipsis depends on the sequential order of the recurring elements at surface level. This information is only</context>
</contexts>
<marker>Halliday, 1994</marker>
<rawString>Michael A. K. Halliday. 1994. An Introduction to Functional Grammar. Edward Arnold, London, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoron Huang</author>
<author>Armin Fiedler</author>
</authors>
<title>Paraphrasing and aggregating argumentative text using text structure.</title>
<date>1996</date>
<booktitle>In Proc. of the 8th International Natural Language Generation Workshop,</booktitle>
<pages>21--3</pages>
<location>Sussex, UK.</location>
<contexts>
<context position="6750" citStr="Huang and Fiedler, 1996" startWordPosition="1039" endWordPosition="1042">oordination can express a lot of information with fewer words, many text generation systems have implemented the generation of coordination with various levels of complexities. In earlier systems such as EPICURE (Dale, 1992), sentences with conjunction are formed in the strategic component as discourse-level optimizations. Current systems handle aggregations decisions including coordination and lexical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, object, or predicate. In contrast to these localized rules, the staged algorithm used in CASPER is global in the sense that it tries to find the most concise coordination structures across all the propositions. In addition, a simple heuristic was proposed to avoid generating overly complex and potentially ambiguous sentences as a result of coordination. CASPER also systematically handles ellipsis and </context>
</contexts>
<marker>Huang, Fiedler, 1996</marker>
<rawString>Xiaoron Huang and Armin Fiedler. 1996. Paraphrasing and aggregating argumentative text using text structure. In Proc. of the 8th International Natural Language Generation Workshop, pages 21-3, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantic Structures.</title>
<date>1990</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7995" citStr="Jackendoff, 1990" startWordPosition="1226" endWordPosition="1228">l clauses which were not addressed before. When multiple propositions are combined, the sequential order of the propositions is an interesting issue. (Dalianis and Hovy, 1993) proposed a domain specific ordering, such as preferring a proposition with an animate subject to appear before a proposition with an inanimate subject. CASPER sequentializes the propositions according to an order that allows the most concise coordination of propositions. 4 The Semantic Representation CASPER uses a representation influenced by Lexical-Functional Grammar (Kaplan and Bresnan, 1982) and Semantic Structures (Jackendoff, 1990). While it would have been natural to use thematic roles proposed in Functional Grammar, because our realization component, FUF/SURGE, uses them, these roles would add more complexity into the coordination process. One major task of generating coordination expression is identifying identical elements in the propositions being combined. In Func1221 ((pred ((pred c-lose) (type EVENT) (tense past))) (argl ((pred c-name) (type THING) (first-name &amp;quot;John&amp;quot;))) (arg2 ((pred c-laptop) (type THING) (specific no) (mod ((pred c-expensive) (type ATTRIBUTE))))) (mod ((pred c-yesterday) (type TIME)))) Figure 1</context>
</contexts>
<marker>Jackendoff, 1990</marker>
<rawString>Ray Jackendoff. 1990. Semantic Structures. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-functional grammar: A formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations, chapter 4.</booktitle>
<editor>In Joan Bresnan, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7952" citStr="Kaplan and Bresnan, 1982" startWordPosition="1218" endWordPosition="1222">y handles ellipsis and coordination in prepositional clauses which were not addressed before. When multiple propositions are combined, the sequential order of the propositions is an interesting issue. (Dalianis and Hovy, 1993) proposed a domain specific ordering, such as preferring a proposition with an animate subject to appear before a proposition with an inanimate subject. CASPER sequentializes the propositions according to an order that allows the most concise coordination of propositions. 4 The Semantic Representation CASPER uses a representation influenced by Lexical-Functional Grammar (Kaplan and Bresnan, 1982) and Semantic Structures (Jackendoff, 1990). While it would have been natural to use thematic roles proposed in Functional Grammar, because our realization component, FUF/SURGE, uses them, these roles would add more complexity into the coordination process. One major task of generating coordination expression is identifying identical elements in the propositions being combined. In Func1221 ((pred ((pred c-lose) (type EVENT) (tense past))) (argl ((pred c-name) (type THING) (first-name &amp;quot;John&amp;quot;))) (arg2 ((pred c-laptop) (type THING) (specific no) (mod ((pred c-expensive) (type ATTRIBUTE))))) (mod </context>
</contexts>
<marker>Kaplan, Bresnan, 1982</marker>
<rawString>Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-functional grammar: A formal system for grammatical representation. In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, chapter 4. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional Unification Grammar: A formalism for machine translation.</title>
<date>1984</date>
<booktitle>In Proc. of the 10th COLING and 22nd ACL,</booktitle>
<pages>75--78</pages>
<contexts>
<context position="4778" citStr="Kay, 1984" startWordPosition="731" endWordPosition="732">asing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture contributes to the generation of coordinate constructions, most of the coordination actions take place in the sentence planner and the lexical chooser. These two modules reflect the two main tasks of generating coordination conjunction: the sentence planner identifies recurring elements among the coordinated propositions, and the lexical chooser determines which recurring elements to delete. The reason for such a division is that ellipsis depends on the sequential order of the recurring elements at surface level. This information is only available a</context>
</contexts>
<marker>Kay, 1984</marker>
<rawString>Martin Kay. 1984. Functional Unification Grammar: A formalism for machine translation. In Proc. of the 10th COLING and 22nd ACL, pages 75-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D McCawley</author>
</authors>
<title>Everything that linguists have always wanted to know about logic (but were ashamed to ask).</title>
<date>1981</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="14128" citStr="McCawley, 1981" startWordPosition="2204" endWordPosition="2205">ents the result of combining the propositions. One task of the sentence planner is to find a way to combine the next proposition in the ordered propositions into the resulting proposition. In Stage 2, it is concerned with how many slots have distinct values and which slots they are. When multiple adjacent propositions have only one slot with distinct elements, these propositions are 1-distinct. A special optimization can be carried out between the 1-distinct propositions by conjoining their distinct elements into a coordinate structure, such as conjoined verbs, nouns, or adjectives. McCawley (McCawley, 1981) described this phenomenon as Conjunction Reduction — &amp;quot;whereby conjoined clauses that differ only in one item can be replaced by a simple clause that involves conjoining that item.&amp;quot; In our example, the first and second propositions are 1-distinct at ARG2, and they are combined into a semantic structure representing &amp;quot;Al re-stocked coffee and tea in Aisle 2 on Monday.&amp;quot; If the third proposition is 1- distinct at ARG2 in respect to the result proposition also, the element &amp;quot;milk&amp;quot; in ARG2 of the third proposition would be similarly combined. In the example, it is not. As a result, we cannot combine </context>
</contexts>
<marker>McCawley, 1981</marker>
<rawString>James D. McCawley. 1981. Everything that linguists have always wanted to know about logic (but were ashamed to ask). University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Karen Kuldch</author>
<author>James Shaw</author>
</authors>
<title>Practical issues in automatic documentation generation.</title>
<date>1994</date>
<booktitle>In Proc. of the 4th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>7--14</pages>
<location>Stuttgart.</location>
<marker>McKeown, Kuldch, Shaw, 1994</marker>
<rawString>Kathleen McKeown, Karen Kuldch, and James Shaw. 1994. Practical issues in automatic documentation generation. In Proc. of the 4th ACL Conference on Applied Natural Language Processing, pages 7-14, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Shimei Pan</author>
<author>James Shaw</author>
<author>Desmond Jordan</author>
<author>Barry Allen</author>
</authors>
<title>Language generation for multimedia healthcare briefings.</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth ACL Conf. on ANLP,</booktitle>
<pages>277--282</pages>
<marker>McKeown, Pan, Shaw, Jordan, Allen, 1997</marker>
<rawString>Kathleen McKeown, Shimei Pan, James Shaw, Desmond Jordan, and Barry Allen. 1997. Language generation for multimedia healthcare briefings. In Proc. of the Fifth ACL Conf. on ANLP, pages 277-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anneke H Neijt</author>
</authors>
<title>Gapping: a constribution to Sentence Grammar.</title>
<date>1979</date>
<publisher>Foris Publications.</publisher>
<location>Dordrecht:</location>
<marker>Neijt, 1979</marker>
<rawString>Anneke H. Neijt. 1979. Gapping: a constribution to Sentence Grammar. Dordrecht: Foris Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan Sag</author>
</authors>
<title>HeadDriven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="655" citStr="Pollard and Sag, 1994" startWordPosition="91" endWordPosition="94">sis in Text Generation James Shaw Dept. of Computer Science Columbia University New York, NY 10027, USA shawOcs.columbia.edu Abstract In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations. An algorithm is developed and various examples from linguistic literature will be used to demonstrate that the algorithm does its job well. 1 Introduction The linguistic literature has described numerous coordination phenomena (Gleitman, 1965; Ross, 1967; Neijt, 1979; Quirk et al., 1985; van Oirsouw, 1987; Steedman, 1990; Pollard and Sag, 1994; Carpenter, 1998). We will not address common problems associated with parsing, such as disambiguation and construction of syntactic structures from a string. Instead, we show how to generate sentences with complex coordinate constructions starting from semantic representations. We have divided the process of generating coordination expressions into two major tasks, identifying recurring elements in the conjoined semantic structure and deleting redundant elements using syntactic information. Using this model, we are able to handle coordination phenomenon uniformly, including difficult cases s</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greebaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A Comprehensive Grammar of the English Language.</title>
<date>1985</date>
<publisher>Longman Publishers,</publisher>
<location>London.</location>
<marker>Quirk, Greebaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greebaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman Publishers, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Revision-Based Generation of Natural Language Summaries Providing Historical Background.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="4685" citStr="Robin, 1995" startWordPosition="718" endWordPosition="719">sks of the sentence planner are clause aggregation, sentence boundary determination and paraphrasing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture contributes to the generation of coordinate constructions, most of the coordination actions take place in the sentence planner and the lexical chooser. These two modules reflect the two main tasks of generating coordination conjunction: the sentence planner identifies recurring elements among the coordinated propositions, and the lexical chooser determines which recurring elements to delete. The reason for such a division is that ellipsis depends on the sequ</context>
</contexts>
<marker>Robin, 1995</marker>
<rawString>Jacques Robin. 1995. Revision-Based Generation of Natural Language Summaries Providing Historical Background. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Robert Ross</author>
</authors>
<title>Constraints on variables in syntax.</title>
<date>1967</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<marker>Ross, 1967</marker>
<rawString>John Robert Ross. 1967. Constraints on variables in syntax. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
</authors>
<title>Deletion and Logical Form.</title>
<date>1976</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<marker>Sag, 1976</marker>
<rawString>Ivan A. Sag. 1976. Deletion and Logical Form. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia R Scott</author>
<author>Clarisse S de Souza</author>
</authors>
<title>Getting the message across in RST-based text generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation,</booktitle>
<pages>47--73</pages>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Scott, de Souza, 1990</marker>
<rawString>Donia R. Scott and Clarisse S. de Souza. 1990. Getting the message across in RST-based text generation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation, pages 47-73. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Conciseness through aggregation in text generation.</title>
<date>1995</date>
<booktitle>In Proc. of the 33rd ACL (Student Session),</booktitle>
<pages>329--331</pages>
<contexts>
<context position="4236" citStr="Shaw, 1995" startWordPosition="645" endWordPosition="646">sion, but these concepts exist in the semantic representation. 1220 the strategic component must first decide which clauses potentially might be combined, it does not have access to lexical and syntactic knowledge to perform clause combining as the tactical component does. We have implemented a sentence planner, CASPER (Clause Aggregation in Sentence PlannER), as the first module in the tactical component to handle clause combining. The main tasks of the sentence planner are clause aggregation, sentence boundary determination and paraphrasing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture contributes t</context>
</contexts>
<marker>Shaw, 1995</marker>
<rawString>James Shaw. 1995. Conciseness through aggregation in text generation. In Proc. of the 33rd ACL (Student Session), pages 329-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Clause aggregation using linguistic knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of the 9th International Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="6790" citStr="Shaw, 1998" startWordPosition="1047" endWordPosition="1048">wer words, many text generation systems have implemented the generation of coordination with various levels of complexities. In earlier systems such as EPICURE (Dale, 1992), sentences with conjunction are formed in the strategic component as discourse-level optimizations. Current systems handle aggregations decisions including coordination and lexical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, object, or predicate. In contrast to these localized rules, the staged algorithm used in CASPER is global in the sense that it tries to find the most concise coordination structures across all the propositions. In addition, a simple heuristic was proposed to avoid generating overly complex and potentially ambiguous sentences as a result of coordination. CASPER also systematically handles ellipsis and coordination in prepositional clauses wh</context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>James Shaw. 1998. Clause aggregation using linguistic knowledge. In Proc. of the 9th International Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Gapping as constituent coordination.</title>
<date>1990</date>
<journal>Linguistics and Philosophy,</journal>
<pages>13--207</pages>
<contexts>
<context position="632" citStr="Steedman, 1990" startWordPosition="89" endWordPosition="90">nation and Ellipsis in Text Generation James Shaw Dept. of Computer Science Columbia University New York, NY 10027, USA shawOcs.columbia.edu Abstract In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations. An algorithm is developed and various examples from linguistic literature will be used to demonstrate that the algorithm does its job well. 1 Introduction The linguistic literature has described numerous coordination phenomena (Gleitman, 1965; Ross, 1967; Neijt, 1979; Quirk et al., 1985; van Oirsouw, 1987; Steedman, 1990; Pollard and Sag, 1994; Carpenter, 1998). We will not address common problems associated with parsing, such as disambiguation and construction of syntactic structures from a string. Instead, we show how to generate sentences with complex coordinate constructions starting from semantic representations. We have divided the process of generating coordination expressions into two major tasks, identifying recurring elements in the conjoined semantic structure and deleting redundant elements using syntactic information. Using this model, we are able to handle coordination phenomenon uniformly, incl</context>
</contexts>
<marker>Steedman, 1990</marker>
<rawString>Mark Steedman. 1990. Gapping as constituent coordination. Linguistics and Philosophy, 13:207-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H-Y Tai</author>
</authors>
<title>Coordination Reduction.</title>
<date>1969</date>
<tech>Ph.D. thesis,</tech>
<institution>Indiana University. Robert van Oirsouw.</institution>
<location>Beckenham.</location>
<contexts>
<context position="18169" citStr="Tai, 1969" startWordPosition="2829" endWordPosition="2830">ntial order can be demonstrated by the following example. In the sentence &amp;quot;On Monday, Al re-stocked coffee and [on Monday,] [Al] removed rotten milk.&amp;quot;, the elements in MOD-TIME delete forward (i.e. the subsequent occurrence of the identical constituent disappears). When MOD-TIME elements are realized at the end of the clause, the same elements in MOD-TIME delete backward (i.e. the antecedent occurrence of the identical constituent disappears): &amp;quot;Al re-stocked coffee [on Monday,] and [Al] removed rotten milk on Monday.&amp;quot; Our deletion algorithm is an extension to the Directionality Constraint in (Tai, 1969), which is based on syntactic structure. Instead, our algorithm uses the sequential order of the recurring element for making deletion decisions. In general, if a slot is realized at the front or medial of a clause, the recurring elements in that slot delete forward. In the first example, MOD-TIME is realized as the front adverbial while ARG1, &amp;quot;Al&amp;quot;, appears in the middle of the clause, so elements in both slots delete forward. On the other hand, if a slot is realized at the end position of a clause, the recurring elements in such slot delete backward, as the MOD-TIME in second example. The ext</context>
</contexts>
<marker>Tai, 1969</marker>
<rawString>J. H.-Y. Tai. 1969. Coordination Reduction. Ph.D. thesis, Indiana University. Robert van Oirsouw. 1987. The Syntax of Coordination. Croom Helm, Beckenham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Wanner</author>
<author>Eduard Hovy</author>
</authors>
<title>The HealthDoc sentence planner.</title>
<date>1996</date>
<booktitle>In Proc. of the 8th International Natural Language Generation Workshop,</booktitle>
<pages>1--10</pages>
<location>Sussex, UK.</location>
<contexts>
<context position="4223" citStr="Wanner and Hovy, 1996" startWordPosition="641" endWordPosition="644">from the surface expression, but these concepts exist in the semantic representation. 1220 the strategic component must first decide which clauses potentially might be combined, it does not have access to lexical and syntactic knowledge to perform clause combining as the tactical component does. We have implemented a sentence planner, CASPER (Clause Aggregation in Sentence PlannER), as the first module in the tactical component to handle clause combining. The main tasks of the sentence planner are clause aggregation, sentence boundary determination and paraphrasing decisions based on context (Wanner and Hovy, 1996; Shaw, 1995). The output of the sentence planner is an ordered list of semantic structures each of which can be realized as a sentence. A lexical chooser, based on a lexicon and the preferences specified from the sentence planner, determines the lexical items to represent the semantic concepts in the representation. The lexicalized result is then transformed into a syntactic structure and linearized into a string using FUF/SURGE (Elhadad, 1993; Robin, 1995), a realization component based on Functional Unification Grammar (Halliday, 1994; Kay, 1984). Though every component in the architecture </context>
</contexts>
<marker>Wanner, Hovy, 1996</marker>
<rawString>Leo Wanner and Eduard Hovy. 1996. The HealthDoc sentence planner. In Proc. of the 8th International Natural Language Generation Workshop, pages 1-10, Sussex, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>