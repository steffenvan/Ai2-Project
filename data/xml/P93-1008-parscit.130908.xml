<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000079">
<title confidence="0.9458265">
GEMINI: A NATURAL LANGUAGE SYSTEM FOR
SPOKEN-LANGUAGE UNDERSTANDING*
</title>
<author confidence="0.981827">
John Dowding, Jean Mark Gawron, Doug Appelt,
John Bear, Lynn Cherny, Robert Moore, and Douglas Moran
</author>
<affiliation confidence="0.720086">
SRI International
</affiliation>
<address confidence="0.958512">
333 Ravenswood Avenue
Menlo Park, CA 94025
</address>
<email confidence="0.995022">
Internet: dowding@ai.sri.com
</email>
<sectionHeader confidence="0.999769" genericHeader="abstract">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.972946588235294">
Gemini is a natural language (NL) under-
standing system developed for spoken language
applications. This paper describes the details of
the system, and includes relevant measurements
of size, efficiency, and performance of each of its
components.
In designing any NL understanding system,
there is a tension between robustness and correct-
ness. Forgiving an error risks throwing away cru-
cial information; furthermore, devices added to a
system to enhance robustness can sometimes en-
rich the ways of finding an analysis, multiplying
the number of analyses for a given input, and mak-
ing it more difficult to find the correct analysis. In
processing spoken language this tension is height-
ened because the task of speech recognition in-
troduces a new source of error. The robust sys-
tem will attempt to find a sensible interpretation,
even in the presence of performance errors by the
speaker, or recognition errors by the speech rec-
ognizer. On the other hand, a system should be
able to detect that a recognized string is not a sen-
tence of English, to help filter recognition errors by
the speech recognizer. Furthermore, if parsing and
recognition are interleaved, then the parser should
enforce constraints on partial utterances.
The approach taken in Gemini is to con-
strain language recognition with fairly conven-
tional grammar, but to augment that grammar
with two orthogonal rule-based recognition mod-
ules, one for glueing together the fragments found
during the conventional grammar parsing phase,
and another for recognizing and eliminating dis-
fluencies known as &amp;quot;repairs.&amp;quot; At the same time,
*This research was supported by the Advanced Re-
search Projects Agency under Contract ONR N00014-
90-C-0085 with the Office of Naval Research. The
views and conclusions contained in this document are
those of the authors and should not be interpreted as
necessarily representing the official policies, either ex-
pressed or implied, of the Advanced Research Projects
Agency of the U.S. Government.
the multiple analyses arising before and after all
this added robustness are managed in two ways:
first, by highly constraining the additional rule-
based modules by partitioning the rules into pref-
erence classes, and second, through the addition
of a postprocessing parse preference component.
Processing starts in Gemini when syntac-
tic, semantic, and lexical rules are applied by a
bottom-up all-paths constituent parser to populate
a chart with edges containing syntactic, seman-
tic, and logical form information. Then, a second
utterance parser is used to apply a second set of
syntactic and semantic rules that are required to
span the entire utterance. If no semantically ac-
ceptable utterance-spanning edges are found dur-
ing this phase, a component to recognize and cor-
rect certain grammatical disfluencies is applied.
When an acceptable interpretation is found, a set
of parse preferences is used to choose a single best
interpretation from the chart to be used for sub-
sequent processing. Quantifier scoping rules are
applied to this best interpretation to produce the
final logical form, which is then used as input to
a query-answering system. The following sections
describe each of these components in detail, with
the exception of the query-answering subsystem,
which is not described in this paper.
In our component-by-component view of
Gemini, we provide detailed statistics on each
component&apos;s size, speed, coverage, and accuracy.
These numbers detail our performance on the sub-
domain of air-travel planning that is currently be-
ing used by the ARPA spoken language under-
standing community (MADCOW, 1992). Gem-
ini was trained on a 5875-utterance dataset from
this domain, with another 688 utterances used as
a blind test (not explicitly trained on, but run
multiple times) to monitor our performance on a
dataset on which we did not train. We also report
here our results on another 756-utterance fair test
set that we ran only once. Table 1 contains a sum-
mary of the coverage of the various components on
both the training and fair test sets. More detailed
</bodyText>
<page confidence="0.997639">
54
</page>
<table confidence="0.979233625">
explanations of these numbers are given in the rel- and Lexical Functional Grammar (Bresnan, 1982).
evant sections.
- Training Test
Lexicon 99.1% 95.9%
Syntax 94.2% 90.9%
Semantics 87.4% 83.7%
Syntax (repair correction) 96.0% 93.1%
Semantics (repair correction) 89.1% 86.0%
</table>
<tableCaption confidence="0.999449">
Table 1: Domain Coverage by Component
</tableCaption>
<sectionHeader confidence="0.975557" genericHeader="categories and subject descriptors">
2. SYSTEM DESCRIPTION
</sectionHeader>
<bodyText confidence="0.999644333333333">
Gemini maintains a firm separation between
the language- and domain-specific portions of the
system, and the underlying infrastructure and ex-
ecution strategies. The Gemini kernel consists of
a set of compilers to interpret the high-level lan-
guages in which the lexicon and syntactic and se-
mantic grammar rules are written, as well as the
parser, semantic interpretation, quantifier scop-
ing, repair correction mechanisms, and all other
aspects of Gemini that are not specific to a lan-
guage or domain. Although this paper describes
the lexicon, grammar, and semantics of English,
Gemini has also been used in a Japanese spo-
ken language understanding system (Kameyama,
1992).
</bodyText>
<subsectionHeader confidence="0.994071">
2.1. Grammar Formalism
</subsectionHeader>
<bodyText confidence="0.986826225806452">
Gemini includes a midsized constituent gram-
mar of English (described in section 2.3), a small
utterance grammar for assembling constituents
into utterances (described in section 2.7), and a
lexicon. All three are written in a variant of the
unification formalism used in the Core Language
Engine (Alshawi, 1992) .
The basic building block of the grammar for-
malism is a category with feature constraints.
Here is an example:
up: [wh=ynq , case= (nomVacc ) ,
pers _num= ( 3rdAsg )
This category can be instantiated by any noun
phrase with the value ynq for its wh feature (which
means it must be a wh-bearing noun phrase like
which book, who, or whose mother), either acc (ac-
cusative) or nom (nominative) for its case feature,
and the conjunctive value 3rdAsg (third and sin-
gular) for its person-number feature. This for-
malism is related directly to the Core Language
Engine, but more conceptually it is closely re-
lated to that of other unification-based grammar
formalisms with a context-free skeleton, such as
PATR-II (Shieber et al., 1983), Categorial Uni-
fication Grammar (Uszkoreit, 1986), Generalized
Phrase-Structure Grammar (Gazdar et al., 1982),
Gemini differs from other unification for-
malisms in the following ways. Since many of
the most interesting issues regarding the formal-
ism concern typing, we defer discussing motivation
until section 2.5.
</bodyText>
<listItem confidence="0.946460095238095">
• Gemini uses typed unification. Each category
has a set of features declared for it. Each fea-
ture has a declared value space of possible values
(value spaces may be shared by different fea-
tures). Feature structures in Gemini can be re-
cursive, but only by having categories in their
value space; so typing is also recursive. Typed
feature structures are also used in HPSG (Pol-
lard and Sag, in press). One important differ-
ence with the use in Gemini is that Gemini has
no type inheritance.
• Some approaches do not assume a syntactic
skeleton of category-introducing rules (for ex-
ample, Functional Unification Grammar (Kay,
1979)). Some make such rules implicit (for
example, the various categorial unification ap-
proaches, such as Unification Categorial Gram-
mar (Zeevat, Klein, and Calder, 1987)).
• Even when a syntactic skeleton is assumed,
some approaches do not distinguish the category
of a constituent (for example, up, vp) from its
</listItem>
<bodyText confidence="0.980846666666667">
other features (for example, pers_num, gaps in,
gapsout). Thus, for example, in one version of
GPSG, categories were simply feature bundles
(attribute value structures) and there was a fea-
ture MAJ taking values like N,V,A, and P which
determined the major category of constituent.
</bodyText>
<listItem confidence="0.8276165">
• Gemini does not allow rules schematizing over
syntactic categories.
</listItem>
<subsectionHeader confidence="0.991377">
2.2. Lexicon
</subsectionHeader>
<bodyText confidence="0.997237076923077">
The Gemini lexicon uses the same category
notation as the Gemini syntactic rules. Lexical
categories are types as well, with sets of features
defined for them. The lexical component of Gem-
ini includes the lexicon of base forms, lexical tem-
plates, morphological rules, and the lexical type
and feature default specifications.
The Gemini lexicon used for the air-travel
planning domain contains 1,315 base entries.
These expand by morphological rules to 2,019. In
the 5875-utterance training set, 52 sentences con-
tained unknown words (0.9%), compared to 31
sentences in the 756-utterance fair test set (4.1%).
</bodyText>
<subsectionHeader confidence="0.997921">
2.3. Constituent Grammar
</subsectionHeader>
<bodyText confidence="0.995749">
A simplified example of a syntactic rule is
</bodyText>
<page confidence="0.925921">
55
</page>
<equation confidence="0.767407333333333">
syn(whq_ynq_slash_np,
[ s: [sentence_type=whq, form=tnsd,
gapsin=G, gapsout=GJ ,
</equation>
<bodyText confidence="0.922301833333333">
np: [wh=ynq, pers_num=N] ,
s: [sentence_type=ynq, form=tnsd,
gapsin=np: [pers_num=N] ,
gapsout=null]] ) .
This syntax rule (named whq_ynq_slash_np)
says that a sentence (category s) can be built by
finding a noun phrase (category up) followed by a
sentence. It requires that the daughter up have the
value ynq for its wh feature and that it have the
value N (a variable) for its person-number feature.
It requires that the daughter sentence have a cat-
egory value for its gaps in feature, namely an up
with a person number value N, which is the same as
the person number value on the wh-bearing noun
phrase. The interpretation of the entire rule is
that a gapless sentence with sentence_type whq
can be built by finding a wh-phrase followed by a
sentence with a noun phrase gap in it that has the
same person number as the wh-phrase.
Semantic rules are written in much the same
rule format, except that in a semantic rule, each of
the constituents mentioned in the phrase structure
skeleton is associated with a logical form. Thus,
the semantics for the rule above is
</bodyText>
<equation confidence="0.77748475">
sem(whq_ynq_slash_np,
[( [whq, S] , s: 0 ) ,
(Np, np: 0 ) ,
(S, s: [gapsin=np: [gapsem=Np] ] )]) .
</equation>
<bodyText confidence="0.999994368421052">
Here the semantics of the mother s is just the
semantics of the daughter s with the illocution-
ary force marker whq wrapped around it. In addi-
tion, the semantics of the s gap&apos;s np&apos;s gapsem has
been unified with the semantics of the wh-phrase.
Through a succession of unifications this will end
up assigning the wh-phrase&apos;s semantics to the gap
position in the argument structure of the s. Al-
though each semantic rule must be keyed to a pre-
existing syntactic rule, there is no assumption of
rule-to-rule uniqueness. Any number of semantic
rules may be written for a single syntactic rule.
We discuss some further details of the semantics
in section 2.6
The constituent grammar used in Gemini con-
tains 243 syntactic rules, and 315 semantic rules.
Syntactic coverage on the 5875-utterance training
set was 94.2%, and on the 756-utterance test set
it was 90.9%.
</bodyText>
<subsectionHeader confidence="0.893353">
2.4. Parser
</subsectionHeader>
<bodyText confidence="0.985020333333333">
Since Gemini was designed with spoken lan-
guage interpretation in mind, key aspects of the
Gemini parser are motivated by the increased
needs for robustness and efficiency that charac-
terize spoken language. Gemini uses essentially
a pure bottom-up chart parser, with some limited
left-context constraints applied to control creation
of categories containing syntactic gaps.
Some key properties of the parser are
</bodyText>
<listItem confidence="0.939045">
• The parser is all-paths bottom-up, so that all
possible edges admissible by the grammar are
found.
• The parser uses subsumption checking to reduce
the size of the chart. Essentially, an edge is not
added to the chart if it is less general than a
preexisting edge, and preexisting edges are re-
moved from the chart if the new edge is more
general.
• The parser is on-line (Graham, Harrison, and
Russo, 1980), essentially meaning that all edges
that end at position i are constructed before
any that end at position i -I- 1. This feature is
particularly desirable if the final architecture of
the speech understanding system couples Gem-
ini tightly with the speech recognizer, since it
guarantees for any partial recognition input that
all possible constituents will be built.
</listItem>
<bodyText confidence="0.999404642857143">
An important feature of the parser is the
mechanism used to constrain the construction of
categories containing syntactic gaps. In earlier
work (Moore and Dowding, 1991), we showed that
approximately 80% of the edges built in an all-
paths bottom-up parser contained gaps, and that
it is possible to use prediction in a bottom-up
parser only to constrain the gap categories, with-
out requiring prediction for nongapped categories.
This limited form of left-context constraint greatly
reduces the total number of edges built for a very
low overhead. In the 5875-utterance training set,
the chart for the average sentence contained 313
edges, but only 23 predictions.
</bodyText>
<subsectionHeader confidence="0.953045">
2.5. Typing
</subsectionHeader>
<bodyText confidence="0.999862722222222">
The main advantage of typed unification is for
grammar development. The type information on
features allows the lexicon, grammar, and seman-
tics compilers to provide detailed error analysis re-
garding the flow of values through the grammar,
and to warn if features are assigned improper val-
ues, or variables of incompatible types are unified.
Since the type-analysis is performed statically at
compile time, there is no run-time overhead asso-
ciated with adding types to the grammar.
The major grammatical category plays a spe-
cial role in the typing scheme of Gemini. For each
category, Gemini makes a set of declarations stipu-
lating its allowable features and the relevant value
spaces. Thus, the distinction between the syntac-
tic category of a constituent and its other features
can be cashed out as follows: the syntactic cat-
egory can be thought of as the feature structure
</bodyText>
<page confidence="0.984973">
56
</page>
<bodyText confidence="0.999941714285714">
type. The only other types needed by Gemini are
the value spaces used by features. Thus for ex-
ample, the type v (verb) admits a feature vform,
whose value space vform-types can be instanti-
ated with values like present participle, finite, and
past participle. Since all recursive features are
category-valued, these two kinds of types suffice.
</bodyText>
<subsectionHeader confidence="0.9708935">
2.6. Interleaving Syntactic and Se-
mantic Information
</subsectionHeader>
<bodyText confidence="0.986870416666667">
Sortal Constraints Selectional restrictions
are imposed in Gemini through the sorts mecha-
nism. Selectional restrictions include both highly
domain-specific information about predicate-
argument and very general predicate restrictions.
For example, in our application the object of
the transitive verb depart (as in flights departing
Boston) is restricted to be an airport or a city,
obviously a domain-specific requirement. But the
same machinery also restricts a determiner like all
to take two propositions, and an adjective like fur-
ther to take distances as its measure-specifier (as
in thirty miles further). In fact, sortal constraints
are assigned to every atomic predicate and opera-
tor appearing in the logical forms constructed by
the semantic rules.
Sorts are located in a conceptual hierarchy
and are implemented as Prolog terms such that
more general sorts subsume more specific sorts
(Mellish, 1988). This allows the subsumption
checking and packing in the parser to share struc-
ture whenever possible. Semantic coverage with
sortal constraints applied was 87.4% on the train-
ing set, and on the test set it was 83.7%.
</bodyText>
<subsectionHeader confidence="0.507731">
Interleaving Semantics with Parsing In
</subsectionHeader>
<bodyText confidence="0.963736181818182">
Gemini, syntactic and semantic processing is fully
interleaved. Building an edge requires that syntac-
tic constraints be applied, which results in a tree
structure, to which semantic rules can be applied,
which results in a logical form to which sortal con-
traints can be applied. Only if the syntactic edge
leads to a well-sorted semantically-acceptable log-
ical form fragment is it added to the chart.
Interleaving the syntax and semantics in this
way depends on a crucial property of the seman-
tics: a semantic interpretation is available for each
syntactic node. This is guaranteed by the seman-
tic rule formalism and by the fact that every lexical
item has a semantics associated with it.
Table 2 contains average edge counts and
parse timing statisticsi for the 5875-utterance
training set.
&apos;Gemini is implemented primarily in Quintus Pro-
log version 3.1.1. All timing numbers given in this
paper were run on a lightly loaded Sun SPARCsta-
tion 2 with at least 48 MB of memory. Under normal
conditions, Gemini runs in under 12 MB of memory.
</bodyText>
<table confidence="0.89813825">
Edges Time
Syntax only 197 3.4 sec.
Syntax + semantics 234 4.47 sec.
Syntax + semantics + sorts 313 13.5 sec.
</table>
<tableCaption confidence="0.9756535">
Table 2: Average Number of Edges Built by In-
terleaved Processing
</tableCaption>
<subsectionHeader confidence="0.986926">
2.7. Utterance Parsing
</subsectionHeader>
<bodyText confidence="0.999949659574468">
The constituent parser uses the constituent
grammar to build all possible categories bottom-
up, independent of location within the string.
Thus, the constituent parser does not force any
constituent to occur either at the beginning of
the utterance, or at the end. Those constraints
are stated in what we call the utterance grammar.
They are applied after constituent parsing is com-
plete by the utterance parser. The utterance gram-
mar specifies ways of combining the categories
found by the constituent parser into an analysis
of the complete utterance. It is at this point that
the system recognizes whether the sentence was
a simple complete sentence, an isolated sentence
fragment, a run-on sentence, or a sequence of re-
lated fragments.
Many systems (Carbonell and Hayes, 1983),
(Hobbs et al., 1992), (Seneff, 1992), (Stallard and
Bobrow, 1992) have added robustness with a sim-
ilar postprocessing phase. The approach taken
in Gemini differs in that the utterance grammar
uses the same syntactic and semantic rule for-
malism used by the constituent grammar. Thus,
the same kinds of logical forms built during con-
stituent parsing are the output of utterance pars-
ing, with the same sortal constraints enforced. For
example, an utterance consisting of a sequence
of modifier, fragments (like on Tuesday at three
o&apos;clock on United) is interpreted as a conjoined
property of a flight, because the only sort of thing
in the ATIS domain that can be on Tuesday at
three o&apos;clock on United is a flight.
The utterance parser partitions the utterance
grammar into equivalence classes and considers
each class according to an ordering. Utterance
parsing terminates when all constituents satisfy-
ing the rules of the current equivalence class are
built, unless there are none, in which case the next
class is considered. The highest ranked class con-
sists of rules to identify simple complete sentences,
the next highest class consists of rules to iden-
tify simple isolated sentence fragments, and so on.
Thus, the utterance parser allows us to enforce a
very coarse form of parse preferences (for exam-
ple, prefering complete sentences to sentence frag-
ments). These coarse preferences could also be
enforced by the parse preference component de-
</bodyText>
<page confidence="0.996669">
57
</page>
<bodyText confidence="0.9996024">
scribed in section 2.9, but for the sake of efficiency
we choose to enforce them here.
The utterance grammar is significantly
smaller than the constituent grammar — only 37
syntactic rules and 43 semantic rules.
</bodyText>
<subsectionHeader confidence="0.956232">
2.8. Repairs
</subsectionHeader>
<bodyText confidence="0.996998304347826">
Grammatical disfluencies occur frequently in
spontaneous spoken language. We have imple-
mented a component to detect and correct a large
subclass of these disfluencies (called repairs, or
self-corrections) where the speaker intends that
the meaning of the utterance be gotten by deleting
one or more words. Often, the speaker gives clues
of their intention by repeating words or adding cue
words that signal the repair:
(1) a. How many American airline flights leave
Denver on June June tenth.
b. Can you give me information on all the
flights from San Francisco no from Pitts-
burgh to San Francisco on Monday.
The mechanism used in Gemini to detect and
correct repairs is currently applied as a fallback if
no semantically acceptable interpretation is found
for the complete utterance. The mechanism finds
sequences of identical or related words, possibly
separated by a cue word (for example, oh or no)
that might indicate the presence of a repair, and
deletes the first occurrence of the matching por-
tion. Since there may be several such sequences of
possible repairs in the utterance, the mechanism
produces a ranked set of candidate corrected ut-
terances. These candidates are ranked in order
of the fewest deleted words. The first candidate
that can be given an interpretation is accepted as
the intended meaning of the utterance. This ap-
proach is presented in detail in (Bear, Dowding,
and Shriberg, 1992).
The repair correction mechanism helps in-
crease the syntactic and semantic coverage of
Gemini (as reported in Table 1). In the 5875-
utterance training set, 178 sentences contained
nontrivial repairs2, of which Gemini found 89
(50%). Of the sentences Gemini corrected, 81 were
analyzed correctly (91%), and 8 contained repairs
but were corrected wrongly. Similarly, the 756-
utterance test set contained 26 repairs, of which
Gemini found 11(42%). Of those 11, 8 were ana-
lyzed correctly (77%), and 3 were analyzed incor-
rectly.
Since Gemini&apos;s approach is to extend lan-
guage analysis to recognize specific patterns char-
acteristic of spoken language, it is important for
</bodyText>
<footnote confidence="0.717332">
2For these results, we ignored repairs consisting of
only an isolate fragment word, or sentence-initial filler
words like &amp;quot;yes&amp;quot; and &amp;quot;okay&amp;quot;.
</footnote>
<bodyText confidence="0.999975111111111">
components like repair correction (which provide
the powerful capability of deleting words) not to
be applied in circumstances where no repair is
present. In the 5875-utterance training set, Gem-
ini misidentified only 15 sentences (0.25%) as con-
taining repairs when they did not. In the 756-
utterance test set, only 2 sentences were misiden-
tified as containing repairs (0.26%).
While the repair correction component cur-
rently used in Gemini does not make use of acous-
tic/prosodic information, it is clear that acoustics
can contribute meaningful cues to repair. In fu-
ture work, we hope to improve the performance of
our repair correction component by incorporating
acoustic/prosodic techniques for repair detection
(Bear, Dowding, and Shriberg, 1992) (Nakatani
and Hirschberg, 1993) (O&apos;Shaughnessy, 1992).
A central question about the repairs module
concerns its role in a tightly integrated system in
which the NL component filters speech recognition
hypotheses. The open question: should the repairs
module be part of the recognizer filter or should
it continue to be a post-processing component?
The argument for including it in the filter is that
without a repairs module, the NL system rejects
many sentences with repairs, and will thus dispre-
fer essentially correct recognizer hypotheses. The
argument against including it is efficiency and the
concern that with recognizer errors present, the
repair module&apos;s precision may suffer: it may at-
tempt to repair sentences with no repair in them.
Our current best guess is that recognizer errors
are essentially orthogonal to repairs and that a
filter including the repairs module will not suffer
from precision problems. But we have not yet per-
formed the experiments to decide this.
</bodyText>
<subsectionHeader confidence="0.997443">
2.9. Parse Preference Mechanism
</subsectionHeader>
<bodyText confidence="0.9999143">
In Gemini, parse preferences are enforced
when extracting syntactically and semantically
well-formed parse trees from the chart. In this
respect, our approach differs from many other
approaches to the problem of parse preferences,
which make their preference decisions as pars-
ing progresses, pruning subsequent parsing paths
(Frazier and Fodor, 1978), (Hobbs and Bear,
1990), (Marcus 1980). Applying parse prefer-
ences requires comparing two subtrees spanning
the same portion of the utterance.
The parse preference mechanism begins with
a simple strategy to disprefer parse trees contain-
ing specific &amp;quot;marked&amp;quot; syntax rules. As an example
of a dispreferred rule, consider: Book those three
flights to Boston. This sentence has a parse on
which those three is a noun phrase with a miss-
ing head (consider a continuation of the discourse
Three of our clients have sufficient credit). After
penalizing such dispreferred parses, the preference
</bodyText>
<page confidence="0.995681">
58
</page>
<bodyText confidence="0.9981925">
mechanism applies attachment heuristics based on
the work by Pereira (1985) and Shieber (1983)
Pereira&apos;s paper shows how the heuristics of
Minimal Attachment and Right Association (Kim-
ball, 1973) can both be implemented using a
bottom-up shift-reduce parser.
</bodyText>
<listItem confidence="0.626806666666667">
(2)(a) John sang a song for Mary.
(b) John canceled the room Mary reserved yes-
terday.
</listItem>
<bodyText confidence="0.9994518">
Minimal Attachment selects for the tree with the
fewest nodes, so in (2a), the parse that makes for
Mary a complement of sings is preferred. Right
Association selects for the tree that incorporates
a constituent A into the rightmost possible con-
stituent (where rightmost here means beginning
the furthest to the right). Thus, in (2b) the parse
in which yesterday modifies reserved is preferred.
The problem with these heuristics is that
when they are formulated loosely, as in the pre-
vious paragraph, they appear to conflict. In par-
ticular, in (2a), Right Association seems to call for
the parse that makes for Mary a modifier of song.
Pereira&apos;s goal is to show how a shift-reduce
parser can enforce both heuristics without conflict
and enforce the desired preferences for examples
like (2a) and (2b). He argues that Minimal At-
tachment and Right Association can be enforced in
the desired way by adopting the following heuris-
tics for resolving conflicts:
</bodyText>
<listItem confidence="0.939886">
1. Right Association: In a shift-reduce conflict,
prefer shifts to reduces.
2. Minimal Attachment: In a reduce-reduce con-
flict, prefer longer reduces to shorter reduces.
</listItem>
<bodyText confidence="0.999893125">
Since these two principles never apply to the same
choice, they never conflict.
For purposes of invoking Pereira&apos;s heuristics,
the derivation of a parse can be represented as the
sequence of S&apos;s (Shift) and R&apos;s (Reduce) needed to
construct the parse&apos;s unlabeled bracketing. Con-
sider, for example, the choice between two unla-
beled bracketings of (2a):
</bodyText>
<listItem confidence="0.86996475">
(a) [John [sang [a song] [for Mary ] ] ]
SS RS S RRR
(b) [John [sang [ [a song] [for Mary ] ] ]
SS RSS RRRR
</listItem>
<bodyText confidence="0.999972833333333">
There is a shift for each word and a reduce for
each right bracket. Comparison of the two parses
consists simply of pairing the moves in the shift-
reduce derivation from left to right. Any parse
making a shift move that corresponds to a reduce
move loses by Right Association. Any parse mak-
ing a reduce move that corresponds to a longer
reduce loses by Minimal Attachment. In deriva-
tion (b) above, the third reduce move builds the
constituent a song for Mary from two constituents,
while the corresponding reduce in (a) builds sang
a song for Mary from three constituents. Parse
(b) thus loses by Minimal Attachment.
Questions about the exact nature of parse
preferences (and thus about the empirical ade-
quacy of Pereira&apos;s proposal) still remain open, but
the mechanism sketched does provide plausible re-
sults for a number of examples.
</bodyText>
<subsectionHeader confidence="0.668529">
2.10. Scoping
</subsectionHeader>
<bodyText confidence="0.999958076923077">
The final logical form produced by Gemini
is the result of applying a set of quantifier scop-
ing rules to the best interpretation chosen by the
parse preference mechanism. The semantic rules
build quasi-logical forms, which contain complete
semantic predicate-argument structure, but do not
specify quantifier scoping. The scoping algorithm
that we use combines syntactic and semantic in-
formation with a set of quantifier scoping prefer-
ence rules to rank the possible scoped logical forms
consistent with the quasi-logical form selected by
parse preferences. This algorithm is described in
detail in (Moran, 1988).
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="conclusions">
3. CONCLUSION
</sectionHeader>
<bodyText confidence="0.999967421052631">
In our approach to resolving the tension be-
tween overgeneration and robustness in a spoken
language understanding system, some aspects of
Gemini are specifically oriented towards limiting
overgeneration, such as the on-line property for
the parser, and fully interleaved syntactic and se-
mantic processing. Other components, such as the
fragment and run-on processing provided by the
utterance grammar, and the correction of recog-
nizable grammatical repairs, increase the robust-
ness of Gemini. We believe a robust system can
still recognize and disprefer utterances containing
recognition errors.
Research in the construction of the Gemini
system is ongoing to improve Gemini&apos;s speed and
coverage, as well as to examine deeper integration
strategies with speech recognition, and integration
of prosodic information into spoken language dis-
ambiguation.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.983059111111111">
Alshawi, II. (ed) (1992). The Core Language En-
gine, MIT Press, Cambridge.
Bear, J., Dowding, J., and Shriberg, E. (1992).
&amp;quot;Integrating Multiple Knowledge Sources for
the Detection and Correction of Repairs in
Human-Computer Dialog&amp;quot;, in Proceedings of
the 30th Annual Meeting of the Association
for Computational Linguists, Newark, DE, pp.
56-63.
</reference>
<page confidence="0.993515">
59
</page>
<reference confidence="0.999212869158879">
Bresnan, J. (ed) (1982). The Mental Represen-
tation of Grammatical Relations, MIT Press,
Cambridge.
Carbonell, J., and Hayes, P. (1983). &amp;quot;Recovery
Strategies for Parsing Extragrammatical Lan-
guage&amp;quot;, American Journal of Computational
Linguistics, Vol. 9, Numbers 3-4, pp. 123-146.
Frazier, L., and Fodor, J.D. (1978). &amp;quot;The Sausage
Machine: A New Two-Stage Parsing Model&amp;quot;,
Cognition, Vol. 6, pp. 291-325.
Gazdar, G., Klein, E., Pullum, G., and Sag, I.
(1982). Generalized Phrase Structure Gram-
mar, Harvard University Press, Cambridge.
Graham, S., IIarrison, M., and Ruzzo, W.
(1980). &amp;quot;An Improved Context-Free Recog-
nize?, ACM Transactions on Programming
Languages and Systems, Vol. 2, No. 3, pp. 415-
462.
Hobbs, J., and Bear, J. (1990). &amp;quot;Two Principles
of Parse Preference&amp;quot;, in Proceedings of the
13th International Conference on Computa-
tional Linguistics, Helsinki, Vol. 3, pp. 162-
167.
Hobbs, J., Appelt, D., Bear, J., Tyson, M., and
Magerman, D. (1992). &amp;quot;Robust Processing
of Real-World Natural-Language Texts&amp;quot;, in
Text Based Intelligent Systems, ed. P. Jacobs,
Lawrence Erlbaum Associates, Hillsdale, NJ,
pp. 13-33.
Kameyama, M. (1992). &amp;quot;The Syntax and Seman-
tics of the Japanese Language Engine&amp;quot;, forth-
coming. In Mazuka, R., and N. Nagai, Eds.
Japanese Syntactic Processing, Hillsdale, NJ:
Lawrence Erlbaum Associates.
Kay, M. (1979). &amp;quot;Functional Grammar&amp;quot;, in Pro-
ceedings of the 5th Annual Meeting of the
Berkeley Linguistics Society. pp. 142-158.
Kimball, J. (1973). &amp;quot;Seven Principles of Surface
Structure Parsing in Natural Language&amp;quot;, Cog-
nition, Vol. 2, No. 1, pp. 15-47.
MADCOW (1992). &amp;quot;Multi-site Data Collection for
a Spoken Language Corpus&amp;quot;, in Proceedings
of the DARPA Speech and Natural Language
Workshop, February 23-26, 1992.
Marcus, M. (1980). A Theory of Syntactic Recog-
nition for Natural Language, MIT Press,
Cambridge.
Moran, D. (1988). &amp;quot;Quantifier Scoping in the SRI
Core Language Engine&amp;quot;, in Proceedings of the
26th Annual Meeting of the Association for
Computational Linguistics, State University
of New York at Buffalo, Buffalo, NY, pp. 33-
40.
Mellish, C. (1988). &amp;quot;Implementing Systemic Clas-
sification by Unification&amp;quot;. Computational Lin-
guistics Vol. 14, pp. 40-51.
Moore, R., and Dowding, J. (1991). &amp;quot;Efficient
Bottom-up Parsing&amp;quot;, in Proceedings of the
DARPA Speech and Natural Language Work-
shop, February 19-22, 1991, pp. 200-203.
Nakatani, C., and Hirschberg, J. (1993). &amp;quot;A
Speech-First Model for Repair Detection and
Correction&amp;quot;, in Proceedings of the ARPA
Workshop on Human Language Technology,
March 21-24, 1993, Plainsboro, NJ.
O&apos;Shaughnessy, D. (1992). &amp;quot;Analysis of False
Starts in Spontaneous Speech&amp;quot;, in Proceed-
ings of the 1992 International Conference on
Spoken Language Processing, October 12-16,
1992, Banff, Alberta, Canada, pp. 931-934.
Pereira, F. (1985). &amp;quot;A New Characterization of At-
tachment Preferences&amp;quot;, in Natural Language
Parsing, Ed. by Dowty, D., Karttunen, L.,
and Zwicky, A., Cambridge University Press,
Cambridge, pp. 307-319.
Pollard, C., and Sag, I. (in press). Information-
Based Syntax and Semantics, Vol. 2, CSLI
Lecture Notes.
Seneff, S. (1992). &amp;quot;A Relaxation Method for Un-
derstanding Spontaneous Speech Utterances&amp;quot;,
in Proceedings of the Speech and Natural Lan-
guage Workshop, Harriman, NY, pp. 299-304.
Shieber, S. (1983). &amp;quot;Sentence Disambiguation by a
Shift-Reduce Parsing Technique&amp;quot;, in Proceed-
ings of the 21 Annual Meeting of the Associ-
ation for Computational Linguistics, Boston,
Massachusetts, pp. 113-118.
Shieber, S., Uszkoreit, H., Pereira, F., Robinson,
J., and Tyson, M. (1983). &amp;quot;The Formalism
and Implementation of PATR-II&amp;quot;, in Grosz,
B. and Stickel, M. (eds) Research on Interac-
tive Acquisition and Use of Knowledge, SRI
International, pp. 39-79.
Stallard, D., and Bobrow, R. (1992). &amp;quot;Fragment
Processing in the DELPHI System&amp;quot;, in Pro-
ceedings of the Speech and Natural Language
Workshop, Harriman, NY, pp. 305-310.
Uszkoreit, II. (1986). &amp;quot;Categorial Unification
Grammars&amp;quot;, in Proceedings of the 11th Inter-
national Conference on Computational Lin-
guistics and the 24th Annual Meeting of the
Association for Computational Linguistics,
Institut fur Kummunikkationsforschung und
Phonetik, Bonn University.
Zeevat, H., Klein, E., and Calder, J. (1987). &amp;quot;An
Introduction to Unification Categorial Gram-
mar&amp;quot;, in Haddock, N., Klein, E., Merrill, G.
</reference>
<page confidence="0.966973">
60
</page>
<reference confidence="0.991878666666667">
(eds.) Edinburgh Working Papers in Cogni-
tive Science, Volume 1: Categorial Grammar,
Unification Grammar, and Parsing.
</reference>
<page confidence="0.999265">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.507525">
<title confidence="0.9667335">GEMINI: A NATURAL LANGUAGE SYSTEM FOR SPOKEN-LANGUAGE UNDERSTANDING*</title>
<author confidence="0.9889395">John Dowding</author>
<author confidence="0.9889395">Jean Mark Gawron</author>
<author confidence="0.9889395">Doug Appelt</author>
<author confidence="0.9889395">John Bear</author>
<author confidence="0.9889395">Lynn Cherny</author>
<author confidence="0.9889395">Robert Moore</author>
<author confidence="0.9889395">Douglas Moran</author>
<affiliation confidence="0.990503">International</affiliation>
<address confidence="0.998359">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<email confidence="0.56214">Internet:dowding@ai.sri.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine,</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>1992</marker>
<rawString>Alshawi, II. (ed) (1992). The Core Language Engine, MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>J Dowding</author>
<author>E Shriberg</author>
</authors>
<title>Integrating Multiple Knowledge Sources for the Detection and Correction of Repairs in Human-Computer Dialog&amp;quot;,</title>
<date>1992</date>
<booktitle>in Proceedings of the 30th Annual Meeting of the Association for Computational Linguists,</booktitle>
<pages>56--63</pages>
<location>Newark, DE,</location>
<contexts>
<context position="20219" citStr="Bear, Dowding, and Shriberg, 1992" startWordPosition="3256" endWordPosition="3260">tterance. The mechanism finds sequences of identical or related words, possibly separated by a cue word (for example, oh or no) that might indicate the presence of a repair, and deletes the first occurrence of the matching portion. Since there may be several such sequences of possible repairs in the utterance, the mechanism produces a ranked set of candidate corrected utterances. These candidates are ranked in order of the fewest deleted words. The first candidate that can be given an interpretation is accepted as the intended meaning of the utterance. This approach is presented in detail in (Bear, Dowding, and Shriberg, 1992). The repair correction mechanism helps increase the syntactic and semantic coverage of Gemini (as reported in Table 1). In the 5875- utterance training set, 178 sentences contained nontrivial repairs2, of which Gemini found 89 (50%). Of the sentences Gemini corrected, 81 were analyzed correctly (91%), and 8 contained repairs but were corrected wrongly. Similarly, the 756- utterance test set contained 26 repairs, of which Gemini found 11(42%). Of those 11, 8 were analyzed correctly (77%), and 3 were analyzed incorrectly. Since Gemini&apos;s approach is to extend language analysis to recognize spec</context>
<context position="21777" citStr="Bear, Dowding, and Shriberg, 1992" startWordPosition="3497" endWordPosition="3501">cumstances where no repair is present. In the 5875-utterance training set, Gemini misidentified only 15 sentences (0.25%) as containing repairs when they did not. In the 756- utterance test set, only 2 sentences were misidentified as containing repairs (0.26%). While the repair correction component currently used in Gemini does not make use of acoustic/prosodic information, it is clear that acoustics can contribute meaningful cues to repair. In future work, we hope to improve the performance of our repair correction component by incorporating acoustic/prosodic techniques for repair detection (Bear, Dowding, and Shriberg, 1992) (Nakatani and Hirschberg, 1993) (O&apos;Shaughnessy, 1992). A central question about the repairs module concerns its role in a tightly integrated system in which the NL component filters speech recognition hypotheses. The open question: should the repairs module be part of the recognizer filter or should it continue to be a post-processing component? The argument for including it in the filter is that without a repairs module, the NL system rejects many sentences with repairs, and will thus disprefer essentially correct recognizer hypotheses. The argument against including it is efficiency and th</context>
</contexts>
<marker>Bear, Dowding, Shriberg, 1992</marker>
<rawString>Bear, J., Dowding, J., and Shriberg, E. (1992). &amp;quot;Integrating Multiple Knowledge Sources for the Detection and Correction of Repairs in Human-Computer Dialog&amp;quot;, in Proceedings of the 30th Annual Meeting of the Association for Computational Linguists, Newark, DE, pp. 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>The Mental Representation of Grammatical Relations,</title>
<date>1982</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4432" citStr="Bresnan, 1982" startWordPosition="697" endWordPosition="698"> ARPA spoken language understanding community (MADCOW, 1992). Gemini was trained on a 5875-utterance dataset from this domain, with another 688 utterances used as a blind test (not explicitly trained on, but run multiple times) to monitor our performance on a dataset on which we did not train. We also report here our results on another 756-utterance fair test set that we ran only once. Table 1 contains a summary of the coverage of the various components on both the training and fair test sets. More detailed 54 explanations of these numbers are given in the rel- and Lexical Functional Grammar (Bresnan, 1982). evant sections. - Training Test Lexicon 99.1% 95.9% Syntax 94.2% 90.9% Semantics 87.4% 83.7% Syntax (repair correction) 96.0% 93.1% Semantics (repair correction) 89.1% 86.0% Table 1: Domain Coverage by Component 2. SYSTEM DESCRIPTION Gemini maintains a firm separation between the language- and domain-specific portions of the system, and the underlying infrastructure and execution strategies. The Gemini kernel consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written, as well as the parser, semantic interpret</context>
</contexts>
<marker>Bresnan, 1982</marker>
<rawString>Bresnan, J. (ed) (1982). The Mental Representation of Grammatical Relations, MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>P Hayes</author>
</authors>
<title>Recovery Strategies for Parsing Extragrammatical Language&amp;quot;,</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>9</volume>
<pages>3--4</pages>
<contexts>
<context position="17135" citStr="Carbonell and Hayes, 1983" startWordPosition="2753" endWordPosition="2756">ituent parser does not force any constituent to occur either at the beginning of the utterance, or at the end. Those constraints are stated in what we call the utterance grammar. They are applied after constituent parsing is complete by the utterance parser. The utterance grammar specifies ways of combining the categories found by the constituent parser into an analysis of the complete utterance. It is at this point that the system recognizes whether the sentence was a simple complete sentence, an isolated sentence fragment, a run-on sentence, or a sequence of related fragments. Many systems (Carbonell and Hayes, 1983), (Hobbs et al., 1992), (Seneff, 1992), (Stallard and Bobrow, 1992) have added robustness with a similar postprocessing phase. The approach taken in Gemini differs in that the utterance grammar uses the same syntactic and semantic rule formalism used by the constituent grammar. Thus, the same kinds of logical forms built during constituent parsing are the output of utterance parsing, with the same sortal constraints enforced. For example, an utterance consisting of a sequence of modifier, fragments (like on Tuesday at three o&apos;clock on United) is interpreted as a conjoined property of a flight,</context>
</contexts>
<marker>Carbonell, Hayes, 1983</marker>
<rawString>Carbonell, J., and Hayes, P. (1983). &amp;quot;Recovery Strategies for Parsing Extragrammatical Language&amp;quot;, American Journal of Computational Linguistics, Vol. 9, Numbers 3-4, pp. 123-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>J D Fodor</author>
</authors>
<title>The Sausage Machine: A New Two-Stage Parsing Model&amp;quot;,</title>
<date>1978</date>
<journal>Cognition,</journal>
<volume>6</volume>
<pages>291--325</pages>
<contexts>
<context position="23144" citStr="Frazier and Fodor, 1978" startWordPosition="3707" endWordPosition="3710">hem. Our current best guess is that recognizer errors are essentially orthogonal to repairs and that a filter including the repairs module will not suffer from precision problems. But we have not yet performed the experiments to decide this. 2.9. Parse Preference Mechanism In Gemini, parse preferences are enforced when extracting syntactically and semantically well-formed parse trees from the chart. In this respect, our approach differs from many other approaches to the problem of parse preferences, which make their preference decisions as parsing progresses, pruning subsequent parsing paths (Frazier and Fodor, 1978), (Hobbs and Bear, 1990), (Marcus 1980). Applying parse preferences requires comparing two subtrees spanning the same portion of the utterance. The parse preference mechanism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism </context>
</contexts>
<marker>Frazier, Fodor, 1978</marker>
<rawString>Frazier, L., and Fodor, J.D. (1978). &amp;quot;The Sausage Machine: A New Two-Stage Parsing Model&amp;quot;, Cognition, Vol. 6, pp. 291-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar,</title>
<date>1982</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="6514" citStr="Gazdar et al., 1982" startWordPosition="1023" endWordPosition="1026">e with the value ynq for its wh feature (which means it must be a wh-bearing noun phrase like which book, who, or whose mother), either acc (accusative) or nom (nominative) for its case feature, and the conjunctive value 3rdAsg (third and singular) for its person-number feature. This formalism is related directly to the Core Language Engine, but more conceptually it is closely related to that of other unification-based grammar formalisms with a context-free skeleton, such as PATR-II (Shieber et al., 1983), Categorial Unification Grammar (Uszkoreit, 1986), Generalized Phrase-Structure Grammar (Gazdar et al., 1982), Gemini differs from other unification formalisms in the following ways. Since many of the most interesting issues regarding the formalism concern typing, we defer discussing motivation until section 2.5. • Gemini uses typed unification. Each category has a set of features declared for it. Each feature has a declared value space of possible values (value spaces may be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their value space; so typing is also recursive. Typed feature structures are also used in HPSG (Pollard and Sag, in p</context>
</contexts>
<marker>Gazdar, Klein, Pullum, Sag, 1982</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1982). Generalized Phrase Structure Grammar, Harvard University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Graham</author>
<author>M IIarrison</author>
<author>W Ruzzo</author>
</authors>
<title>An Improved Context-Free Recognize?,</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<volume>2</volume>
<pages>415--462</pages>
<marker>Graham, IIarrison, Ruzzo, 1980</marker>
<rawString>Graham, S., IIarrison, M., and Ruzzo, W. (1980). &amp;quot;An Improved Context-Free Recognize?, ACM Transactions on Programming Languages and Systems, Vol. 2, No. 3, pp. 415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>J Bear</author>
</authors>
<title>Two Principles of Parse Preference&amp;quot;,</title>
<date>1990</date>
<booktitle>in Proceedings of the 13th International Conference on Computational Linguistics, Helsinki,</booktitle>
<volume>3</volume>
<pages>162--167</pages>
<contexts>
<context position="23168" citStr="Hobbs and Bear, 1990" startWordPosition="3711" endWordPosition="3714"> is that recognizer errors are essentially orthogonal to repairs and that a filter including the repairs module will not suffer from precision problems. But we have not yet performed the experiments to decide this. 2.9. Parse Preference Mechanism In Gemini, parse preferences are enforced when extracting syntactically and semantically well-formed parse trees from the chart. In this respect, our approach differs from many other approaches to the problem of parse preferences, which make their preference decisions as parsing progresses, pruning subsequent parsing paths (Frazier and Fodor, 1978), (Hobbs and Bear, 1990), (Marcus 1980). Applying parse preferences requires comparing two subtrees spanning the same portion of the utterance. The parse preference mechanism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism applies attachment heuri</context>
</contexts>
<marker>Hobbs, Bear, 1990</marker>
<rawString>Hobbs, J., and Bear, J. (1990). &amp;quot;Two Principles of Parse Preference&amp;quot;, in Proceedings of the 13th International Conference on Computational Linguistics, Helsinki, Vol. 3, pp. 162-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
<author>D Appelt</author>
<author>J Bear</author>
<author>M Tyson</author>
<author>D Magerman</author>
</authors>
<title>Robust Processing of Real-World Natural-Language Texts&amp;quot;,</title>
<date>1992</date>
<booktitle>in Text Based Intelligent Systems,</booktitle>
<pages>13--33</pages>
<editor>ed. P. Jacobs, Lawrence Erlbaum Associates,</editor>
<location>Hillsdale, NJ,</location>
<contexts>
<context position="17157" citStr="Hobbs et al., 1992" startWordPosition="2757" endWordPosition="2760">any constituent to occur either at the beginning of the utterance, or at the end. Those constraints are stated in what we call the utterance grammar. They are applied after constituent parsing is complete by the utterance parser. The utterance grammar specifies ways of combining the categories found by the constituent parser into an analysis of the complete utterance. It is at this point that the system recognizes whether the sentence was a simple complete sentence, an isolated sentence fragment, a run-on sentence, or a sequence of related fragments. Many systems (Carbonell and Hayes, 1983), (Hobbs et al., 1992), (Seneff, 1992), (Stallard and Bobrow, 1992) have added robustness with a similar postprocessing phase. The approach taken in Gemini differs in that the utterance grammar uses the same syntactic and semantic rule formalism used by the constituent grammar. Thus, the same kinds of logical forms built during constituent parsing are the output of utterance parsing, with the same sortal constraints enforced. For example, an utterance consisting of a sequence of modifier, fragments (like on Tuesday at three o&apos;clock on United) is interpreted as a conjoined property of a flight, because the only sort</context>
</contexts>
<marker>Hobbs, Appelt, Bear, Tyson, Magerman, 1992</marker>
<rawString>Hobbs, J., Appelt, D., Bear, J., Tyson, M., and Magerman, D. (1992). &amp;quot;Robust Processing of Real-World Natural-Language Texts&amp;quot;, in Text Based Intelligent Systems, ed. P. Jacobs, Lawrence Erlbaum Associates, Hillsdale, NJ, pp. 13-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>The Syntax and Semantics of the Japanese Language Engine&amp;quot;, forthcoming. In</title>
<date>1992</date>
<contexts>
<context position="5339" citStr="Kameyama, 1992" startWordPosition="835" endWordPosition="836"> and domain-specific portions of the system, and the underlying infrastructure and execution strategies. The Gemini kernel consists of a set of compilers to interpret the high-level languages in which the lexicon and syntactic and semantic grammar rules are written, as well as the parser, semantic interpretation, quantifier scoping, repair correction mechanisms, and all other aspects of Gemini that are not specific to a language or domain. Although this paper describes the lexicon, grammar, and semantics of English, Gemini has also been used in a Japanese spoken language understanding system (Kameyama, 1992). 2.1. Grammar Formalism Gemini includes a midsized constituent grammar of English (described in section 2.3), a small utterance grammar for assembling constituents into utterances (described in section 2.7), and a lexicon. All three are written in a variant of the unification formalism used in the Core Language Engine (Alshawi, 1992) . The basic building block of the grammar formalism is a category with feature constraints. Here is an example: up: [wh=ynq , case= (nomVacc ) , pers _num= ( 3rdAsg ) This category can be instantiated by any noun phrase with the value ynq for its wh feature (whic</context>
</contexts>
<marker>Kameyama, 1992</marker>
<rawString>Kameyama, M. (1992). &amp;quot;The Syntax and Semantics of the Japanese Language Engine&amp;quot;, forthcoming. In Mazuka, R., and N. Nagai, Eds. Japanese Syntactic Processing, Hillsdale, NJ: Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Grammar&amp;quot;,</title>
<date>1979</date>
<booktitle>in Proceedings of the 5th Annual Meeting of the Berkeley Linguistics Society.</booktitle>
<pages>142--158</pages>
<contexts>
<context position="7348" citStr="Kay, 1979" startWordPosition="1163" endWordPosition="1164">d unification. Each category has a set of features declared for it. Each feature has a declared value space of possible values (value spaces may be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their value space; so typing is also recursive. Typed feature structures are also used in HPSG (Pollard and Sag, in press). One important difference with the use in Gemini is that Gemini has no type inheritance. • Some approaches do not assume a syntactic skeleton of category-introducing rules (for example, Functional Unification Grammar (Kay, 1979)). Some make such rules implicit (for example, the various categorial unification approaches, such as Unification Categorial Grammar (Zeevat, Klein, and Calder, 1987)). • Even when a syntactic skeleton is assumed, some approaches do not distinguish the category of a constituent (for example, up, vp) from its other features (for example, pers_num, gaps in, gapsout). Thus, for example, in one version of GPSG, categories were simply feature bundles (attribute value structures) and there was a feature MAJ taking values like N,V,A, and P which determined the major category of constituent. • Gemini </context>
</contexts>
<marker>Kay, 1979</marker>
<rawString>Kay, M. (1979). &amp;quot;Functional Grammar&amp;quot;, in Proceedings of the 5th Annual Meeting of the Berkeley Linguistics Society. pp. 142-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kimball</author>
</authors>
<title>Seven Principles of Surface Structure Parsing in Natural Language&amp;quot;,</title>
<date>1973</date>
<journal>Cognition,</journal>
<volume>2</volume>
<pages>15--47</pages>
<contexts>
<context position="23929" citStr="Kimball, 1973" startWordPosition="3830" endWordPosition="3832">ism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism applies attachment heuristics based on the work by Pereira (1985) and Shieber (1983) Pereira&apos;s paper shows how the heuristics of Minimal Attachment and Right Association (Kimball, 1973) can both be implemented using a bottom-up shift-reduce parser. (2)(a) John sang a song for Mary. (b) John canceled the room Mary reserved yesterday. Minimal Attachment selects for the tree with the fewest nodes, so in (2a), the parse that makes for Mary a complement of sings is preferred. Right Association selects for the tree that incorporates a constituent A into the rightmost possible constituent (where rightmost here means beginning the furthest to the right). Thus, in (2b) the parse in which yesterday modifies reserved is preferred. The problem with these heuristics is that when they are</context>
</contexts>
<marker>Kimball, 1973</marker>
<rawString>Kimball, J. (1973). &amp;quot;Seven Principles of Surface Structure Parsing in Natural Language&amp;quot;, Cognition, Vol. 2, No. 1, pp. 15-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MADCOW</author>
</authors>
<title>Multi-site Data Collection for a Spoken Language Corpus&amp;quot;,</title>
<date>1992</date>
<booktitle>in Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<contexts>
<context position="3878" citStr="MADCOW, 1992" startWordPosition="600" endWordPosition="601">r scoping rules are applied to this best interpretation to produce the final logical form, which is then used as input to a query-answering system. The following sections describe each of these components in detail, with the exception of the query-answering subsystem, which is not described in this paper. In our component-by-component view of Gemini, we provide detailed statistics on each component&apos;s size, speed, coverage, and accuracy. These numbers detail our performance on the subdomain of air-travel planning that is currently being used by the ARPA spoken language understanding community (MADCOW, 1992). Gemini was trained on a 5875-utterance dataset from this domain, with another 688 utterances used as a blind test (not explicitly trained on, but run multiple times) to monitor our performance on a dataset on which we did not train. We also report here our results on another 756-utterance fair test set that we ran only once. Table 1 contains a summary of the coverage of the various components on both the training and fair test sets. More detailed 54 explanations of these numbers are given in the rel- and Lexical Functional Grammar (Bresnan, 1982). evant sections. - Training Test Lexicon 99.1</context>
</contexts>
<marker>MADCOW, 1992</marker>
<rawString>MADCOW (1992). &amp;quot;Multi-site Data Collection for a Spoken Language Corpus&amp;quot;, in Proceedings of the DARPA Speech and Natural Language Workshop, February 23-26, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language,</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="23183" citStr="Marcus 1980" startWordPosition="3715" endWordPosition="3716">rs are essentially orthogonal to repairs and that a filter including the repairs module will not suffer from precision problems. But we have not yet performed the experiments to decide this. 2.9. Parse Preference Mechanism In Gemini, parse preferences are enforced when extracting syntactically and semantically well-formed parse trees from the chart. In this respect, our approach differs from many other approaches to the problem of parse preferences, which make their preference decisions as parsing progresses, pruning subsequent parsing paths (Frazier and Fodor, 1978), (Hobbs and Bear, 1990), (Marcus 1980). Applying parse preferences requires comparing two subtrees spanning the same portion of the utterance. The parse preference mechanism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism applies attachment heuristics based on </context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. (1980). A Theory of Syntactic Recognition for Natural Language, MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moran</author>
</authors>
<title>Quantifier Scoping in the SRI Core Language Engine&amp;quot;,</title>
<date>1988</date>
<booktitle>in Proceedings of the 26th Annual Meeting of the Association</booktitle>
<pages>33--40</pages>
<institution>for Computational Linguistics, State University of New York at Buffalo,</institution>
<location>Buffalo, NY,</location>
<contexts>
<context position="27124" citStr="Moran, 1988" startWordPosition="4361" endWordPosition="4362">The final logical form produced by Gemini is the result of applying a set of quantifier scoping rules to the best interpretation chosen by the parse preference mechanism. The semantic rules build quasi-logical forms, which contain complete semantic predicate-argument structure, but do not specify quantifier scoping. The scoping algorithm that we use combines syntactic and semantic information with a set of quantifier scoping preference rules to rank the possible scoped logical forms consistent with the quasi-logical form selected by parse preferences. This algorithm is described in detail in (Moran, 1988). 3. CONCLUSION In our approach to resolving the tension between overgeneration and robustness in a spoken language understanding system, some aspects of Gemini are specifically oriented towards limiting overgeneration, such as the on-line property for the parser, and fully interleaved syntactic and semantic processing. Other components, such as the fragment and run-on processing provided by the utterance grammar, and the correction of recognizable grammatical repairs, increase the robustness of Gemini. We believe a robust system can still recognize and disprefer utterances containing recognit</context>
</contexts>
<marker>Moran, 1988</marker>
<rawString>Moran, D. (1988). &amp;quot;Quantifier Scoping in the SRI Core Language Engine&amp;quot;, in Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, State University of New York at Buffalo, Buffalo, NY, pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
</authors>
<title>Implementing Systemic Classification by Unification&amp;quot;.</title>
<date>1988</date>
<journal>Computational Linguistics</journal>
<volume>14</volume>
<pages>40--51</pages>
<contexts>
<context position="14862" citStr="Mellish, 1988" startWordPosition="2379" endWordPosition="2380">erb depart (as in flights departing Boston) is restricted to be an airport or a city, obviously a domain-specific requirement. But the same machinery also restricts a determiner like all to take two propositions, and an adjective like further to take distances as its measure-specifier (as in thirty miles further). In fact, sortal constraints are assigned to every atomic predicate and operator appearing in the logical forms constructed by the semantic rules. Sorts are located in a conceptual hierarchy and are implemented as Prolog terms such that more general sorts subsume more specific sorts (Mellish, 1988). This allows the subsumption checking and packing in the parser to share structure whenever possible. Semantic coverage with sortal constraints applied was 87.4% on the training set, and on the test set it was 83.7%. Interleaving Semantics with Parsing In Gemini, syntactic and semantic processing is fully interleaved. Building an edge requires that syntactic constraints be applied, which results in a tree structure, to which semantic rules can be applied, which results in a logical form to which sortal contraints can be applied. Only if the syntactic edge leads to a well-sorted semantically-a</context>
</contexts>
<marker>Mellish, 1988</marker>
<rawString>Mellish, C. (1988). &amp;quot;Implementing Systemic Classification by Unification&amp;quot;. Computational Linguistics Vol. 14, pp. 40-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
<author>J Dowding</author>
</authors>
<title>Efficient Bottom-up Parsing&amp;quot;,</title>
<date>1991</date>
<booktitle>in Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>200--203</pages>
<contexts>
<context position="12174" citStr="Moore and Dowding, 1991" startWordPosition="1953" endWordPosition="1956">e new edge is more general. • The parser is on-line (Graham, Harrison, and Russo, 1980), essentially meaning that all edges that end at position i are constructed before any that end at position i -I- 1. This feature is particularly desirable if the final architecture of the speech understanding system couples Gemini tightly with the speech recognizer, since it guarantees for any partial recognition input that all possible constituents will be built. An important feature of the parser is the mechanism used to constrain the construction of categories containing syntactic gaps. In earlier work (Moore and Dowding, 1991), we showed that approximately 80% of the edges built in an allpaths bottom-up parser contained gaps, and that it is possible to use prediction in a bottom-up parser only to constrain the gap categories, without requiring prediction for nongapped categories. This limited form of left-context constraint greatly reduces the total number of edges built for a very low overhead. In the 5875-utterance training set, the chart for the average sentence contained 313 edges, but only 23 predictions. 2.5. Typing The main advantage of typed unification is for grammar development. The type information on fe</context>
</contexts>
<marker>Moore, Dowding, 1991</marker>
<rawString>Moore, R., and Dowding, J. (1991). &amp;quot;Efficient Bottom-up Parsing&amp;quot;, in Proceedings of the DARPA Speech and Natural Language Workshop, February 19-22, 1991, pp. 200-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nakatani</author>
<author>J Hirschberg</author>
</authors>
<title>A Speech-First Model for Repair Detection and Correction&amp;quot;,</title>
<date>1993</date>
<booktitle>in Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="21810" citStr="Nakatani and Hirschberg, 1993" startWordPosition="3502" endWordPosition="3505">t. In the 5875-utterance training set, Gemini misidentified only 15 sentences (0.25%) as containing repairs when they did not. In the 756- utterance test set, only 2 sentences were misidentified as containing repairs (0.26%). While the repair correction component currently used in Gemini does not make use of acoustic/prosodic information, it is clear that acoustics can contribute meaningful cues to repair. In future work, we hope to improve the performance of our repair correction component by incorporating acoustic/prosodic techniques for repair detection (Bear, Dowding, and Shriberg, 1992) (Nakatani and Hirschberg, 1993) (O&apos;Shaughnessy, 1992). A central question about the repairs module concerns its role in a tightly integrated system in which the NL component filters speech recognition hypotheses. The open question: should the repairs module be part of the recognizer filter or should it continue to be a post-processing component? The argument for including it in the filter is that without a repairs module, the NL system rejects many sentences with repairs, and will thus disprefer essentially correct recognizer hypotheses. The argument against including it is efficiency and the concern that with recognizer er</context>
</contexts>
<marker>Nakatani, Hirschberg, 1993</marker>
<rawString>Nakatani, C., and Hirschberg, J. (1993). &amp;quot;A Speech-First Model for Repair Detection and Correction&amp;quot;, in Proceedings of the ARPA Workshop on Human Language Technology, March 21-24, 1993, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D O&apos;Shaughnessy</author>
</authors>
<title>Analysis of False Starts in Spontaneous Speech&amp;quot;,</title>
<date>1992</date>
<booktitle>in Proceedings of the 1992 International Conference on Spoken Language Processing,</booktitle>
<pages>931--934</pages>
<location>Banff, Alberta, Canada,</location>
<contexts>
<context position="21832" citStr="O&apos;Shaughnessy, 1992" startWordPosition="3506" endWordPosition="3507">g set, Gemini misidentified only 15 sentences (0.25%) as containing repairs when they did not. In the 756- utterance test set, only 2 sentences were misidentified as containing repairs (0.26%). While the repair correction component currently used in Gemini does not make use of acoustic/prosodic information, it is clear that acoustics can contribute meaningful cues to repair. In future work, we hope to improve the performance of our repair correction component by incorporating acoustic/prosodic techniques for repair detection (Bear, Dowding, and Shriberg, 1992) (Nakatani and Hirschberg, 1993) (O&apos;Shaughnessy, 1992). A central question about the repairs module concerns its role in a tightly integrated system in which the NL component filters speech recognition hypotheses. The open question: should the repairs module be part of the recognizer filter or should it continue to be a post-processing component? The argument for including it in the filter is that without a repairs module, the NL system rejects many sentences with repairs, and will thus disprefer essentially correct recognizer hypotheses. The argument against including it is efficiency and the concern that with recognizer errors present, the repa</context>
</contexts>
<marker>O&apos;Shaughnessy, 1992</marker>
<rawString>O&apos;Shaughnessy, D. (1992). &amp;quot;Analysis of False Starts in Spontaneous Speech&amp;quot;, in Proceedings of the 1992 International Conference on Spoken Language Processing, October 12-16, 1992, Banff, Alberta, Canada, pp. 931-934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
</authors>
<title>A New Characterization of Attachment Preferences&amp;quot;, in Natural Language Parsing, Ed. by</title>
<date>1985</date>
<pages>307--319</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="23809" citStr="Pereira (1985)" startWordPosition="3813" endWordPosition="3814">arse preferences requires comparing two subtrees spanning the same portion of the utterance. The parse preference mechanism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism applies attachment heuristics based on the work by Pereira (1985) and Shieber (1983) Pereira&apos;s paper shows how the heuristics of Minimal Attachment and Right Association (Kimball, 1973) can both be implemented using a bottom-up shift-reduce parser. (2)(a) John sang a song for Mary. (b) John canceled the room Mary reserved yesterday. Minimal Attachment selects for the tree with the fewest nodes, so in (2a), the parse that makes for Mary a complement of sings is preferred. Right Association selects for the tree that incorporates a constituent A into the rightmost possible constituent (where rightmost here means beginning the furthest to the right). Thus, in (</context>
</contexts>
<marker>Pereira, 1985</marker>
<rawString>Pereira, F. (1985). &amp;quot;A New Characterization of Attachment Preferences&amp;quot;, in Natural Language Parsing, Ed. by Dowty, D., Karttunen, L., and Zwicky, A., Cambridge University Press, Cambridge, pp. 307-319.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<journal>InformationBased Syntax and Semantics,</journal>
<volume>2</volume>
<marker>Pollard, Sag, </marker>
<rawString>Pollard, C., and Sag, I. (in press). InformationBased Syntax and Semantics, Vol. 2, CSLI Lecture Notes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>A Relaxation Method for Understanding Spontaneous Speech Utterances&amp;quot;,</title>
<date>1992</date>
<booktitle>in Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>299--304</pages>
<location>Harriman, NY,</location>
<contexts>
<context position="17173" citStr="Seneff, 1992" startWordPosition="2761" endWordPosition="2762">ur either at the beginning of the utterance, or at the end. Those constraints are stated in what we call the utterance grammar. They are applied after constituent parsing is complete by the utterance parser. The utterance grammar specifies ways of combining the categories found by the constituent parser into an analysis of the complete utterance. It is at this point that the system recognizes whether the sentence was a simple complete sentence, an isolated sentence fragment, a run-on sentence, or a sequence of related fragments. Many systems (Carbonell and Hayes, 1983), (Hobbs et al., 1992), (Seneff, 1992), (Stallard and Bobrow, 1992) have added robustness with a similar postprocessing phase. The approach taken in Gemini differs in that the utterance grammar uses the same syntactic and semantic rule formalism used by the constituent grammar. Thus, the same kinds of logical forms built during constituent parsing are the output of utterance parsing, with the same sortal constraints enforced. For example, an utterance consisting of a sequence of modifier, fragments (like on Tuesday at three o&apos;clock on United) is interpreted as a conjoined property of a flight, because the only sort of thing in the</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>Seneff, S. (1992). &amp;quot;A Relaxation Method for Understanding Spontaneous Speech Utterances&amp;quot;, in Proceedings of the Speech and Natural Language Workshop, Harriman, NY, pp. 299-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
</authors>
<title>Sentence Disambiguation by a Shift-Reduce Parsing Technique&amp;quot;,</title>
<date>1983</date>
<booktitle>in Proceedings of the 21 Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--118</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="23828" citStr="Shieber (1983)" startWordPosition="3816" endWordPosition="3817">quires comparing two subtrees spanning the same portion of the utterance. The parse preference mechanism begins with a simple strategy to disprefer parse trees containing specific &amp;quot;marked&amp;quot; syntax rules. As an example of a dispreferred rule, consider: Book those three flights to Boston. This sentence has a parse on which those three is a noun phrase with a missing head (consider a continuation of the discourse Three of our clients have sufficient credit). After penalizing such dispreferred parses, the preference 58 mechanism applies attachment heuristics based on the work by Pereira (1985) and Shieber (1983) Pereira&apos;s paper shows how the heuristics of Minimal Attachment and Right Association (Kimball, 1973) can both be implemented using a bottom-up shift-reduce parser. (2)(a) John sang a song for Mary. (b) John canceled the room Mary reserved yesterday. Minimal Attachment selects for the tree with the fewest nodes, so in (2a), the parse that makes for Mary a complement of sings is preferred. Right Association selects for the tree that incorporates a constituent A into the rightmost possible constituent (where rightmost here means beginning the furthest to the right). Thus, in (2b) the parse in wh</context>
</contexts>
<marker>Shieber, 1983</marker>
<rawString>Shieber, S. (1983). &amp;quot;Sentence Disambiguation by a Shift-Reduce Parsing Technique&amp;quot;, in Proceedings of the 21 Annual Meeting of the Association for Computational Linguistics, Boston, Massachusetts, pp. 113-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>H Uszkoreit</author>
<author>F Pereira</author>
<author>J Robinson</author>
<author>M Tyson</author>
</authors>
<title>The Formalism and Implementation of PATR-II&amp;quot;,</title>
<date>1983</date>
<booktitle>Research on Interactive Acquisition and Use of Knowledge, SRI International,</booktitle>
<pages>39--79</pages>
<editor>(eds)</editor>
<note>in</note>
<contexts>
<context position="6404" citStr="Shieber et al., 1983" startWordPosition="1010" endWordPosition="1013">ple: up: [wh=ynq , case= (nomVacc ) , pers _num= ( 3rdAsg ) This category can be instantiated by any noun phrase with the value ynq for its wh feature (which means it must be a wh-bearing noun phrase like which book, who, or whose mother), either acc (accusative) or nom (nominative) for its case feature, and the conjunctive value 3rdAsg (third and singular) for its person-number feature. This formalism is related directly to the Core Language Engine, but more conceptually it is closely related to that of other unification-based grammar formalisms with a context-free skeleton, such as PATR-II (Shieber et al., 1983), Categorial Unification Grammar (Uszkoreit, 1986), Generalized Phrase-Structure Grammar (Gazdar et al., 1982), Gemini differs from other unification formalisms in the following ways. Since many of the most interesting issues regarding the formalism concern typing, we defer discussing motivation until section 2.5. • Gemini uses typed unification. Each category has a set of features declared for it. Each feature has a declared value space of possible values (value spaces may be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their v</context>
</contexts>
<marker>Shieber, Uszkoreit, Pereira, Robinson, Tyson, 1983</marker>
<rawString>Shieber, S., Uszkoreit, H., Pereira, F., Robinson, J., and Tyson, M. (1983). &amp;quot;The Formalism and Implementation of PATR-II&amp;quot;, in Grosz, B. and Stickel, M. (eds) Research on Interactive Acquisition and Use of Knowledge, SRI International, pp. 39-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stallard</author>
<author>R Bobrow</author>
</authors>
<title>Fragment Processing in the DELPHI System&amp;quot;,</title>
<date>1992</date>
<booktitle>in Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>305--310</pages>
<location>Harriman, NY,</location>
<contexts>
<context position="17202" citStr="Stallard and Bobrow, 1992" startWordPosition="2763" endWordPosition="2766"> beginning of the utterance, or at the end. Those constraints are stated in what we call the utterance grammar. They are applied after constituent parsing is complete by the utterance parser. The utterance grammar specifies ways of combining the categories found by the constituent parser into an analysis of the complete utterance. It is at this point that the system recognizes whether the sentence was a simple complete sentence, an isolated sentence fragment, a run-on sentence, or a sequence of related fragments. Many systems (Carbonell and Hayes, 1983), (Hobbs et al., 1992), (Seneff, 1992), (Stallard and Bobrow, 1992) have added robustness with a similar postprocessing phase. The approach taken in Gemini differs in that the utterance grammar uses the same syntactic and semantic rule formalism used by the constituent grammar. Thus, the same kinds of logical forms built during constituent parsing are the output of utterance parsing, with the same sortal constraints enforced. For example, an utterance consisting of a sequence of modifier, fragments (like on Tuesday at three o&apos;clock on United) is interpreted as a conjoined property of a flight, because the only sort of thing in the ATIS domain that can be on T</context>
</contexts>
<marker>Stallard, Bobrow, 1992</marker>
<rawString>Stallard, D., and Bobrow, R. (1992). &amp;quot;Fragment Processing in the DELPHI System&amp;quot;, in Proceedings of the Speech and Natural Language Workshop, Harriman, NY, pp. 305-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uszkoreit</author>
</authors>
<title>Categorial Unification Grammars&amp;quot;,</title>
<date>1986</date>
<booktitle>in Proceedings of the 11th International Conference on Computational Linguistics and the 24th Annual Meeting of the Association for Computational Linguistics, Institut fur Kummunikkationsforschung und Phonetik,</booktitle>
<location>Bonn University.</location>
<contexts>
<context position="6454" citStr="Uszkoreit, 1986" startWordPosition="1018" endWordPosition="1019">sg ) This category can be instantiated by any noun phrase with the value ynq for its wh feature (which means it must be a wh-bearing noun phrase like which book, who, or whose mother), either acc (accusative) or nom (nominative) for its case feature, and the conjunctive value 3rdAsg (third and singular) for its person-number feature. This formalism is related directly to the Core Language Engine, but more conceptually it is closely related to that of other unification-based grammar formalisms with a context-free skeleton, such as PATR-II (Shieber et al., 1983), Categorial Unification Grammar (Uszkoreit, 1986), Generalized Phrase-Structure Grammar (Gazdar et al., 1982), Gemini differs from other unification formalisms in the following ways. Since many of the most interesting issues regarding the formalism concern typing, we defer discussing motivation until section 2.5. • Gemini uses typed unification. Each category has a set of features declared for it. Each feature has a declared value space of possible values (value spaces may be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their value space; so typing is also recursive. Typed fea</context>
</contexts>
<marker>Uszkoreit, 1986</marker>
<rawString>Uszkoreit, II. (1986). &amp;quot;Categorial Unification Grammars&amp;quot;, in Proceedings of the 11th International Conference on Computational Linguistics and the 24th Annual Meeting of the Association for Computational Linguistics, Institut fur Kummunikkationsforschung und Phonetik, Bonn University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zeevat</author>
<author>E Klein</author>
<author>J Calder</author>
</authors>
<title>An Introduction to Unification Categorial Grammar&amp;quot;,</title>
<date>1987</date>
<booktitle>Edinburgh Working Papers in Cognitive Science, Volume 1: Categorial Grammar, Unification Grammar, and Parsing.</booktitle>
<editor>in Haddock, N., Klein, E., Merrill, G. (eds.)</editor>
<contexts>
<context position="7513" citStr="Zeevat, Klein, and Calder, 1987" startWordPosition="1184" endWordPosition="1188"> be shared by different features). Feature structures in Gemini can be recursive, but only by having categories in their value space; so typing is also recursive. Typed feature structures are also used in HPSG (Pollard and Sag, in press). One important difference with the use in Gemini is that Gemini has no type inheritance. • Some approaches do not assume a syntactic skeleton of category-introducing rules (for example, Functional Unification Grammar (Kay, 1979)). Some make such rules implicit (for example, the various categorial unification approaches, such as Unification Categorial Grammar (Zeevat, Klein, and Calder, 1987)). • Even when a syntactic skeleton is assumed, some approaches do not distinguish the category of a constituent (for example, up, vp) from its other features (for example, pers_num, gaps in, gapsout). Thus, for example, in one version of GPSG, categories were simply feature bundles (attribute value structures) and there was a feature MAJ taking values like N,V,A, and P which determined the major category of constituent. • Gemini does not allow rules schematizing over syntactic categories. 2.2. Lexicon The Gemini lexicon uses the same category notation as the Gemini syntactic rules. Lexical c</context>
</contexts>
<marker>Zeevat, Klein, Calder, 1987</marker>
<rawString>Zeevat, H., Klein, E., and Calder, J. (1987). &amp;quot;An Introduction to Unification Categorial Grammar&amp;quot;, in Haddock, N., Klein, E., Merrill, G. (eds.) Edinburgh Working Papers in Cognitive Science, Volume 1: Categorial Grammar, Unification Grammar, and Parsing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>