<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.799335">
Classifying Japanese Polysemous Verbs based on Fuzzy C-means
Clustering
</title>
<author confidence="0.980546">
Yoshimi Suzuki
</author>
<affiliation confidence="0.966725666666667">
Interdisciplinary Graduate School of
Medicine and Engineering
University of Yamanashi, Japan
</affiliation>
<email confidence="0.998031">
ysuzuki@yamanashi.ac.jp
</email>
<sectionHeader confidence="0.994778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999873214285714">
This paper presents a method for classify-
ing Japanese polysemous verbs using an
algorithm to identify overlapping nodes
with more than one cluster. The algo-
rithm is a graph-based unsupervised clus-
tering algorithm, which combines a gener-
alized modularity function, spectral map-
ping, and fuzzy clustering technique. The
modularity function for measuring cluster
structure is calculated based on the fre-
quency distributions over verb frames with
selectional preferences. Evaluations are
made on two sets of verbs including pol-
ysemies.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951913043478">
There has been quite a lot of research concerned
with automatic clustering of semantically simi-
lar words or automatic retrieval of collocations
among them from corpora. Most of this work is
based on similarity measures derived from the dis-
tribution of words in corpora. However, the facts
that a single word does have more than one sense
and that the distribution of a word in a corpus is a
mixture of usages of different senses of the same
word often hamper such attempts. In general, re-
striction of the subject domain makes the problem
of polysemy less problematic. However, even in
texts from a restricted domain such as economics
or sports, one encounters quite a large number of
polysemous words. Therefore, semantic classifi-
cation of polysemies has been an interest since the
earliest days when a number of large scale corpora
have become available.
In this paper, we focus on Japanese polysemous
verbs, and present a method for polysemous verb
classification. We used a graph-based unsuper-
vised clustering algorithm (Zhang, 2007). The
algorithm combines the idea of modularity func-
</bodyText>
<note confidence="0.808422">
Fumiyo Fukumoto
Interdisciplinary Graduate School of
Medicine and Engineering
</note>
<affiliation confidence="0.930973">
University of Yamanashi, Japan
</affiliation>
<email confidence="0.984335">
fukumoto@yamanashi.ac.jp
</email>
<bodyText confidence="0.999930583333333">
tion Q, spectral relaxation and fuzzy c-means clus-
tering method to identify overlapping nodes with
more than one cluster. The modularity function
measures the quality of a cluster structure. Spec-
tral mapping performs a dimensionality reduction
which makes it possible to cluster in the very high
dimensional spaces. The fuzzy c-means allows for
the detection of nodes with more than one cluster.
We applied the algorithm to cluster polysemous
verbs. The modularity function for measuring the
quality of a cluster structure is calculated based
on the frequency distributions over verb frames
with selectional preferences. We collected seman-
tic classes from IPAL Japanese dictionary (IPAL,
1987), and used them as a gold standard data.
IPAL lists about 900 Japanese basic verbs, and cat-
egorizes each verb into multiple senses. Moreover,
the categorization is based on verbal syntax with
respect to the choice of its arguments. Therefore,
if the clustering algorithm induces a polysemous
verb classification on the basis of verbal syntax,
then the resulting classification should agree the
IPAL classes. We used a large Japanese newspaper
corpus and EDR (Electronic Dictionary Research)
dictionary (EDR, 1986) to obtain verbs and their
subcategorization frames with selectional prefer-
ences 1. The results obtained using two data sets
were better than the baseline, EM algorithm.
The rest of the paper is organized as follows.
The next section presents related work. After
describing Japanese verb with selectional pref-
erences, we present a distributional similarity in
Section 4, and a graph-based unsupervised clus-
tering algorithm in Section 5. Results using two
data sets are reported in Section 6. We give our
conclusion in Section 7.
</bodyText>
<footnote confidence="0.998895666666667">
1We did not use IPAL, but instead EDR sense dictionary.
Because IPAL did not have senses for the case filler which
were used to create selectional preferences.
</footnote>
<page confidence="0.975822">
32
</page>
<note confidence="0.999807">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 32–40,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<sectionHeader confidence="0.997466" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999976894736843">
Graph-based algorithms have been widely used
to classify semantically similar words (Jannink,
1999; Galley, 2003; Widdows, 2002; Muller,
2006). Sinha and Mihalcea proposed a graph-
based algorithm for unsupervised word sense
disambiguation which combines several seman-
tic similarity measures including Resnik’s metric
(Resnik, 1995), and algorithms for graph central-
ity (Sinha, 2007). They reported that the results
using the SENSEVAL-2 and SENSEVAL-3 En-
glish all-words data sets lead to relative error rate
reductions of 5 - 8% as compared to the previsous
work (Mihalcea, 2005). More recently, Matsuo
et al. (2006) presented a method of word clus-
tering based on Web counts using a search en-
gine. They applied Newman clustering (New-
man, 2004) for identifying word clusters. They
reported that the results obtained by the algorithm
were better than those obtained by average-link
agglomerative clustering using 90 Japanese noun
words. However, their method relied on hard-
clustering models, and thus have largely ignored
the issue of polysemy that word belongs to more
than one cluster.
In contrast to hard-clustering algorithms, soft
clustering allows that words to belong to more
than one cluster. Much of the previous work on
word classification with soft clustering is based
on the EM algorithm (Pereira, 1993). Torisawa
et al., (2002) presented a method to detect asso-
ciative relationships between verb phrases. They
used the EM algorithm to calculate the likelihood
of co-occurrences, and reported that the EM is ef-
fective to produce associative relationships with
a certain accuracy. More recent work in this di-
rection is that of Schulte et al., (2008). They
proposed a method for semantic verb classifica-
tion based on verb frames with selectional prefer-
ences. They combined the EM training with the
MDL principle. The MDL principle is used to
induce WordNet-based selectional preferences for
arguments within subcategorization frames. The
results showed the effectiveness of the method.
Our work is similar to their method in the use of
verb frames with selectional preferences. Korho-
nen et al. (2003) used verb–frame pairs to clus-
ter verbs into Levin-style semantic classes (Ko-
rhonen, 2003). They used the Information Bottle-
neck, and classified 110 test verbs into Levin-style
classes. They had a focus on the interpretation of
verbal polysemy as represented by the soft clus-
ters: they interpreted polysemy as multiple-hard
assignments.
In the context of Japanese taxonomy of verbs
and their classes, Utsuro et al. (1995) proposed a
class-based method for sense classification of ver-
bal polysemy in case frame acquisition from paral-
lel corpora (Utsuro, 1995). A measure of bilingual
class/class association is introduced and used for
discovering sense clusters in the sense distribution
of English predicates and Japanese case element
nouns. They used the test data consisting of 10 En-
glish and Japanese verbs taken from Roget’s The-
saurus and BGH (Bunrui Goi Hyo) (BGH, 1989).
They reported 92.8% of the discovered clusters
were correct. Tokunaga et al. (1997) presented
a method for extending an existing thesaurus by
classifying new words in terms of that thesaurus.
New words are classified on the basis of relative
probabilities of a word belonging to a given word
class, with the probabilities calculated using noun-
verb co-occurrence pairs. Experiments using the
Japanese BGH thesaurus showed that new words
can be classified correctly with a maximum accu-
racy of more than 80%, while they did not report
in detail whether the clusters captured polysemies.
</bodyText>
<sectionHeader confidence="0.984983" genericHeader="method">
3 Selectional Preferences
</sectionHeader>
<bodyText confidence="0.999905347826087">
A major approach on word clustering task is to use
distribution of a word in a corpus, i.e., words are
classified into classes based on their distributional
similarity. Similarity measures based on distribu-
tional hypothesis compare a pair of weighted fea-
ture vectors that characterize two words (Hindle,
1990; Lin, 1998; Dagan, 1999).
Like previous work on verb classification, we
used subcategorization frame distributions with
selectional preferences to calculate similarity be-
tween verbs (Schulte, 2008). We used the EDR
dictionary of selectional preferences consisting of
5,269 basic Japanese verbs and the EDR concept
dictionary (EDR, 1986). For selectional prefer-
ences, the dictionary has each concept of a verb,
the group of possible co-occurrence surface-level
case particles, the types of concept relation label
that correspond to the surface-level case as well
as the range of possible concepts that may fill the
deep-level case. Figure 1 illustrates an example of
a verb “taberu (eat)”.
In Figure 1, “Sentence pattern” refers to the co-
occurrence pattern between a verb and a noun
</bodyText>
<page confidence="0.996507">
33
</page>
<figure confidence="0.813117714285714">
[Sentence pattern] &lt;word1&gt; ga &lt;word2&gt; wo taberu (eat)
[Sense relation] agent object
[Case particle] ga (nominative) wo (accusative)
[Sense identifier] 30f6b0 (human);30f6bf (animal) 30f6bf(animal);30f6ca(plants);
30f6e5(parts of plants);
3f9639(food and drink);
3f963a(feed)
</figure>
<figureCaption confidence="0.999231">
Figure 1: An example of a verb “taberu (eat)”
</figureCaption>
<bodyText confidence="0.9859485">
with a case marker. “Sense relation” expresses the
deep-level case, while “Case particle” shows the
surface-level case. “Sense identifier” refers to the
range of possible concepts for the case filler. The
subcategorization frame pattern of a sentence (1),
for example consists of two arguments with selec-
tional preferences and is given below:
(1) Nana ga apple wo taberu.
‘Nana eats an apple.’
taberu 30f6b0 ga 3f9639 wo
eat human nom entity acc
In the above frame pattern, x of the argument
“x y” refers to sense identifier and y denotes case
particle.
</bodyText>
<sectionHeader confidence="0.984275" genericHeader="method">
4 Distributional Similarity
</sectionHeader>
<bodyText confidence="0.998383428571429">
Various similarity measures have been proposed
and used for NLP tasks (Korhonen, 2002). In
this paper, we concentrate on three distance-based,
and entropy-based similarity measures. In the fol-
lowing formulae, x and y refer to the verb vec-
tors, their subscripts to the verb subcategorization
frame values.
</bodyText>
<listItem confidence="0.51390665">
1. The Cosine measure (Cos): The cosine
measures the similarity of the two vectors x
and y by calculating the cosine of the an-
gle between vectors, where each dimension
of the vector corresponds to each frame with
selectional preferences patterns of verbs and
each value of the dimension is the frequency
of each pattern.
2. The Cosine measure based on probability
of relative frequencies (rfCos): The differ-
ences between the cosine and the value based
on relative frequencies of verb frames with
selectional preferences are the values of each
dimension, i.e., the former are frequencies of
each pattern and the latter are the fraction of
the total number of verb frame patterns be-
longing to the verb.
3. L1 Norm (L1): The L1 Norm is a mem-
ber of a family of measures known as the
Minkowski Distance, for measuring the dis-
</listItem>
<bodyText confidence="0.8509415">
tance between two points in space. The L1
distance between two verbs can be written as:
</bodyText>
<equation confidence="0.997339">
L1(x, y) = �n  |xi − yi  |.
i=1
</equation>
<listItem confidence="0.937948">
4. Kullback-Leibler (KL): Kullback-Leibler is
a measure from information theory that deter-
mines the inefficiency of assuming a model
probability distribution given the true distri-
bution.
</listItem>
<equation confidence="0.999153666666667">
P(xi)
P(xi) ∗ log .
P(yi)
</equation>
<bodyText confidence="0.995479">
where P(xi) =xi
|x|. KL is not defined in
case yi = 0. So, the probability distribu-
tions must be smoothed (Korhonen, 2002).
We used two smoothing methods, i.e., Add-
one smoothing and Witten and Bell smooth-
ing (Witten, 1991).2 Moreover, two variants
of KL, α-skew divergence and the Jensen-
Shannon, were used to perform smoothing.
</bodyText>
<listItem confidence="0.953456333333333">
5. α-skew divergence (α div.): The α-skew di-
vergence measure is a variant of KL, and is
defined as:
</listItem>
<equation confidence="0.625481">
αdiv(x, y) = KL(y, α · x + (1 − α) · y).
</equation>
<bodyText confidence="0.813220125">
Lee (1999) reported the best results with α =
0.9. We used the same value.
6. The Jensen-Shannon (JS): The Jensen-
Shannon is a measure that relies on the as-
sumption that if x and y are similar, they are
close to their average. It is defined as:
2We report Add-one smoothing results in the evaluation,
as it was better than Witten and Bell smoothing.
</bodyText>
<equation confidence="0.6626676">
KL(x, y) = �n
i=1
34
1 2 ) + KL(y, x + y carried out through an iterative opti-
JS(x, y) = 2[KL(x, x + y 2 )]. mization (minimization) of the objective
</equation>
<bodyText confidence="0.852535">
function Jm with the update of member-
ship degree uij and the cluster centers
cj. Jm is defined as:
All measures except Cos and rfCos showed that Jm = n k umij  ||vi − cj ||2,
smaller values indicate a closer relation between i=1 E
two verbs. Thus, we used inverse of each value. j=1
</bodyText>
<sectionHeader confidence="0.937679" genericHeader="method">
5 Clustering Method
</sectionHeader>
<bodyText confidence="0.999921166666667">
The clustering algorithm used in this study was a
graph-based unsupervised clustering reported by
(Zhang, 2007). This algorithm detects overlap-
ping nodes by the combination of a modularity
function based on Newman Girvan’s Q function
(Newman, 2004), spectral mapping that maps in-
put nodes into Euclidean space, and fuzzy c-means
clustering which allows node to belong to more
than one cluster. They evaluated their method by
applying several data including the American col-
lege football team network, and found that the al-
gorithm successfully detected overlapping nodes.
We thus used the algorithm to cluster verbs.
Here are the key steps of the algorithm: Given
a set of input verbs V = {v1, v2, · · · vn}, an up-
per bound K of the number of clusters, the adja-
cent matrix A = (aij)nxn of an input verbs and a
threshold λ that can convert a soft assignment into
final clustering, i.e., the value of λ decreases, each
verb is distributed into larger number of clusters.
We calculated the adjacent matrix A by using one
of the similarity measures mentioned in Section 4,
i.e., the value of the edge between vi and vj. aij
refers to the similarity value between them.
</bodyText>
<listItem confidence="0.997196357142857">
1. Form a diagonal matrix D = (dii), where dii
= Ek aik.
2. Form the eigenvector matrix EK =
[e1, e2, · · · , eK] by calculating the top K
eigenvectors of the generalized eigensystem
Ax = tDx.
3. For each value of k, 2 &lt; k &lt; K:
(a) Form the matrix Ek = [e2, · · · , ek] where
ek refers to the top k-th eigenvector.
(b) Normalize the rows of Ek to unit length
using Euclidean distance norm.
(c) Cluster the row vectors of Ek using
fuzzy c-means to obtain a soft assign-
ment matrix Uk. Fuzzy c-means is
</listItem>
<bodyText confidence="0.9999318">
where uij is the membership degree of
vi in the cluster j, and Ej uij = 1. m E
[1, oc] is a weight exponent controlling
the degree of fuzzification. cj is the d-
dimensional center of the cluster j.
</bodyText>
<equation confidence="0.8956635">
 ||vi − cj  ||is defined as:
 ||vi − cj ||2 = (vi − cj)E(vi − cj)T .
</equation>
<bodyText confidence="0.988366333333333">
where E denotes an unit matrix. The
procedure converges to a saddle point of
Jm.
</bodyText>
<listItem confidence="0.722303571428571">
4. Pick the k and the corresponding n x k
soft assignment matrix Uk that maximizes
the modularity function Q(Uk). Here Uk =
[u1, · · · uk] with 0 &lt; uic &lt; 1 for each c = 1,
· · ·, k, and Ek1 uic = 1 for each i = 1, · · ·, n.
A modularity function of a soft assignment
matrix is defined as:
</listItem>
<bodyText confidence="0.8015255">
A(V, V )A(Vc, Vc) − A(˜Vc, V ) 2
where
</bodyText>
<equation confidence="0.9980447">
A( ˜Vc, ˜Vc) = ~ { 2 }aij,
(uic + ujc)
i∈
�Vc�j∈�Vc
A( ˜Vc, V ) = A( ˜Vc, ˜Vc) +
{ (uic + (1 − ujc)) }aij,
2
�
A(V, V ) =
i∈VJ∈V
</equation>
<bodyText confidence="0.9971598">
Q(Uk) shows comparison of the actual val-
ues of internal or external edges with its re-
spective expectation value under the assump-
tion of equally probable links and given data
sizes.
</bodyText>
<equation confidence="0.681399875">
k
˜Q(Uk) =
c=1
11
i∈
�Vc
�VcJ∈V \
aij.
</equation>
<page confidence="0.992351">
35
</page>
<sectionHeader confidence="0.998337" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998172">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.998083918367347">
We created test verbs using two sets of Japanese
Mainichi newspaper corpus. One is a set con-
sisting one year (2007) newspapers (We call it a
set from 2007), and another is a set of 17 years
(from 1991 to 2007) Japanese Mainichi newspa-
pers (We call it a set from 1991 2007). For each
set, all Japanese documents were parsed using the
syntactic analyzer Cabocha (Kudo, 2003). We
selected verbs, each frequency f(v) is, 500 &lt;
f(v) &lt; 10,000. As a result, we obtained 279
verbs for a set from 2007 and 1,692 verbs for
a set from 1991 2007. From these verbs, we
chose verbs which appeared in the machine read-
able dictionary, IPAL. This selection resulted in
a total of 81 verbs for a set from 2007, and 170
verbs, for a set from 1991 2007. We obtained
Japanese verb frames with selectional preferences
using these two sets. We extracted sentence pat-
terns with their frequencies. Noun words within
each sentence were tagged sense identifier by us-
ing the EDR Japanese sense dictionary. As a re-
sult, we obtained 56,400 verb frame patterns for a
set from 2007, and 300,993 patterns for a set from
1991 2007.
We created the gold standard data, verb classes,
using IPAL. IPAL lists about 900 Japanese verbs
and categorizes each verb into multiple senses,
based on verbal syntax and semantics. It also
listed synonym verbs. Table 1 shows a fragment of
the entry associated with the Japanese verb taberu.
The verb “taberu” has two senses, “eat” and
“live”. “pattern” refers to the case frame(s) associ-
ated with each verb sense. According to the IPAL,
we obtained verb classes, each class corresponds
to a sense of each verb. There are 87 classes for
a set from 2007, and 152 classes for a set from
1991 2007. The examples of the test verbs and
their senses are shown in Table 2.
For evaluation of verb classification, we used
the precision, recall, and F-score, which were de-
fined by (Schulte, 2000), especially to capture
how many verbs does the algorithm actually de-
tect more than just the predominant sense.
For comparison against polysemies, we utilized
the EM algorithm which is widely used as a soft
clustering technique (Schulte, 2008). We followed
the method presented in (Rooth, 1999). We used
a probability distribution over verb frames with
selectional preferences. The initial probabilities
</bodyText>
<tableCaption confidence="0.990412">
Table 3: Results for a set from 2007
</tableCaption>
<table confidence="0.9977145">
Method m A C Prec Rec F
FCM 2.0 0.09 74 .815 .483 .606
FCM(none) 1.5 0.07 74 .700 .477 .567
EM – – 87 .308 .903 .463
</table>
<tableCaption confidence="0.997369">
Table 4: Results against each measure
</tableCaption>
<table confidence="0.997795375">
Measure m A C Prec Rec F
cos 3.0 0.02 74 .660 .517 .580
rfcos 2.0 0.04 74 .701 .488 .576
Ll 2.0 0.04 74 .680 .500 .576
KL 2.0 0.09 74 .815 .483 .606
α div. 2.0 0.04 74 .841 .471 .604
JS 1.5 0.03 74 .804 .483 .603
EM – – 87 .308 .903 .463
</table>
<bodyText confidence="0.999287285714286">
were often determined randomly. We set the ini-
tial probabilities by using the result of the standard
k-means. For k-means, we used 50 random repli-
cations of the initialization, each time initializing
the cluster center with k randomly chosen. We
used up to 20 iterations to learn the model prob-
abilities.
</bodyText>
<subsectionHeader confidence="0.999622">
6.2 Basic results
</subsectionHeader>
<bodyText confidence="0.999680444444444">
The results using a set from 2007 are shown in
Table 3. We used KL as a similarity measure in
FCM. “FCM(none)” shows the result not applying
a spectral mapping, i.e., we applied fuzzy c-means
to each vector of verb, where each dimension of
the vector corresponds to each frame with selec-
tional preferences. “m” and “A” refer to the pa-
rameters used by Fuzzy C-means. “C” refers to
the number of clusters obtained by each method.
“m”, “A” and “C” in Table 3 denote the value that
maximized the F-score. “C” in the EM is fixed
in advance. The result of EM shows the best re-
sult among 20 iterations. As can be seen clearly
from Table 3, the result obtained by fuzzy c-means
was better to the result by EM algorithm. Table
3 also shows that a dimensionality reduction, i.e.,
spectral mapping improved overall performance,
especially we have obtained better precision. The
result suggests that a dimensionality reduction is
effective for clustering. Table 4 shows the results
obtained by using each similarity measure. As we
can see from Table 4, the overall results obtained
by information theory based measures, KL, α div.,
and JS were slightly better to the results obtained
by other distance based measures.
We note that the fuzzy c-means has two param-
eters A and m, where A is a threshold of the as-
</bodyText>
<page confidence="0.999408">
36
</page>
<tableCaption confidence="0.999676">
Table 1: A fragment of the entry associated with the Japanese verb “taberu”
</tableCaption>
<table confidence="0.984190666666667">
Sense id Pattern Synonyms
1 kare(he) ga(nominative) soba(noodles) wo(accusative) kuu (eat)
2 kare (he) ga(nominative) fukugyo(a part-time job) de(accusative) kurasu (live)
</table>
<tableCaption confidence="0.964532">
Table 2: Examples of test verbs and their polysemic gold standard senses
</tableCaption>
<table confidence="0.782385333333333">
Id Sense Verb Classes Id Sense Verb Classes
1 treat {ashirau, atsukau} 11 tell {oshieru, shimesu, shiraseru}
2 prey {negau,inoru} 12 persuade {oshieru,satosu}
3 wish {negau, nozomu} 13 congratulate {iwau, syukufukusuru}
4 ask {negau, tanomu} 14 accept {uketoru, ukeru, morau, osameru}
5 leave {saru,hanareru} 15 take {uketoru,toru,kaisyakusuru, miru}
6 move {saru, utsuru} 16 lose {ushinau, nakusu}
7 pass {saru,kieru,sugiru} 17 miss {ushinau,torinogasu,itusuru}
8 go {saru,sugiru,iku} 18 survive, lose {ushinau, nakusu,shinareru}
9 remove {saru, hanareru, toozakeru 19 give {kubaru, watasu, wakeru}
torinozoku}
10 lead {oshieru, michibiku,tugeru} 20 arrange {kubaru,haichisuru}
</table>
<figureCaption confidence="0.905478">
Figure 2: F-score against A Figure 3: F-score against m
</figureCaption>
<bodyText confidence="0.999979611111111">
signment in the fuzzy c-means, and m is a weight
controlling the degree of fuzzification. To exam-
ine how these parameters affect the overall per-
formance of the algorithm, we performed exper-
iments by varying these parameters. Figure 2 il-
lustrates F-score of polysemies against the value
of A. We used KL as a similarity measure, m = 2,
and C = 74.
As shown in Figure 2, the best result was ob-
tained when the value of A was 0.09. When A
value was larger than 0.09, the overall perfor-
mance decreased, and when it exceeded 1.2, no
verbs were assigned to multiple sense. Figure 3
illustrates F-score against the value of m. As il-
lustrated in Figure 3, we could not find effects on
accuracy against the value of m. It is necessary to
investigate on the influence of the parameter m by
performing further quantitative evaluation.
</bodyText>
<subsectionHeader confidence="0.998828">
6.3 Error analysis against polysemy
</subsectionHeader>
<bodyText confidence="0.999162333333333">
We examined whether 46 polysemous verbs in
a set from 2007 were correctly classified into
classes. We manually analyzed clustering results
obtained by running fuzzy c-means with KL as a
similarity measure. They were classified into three
types of error.
</bodyText>
<listItem confidence="0.541519">
1. Partially correct: Some senses of a poly-
</listItem>
<bodyText confidence="0.996572545454545">
semous verb were correctly identified, but
others were not. The first example of this
pattern is that “nigiru” has at least two
senses, “motsu (have)” and “musubu (dou-
ble)”. However, only one sense was identi-
fied correctly. The second example is that one
of the senses of the verb “watasu” was clas-
sified correctly into the class “ataeru (give)”,
while it was classified incorrectly into the
class “uru (sell)”. This was the most frequent
error type.
</bodyText>
<page confidence="0.998597">
37
</page>
<bodyText confidence="0.973325658536586">
{nigiru, motsu (have)}
φ
{watasu, ataeru (give)}
{watasu, uru (sell)}
2. Polysemous verbs classified into only one
cluster: “hakobu” has two senses “carry”,
and “progress”. However, it was classified
into one cluster including verbs “motuteiku
(carry)”, and “susumu (progress)”. Because
it often takes the same nominative subjects
such as “human” and accusative object such
as “abstract”.
{hakobu (carry, progress),
motuteiku (carry), susumu (progress)}
3. Polysemous verb incorrectly classified into
clusters: The polysemous verb “hataraku”
has two senses, “work”, and “operate”. How-
ever, it was classified incorrectly into “ochiru
(fall)” and “tsukuru (make)”.
{hataraku (work, operate), ochiru (fall),
tsukuru (make)}
Apart from the above error analysis, we found
that we should improve the definition and demar-
cation of semantic classes by using other exist-
ing thesaurus, e.g., EDR or BGH (Bunrui Goi
Hyo) (BGH, 1989). We recall that we created
the gold standard data by using synonymous infor-
mation. However, the algorithm classified some
antonymous words such as “uketoru” (receive) and
“watasu” (give) into one cluster. Similarly, transi-
tive and intransitive verbs are classified into the
same cluster. For example, intransitive verb of the
verb “ochiru” (drop) is “otosu”. They were clas-
sified into one cluster. It would provide further
potential, i.e., not only to improve the accuracy
of classification, but also to reveal the relationship
between semantic verb classes and their syntactic
behaviors.
An investigation of the resulting clusters re-
vealed another interesting direction of the method.
We found that some senses of a polysemous verb
</bodyText>
<tableCaption confidence="0.97466">
Table 5: Results for a set from 1991 2007
</tableCaption>
<table confidence="0.99225075">
Method m λ C Prec Rec F
FCM 2.0 0.24 152 .792 .477 .595
FCM(none) 2.0 0.07 147 .687 .459 .550
EM – – 152 .284 .722 .408
</table>
<bodyText confidence="0.999165">
which is not listed in the IPAL are correctly identi-
fied by the algorithm. For example, “ukeireru” and
“yurusu” (forgive) were correctly classified into
one cluster. Figure 4 illustrates a sample of verb
frames with selectional preferences extracted by
our method.
“ukeireru” and “yurusu” in Table 4 have the same
frame pattern, and the sense identifiers of the case
filler “wo”, for example, are “a human being”
(0f0157) and “human” (30f6b0). However, these
verbs are not classified into one class in the IPAL:
“ukeireru” is not listed in the IPAL as a synonym
verb of “yurusu”. The example illustrates that
these verbs within a cluster are semantically re-
lated, and that they share obvious verb frames with
intuitively plausible selectional preferences. This
indicates that we can extend the algorithm to solve
this resource scarcity problem: semantic classifi-
cation of words which do not appear in the re-
source, but appear in corpora.
</bodyText>
<sectionHeader confidence="0.982303" genericHeader="method">
6.4 Results for a set of verbs from 1991 2007
corpus
</sectionHeader>
<bodyText confidence="0.999847863636364">
One goal of this work was to develop a cluster-
ing methodology with respect to the automatic
recognition of Japanese verbal polysemies cover-
ing large-scale corpora. For this task, we tested a
set of 170 verbs including 82 polysemies. The re-
sults are shown in Table 5. We used KL as a simi-
larity measure in FCM. Each value of the parame-
ter shows the value that maximized the F-score.
As shown in Table 5, the result obtained by fuzzy
c-means was as good as for the smaller set, a set
of 78 verbs. Moreover, we can see that the fuzzy
c-means is better than the EM algorithm and the
method not applying a spectral mapping, as an in-
crease in the F-score of 18.7% compared with the
EM, and 4.5% compared with a method without
spectral mapping. This shows that our method is
effective for a size of the input test data consisting
178 verbs.
One thing should be noted is that when the al-
gorithm is applied to large data, it is computation-
ally expensive. There are at least two ways to ad-
dress the problem. One is to use several methods
</bodyText>
<page confidence="0.997043">
38
</page>
<table confidence="0.793164777777778">
[Sentence pattern] &lt;word1&gt; ga &lt;word2&gt; wo ukeireru / yurusu (forgive)
[Concept relation] agent object
[Case particle] ga (nominative) wo (accusative)
[Sense identifier] 0ee0de; 0f58b4; 0f98ee 0f0157; 30f6b0
0ee0de: the part of a something written that makes reference to a particular matter
0f58b4: a generally-held opinion
0f98ee: the people who citizens of a nation
0f0157: a human being
30f6b0: human
</table>
<figureCaption confidence="0.999214">
Figure 4: Extracted Verb frames of “ukeireru” and “yurusu” (forgive)
</figureCaption>
<bodyText confidence="0.9998839">
of fuzzy c-means acceleration. Kelen et al. (2002)
presented an efficient implementation of the fuzzy
c-means algorithm, and showed that the algorithm
had the worse-case complexity of O(nK2), where
n is the number of nodes, and K is the number of
eigenvectors. Another approach is to parallelize
the algorithm by using the Message Passing Inter-
face (MPI) to estimate the optimal number of k (2
&lt; k &lt; K). This is definitely worth trying with our
method.
</bodyText>
<sectionHeader confidence="0.998204" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999945071428571">
We have developed an approach for classifying
Japanese polysemous verbs using fuzzy c-means
clustering. The results were comparable to other
unsupervised techniques. Future work will assess
by a comparison against other existing soft clus-
tering algorithms such as the Clique Percolation
method (Palla, 2005). Moreover, it is necessary
to apply the method to other verbs for quantitative
evaluation. New words including polysemies are
generated daily. We believe that classifying these
words into semantic classes potentially enhances
many semantic-oriented NLP applications. It is
necessary to apply the method to other verbs, espe-
cially low frequency of verbs to verify that claim.
</bodyText>
<sectionHeader confidence="0.996982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999335">
This work was supported by the Grant-in-aid for
the Japan Society for the Promotion of Science
(JSPS).
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999540595744681">
E. Iwabuchi. 1989. Word List by Semantic Principles,
National Language Research Institute Publications,
Shuei Shuppan.
I. Dagan and L. Lee and F. C. N. Pereira. 1999.
Similarity-based Models of Word Cooccurrence
Probabilities. Machine Learning, 34(1-3), pages
43–69.
Japan Electronic Dictionary Research Institute, Ltd.
http://www2.nict.go.jp/r/r312/EDR/index.html
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining, In Proc.
of 19th International Joint Conference on Artificial
Intelligence, pages 1486–1488.
D. Hindle. 1990. Noun Classification from Predicate-
Argument Structures, In Proc. of 28th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 268–275.
GSK2007-D. http://www.gsk.or.jp/catalog/GSK2007-
D/catalog.html
J. Jannink and G. Wiederhold. 1999. Thesaurus Entry
Extraction from an On-line Dictionary, In Proc. of
Fusion’99.
J. F. Kelen and T. Hutcheson. 2002. Reducing the
Time Complexity of the Fuzzy C-means Algorithm,
In Trans. of IEEE Fuzzy Systems, 10(2), pages 263–
267.
A. Korhonen and Y. Krymolowski. 2002. On the
Robustness of Entropy-based Similarity Measures
in Evaluation of Subcategorization Acquisiton Sys-
tems. In Proc. of the 6th Conference on Natural
Language Learning, pages 91–97.
A. Korhonen and Y. Krymolowski and Z. Marx. 2003.
Clustering Polysemic Subcategorization Frame Dis-
tributions Semantically. In Proc. of the 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 64–71.
T.Kudo and Y.Matsumoto. 2003. Fast Methods for
Kernel-based Text Analysis. In Proc. of 41th ACL,
pages 24–31.
L. Lee. 1999. Measures of Distributional Similarity.
In Proc. of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 25–32.
D. Lin. 1998. Automatic Retrieval and Clustering
of Similar Words, In Proc. of 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, pages 768–773.
</reference>
<page confidence="0.988219">
39
</page>
<reference confidence="0.999934246575342">
Y. Matsuo and T. Sakaki and K. Uchiyama and M.
Ishizuka. 2006. Graph-based Word Clustering us-
ing a Web Search Engine, In Proc. of 2006 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2006), pages 542–550.
R. Mihalcea. 2005. Unsupervised Large Vocabulary
Word Sense Disambiguation with Graph-based Al-
gorithms for Sequence Data Labeling, In Proc. of
the Human Language Technology /Empirical Meth-
ods in Natural Language Processing Conference,
pages 411–418.
P. Muller and N. Hathout and B. Gaume. 2006. Syn-
onym Extraction Using a Semantic Distance on a
Dictionary, In Proc. of the Workshop on TextGraphs,
pages 65–72.
M.E.J.Newman. 2004. Fast Algorithm for Detecting
Community Structure in Networks, Physical Re-
view, E 2004, 69, 066133.
G. Palla and I. Der´enyi and I. Farkas and T. Vic-
sek. 2005. Uncovering the Overlapping Commu-
nity Structure of Complex Networks in Nature and
Society, Nature. 435(7043), 814–8.
F. Pereira and N. Tishby and L. Lee. 1993. Distribu-
tional Clustering of English Words. In Proc. of the
31st Annual Meeting of the Association for Compu-
tational Linguistics, pages 183–190.
P. Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proc.
of 14th International Joint Conference on Artificial
Intelligence, pages 448–453.
M. Rooth et al. 1999. Inducing a Semantically Anno-
tated Lexicon via EM-Based Clustering, In Proc. of
37th ACL, pages 104–111.
R. Sinha and R. Mihalcea. 2007. Unsupervised Graph-
based Word Sense Disambiguation Using Measures
of Word Semantic Similarity. In Proc. of the IEEE
International Conference on Semantic Computing,
pages 46–54.
S. Schulte im Walde. 2000. Clustering Verbs Seman-
tically according to their Alternation Behaviour. In
Proc. of the 18th COLING, pages 747–753.
S. Schulte im Walde et al. 2008. Combining EM
Training and the MDL Principle for an Automatic
Verb Classification Incorporating Selectional Pref-
erences. In Proc. of the 46th ACL, pages 496–504.
T. Tokunaga and A. Fujii and M. Iwayama and N. Saku-
rai and H. Tanaka. 1997. Extending a thesaurus
by classifying words. In Proc. of the ACL-EACL
Workshop on Automatic Information Extraction and
Building of Lexical Semantic Resources, pages 16–
21.
K. Torisawa. 2002. An Unsupervised Learning
Method for Associative Relationships between Verb
Phrases, In Proc. of 19th International Confer-
ence on Computational Linguistics (COLING2002),
pages 1009–1015.
T. Utsuro. 1995. Class-based sense classification of
verbal polysemy in case frame acquisition from par-
allel corpora. In Proc. of the 3rd Natural Language
Processing Pacific Rim Symposium, pages 671–677.
D. Widdows and B. Dorow. 2002. A Graph Model for
Unsupervised Lexical Acquisition. In Proc. of 19th
International conference on Computational Linguis-
tics (COLING2002), pages 1093–1099.
I. H. Witten and T. C. Bell. 1991. The Zero-
Frequency Problem: Estimating the Probabilities of
Novel Events in Adaptive Text Compression. IEEE
Transactions on Information Theory, 37(4), pages
1085–1094.
S. Zhang et al. 2007. Identification of Overlapping
Community Structure in Complex Networks using
Fuzzy C-means Clustering. PHYSICA A, 374, pages
483–490.
</reference>
<page confidence="0.998638">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316749">
<title confidence="0.988752">Classifying Japanese Polysemous Verbs based on Fuzzy C-means Clustering</title>
<author confidence="0.544467">Yoshimi</author>
<affiliation confidence="0.782930666666667">Interdisciplinary Graduate School Medicine and University of Yamanashi,</affiliation>
<email confidence="0.987865">ysuzuki@yamanashi.ac.jp</email>
<abstract confidence="0.9911256">This paper presents a method for classifying Japanese polysemous verbs using an algorithm to identify overlapping nodes with more than one cluster. The algorithm is a graph-based unsupervised clustering algorithm, which combines a generalized modularity function, spectral mapping, and fuzzy clustering technique. The modularity function for measuring cluster structure is calculated based on the frequency distributions over verb frames with selectional preferences. Evaluations are made on two sets of verbs including polysemies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Iwabuchi</author>
</authors>
<title>Word List by Semantic Principles,</title>
<date>1989</date>
<institution>National Language Research Institute Publications, Shuei Shuppan.</institution>
<marker>Iwabuchi, 1989</marker>
<rawString>E. Iwabuchi. 1989. Word List by Semantic Principles, National Language Research Institute Publications, Shuei Shuppan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F C N Pereira</author>
</authors>
<title>Similarity-based Models of Word Cooccurrence Probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan and L. Lee and F. C. N. Pereira. 1999. Similarity-based Models of Word Cooccurrence Probabilities. Machine Learning, 34(1-3), pages 43–69.</rawString>
</citation>
<citation valid="false">
<institution>Japan Electronic Dictionary Research Institute, Ltd.</institution>
<note>http://www2.nict.go.jp/r/r312/EDR/index.html</note>
<marker></marker>
<rawString>Japan Electronic Dictionary Research Institute, Ltd. http://www2.nict.go.jp/r/r312/EDR/index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining,</title>
<date>2003</date>
<booktitle>In Proc. of 19th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1486--1488</pages>
<marker>Galley, McKeown, 2003</marker>
<rawString>M. Galley and K. McKeown. 2003. Improving Word Sense Disambiguation in Lexical Chaining, In Proc. of 19th International Joint Conference on Artificial Intelligence, pages 1486–1488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun Classification from PredicateArgument Structures,</title>
<date>1990</date>
<booktitle>In Proc. of 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="7970" citStr="Hindle, 1990" startWordPosition="1231" endWordPosition="1232">e probabilities calculated using nounverb co-occurrence pairs. Experiments using the Japanese BGH thesaurus showed that new words can be classified correctly with a maximum accuracy of more than 80%, while they did not report in detail whether the clusters captured polysemies. 3 Selectional Preferences A major approach on word clustering task is to use distribution of a word in a corpus, i.e., words are classified into classes based on their distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words (Hindle, 1990; Lin, 1998; Dagan, 1999). Like previous work on verb classification, we used subcategorization frame distributions with selectional preferences to calculate similarity between verbs (Schulte, 2008). We used the EDR dictionary of selectional preferences consisting of 5,269 basic Japanese verbs and the EDR concept dictionary (EDR, 1986). For selectional preferences, the dictionary has each concept of a verb, the group of possible co-occurrence surface-level case particles, the types of concept relation label that correspond to the surface-level case as well as the range of possible concepts tha</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun Classification from PredicateArgument Structures, In Proc. of 28th Annual Meeting of the Association for Computational Linguistics, pages 268–275.</rawString>
</citation>
<citation valid="false">
<note>GSK2007-D. http://www.gsk.or.jp/catalog/GSK2007-D/catalog.html</note>
<marker></marker>
<rawString>GSK2007-D. http://www.gsk.or.jp/catalog/GSK2007-D/catalog.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jannink</author>
<author>G Wiederhold</author>
</authors>
<title>Thesaurus Entry Extraction from an On-line Dictionary,</title>
<date>1999</date>
<booktitle>In Proc. of Fusion’99.</booktitle>
<marker>Jannink, Wiederhold, 1999</marker>
<rawString>J. Jannink and G. Wiederhold. 1999. Thesaurus Entry Extraction from an On-line Dictionary, In Proc. of Fusion’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Kelen</author>
<author>T Hutcheson</author>
</authors>
<title>Reducing the Time Complexity of the Fuzzy C-means Algorithm, In Trans.</title>
<date>2002</date>
<journal>of IEEE Fuzzy Systems,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>263--267</pages>
<marker>Kelen, Hutcheson, 2002</marker>
<rawString>J. F. Kelen and T. Hutcheson. 2002. Reducing the Time Complexity of the Fuzzy C-means Algorithm, In Trans. of IEEE Fuzzy Systems, 10(2), pages 263– 267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
</authors>
<title>On the Robustness of Entropy-based Similarity Measures in Evaluation of Subcategorization Acquisiton Systems.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning,</booktitle>
<pages>91--97</pages>
<marker>Korhonen, Krymolowski, 2002</marker>
<rawString>A. Korhonen and Y. Krymolowski. 2002. On the Robustness of Entropy-based Similarity Measures in Evaluation of Subcategorization Acquisiton Systems. In Proc. of the 6th Conference on Natural Language Learning, pages 91–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
<author>Z Marx</author>
</authors>
<title>Clustering Polysemic Subcategorization Frame Distributions Semantically.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="6175" citStr="Korhonen et al. (2003)" startWordPosition="945" endWordPosition="949">es, and reported that the EM is effective to produce associative relationships with a certain accuracy. More recent work in this direction is that of Schulte et al., (2008). They proposed a method for semantic verb classification based on verb frames with selectional preferences. They combined the EM training with the MDL principle. The MDL principle is used to induce WordNet-based selectional preferences for arguments within subcategorization frames. The results showed the effectiveness of the method. Our work is similar to their method in the use of verb frames with selectional preferences. Korhonen et al. (2003) used verb–frame pairs to cluster verbs into Levin-style semantic classes (Korhonen, 2003). They used the Information Bottleneck, and classified 110 test verbs into Levin-style classes. They had a focus on the interpretation of verbal polysemy as represented by the soft clusters: they interpreted polysemy as multiple-hard assignments. In the context of Japanese taxonomy of verbs and their classes, Utsuro et al. (1995) proposed a class-based method for sense classification of verbal polysemy in case frame acquisition from parallel corpora (Utsuro, 1995). A measure of bilingual class/class assoc</context>
</contexts>
<marker>Korhonen, Krymolowski, Marx, 2003</marker>
<rawString>A. Korhonen and Y. Krymolowski and Z. Marx. 2003. Clustering Polysemic Subcategorization Frame Distributions Semantically. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast Methods for Kernel-based Text Analysis.</title>
<date>2003</date>
<booktitle>In Proc. of 41th ACL,</booktitle>
<pages>24--31</pages>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T.Kudo and Y.Matsumoto. 2003. Fast Methods for Kernel-based Text Analysis. In Proc. of 41th ACL, pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measures of Distributional Similarity.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11594" citStr="Lee (1999)" startWordPosition="1824" endWordPosition="1825">at determines the inefficiency of assuming a model probability distribution given the true distribution. P(xi) P(xi) ∗ log . P(yi) where P(xi) =xi |x|. KL is not defined in case yi = 0. So, the probability distributions must be smoothed (Korhonen, 2002). We used two smoothing methods, i.e., Addone smoothing and Witten and Bell smoothing (Witten, 1991).2 Moreover, two variants of KL, α-skew divergence and the JensenShannon, were used to perform smoothing. 5. α-skew divergence (α div.): The α-skew divergence measure is a variant of KL, and is defined as: αdiv(x, y) = KL(y, α · x + (1 − α) · y). Lee (1999) reported the best results with α = 0.9. We used the same value. 6. The Jensen-Shannon (JS): The JensenShannon is a measure that relies on the assumption that if x and y are similar, they are close to their average. It is defined as: 2We report Add-one smoothing results in the evaluation, as it was better than Witten and Bell smoothing. KL(x, y) = �n i=1 34 1 2 ) + KL(y, x + y carried out through an iterative optiJS(x, y) = 2[KL(x, x + y 2 )]. mization (minimization) of the objective function Jm with the update of membership degree uij and the cluster centers cj. Jm is defined as: All measures</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measures of Distributional Similarity. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words,</title>
<date>1998</date>
<booktitle>In Proc. of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="7981" citStr="Lin, 1998" startWordPosition="1233" endWordPosition="1234">s calculated using nounverb co-occurrence pairs. Experiments using the Japanese BGH thesaurus showed that new words can be classified correctly with a maximum accuracy of more than 80%, while they did not report in detail whether the clusters captured polysemies. 3 Selectional Preferences A major approach on word clustering task is to use distribution of a word in a corpus, i.e., words are classified into classes based on their distributional similarity. Similarity measures based on distributional hypothesis compare a pair of weighted feature vectors that characterize two words (Hindle, 1990; Lin, 1998; Dagan, 1999). Like previous work on verb classification, we used subcategorization frame distributions with selectional preferences to calculate similarity between verbs (Schulte, 2008). We used the EDR dictionary of selectional preferences consisting of 5,269 basic Japanese verbs and the EDR concept dictionary (EDR, 1986). For selectional preferences, the dictionary has each concept of a verb, the group of possible co-occurrence surface-level case particles, the types of concept relation label that correspond to the surface-level case as well as the range of possible concepts that may fill </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic Retrieval and Clustering of Similar Words, In Proc. of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 768–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>T Sakaki</author>
<author>K Uchiyama</author>
<author>M Ishizuka</author>
</authors>
<title>Graph-based Word Clustering using a Web Search Engine,</title>
<date>2006</date>
<booktitle>In Proc. of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006),</booktitle>
<pages>542--550</pages>
<contexts>
<context position="4683" citStr="Matsuo et al. (2006)" startWordPosition="709" endWordPosition="712"> Work Graph-based algorithms have been widely used to classify semantically similar words (Jannink, 1999; Galley, 2003; Widdows, 2002; Muller, 2006). Sinha and Mihalcea proposed a graphbased algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality (Sinha, 2007). They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 - 8% as compared to the previsous work (Mihalcea, 2005). More recently, Matsuo et al. (2006) presented a method of word clustering based on Web counts using a search engine. They applied Newman clustering (Newman, 2004) for identifying word clusters. They reported that the results obtained by the algorithm were better than those obtained by average-link agglomerative clustering using 90 Japanese noun words. However, their method relied on hardclustering models, and thus have largely ignored the issue of polysemy that word belongs to more than one cluster. In contrast to hard-clustering algorithms, soft clustering allows that words to belong to more than one cluster. Much of the previ</context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Y. Matsuo and T. Sakaki and K. Uchiyama and M. Ishizuka. 2006. Graph-based Word Clustering using a Web Search Engine, In Proc. of 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP2006), pages 542–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised Large Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling,</title>
<date>2005</date>
<booktitle>In Proc. of the Human Language Technology /Empirical Methods in Natural Language Processing Conference,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="4646" citStr="Mihalcea, 2005" startWordPosition="705" endWordPosition="706">. c�2009 ACL and AFNLP 2 Related Work Graph-based algorithms have been widely used to classify semantically similar words (Jannink, 1999; Galley, 2003; Widdows, 2002; Muller, 2006). Sinha and Mihalcea proposed a graphbased algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality (Sinha, 2007). They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 - 8% as compared to the previsous work (Mihalcea, 2005). More recently, Matsuo et al. (2006) presented a method of word clustering based on Web counts using a search engine. They applied Newman clustering (Newman, 2004) for identifying word clusters. They reported that the results obtained by the algorithm were better than those obtained by average-link agglomerative clustering using 90 Japanese noun words. However, their method relied on hardclustering models, and thus have largely ignored the issue of polysemy that word belongs to more than one cluster. In contrast to hard-clustering algorithms, soft clustering allows that words to belong to mor</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>R. Mihalcea. 2005. Unsupervised Large Vocabulary Word Sense Disambiguation with Graph-based Algorithms for Sequence Data Labeling, In Proc. of the Human Language Technology /Empirical Methods in Natural Language Processing Conference, pages 411–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Muller</author>
<author>N Hathout</author>
<author>B Gaume</author>
</authors>
<title>Synonym Extraction Using a Semantic Distance on a Dictionary,</title>
<date>2006</date>
<booktitle>In Proc. of the Workshop on TextGraphs,</booktitle>
<pages>65--72</pages>
<marker>Muller, Hathout, Gaume, 2006</marker>
<rawString>P. Muller and N. Hathout and B. Gaume. 2006. Synonym Extraction Using a Semantic Distance on a Dictionary, In Proc. of the Workshop on TextGraphs, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>Fast Algorithm for Detecting Community Structure in Networks, Physical Review, E</title>
<date>2004</date>
<volume>69</volume>
<pages>066133</pages>
<contexts>
<context position="4810" citStr="Newman, 2004" startWordPosition="733" endWordPosition="735"> Muller, 2006). Sinha and Mihalcea proposed a graphbased algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality (Sinha, 2007). They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 - 8% as compared to the previsous work (Mihalcea, 2005). More recently, Matsuo et al. (2006) presented a method of word clustering based on Web counts using a search engine. They applied Newman clustering (Newman, 2004) for identifying word clusters. They reported that the results obtained by the algorithm were better than those obtained by average-link agglomerative clustering using 90 Japanese noun words. However, their method relied on hardclustering models, and thus have largely ignored the issue of polysemy that word belongs to more than one cluster. In contrast to hard-clustering algorithms, soft clustering allows that words to belong to more than one cluster. Much of the previous work on word classification with soft clustering is based on the EM algorithm (Pereira, 1993). Torisawa et al., (2002) pres</context>
<context position="12633" citStr="Newman, 2004" startWordPosition="2016" endWordPosition="2017">= 2[KL(x, x + y 2 )]. mization (minimization) of the objective function Jm with the update of membership degree uij and the cluster centers cj. Jm is defined as: All measures except Cos and rfCos showed that Jm = n k umij ||vi − cj ||2, smaller values indicate a closer relation between i=1 E two verbs. Thus, we used inverse of each value. j=1 5 Clustering Method The clustering algorithm used in this study was a graph-based unsupervised clustering reported by (Zhang, 2007). This algorithm detects overlapping nodes by the combination of a modularity function based on Newman Girvan’s Q function (Newman, 2004), spectral mapping that maps input nodes into Euclidean space, and fuzzy c-means clustering which allows node to belong to more than one cluster. They evaluated their method by applying several data including the American college football team network, and found that the algorithm successfully detected overlapping nodes. We thus used the algorithm to cluster verbs. Here are the key steps of the algorithm: Given a set of input verbs V = {v1, v2, · · · vn}, an upper bound K of the number of clusters, the adjacent matrix A = (aij)nxn of an input verbs and a threshold λ that can convert a soft ass</context>
</contexts>
<marker>Newman, 2004</marker>
<rawString>M.E.J.Newman. 2004. Fast Algorithm for Detecting Community Structure in Networks, Physical Review, E 2004, 69, 066133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Palla</author>
<author>I Der´enyi</author>
<author>I Farkas</author>
<author>T Vicsek</author>
</authors>
<date>2005</date>
<journal>Uncovering the Overlapping Community Structure of Complex Networks in Nature and Society, Nature.</journal>
<volume>435</volume>
<issue>7043</issue>
<pages>814--8</pages>
<marker>Palla, Der´enyi, Farkas, Vicsek, 2005</marker>
<rawString>G. Palla and I. Der´enyi and I. Farkas and T. Vicsek. 2005. Uncovering the Overlapping Community Structure of Complex Networks in Nature and Society, Nature. 435(7043), 814–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional Clustering of English Words.</title>
<date>1993</date>
<booktitle>In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira and N. Tishby and L. Lee. 1993. Distributional Clustering of English Words. In Proc. of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proc. of 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="4399" citStr="Resnik, 1995" startWordPosition="664" endWordPosition="665">did not have senses for the case filler which were used to create selectional preferences. 32 Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 32–40, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP 2 Related Work Graph-based algorithms have been widely used to classify semantically similar words (Jannink, 1999; Galley, 2003; Widdows, 2002; Muller, 2006). Sinha and Mihalcea proposed a graphbased algorithm for unsupervised word sense disambiguation which combines several semantic similarity measures including Resnik’s metric (Resnik, 1995), and algorithms for graph centrality (Sinha, 2007). They reported that the results using the SENSEVAL-2 and SENSEVAL-3 English all-words data sets lead to relative error rate reductions of 5 - 8% as compared to the previsous work (Mihalcea, 2005). More recently, Matsuo et al. (2006) presented a method of word clustering based on Web counts using a search engine. They applied Newman clustering (Newman, 2004) for identifying word clusters. They reported that the results obtained by the algorithm were better than those obtained by average-link agglomerative clustering using 90 Japanese noun word</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proc. of 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rooth</author>
</authors>
<title>Inducing a Semantically Annotated Lexicon via EM-Based Clustering,</title>
<date>1999</date>
<booktitle>In Proc. of 37th ACL,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="17289" citStr="Rooth, 1999" startWordPosition="2901" endWordPosition="2902"> class corresponds to a sense of each verb. There are 87 classes for a set from 2007, and 152 classes for a set from 1991 2007. The examples of the test verbs and their senses are shown in Table 2. For evaluation of verb classification, we used the precision, recall, and F-score, which were defined by (Schulte, 2000), especially to capture how many verbs does the algorithm actually detect more than just the predominant sense. For comparison against polysemies, we utilized the EM algorithm which is widely used as a soft clustering technique (Schulte, 2008). We followed the method presented in (Rooth, 1999). We used a probability distribution over verb frames with selectional preferences. The initial probabilities Table 3: Results for a set from 2007 Method m A C Prec Rec F FCM 2.0 0.09 74 .815 .483 .606 FCM(none) 1.5 0.07 74 .700 .477 .567 EM – – 87 .308 .903 .463 Table 4: Results against each measure Measure m A C Prec Rec F cos 3.0 0.02 74 .660 .517 .580 rfcos 2.0 0.04 74 .701 .488 .576 Ll 2.0 0.04 74 .680 .500 .576 KL 2.0 0.09 74 .815 .483 .606 α div. 2.0 0.04 74 .841 .471 .604 JS 1.5 0.03 74 .804 .483 .603 EM – – 87 .308 .903 .463 were often determined randomly. We set the initial probabili</context>
</contexts>
<marker>Rooth, 1999</marker>
<rawString>M. Rooth et al. 1999. Inducing a Semantically Annotated Lexicon via EM-Based Clustering, In Proc. of 37th ACL, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sinha</author>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised Graphbased Word Sense Disambiguation Using Measures of Word Semantic Similarity.</title>
<date>2007</date>
<booktitle>In Proc. of the IEEE International Conference on Semantic Computing,</booktitle>
<pages>46--54</pages>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>R. Sinha and R. Mihalcea. 2007. Unsupervised Graphbased Word Sense Disambiguation Using Measures of Word Semantic Similarity. In Proc. of the IEEE International Conference on Semantic Computing, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Clustering Verbs Semantically according to their Alternation Behaviour.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th COLING,</booktitle>
<pages>747--753</pages>
<marker>Walde, 2000</marker>
<rawString>S. Schulte im Walde. 2000. Clustering Verbs Semantically according to their Alternation Behaviour. In Proc. of the 18th COLING, pages 747–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Combining EM Training and the MDL Principle for an Automatic Verb Classification Incorporating Selectional Preferences.</title>
<date>2008</date>
<booktitle>In Proc. of the 46th ACL,</booktitle>
<pages>496--504</pages>
<marker>Walde, 2008</marker>
<rawString>S. Schulte im Walde et al. 2008. Combining EM Training and the MDL Principle for an Automatic Verb Classification Incorporating Selectional Preferences. In Proc. of the 46th ACL, pages 496–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tokunaga</author>
<author>A Fujii</author>
<author>M Iwayama</author>
<author>N Sakurai</author>
<author>H Tanaka</author>
</authors>
<title>Extending a thesaurus by classifying words.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL-EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources,</booktitle>
<pages>16--21</pages>
<contexts>
<context position="7135" citStr="Tokunaga et al. (1997)" startWordPosition="1097" endWordPosition="1100">n the context of Japanese taxonomy of verbs and their classes, Utsuro et al. (1995) proposed a class-based method for sense classification of verbal polysemy in case frame acquisition from parallel corpora (Utsuro, 1995). A measure of bilingual class/class association is introduced and used for discovering sense clusters in the sense distribution of English predicates and Japanese case element nouns. They used the test data consisting of 10 English and Japanese verbs taken from Roget’s Thesaurus and BGH (Bunrui Goi Hyo) (BGH, 1989). They reported 92.8% of the discovered clusters were correct. Tokunaga et al. (1997) presented a method for extending an existing thesaurus by classifying new words in terms of that thesaurus. New words are classified on the basis of relative probabilities of a word belonging to a given word class, with the probabilities calculated using nounverb co-occurrence pairs. Experiments using the Japanese BGH thesaurus showed that new words can be classified correctly with a maximum accuracy of more than 80%, while they did not report in detail whether the clusters captured polysemies. 3 Selectional Preferences A major approach on word clustering task is to use distribution of a word</context>
</contexts>
<marker>Tokunaga, Fujii, Iwayama, Sakurai, Tanaka, 1997</marker>
<rawString>T. Tokunaga and A. Fujii and M. Iwayama and N. Sakurai and H. Tanaka. 1997. Extending a thesaurus by classifying words. In Proc. of the ACL-EACL Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources, pages 16– 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
</authors>
<title>An Unsupervised Learning Method for Associative Relationships between Verb Phrases,</title>
<date>2002</date>
<booktitle>In Proc. of 19th International Conference on Computational Linguistics (COLING2002),</booktitle>
<pages>1009--1015</pages>
<marker>Torisawa, 2002</marker>
<rawString>K. Torisawa. 2002. An Unsupervised Learning Method for Associative Relationships between Verb Phrases, In Proc. of 19th International Conference on Computational Linguistics (COLING2002), pages 1009–1015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
</authors>
<title>Class-based sense classification of verbal polysemy in case frame acquisition from parallel corpora.</title>
<date>1995</date>
<booktitle>In Proc. of the 3rd Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>671--677</pages>
<contexts>
<context position="6733" citStr="Utsuro, 1995" startWordPosition="1035" endWordPosition="1036">es with selectional preferences. Korhonen et al. (2003) used verb–frame pairs to cluster verbs into Levin-style semantic classes (Korhonen, 2003). They used the Information Bottleneck, and classified 110 test verbs into Levin-style classes. They had a focus on the interpretation of verbal polysemy as represented by the soft clusters: they interpreted polysemy as multiple-hard assignments. In the context of Japanese taxonomy of verbs and their classes, Utsuro et al. (1995) proposed a class-based method for sense classification of verbal polysemy in case frame acquisition from parallel corpora (Utsuro, 1995). A measure of bilingual class/class association is introduced and used for discovering sense clusters in the sense distribution of English predicates and Japanese case element nouns. They used the test data consisting of 10 English and Japanese verbs taken from Roget’s Thesaurus and BGH (Bunrui Goi Hyo) (BGH, 1989). They reported 92.8% of the discovered clusters were correct. Tokunaga et al. (1997) presented a method for extending an existing thesaurus by classifying new words in terms of that thesaurus. New words are classified on the basis of relative probabilities of a word belonging to a </context>
</contexts>
<marker>Utsuro, 1995</marker>
<rawString>T. Utsuro. 1995. Class-based sense classification of verbal polysemy in case frame acquisition from parallel corpora. In Proc. of the 3rd Natural Language Processing Pacific Rim Symposium, pages 671–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of 19th International conference on Computational Linguistics (COLING2002),</booktitle>
<pages>1093--1099</pages>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A Graph Model for Unsupervised Lexical Acquisition. In Proc. of 19th International conference on Computational Linguistics (COLING2002), pages 1093–1099.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>T C Bell</author>
</authors>
<title>The ZeroFrequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>1085--1094</pages>
<marker>Witten, Bell, 1991</marker>
<rawString>I. H. Witten and T. C. Bell. 1991. The ZeroFrequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression. IEEE Transactions on Information Theory, 37(4), pages 1085–1094.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhang</author>
</authors>
<title>Identification of Overlapping Community Structure in Complex Networks using Fuzzy C-means Clustering.</title>
<date>2007</date>
<journal>PHYSICA A,</journal>
<volume>374</volume>
<pages>483--490</pages>
<contexts>
<context position="1796" citStr="Zhang, 2007" startWordPosition="274" endWordPosition="275">nses of the same word often hamper such attempts. In general, restriction of the subject domain makes the problem of polysemy less problematic. However, even in texts from a restricted domain such as economics or sports, one encounters quite a large number of polysemous words. Therefore, semantic classification of polysemies has been an interest since the earliest days when a number of large scale corpora have become available. In this paper, we focus on Japanese polysemous verbs, and present a method for polysemous verb classification. We used a graph-based unsupervised clustering algorithm (Zhang, 2007). The algorithm combines the idea of modularity funcFumiyo Fukumoto Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Japan fukumoto@yamanashi.ac.jp tion Q, spectral relaxation and fuzzy c-means clustering method to identify overlapping nodes with more than one cluster. The modularity function measures the quality of a cluster structure. Spectral mapping performs a dimensionality reduction which makes it possible to cluster in the very high dimensional spaces. The fuzzy c-means allows for the detection of nodes with more than one cluster. We applied the alg</context>
<context position="12496" citStr="Zhang, 2007" startWordPosition="1995" endWordPosition="1996">as it was better than Witten and Bell smoothing. KL(x, y) = �n i=1 34 1 2 ) + KL(y, x + y carried out through an iterative optiJS(x, y) = 2[KL(x, x + y 2 )]. mization (minimization) of the objective function Jm with the update of membership degree uij and the cluster centers cj. Jm is defined as: All measures except Cos and rfCos showed that Jm = n k umij ||vi − cj ||2, smaller values indicate a closer relation between i=1 E two verbs. Thus, we used inverse of each value. j=1 5 Clustering Method The clustering algorithm used in this study was a graph-based unsupervised clustering reported by (Zhang, 2007). This algorithm detects overlapping nodes by the combination of a modularity function based on Newman Girvan’s Q function (Newman, 2004), spectral mapping that maps input nodes into Euclidean space, and fuzzy c-means clustering which allows node to belong to more than one cluster. They evaluated their method by applying several data including the American college football team network, and found that the algorithm successfully detected overlapping nodes. We thus used the algorithm to cluster verbs. Here are the key steps of the algorithm: Given a set of input verbs V = {v1, v2, · · · vn}, an </context>
</contexts>
<marker>Zhang, 2007</marker>
<rawString>S. Zhang et al. 2007. Identification of Overlapping Community Structure in Complex Networks using Fuzzy C-means Clustering. PHYSICA A, 374, pages 483–490.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>