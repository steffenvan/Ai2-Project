<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000558">
<title confidence="0.99849">
Some Experiments on Indicators of
Parsing Complexity for Lexicalized Grammars
</title>
<author confidence="0.997968">
Anoop Sarkar, Fei Xia and Aravind Joshi
</author>
<affiliation confidence="0.99858">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.941502">
200 South 33rd Street,
Philadelphia, PA 19104-6389, USA
</address>
<email confidence="0.999566">
anoop,fxia,joshi @linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.997398" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763555555556">
In this paper, we identify syntactic lexical ambi-
guity and sentence complexity as factors that con-
tribute to parsing complexity in fully lexicalized
grammar formalisms such as Lexicalized Tree Ad-
joining Grammars. We also report on experiments
that explore the effects of these factors on parsing
complexity. We discuss how these constraints can
be exploited in improving efficiency of parsers for
such grammar formalisms.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998477631579">
The time taken by a parser to produce derivations
for input sentences is typically associated with the
length of those sentences. The longer the sentence,
the more time the parser is expected to take. How-
ever, complex algorithms like parsers are typically
affected by several factors. A common experience
is that parsing algorithms differ in the number of
edges inserted into the chart while parsing. In this
paper, we explore some of these constraints from
the perspective of lexicalized grammars and explore
how these constraints might be exploited to improve
parser efficiency.
We concentrate on the problem of parsing using
fully lexicalized grammars by looking at parsers for
Lexicalized Tree Adjoining Grammar (LTAG). By
a fully lexicalized grammar we mean a grammar
in which there are one or more syntactic structures
associated with each lexical item. In the case of
LTAG each structure is a tree (or, in general, a di-
rected acyclic graph). For each structure there is an
explicit structural slot for each of the arguments of
the lexical item. The various advantages of defin-
ing a lexicalized grammar formalism in this way are
discussed in (Joshi and Schabes,1991).
An example LTAG is shown in Figure 1. To
parse the sentence Ms. Haag plays Elianti the parser
has to combine the trees selected by each word in
the sentence by using the operations of substitution
and adjunction (the two composition operations in
LTAG) producing a valid derivation for the sen-
tence.
Notice that as a consequence of this kind of lexi-
calized grammatical description there might be sev-
eral different factors that affect parsing complex-
ity. Each word can select many different trees; for
example, the word plays in Figure 1 might select
several trees for each syntactic context in which it
can occur. The verb plays can be used in a rela-
tive clause, a wh-extraction clause, among others.
While grammatical notions of argument structure
and syntax can be processed in abstract terms just
as in other kinds of formalisms, the crucial differ-
ence in LTAG is that all of this information is com-
piled into a finite set of trees before parsing. Each
of these separate lexicalized trees is now considered
by the parser. This compilation is repeated for other
argument structures, e.g. the verb plays could also
select trees which are intransitive thus increasing the
set of lexicalized trees it can select. The set of trees
selected by different lexical items is what we term
in this paper as lexical syntactic ambiguity.
The importance of this compilation into a set
of lexicalized trees is that each predicate-argument
structure across each syntactic context has its own
lexicalized tree. Most grammar formalisms use
feature structures to capture the same grammatical
and predicate-argument information. In LTAG, this
larger set of lexicalized trees directly corresponds
to the fact that recursive feature structures are not
needed for linguistic description. Feature struc-
tures are typically atomic with a few instances of
re-entrant features.
Thus, in contrast with LTAG parsing, parsing
for formalisms like HPSG or LFG concentrates on
efficiently managing the unification of large fea-
ture structures and also the packing of ambiguities
when these feature structures subsume each other
(see (Oepen and Carroll, 2000) and references cited
there). We argue in this paper that the result of hav-
ing compiled out abstract grammatical descriptions
into a set of lexicalized trees allows us to predict the
number of edges that will be proposed by the parser
even before parsing begins. This allows us to ex-
plore novel methods of dealing with parsing com-
plexity that are difficult to consider in formalisms
that are not fully lexicalized.
</bodyText>
<page confidence="0.994825">
37
</page>
<figure confidence="0.949783285714285">
NNP
n
NP
u
NNP
m
NP
n
NP
u
sNP NNP@=4 1[Haag] m NNP@ NP*=2 1[Ms.]
NPu
NNPn
Su
VBZn
NP
arg
NP
arg
VPn
sNP NNP@=4 1[Elianti] sS NPs VBZ@ NPs=20 1[plays]
</figure>
<figureCaption confidence="0.99997">
Figure 1: Example lexicalized elementary trees. They are shown in the usual notation: anchor
</figureCaption>
<bodyText confidence="0.9960645">
substitution node footnode na null-adjunction constraint. These trees can be combined using sub-
stitution and adjunction to parse the sentence Ms. Haag plays Elianti.
</bodyText>
<subsectionHeader confidence="0.508718">
Number of trees selected
</subsectionHeader>
<bodyText confidence="0.999610428571428">
Furthermore, as the sentence length increases, the
number of lexicalized trees increase proportionally
increasing the attachment ambiguity. Each sentence
is composed of several clauses. In a lexicalized
grammar, each clause can be seen as headed by a
single predicate tree with its arguments and asso-
ciated adjuncts. We shall see that empirically the
number of clauses grow with increasing sentence
length only up to a certain point. For sentences
greater than a certain length the number of clauses
do not keep increasing.
Based on these intuitions we identify the follow-
ing factors that affect parsing complexity for lexi-
calized grammars:
Syntactic Lexical Ambiguity The number of trees
selected by the words in the sentence being
parsed. We show that this is a better indica-
tor of parsing time than sentence length. This
is also a predictor of the number of edges that
will be proposed by a parser, allowing us to
better handle difficult cases before parsing.
Sentence Complexity The clausal complexity in
the sentences to be parsed. We observe that the
number of clauses in a sentence stops grow-
ing in proportion to the sentence length after a
point. We show that before this point parsing
complexity is related to attachment of adjuncts
rather than attachment of arguments.
</bodyText>
<sectionHeader confidence="0.988755" genericHeader="method">
2 LTAG Treebank Grammar
</sectionHeader>
<bodyText confidence="0.999069666666667">
The grammar we used for our experiments was a
LTAG Treebank Grammar which was automatically
extracted from Sections 02â€“21 of the Wall Street
Journal Penn Treebank II corpus (Marcus et al.,
1993). The extraction tool (Xia, 1999) converted the
derived trees of the Treebank into derivation trees in
LTAG which represent the attachments of lexical-
ized elementary trees. There are tree templates
in the grammar with tree nodes. Each word
in the corpus selects some set of tree templates. The
total number of lexicalized trees is . The
total number of word types in the lexicon is .
The average number of trees per word type is .
However, this average is misleading since it does
not consider the frequency with which words that
select a large number of trees occur in the corpus.
In Figure 2 we see that many frequently seen words
can select a large number of trees.
</bodyText>
<figure confidence="0.987089090909091">
400
350
300
250
200
150
100
50
0
0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000
Word frequency
</figure>
<figureCaption confidence="0.9833784">
Figure 2: Number of trees selected plotted against
words with a particular frequency. (x-axis: words of
frequency ; y-axis: number of trees selected, error
bars indicate least and most ambiguous word of a
particular frequency )
</figureCaption>
<bodyText confidence="0.921366">
Another objection that can be raised against a
</bodyText>
<page confidence="0.978331">
38
</page>
<bodyText confidence="0.981440772727273">
log(time) in seconds
Treebank grammar which has been automatically
extracted is that any parsing results using such a
grammar might not be indicative of parsing us-
ing a hand-crafted linguistically sophisticated gram-
mar. To address this point (Xia and Palmer, 2000),
compares this Treebank grammar with the XTAG
grammar (XTAG-Group, 1998), a large-scale hand-
crafted LTAG grammar for English. The experiment
shows that 82.1% of template tokens in the Tree-
bank grammar matches with a corresponding tem-
plate in the XTAG grammar; 14.0% are covered by
the XTAG grammar but the templates in two gram-
mars look different because the Treebank and the
XTAG grammar have adopted different analyses for
the corresponding constructions; 1.1% of template
tokens in the Treebank grammar are not linguisti-
cally sound due to annotation errors in the original
Treebank; and the remaining 2.8% are not currently
covered by the XTAG grammar. Thus, a total of
96.1% of the structures in the Treebank grammar
match up with structures in the XTAG grammar.
</bodyText>
<sectionHeader confidence="0.983883" genericHeader="method">
3 Syntactic Lexical Ambiguity
</sectionHeader>
<bodyText confidence="0.9998342">
In a fully lexicalized grammar such as LTAG the
combinations of trees (by substitution and adjunc-
tion) can be thought of as attachments. It is this per-
spective that allows us to define the parsing problem
in two steps (Joshi and Schabes, 1991):
</bodyText>
<listItem confidence="0.974985">
1. Assigning a set of lexicalized structures to each
word in the input sentence.
2. Finding the correct attachments between these
structures to get all parses for the sentence.
</listItem>
<bodyText confidence="0.999827611111111">
In this section we will try to find which of these
factors determines parsing complexity when finding
all parses in an LTAG parser.
To test the performance of LTAG parsing on a
realistic corpus using a large grammar (described
above) we parsed sentences from the Wall
Street Journal using the lexicalized grammar de-
scribed in Section 2.1 All of these sentences were
of length words or less. These sentences were
taken from the same sections (02-21) of the Tree-
bank from which the original grammar was ex-
tracted. This was done to avoid the complication
of using default rules for unknown words.
In all of the experiments reported here, the parser
produces all parses for each sentence. It produces
a shared derivation forest for each sentence which
stores, in compact form, all derivations for each sen-
tence.
</bodyText>
<footnote confidence="0.7504315">
1Some of these results appear in (Sarkar, 2000). In this sec-
tion we present some additional data on the previous results and
also the results of some new experiments that do not appear in
the earlier work.
</footnote>
<bodyText confidence="0.99942904">
We found that the observed complexity of pars-
ing for LTAG is dominated by factors other than
sentence length.2 Figure 3 shows the time taken
in seconds by the parser plotted against sentence
length. We see a great deal of variation in timing
for the same sentence length, especially for longer
sentences.
We wanted to find the relevant variable other than
sentence length which would be the right predictor
of parsing time complexity. There can be a large
variation in syntactic lexical ambiguity which might
be a relevant factor in parsing time complexity. To
draw this out, in Figure 4 we plotted the number of
trees selected by a sentence against the time taken
to parse that sentence. By examining this graph we
can visually infer that the number of trees selected is
a better predictor of increase in parsing complexity
than sentence length. We can also compare numer-
ically the two hypotheses by computing the coeffi-
cient of determination ( ) for the two graphs. We
get a value of for Figure 3 and a value of
for Figure 4. Thus, we infer that it is the syn-
tactic lexical ambiguity of the words in the sentence
which is the major contributor to parsing time com-
plexity.
</bodyText>
<figure confidence="0.996979846153846">
10
9
8
7
6
5
4
3
2
1
0
2 4 6 8 10 12 14 16 18 20
Sentence length
</figure>
<figureCaption confidence="0.999765">
Figure 3: Parse times plotted against sentence
</figureCaption>
<bodyText confidence="0.797058923076923">
length. Coefficient of determination: .
(x-axis: Sentence length; y-axis: log(time in sec-
onds))
Since we can easily determine the number of
trees selected by a sentence before we start parsing,
we can use this number to predict the number of
edges that will be proposed by a parser when pars-
ing this sentence, allowing us to better handle diffi-
cult cases before parsing.
2Note that the precise number of edges proposed by the
parser and other common indicators of complexity can be ob-
tained only while or after parsing. We are interested in predict-
ing parsing complexity.
</bodyText>
<page confidence="0.992724">
39
</page>
<figure confidence="0.894514">
log(Time taken in secs)
0 200 400 600 800 1000
Total num of trees selected by a sentence
</figure>
<figureCaption confidence="0.9954005">
Figure 4: The impact of syntactic lexical ambigu-
ity on parsing times. Log of the time taken to parse
a sentence plotted against the total number of trees
selected by the sentence. Coefficient of determina-
</figureCaption>
<bodyText confidence="0.959031770833333">
tion: . (x-axis: Total number of trees se-
lected by a sentence; y-axis: log(time) in seconds).
We test the above hypothesis further by parsing
the same set of sentences as above but this time us-
ing an oracle which tells us the correct elementary
lexicalized structure for each word in the sentence.
This eliminates lexical syntactic ambiguity but does
not eliminate attachment ambiguity for the parser.
The graph comparing the parsing times is shown in
Figure 5. As the comparison shows, the elimina-
tion of lexical ambiguity leads to a drastic increase
in parsing efficiency. The total time taken to parse
all sentences went from 548K seconds to 31.2
seconds.
Figure 5 shows us that a model which disam-
biguates syntactic lexical ambiguity can potentially
be extremely useful in terms of parsing efficiency.
Thus disambiguation of tree assignment or Su-
perTagging (Srinivas, 1997) of a sentence before
parsing it might be a way of improving parsing ef-
ficiency. This gives us a way to reduce the pars-
ing complexity for precisely the sentences which
were problematic: the ones which selected too many
trees. To test whether parsing times are reduced af-
ter SuperTagging we conducted an experiment in
which the output of an -best SuperTagger was
taken as input to the parser. In our experiment we
set to be .3 The time taken to parse the same set
of sentences was again dramatically reduced (the to-
tal time taken was 21K seconds). However, the dis-
advantage of this method was that the coverage of
3(Chen et al., 1999) shows that to get greater than 97% ac-
curacy using SuperTagging the value of must be quite high
( ). They use a different set of SuperTags and so we used
their result simply to get an approximate estimate of the value
of .
the parser was reduced: 926 sentences (out of the
2250) did not get any parse. This was because some
crucial tree was missing in the -best output. The
results are graphed in Figure 6. The total number of
derivations for all sentences went down to 1.01e+10
(the original total number was 1.4e+18) indicating
(not surprisingly) that some attachment ambiguities
persist although the number of trees are reduced.
We are experimenting with techniques where the
output of the -best SuperTagger is combined with
other pieces of evidence to improve the coverage of
the parser while retaining the speedup.
</bodyText>
<figure confidence="0.999133307692308">
0
-0.5
-1
-1.5
-2
-2.5
-3
-3.5
-4
-4.5
-5
0 5 10 15 20
Sentence length
</figure>
<figureCaption confidence="0.9893405">
Figure 5: Parse times when the parser gets the cor-
rect tree for each word in the sentence (eliminating
any syntactic lexical ambiguity). The parsing times
for all the sentences for all lengths never goes
above second. (x-axis: Sentence length; y-axis:
log(time) in seconds)
</figureCaption>
<figure confidence="0.9969729">
8
6
4
log(Time in secs) 2
0
-2
-4
-6
0 5 10 15 20 25
Sentence length
</figure>
<figureCaption confidence="0.999753">
Figure 6: Time taken by the parser after -best Su-
</figureCaption>
<bodyText confidence="0.669203">
perTagging ( ). (x-axis: Sentence length; y-
axis: log(time) in seconds)
</bodyText>
<sectionHeader confidence="0.986298" genericHeader="method">
4 Sentence Complexity
</sectionHeader>
<bodyText confidence="0.9986025">
There are many ways of describing sentence com-
plexity, which are not necessarily independent of
</bodyText>
<figure confidence="0.994153454545455">
log(Time taken) in seconds 10
9
8
7
6
5
4
3
2
1
0
</figure>
<page confidence="0.994288">
40
</page>
<bodyText confidence="0.997851943396226">
each other. In the context of lexicalized tree-
adjoining grammar (and in other lexical frame-
works, perhaps with some modifications) the com-
plexity of syntactic and semantic processing is re-
lated to the number of predicate-argument structures
being computed for a given sentence.
In this section, we explore the possibility of char-
acterizing sentence complexity in terms of the num-
ber of clauses which is used as an approximation to
the number of predicate-argument structures to be
found in a sentence.
The number of clauses of a given sentence in
the Penn Treebank is counted using the bracketing
tags. The count is computed to be the number of
S/SIMV/SQ/RRC nodes which have a VP child or a
child with -PRD function tag. In principle number
of clauses can grow continuously as the sentence
length increases. However it is interesting to note
that 99.1% of sentences in the Penn Treebank con-
tain 6 or fewer clauses.
Figure 7 shows the average number of clauses
plotted against sentence length. For sentences with
no more than 50 words, which accounts for 98.2%
of the corpus, we see a linear increase in the av-
erage number of clauses with respect to sentence
length. But from that point on, increasing the sen-
tence length does not lead to a proportional increase
in the number of clauses. Thus, empirically, the
number of clauses is bounded by a constant. For
some very long sentences, the number of clauses
actually decreases because these sentences include
long but flat coordinated phrases.
Figure 8 shows the standard deviation of the
clause number plotted against sentence length.
There is an increase in deviation for sentences
longer than 50 words. This is due to two reasons:
first, quite often, long sentences either have many
embedded clauses or are flat with long coordinated
phrases; second, the data become sparse as the sen-
tence length grows, resulting in high deviation.4
In Figure 9 and Figure 10 we show how parsing
time varies as a function of the number of clauses
present in the sentence being parsed. The figures
are analogous to the earlier graphs relating parsing
time with other factors (see Figure 3 and Figure 4).
Surprisingly, in both graphs we see that when the
number of clauses is small (in this case less than
), an increase in the number of clauses has no ef-
fect on the parsing complexity. Even when the num-
ber of clauses is we find the same pattern of time
complexity that we have seen in the earlier graphs
when we ignored clause complexity. Thus, when
the number of clauses is small parsing complexity
</bodyText>
<footnote confidence="0.988235">
4For some sentence lengths (e.g., length ), there is
</footnote>
<tableCaption confidence="0.5102965">
only one sentence with that length in the whole corpus, result-
ing in zero deviation.
</tableCaption>
<figure confidence="0.974271">
14
12
Average number of clauses in the sentences 10
8
6
4
2
0
0 50 100 150 200 250
Sentence length
</figure>
<figureCaption confidence="0.85728">
Figure 7: Average number of clause plotted against
sentence length
</figureCaption>
<figure confidence="0.9908825">
0 50 100 150 200 250
Sentence length
</figure>
<figureCaption confidence="0.981358">
Figure 8: Standard deviation of clause number plot-
ted against sentence length
</figureCaption>
<bodyText confidence="0.999260842105263">
is related to attachment of adjuncts rather than argu-
ments. It would be interesting to continue increas-
ing the number of clauses and the sentence length
and then compare the differences in parsing times.5
We have seen that beyond a certain sentence
length, the number of clauses do not increase pro-
portionally. We conjecture that a parser can ex-
ploit this observed constraint on clause complexity
in sentences to improve its efficiency. In a way sim-
ilar to methods that account for low attachment of
adjuncts while parsing, we can introduce constraints
on how many clauses a particular node can domi-
nate in a parse. By making the parser sensitive to
this measure, we can prune out unlikely derivations
previously considered to be plausible by the parser.
There is also an independent reason for pursuing
this measure of clausal complexity. It can be ex-
tended to a notion of syntactic and semantic com-
plexity as they relate to both the representational
</bodyText>
<footnote confidence="0.7103295">
5We plan to conduct this experiment and present the results
during the workshop.
</footnote>
<figure confidence="0.9833126">
4
3.5
Standard deviation of clause number
2
0.5
0
3
2.5
1.5
1
</figure>
<page confidence="0.943451">
41
</page>
<figureCaption confidence="0.855760166666667">
Figure 9: Variation in times for parsing plotted
against length of each sentence while identifying the
number of clauses.
Figure 10: Variation in times for parsing plotted
against the number of trees selected by each sen-
tence while identifying the number of clauses.
</figureCaption>
<bodyText confidence="0.99987125">
and processing aspects (Joshi, 2000). The empirical
study of clausal complexity described in this section
might shed some light on the general issue of syn-
tactic and semantic complexity.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999964448275862">
In this paper, we identified syntactic lexical ambi-
guity and sentence complexity as factors that con-
tribute to parsing complexity in fully lexicalized
grammars.
We showed that lexical syntactic ambiguity has a
strong effect on parsing time and that a model which
disambiguates syntactic lexical ambiguity can po-
tentially be extremely useful in terms of parsing ef-
ficiency. By assigning each word in the sentence
with the correct elementary tree showed that parsing
times were reduced by several orders of magnitude
(the total time taken to parse sentences went
from 548K seconds to 31.2 seconds).
We conducted an experiment in which the out-
put of an -best SuperTagger was taken as input to
the parser. The time taken to parse the same set of
sentences was again dramatically reduced (the total
time taken was 21K seconds). The disadvantage of
this approach was that 926 out of the original 2250
sentences did not get any parse.
We showed that even as sentence length increases
the number of clauses is empirically bounded by a
constant. The number of clauses in 99.1% of sen-
tences in the Penn Treebank was bounded by 6. We
discussed how this finding affects parsing efficiency
and showed that for when the number of clauses
is smaller than , parsing efficiency is dominated
by adjunct attachments rather than argument attach-
ments.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999662075">
John Chen, Srinivas Bangalore, and K. Vijay-Shanker.
1999. New models for improving supertag disam-
biguation. In Proceedings of the 9th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, Bergen, Norway.
A. Joshi and Y. Schabes. 1991. Tree adjoining grammars
and lexicalized grammars. In M. Nivat and A. Podel-
ski, editors, Tree automata and languages. North-
Holland.
Aravind K. Joshi. 2000. Some aspects of syntactic
and semantic complexity and underspecification. Talk
given at Syntactic and Semantic Complexity in Natural
Language Processing Systems, Workshop at ANLP-
NAACL 2000, Seattle, May.
M. Marcus, B. Santorini, and M. Marcinkiewiecz. 1993.
Building a large annotated corpus of english. Compu-
tational Linguistics, 19(2):313â€“330.
Stephan Oepen and John Carroll. 2000. Ambiguity
packing in constraint-based parsing â€“ practical results.
In Proceedings of the 1st Meeting of the North Amer-
ican ACL, NAACL-2000, Seattle, Washington, Apr 29
â€“ May 4.
Anoop Sarkar. 2000. Practical experiments in parsing
using tree adjoining grammars. In Proceedings of the
Fifth Workshop on Tree Adjoining Grammars, Paris,
France, May 25â€“27.
The XTAG-Group. 1998. A Lexicalized Tree Adjoining
Grammar for English. Technical Report IRCS 98-18,
University of Pennsylvania.
B. Srinivas. 1997. Performance Evaluation of Supertag-
ging for Partial Parsing. In Proceedings of Fifth In-
ternational Workshop on Parsing Technology, Boston,
USA, September.
Fei Xia and Martha Palmer. 2000. Evaluating the Cover-
age of LTAGs on Annotated Corpora. In Proceedings
of LREC satellite workshop Using Evaluation within
HLT Programs: Results and Trends.
Fei Xia. 1999. Extracting tree adjoining grammars from
bracketed corpora. In Proc. of NLPRS-99, Beijing,
China.
</reference>
<figure confidence="0.997693268292683">
log(Time taken in secs)
10
9
8
7
6
5
4
3
2
1
0
20
15
1 1.5 10
2
2.5 3
3.5
Num of clauses 4 4.5
5
5
Sentence length
log(Time taken in secs)
10
9
8
7
6
5
4
3
2
1
0
1000
1 1.5
500
2 2.5 Num of trees selected
3 3.5
Num of clauses 4 4.5
5
</figure>
<page confidence="0.981892">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.901375">
<title confidence="0.9995355">Some Experiments on Indicators Parsing Complexity for Lexicalized Grammars</title>
<author confidence="0.932737">Anoop Sarkar</author>
<author confidence="0.932737">Fei Xia</author>
<author confidence="0.932737">Aravind</author>
<affiliation confidence="0.9991785">Dept. of Computer and Information University of</affiliation>
<address confidence="0.9905665">200 South 33rd Philadelphia, PA 19104-6389,</address>
<email confidence="0.997381">anoop,fxia,joshi@linc.cis.upenn.edu</email>
<abstract confidence="0.9987606">In this paper, we identify syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammar formalisms such as Lexicalized Tree Adjoining Grammars. We also report on experiments that explore the effects of these factors on parsing complexity. We discuss how these constraints can be exploited in improving efficiency of parsers for such grammar formalisms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Srinivas Bangalore</author>
<author>K Vijay-Shanker</author>
</authors>
<title>New models for improving supertag disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="13593" citStr="Chen et al., 1999" startWordPosition="2275" endWordPosition="2278">before parsing it might be a way of improving parsing efficiency. This gives us a way to reduce the parsing complexity for precisely the sentences which were problematic: the ones which selected too many trees. To test whether parsing times are reduced after SuperTagging we conducted an experiment in which the output of an -best SuperTagger was taken as input to the parser. In our experiment we set to be .3 The time taken to parse the same set of sentences was again dramatically reduced (the total time taken was 21K seconds). However, the disadvantage of this method was that the coverage of 3(Chen et al., 1999) shows that to get greater than 97% accuracy using SuperTagging the value of must be quite high ( ). They use a different set of SuperTags and so we used their result simply to get an approximate estimate of the value of . the parser was reduced: 926 sentences (out of the 2250) did not get any parse. This was because some crucial tree was missing in the -best output. The results are graphed in Figure 6. The total number of derivations for all sentences went down to 1.01e+10 (the original total number was 1.4e+18) indicating (not surprisingly) that some attachment ambiguities persist although t</context>
</contexts>
<marker>Chen, Bangalore, Vijay-Shanker, 1999</marker>
<rawString>John Chen, Srinivas Bangalore, and K. Vijay-Shanker. 1999. New models for improving supertag disambiguation. In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree adjoining grammars and lexicalized grammars.</title>
<date>1991</date>
<editor>In M. Nivat and A. Podelski, editors,</editor>
<publisher>NorthHolland.</publisher>
<contexts>
<context position="8736" citStr="Joshi and Schabes, 1991" startWordPosition="1425" endWordPosition="1428">yses for the corresponding constructions; 1.1% of template tokens in the Treebank grammar are not linguistically sound due to annotation errors in the original Treebank; and the remaining 2.8% are not currently covered by the XTAG grammar. Thus, a total of 96.1% of the structures in the Treebank grammar match up with structures in the XTAG grammar. 3 Syntactic Lexical Ambiguity In a fully lexicalized grammar such as LTAG the combinations of trees (by substitution and adjunction) can be thought of as attachments. It is this perspective that allows us to define the parsing problem in two steps (Joshi and Schabes, 1991): 1. Assigning a set of lexicalized structures to each word in the input sentence. 2. Finding the correct attachments between these structures to get all parses for the sentence. In this section we will try to find which of these factors determines parsing complexity when finding all parses in an LTAG parser. To test the performance of LTAG parsing on a realistic corpus using a large grammar (described above) we parsed sentences from the Wall Street Journal using the lexicalized grammar described in Section 2.1 All of these sentences were of length words or less. These sentences were taken fro</context>
</contexts>
<marker>Joshi, Schabes, 1991</marker>
<rawString>A. Joshi and Y. Schabes. 1991. Tree adjoining grammars and lexicalized grammars. In M. Nivat and A. Podelski, editors, Tree automata and languages. NorthHolland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Some aspects of syntactic and semantic complexity and underspecification.</title>
<date>2000</date>
<booktitle>Talk given at Syntactic and Semantic Complexity in Natural Language Processing Systems, Workshop at ANLPNAACL</booktitle>
<location>Seattle,</location>
<contexts>
<context position="19475" citStr="Joshi, 2000" startWordPosition="3304" endWordPosition="3305">n for pursuing this measure of clausal complexity. It can be extended to a notion of syntactic and semantic complexity as they relate to both the representational 5We plan to conduct this experiment and present the results during the workshop. 4 3.5 Standard deviation of clause number 2 0.5 0 3 2.5 1.5 1 41 Figure 9: Variation in times for parsing plotted against length of each sentence while identifying the number of clauses. Figure 10: Variation in times for parsing plotted against the number of trees selected by each sentence while identifying the number of clauses. and processing aspects (Joshi, 2000). The empirical study of clausal complexity described in this section might shed some light on the general issue of syntactic and semantic complexity. 5 Conclusion In this paper, we identified syntactic lexical ambiguity and sentence complexity as factors that contribute to parsing complexity in fully lexicalized grammars. We showed that lexical syntactic ambiguity has a strong effect on parsing time and that a model which disambiguates syntactic lexical ambiguity can potentially be extremely useful in terms of parsing efficiency. By assigning each word in the sentence with the correct element</context>
</contexts>
<marker>Joshi, 2000</marker>
<rawString>Aravind K. Joshi. 2000. Some aspects of syntactic and semantic complexity and underspecification. Talk given at Syntactic and Semantic Complexity in Natural Language Processing Systems, Workshop at ANLPNAACL 2000, Seattle, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewiecz</author>
</authors>
<title>Building a large annotated corpus of english.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="6375" citStr="Marcus et al., 1993" startWordPosition="1026" endWordPosition="1029">be proposed by a parser, allowing us to better handle difficult cases before parsing. Sentence Complexity The clausal complexity in the sentences to be parsed. We observe that the number of clauses in a sentence stops growing in proportion to the sentence length after a point. We show that before this point parsing complexity is related to attachment of adjuncts rather than attachment of arguments. 2 LTAG Treebank Grammar The grammar we used for our experiments was a LTAG Treebank Grammar which was automatically extracted from Sections 02â€“21 of the Wall Street Journal Penn Treebank II corpus (Marcus et al., 1993). The extraction tool (Xia, 1999) converted the derived trees of the Treebank into derivation trees in LTAG which represent the attachments of lexicalized elementary trees. There are tree templates in the grammar with tree nodes. Each word in the corpus selects some set of tree templates. The total number of lexicalized trees is . The total number of word types in the lexicon is . The average number of trees per word type is . However, this average is misleading since it does not consider the frequency with which words that select a large number of trees occur in the corpus. In Figure 2 we see</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewiecz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewiecz. 1993. Building a large annotated corpus of english. Computational Linguistics, 19(2):313â€“330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing â€“ practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American ACL, NAACL-2000,</booktitle>
<volume>29</volume>
<location>Seattle, Washington,</location>
<contexts>
<context position="4025" citStr="Oepen and Carroll, 2000" startWordPosition="633" endWordPosition="636">grammar formalisms use feature structures to capture the same grammatical and predicate-argument information. In LTAG, this larger set of lexicalized trees directly corresponds to the fact that recursive feature structures are not needed for linguistic description. Feature structures are typically atomic with a few instances of re-entrant features. Thus, in contrast with LTAG parsing, parsing for formalisms like HPSG or LFG concentrates on efficiently managing the unification of large feature structures and also the packing of ambiguities when these feature structures subsume each other (see (Oepen and Carroll, 2000) and references cited there). We argue in this paper that the result of having compiled out abstract grammatical descriptions into a set of lexicalized trees allows us to predict the number of edges that will be proposed by the parser even before parsing begins. This allows us to explore novel methods of dealing with parsing complexity that are difficult to consider in formalisms that are not fully lexicalized. 37 NNP n NP u NNP m NP n NP u sNP NNP@=4 1[Haag] m NNP@ NP*=2 1[Ms.] NPu NNPn Su VBZn NP arg NP arg VPn sNP NNP@=4 1[Elianti] sS NPs VBZ@ NPs=20 1[plays] Figure 1: Example lexicalized e</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Stephan Oepen and John Carroll. 2000. Ambiguity packing in constraint-based parsing â€“ practical results. In Proceedings of the 1st Meeting of the North American ACL, NAACL-2000, Seattle, Washington, Apr 29 â€“ May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Practical experiments in parsing using tree adjoining grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth Workshop on Tree Adjoining Grammars,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="9770" citStr="Sarkar, 2000" startWordPosition="1601" endWordPosition="1602">d sentences from the Wall Street Journal using the lexicalized grammar described in Section 2.1 All of these sentences were of length words or less. These sentences were taken from the same sections (02-21) of the Treebank from which the original grammar was extracted. This was done to avoid the complication of using default rules for unknown words. In all of the experiments reported here, the parser produces all parses for each sentence. It produces a shared derivation forest for each sentence which stores, in compact form, all derivations for each sentence. 1Some of these results appear in (Sarkar, 2000). In this section we present some additional data on the previous results and also the results of some new experiments that do not appear in the earlier work. We found that the observed complexity of parsing for LTAG is dominated by factors other than sentence length.2 Figure 3 shows the time taken in seconds by the parser plotted against sentence length. We see a great deal of variation in timing for the same sentence length, especially for longer sentences. We wanted to find the relevant variable other than sentence length which would be the right predictor of parsing time complexity. There </context>
</contexts>
<marker>Sarkar, 2000</marker>
<rawString>Anoop Sarkar. 2000. Practical experiments in parsing using tree adjoining grammars. In Proceedings of the Fifth Workshop on Tree Adjoining Grammars, Paris, France, May 25â€“27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>The XTAG-Group</author>
</authors>
<title>A Lexicalized Tree Adjoining Grammar for English.</title>
<date>1998</date>
<tech>Technical Report IRCS 98-18,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7768" citStr="XTAG-Group, 1998" startWordPosition="1265" endWordPosition="1266">y Figure 2: Number of trees selected plotted against words with a particular frequency. (x-axis: words of frequency ; y-axis: number of trees selected, error bars indicate least and most ambiguous word of a particular frequency ) Another objection that can be raised against a 38 log(time) in seconds Treebank grammar which has been automatically extracted is that any parsing results using such a grammar might not be indicative of parsing using a hand-crafted linguistically sophisticated grammar. To address this point (Xia and Palmer, 2000), compares this Treebank grammar with the XTAG grammar (XTAG-Group, 1998), a large-scale handcrafted LTAG grammar for English. The experiment shows that 82.1% of template tokens in the Treebank grammar matches with a corresponding template in the XTAG grammar; 14.0% are covered by the XTAG grammar but the templates in two grammars look different because the Treebank and the XTAG grammar have adopted different analyses for the corresponding constructions; 1.1% of template tokens in the Treebank grammar are not linguistically sound due to annotation errors in the original Treebank; and the remaining 2.8% are not currently covered by the XTAG grammar. Thus, a total of</context>
</contexts>
<marker>XTAG-Group, 1998</marker>
<rawString>The XTAG-Group. 1998. A Lexicalized Tree Adjoining Grammar for English. Technical Report IRCS 98-18, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Performance Evaluation of Supertagging for Partial Parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of Fifth International Workshop on Parsing Technology,</booktitle>
<location>Boston, USA,</location>
<contexts>
<context position="12960" citStr="Srinivas, 1997" startWordPosition="2162" endWordPosition="2163">ture for each word in the sentence. This eliminates lexical syntactic ambiguity but does not eliminate attachment ambiguity for the parser. The graph comparing the parsing times is shown in Figure 5. As the comparison shows, the elimination of lexical ambiguity leads to a drastic increase in parsing efficiency. The total time taken to parse all sentences went from 548K seconds to 31.2 seconds. Figure 5 shows us that a model which disambiguates syntactic lexical ambiguity can potentially be extremely useful in terms of parsing efficiency. Thus disambiguation of tree assignment or SuperTagging (Srinivas, 1997) of a sentence before parsing it might be a way of improving parsing efficiency. This gives us a way to reduce the parsing complexity for precisely the sentences which were problematic: the ones which selected too many trees. To test whether parsing times are reduced after SuperTagging we conducted an experiment in which the output of an -best SuperTagger was taken as input to the parser. In our experiment we set to be .3 The time taken to parse the same set of sentences was again dramatically reduced (the total time taken was 21K seconds). However, the disadvantage of this method was that the</context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>B. Srinivas. 1997. Performance Evaluation of Supertagging for Partial Parsing. In Proceedings of Fifth International Workshop on Parsing Technology, Boston, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Evaluating the Coverage of LTAGs on Annotated Corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC satellite workshop Using Evaluation within HLT Programs: Results and Trends.</booktitle>
<contexts>
<context position="7695" citStr="Xia and Palmer, 2000" startWordPosition="1253" endWordPosition="1256"> 0 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 Word frequency Figure 2: Number of trees selected plotted against words with a particular frequency. (x-axis: words of frequency ; y-axis: number of trees selected, error bars indicate least and most ambiguous word of a particular frequency ) Another objection that can be raised against a 38 log(time) in seconds Treebank grammar which has been automatically extracted is that any parsing results using such a grammar might not be indicative of parsing using a hand-crafted linguistically sophisticated grammar. To address this point (Xia and Palmer, 2000), compares this Treebank grammar with the XTAG grammar (XTAG-Group, 1998), a large-scale handcrafted LTAG grammar for English. The experiment shows that 82.1% of template tokens in the Treebank grammar matches with a corresponding template in the XTAG grammar; 14.0% are covered by the XTAG grammar but the templates in two grammars look different because the Treebank and the XTAG grammar have adopted different analyses for the corresponding constructions; 1.1% of template tokens in the Treebank grammar are not linguistically sound due to annotation errors in the original Treebank; and the remai</context>
</contexts>
<marker>Xia, Palmer, 2000</marker>
<rawString>Fei Xia and Martha Palmer. 2000. Evaluating the Coverage of LTAGs on Annotated Corpora. In Proceedings of LREC satellite workshop Using Evaluation within HLT Programs: Results and Trends.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting tree adjoining grammars from bracketed corpora.</title>
<date>1999</date>
<booktitle>In Proc. of NLPRS-99,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="6408" citStr="Xia, 1999" startWordPosition="1033" endWordPosition="1034">ter handle difficult cases before parsing. Sentence Complexity The clausal complexity in the sentences to be parsed. We observe that the number of clauses in a sentence stops growing in proportion to the sentence length after a point. We show that before this point parsing complexity is related to attachment of adjuncts rather than attachment of arguments. 2 LTAG Treebank Grammar The grammar we used for our experiments was a LTAG Treebank Grammar which was automatically extracted from Sections 02â€“21 of the Wall Street Journal Penn Treebank II corpus (Marcus et al., 1993). The extraction tool (Xia, 1999) converted the derived trees of the Treebank into derivation trees in LTAG which represent the attachments of lexicalized elementary trees. There are tree templates in the grammar with tree nodes. Each word in the corpus selects some set of tree templates. The total number of lexicalized trees is . The total number of word types in the lexicon is . The average number of trees per word type is . However, this average is misleading since it does not consider the frequency with which words that select a large number of trees occur in the corpus. In Figure 2 we see that many frequently seen words </context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proc. of NLPRS-99, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>