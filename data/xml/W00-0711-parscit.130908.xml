<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002547">
<note confidence="0.815921">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 61-66, Lisbon, Portugal, 2000.
</note>
<title confidence="0.995989">
Modeling the Effect of Cross-Language Ambiguity on
Human Syntax Acquisition
</title>
<author confidence="0.994955">
William Gregory Sakas
</author>
<affiliation confidence="0.998764333333333">
Department of Computer Science
Hunter College and The Graduate Center
City University of New York
</affiliation>
<address confidence="0.992656">
New York, NY 10021
</address>
<email confidence="0.996117">
sakas@hunter. cuny.edu
</email>
<sectionHeader confidence="0.982922" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999254888888889">
A computational framework is presented which
is used to model the process by which human
language learners acquire the syntactic compo-
nent of their native language. The focus is fea-
sibility — is acquisition possible within a rea-
sonable amount of time and/or with a reason-
able amount of work? The approach abstracts
away from specific linguistic descriptions in or-
der to make a &apos;broad-stroke&apos; prediction of an
acquisition model&apos;s behavior by formalizing fac-
tors that contribute to cross-linguistic ambigu-
ity. Discussion centers around an application
to Fodor&apos;s Structural Trigger&apos;s Learner (STL)
(1998)1 and concludes with the proposal that
successful computational modeling requires a
parallel psycholinguistic investigation of the dis-
tribution of ambiguity across the domain of hu-
man languages.
</bodyText>
<sectionHeader confidence="0.575924" genericHeader="categories and subject descriptors">
1 Principles and Parameters
</sectionHeader>
<bodyText confidence="0.978987844444445">
Chomsky (1981) (and elsewhere) has proposed
that all natural languages share the same in-
nate universal principles (Universal Grammar
— UG) and differ only with respect to the set-
tings of a finite number of parameters. The syn-
tactic component of a grammar in the principles
and parameters (henceforth P&amp;P) framework,
is simply a collection of parameter values — one
value per parameter. (Standardly, two values
are available per parameter.) The set of human
&apos;The STL is an acquisition model in the principles
and parameters paradigm. The results presented here
are not intended to forward an argument for or against
the model, or for that matter, for or against the prin-
ciples and parameters paradigm. Rather, the results
are presented to point out (the possibly not-so- earth-
shattering observation) that the acquisition mechanism
can be extremely sensitive to ambiguity.
grammars is the set of all possible combinations
of parameter values (and lexicon).
The P&amp;P framework was motivated to a large
degree by psycholinguistic data demonstrating
the extreme efficiency of human language acqui-
sition. Children acquire the grammar of their
native language at an early age — generally ac-
cepted to be in the neighborhood of five years
old. In the P&amp;P framework, even if the lin-
guistic theory delineates over a billion possible
grammars, a learner need only determine the
correct 30 values that correspond to the gram-
mar that generates the sentences of the tar-
get language.2 given that a successful syntac-
tic theory must provide for an efficient acquisi-
tion mechanism, and since, prima facie, param-
eter values seem transparently learnable, it is
not surprising that parameters have been incor-
porated into current generative syntactic the-
ories. However, the exact process of parame-
ter setting has been studied only recently (e.g.
Clark (1992), Gibson and Wexler (1994), Yang
(1999), Briscoe (2000), among others) and al-
though it has proved linguistically fruitful to
construct parametric analyses, it turns out to
be surprisingly difficult to construct a workable
model of parameter-value acquisition.
</bodyText>
<subsectionHeader confidence="0.997597">
1.1 Parametric Ambiguity
</subsectionHeader>
<bodyText confidence="0.999926555555555">
A sentence is parametrically ambiguous if it is
licensed by two or more distinct combinations
of parameter values. Ambiguity is a natural en-
emy of efficient language acquisition. The prob-
lem is that, due to ambiguity, there does not
exist a one-to-one correspondence between the
linear &apos;word-order&apos; surface strings of the input
sample and the correct parameter values that
generate the target language. Clearly, if ev-
</bodyText>
<footnote confidence="0.5864795">
230 binary parameters entails approximately a billion
grammars (23° = 1,073, 741, 824.)
</footnote>
<page confidence="0.999041">
61
</page>
<bodyText confidence="0.99993335">
ery sentence of the target language triggers one
and only one set of parameter values (i.e. ev-
ery sentence is completely unambiguous) and
the learner, upon encountering an input, can
determine what those values are, the parameter
setting process is truly transparent. Unfortu-
nately, not all natural languages sentences are
so distinctively earmarked by their parametric
signatures. However, if there exists some degree
of parametric unambiguity in a learner&apos;s input
sample, a learner can set parameters by: 1) de-
coding the parametric signature of an input sen-
tence, 2) determining if ambiguity exists, and 3)
using the input to guide parameter setting only
in the case that the sentence is parametrically
unambiguous. The motto of such a learner is:
Don&apos;t learn from ambiguous input and learning
efficiency can be measured by the number of
sentences the learner has to wait for usable, un-
ambiguous inputs to occur in the input stream.3
</bodyText>
<sectionHeader confidence="0.904199" genericHeader="method">
2 The Structural Triggers Learner
</sectionHeader>
<bodyText confidence="0.919788071428571">
One recent model of human syntax acquisi-
tion, The Structural Triggers Learner
(STL) (Fodor, 1998), employs the human
parsing mechanism to determine if an input is
parametrically ambiguous. Parameter values
are viewed as bits of tree structure (treelets).
When the learner&apos;s current grammar is insuffi-
cient to parse the current input sentence, the
treelets may be utilized during the parsing pro-
cess in the same way as any natural language
grammar would be applied; no unusual parsing
activity is necessary. The treelets are adopted
as part of the learner&apos;s current grammar hy-
pothesis when: 1) they are required for a suc-
cessful parse of the current input sentence and
2) the sentence is unambiguous. The STL thus
learns only from fully unambiguous sentences.4
30f course, the extent to which such unambiguous
sentences exist in the domain of human languages is an
empirical issue. This is an important open research ques-
tion which is the focus of a recent research endeavor here
at CUNY. Our approach involves tagging a large, cross-
linguistic set of child-directed sentences, drawn from the
CHILDES database, with each sentence&apos;s parametric sig-
nature. By cross-tabulating the shared parameter values
against different languages, the study should shed some
light as to the shape of ambiguity in input samples typ-
ically encountered by children.
</bodyText>
<footnote confidence="0.978808333333333">
4This is actually the strategy employed by just one
of several different STL variants, some of which are de-
signed to manage domains in which unambiguous sen-
</footnote>
<figureCaption confidence="0.986567">
Figure 1: An example of how the STL acquires
new parameter values.
</figureCaption>
<bodyText confidence="0.919229">
See Figure 1.
</bodyText>
<sectionHeader confidence="0.962771" genericHeader="method">
3 The Feasibility of the STL
</sectionHeader>
<bodyText confidence="0.999578882352941">
The number of input sentences consumed by the
STL before convergence on the target grammar
can be derived from a relatively straightforward
Markov analysis. Importantly, the formulation
most useful to analyze performance does not re-
quire states which represent the grammars of
the parameter space (contra Niyogi and Berwick
(1996)). Instead, each state of the system de-
picts the number of parameters that have been
set, t, and the state transitions represent the
probability that the STL will adopt some num-
ber of new parameter values, w, on the basis of
the current state and whatever usable paramet-
ric information is revealed by the current input
sentence. See Figure 2.
The following factors (described in detail be-
low) determine the transition probabilities:
</bodyText>
<listItem confidence="0.9996286">
• the number of parameters that have been
set (t)
• the number of relevant parameters (r)
• the expression rate (e)
• the effective expression rate (e&apos;)
</listItem>
<bodyText confidence="0.9502916">
Not all parameters are relevant parameters.
Irrelevant parameters control properties of phe-
nomena not present in the target language, such
as clitic order in a language without clitics. For
tences are rare or nonexistent.
</bodyText>
<figure confidence="0.996188375">
2, A
Parameter value
treelets
sentence-O.
Parser
/I.
). n
Current grammar
</figure>
<page confidence="0.757671">
62
</page>
<figureCaption confidence="0.994313">
Figure 2: A transition diagram for the STL per-
</figureCaption>
<bodyText confidence="0.9979315">
forming in a parameter space of three parame-
ters. Nodes represent the current number of
parameters that have been correctly set. Arcs
indicate a change in the number that are cor-
rectly set. In this diagram, after each input is
consumed, 0, 1 or 2 new parameters may be set.
Once the learner enters state 3, it has converged
on the target.
our purposes, the number of relevant parame-
ters, r, is the total number of parameters that
need to be set in order to license all and only
the sentences of the target language.
Of the parameters relevant to the target lan-
guage as a whole, only some will be relevant to
any given sentence. A sentence expresses those
parameters for which a specific value is required
in order to build a parse tree, i.e. those pa-
rameters which are essential to the sentence&apos;s
structural description. For instance, if a sen-
tence does not have a relative clause, it will not
express parameters that concern only relative
clauses; if it is a declarative sentence, it won&apos;t
express the properties peculiar to questions; and
so on. The expression rate, e, for a language,
is the average number of parameters expressed
by its input sentences. Suppose that each sen-
tence, on average, is ambiguous with respect to
a of the parameters it expresses. The effective
expression rate, e&apos;, is the mean proportion of
expressed parameters that are expressed unam-
biguously (i.e. e&apos; (e — a)/e). It will also be
useful to consider a&apos; = (1 — e&apos;).
</bodyText>
<subsectionHeader confidence="0.999514">
3.1 Derivation of a Transition
Probability Function
</subsectionHeader>
<bodyText confidence="0.9998125625">
To present the derivation of the probability that
the system will change from an arbitrary state
St to state St±„„ (0 &lt; w &lt; e) it is useful to set
ambiguity aside for a moment. In order to set all
r parameters, the STL has to encounter enough
batches of e parameter values, possibly overlap-
ping with each other, to make up the full set of
r parameter values that have to be established.
Let H(wit, r, e) be the probability that an ar-
bitrary input sentence expresses w new (i.e. as
yet unset) parameters, out of the e parameters
expressed, given that the learner has already set
t parameters (correctly), for a domain in which
there are r total parameters that need to be set.
This is a specification of the hypergeometric
distribution and is given in Equation 1.
</bodyText>
<equation confidence="0.7459235">
(r,-,-t) (et
H(wit,r,e)=‘ 1‘
</equation>
<bodyText confidence="0.999940727272727">
Now, to deal with ambiguity, the effective
rate of expression, e&apos;, is brought into play. Re-
call that e&apos; is the proportion of expressed pa-
rameters that are expressed unambiguously. It
follows that the probability that any single pa-
rameter is expressed unambiguously is also e&apos;
and the probability that all of the expressed,
but as yet unset parameters are expressed un-
ambiguously is e&apos;w. That is, the probability that
an input is effectively unambiguous and hence
usable for learning is equal to e&apos;w.
</bodyText>
<equation confidence="0.986411">
w{H(wit,r,e)elw, 0&lt;w&lt;e
H(Olt,r,e)+ E H(ilt,r,e)(1—eii),w=0
P(St—&gt;St+)= rnin(e,r-t)
</equation>
<bodyText confidence="0.999816333333333">
Equation (2) can be used to calculate the prob-
ability of any possible transition of the Markov
system that models STL performance. One
method to determine the number of sentences
expected to be consumed by the STL is to sum
the number of sentences consumed in each state.
Let E(Si) represent the expected number of sen-
tences that will be consumed in state S. E is
given by the following recurrence relation:5
</bodyText>
<equation confidence="0.884899142857143">
E(S0) =
E(S) = 1/(1—P(Sn—&gt;Sn)) P(Si—S.)(Si) (3)
=n-e
The expected total is simply:
r-1
Etot=--E(S0)-F E(S)
i=e
</equation>
<footnote confidence="0.721241">
which is equal to the expected number to be
consumed before any parameters have been set
5The functional E is derived from basic properties
of Markov Chains. See Taylor and Karlin (1994) for a
general derivation.
</footnote>
<equation confidence="0.840108">
i=1
(4)
</equation>
<page confidence="0.988312">
63
</page>
<table confidence="0.999723761904762">
e a&apos;(%) r = 15 r = 20 r = 25 r = 30
1 20 62 90 119 150
40 83 120 159 200
60 124 180 238 300
80 249 360 477 599
5 20 15 22 29 36
40 34 46 59 73
60 144 176 210 245
80 3,300 3,466 3,666 3,891
10 20 14 18 23 28
40 174 187 203 221
60 9,560 9,621 9,727 9,878
80 9,765,731 9,766,375 9,768,376 9,772,740
15 20 28 32 37 41
40 2,127 2,136 2,153 2,180
60 931,323 931,352 931,479 931,822
80 - ... over 10 billion ...
20 20 - 87 91 95
40 - 27,351 27,361 27,383
60 - 90,949,470 90,949,504 90,949,728
80 - ...in the trillions ...
</table>
<tableCaption confidence="0.998931">
Table 1: Average number of inputs consumed
</tableCaption>
<bodyText confidence="0.9596076">
by the waiting-STL before convergence. Fixed
rate of expression.
(= E(So)) plus the number expected to be con-
sumed after the first successful learning event
(at which point the learner will be in state Se)
summed with the number of sentences expected
to be consumed in every other state up to the
state just before the target is attained (Sr_i).
Etot can be tractably calculated using dynamic
programming.
</bodyText>
<subsectionHeader confidence="0.999836">
3.2 Some Results
</subsectionHeader>
<bodyText confidence="0.999981234375">
Table 1 presents numerical results derived by
fixing different values of r, e, and e&apos;. In order
to make assessments of performance across dif-
ferent situations in terms of increasing rates of
ambiguity, a percentage measure of ambiguity,
a&apos;, is employed which is directly derived from
e&apos;: a&apos; = 1 — e&apos;, and is presented in Table 1 as a
percent (the proportion is multiplied by 100).
Notice that the number of parameters to be
set (r) has relatively little effect on convergence
time. What dominates learning speed is am-
biguity and expression rates. When a&apos; and e
are both high, the STL is consuming an unrea-
sonable number of input sentences. However,
the problem is not intrinsic to the STL model
of acquisition. Rather, the problem is due to
a too rigid restriction present in the current
formulation of the input sample. By relaxing
the restriction, the expected performance of the
STL improves dramatically. But first, it is in-
formative to discuss why the framework, as pre-
sented so far, leads to the prediction that the
STL will consume an extremely large number
of sentences at rates of ambiguity and expres-
sion approaching natural language.
By far the greatest amount of damage in-
flicted by ambiguity occurs at the very earliest
stages of learning. This is because before any
learning takes place, the STL must wait for the
occurrence of a sentence that is fully unambigu-
ous. Such sentences are bound to be extremely
rare if the expression rate and the degree of am-
biguity is high. For instance, a sentence with 20
out of 20 parameters unambiguous will virtually
never occur if parameters are ambiguous on av-
erage 99% of the time (the probability would be
(1/100)20).
After learning gets underway, STL perfor-
mance improves tremendously; the generally
damaging effect of ambiguity is mitigated. Ev-
ery successful learning event decreases the num-
ber of parameters still to be set. Hence, the
expression rate of unset parameters decreases
as learning proceeds. And to be usable by the
STL, the only parameters that need to be ex-
pressed unambiguously are those that have not
yet been set. For example, if 19 parameters
have already been set and e = r = 20 as in the
example above, the probability of encountering
a usable sentence in the case that parameters
are ambiguous on average 99% of the time and
the input sample consists of sentences express-
ing 20 parameters, is only (1/100)1 = 1/100.
This can be derived by plugging into Equation
(2): w= 1, t= 19, e = 20, and r = 20 which is
equal to: H(1119, 20, 20)(1/100)1 = (1)(1/100).
Clearly, the probability of seeing usable in-
puts increases rapidly as the number of param-
eters that are set increases. All that is needed,
therefore, is to get parameter setting started, so
that the learner can be quickly be pulled down
into more comfortable regions of parametric ex-
pression. Once parameter setting is underway,
the STL is extremely efficient.
</bodyText>
<subsectionHeader confidence="0.997892">
3.3 Distributed Expression Rate
</subsectionHeader>
<bodyText confidence="0.9999201">
So far e has been conveniently taken to be fixed
across all sentences of the target language. In
which case, when e = 10, the learner will have
to wait for a sentence with exactly 10 unam-
biguously expressed parameters in order to get
started on learning, and as discussed above, it
can be expected that this will be a very long
wait. However, if one takes the value of e to be
uniformly distributed (rather than fixed) then
the learner will encounter some sentences which
</bodyText>
<page confidence="0.998601">
64
</page>
<bodyText confidence="0.995785375">
express fewer than 10 parameters, and which
are correspondingly more likely to be fully un-
ambiguous and hence usable for learning.
In fact, any distribution of e can be incorpo-
rated into the framework presented so far. Let
Di(x) denote the probability distribution of ex-
pression of the input sample. That is, the prob-
ability that an arbitrarily chosen sentence from
the input sample I expresses x parameters. For
example, if D1 imposes a uniform distribution,
then Di(x) = 1/emax where every sentence ex-
presses at least 1 parameter and emax is the
maximum number of parameters expressed by
any sentence. Given D I , a new transition prob-
ability 131 (St —&gt; St+w) = (wit, r, emax , e&apos;) can
be formulated as:
</bodyText>
<equation confidence="0.981345">
emax
Pi(wit,r,erna.,e)= E Di(i)P(wit,r,i,e) (5)
</equation>
<bodyText confidence="0.999978466666667">
where P is defined in (2) above and emax repre-
sents the maximum number of parameters that
a sentence may express instead of a fixed num-
ber for all sentences.
To see why Equation (5) is valid, consider
that to set w new parameters at least w must
be expressed in the current input sentence. Also
usable, are sentences that express more param-
eters (w + 1, w + 2, w + 3,... , emax). Thus, the
probability of setting w new parameters is sim-
ply the sum of the probabilities that a sentence
expressing a number of parameters, i, from w
to emax, is encountered by the STL (= Di(i)),
times the probability that the STL can set w ad-
ditional parameters given that i are expressed.
By replacing P with P&apos; in Equation 3 and mod-
ifying the derivation of the base case,6 the to-
tal expected number of sentences that will be
consumed by the STL given a distribution of
expression Di(x) can be calculated.
Table 2 presents numerical results derived by
fixing r and a&apos; and allowing e to vary uniformly
from 0 to emax. As in Table 1, a percentage
measure of ambiguity, a&apos;, is employed.
The results displayed in the table indicate
a striking decrease in the number of sentences
that the that the STL can be expected to con-
sume compared to those obtained with a fixed
expression rate in place. As a rough compari-
son, with the ambiguity rate (a&apos;) at 80%; when
</bodyText>
<equation confidence="0.868765">
6E(S0) = 1/(1 — Pi(S0 -4 SO))
e,,.. ct&apos;(%) r = 15 r = 20 r = 25 r = 30
</equation>
<table confidence="0.999448">
1 20 124 180 238 300
40 166 240 318 399
60 249 360 477 599
80 498 720 954 1198
5 20 28 40 53 67
40 46 65 86 107
60 89 124 161 199
80 235 324 417 511
10 20 17 24 32 40
40 40 55 70 86
60 102 137 173 209
80 323 430 538 648
15 20 15 21 27 33
40 46 62 77 93
60 134 176 219 262
80 447 586 726 868
20 20 - 20 26 32
40 74 91 109
60 223 275 327
80 755 931 1108
</table>
<tableCaption confidence="0.877301">
Table 2: Average number of inputs consumed
by the STL before convergence. Uniformly dis-
tributed rate of expression.
</tableCaption>
<bodyText confidence="0.8391735">
e varies uniformly from 0 to 10, the STL re-
quires 430 sentences (from Table 2); when e is
fixed at 5, the number of sentences required is
3,466 (from Table 1).
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999988333333333">
Although presented as a feasibility analysis of
parameter-setting — specifically of STL perfor-
mance, it should be clear that the relevant fac-
tors e&apos;, e, r, etc. can be applied to shape an
abstract input domain for almost any learning
strategy. This is important because questions of
a model&apos;s feasibility have proved difficult to an-
swer in spaces of a linguistically plausible size.
Recent attempts necessarily rely on severely
small, highly circumscribed language domains
(e.g. Gibson and Wexler (1994), among others).
These studies frequently involve the construc-
tion of an idealized language sample which is
(at best) an accurate subset of sentences that
a child might hear. A simulated learner is let
loose on the input space and results consist of
either the structure of the grammar(s) acquired
or the specific circumstances under which the
learner succeeds or fails to attain the target.
Without question, this research agenda is valu-
able and can bring to light interesting charac-
teristics of the acquisition process. (cf. Gibson
and Wexler&apos;s (1994) argument for certain de-
fault parameter values based on the potential
success or failure of verb-second acquisition in
a three-parameter domain. And, for a different
perspective, Elman et al.&apos;s (1996) discussions of
</bodyText>
<page confidence="0.998727">
65
</page>
<bodyText confidence="0.992032538461539">
English part-of-speech and past-tense morphol-
ogy acquisition in a connectionist framework.)
I stress that my point here is not to give a
full accounting of STL performance. Substan-
tial work has been completed towards this end
(Sakas (2000), Sakas and Fodor (In press.)),
as well as development of a similar frame-
work to other models (See Sakas and Demner-
Fushman (In prep.) for an application to Gib-
son and Wexler&apos;s Triggering Learning Algo-
rithm). Rather, I intend to put forth the conjec-
ture that syntax acquisition is extremely sensi-
tive to the distribution of ambiguity, and, given
this extreme sensitivity, suggest that simulation
studies need to be conducted in conjunction
with a broader analysis which abstracts away
from whatever linguistic particulars are neces-
sary to bring about the sentences required to
build the input sample that feeds the simulated
learner.
Ultimately, whether a particular acquisition
model is successful is an empirical issue and de-
pends on the exact conditions under which the
model performs well and the extent to which
those favorable conditions are in line with the
facts of human language. Thus, I believe a
three-fold approach to validate a computational
model of acquisition is warranted. First, an
abstract analysis (similar to the one presented
here) should be constructed that can be used
to uncover a model&apos;s sweet spots — where the
shape of ambiguity is favorable to learning per-
formance. Second, a computational psycholin-
guistic study should be undertaken to see if the
model&apos;s sweet spots are in line with the distri-
bution of ambiguity in natural language. And
finally, a simulation should be carried out.
Obviously, this a huge proposal requiring
years of person-hours and coordinated planning
among researchers with diverse skills. But if
computational modeling is going to eventually
lay claim to a model which accurately mir-
rors the human process of language acquisition,
years of fine grinding are necessary.
Acknowledgments. This work was supported
in part by PSC-CUNY-30 Research Grant 61595-
00-30. The three-fold approach is at the root of a
project we have begun at The City University of New
York. Much thanks to my collaborators Janet Dean
Fodor and Virginia Teller for many useful discussions
and input, as well as to two anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.973119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778023809524">
E.J. Briscoe. 2000. Grammatical Acquisition:
Inductive Bias and Coevolution of Language
and the Language Acquisition Device. Lan-
guage, 76(2).
N. Chomsky. 1981. Lectures on Government
and Binding. Foris. Dordrecht
R. Clark. 1992. The selection of syntactic
knowledge. Language Acquisition, 2(2):83–
149.
J.L. Elman, E. Bates, M.A. Johnson,
A. Karmiloff-Smith, D. Parisi, and K. Plun-
kett. Rethinking Innateness: A Connection-
ist Perspective on Development. MIT Press,
Cambridge, MA.
J.D. Fodor. 1998. Unambiguous triggers. Lin-
guistic Inquiry, 29(1):1-36.
E. Gibson and K. Wexler. 1994. Triggers. Lin-
guistic Inquiry, 25(3):407-454.
P. Niyogi and R.C. Berwick. 1996. A language
learning model for finite parameter spaces.
Cognition, 61:161-193.
W.G. Sakas. 2000. Ambiguity and the Compu-
tational Feasibility of Syntax Acquisition. Un-
published Ph.D. dissertation, City University
of New York.
W.G. Sakas and D. Demner-Fushman. In Prep.
Simulating Parameter Setting Performance in
Domains with a Large Number of Parameters:
A Hybrid Approach.
W.G. Sakas and J.D. Fodor. In Press. The
Structural Triggers Learner. In Stefano
Bertolo, editor, Parametric Linguistics and
Learnability: A Self-contained Tutorial for
Linguists. Cambridge University Press, Cam-
bridge,UK.
H.M. Taylor and S. Karlin. 1994. An Introduc-
tion to Stochastic Modeling. Academic Press,
San Diego, CA.
C.D. Yang. 1999. A selectionist theory of lan-
guage acquisition. In Proceedings of the 37th
Annual Meeting of the ACL. Association for
Computational Linguistics.
</reference>
<page confidence="0.988796">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.988495">of CoNLL-2000 and LLL-2000, 61-66, Lisbon, Portugal, 2000.</note>
<title confidence="0.994075">Modeling the Effect of Cross-Language Ambiguity Human Syntax Acquisition</title>
<author confidence="0.999817">William Gregory</author>
<affiliation confidence="0.950243333333333">Department of Computer Hunter College and The Graduate City University of New</affiliation>
<address confidence="0.993535">New York, NY</address>
<email confidence="0.99391">sakas@hunter.cuny.edu</email>
<abstract confidence="0.997863910447762">A computational framework is presented which used to model the process by human language learners acquire the syntactic compoof their native language. The focus is feasibility — is acquisition possible within a reasonable amount of time and/or with a reasonamount of work? approach abstracts away from specific linguistic descriptions in order to make a &apos;broad-stroke&apos; prediction of an acquisition model&apos;s behavior by formalizing factors that contribute to cross-linguistic ambiguity. Discussion centers around an application Fodor&apos;s Trigger&apos;s Learner (STL) and concludes with the proposal that successful computational modeling requires a parallel psycholinguistic investigation of the distribution of ambiguity across the domain of human languages. 1 Principles and Parameters Chomsky (1981) (and elsewhere) has proposed that all natural languages share the same inuniversal Grammar — UG) and differ only with respect to the setof a finite number of syncomponent of a grammar in the parameters P&amp;P) framework, is simply a collection of parameter values — one value per parameter. (Standardly, two values are available per parameter.) The set of human &apos;The STL is an acquisition model in the principles and parameters paradigm. The results presented here are not intended to forward an argument for or against the model, or for that matter, for or against the principles and parameters paradigm. Rather, the results are presented to point out (the possibly not-soearthshattering observation) that the acquisition mechanism can be extremely sensitive to ambiguity. grammars is the set of all possible combinations of parameter values (and lexicon). The P&amp;P framework was motivated to a large degree by psycholinguistic data demonstrating the extreme efficiency of human language acquisition. Children acquire the grammar of their native language at an early age — generally accepted to be in the neighborhood of five years old. In the P&amp;P framework, even if the linguistic theory delineates over a billion possible grammars, a learner need only determine the correct 30 values that correspond to the grammar that generates the sentences of the targiven that a successful syntactic theory must provide for an efficient acquisition mechanism, and since, prima facie, parameter values seem transparently learnable, it is not surprising that parameters have been incorporated into current generative syntactic theories. However, the exact process of parameter setting has been studied only recently (e.g. Clark (1992), Gibson and Wexler (1994), Yang (1999), Briscoe (2000), among others) and although it has proved linguistically fruitful to construct parametric analyses, it turns out to be surprisingly difficult to construct a workable model of parameter-value acquisition. 1.1 Parametric Ambiguity sentence is ambiguous it is licensed by two or more distinct combinations of parameter values. Ambiguity is a natural enemy of efficient language acquisition. The problem is that, due to ambiguity, there does not exist a one-to-one correspondence between the linear &apos;word-order&apos; surface strings of the input sample and the correct parameter values that the target language. Clearly, if evbinary parameters entails approximately a billion = 1,073, 741, 824.) 61 ery sentence of the target language triggers one and only one set of parameter values (i.e. every sentence is completely unambiguous) and the learner, upon encountering an input, can determine what those values are, the parameter setting process is truly transparent. Unfortunately, not all natural languages sentences are so distinctively earmarked by their parametric However, if there exists of parametric unambiguity in a learner&apos;s input sample, a learner can set parameters by: 1) decoding the parametric signature of an input sentence, 2) determining if ambiguity exists, and 3) using the input to guide parameter setting only in the case that the sentence is parametrically unambiguous. The motto of such a learner is: learn from ambiguous input learning efficiency can be measured by the number of sentences the learner has to wait for usable, uninputs to occur in the input 2 The Structural Triggers Learner One recent model of human syntax acquisi- Structural Triggers Learner 1998), employs the human parsing mechanism to determine if an input is parametrically ambiguous. Parameter values are viewed as bits of tree structure (treelets). When the learner&apos;s current grammar is insufficient to parse the current input sentence, the treelets may be utilized during the parsing process in the same way as any natural language grammar would be applied; no unusual parsing activity is necessary. The treelets are adopted as part of the learner&apos;s current grammar hypothesis when: 1) they are required for a successful parse of the current input sentence and 2) the sentence is unambiguous. The STL thus only from fully unambiguous course, the extent to which such unambiguous sentences exist in the domain of human languages is an empirical issue. This is an important open research question which is the focus of a recent research endeavor here at CUNY. Our approach involves tagging a large, crosslinguistic set of child-directed sentences, drawn from the CHILDES database, with each sentence&apos;s parametric signature. By cross-tabulating the shared parameter values against different languages, the study should shed some light as to the shape of ambiguity in input samples typically encountered by children. is actually the strategy employed by just one of several different STL variants, some of which are deto manage domains in which unambiguous sen- Figure 1: An example of how the STL acquires new parameter values. See Figure 1. 3 The Feasibility of the STL The number of input sentences consumed by the STL before convergence on the target grammar can be derived from a relatively straightforward Markov analysis. Importantly, the formulation most useful to analyze performance does not require states which represent the grammars of the parameter space (contra Niyogi and Berwick (1996)). Instead, each state of the system depicts the number of parameters that have been the state transitions represent the probability that the STL will adopt some number of new parameter values, w, on the basis of the current state and whatever usable parametric information is revealed by the current input sentence. See Figure 2. The following factors (described in detail below) determine the transition probabilities: • the number of parameters that have been • the number of relevant parameters (r) • the expression rate (e) • the effective expression rate (e&apos;) Not all parameters are relevant parameters. Irrelevant parameters control properties of phenomena not present in the target language, such as clitic order in a language without clitics. For tences are rare or nonexistent. A Parameter value treelets Parser /I. ). n Current grammar 62 Figure 2: A transition diagram for the STL performing in a parameter space of three parameters. Nodes represent the current number of parameters that have been correctly set. Arcs indicate a change in the number that are correctly set. In this diagram, after each input is consumed, 0, 1 or 2 new parameters may be set. Once the learner enters state 3, it has converged on the target. purposes, the number of paramer, the total number of parameters that need to be set in order to license all and only the sentences of the target language. Of the parameters relevant to the target language as a whole, only some will be relevant to given sentence. A sentence parameters for which a specific value is required in order to build a parse tree, i.e. those parameters which are essential to the sentence&apos;s structural description. For instance, if a sentence does not have a relative clause, it will not express parameters that concern only relative clauses; if it is a declarative sentence, it won&apos;t express the properties peculiar to questions; and so on. The expression rate, e, for a language, is the average number of parameters expressed by its input sentences. Suppose that each sentence, on average, is ambiguous with respect to of the parameters it expresses. The rate, e&apos;, the mean proportion of parameters that are expressed unam- (i.e. e&apos; (e — will also useful to consider a&apos; = (1 — e&apos;).</abstract>
<title confidence="0.5915095">3.1 Derivation of a Transition Probability Function</title>
<abstract confidence="0.951810230769231">To present the derivation of the probability that the system will change from an arbitrary state state (0 &lt; w &lt; is useful to set ambiguity aside for a moment. In order to set all r parameters, the STL has to encounter enough batches of e parameter values, possibly overlapother, to make up the full set of r parameter values that have to be established. e) be the probability that an arbitrary input sentence expresses w new (i.e. as yet unset) parameters, out of the e parameters expressed, given that the learner has already set (correctly), for a domain in which there are r total parameters that need to be set. This is a specification of the hypergeometric distribution and is given in Equation 1. Now, to deal with ambiguity, the effective of expression, brought into play. Recall that e&apos; is the proportion of expressed parameters that are expressed unambiguously. It follows that the probability that any single parameter is expressed unambiguously is also e&apos; and the probability that all of the expressed, but as yet unset parameters are expressed unambiguously is e&apos;w. That is, the probability that an input is effectively unambiguous and hence usable for learning is equal to e&apos;w. 0&lt;w&lt;e Equation (2) can be used to calculate the probability of any possible transition of the Markov system that models STL performance. One method to determine the number of sentences expected to be consumed by the STL is to sum the number of sentences consumed in each state. the expected number of senthat will be consumed in state E by the following recurrence E(S0) = = =n-e The expected total is simply: r-1 i=e which is equal to the expected number to be consumed before any parameters have been set functional derived from basic properties of Markov Chains. See Taylor and Karlin (1994) for a general derivation. i=1 (4) 63 e a&apos;(%) r = 15 r = 20 r = 25 =</abstract>
<phone confidence="0.674410428571428">1 20 62 90 119 150 40 83 120 159 200 60 124 180 238 300 80 249 360 477 599 5 20 15 22 29 36 40 34 46 59 73 60 144 176 210 245</phone>
<address confidence="0.727302916666667">80 3,300 3,466 3,666 3,891 10 20 14 18 23 28 40 174 187 203 221 60 9,560 9,621 9,727 9,878 80 9,765,731 9,766,375 9,768,376 9,772,740 15 20 28 32 37 41 40 2,127 2,136 2,153 2,180 60 931,323 931,352 931,479 931,822 80 - 10 billion ... 20 20 - 87 91 95 40 - 27,351 27,361 27,383 60 - 90,949,470 90,949,504 90,949,728</address>
<abstract confidence="0.995140753623188">80 - ...in the trillions ... Table 1: Average number of inputs consumed by the waiting-STL before convergence. Fixed rate of expression. the number expected to be consumed after the first successful learning event which point the learner will be in state summed with the number of sentences expected to be consumed in every other state up to the just before the target is attained can be tractably calculated using dynamic programming. 3.2 Some Results Table 1 presents numerical results derived by different values of r, e&apos;. In order to make assessments of performance across different situations in terms of increasing rates of ambiguity, a percentage measure of ambiguity, a&apos;, is employed which is directly derived from e&apos;: a&apos; = 1 — e&apos;, and is presented in Table 1 as a percent (the proportion is multiplied by 100). Notice that the number of parameters to be set (r) has relatively little effect on convergence time. What dominates learning speed is ambiguity and expression rates. When a&apos; and e are both high, the STL is consuming an unreasonable number of input sentences. However, the problem is not intrinsic to the STL model of acquisition. Rather, the problem is due to a too rigid restriction present in the current formulation of the input sample. By relaxing the restriction, the expected performance of the STL improves dramatically. But first, it is informative to discuss why the framework, as presented so far, leads to the prediction that the STL will consume an extremely large number of sentences at rates of ambiguity and expression approaching natural language. By far the greatest amount of damage inflicted by ambiguity occurs at the very earliest stages of learning. This is because before any learning takes place, the STL must wait for the occurrence of a sentence that is fully unambiguous. Such sentences are bound to be extremely rare if the expression rate and the degree of ambiguity is high. For instance, a sentence with 20 out of 20 parameters unambiguous will virtually never occur if parameters are ambiguous on average 99% of the time (the probability would be After learning gets underway, STL performance improves tremendously; the generally damaging effect of ambiguity is mitigated. Every successful learning event decreases the number of parameters still to be set. Hence, the expression rate of unset parameters decreases as learning proceeds. And to be usable by the STL, the only parameters that need to be expressed unambiguously are those that have not yet been set. For example, if 19 parameters have already been set and e = r = 20 as in the example above, the probability of encountering a usable sentence in the case that parameters are ambiguous on average 99% of the time and the input sample consists of sentences express- 20 parameters, is only = 1/100. This can be derived by plugging into Equation w= 1, 19, e = 20, and r = 20 which is to: H(1119, 20, = (1)(1/100). Clearly, the probability of seeing usable inputs increases rapidly as the number of parameters that are set increases. All that is needed, therefore, is to get parameter setting started, so that the learner can be quickly be pulled down into more comfortable regions of parametric expression. Once parameter setting is underway, the STL is extremely efficient. 3.3 Distributed Expression Rate So far e has been conveniently taken to be fixed across all sentences of the target language. In which case, when e = 10, the learner will have to wait for a sentence with exactly 10 unambiguously expressed parameters in order to get started on learning, and as discussed above, it can be expected that this will be a very long wait. However, if one takes the value of e to be uniformly distributed (rather than fixed) then the learner will encounter some sentences which 64 express fewer than 10 parameters, and which are correspondingly more likely to be fully unambiguous and hence usable for learning. In fact, any distribution of e can be incorporated into the framework presented so far. Let Di(x) denote the probability distribution of expression of the input sample. That is, the probability that an arbitrarily chosen sentence from input sample For if a uniform distribution, where every sentence exat least 1 parameter and is the maximum number of parameters expressed by sentence. Given I , new transition prob- —&gt; = (wit, r, emax , e&apos;) be formulated as: emax (5) defined in (2) above and represents the maximum number of parameters that a sentence may express instead of a fixed number for all sentences. To see why Equation (5) is valid, consider that to set w new parameters at least w must be expressed in the current input sentence. Also usable, are sentences that express more param- (w + 1, w + 2, w + 3,... , Thus, the probability of setting w new parameters is simply the sum of the probabilities that a sentence expressing a number of parameters, i, from w is encountered by the STL (= times the probability that the STL can set w additional parameters given that i are expressed. replacing Equation 3 and modthe derivation of the base the total expected number of sentences that will be consumed by the STL given a distribution of be calculated. Table 2 presents numerical results derived by fixing r and a&apos; and allowing e to vary uniformly 0 to As in Table 1, a percentage measure of ambiguity, a&apos;, is employed. The results displayed in the table indicate a striking decrease in the number of sentences that the that the STL can be expected to consume compared to those obtained with a fixed expression rate in place. As a rough comparison, with the ambiguity rate (a&apos;) at 80%; when = 1/(1 — SO)) e,,.. ct&apos;(%)</abstract>
<phone confidence="0.6652153">1 20 124 180 238 300 40 166 240 318 399 60 249 360 477 599 80 498 720 954 1198 5 20 28 40 53 67 40 46 65 86 107 60 89 124 161 199 80 235 324 417 511 10 20 17 24 32 40 40 40 55 70 86 60 102 137 173 209 80 323 430 538 648 15 20 15 21 27 33 40 46 62 77 93 60 134 176 219 262 80 447 586 726 868 20 20 - 20 26 32 40 74 91 109 60 223 275 327 80 755 931 1108</phone>
<abstract confidence="0.994643522727273">Table 2: Average number of inputs consumed by the STL before convergence. Uniformly distributed rate of expression. e varies uniformly from 0 to 10, the STL re- 430 sentences (from Table 2); when fixed at 5, the number of sentences required is 3,466 (from Table 1). 4 Discussion Although presented as a feasibility analysis of parameter-setting — specifically of STL performance, it should be clear that the relevant factors e&apos;, e, r, etc. can be applied to shape an abstract input domain for almost any learning strategy. This is important because questions of a model&apos;s feasibility have proved difficult to answer in spaces of a linguistically plausible size. Recent attempts necessarily rely on severely small, highly circumscribed language domains (e.g. Gibson and Wexler (1994), among others). These studies frequently involve the construction of an idealized language sample which is (at best) an accurate subset of sentences that a child might hear. A simulated learner is let loose on the input space and results consist of either the structure of the grammar(s) acquired or the specific circumstances under which the learner succeeds or fails to attain the target. Without question, this research agenda is valuable and can bring to light interesting characteristics of the acquisition process. (cf. Gibson and Wexler&apos;s (1994) argument for certain default parameter values based on the potential success or failure of verb-second acquisition in a three-parameter domain. And, for a different perspective, Elman et al.&apos;s (1996) discussions of 65 English part-of-speech and past-tense morphology acquisition in a connectionist framework.) I stress that my point here is not to give a full accounting of STL performance. Substantial work has been completed towards this end (Sakas (2000), Sakas and Fodor (In press.)), as well as development of a similar framework to other models (See Sakas and Demner- Fushman (In prep.) for an application to Giband Wexler&apos;s Learning Algo- I intend to put forth the conjecture that syntax acquisition is extremely sensitive to the distribution of ambiguity, and, given this extreme sensitivity, suggest that simulation studies need to be conducted in conjunction with a broader analysis which abstracts away from whatever linguistic particulars are necessary to bring about the sentences required to build the input sample that feeds the simulated learner. Ultimately, whether a particular acquisition model is successful is an empirical issue and depends on the exact conditions under which the model performs well and the extent to which those favorable conditions are in line with the facts of human language. Thus, I believe a three-fold approach to validate a computational model of acquisition is warranted. First, an abstract analysis (similar to the one presented here) should be constructed that can be used uncover a model&apos;s spots — the of ambiguity favorable to learning performance. Second, a computational psycholinguistic study should be undertaken to see if the model&apos;s sweet spots are in line with the distribution of ambiguity in natural language. And finally, a simulation should be carried out. Obviously, this a huge proposal requiring years of person-hours and coordinated planning among researchers with diverse skills. But if computational modeling is going to eventually lay claim to a model which accurately mirrors the human process of language acquisition, years of fine grinding are necessary. work was supported in part by PSC-CUNY-30 Research Grant 61595- 00-30. The three-fold approach is at the root of a project we have begun at The City University of New York. Much thanks to my collaborators Janet Dean Fodor and Virginia Teller for many useful discussions and input, as well as to two anonymous reviewers for their helpful comments.</abstract>
<note confidence="0.792547666666667">References E.J. Briscoe. 2000. Grammatical Acquisition: Inductive Bias and Coevolution of Language the Language Acquisition Device. Lan- Chomsky. 1981. on Government Binding. Dordrecht R. Clark. 1992. The selection of syntactic Acquisition, 149.</note>
<author confidence="0.8756885">J L Elman</author>
<author confidence="0.8756885">E Bates</author>
<author confidence="0.8756885">M A Johnson</author>
<author confidence="0.8756885">A Karmiloff-Smith</author>
<author confidence="0.8756885">D Parisi</author>
<author confidence="0.8756885">K Plun-</author>
<affiliation confidence="0.90427">Innateness: A Connection- Perspective on Development. Press,</affiliation>
<address confidence="0.970657">Cambridge, MA.</address>
<note confidence="0.223688571428571">Fodor. 1998. Unambiguous triggers. Lin- Inquiry, Gibson and K. Wexler. 1994. Triggers. Lin- Inquiry, P. Niyogi and R.C. Berwick. 1996. A language learning model for finite parameter spaces. Sakas. 2000. and the Compu-</note>
<affiliation confidence="0.907618">Feasibility of Syntax Acquisition. Unpublished Ph.D. dissertation, City University</affiliation>
<address confidence="0.912786">of New York.</address>
<note confidence="0.436662">W.G. Sakas and D. Demner-Fushman. In Prep.</note>
<title confidence="0.960055">Simulating Parameter Setting Performance in Domains with a Large Number of Parameters: A Hybrid Approach.</title>
<author confidence="0.892344">In Press The Structural Triggers Learner In Stefano</author>
<affiliation confidence="0.820605">editor, Linguistics and Learnability: A Self-contained Tutorial for</affiliation>
<address confidence="0.484843">University Press, Cam-</address>
<email confidence="0.779607">bridge,UK.</email>
<author confidence="0.557694">An Introduc-</author>
<affiliation confidence="0.65075">to Stochastic Modeling. Press,</affiliation>
<address confidence="0.694956">San Diego, CA.</address>
<note confidence="0.6681838">C.D. Yang. 1999. A selectionist theory of lanacquisition. In of the 37th Meeting of the ACL. for Computational Linguistics. 66</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>Grammatical Acquisition: Inductive Bias and Coevolution of Language and the Language Acquisition Device.</title>
<date>2000</date>
<journal>Language,</journal>
<volume>76</volume>
<issue>2</issue>
<contexts>
<context position="3041" citStr="Briscoe (2000)" startWordPosition="471" endWordPosition="472">tic theory delineates over a billion possible grammars, a learner need only determine the correct 30 values that correspond to the grammar that generates the sentences of the target language.2 given that a successful syntactic theory must provide for an efficient acquisition mechanism, and since, prima facie, parameter values seem transparently learnable, it is not surprising that parameters have been incorporated into current generative syntactic theories. However, the exact process of parameter setting has been studied only recently (e.g. Clark (1992), Gibson and Wexler (1994), Yang (1999), Briscoe (2000), among others) and although it has proved linguistically fruitful to construct parametric analyses, it turns out to be surprisingly difficult to construct a workable model of parameter-value acquisition. 1.1 Parametric Ambiguity A sentence is parametrically ambiguous if it is licensed by two or more distinct combinations of parameter values. Ambiguity is a natural enemy of efficient language acquisition. The problem is that, due to ambiguity, there does not exist a one-to-one correspondence between the linear &apos;word-order&apos; surface strings of the input sample and the correct parameter values th</context>
</contexts>
<marker>Briscoe, 2000</marker>
<rawString>E.J. Briscoe. 2000. Grammatical Acquisition: Inductive Bias and Coevolution of Language and the Language Acquisition Device. Language, 76(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding.</booktitle>
<location>Foris. Dordrecht</location>
<contexts>
<context position="1166" citStr="Chomsky (1981)" startWordPosition="170" endWordPosition="171"> reasonable amount of time and/or with a reasonable amount of work? The approach abstracts away from specific linguistic descriptions in order to make a &apos;broad-stroke&apos; prediction of an acquisition model&apos;s behavior by formalizing factors that contribute to cross-linguistic ambiguity. Discussion centers around an application to Fodor&apos;s Structural Trigger&apos;s Learner (STL) (1998)1 and concludes with the proposal that successful computational modeling requires a parallel psycholinguistic investigation of the distribution of ambiguity across the domain of human languages. 1 Principles and Parameters Chomsky (1981) (and elsewhere) has proposed that all natural languages share the same innate universal principles (Universal Grammar — UG) and differ only with respect to the settings of a finite number of parameters. The syntactic component of a grammar in the principles and parameters (henceforth P&amp;P) framework, is simply a collection of parameter values — one value per parameter. (Standardly, two values are available per parameter.) The set of human &apos;The STL is an acquisition model in the principles and parameters paradigm. The results presented here are not intended to forward an argument for or against</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>N. Chomsky. 1981. Lectures on Government and Binding. Foris. Dordrecht</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Clark</author>
</authors>
<title>The selection of syntactic knowledge.</title>
<date>1992</date>
<journal>Language Acquisition,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>149</pages>
<contexts>
<context position="2986" citStr="Clark (1992)" startWordPosition="463" endWordPosition="464"> years old. In the P&amp;P framework, even if the linguistic theory delineates over a billion possible grammars, a learner need only determine the correct 30 values that correspond to the grammar that generates the sentences of the target language.2 given that a successful syntactic theory must provide for an efficient acquisition mechanism, and since, prima facie, parameter values seem transparently learnable, it is not surprising that parameters have been incorporated into current generative syntactic theories. However, the exact process of parameter setting has been studied only recently (e.g. Clark (1992), Gibson and Wexler (1994), Yang (1999), Briscoe (2000), among others) and although it has proved linguistically fruitful to construct parametric analyses, it turns out to be surprisingly difficult to construct a workable model of parameter-value acquisition. 1.1 Parametric Ambiguity A sentence is parametrically ambiguous if it is licensed by two or more distinct combinations of parameter values. Ambiguity is a natural enemy of efficient language acquisition. The problem is that, due to ambiguity, there does not exist a one-to-one correspondence between the linear &apos;word-order&apos; surface strings </context>
</contexts>
<marker>Clark, 1992</marker>
<rawString>R. Clark. 1992. The selection of syntactic knowledge. Language Acquisition, 2(2):83– 149.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J L Elman</author>
<author>E Bates</author>
<author>M A Johnson</author>
<author>A Karmiloff-Smith</author>
<author>D Parisi</author>
<author>K Plunkett</author>
</authors>
<title>Rethinking Innateness: A Connectionist Perspective on Development.</title>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Elman, Bates, Johnson, Karmiloff-Smith, Parisi, Plunkett, </marker>
<rawString>J.L. Elman, E. Bates, M.A. Johnson, A. Karmiloff-Smith, D. Parisi, and K. Plunkett. Rethinking Innateness: A Connectionist Perspective on Development. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Fodor</author>
</authors>
<title>Unambiguous triggers. Linguistic Inquiry,</title>
<date>1998</date>
<pages>29--1</pages>
<contexts>
<context position="4846" citStr="Fodor, 1998" startWordPosition="754" endWordPosition="755">er&apos;s input sample, a learner can set parameters by: 1) decoding the parametric signature of an input sentence, 2) determining if ambiguity exists, and 3) using the input to guide parameter setting only in the case that the sentence is parametrically unambiguous. The motto of such a learner is: Don&apos;t learn from ambiguous input and learning efficiency can be measured by the number of sentences the learner has to wait for usable, unambiguous inputs to occur in the input stream.3 2 The Structural Triggers Learner One recent model of human syntax acquisition, The Structural Triggers Learner (STL) (Fodor, 1998), employs the human parsing mechanism to determine if an input is parametrically ambiguous. Parameter values are viewed as bits of tree structure (treelets). When the learner&apos;s current grammar is insufficient to parse the current input sentence, the treelets may be utilized during the parsing process in the same way as any natural language grammar would be applied; no unusual parsing activity is necessary. The treelets are adopted as part of the learner&apos;s current grammar hypothesis when: 1) they are required for a successful parse of the current input sentence and 2) the sentence is unambiguou</context>
</contexts>
<marker>Fodor, 1998</marker>
<rawString>J.D. Fodor. 1998. Unambiguous triggers. Linguistic Inquiry, 29(1):1-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<date>1994</date>
<journal>Triggers. Linguistic Inquiry,</journal>
<pages>25--3</pages>
<contexts>
<context position="3012" citStr="Gibson and Wexler (1994)" startWordPosition="465" endWordPosition="468"> the P&amp;P framework, even if the linguistic theory delineates over a billion possible grammars, a learner need only determine the correct 30 values that correspond to the grammar that generates the sentences of the target language.2 given that a successful syntactic theory must provide for an efficient acquisition mechanism, and since, prima facie, parameter values seem transparently learnable, it is not surprising that parameters have been incorporated into current generative syntactic theories. However, the exact process of parameter setting has been studied only recently (e.g. Clark (1992), Gibson and Wexler (1994), Yang (1999), Briscoe (2000), among others) and although it has proved linguistically fruitful to construct parametric analyses, it turns out to be surprisingly difficult to construct a workable model of parameter-value acquisition. 1.1 Parametric Ambiguity A sentence is parametrically ambiguous if it is licensed by two or more distinct combinations of parameter values. Ambiguity is a natural enemy of efficient language acquisition. The problem is that, due to ambiguity, there does not exist a one-to-one correspondence between the linear &apos;word-order&apos; surface strings of the input sample and th</context>
<context position="18926" citStr="Gibson and Wexler (1994)" startWordPosition="3258" endWordPosition="3261">0 sentences (from Table 2); when e is fixed at 5, the number of sentences required is 3,466 (from Table 1). 4 Discussion Although presented as a feasibility analysis of parameter-setting — specifically of STL performance, it should be clear that the relevant factors e&apos;, e, r, etc. can be applied to shape an abstract input domain for almost any learning strategy. This is important because questions of a model&apos;s feasibility have proved difficult to answer in spaces of a linguistically plausible size. Recent attempts necessarily rely on severely small, highly circumscribed language domains (e.g. Gibson and Wexler (1994), among others). These studies frequently involve the construction of an idealized language sample which is (at best) an accurate subset of sentences that a child might hear. A simulated learner is let loose on the input space and results consist of either the structure of the grammar(s) acquired or the specific circumstances under which the learner succeeds or fails to attain the target. Without question, this research agenda is valuable and can bring to light interesting characteristics of the acquisition process. (cf. Gibson and Wexler&apos;s (1994) argument for certain default parameter values </context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>E. Gibson and K. Wexler. 1994. Triggers. Linguistic Inquiry, 25(3):407-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Niyogi</author>
<author>R C Berwick</author>
</authors>
<title>A language learning model for finite parameter spaces.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--161</pages>
<contexts>
<context position="6688" citStr="Niyogi and Berwick (1996)" startWordPosition="1050" endWordPosition="1053">lly encountered by children. 4This is actually the strategy employed by just one of several different STL variants, some of which are designed to manage domains in which unambiguous senFigure 1: An example of how the STL acquires new parameter values. See Figure 1. 3 The Feasibility of the STL The number of input sentences consumed by the STL before convergence on the target grammar can be derived from a relatively straightforward Markov analysis. Importantly, the formulation most useful to analyze performance does not require states which represent the grammars of the parameter space (contra Niyogi and Berwick (1996)). Instead, each state of the system depicts the number of parameters that have been set, t, and the state transitions represent the probability that the STL will adopt some number of new parameter values, w, on the basis of the current state and whatever usable parametric information is revealed by the current input sentence. See Figure 2. The following factors (described in detail below) determine the transition probabilities: • the number of parameters that have been set (t) • the number of relevant parameters (r) • the expression rate (e) • the effective expression rate (e&apos;) Not all parame</context>
</contexts>
<marker>Niyogi, Berwick, 1996</marker>
<rawString>P. Niyogi and R.C. Berwick. 1996. A language learning model for finite parameter spaces. Cognition, 61:161-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W G Sakas</author>
</authors>
<title>Ambiguity and the Computational Feasibility of Syntax Acquisition.</title>
<date>2000</date>
<institution>City University of New York.</institution>
<note>Unpublished Ph.D. dissertation,</note>
<contexts>
<context position="19937" citStr="Sakas (2000)" startWordPosition="3420" endWordPosition="3421"> question, this research agenda is valuable and can bring to light interesting characteristics of the acquisition process. (cf. Gibson and Wexler&apos;s (1994) argument for certain default parameter values based on the potential success or failure of verb-second acquisition in a three-parameter domain. And, for a different perspective, Elman et al.&apos;s (1996) discussions of 65 English part-of-speech and past-tense morphology acquisition in a connectionist framework.) I stress that my point here is not to give a full accounting of STL performance. Substantial work has been completed towards this end (Sakas (2000), Sakas and Fodor (In press.)), as well as development of a similar framework to other models (See Sakas and DemnerFushman (In prep.) for an application to Gibson and Wexler&apos;s Triggering Learning Algorithm). Rather, I intend to put forth the conjecture that syntax acquisition is extremely sensitive to the distribution of ambiguity, and, given this extreme sensitivity, suggest that simulation studies need to be conducted in conjunction with a broader analysis which abstracts away from whatever linguistic particulars are necessary to bring about the sentences required to build the input sample t</context>
</contexts>
<marker>Sakas, 2000</marker>
<rawString>W.G. Sakas. 2000. Ambiguity and the Computational Feasibility of Syntax Acquisition. Unpublished Ph.D. dissertation, City University of New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W G Sakas</author>
<author>D Demner-Fushman</author>
</authors>
<title>In Prep. Simulating Parameter Setting Performance in Domains with a Large Number of Parameters: A Hybrid Approach.</title>
<marker>Sakas, Demner-Fushman, </marker>
<rawString>W.G. Sakas and D. Demner-Fushman. In Prep. Simulating Parameter Setting Performance in Domains with a Large Number of Parameters: A Hybrid Approach.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W G Sakas</author>
<author>J D Fodor</author>
</authors>
<title>In Press. The Structural Triggers Learner.</title>
<editor>In Stefano Bertolo, editor,</editor>
<publisher>Cambridge University Press, Cambridge,UK.</publisher>
<marker>Sakas, Fodor, </marker>
<rawString>W.G. Sakas and J.D. Fodor. In Press. The Structural Triggers Learner. In Stefano Bertolo, editor, Parametric Linguistics and Learnability: A Self-contained Tutorial for Linguists. Cambridge University Press, Cambridge,UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Taylor</author>
<author>S Karlin</author>
</authors>
<title>An Introduction to Stochastic Modeling.</title>
<date>1994</date>
<publisher>Academic Press,</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="11199" citStr="Taylor and Karlin (1994)" startWordPosition="1825" endWordPosition="1828">f the Markov system that models STL performance. One method to determine the number of sentences expected to be consumed by the STL is to sum the number of sentences consumed in each state. Let E(Si) represent the expected number of sentences that will be consumed in state S. E is given by the following recurrence relation:5 E(S0) = E(S) = 1/(1—P(Sn—&gt;Sn)) P(Si—S.)(Si) (3) =n-e The expected total is simply: r-1 Etot=--E(S0)-F E(S) i=e which is equal to the expected number to be consumed before any parameters have been set 5The functional E is derived from basic properties of Markov Chains. See Taylor and Karlin (1994) for a general derivation. i=1 (4) 63 e a&apos;(%) r = 15 r = 20 r = 25 r = 30 1 20 62 90 119 150 40 83 120 159 200 60 124 180 238 300 80 249 360 477 599 5 20 15 22 29 36 40 34 46 59 73 60 144 176 210 245 80 3,300 3,466 3,666 3,891 10 20 14 18 23 28 40 174 187 203 221 60 9,560 9,621 9,727 9,878 80 9,765,731 9,766,375 9,768,376 9,772,740 15 20 28 32 37 41 40 2,127 2,136 2,153 2,180 60 931,323 931,352 931,479 931,822 80 - ... over 10 billion ... 20 20 - 87 91 95 40 - 27,351 27,361 27,383 60 - 90,949,470 90,949,504 90,949,728 80 - ...in the trillions ... Table 1: Average number of inputs consumed by t</context>
</contexts>
<marker>Taylor, Karlin, 1994</marker>
<rawString>H.M. Taylor and S. Karlin. 1994. An Introduction to Stochastic Modeling. Academic Press, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Yang</author>
</authors>
<title>A selectionist theory of language acquisition.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3025" citStr="Yang (1999)" startWordPosition="469" endWordPosition="470">f the linguistic theory delineates over a billion possible grammars, a learner need only determine the correct 30 values that correspond to the grammar that generates the sentences of the target language.2 given that a successful syntactic theory must provide for an efficient acquisition mechanism, and since, prima facie, parameter values seem transparently learnable, it is not surprising that parameters have been incorporated into current generative syntactic theories. However, the exact process of parameter setting has been studied only recently (e.g. Clark (1992), Gibson and Wexler (1994), Yang (1999), Briscoe (2000), among others) and although it has proved linguistically fruitful to construct parametric analyses, it turns out to be surprisingly difficult to construct a workable model of parameter-value acquisition. 1.1 Parametric Ambiguity A sentence is parametrically ambiguous if it is licensed by two or more distinct combinations of parameter values. Ambiguity is a natural enemy of efficient language acquisition. The problem is that, due to ambiguity, there does not exist a one-to-one correspondence between the linear &apos;word-order&apos; surface strings of the input sample and the correct par</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>C.D. Yang. 1999. A selectionist theory of language acquisition. In Proceedings of the 37th Annual Meeting of the ACL. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>