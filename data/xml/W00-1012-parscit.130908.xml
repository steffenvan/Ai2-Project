<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9997965">
Dialogue Management in the Agreement Negotiation Process:
A Model that Involves Natural Reasoning
</title>
<author confidence="0.987114">
Mare KOIT
</author>
<affiliation confidence="0.999459">
Institute of Computer Science, Tartu University
</affiliation>
<address confidence="0.924328">
Liivi 2
50409 Tartu, Estonia
</address>
<email confidence="0.995068">
koit@ut.ee
</email>
<author confidence="0.92327">
Haldur OIM
</author>
<affiliation confidence="0.974988">
Dept. of General Linguistics, Tartu University
</affiliation>
<address confidence="0.9016865">
Tiigi 78
51014 Tartu, Estonia
</address>
<email confidence="0.996748">
hoim@psych.ut.ee
</email>
<sectionHeader confidence="0.994796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998326416666667">
In the paper we describe an approach to
dialogue management in the agreement
negotiation where one of the central roles is
attributed to the model of natural human
reasoning. The reasoning model consists of
the model of human motivational sphere,
and of reasoning algorithms. The reasoning
model is interacting with the model of
communication process. The latter is
considered as rational activity where central
role play the concepts of communicative
strategies and tactics.
</bodyText>
<sectionHeader confidence="0.958097" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.9974476">
Several researches have modelled the process of
argument negotiation in cooperative dialogue
where one participant makes a proposal to
another participant and as the result of
negotiation this is accepted or rejected.
Chu-Carroll and Carberry (1998) present a
cooperative response-generation model as a
recursive cycle Propose-Evaluate-Modify. They
concentrate on dialogues of information sharing
and negotiation. An information sharing
dialogue is started, when the agent recognised a
turn of his/her partner as a proposal, but does not
have enough information to decide whether to
accept it or not. A negotiation dialogue is
started, when the agent concludes that the
proposal is in conflict with his/her beliefs and
preferences, i.e. tends to reject it.
Heeman and Hirst (1995) model cooperation by
the cycle Present-Judge-Refashion. They use
two levels of modelling — planning and
cooperation. On the first level utterances are
generated and interpreted, on the second level
the cooperation of agents is modelled, relating it
to agent&apos;s mental states and planning processes.
The Shared Plans cooperation model deals with
planning processes in which participate multiple
agents, see Lochbaum (1998). The model
concentrates on group tasks that can be divided
into separate, but interacting subtasks, and the
central problem is coordination of intentions and
goals of partners.
Di Eugenio et al. (2000) present a model
BalanceProposeDispose: first, the relevant
information concerning the task is considered
and discussed, then a proposal is made and,
lastly, the decision concerning the proposal is
made — it is accepted or rejected.
In our model we depart from the same type of
situation. One agent, A, addresses another agent,
B, with the intention that B will carry out an
action D. After some negotiation, B agrees or
rejects the proposal.
In this paper we concentrate on the problems
connected with modelling participants as
conversation agents who are able to participate
in negotiation in the form of natural dialogue —
dialogue that is carried out in natural language
and according to the rules of human
communication.
Such a dialogue can be considered as rational
behaviour which is based on beliefs, wants and
intentions of agents, at the same time being
restricted by their resources, see Jokinen (1995),
Webber (2000). Conversation agent is a kind of
intelligent agent — a computer program that is
able to communicate with humans as another
human being.
As it is generally accepted, in a model of
conversation agent it is necessary to represent its
cognitive states as well as cognitive processes.
</bodyText>
<page confidence="0.997677">
102
</page>
<bodyText confidence="0.999931083333333">
One of the most well-known models of this type
is the BDI model, see Allen (1994).
Our main point in this paper is that the general
concepts of cognitive states and processes used
in BDI-type models should be extended in order
to include certain factors from human
motivational sphere and certain social principles
in order to guarantee naturalness of dialogues of
the type we are concerned with. This is
especially important in connection with the fact
that interest in modelling cooperative dialogues
where partners are pursuing a common goal has
considerably increased in recent years. On the
one hand, this is connected with rapid spreading
of Internet-based services. On the other hand,
the interest in models of full natural dialogue
derives from the possibility of building speech
interfaces with different knowledge and
databases, see Dybkjwr (2000). Both of these
developments broaden the concept of
naturalness of dialogue considerably and present
to it much stronger requirements concerning its
empirical adequacy as it has been generally
accepted thus far.
</bodyText>
<sectionHeader confidence="0.498437" genericHeader="method">
1 Model of Conversation Agent
</sectionHeader>
<bodyText confidence="0.994718">
In our model a conversation agent, A, is a
program that consists of 6 (interacting) modules:
A = (PL, PS, DM, INT, GEN, LP),
where PL — planner, PS — problem solver, DM —
dialogue manager, INT — interpreter, GEN —
generator, LP — linguistic processor. PL directs
the work of both DM and PS, where DM
controls communication process and PS solves
domain-related tasks. The task of INT is to make
semantic analysis of partner&apos;s utterances and
that of GEN is to generate semantic
representations of agent&apos;s own contributions. LP
carries out linguistic analysis and generation.
Conversation agent uses in its work goal base
GB and knowledge base KB. In our model, KB
consists of 4 components:
KB = (KB, KBL, KBD, KB),
where KB w contains world knowledge, KBL —
linguistic knowledge, KBE. — knowledge about
dialogue and 1(B5 — knowledge about interacting
subjects. For instance, KBD contains definitions
of communicative acts, turns and transactions
(declarative knowledge), and algorithms that are
applied to reach communicative goals —
communicative strategies and tactics (procedural
knowledge); KBs contains knowledge about
evaluative dispositions of participants towards
the world (e.g. what do they consider as pleasant
or unpleasant, useful or harmful), and, on the
other hand, algorithms that are used to generate
plans for acting on the world.
A necessary precondition of a communicative
interaction is existence of shared (mutual)
knowledge of interacting agents. This concerns
goal bases as well as all types of knowledge
bases; the intersections of the corresponding
bases of interacting agents A and B cannot be
empty: GBA fl GBB *0, KBAw n KBBw #0,
KBAL n KB&apos; L , KBA D KBBD #0, KB ABs
KBBs c?3, KBBAs KBA s #0.
In this paper we will consider a specific type of
dialogue where the communicative goal of agent
A is to get agent B to agree to carry out an
action D — so-called agreement negotiation
dialogue. We will concentrate here on dialogue
management in such kind of interaction, i.e. on
the functioning of the module DM.
</bodyText>
<sectionHeader confidence="0.791632" genericHeader="method">
2 Dialogue Management
2.1 Reasoning Model
</sectionHeader>
<bodyText confidence="0.999809888888889">
A dialogue participant chooses his/her responses
to the parter&apos;s communicative acts as a result of
certain reasoning process. After A has made B a
proposal to do D, B can respond with agreement
or rejection, depending on the result of his/her
reasoning.
Because we consider the model of natural
human reasoning as one of the important
components in attaining naturalness of dialogue
as a whole, we will discuss our model of
reasoning in some detail. From the point of view
of practical NLP the approach we will present
below may seem too abstract. But without solid
theoretical basis it will appear impossible to
guarantee naturalness of dialogues carried out by
computers with human users. We think that the
model we describe here can be taken as a basis
for the corresponding discussion.
Our model is not based on any scientific theory
of how human reasoning proceeds; our aim is to
model a &amp;quot;naive theory of reasoning&amp;quot; which
humans follow in everyday life when trying to
understand, predict and influence other persons&apos;
decisions and behavior, see Koit and Oim
(2000). The reasoning model consists of two
functionally linked parts: 1) a model of human
motivational sphere; 2) reasoning schemes.
</bodyText>
<page confidence="0.998955">
103
</page>
<bodyText confidence="0.957963414634147">
In the motivational sphere three basic factors
that regulate reasoning of a subject concerning D
are differentiated. First, subject may wish to do
D, if pleasant aspects of 13 for him/her
overweight unpleasant ones; second, subject
may find reasonable to do D, if D is needed to
reach some higher goal, and useful aspects of D
overweight harmful ones; and third, subject can
be in a situation where he/she must (is obliged)
to do D — if not doing D will lead to some kind
of punishment. We call these factors wish-,
needed- and must-factors, respectively.
For instance, in reasoning about some action D
(e.g. proposed by another agent), an agent as an
individual subject typically starts with checking
his/her wish-factor, i.e. whether D&apos;s pleasant
aspects overweight unpleasant ones. If this
holds, then the subject checks his/her resources,
and if these exist, proceeds to other positive and
negative aspects of D: its usefulness and
harmfulness, and if D is prohibited, then also
possible punishment(s). If the positive aspects in
sum overweight negative ones, the resulting
decision will be to do D, otherwise — not to do
D.
There can exist other typical situations. If the
agent is an &amp;quot;official&amp;quot; person, or a group of
subjects formed to fulfil certain tasks and/or to
pursue certain pre-established goal(s), then
typically the starting point of reasoning is
needed- and/or must-factor.
This means that there exist certain general
principles that determine how the reasoning
process proceeds. These principles depend, in
part, on the type of the reasoning agent. Before
starting to construct a concrete reasoning model
the types of agents involved should be
established. In our implementation the agent is
supposed to be a &amp;quot;simple&amp;quot; human being and the
actions under consideration are from everyday
life. In this case as examples of such principles
used in our model we can present the following
ones. For more details, see Oim (1996).
Pl. People prefer pleasant (more pleasant)
states to unpleasant (less pleasant) ones.
P2. People don&apos;t take an action of which they
don&apos;t assume that its consequence will be a
pleasant (useful) situation, or avoidance of an
unpleasant (harmful) situation.
The following principles illustrate more concrete
(operational) rules.
P3. In assessing an action D the values of
(internal — wish- and needed-) factors are
checked before the external (must-) factors.
P4. If D is found pleasant enough (i.e. D&apos;s
pleasant aspects overweight unpleasant ones),
then the needed- and must-factors will first be
checked from the point of view of their negative
aspects (&amp;quot;to what harmful consequences or
punishments D would lead?&amp;quot;).
The rule P4 explains, for example, why in
Figure 1 step 1 is immediately followed by step
2.
The weights of different aspects of D
(pleasantness, unpleasantness, usefulness,
harmfulness, punishment for doing a prohibited
action or not-doing an obligatory action) must be
summed up in some way. Thus, in a
computational model weights must have
numerial values. In reality people do not operate
with numbers but, rather, with some fuzzy sets.
On the other hand, existence of certain scales
also in human everyday reasoning is apparent.
For instance, for the characterisation of pleasant
and unpleasant aspects of some action there are
specific words: enticing, delighul, enjoyable,
attractive, acceptable, unattractive, displeasing,
repulsive etc. Each of these adjectives can be
expressed quantitatively. This presupposes
empirical studies, though.
We have represented the model of motivational
sphere by the following vector of weights:
</bodyText>
<equation confidence="0.947025444444444">
wA = (w(resourcesAm), w(pleasAm),
w(unpleasAni), w(useAni), w(hannAni),
w(ob1igatoryAD1), w(prohibitedAm),
w(puni5hAD1), w(punishAnot-Di),• • • 9
w(resourcesADn), w(pleasAnn), w(unpleasADO,
w(useArn,), w(harmAD,), w(obligatoryADn),
w(punrohiisbhAniteotd..DAno w(PunishADO,
w(p
Here Di, Dn represent human actions;
</equation>
<bodyText confidence="0.994503166666667">
w(resourcesA 13)=1, if A has resources necessary
to do Di (otherwise 0); w(obligatoryAN)=1, if Di
is obligatory for A (otherwise 0);
w(prohibitedAD)=1, if Di is prohibited for A
(otherwise 0). The values of other weights are
non-negative natural numbers.
The second part of the reasoning model consists
of reasoning schemas, that supposedly regulate
human action-oriented reasoning. A reasoning
scheme represents steps that the agent goes
through in his reasoning process; these consist in
computing and comparing the weights of
</bodyText>
<page confidence="0.995089">
104
</page>
<bodyText confidence="0.999027454545455">
different aspects of D; and the result is the
decision to do or not to do D.
Figure 1 presents the reasoning scheme that
departs from the wish of a subject to do D.
The scheme also illustrates one of the general
principles referred to above. It explains the order
the steps are taken by the reasoning agent: if a
subject is in a state where he/she wishes to do D,
then he/she checks first the harmful/useful
aspects of D, and after this proceeds to aspects
connected with possible punishments.
</bodyText>
<figure confidence="0.853712771428571">
Presupposition:
w(pleas) &gt; w(unpleas).
1) Are there enough resources
for doing Er?
If not then not to do D.
2) Is w(pleas) &gt; w(unpleas)
w (harm) ?
If not then go to step 6.
3) Is D prohibited?
If not then to do D.
4) Is w(pleas) &gt; w(unpleas)
w (harrn) + w (punish) ?
If yes then to do D.
5) Is w(pleas) + w(use)
w(unpleas) w (harm)
w (punisk) ?
If yes then to do D else
not to do D.
6) Is w(pleas) + w(use)
w(unpleas) + w(harrn)?
If yes then go to step 9.
7) Is D obligatory?
If not then not to do D.
8) Is w(pleas) + w(use) +
w (punish.„) &gt; w (unpleas ) +
w (harm) ?
If yes then to do .D else
not to do D.
9) Is D prohibited?
If not then to do D.
10) Is w(pleas) + w(use)
w (unpleas ) w (harm)
w (punishn) ?
If yes then to do D else
not to do D.
</figure>
<figureCaption confidence="0.9492745">
Figure 1. The reasoning procedure that departs
from the wish of a subject to do D.
</figureCaption>
<bodyText confidence="0.99242225">
The prerequisite for triggering this reasoning
procedure is w(pleas) &gt; w(unpleas), which is
based on the following assumption: if a person
wishes to do something, then he/she assumes
that the pleasant aspects of D (including its
consequences) overweigh its unpleasant aspects.
The same kinds of reasoning schemes are
constructed for the needed- and must-factors.
The reasoning model is connected with the
general model of conversation agent in the
following way. First, the planner PL makes use
of reasoning schemes and second, the KBs
contains the vector wA (A&apos;s subjective
evaluations of all possible actions) as well as
vectors wAB (A&apos;s beliefs concerning B&apos;s
evaluations, where B denotes agents A may
communicate with). The vector wAB do not
represent truthful knowledge, it is used as a
partner model.
When comparing our model with BDI model,
then beliefs are represented by knowledge of the
conversation agent with reliability less than 1;
desires are generated by the vector of weights
WA; and intentions correspond to goals in GB. In
addition to desires, from the weights vector we
also can derive some parameters of the
motivational sphere that are not explicitly
covered by the basic BDI model: needs,
obligations and prohibitions. Some wishes or
needs can be stronger than others: if w(pleasAui)
- w(unpleasAD) &gt; w(pleasADJ) - w(unpleasADJ),
then the wish to do D, is stronger than the wish
to do A. In the same way, some obligations
(prohibitions) can be stronger than others,
depending on the weight of the corresponding
punishment. It should be mentioned that adding
obligations to the standard BDI model is not
new. Traum and Allen (1994) show how
discourse obligations can be used to account in a
natural manner for the connection between a
question and its answer in dialogue and how
obligations can be used along with other parts of
the discourse context to extend the coverage of a
dialogue system.
</bodyText>
<subsectionHeader confidence="0.997705">
2.2 Communicative Strategies and
Tactics
</subsectionHeader>
<bodyText confidence="0.999988555555555">
Knowledge about dialogue KBD, which is used
by the Dialogue Manager, consists of two
functional parts: knowledge of the regularities of
dialogue, and rules of constructing and
combining speech acts.
The top level concept of dialogue rules in our
model is communicative strategy. This concept
is reserved for such basic communication types
as information exchange, directive dialogue,
</bodyText>
<page confidence="0.996756">
105
</page>
<bodyText confidence="0.994140547169812">
phatic communication, etc. On the more
concrete level, the conversation agent can realise
a communicative strategy by means of several
communicative tactics; this concept more
closely corresponds to the concept of
communicative strategy as used in some other
approaches, see e.g. Jokinen (1996). In the case
of directive communication (which is the
strategy we are interested in) the agent A can use
tactics of enticing, persuading, threatening. In
the case of enticing, A stresses pleasant aspects,
in the case of persuading — useful aspects of D
for B; in the case of ordering A addresses
obligations of B, in the case of threatening A
explicitly refers to possible punishment for not
doing D.
Which one of these tactics A chooses depends
on several factors. There is one relevant aspect
of human-human communication which is
relatively well studied in pragmatics of human
communication and which we have included in
our model as the concept of communicative
space.
Communicative space is defined by a number of
coordinates that characterise the relationships of
participants in a communicative encounter.
Communication can be collaborative or
confrontational, personal or impersonal; it can
be characterised by the social distance between
participants; by the modality (friendly, ironic,
hostile, etc.) and by intensity (peaceful,
vehement, etc.). Just as in case of motivations of
human behaviour, people have an intuitive,
&amp;quot;naive theory&amp;quot; of these coordinates. This
constitutes a part of the social conceptualisation
of communication, and it also should not be
ignored in serious attempts to model natural
communication in NLP systems.
In our model the choice of a communicative
tactics depends on the &amp;quot;point&amp;quot; of the
communicative space in which the participants
place themselves. The values of the coordinates
are again given in the form of numerical values.
The communicative strategy can be presented as
an algorithm (Figure 2).
Figure 3 presents a tactic of enticement.
In our model there are three different
communicative tactics that A can use within the
frames of the directive communicative strategy:
those of enticement, persuasion and threatening.
Each communicative tactic constitutes a
procedure for compiling a turn in the ongoing
dialogue.
</bodyText>
<listItem confidence="0.997317473684211">
1) Choose the communicative
tactic.
2) Implement the tactic to
generate an expression (inform
the partner of the communicative
goal).
3) Did the partner agree to do
D? If yes then finish (the
communicative goal has been
reached).
4) Give up? If yes then finish
(the communicative goal has not
been reached).
5) Change the communicative
tactic? If yes then choose the
new tactic.
6) Implement the tactic to
generate an expression. Go to
step 3.
</listItem>
<figureCaption confidence="0.6327365">
Figure 2. Communicative strategy used by the
initiator of communication.
</figureCaption>
<listItem confidence="0.9781025">
1) If le (resources ) =0 then
present a counterargument in
order to point at the presence
of possible resources or at the
possibility to gain them.
2) If w8(harm) &gt; w&amp;quot;(harm) then
present a counterargument in
order to downgrade the value of
harm.
3) If le(obligatory)=1 &amp;
</listItem>
<bodyText confidence="0.728233">
wB(punish.,,,) &lt; w&amp;quot;(punish.,) then
present a counterargument in
order to decrease the weight of
the punishment.
</bodyText>
<listItem confidence="0.947572923076923">
4) If w3(prohibited)=1 &amp;
wB(punisk) &gt; w( punish,,) then
present a counterargument in
order to downgrade the weight of
the punishment.
5) If wa(unpleas) &gt; w&amp;quot;(unpleas)
then
present a counterargument in
order to downgrade the value of
the unpleasant aspects of D.
6) Present a counterargument in
order to stress the pleasant
aspects of D.
</listItem>
<figureCaption confidence="0.991729">
Figure 3. A&apos;s tactics of enticement.
</figureCaption>
<page confidence="0.995621">
106
</page>
<bodyText confidence="0.99995088">
The tactic of enticement consists in increasing
B&apos;s wish to do D; the tactic of persuasion
consists in increasing B&apos;s belief of the usefulness
of D for him/her, and the tactic of threatening
consists in increasing B&apos;s understanding that
he/she must do D.
Communicative tactics are directly related to the
reasoning process of the partner. If A is applying
the tactics of enticement he/she should be able to
imagine the reasoning process in B that is
triggered by the input parameter wish. If B
refuses to do D, then A should be able to guess
at which point the reasoning of B went into the
&amp;quot;negative branch&amp;quot;, in order to adequately
construct his/her reactive turn.
Analogously, the tactic of persuasion is related
to the reasoning process triggered by the needed-
parameter, and the threatening tactic is related to
the reasoning process triggered by the must-
parameter. For more details see, for example,
Koit (1996), Koit and Oim (1998), Koit and Oim
(1999).
Thus, in order to model various communicative
tactics, one must know how to model the process
of reasoning.
</bodyText>
<subsectionHeader confidence="0.999209">
2.3 Speech Acts
</subsectionHeader>
<bodyText confidence="0.997127923076923">
The minimal communicative unit in our model is
speech act (SA). In the implementation we make
use of a limited number of SAs the
representational formalism of which is frames.
Figure 4 presents the frame of SA Proposal in
the context of co-operative interaction. Other
SAs are represented in the same form. Each SA
contains a static (declarative) and a dynamic
(procedural) part. The static part consists of
preconditions, goal, content (immediate act) and
consequences. The dynamic part is made up
from two kinds of procedures: 1) those that the
author of the SA applies in the generation of a
communicative turn that contains the given SA;
2) those that the addressee applies in the process
of response generation.
As one can see, such a two-part representation
contains also rules for combining SAs in a turn,
and on the other hand, guarantees coherence of
turn-takings: when we have tagged in KBD
initiating SAs (such as Question or Proposal),
then the following chain of SAs follows from
the interpretation-generation procedures as
applied by participants.
PROPOSAL (author A, recipient B,
A proposes B to do an action D)
</bodyText>
<listItem confidence="0.875669035714286">
I. Static part
SETTING
(1) A has a goal G
(2) A believes that B in the
same way has the goal G
(3) A believes that in order to
reach G an instrumental goal
Gi should be reached
(4) A believes that B in the
same way believes that in
order to reach G an
instrumental goal G. should be
reached
(5) A believes that to attain
the goal Gi B has to do D
(6) A believes that B has
resources for doing D
(7) A believes that B will
decide to do D
GOAL: B decides to do D
CONTENT: A informs B that
he/she wishes B to do D
CONSEQUENCES
(1) B knows the SETTING, GOAL
and CONTENT
(2) A knows that B knows the
SETTING, GOAL and CONTENT
II. Dynamic part
</listItem>
<bodyText confidence="0.9383576">
Generating procedures (A&apos;s
possibilities to build his/her
turn that contains Proposal as
the dominant SA).
A has Goal G; A believes that B
also has Goal G; A believes that
in order to reach G, Gi should be
reached; A has decided to
formulate this as Proposal to B
to do D.
Procedures (before formulating
the turn) consist in checking
whether the preconditions of
proposal hold and in making
decisions about information to
be added in the turn:
- in case of (2) : is G
actualised in B? If not, then
actualise it by adding SA
Inform;
- in case of (4) : does B
believe that in order to
reach B, Gi should be reached
first. If not, then add SA
Explanation (Argument);
- in case of (6) : if A is not
sure that B has resources for
D, then add Question;
- in case of (7) : if A is not
sure that B will agree to do
</bodyText>
<page confidence="0.995751">
107
</page>
<note confidence="0.6889966">
D (for this A should model
B&apos;s reasoning), then add
Argument.
Procedures of interpretation-
generation
</note>
<bodyText confidence="0.905822">
(B&apos;s possibilities to react to
proposal) are started after B
has recognised SA Proposal:
</bodyText>
<listItem confidence="0.995817588235294">
- in case of (2), (4), (5) : if
B does not have Goal G and/or
he/she does not have the
corresponding beliefs and A
has not provided the needed
additional information, then
add Question (ask for
additional information);
- in case of (6) : if B does
not have Resources for D,
then Reject + Argument;
- in case of (7) : if the
decision of B to do D (as the
result of the application of
reasoning scheme(s)) is
negative, then Reject +
Argument.
</listItem>
<figureCaption confidence="0.9432285">
Figure 4. Speech act Proposal in the context of
co-operative interaction.
</figureCaption>
<bodyText confidence="0.999461523809524">
Such a representation does not guarantee
coherence of dialogic encounters (transactions)
on a more general level. For instance, it does not
cover such phenomena as topic change,
inadequate responses caused by
misunderstandings; but, more importantly, also
various kinds of initiative overtalcings. For
instance, after rejecting the Proposal made by A,
B can, in addition to explaining the rejection by
Argument, initiate various &amp;quot;compensatory&amp;quot;
communicative activities. Such things are
normal in human co-operative interaction and
they are regulated by general pragmatic
principles that require from participants, in
addition to being co-operative and informative,
also being considerate and helpful. In our case
this means that KBD should also include general
level dialogue scenarios (in the form of a graph)
and formalisations of the mentioned pragmatic
principles; for an example of the latter, see
Jolcinen (1996).
</bodyText>
<sectionHeader confidence="0.623616" genericHeader="method">
3 Process of Dialogue
</sectionHeader>
<bodyText confidence="0.9772595">
Let us describe the case where both A and B are
intelligent agents; i.e. computer programs.
</bodyText>
<listItem confidence="0.904318833333333">
1. A constructs
a) the frame exemplar of D, putting in it all
relevant information A has about D;
b) the model of partner B, putting in it all
relevant information it has about B&apos;s
evaluations concerning the contents of the
slots in D&apos;s frame.
2. A chooses the point in communicative space
from which it intents to start the interaction.
3. A starts to apply communicative strategy. A
models B&apos;s reasoning process, using B&apos;s
model. First A applies the reasoning scheme
</listItem>
<bodyText confidence="0.952899351351351">
based on the wish of B. If it results in &apos;to do
D&apos;, then A actualises the tactic of enticing
and generates its first turn which contains a
frame exemplar of Proposal. If the result of
modelled reasoning results in &apos;not to do D&apos;,
then A tries reasoning which starts from
needed-factor and then the one triggered by
must-factor, and according to the result
actualises tactics of persuading or
threatening, and generates the first utterance.
If the application of all reasoning schemes
results in &apos;not to do D&apos;, then A abandons its
goal.
4. B interprets A&apos;s turn and recognises
Proposal in it. B constructs it&apos;s the exemplar
representation of D (this may not coincide
with that of A). B starts reasoning, in the
course of which it may need additional
information from A. On the basis of the
frame of Proposal B formulates the result of
reasoning as its response turn: yes/no +
(maybe) Argument.
5. A interprets B&apos;s answer and determines
which point in the dialogue scenario this
corresponds to. If B&apos;s answer was positive
(decision to do D), then according to
communicative strategy the encounter has
come to its successful end. If B&apos;s answer is
negative, then according to the dialogue
scenario A must formulate a
(counter)Argument. The communicative
strategy also allows to choose a new point in
communicative space and/or a new tactic.
To formulate the counter-argument, A uses
information from the exemplar of D (it may
be updated on the basis of B&apos;s negative
answer) and its model of B (which it had to
</bodyText>
<page confidence="0.997458">
108
</page>
<bodyText confidence="0.998177333333333">
change because of B&apos;s negative answer). A
models anew B&apos;s reasoning, i.e. the process
is repeated cyclically.
</bodyText>
<sectionHeader confidence="0.774663" genericHeader="method">
4 Dialogue examples
4.1 Example 1
</sectionHeader>
<bodyText confidence="0.975230666666667">
The example represents a dialogue where the
computer plays A&apos;s role and is implementing the
tactic of enticement. The user implements the
simple tactic: point out• little pleasantness and
unpleasantness of the action.
Let us suppose that the action D is &amp;quot;travel to
Venice and conclude a contract there&amp;quot;. The
computer has chosen the tactics of enticement
and has generated the following user model:
wAB(resources)=1, w(pleas)=l0,
wAB(unpleas)=4, w(use)=0, w(harm)=10,
wAB(obligatory)=1, w(prohibited)=0,
wAB(punishD)=0, wAB(pun1shnot-o)=6.
The reasoning procedure WISH on this model
yields a positive decision. A following dialogue
was generated (translated from Estonian):
C(omputer): Do you want to travel to Venice?
Our firm needs to conclude a contract there.
The computer informed the user about its
communicative goal.
U(ser): It is not interesting for me.
The user pointed out little pleasantness of the
action. The computer must correct w(pleas) in
the user model. The new value of w(pleas)
will be 8.
C: You can meet interesting people.
The computer chose its reply from the class of
expressions for increasing pleasantness. After
this turn the value of w(pleas) will be 9.
U: What&apos;s so pleasant about this trip?
The user pointed out little pleasantness once
more. The corrected value of w(pleas) will be
8.
C: There are many nice men in Italy.
After this turn w(pleas)=9.
U: This trip is too intense for me.
The user indicated unpleasantness of D. The
corrected value for wAB(unpleas) will be 5.
C: The company will pay all your expenses.
The computer chose its reply from the class of
expressions for decreasing unpleasantness and
supposes that after its reply the value of
wAB(unpleas) will be 4.
U. I don&apos;t like to travel.
The user indicated unpleasantness of the action
once more. Thus, the new value of wAB(unpleas)
will be 5.
C: You can take plane.
The computer decreased the unpleasantness of D
once more. The new value of wAB(unpleas) is 4.
U: You are right - I shall travel.
The user agreed to do D, the communicative
goal of the computer is achieved.
C: Jam glad.
</bodyText>
<subsectionHeader confidence="0.835681">
4.2 Example 2
</subsectionHeader>
<bodyText confidence="0.985078657894737">
The example represents a dialogue where the
computer is implementing the tactic of
threatening. The user implements the tactic as
before: point out little pleasantness and
unpleasantness of the action.
The action D is &amp;quot;travel to Venice and conclude a
contract there&amp;quot; as before. The computer has
chosen the tactic of threatening and has
generated the following user model:
wAB(resources)=1, wAB(pleas)=10,
wAB(unpleas)=4, w(use)4, w(harm)=6,
w(obligatory)=1, w(prohibited)=0,
wAB(punishD)=0, wAB(punishnot-D)=8.
The reasoning procedure MUST on this model
yields a positive decision thus threatening is
possible. A following dialogue was generated:
C: You must travel to Venice. Our firm needs to
conclude a contract there.
U: It is not interesting for me.
The user pointed out little pleasantness of the
action. The computer must correct w(pleas) in
the user model. The new value of w(pleas)
will be 0.
C: There are many people interested on getting
your job.
Using the tactic of threatening the computer
chose its reply from the class of expressions for
increasing punishment for not doing D. After
this turn the value of wAB(punishnot_D) will be 9.
U: What&apos;s so pleasant about this trip?.
The user pointed out little pleasantness once
more.
C: Refusing will be harmful for your reputation.
After this turn wAB(punishn0t_D)=10.
U. This trip is too intense for me.
The user indicated unpleasantness of D. The
corrected value for wAB(unpleas) will be 14.
C: lf you have money you can travel by plane.
</bodyText>
<page confidence="0.997424">
109
</page>
<bodyText confidence="0.95915">
The computer chose its reply from the class of
expressions for decreasing unpleasantness. The
value of wAB(unpleas) will be 13.
U: I don&apos;t like to travel.
The user indicated unpleasantness of the action
once more.
The reasoning procedure MUST on the user
model will give the negative decision thus
threatening is impossible. The computer can&apos;t
choose new tactics because reasoning
procedures WISH and NEEDED will give on the
user model the negative decision too. The
computer must give up.
C: Jam sorry.
</bodyText>
<sectionHeader confidence="0.948321" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999989190476191">
At present there exists implemented program
which can play the role of both A and B in a
simple communication situation where the goal
of A is that B would decide to do D. At the
moment the computer operates with semantic
representations of linguistic input/output only,
the surface linguistic part of interaction is
provided in the form of a list of possible
utterances. The work on linguistic processor is
in progress.
We have deliberately concentrated on modelling
the processes of reasoning of conversation
agents, as these processes form the heart of the
&amp;quot;cognitive&amp;quot; part of human communication, and
on modelling the use of communicative
strategies and tactics which constitute the
&amp;quot;social&amp;quot; part of communication.
Although the concepts and models we have
reported in the paper may seem too abstract
from the point of view of practical NLP, we are
convinced that without serious study and
modelling of cognitive and social aspects of
human communication it will appear impossible
to guarantee naturalness of dialogues carried out
by a computer system with a human user.
As we have so far mostly dealt with agreement
negotiation dialogues, we have planned as one
of the practical applications of the system as a
participant in communication training sessions.
Here the system can, for instance, establish
certain restrictions on argument types, on the
order in the use of arguments and counter-
arguments, etc.
Second, we have started to work, using our
experience in modelling cognitive and social
aspects of dialogue, on modelling information
seeking dialogues in the same lines. This type of
dialogue clearly will be the area where in the
next few years already systems will be required
that would be practically reliable, but at the
same time could follow the rules of natural
human communication.
</bodyText>
<sectionHeader confidence="0.997597" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.568374">
This research was supported by Estonian
Science Foundation (grant No 4467).
</bodyText>
<sectionHeader confidence="0.997043" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998120871794872">
James Allen (1994) Natural Language
Understanding. 2nd ed. The Benjamin/Cummings
Publ. Comp., Inc.
Jennifer Chu-Carroll and Sandra Carberry (1998)
Collaborative Response Generation in Planning
Dialogues. Computational Linguistics, 24/3, pp.
355-400.
Barbara Di Eugenio, Pamela W. Jordan, Richmond
H. Thomason, Johanna D. Moore (2000) The
Acceptance Cycle: An empirical investigation of
human-human collaborative dialogues, to appear
in International Journal of Human Computer
Studies.
Laila Dybkjwr (2000) Preface. — From Spoken
Dialogue to Full Natural Interactive Dialogue —
Theory, Empirical Analysis and Evaluation. LREC
2000 Workshop Proceedings. L. Dybkjaer, ed.
Athen, pp. 1-2.
Peter Heeman and Graeme Hirst (1995)
Collaborating on referring expressions.
Computational Linguistics, 21/3, pp. 351-382.
Kristiina Jokinen (1995) Rational Agency. In
&amp;quot;Rational Agency: Concepts, Theories, Models,
and Applications&amp;quot;, M. Fehling, ed.. Proc. of the
AAAI Fall Symposium. MIT, Boston, pp. 89-93.
Kristiina Jokinen (1996) Cooperative Response
Planning in CDM: Reasoning about
Communicative Strategies. In &amp;quot;TWLT11. Dialogue
Management in Natural Language Systems&amp;quot;, S.
LuperFoy, A. Nijholt &amp; G. Veldhuijzen van
Zanten, ed. Enschede: Universiteit Twente, pp.
159-168.
Mare Koit (1996) Implementing a dialogue model on
the computer. In &amp;quot;Estonian in the Changing World.
Papers in Theoretical and Computational
Linguictics&amp;quot;, H. Oim, ed. Tartu, pp.. 99-114.
Mare Koit and Haldur Oim (2000) Developing a
model of natural dialogue. In &amp;quot;From spoken
dialogue to full natural interactive dialogue-theory,
</reference>
<page confidence="0.983183">
110
</page>
<reference confidence="0.999142551724138">
Empirical analysis and evaluation. LREC2000
Workshop proceedings&amp;quot;, L. Dybkj&amp;-r, ed. Athen,
pp. 18-21.
Mare Koit and Haldur Oim (1999) Communicative
strategies in human-computer interaction: a model
that involves natural reasoning. In &amp;quot;23. Deutsche
Jahrestag fir Kfinstliche Intelligene. Bonn,
http://www.ilcp.uni-bonn.de/NDS99/Finals/1_2.ps
Mare Koit and Haldur Oim (1998) Developing a
model of dialog strategy. In &amp;quot;Text, Speech,
Dialogue — TSD&apos;98 Proceedings&amp;quot;. Brno, pp. 387-
390.
Karen Lochbaum (1998) A Collaborative Planning
Model of Intentional Structure. Computational
Linguistics, 24/4, pp. 525-572.
Haldur Oim (1996) Naïve theories and
communicative competence: reasoning in
communication. In &amp;quot;Estonian in the Changing
World. Papers in Theoretical and Computational
Linguictics&amp;quot;, H. Oim, ed. Tartu, pp. 211-231.
David R. Traum and James F. Allen (1994)
Discourse Obligations in Dialogue Processing. In
&amp;quot;Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics (ACL-
94)&amp;quot;, pp 1-8.
Bonnie Webber (2000) Computational Perspectives
on Discourse and Dialogue. In &amp;quot;The Handbook of
Discourse Analysis&amp;quot;. D. Schiffrin, D. Tamen, H.
Hamilton, ed. Blackwell Publishers Ltd.
</reference>
<page confidence="0.998799">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.201846">
<title confidence="0.9985575">Dialogue Management in the Agreement Negotiation A Model that Involves Natural Reasoning</title>
<author confidence="0.963459">Mare</author>
<affiliation confidence="0.8405245">Institute of Computer Science, Tartu Liivi</affiliation>
<address confidence="0.994991">50409 Tartu,</address>
<email confidence="0.993096">koit@ut.ee</email>
<author confidence="0.524294">Haldur</author>
<affiliation confidence="0.7948155">Dept. of General Linguistics, Tartu Tiigi</affiliation>
<address confidence="0.997081">51014 Tartu,</address>
<email confidence="0.994502">hoim@psych.ut.ee</email>
<abstract confidence="0.998483692307692">In the paper we describe an approach to dialogue management in the agreement negotiation where one of the central roles is attributed to the model of natural human reasoning. The reasoning model consists of the model of human motivational sphere, and of reasoning algorithms. The reasoning model is interacting with the model of communication process. The latter is considered as rational activity where central role play the concepts of communicative strategies and tactics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding. 2nd ed. The Benjamin/Cummings Publ.</title>
<date>1994</date>
<publisher>Comp., Inc.</publisher>
<contexts>
<context position="3496" citStr="Allen (1994)" startWordPosition="538" endWordPosition="539">ording to the rules of human communication. Such a dialogue can be considered as rational behaviour which is based on beliefs, wants and intentions of agents, at the same time being restricted by their resources, see Jokinen (1995), Webber (2000). Conversation agent is a kind of intelligent agent — a computer program that is able to communicate with humans as another human being. As it is generally accepted, in a model of conversation agent it is necessary to represent its cognitive states as well as cognitive processes. 102 One of the most well-known models of this type is the BDI model, see Allen (1994). Our main point in this paper is that the general concepts of cognitive states and processes used in BDI-type models should be extended in order to include certain factors from human motivational sphere and certain social principles in order to guarantee naturalness of dialogues of the type we are concerned with. This is especially important in connection with the fact that interest in modelling cooperative dialogues where partners are pursuing a common goal has considerably increased in recent years. On the one hand, this is connected with rapid spreading of Internet-based services. On the o</context>
<context position="15207" citStr="Allen (1994)" startWordPosition="2455" endWordPosition="2456"> addition to desires, from the weights vector we also can derive some parameters of the motivational sphere that are not explicitly covered by the basic BDI model: needs, obligations and prohibitions. Some wishes or needs can be stronger than others: if w(pleasAui) - w(unpleasAD) &gt; w(pleasADJ) - w(unpleasADJ), then the wish to do D, is stronger than the wish to do A. In the same way, some obligations (prohibitions) can be stronger than others, depending on the weight of the corresponding punishment. It should be mentioned that adding obligations to the standard BDI model is not new. Traum and Allen (1994) show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system. 2.2 Communicative Strategies and Tactics Knowledge about dialogue KBD, which is used by the Dialogue Manager, consists of two functional parts: knowledge of the regularities of dialogue, and rules of constructing and combining speech acts. The top level concept of dialogue rules in our model is communicative strategy. This concept is</context>
</contexts>
<marker>Allen, 1994</marker>
<rawString>James Allen (1994) Natural Language Understanding. 2nd ed. The Benjamin/Cummings Publ. Comp., Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>Collaborative Response Generation in Planning Dialogues.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>355--400</pages>
<contexts>
<context position="1048" citStr="Chu-Carroll and Carberry (1998)" startWordPosition="148" endWordPosition="151">entral roles is attributed to the model of natural human reasoning. The reasoning model consists of the model of human motivational sphere, and of reasoning algorithms. The reasoning model is interacting with the model of communication process. The latter is considered as rational activity where central role play the concepts of communicative strategies and tactics. Introduction Several researches have modelled the process of argument negotiation in cooperative dialogue where one participant makes a proposal to another participant and as the result of negotiation this is accepted or rejected. Chu-Carroll and Carberry (1998) present a cooperative response-generation model as a recursive cycle Propose-Evaluate-Modify. They concentrate on dialogues of information sharing and negotiation. An information sharing dialogue is started, when the agent recognised a turn of his/her partner as a proposal, but does not have enough information to decide whether to accept it or not. A negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it. Heeman and Hirst (1995) model cooperation by the cycle Present-Judge-Refashion. They use two</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1998</marker>
<rawString>Jennifer Chu-Carroll and Sandra Carberry (1998) Collaborative Response Generation in Planning Dialogues. Computational Linguistics, 24/3, pp. 355-400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Pamela W Jordan</author>
<author>Richmond H Thomason</author>
<author>Johanna D Moore</author>
</authors>
<title>The Acceptance Cycle: An empirical investigation of human-human collaborative dialogues, to appear in</title>
<date>2000</date>
<journal>International Journal of Human Computer Studies.</journal>
<marker>Di Eugenio, Jordan, Thomason, Moore, 2000</marker>
<rawString>Barbara Di Eugenio, Pamela W. Jordan, Richmond H. Thomason, Johanna D. Moore (2000) The Acceptance Cycle: An empirical investigation of human-human collaborative dialogues, to appear in International Journal of Human Computer Studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laila Dybkjwr</author>
</authors>
<date>2000</date>
<booktitle>Preface. — From Spoken Dialogue to Full Natural Interactive Dialogue — Theory, Empirical Analysis and Evaluation. LREC 2000 Workshop Proceedings. L. Dybkjaer, ed. Athen,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="4272" citStr="Dybkjwr (2000)" startWordPosition="658" endWordPosition="659">in factors from human motivational sphere and certain social principles in order to guarantee naturalness of dialogues of the type we are concerned with. This is especially important in connection with the fact that interest in modelling cooperative dialogues where partners are pursuing a common goal has considerably increased in recent years. On the one hand, this is connected with rapid spreading of Internet-based services. On the other hand, the interest in models of full natural dialogue derives from the possibility of building speech interfaces with different knowledge and databases, see Dybkjwr (2000). Both of these developments broaden the concept of naturalness of dialogue considerably and present to it much stronger requirements concerning its empirical adequacy as it has been generally accepted thus far. 1 Model of Conversation Agent In our model a conversation agent, A, is a program that consists of 6 (interacting) modules: A = (PL, PS, DM, INT, GEN, LP), where PL — planner, PS — problem solver, DM — dialogue manager, INT — interpreter, GEN — generator, LP — linguistic processor. PL directs the work of both DM and PS, where DM controls communication process and PS solves domain-relate</context>
</contexts>
<marker>Dybkjwr, 2000</marker>
<rawString>Laila Dybkjwr (2000) Preface. — From Spoken Dialogue to Full Natural Interactive Dialogue — Theory, Empirical Analysis and Evaluation. LREC 2000 Workshop Proceedings. L. Dybkjaer, ed. Athen, pp. 1-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<pages>351--382</pages>
<contexts>
<context position="1579" citStr="Heeman and Hirst (1995)" startWordPosition="228" endWordPosition="231">d as the result of negotiation this is accepted or rejected. Chu-Carroll and Carberry (1998) present a cooperative response-generation model as a recursive cycle Propose-Evaluate-Modify. They concentrate on dialogues of information sharing and negotiation. An information sharing dialogue is started, when the agent recognised a turn of his/her partner as a proposal, but does not have enough information to decide whether to accept it or not. A negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it. Heeman and Hirst (1995) model cooperation by the cycle Present-Judge-Refashion. They use two levels of modelling — planning and cooperation. On the first level utterances are generated and interpreted, on the second level the cooperation of agents is modelled, relating it to agent&apos;s mental states and planning processes. The Shared Plans cooperation model deals with planning processes in which participate multiple agents, see Lochbaum (1998). The model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners. D</context>
</contexts>
<marker>Heeman, Hirst, 1995</marker>
<rawString>Peter Heeman and Graeme Hirst (1995) Collaborating on referring expressions. Computational Linguistics, 21/3, pp. 351-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristiina Jokinen</author>
</authors>
<title>Rational Agency. In &amp;quot;Rational Agency: Concepts, Theories, Models, and Applications&amp;quot;,</title>
<date>1995</date>
<booktitle>Proc. of the AAAI Fall Symposium. MIT,</booktitle>
<pages>89--93</pages>
<editor>M. Fehling, ed..</editor>
<location>Boston,</location>
<contexts>
<context position="3115" citStr="Jokinen (1995)" startWordPosition="472" endWordPosition="473"> A, addresses another agent, B, with the intention that B will carry out an action D. After some negotiation, B agrees or rejects the proposal. In this paper we concentrate on the problems connected with modelling participants as conversation agents who are able to participate in negotiation in the form of natural dialogue — dialogue that is carried out in natural language and according to the rules of human communication. Such a dialogue can be considered as rational behaviour which is based on beliefs, wants and intentions of agents, at the same time being restricted by their resources, see Jokinen (1995), Webber (2000). Conversation agent is a kind of intelligent agent — a computer program that is able to communicate with humans as another human being. As it is generally accepted, in a model of conversation agent it is necessary to represent its cognitive states as well as cognitive processes. 102 One of the most well-known models of this type is the BDI model, see Allen (1994). Our main point in this paper is that the general concepts of cognitive states and processes used in BDI-type models should be extended in order to include certain factors from human motivational sphere and certain soc</context>
</contexts>
<marker>Jokinen, 1995</marker>
<rawString>Kristiina Jokinen (1995) Rational Agency. In &amp;quot;Rational Agency: Concepts, Theories, Models, and Applications&amp;quot;, M. Fehling, ed.. Proc. of the AAAI Fall Symposium. MIT, Boston, pp. 89-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristiina Jokinen</author>
</authors>
<title>Cooperative Response Planning in CDM: Reasoning about Communicative Strategies.</title>
<date>1996</date>
<booktitle>In &amp;quot;TWLT11. Dialogue Management in Natural Language Systems&amp;quot;,</booktitle>
<pages>159--168</pages>
<editor>S. LuperFoy, A. Nijholt &amp; G. Veldhuijzen van Zanten, ed. Enschede: Universiteit Twente,</editor>
<contexts>
<context position="16195" citStr="Jokinen (1996)" startWordPosition="2607" endWordPosition="2608"> consists of two functional parts: knowledge of the regularities of dialogue, and rules of constructing and combining speech acts. The top level concept of dialogue rules in our model is communicative strategy. This concept is reserved for such basic communication types as information exchange, directive dialogue, 105 phatic communication, etc. On the more concrete level, the conversation agent can realise a communicative strategy by means of several communicative tactics; this concept more closely corresponds to the concept of communicative strategy as used in some other approaches, see e.g. Jokinen (1996). In the case of directive communication (which is the strategy we are interested in) the agent A can use tactics of enticing, persuading, threatening. In the case of enticing, A stresses pleasant aspects, in the case of persuading — useful aspects of D for B; in the case of ordering A addresses obligations of B, in the case of threatening A explicitly refers to possible punishment for not doing D. Which one of these tactics A chooses depends on several factors. There is one relevant aspect of human-human communication which is relatively well studied in pragmatics of human communication and w</context>
</contexts>
<marker>Jokinen, 1996</marker>
<rawString>Kristiina Jokinen (1996) Cooperative Response Planning in CDM: Reasoning about Communicative Strategies. In &amp;quot;TWLT11. Dialogue Management in Natural Language Systems&amp;quot;, S. LuperFoy, A. Nijholt &amp; G. Veldhuijzen van Zanten, ed. Enschede: Universiteit Twente, pp. 159-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mare Koit</author>
</authors>
<title>Implementing a dialogue model on the computer.</title>
<date>1996</date>
<booktitle>In &amp;quot;Estonian in the Changing World. Papers in Theoretical and Computational Linguictics&amp;quot;, H. Oim, ed. Tartu,</booktitle>
<pages>99--114</pages>
<contexts>
<context position="20350" citStr="Koit (1996)" startWordPosition="3278" endWordPosition="3279">e reasoning process of the partner. If A is applying the tactics of enticement he/she should be able to imagine the reasoning process in B that is triggered by the input parameter wish. If B refuses to do D, then A should be able to guess at which point the reasoning of B went into the &amp;quot;negative branch&amp;quot;, in order to adequately construct his/her reactive turn. Analogously, the tactic of persuasion is related to the reasoning process triggered by the neededparameter, and the threatening tactic is related to the reasoning process triggered by the mustparameter. For more details see, for example, Koit (1996), Koit and Oim (1998), Koit and Oim (1999). Thus, in order to model various communicative tactics, one must know how to model the process of reasoning. 2.3 Speech Acts The minimal communicative unit in our model is speech act (SA). In the implementation we make use of a limited number of SAs the representational formalism of which is frames. Figure 4 presents the frame of SA Proposal in the context of co-operative interaction. Other SAs are represented in the same form. Each SA contains a static (declarative) and a dynamic (procedural) part. The static part consists of preconditions, goal, con</context>
</contexts>
<marker>Koit, 1996</marker>
<rawString>Mare Koit (1996) Implementing a dialogue model on the computer. In &amp;quot;Estonian in the Changing World. Papers in Theoretical and Computational Linguictics&amp;quot;, H. Oim, ed. Tartu, pp.. 99-114.</rawString>
</citation>
<citation valid="true">
<title>Developing a model of natural dialogue.</title>
<date>2000</date>
<booktitle>Mare Koit and Haldur Oim</booktitle>
<pages>18--21</pages>
<editor>proceedings&amp;quot;, L. Dybkj&amp;-r, ed. Athen,</editor>
<contexts>
<context position="2202" citStr="(2000)" startWordPosition="324" endWordPosition="324">n by the cycle Present-Judge-Refashion. They use two levels of modelling — planning and cooperation. On the first level utterances are generated and interpreted, on the second level the cooperation of agents is modelled, relating it to agent&apos;s mental states and planning processes. The Shared Plans cooperation model deals with planning processes in which participate multiple agents, see Lochbaum (1998). The model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners. Di Eugenio et al. (2000) present a model BalanceProposeDispose: first, the relevant information concerning the task is considered and discussed, then a proposal is made and, lastly, the decision concerning the proposal is made — it is accepted or rejected. In our model we depart from the same type of situation. One agent, A, addresses another agent, B, with the intention that B will carry out an action D. After some negotiation, B agrees or rejects the proposal. In this paper we concentrate on the problems connected with modelling participants as conversation agents who are able to participate in negotiation in the f</context>
<context position="4272" citStr="(2000)" startWordPosition="659" endWordPosition="659">rs from human motivational sphere and certain social principles in order to guarantee naturalness of dialogues of the type we are concerned with. This is especially important in connection with the fact that interest in modelling cooperative dialogues where partners are pursuing a common goal has considerably increased in recent years. On the one hand, this is connected with rapid spreading of Internet-based services. On the other hand, the interest in models of full natural dialogue derives from the possibility of building speech interfaces with different knowledge and databases, see Dybkjwr (2000). Both of these developments broaden the concept of naturalness of dialogue considerably and present to it much stronger requirements concerning its empirical adequacy as it has been generally accepted thus far. 1 Model of Conversation Agent In our model a conversation agent, A, is a program that consists of 6 (interacting) modules: A = (PL, PS, DM, INT, GEN, LP), where PL — planner, PS — problem solver, DM — dialogue manager, INT — interpreter, GEN — generator, LP — linguistic processor. PL directs the work of both DM and PS, where DM controls communication process and PS solves domain-relate</context>
<context position="7659" citStr="(2000)" startWordPosition="1219" endWordPosition="1219">t of view of practical NLP the approach we will present below may seem too abstract. But without solid theoretical basis it will appear impossible to guarantee naturalness of dialogues carried out by computers with human users. We think that the model we describe here can be taken as a basis for the corresponding discussion. Our model is not based on any scientific theory of how human reasoning proceeds; our aim is to model a &amp;quot;naive theory of reasoning&amp;quot; which humans follow in everyday life when trying to understand, predict and influence other persons&apos; decisions and behavior, see Koit and Oim (2000). The reasoning model consists of two functionally linked parts: 1) a model of human motivational sphere; 2) reasoning schemes. 103 In the motivational sphere three basic factors that regulate reasoning of a subject concerning D are differentiated. First, subject may wish to do D, if pleasant aspects of 13 for him/her overweight unpleasant ones; second, subject may find reasonable to do D, if D is needed to reach some higher goal, and useful aspects of D overweight harmful ones; and third, subject can be in a situation where he/she must (is obliged) to do D — if not doing D will lead to some k</context>
</contexts>
<marker>2000</marker>
<rawString>Mare Koit and Haldur Oim (2000) Developing a model of natural dialogue. In &amp;quot;From spoken dialogue to full natural interactive dialogue-theory, Empirical analysis and evaluation. LREC2000 Workshop proceedings&amp;quot;, L. Dybkj&amp;-r, ed. Athen, pp. 18-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mare Koit</author>
<author>Haldur Oim</author>
</authors>
<title>Communicative strategies in human-computer interaction: a model that involves natural reasoning.</title>
<date>1999</date>
<booktitle>In &amp;quot;23. Deutsche Jahrestag fir Kfinstliche Intelligene.</booktitle>
<location>Bonn, http://www.ilcp.uni-bonn.de/NDS99/Finals/1_2.ps</location>
<contexts>
<context position="20392" citStr="Koit and Oim (1999)" startWordPosition="3284" endWordPosition="3287">. If A is applying the tactics of enticement he/she should be able to imagine the reasoning process in B that is triggered by the input parameter wish. If B refuses to do D, then A should be able to guess at which point the reasoning of B went into the &amp;quot;negative branch&amp;quot;, in order to adequately construct his/her reactive turn. Analogously, the tactic of persuasion is related to the reasoning process triggered by the neededparameter, and the threatening tactic is related to the reasoning process triggered by the mustparameter. For more details see, for example, Koit (1996), Koit and Oim (1998), Koit and Oim (1999). Thus, in order to model various communicative tactics, one must know how to model the process of reasoning. 2.3 Speech Acts The minimal communicative unit in our model is speech act (SA). In the implementation we make use of a limited number of SAs the representational formalism of which is frames. Figure 4 presents the frame of SA Proposal in the context of co-operative interaction. Other SAs are represented in the same form. Each SA contains a static (declarative) and a dynamic (procedural) part. The static part consists of preconditions, goal, content (immediate act) and consequences. The</context>
</contexts>
<marker>Koit, Oim, 1999</marker>
<rawString>Mare Koit and Haldur Oim (1999) Communicative strategies in human-computer interaction: a model that involves natural reasoning. In &amp;quot;23. Deutsche Jahrestag fir Kfinstliche Intelligene. Bonn, http://www.ilcp.uni-bonn.de/NDS99/Finals/1_2.ps</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mare Koit</author>
<author>Haldur Oim</author>
</authors>
<title>Developing a model of dialog strategy.</title>
<date>1998</date>
<booktitle>In &amp;quot;Text, Speech, Dialogue — TSD&apos;98 Proceedings&amp;quot;. Brno,</booktitle>
<pages>387--390</pages>
<contexts>
<context position="20371" citStr="Koit and Oim (1998)" startWordPosition="3280" endWordPosition="3283">rocess of the partner. If A is applying the tactics of enticement he/she should be able to imagine the reasoning process in B that is triggered by the input parameter wish. If B refuses to do D, then A should be able to guess at which point the reasoning of B went into the &amp;quot;negative branch&amp;quot;, in order to adequately construct his/her reactive turn. Analogously, the tactic of persuasion is related to the reasoning process triggered by the neededparameter, and the threatening tactic is related to the reasoning process triggered by the mustparameter. For more details see, for example, Koit (1996), Koit and Oim (1998), Koit and Oim (1999). Thus, in order to model various communicative tactics, one must know how to model the process of reasoning. 2.3 Speech Acts The minimal communicative unit in our model is speech act (SA). In the implementation we make use of a limited number of SAs the representational formalism of which is frames. Figure 4 presents the frame of SA Proposal in the context of co-operative interaction. Other SAs are represented in the same form. Each SA contains a static (declarative) and a dynamic (procedural) part. The static part consists of preconditions, goal, content (immediate act) </context>
</contexts>
<marker>Koit, Oim, 1998</marker>
<rawString>Mare Koit and Haldur Oim (1998) Developing a model of dialog strategy. In &amp;quot;Text, Speech, Dialogue — TSD&apos;98 Proceedings&amp;quot;. Brno, pp. 387-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Lochbaum</author>
</authors>
<title>A Collaborative Planning Model of Intentional Structure.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>525--572</pages>
<contexts>
<context position="2000" citStr="Lochbaum (1998)" startWordPosition="291" endWordPosition="292">ept it or not. A negotiation dialogue is started, when the agent concludes that the proposal is in conflict with his/her beliefs and preferences, i.e. tends to reject it. Heeman and Hirst (1995) model cooperation by the cycle Present-Judge-Refashion. They use two levels of modelling — planning and cooperation. On the first level utterances are generated and interpreted, on the second level the cooperation of agents is modelled, relating it to agent&apos;s mental states and planning processes. The Shared Plans cooperation model deals with planning processes in which participate multiple agents, see Lochbaum (1998). The model concentrates on group tasks that can be divided into separate, but interacting subtasks, and the central problem is coordination of intentions and goals of partners. Di Eugenio et al. (2000) present a model BalanceProposeDispose: first, the relevant information concerning the task is considered and discussed, then a proposal is made and, lastly, the decision concerning the proposal is made — it is accepted or rejected. In our model we depart from the same type of situation. One agent, A, addresses another agent, B, with the intention that B will carry out an action D. After some ne</context>
</contexts>
<marker>Lochbaum, 1998</marker>
<rawString>Karen Lochbaum (1998) A Collaborative Planning Model of Intentional Structure. Computational Linguistics, 24/4, pp. 525-572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haldur Oim</author>
</authors>
<title>Naïve theories and communicative competence: reasoning in communication.</title>
<date>1996</date>
<booktitle>In &amp;quot;Estonian in the Changing World. Papers in Theoretical and Computational Linguictics&amp;quot;, H. Oim, ed. Tartu,</booktitle>
<pages>211--231</pages>
<contexts>
<context position="9728" citStr="Oim (1996)" startWordPosition="1554" endWordPosition="1555"> the starting point of reasoning is needed- and/or must-factor. This means that there exist certain general principles that determine how the reasoning process proceeds. These principles depend, in part, on the type of the reasoning agent. Before starting to construct a concrete reasoning model the types of agents involved should be established. In our implementation the agent is supposed to be a &amp;quot;simple&amp;quot; human being and the actions under consideration are from everyday life. In this case as examples of such principles used in our model we can present the following ones. For more details, see Oim (1996). Pl. People prefer pleasant (more pleasant) states to unpleasant (less pleasant) ones. P2. People don&apos;t take an action of which they don&apos;t assume that its consequence will be a pleasant (useful) situation, or avoidance of an unpleasant (harmful) situation. The following principles illustrate more concrete (operational) rules. P3. In assessing an action D the values of (internal — wish- and needed-) factors are checked before the external (must-) factors. P4. If D is found pleasant enough (i.e. D&apos;s pleasant aspects overweight unpleasant ones), then the needed- and must-factors will first be ch</context>
</contexts>
<marker>Oim, 1996</marker>
<rawString>Haldur Oim (1996) Naïve theories and communicative competence: reasoning in communication. In &amp;quot;Estonian in the Changing World. Papers in Theoretical and Computational Linguictics&amp;quot;, H. Oim, ed. Tartu, pp. 211-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>James F Allen</author>
</authors>
<date>1994</date>
<booktitle>Discourse Obligations in Dialogue Processing. In &amp;quot;Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL94)&amp;quot;,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="15207" citStr="Traum and Allen (1994)" startWordPosition="2453" endWordPosition="2456"> in GB. In addition to desires, from the weights vector we also can derive some parameters of the motivational sphere that are not explicitly covered by the basic BDI model: needs, obligations and prohibitions. Some wishes or needs can be stronger than others: if w(pleasAui) - w(unpleasAD) &gt; w(pleasADJ) - w(unpleasADJ), then the wish to do D, is stronger than the wish to do A. In the same way, some obligations (prohibitions) can be stronger than others, depending on the weight of the corresponding punishment. It should be mentioned that adding obligations to the standard BDI model is not new. Traum and Allen (1994) show how discourse obligations can be used to account in a natural manner for the connection between a question and its answer in dialogue and how obligations can be used along with other parts of the discourse context to extend the coverage of a dialogue system. 2.2 Communicative Strategies and Tactics Knowledge about dialogue KBD, which is used by the Dialogue Manager, consists of two functional parts: knowledge of the regularities of dialogue, and rules of constructing and combining speech acts. The top level concept of dialogue rules in our model is communicative strategy. This concept is</context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>David R. Traum and James F. Allen (1994) Discourse Obligations in Dialogue Processing. In &amp;quot;Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL94)&amp;quot;, pp 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>Computational Perspectives on Discourse and Dialogue. In &amp;quot;The Handbook of Discourse Analysis&amp;quot;.</title>
<date>2000</date>
<editor>D. Schiffrin, D. Tamen, H. Hamilton, ed.</editor>
<publisher>Blackwell Publishers Ltd.</publisher>
<contexts>
<context position="3130" citStr="Webber (2000)" startWordPosition="474" endWordPosition="475">other agent, B, with the intention that B will carry out an action D. After some negotiation, B agrees or rejects the proposal. In this paper we concentrate on the problems connected with modelling participants as conversation agents who are able to participate in negotiation in the form of natural dialogue — dialogue that is carried out in natural language and according to the rules of human communication. Such a dialogue can be considered as rational behaviour which is based on beliefs, wants and intentions of agents, at the same time being restricted by their resources, see Jokinen (1995), Webber (2000). Conversation agent is a kind of intelligent agent — a computer program that is able to communicate with humans as another human being. As it is generally accepted, in a model of conversation agent it is necessary to represent its cognitive states as well as cognitive processes. 102 One of the most well-known models of this type is the BDI model, see Allen (1994). Our main point in this paper is that the general concepts of cognitive states and processes used in BDI-type models should be extended in order to include certain factors from human motivational sphere and certain social principles </context>
</contexts>
<marker>Webber, 2000</marker>
<rawString>Bonnie Webber (2000) Computational Perspectives on Discourse and Dialogue. In &amp;quot;The Handbook of Discourse Analysis&amp;quot;. D. Schiffrin, D. Tamen, H. Hamilton, ed. Blackwell Publishers Ltd.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>