<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.5489225">
ALIGNING SENTENCES IN BILINGUAL CORPORA USI\
LEXICAL INFORMATION
</title>
<author confidence="0.988968">
Stanley F. Chen*
</author>
<affiliation confidence="0.981718">
Aiken Computation Laboratory
Division of Applied Sciences
Harvard University
</affiliation>
<address confidence="0.921423">
Cambridge, MA 02138
</address>
<email confidence="0.996663">
Internet: sfc@calliope.harvard.edu
</email>
<sectionHeader confidence="0.972368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999742071428572">
In this paper, we describe a fast algorithm for
aligning sentences with their translations in a
bilingual corpus. Existing efficient algorithms ig-
nore word identities and only consider sentence
length (Brown et al., 1991b; Gale and Church,
1991). Our algorithm constructs a simple statisti-
cal word-to-word translation model on the fly dur-
ing alignment. We find the alignment that maxi-
mizes the probability of generating the corpus with
this translation model. We have achieved an error
rate of approximately 0.4% on Canadian Hansard
data, which is a significant improvement over pre-
vious results. The algorithm is language indepen-
dent.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999297018181819">
In this paper, we describe an algorithm for align-
ing sentences with their translations in a bilingual
corpus. Aligned bilingual corpora have proved
useful in many tasks, including machine transla-
tion (Brown et al., 1990; Sadler, 1989), sense dis-
ambiguation (Brown et al., 1991a; Dagan et al.,
1991; Gale et al., 1992), and bilingual lexicogra-
phy (Klavans and Tzoukermann, 1990; Warwick
and Russell, 1990).
The task is difficult because sentences frequently
do not align one-to-one. Sometimes sentences
align many-to-one, and often there are deletions in
•The author wishes to thank Peter Brown, Stephen Del-
laPietra, Vincent DellaPietra, and Robert Mercer for their
suggestions, support, and relentless taunting. The author
also wishes to thank Jan Hajic and Meredith Goldsmith
as well as the aforementioned for checking the alignments
produced by the implementation.
one of the supposedly parallel corpora of a bilin-
gual corpus. These deletions can be substantial;
in the Canadian Hansard corpus, there are many
deletions of several thousand sentences and one
deletion of over 90,000 sentences.
Previous work includes (Brown et al., 1991b)
and (Gale and Church, 1991). In Brown, align-
ment is based solely on the number of words in
each sentence; the actual identities of words are
ignored. The general idea is that the closer in
length two sentences are, the more likely they
align. To perform the search for the best align-
ment, dynamic programming (Bellman, 1957) is
used. Because dynamic programming requires
time quadratic in the length of the text aligned,
it is not practical to align a large corpus as a sin-
gle unit. The computation required is drastically
reduced if the bilingual corpus can be subdivided
into smaller chunks. Brown uses anchors to per-
form this subdivision. An anchor is a piece of text
likely to be present at the same location in both
of the parallel corpora of a bilingual corpus. Dy-
namic programming is used to align anchors, and
then dynamic programming is used again to align
the text between anchors.
The Gale algorithm is similar to the Brown al-
gorithm except that instead of basing alignment
on the number of words in sentences, alignment is
based on the number of characters in sentences.
Dynamic programming is also used to search for
the best alignment. Large corpora are assumed to
be already subdivided into smaller chunks.
While these algorithms have achieved remark-
ably good performance, there is definite room for
improvement. These algorithms are not robust
with respect to non-literal translations and small
deletions; they can easily misalign small passages
</bodyText>
<page confidence="0.991176">
9
</page>
<note confidence="0.999462333333333">
Mr. McInnis? M. McInnis? tion, i.e., for all E and Fp we have an estimate for
Yes. Oui. P(FplE), the probability that the English sentence
Mr. Saunders? M. Saunders? E translates to the French passage Fp. Then, we
No. Non. can assign a probability to the English corpus C
Mr. Cossitt? M. Cossitt? translating to the French corpus ..F with a partic-
Yes. Oui. ular alignment. For example, consider the align-
ment A1 where sentence El corresponds to sen-
tence F1 and sentence E2 corresponds to sentences
F2 and F3. We get
</note>
<figureCaption confidence="0.999853">
Figure 1: A Bilingual Corpus Fragment
</figureCaption>
<bodyText confidence="0.999975935483871">
because they ignore word identities. For example,
the type of passage depicted in Figure 1 occurs in
the Hansard corpus. With length-based alignment
algorithms, these passages may well be misaligned
by an even number of sentences if one of the cor-
pora contains a deletion. In addition, with length-
based algorithms it is difficult to automatically re-
cover from large deletions. In Brown, anchors are
used to deal with this issue, but the selection of
anchors requires manual inspection of the corpus
to be aligned. Gale does not discuss this issue.
Alignment algorithms that use lexical informa-
tion offer a potential for higher accuracy. Previ-
ous work includes (Kay, 1991) and (Catizone et
al., 1989). However, to date lexically-based al-
gorithms have not proved efficient enough to be
suitable for large corpora.
In this paper, we describe a fast algorithm
for sentence alignment that uses lexical informa-
tion. The algorithm constructs a simple statistical
word-to-word translation model on the fly during
sentence alignment. We find the alignment that
maximizes the probability of generating the corpus
with this translation model. The search strategy
used is dynamic programming with thresholding.
Because of thresholding, the search is linear in the
length of the corpus so that a corpus need not be
subdivided into smaller chunks. The search strat-
egy is robust with respect to large deletions; lex-
ical information allows us to confidently identify
the beginning and end of deletions.
</bodyText>
<sectionHeader confidence="0.999418" genericHeader="method">
2 The Alignment Model
</sectionHeader>
<subsectionHeader confidence="0.998786">
2.1 The Alignment Framework
</subsectionHeader>
<bodyText confidence="0.99972475">
We use an example to introduce our framework for
alignment. Consider the bilingual corpus (C, Y)
displayed in Figure 2. Assume that we have con-
structed a model for English-to-French transla-
</bodyText>
<equation confidence="0.56549">
p(Y, At le) — P(Fi lEi)P(F2, F31E2),
</equation>
<bodyText confidence="0.996833833333333">
assuming that successive sentences translate inde-
pendently of each other. This value should be rel-
atively large, since F1 is a good translation of El
and (F2, F3) is a good translation of E2. Another
possible alignment A2 is one where E1 maps to
nothing and E2 maps to F1, F2, and F3. We get
</bodyText>
<equation confidence="0.954928">
F (. T . , A21e) = P (EIEOP (Fi, F2, F31E2)
</equation>
<bodyText confidence="0.99990125">
This value should be fairly low, since the align-
ment does not map the English sentences to their
translations. Hence, if our translation model is
accurate we will have
</bodyText>
<equation confidence="0.963282">
P(T,A11E)&gt;P(Y,A21e)
</equation>
<bodyText confidence="0.997259518518518">
In general, the more sentences that are mapped
to their translations in an alignment A, the higher
the value of P(Y , AV). We can extend this idea
to produce an alignment algorithm given a trans-
lation model. In particular, we take the alignment
of a corpus (C, .T) to be the alignment A that max-
imizes P(..r , Ale). The more accurate the transla-
tion model, the more accurate the resulting align-
ment will be.
However, because the parameters are all of the
form P (FAL&apos;) where E is a sentence, the above
framework is not amenable to the situation where
a French sentence corresponds to no English sen-
tences. Hence, we use a slightly different frame-
work. We view a bilingual corpus as a sequence
of sentence beads (Brown et al., 1991b), where a
sentence bead corresponds to an irreducible group
of sentences that align with each other. For exam-
ple, the correct alignment of the bilingual corpus
in Figure 2 consists of the sentence bead [Ei.; Fd
followed by the sentence bead [E2; F2, F3]. We
can represent an alignment A of a corpus as a se-
quence of sentence beads GE71); Fin , {Epz ; Fpz i , . . .) ,
where the EP i and FP i can be zero, one, or more
sentences long.
Under this paradigm, instead of expressing the
translation model as a conditional distribution
</bodyText>
<page confidence="0.929502">
10
</page>
<equation confidence="0.428683">
English (E)
</equation>
<bodyText confidence="0.857470058823529">
14 That is what the consumers
are interested in and that
is what the party is
interested in.
E2 Hon. members opposite scoff
at the freeze suggested by
this party; to them it is
laughable.
French (.7&apos;)
Voila ce qui interesse le
consommateur et voila ce
que interesse notre parti.
Les deputes d&apos;en face se
moquent du gel que a
propose notre parti.
Pour eux, c&apos;est une mesure
risible.
</bodyText>
<figure confidence="0.833525">
F2
F3
</figure>
<figureCaption confidence="0.999076">
Figure 2: A Bilingual Corpus
</figureCaption>
<bodyText confidence="0.998192666666667">
P(FplE) we express the translation model as a
distribution P([Ep; Fp]) over sentence beads. The
alignment problem becomes discovering the align-
ment A that maximizes the joint distribution
P(E, Y, A) . Assuming that successive sentence
beads are generated independently, we get
</bodyText>
<equation confidence="0.9651675">
P(E , F, A) = p(L) H P([4,;
k=1
</equation>
<bodyText confidence="0.8956325">
where A = ([Epi; Fpii; ; ; .;[EpL; FpL])\
is consistent
with E and .T and where p(L) is the probability
that a corpus contains L sentence beads.
</bodyText>
<subsectionHeader confidence="0.990909">
2.2 The Basic Translation Model
</subsectionHeader>
<bodyText confidence="0.999589888888889">
For our translation model, we desire the simplest
model that incorporates lexical information effec-
tively. We describe our model in terms of a series
of increasingly complex models. In this section,
we only consider the generation of sentence beads
containing a single English sentence E = el • • • en
and single French sentence F = Ii • • fm. As a
starting point, consider a model that assumes that
all individual words are independent. We take
</bodyText>
<equation confidence="0.999846">
P({E; = P(n)P(m) P(e) p(fi)
i=1 j=1
</equation>
<bodyText confidence="0.996533090909091">
where p(n) is the probability that an English sen-
tence is n words long, p(m) is the probability that
a French sentence is m words long, p(ei) is the fre-
quency of the word ei in English, and p(f2) is the
frequency of the word h in French.
To capture the dependence between individual
English words and individual French words, we
generate English and French words in pairs in
addition to singly. For two words e and f that
are mutual translations, instead of having the two
terms p(e) and p(f) in the above equation we
would like a single term p(e, f) that is substan-
tially larger than p(e)p(f). To this end, we intro-
duce the concept of a word bead. A word bead is
either a single English word, a single French word,
or a single English word and a single French word.
We refer to these as 1:0, 0:1, and 1:1 word beads,
respectively. Instead of generating a pair of sen-
tences word by word, we generate sentences bead
by bead, using the 1:1 word beads to capture the
dependence between English and French words.
As a first cut, consider the following &amp;quot;model&amp;quot;:
</bodyText>
<equation confidence="0.579557">
P* (B) =p(l)Hp(b)
</equation>
<bodyText confidence="0.999994916666667">
where B = {b1,...,b1} is a multiset of word beads,
p(1) is the probability that an English sentence
and a French sentence contain 1 word beads, and
p(b) denotes the frequency of the word bead bi.
This simple model captures lexical dependencies
between English and French sentences.
However, this &amp;quot;model&amp;quot; does not satisfy the con-
straint that EB P*(B) = 1; because beadings B
are unordered multisets, the sum is substantially
less than one. To force this model to sum to one,
we simply normalize by a constant so that we re-
tain the qualitative aspects of the model. We take
</bodyText>
<equation confidence="0.99883">
p(1) 1
P(B) = — Hp(b)
i=1
</equation>
<bodyText confidence="0.999631444444445">
While a beading B describes an unordered mul-
tiset of English and French words, sentences are
in actuality ordered sequences of words. We need
to model word ordering, and ideally the probabil-
ity of a sentence bead should depend on the or-
dering of its component words. For example, the
sentence John de Fido should have a higher prob-
ability of aligning with the sentence Jean a mange
Fido than with the sentence Fido a mange Jean.
</bodyText>
<page confidence="0.995399">
11
</page>
<bodyText confidence="0.999124222222222">
However, modeling word order under translation
is notoriously difficult (Brown ei al., 1993), and it
is unclear how much improvement in accuracy a
good model of word order would provide. Hence,
we model word order using a uniform distribution;
we take
where the sum ranges over beadings B consistent
with the sentence bead. We use an analogous
equation for 1:2 sentence beads.
</bodyText>
<equation confidence="0.879477375">
3 Implementation
P(1)
P([E; F], = Hp(bi)
Nin!m!
which gives us
1(B)
P([E; F]) = E P(1!)m H p(b)
B ArIn** i=1
</equation>
<bodyText confidence="0.8809563125">
where B ranges over beadings consistent with
[E; F] and 1(B) denotes the number of beads in
B. Recall that n is the length of the English sen-
tence and in is the length of the French sentence.
2.3 The Complete Translation
Model
In this section, we extend the translation model
to other types of sentence beads. For simplicity,
we only consider sentence beads consisting of one
English sentence, one French sentence, one En-
glish sentence and one French sentence, two En-
glish sentences and one French sentence, and one
English sentence and two French sentences. We
refer to these as 1:0, 0:1, 1:1, 2:1, and 1:2 sentence
beads, respectively.
For 1:1 sentence beads, we take
</bodyText>
<equation confidence="0.891307">
Pi:1(1) 1(B)
P([E; FD = P1:1 E H p(b1)
B n!m!
</equation>
<bodyText confidence="0.9230615">
where B ranges over beadings consistent with
[E; F] and where p1:1 is the probability of gen-
erating a 1:1 sentence bead.
To model 1:0 sentence beads, we use a similar
equation except that we only use 1:0 word beads,
and we do not need to sum over beadings since
there is only one word beading consistent with a
1:0 sentence bead. We take
</bodyText>
<equation confidence="0.481755666666667">
rr
P([E])= p1:0 Pi:o(1)p(ei)
Aft Lon!
</equation>
<bodyText confidence="0.504414">
Notice that n = I. We use an analogous equation
for 0:1 sentence beads.
For 2:1 sentence beads, we take
</bodyText>
<equation confidence="0.723978">
Pr({El, E2; F]) = p2:1 1(B)
P2:1(0 p(b)
1V1,2:ini!n2!m!
i=1
i=1
</equation>
<bodyText confidence="0.9993012">
Due to space limitations, we cannot describe the
implementation in full detail. We present its most
significant characteristics in this section; for a
more complete discussion please refer to (Chen,
1993).
</bodyText>
<subsectionHeader confidence="0.994249">
3.1 Parameterization
</subsectionHeader>
<bodyText confidence="0.999541">
We chose to model sentence length using a Poisson
distribution, i.e., we took
</bodyText>
<equation confidence="0.963636">
P1:0(1) = 1:0
eA&amp;quot;
</equation>
<bodyText confidence="0.999918">
for some Aim, and analogously for the other types
of sentence beads. At first, we tried to estimate
each A parameter independently, but we found
that after training one or two A would be unnat-
urally small or large in order to specifically model
very short or very long sentences. To prevent this
phenomenon, we tied the A values for the different
types of sentence beads together. We took
</bodyText>
<equation confidence="0.954014333333333">
Ai:i A2:1 A1:2
A1:0 = Ao:1 = —2 = —3 =
3
</equation>
<bodyText confidence="0.999145045454546">
To model the parameters p(L) representing the
probability that the bilingual corpus is L sen-
tence beads in length, we assumed a uniform
distribution.&apos; This allows us to ignore this term,
since length will not influence the probability of
an alignment. We felt this was reasonable becau,se
it is unclear what a priori information we have on
the length of a corpus.
In modeling the frequency of word beads, notice
that there are five distinct distributions we need
to model: the distribution of 1:0 word beads in 1:0
sentence beads, the distribution of 0:1 word beads
in 0:1 sentence beads, and the distribution of all
word beads in 1:1, 2:1, and 1:2 sentence beads. To
reduce the number of independent parameters we
need to estimate, we tied these distributions to-
gether. We assumed that the distribution of word
beads in 1:1, 2:1, and 1:2 sentence beads are iden-
tical. We took the distribution of word beads in
I To be precise, we assumed a uniform distribution over
some arbitrarily large finite range, as one cannot have a
uniform distribution over a countably infinite set.
</bodyText>
<equation confidence="0.852911">
(1)
</equation>
<page confidence="0.936724">
12
</page>
<bodyText confidence="0.990939666666667">
1:0 and 0:1 sentence beads to be identical as well
except restricted to the relevant subset of word
beads and normalized appropriately, i.e., we took
</bodyText>
<equation confidence="0.6750162">
pe(e) Pb(e)
v,
Z-,e&apos;EBPbte&apos;
and
Ef,EB, pb(f
</equation>
<bodyText confidence="0.999979">
where pe refers to the distribution of word beads
in 1:0 sentence beads, pi refers to the distribu-
tion of word beads in 0:1 sentence beads, Pb refers
to the distribution of word beads in 1:1, 2:1, and
1:2 sentence beads, and Be and B1 refer to the
sets of 1:0 and 0:1 word beads in the vocabulary,
respectively.
</bodyText>
<subsectionHeader confidence="0.994952">
3.2 Evaluating the Probability of a
Sentence Bead
</subsectionHeader>
<bodyText confidence="0.999980125">
The probability of generating a 0:1 or 1:0 sentence
bead can be calculated efficiently using the equa-
tion given in Section 2.3. To evaluate the proba-
bilities of the other sentence beads requires a sum
over an exponential number of word beadings. We
make the gross approximation that this sum is
roughly equal to the maximum term in the sum.
For example, with 1:1 sentence beads we have
</bodyText>
<equation confidence="0.998969888888889">
1(B)
P1:1(1) -Fr
p(b)
i.in!rn! II
B &apos; •
1(B)
pl:lP1:1 -r-r
max{
B (1)
</equation>
<bodyText confidence="0.999794666666667">
Even with this approximation, the calculation
of PaE; FD is still intractable since it requires a
search for the most probable beading. We use a
greedy heuristic to perform this search; we are not
guaranteed to find the most probable beading. We
begin with every word in its own bead. We then
find the 0:1 bead and 1:0 bead that, when replaced
with a 1:1 word bead, results in the greatest in-
crease in probability. We repeat this process until
we can no longer find a 0:1 and 1:0 bead pair that
when replaced would increase the probability of
the beading.
</bodyText>
<subsectionHeader confidence="0.960887">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999941548387097">
We estimate parameters by using a variation of the
Viterbi version of the expectation-maximization
(EM) algorithm (Dempster et al., 1977). The
Viterbi version is used to reduce computational
complexity. We use an incremental variation of the
algorithm to reduce the number of passes through
the corpus required.
In the EM algorithm, an expectation phase,
where counts on the corpus are taken using the
current estimates of the parameters, is alternated
with a maximization phase, where parameters are
re-estimated based on the counts just taken. Im-
proved parameters lead to improved counts which
lead to even more accurate parameters. In the in-
cremental version of the EM algorithm we use, in-
stead of re-estimating parameters after each com-
plete pass through the corpus, we re-estimate pa-
rameters after each sentence. By re-estimating pa-
rameters continually as we take counts on the cor-
pus, we can align later sections of the corpus more
reliably based on alignments of earlier sections.
We can align a corpus with only a single pass, si-
multaneously producing alignments and updating
the model as we proceed.
More specifically, we initialize parameters by
taking counts on a small body of previously
aligned data. To estimate word bead frequencies,
we maintain a count c(b) for each word bead that
records the number of times the word bead b oc-
curs in the most probable word beading of a sen-
tence bead. We take
</bodyText>
<equation confidence="0.968818666666667">
c(b)
NO) =
0(10
</equation>
<bodyText confidence="0.999973619047619">
We initialize the counts c(b) to 1 for 0:1 and 1:0
word beads, so that these beads can occur in bead-
ings with nonzero probability. To enable 1:1 word
beads to occur in beadings with nonzero probabil-
ity, we initialize their counts to a small value when-
ever we see the corresponding 0:1 and 1:0 word
beads occur in the most probable word beading of
a sentence bead.
To estimate the sentence length parameters A,
we divide the number of word beads in the most
probable beading of the initial training data by
the total number of sentences. This gives us an
estimate for A1,0 , and the other A parameters can
be calculated using equation (1).
We have found that one hundred sentence pairs
are sufficient to train the model to a state where it
can align adequately. At this point, we can process
unaligned text and use the alignments we produce
to further train the model. We update parameters
based on the newly aligned text in the same way
that we update parameters based on the initial
</bodyText>
<equation confidence="0.98232025">
for e E Be
Pb(f)
Pi = for f E Bi
P([E; FJ) = P1:1
</equation>
<page confidence="0.94442">
13
</page>
<bodyText confidence="0.9802852">
training data.2
To align a corpus in a single pass the model
must be fairly accurate before starting or else the
beginning of the corpus will be poorly aligned.
Hence, after bootstrapping the model on one hun-
dred sentence pairs, we train the algorithm on a
chunk of the unaligned target bilingual corpus,
typically 20,000 sentence pairs, before making one
pass through the entire corpus to produce the ac-
tual alignment.
</bodyText>
<subsectionHeader confidence="0.997636">
3.4 Search
</subsectionHeader>
<bodyText confidence="0.999994125">
It is natural to use dynamic programming to
search for the best alignment; one can find the
most probable of an exponential number of align-
ments using quadratic time and memory. Align-
ment can be viewed as a &amp;quot;shortest distance&amp;quot; prob-
lem, where the &amp;quot;distance&amp;quot; associated with a sen-
tence bead is the negative logarithm of its proba-
bility. The probability of an alignment is inversely
related to the sum of the distances associated with
its component sentence beads.
Given the size of existing bilingual corpora and
the computation necessary to evaluate the proba-
bility of a sentence bead, a quadratic algorithm is
still too profligate. However, most alignments are
one-to-one, so we can reap great benefits through
intelligent thresholding. By considering only a
subset of all possible alignments, we reduce the
computation to a linear one.
Dynamic programming consists of incrementally
finding the best alignment of longer and longer
prefixes of the bilingual corpus. We prune all
alignment prefixes that have a substantially lower
probability than the most probable alignment pre-
fix of the same length.
</bodyText>
<footnote confidence="0.687925714285714">
2 Ln theory, one cannot decide whether a particular sen-
tence bead belongs to the best alignment of a corpus un-
til the whole corpus has been processed. In practice, some
partial alignments will have much higher probabilities than
all other alignments, and it is desirable to train on these
partial alignments to aid in aligning later sections of the
corpus. To decide when it is reasonably safe to train on a
particular sentence bead, we take advantage of the thresh-
olding described in Section 3.4, where improbable partial
alignments are discarded. At a given point in time in align-
ing a corpus, all undiscarded partial alignments will have
some sentence beads in common. When a sentence bead is
common to all active partial alignments, we consider it to
be safe to train on.
</footnote>
<subsectionHeader confidence="0.975952">
3.5 Deletion Identification
</subsectionHeader>
<bodyText confidence="0.999978297297297">
Deletions are automatically handled within the
standard dynamic programming framework. How-
ever, because of thresholding, we must handle
large deletions using a separate mechanism.
Because lexical information is used, correct
alignments receive vastly greater probabilities
than incorrect alignments. Consequently, thresh-
olding is generally very aggressive and our search
beam in the dynamic programming array is nar-
row. However, when there is a large deletion in
one of the parallel corpora, consistent lexical cor-
respondences disappear so no one alignment has
a much higher probability than the others and
our search beam becomes wide. When the search
beam reaches a certain width, we take this to in-
dicate the beginning of a deletion.
To identify the end of a deletion, we search lin-
early through both corpora simultaneously. All
occurrences of words whose frequency is below a
certain value are recorded in a hash table. When-
ever we notice the occurrence of a rare word in
one corpus and its translation in the other, we
take this as a candidate location for the end of the
deletion. For each candidate location, we exam-
ine the forty sentences following the occurrence of
the rare word in each of the two parallel corpora.
We use dynamic programming to find the prob-
ability of the best alignment of these two blocks
of sentences. If this probability is sufficiently high
we take the candidate location to be the end of
the deletion. Because it is extremely unlikely that
there are two very similar sets of forty sentences
in a corpus, this deletion identification algorithm
is robust. In addition, because we key off of rare
words in considering ending points, deletion iden-
tification requires time linear in the length of the
deletion.
</bodyText>
<sectionHeader confidence="0.999978" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9997878">
Using this algorithm, we have aligned three large
English/French corpora. We have aligned a cor-
pus of 3,000,000 sentences (of both English and
French) of the Canadian Hansards, a corpus of
1,000,000 sentences of newer Hansard proceedings,
and a corpus of 2,000,000 sentences of proceed-
ings from the European Economic Community. In
each case, we first bootstrapped the translation
model by training on 100 previously aligned sen-
tence pairs. We then trained the model further on
</bodyText>
<page confidence="0.996669">
14
</page>
<bodyText confidence="0.999979745098039">
20,000 sentences of the target corpus. Note that
these 20,000 sentences were not previously aligned.
Because of the very low error rates involved, in-
stead of direct sampling we decided to estimate
the error of the old Hansard corpus through com-
parison with the alignment found by Brown of the
same corpus. We manually inspected over 500 lo-
cations where the two alignments differed to esti-
mate our error rate on the alignments disagreed
upon. Taking the error rate of the Brown align-
ment to be 0.6%, we estimated the overall error
rate of our alignment to be 0.4%.
In addition, in the Brown alignment approxi-
mately 10% of the corpus was discarded because
of indications that it would be difficult to align.
Their error rate of 0.6% holds on the remaining
sentences. Our error rate of 0.4% holds on the
entire corpus. Gale reports an approximate error
rate of 2% on a different body of Hansard data
with no discarding, and an error rate of 0.4% if
20% of the sentences can be discarded.
Hence, with our algorithm we can achieve at
least as high accuracy as the Brown and Gale algo-
rithms without discarding any data. This is espe-
cially significant since, presumably, the sentences
discarded by the Brown and Gale algorithms are
those sentences most difficult to align.
In addition, the errors made by our algorithm
are generally of a fairly trivial nature. We ran-
domly sampled 300 alignments from the newer
Hansard corpus. The two errors we found are
displayed in Figures 3 and 4. In the first error,
Ei was aligned with F1 and E2 was aligned with
F2. The correct alignment maps El and E2 to F1
and F2 to nothing. In the second error, E1 was
aligned with F1 and F2 was aligned to nothing.
Both of these errors could have been avoided with
improved sentence boundary detection. Because
length-based alignment algorithms ignore lexical
information, their errors can be of a more spec-
tacular nature.
The rate of alignment ranged from 2,000 to
5,000 sentences of both English and French per
hour on an IBM RS/6000 53011 workstation. The
alignment algorithm lends itself well to paralleliza-
tion; we can use the deletion identification mecha-
nism to automatically identify locations where we
can subdivide a bilingual corpus. While it required
on the order of 500 machine-hours to align the
newer Hansard corpus, it took only 1.5 days of
real time to complete the job on fifteen machines.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999993095238095">
We have described an accurate, robust, and fast
algorithm for sentence alignment. The algorithm
can handle large deletions in text, it is language
independent, and it is parallelizable. It requires
a minimum of human intervention; for each lan-
guage pair 100 sentences need to be aligned by
hand to bootstrap the translation model.
The use of lexical information requires a great
computational cost. Even with numerous approxi-
mations, this algorithm is tens of times slower than
the Brown and Gale algorithms. This is acceptable
given that alignment is a one-time cost and given
available computing power. It is unclear, though,
how much further it is worthwhile to proceed.
The natural next step in sentence alignment is
to account for word ordering in the translation
model, e.g., the models described in (Brown et
al., 1993) could be used. However, substantially
greater computing power is required before these
approaches can become practical, and there is not
much room for further improvements in accuracy.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843857142857">
(Bellman, 1957) Richard Bellman. Dynamic Pro-
gramming. Princeton University Press, Princeton
N.J., 1957.
(Brown et al., 1990) Peter F. Brown, John Cocke,
Stephen A. DellaPietra, Vincent J. DellaPietra,
Frederick Jelinek, John D. Lafferty, Robert L.
Mercer, and Paul S. Roossin. A statistical ap-
proach to machine translation. Computational
Linguistics, 16(2):79-85, June 1990.
(Brown et al., 1991a) Peter F. Brown, Stephen A.
DellaPietra, Vincent J. DellaPietra, and Ro-
bert L. Mercer. Word sense disambiguation using
statistical methods. In Proceedings 29th Annu-
al Meeting of the ACL, pages 265-270, Berkeley,
CA, June 1991.
(Brown et al., 1991b) Peter F. Brown, Jennifer C.
Lai, and Robert L. Mercer. Aligning sentences
in parallel corpora. In Proceedings 29th Annual
Meeting of the ACL, pages 169-176, Berkeley,
CA, June 1991.
(Brown et al., 1993) Peter F. Brown, Stephen A. Del-
laPietra, Vincent J. DellaPietra, and Robert L.
Mercer. The mathematics of machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 1993. To appear.
(Catizone et al., 1989) Roberta Catizone, Graham
Russell, and Susan Warwick. Deriving transla-
tion data from bilingual texts. In Proceedings
</reference>
<page confidence="0.996356">
15
</page>
<bodyText confidence="0.538004333333333">
E4 If there is some evidence Si on peut prouver que elle
that it ... and I will see je verrais a ce que
that it does. elle se y conforme. \SCM11
</bodyText>
<note confidence="0.8265275">
E72 \SCM{} Translation \ECK} Language = French \ECM{}
F2 \SCM{} Paragraph \ECK}
</note>
<figureCaption confidence="0.993048166666667">
Figure 3: An Alignment Error
Motion No. 22 that Bill
C-84 be amended in ... and
substituting the following
therefor : second
anniversary of.
</figureCaption>
<bodyText confidence="0.665604">
F1 Motion No 22 que on modifie
le projet de loi C-84 ...
et en la remplagant par ce
</bodyText>
<figure confidence="0.432992">
qui suit : 18.
F2 Deux ans apres :
</figure>
<figureCaption confidence="0.991657">
Figure 4: Another Alignment Error
</figureCaption>
<reference confidence="0.988119846153846">
bilingual lexicography. In E URA LEX 4th Inter-
national Congress, Malaga, Spain, 1990.
of the First International Acquisition Workshop,
Detroit, Michigan, August 1989.
(Chen, 1993) Stanley F. Chen. Aligning sentences in
bilingual corpora using lexical information. Tech-
nical Report TR-12-93, Harvard University, 1993.
(Dagan et al., 1991) Ido Dagan, Alon Itai, and Ul-
rike Schwan. Two languages are more informa-
tive than one. In Proceedings of the 29th Annual
Meeting of the ACL, pages 130-137, 1991.
(Dempster et al., 1977) A.P. Dempster, N.M. Laird,
and D.B. Rubin. Maximum likelihood from in-
complete data via the EM algorithm. Journal of
the Royal Statistical Society, 39(B):1-38, 1977.
(Gale and Church, 1991) William A. Gale and Ken-
neth W. Church. A program for aligning sen-
tences in bilingual corpora. In Proceedings of the
29th Annual Meeting of the ACL, Berkeley, Cali-
fornia, June 1991.
(Gale et al., 1992) William A. Gale, Kenneth W.
Church, and David Yarowsky. Using bilingual
materials to develop word sense disambiguation
methods. In Proceedings of the Fourth Interna-
tional Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, pages 101-
112, Montréal, Canada, June 1992.
(Kay, 1991) Martin Kay. Text-translation alignment.
In ACH/ALLC &apos;91: &amp;quot;Making Connections&amp;quot; Con-
ference Handbook, Tempe, Arizona, March 1991.
(Klavans and Tzoukermann, 1990) Judith Klavans
and Evelyne Tzoukermann. The bicord system.
In COLING-90, pages 174-179, Helsinki, Fin-
land, August 1990.
(Sadler, 1989) V. Sadler. The Bilingual Knowledge
Bank - A New Conceptual Basis for MT.
BSO/Research, Utrecht, 1989.
(Warwick and Russell, 1990) Susan Warwick and
Graham Russell. Bilingual concordancing and
</reference>
<page confidence="0.998701">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723630">
<title confidence="0.994831">ALIGNING SENTENCES IN BILINGUAL CORPORA USI\ LEXICAL INFORMATION</title>
<author confidence="0.998766">Stanley F Chen</author>
<affiliation confidence="0.999944666666667">Aiken Computation Laboratory Division of Applied Sciences Harvard University</affiliation>
<address confidence="0.999997">Cambridge, MA 02138</address>
<email confidence="0.997962">Internet:sfc@calliope.harvard.edu</email>
<abstract confidence="0.981724066666667">In this paper, we describe a fast algorithm for aligning sentences with their translations in a bilingual corpus. Existing efficient algorithms ignore word identities and only consider sentence (Brown al., Gale and Church, 1991). Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment. We find the alignment that maximizes the probability of generating the corpus with this translation model. We have achieved an error rate of approximately 0.4% on Canadian Hansard data, which is a significant improvement over previous results. The algorithm is language independent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Bellman</author>
</authors>
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton N.J.,</location>
<marker>(Bellman, 1957)</marker>
<rawString>Richard Bellman. Dynamic Programming. Princeton University Press, Princeton N.J., 1957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Frederick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<marker>(Brown et al., 1990)</marker>
<rawString>Peter F. Brown, John Cocke, Stephen A. DellaPietra, Vincent J. DellaPietra, Frederick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. A statistical approach to machine translation. Computational Linguistics, 16(2):79-85, June 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>Word sense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings 29th Annual Meeting of the ACL,</booktitle>
<pages>265--270</pages>
<location>Berkeley, CA,</location>
<marker>(Brown et al., 1991a)</marker>
<rawString>Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. Word sense disambiguation using statistical methods. In Proceedings 29th Annual Meeting of the ACL, pages 265-270, Berkeley, CA, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In Proceedings 29th Annual Meeting of the ACL,</booktitle>
<pages>169--176</pages>
<location>Berkeley, CA,</location>
<marker>(Brown et al., 1991b)</marker>
<rawString>Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer. Aligning sentences in parallel corpora. In Proceedings 29th Annual Meeting of the ACL, pages 169-176, Berkeley, CA, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A DellaPietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<note>To appear.</note>
<marker>(Brown et al., 1993)</marker>
<rawString>Peter F. Brown, Stephen A. DellaPietra, Vincent J. DellaPietra, and Robert L. Mercer. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 1993. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberta Catizone</author>
<author>Graham Russell</author>
<author>Susan Warwick</author>
</authors>
<title>Deriving translation data from bilingual texts.</title>
<date>1990</date>
<booktitle>In Proceedings bilingual lexicography. In E URA LEX 4th International Congress,</booktitle>
<location>Malaga,</location>
<marker>(Catizone et al., 1989)</marker>
<rawString>Roberta Catizone, Graham Russell, and Susan Warwick. Deriving translation data from bilingual texts. In Proceedings bilingual lexicography. In E URA LEX 4th International Congress, Malaga, Spain, 1990. of the First International Acquisition Workshop, Detroit, Michigan, August 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<tech>Technical Report TR-12-93,</tech>
<institution>Harvard University,</institution>
<marker>(Chen, 1993)</marker>
<rawString>Stanley F. Chen. Aligning sentences in bilingual corpora using lexical information. Technical Report TR-12-93, Harvard University, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
<author>Ulrike Schwan</author>
</authors>
<title>Two languages are more informative than one.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL,</booktitle>
<pages>130--137</pages>
<marker>(Dagan et al., 1991)</marker>
<rawString>Ido Dagan, Alon Itai, and Ulrike Schwan. Two languages are more informative than one. In Proceedings of the 29th Annual Meeting of the ACL, pages 130-137, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>(Dempster et al., 1977)</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the ACL,</booktitle>
<location>Berkeley, California,</location>
<marker>(Gale and Church, 1991)</marker>
<rawString>William A. Gale and Kenneth W. Church. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting of the ACL, Berkeley, California, June 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Using bilingual materials to develop word sense disambiguation methods.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<pages>101--112</pages>
<location>Montréal, Canada,</location>
<marker>(Gale et al., 1992)</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. Using bilingual materials to develop word sense disambiguation methods. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation, pages 101-112, Montréal, Canada, June 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Text-translation alignment.</title>
<date>1991</date>
<booktitle>In ACH/ALLC &apos;91: &amp;quot;Making Connections&amp;quot; Conference Handbook,</booktitle>
<location>Tempe, Arizona,</location>
<marker>(Kay, 1991)</marker>
<rawString>Martin Kay. Text-translation alignment. In ACH/ALLC &apos;91: &amp;quot;Making Connections&amp;quot; Conference Handbook, Tempe, Arizona, March 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Klavans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>The bicord system.</title>
<date>1990</date>
<booktitle>In COLING-90,</booktitle>
<pages>174--179</pages>
<location>Helsinki, Finland,</location>
<marker>(Klavans and Tzoukermann, 1990)</marker>
<rawString>Judith Klavans and Evelyne Tzoukermann. The bicord system. In COLING-90, pages 174-179, Helsinki, Finland, August 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
</authors>
<title>The Bilingual Knowledge Bank - A New Conceptual Basis for MT. BSO/Research,</title>
<date>1989</date>
<location>Utrecht,</location>
<marker>(Sadler, 1989)</marker>
<rawString>V. Sadler. The Bilingual Knowledge Bank - A New Conceptual Basis for MT. BSO/Research, Utrecht, 1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Susan Warwick</author>
<author>Graham Russell</author>
</authors>
<note>Bilingual concordancing and</note>
<marker>(Warwick and Russell, 1990)</marker>
<rawString>Susan Warwick and Graham Russell. Bilingual concordancing and</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>