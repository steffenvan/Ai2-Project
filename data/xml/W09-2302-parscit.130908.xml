<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998417">
Statistical Phrase Alignment Model
Using Dependency Relation Probability
</title>
<author confidence="0.984336">
Toshiaki Nakazawa Sadao Kurohashi
</author>
<affiliation confidence="0.989778">
Graduate School of Informatics, Kyoto University
</affiliation>
<address confidence="0.8030695">
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
</address>
<email confidence="0.998878">
nakazawa@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999617631578948">
When aligning very different language pairs,
the most important needs are the use of struc-
tural information and the capability of gen-
erating one-to-many or many-to-many corre-
spondences. In this paper, we propose a
novel phrase alignment method which models
word or phrase dependency relations in depen-
dency tree structures of source and target lan-
guages. The dependency relation model is a
kind of tree-based reordering model, and can
handle non-local reorderings which sequen-
tial word-based models often cannot handle
properly. The model is also capable of esti-
mating phrase correspondences automatically
without any heuristic rules. Experimental re-
sults of alignment show that our model could
achieve F-measure 1.7 points higher than the
conventional word alignment model with sym-
metrization algorithms.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974214285714">
We consider that there are two important needs in
aligning parallel sentences written in very differ-
ent languages such as Japanese and English. One
is to adopt structural or dependency analysis into
the alignment process to overcome the difference in
word order. The other is that the method needs to
have the capability of generating phrase correspon-
dences, that is, one-to-many or many-to-many word
correspondences. Most existing alignment methods
simply consider a sentence as a sequence of words
(Brown et al., 1993), and generate phrase correspon-
dences using heuristic rules (Koehn et al., 2003).
Some studies incorporate structural information into
the alignment process after this simple word align-
</bodyText>
<page confidence="0.968909">
10
</page>
<bodyText confidence="0.999201176470588">
ment (Quirk et al., 2005; Cowan et al., 2006). How-
ever, this is not sufficient because the basic word
alignment itself is not good.
On the other hand, a few models have been pro-
posed which use structural information from the be-
ginning of the alignment process. Watanabe et al.
(2000) and Menezes and Richardson (2001) pro-
posed a structural alignment methods. These meth-
ods use heuristic rules when resolving correspon-
dence ambiguities. Yamada and Knight (2001) and
Gildea (2003) proposed a tree-based probabilistic
alignment methods. These methods reorder, insert
or delete sub-trees on one side to reproduce the other
side, but the constraints of using syntactic informa-
tion is often too rigid. Yamada and Knight flat-
tened the trees by collapsing nodes. Gildea cloned
sub-trees to deal with the problem. Cherry and Lin
(2003) proposed a model which uses a source side
dependency tree structure and constructs a discrim-
inative model. However, there is the defect that its
alignment unit is a word, so it can only find one-
to-one alignments. Nakazawa and Kurohashi (2008)
also proposed a model focusing on the dependency
relations. Their model has the constraint that content
words can only correspond to content words on the
other side, and the same applies for function words.
This sometimes leads to an incorrect alignment. We
have removed this constraint to make more flexi-
ble alignments possible. Moreover, in their model,
some function words are brought together, and thus
they cannot handle the situation where each func-
tion word corresponds to a different part. The small-
est unit of our model is a single word, which should
solve this problem.
</bodyText>
<note confidence="0.965072">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 10–18,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999844684210527">
In this paper, we propose a novel phrase align-
ment method which models word or phrase de-
pendency relations in dependency tree structures of
source and target languages. For a pair of correspon-
dences which has a parent-child relation on one side,
the dependency relation on the other side is defined
as the relation between the two correspondences.
It is a kind of tree-based reordering model, and
can capture non-local reorderings which sequential
word-based models often cannot handle properly.
The model is also capable of estimating phrase cor-
respondences automatically without heuristic rules.
The model is trained in two steps: Step 1 estimates
word translation probabilities, and Step 2 estimates
phrase translation probabilities and dependency re-
lation probabilities. Both Step 1 and Step 2 are per-
formed iteratively by the EM algorithm. During the
Step 2 iterations, word correspondences are grown
into phrase correspondences.
</bodyText>
<sectionHeader confidence="0.995521" genericHeader="method">
2 Proposed Model
</sectionHeader>
<bodyText confidence="0.9999503125">
We suppose that Japanese is the source language and
English is the target language in the description of
our model. Note that the model is not specialized
for this language pair, and it can be applied to any
language pair.
Because our model uses dependency tree struc-
tures, both source and target sentences are parsed
beforehand. Japanese sentences are converted into
dependency structures using the morphological ana-
lyzer JUMAN (Kurohashi et al., 1994), and the de-
pendency analyzer KNP (Kawahara and Kurohashi,
2006). MSTparser (McDonald et al., 2005) is used
to convert English sentences. Figure 1 shows an ex-
ample of dependency structures. The root of a tree is
placed at the extreme left and words are placed from
top to bottom.
</bodyText>
<subsectionHeader confidence="0.82502">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999088">
This section outlines our proposed model in compar-
ison to the IBM models, which are the conventional
statistical alignment models.
In the IBM models (Brown et al., 1993), the best
alignment aˆ between a given source sentence f and
its target sentence e is acquired by the following
equation:
</bodyText>
<equation confidence="0.9035625">
a� = argmax p(f, a|e)
a
= argmax p(f|e, a) - p(a|e) (1)
a
</equation>
<figureCaption confidence="0.997298">
Figure 1: An example of a dependency tree and its align-
ment.
</figureCaption>
<bodyText confidence="0.996867125">
where p(f|e,a) is called lexicon probability and
p(a|e) is called alignment probability.
Suppose f consists of n words f1, f2,..., fn, and e
consists of m words e1, e2,..., en and a NULL word
(e0). The alignment mapping a consists of associa-
tions j —* i = aj from source position j to target
position i = aj. The two probabilities above are
broken down as:
</bodyText>
<equation confidence="0.9966196">
J
p(f|e, a) = p(fj|ead) (2)
j�1
I
z�1
</equation>
<bodyText confidence="0.997150785714286">
where Aj is a relative position of words in the
source side which corresponds to ez. Equation 2 is
the product of the word translation probabilities, and
Equation 3 is the product of relative position proba-
bilities.
In the proposed model, we refine the IBM models
in three ways. First, as for Equation 2, we consider
phrases instead of words. Second, as for Equation 3,
we consider dependencies of words instead of their
positions in a sentence.
Finally, the proposed model can find the best
alignment a� by not using f-to-e alone, but simulta-
neously with e-to-f. That is, Equation 1 is modified
as follows:
</bodyText>
<equation confidence="0.998809666666667">
a� = argmax p(f|e, a) - p(a|e) -
a
p(e|f, a) - p(a|f) (4)
</equation>
<bodyText confidence="0.9873535">
Since our model regards a phrase as a basic unit,
the above formula is calculated in a straightforward
way. In contrast, the IBM models can consider
a many-to-one alignment by combining one-to-one
</bodyText>
<figure confidence="0.978961615384615">
Nk
(device)
A-T
(light)
.46
(accept)
A
photogate
I;:
is
(ni)
(ha)
lj:
7-th
used
for
(photo)
—h
the
(gate)
�
photodetector
(wo)
Mot_-
(used)
�
</figure>
<equation confidence="0.769667">
p(a|e) =
p(Aj|ez) (3)
</equation>
<page confidence="0.994389">
11
</page>
<bodyText confidence="0.985750285714286">
alignments, but they cannot consider a one-to-many
or many-to-many alignment.
The models are estimated by EM-like algorithm
which is very similar to (Liang et al., 2006). The
important difference is that we are using tree struc-
tures.
We maximize the data likelihood:
</bodyText>
<equation confidence="0.9399456">
�
max (log pef(f, e; Bef) + log pfe(f, e; Bfe))
θef,θfe
f,e
(5)
</equation>
<bodyText confidence="0.948737">
In the E-step, we compute the posterior distribution
of the alignments with the current parameter B:
</bodyText>
<equation confidence="0.983505555555556">
q(a; f, e) pef(a|f, e; Bef) · pfe(a|f, e; Bfe) (6)
In the M-step, we update the parameter B:
EB&apos; := argmax
θ a,f,e q(a; f, e) log pef(a, f, e; Bef)
+ E q(a; f, e) logpfe(a, f, e; Bfe)
a,f,e
q(a; f, e) logp(e) - pef(a, f|e; Bef)
q(a; f, e) logp(f) - pfe(a, e|f; Bfe)
(7)
</equation>
<bodyText confidence="0.9999144">
Note that p(e) and p(f) have no effect on maxi-
mization, and pef(a, f|e; Bef) and pfe(a, e|f; Bfe)
appeared in Equation 1 or Equation 4.
In the following sections, we decompose the lexi-
con probability and alignment probability.
</bodyText>
<subsectionHeader confidence="0.998523">
2.2 Phrase Translation Probability
</subsectionHeader>
<bodyText confidence="0.999840384615384">
Suppose f consists of N phrases F1, F2,..., FN, and
e consists of M phrases E1, E2,..., EM. The align-
ment mapping a consists of associations j → i =
Aj from source phrase j to target phrase i = Aj.
We consider phrase translation probability
p(Fj|Ei) instead of word translation probability.
There is one restriction: that phrases composed of
more than one word cannot be aligned to NULL.
Only a single word can be aligned to NULL.
We denote a phrase which the word fj belongs to
as Fs(j), and a phrase which the word ei belongs to
as Et(i). With these notations, we refine Equation 2
as follows:
</bodyText>
<equation confidence="0.945252333333333">
J
p(f|e, a) = p(Fs(j)|EAs(j)) (8)
j�1
</equation>
<bodyText confidence="0.999038">
Suppose phrase Fj and Ei are aligned where the
number of words in Fj is denoted by |Fj |and that
number in Ei is |Ei|, the probability mass related to
this alignment in Equation 8 is as follows:
</bodyText>
<equation confidence="0.942416">
p(Fj|Ei)|Fj|· p(Ei|Fj)|Ei |(9)
</equation>
<bodyText confidence="0.99980125">
We call this probability for the link between Fj and
Ei phrase alignment probability. The upper part of
Table 1 shows phrase alignment probabilities for the
alignment in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997165">
2.3 Dependency Relation Probability
</subsectionHeader>
<bodyText confidence="0.964584277777778">
The reordering model in the IBM Models is defined
on the relative position between an alignment and
its previous alignment, as shown in Equation 3. Our
model, on the other hand, considers dependencies of
words instead of positional relations.
We start with a dependency relation where fc de-
pends on fp in the source sentence. In a possible
alignment, fc belongs to Fs(c), fp belongs to Fs(p),
and Fs(c) depends on Fs(p). In this situation, we con-
sider the relation between EAs(p) and EAs(c). Even
if two languages have different word order, their de-
pendency structures are similar in many cases, and
EAs(c) tends to depend on EAs(p). Our model takes
this tendency into consideration. In order to de-
note the relationship between phrases, we introduce
rel(EAs(p), EAs(c)). This is defined as the path from
EAs(p) to EAs(c). It is represented by applying the
notations below:
</bodyText>
<listItem confidence="0.9992265">
• ’c’ if going down to the child node
• ’p’ if going down to the parent node
</listItem>
<bodyText confidence="0.995505142857143">
For example, in Figure 1, the path from “for” to
“photodetector” is ’c’, from “the” to “for” is ’p;p’
because it travels across two nodes. All the phrases
are considered as a single node, so the path from
“photogate” to “the” is ’p;c;c;c’ with the alignment
in Figure 1.
We refine Equation 3 using rel as follows:
</bodyText>
<equation confidence="0.994916">
p(a|e) = � pt(rel(EAs(p), EAs(c))|pc)
(p,c)EDs-pc
(10)
</equation>
<bodyText confidence="0.9996062">
where Ds-pc denotes a set of parent-child
word pairs in the source sentence. We call
pt(rel(EAs(p), EAs(c))|pc) target side dependency
relation probability. pt is a kind of tree-based
reordering model.
</bodyText>
<figure confidence="0.7121452">
E= argmax
θ a,f,e
E
+
a,f,e
</figure>
<page confidence="0.975157">
12
</page>
<tableCaption confidence="0.997065">
Table 1: A probability calculation example.
</tableCaption>
<table confidence="0.654273833333333">
Source Target Phrase alignment probability
NULL photodetector p( |photodetector)3 · p(photodetector |) )2
for p( |for)2 · p(for |)
photogate p( |a photogate)2 · p(a photogate|
is used p( |is used)2 · p(is used |)2
the p(the|NULL)
</table>
<figure confidence="0.990485148148148">
relation probability c
pt(SAME|pc) A
pt(SAME|pc) photogate
pt(c|pc) used
pt(SAME|pc) for
pt(c|pc) the
pt(SAME|pc) photodetector
p
photogate
is
is
used
photodetector
for
Target dependency Target
pt(c|pc)
pt(SAME|pc)
Source dependency
relation probability
ps(SAME|pc)
ps(c|pc)
ps(SAME|pc)
ps(c|pc)
ps(NULL c|pc)
ps(c|pc)
Source
c p
</figure>
<bodyText confidence="0.999617333333333">
There are some special cases for rel. When Fs(c)
and Fs(p) are the same, that is, fc and fp belong
to the same phrase, rel is represented as ’SAME’.
When fp is aligned to NULL, fc is aligned to NULL,
and both of them are aligned to NULL, rel is repre-
sented as ’NULL p’, ’NULL c’, and ’NULL b’, re-
spectively. The lower part of Table 1 shows depen-
dency relation probabilities corresponding to Figure
1.
Actually, we extend the dependency relation
probability to consider a wider relation, i.e, the
grandparent-child relation, as follows:
</bodyText>
<equation confidence="0.976961333333333">
�
p(a|e) =
(p,c)EDs-pc
ri pt(rel(EAs(g), EAs(c))|gc)
(g,c)EDs-gc
(11)
</equation>
<bodyText confidence="0.999988">
where Ds-gc denotes a set of grandparent-child word
pairs in the source sentence.
</bodyText>
<sectionHeader confidence="0.925946" genericHeader="method">
3 Model Training
</sectionHeader>
<bodyText confidence="0.999979666666667">
Our model is trained in two steps. In Step 1, word
translation probability is estimated. Then, in Step 2,
possible phrases are acquired, and both phrase trans-
lation probability and dependency relation probabil-
ity are estimated. In both steps, parameter estima-
tion is done with the EM algorithm.
</bodyText>
<subsectionHeader confidence="0.991595">
3.1 Step 1
</subsectionHeader>
<bodyText confidence="0.998781222222222">
In Step 1, word translation probability in each di-
rection is estimated independently. This is done in
exactly the same way as in IBM Model 1.
In this process, the alignment unit is a word.
When we consider f-to-e alignment, each word on
the source side fj can correspond to a word on the
target side ei or a NULL word, independently of
other source words. The probability of one possible
alignment a is calculated as follows:
</bodyText>
<equation confidence="0.992305">
J
p(a, f|e) = p(fj|eaj) (12)
j=1
</equation>
<bodyText confidence="0.595627">
By considering all possible alignments, p(f|e) is
calculated as:
</bodyText>
<equation confidence="0.986303">
p(a,f|e) (13)
</equation>
<bodyText confidence="0.996415333333333">
As initial parameters of p(f|e), we use uniform
probabilities. Then, after calculating Equation 12
and 13, we give the fractional count p(a,f|e)
</bodyText>
<equation confidence="0.622251">
p(f|e) to all
</equation>
<bodyText confidence="0.98616475">
word alignments in a, and we estimate p(f|e) by
MLE. We perform this estimation iteratively.
The inverse model e-to-f can be calculated in the
same manner.
</bodyText>
<subsectionHeader confidence="0.989925">
3.2 Step 2
</subsectionHeader>
<bodyText confidence="0.999880375">
Both phrase translation probability and dependency
relation probability are estimated, and one undi-
rected alignment is found using the e-to-f and f-to-e
probabilities simultaneously in this step. In contrast
to Step 1, it is impossible to enumerate all the possi-
ble alignments. To find the best alignment, we first
create an initial alignment based on phrase trans-
lation probability only, and then gradually revise it
</bodyText>
<figure confidence="0.903926">
pt(rel(EAs(p), EAs(c))|pc) · �
p(f|e) =
a
</figure>
<page confidence="0.993025">
13
</page>
<bodyText confidence="0.999828">
by considering the dependency relation probability
with a hill-climbing algorithm.
The initial parameters of Step 2 are calculated
as follows. The dependency relation probability is
calculated using the final alignment result of Step
1, and we use the word translation probability esti-
mated in Step 1 as the initial phrase translation prob-
ability.
</bodyText>
<subsubsectionHeader confidence="0.451559">
3.2.1 Initial Alignment
</subsubsectionHeader>
<bodyText confidence="0.999969090909091">
We first create an initial alignment based on the
phrase translation probability without considering
the dependency relation probabilities.
For all the combinations of possible phrases
(including NULL), phrase alignment probabilities
are calculated (equation 9). Correspondences are
adopted one by one in descending order of geomet-
ric mean of the phrase alignment probabilities. All
the words should be aligned only once, that is, the
correspondences are adopted exclusively. Genera-
tion of possible phrases is explained in Section 3.2.3.
</bodyText>
<subsectionHeader confidence="0.971614">
3.2.2 Hill-climbing
</subsectionHeader>
<bodyText confidence="0.946593038461539">
To find better alignments, the initial alignment is
gradually revised with a hill-climbing algorithm. We
use four kinds of revising operations:
Swap: Focusing on any two correspondences, the
partners are swapped. In the first step in
Figure2, the correspondences “ ↔ photo-
gate” and “ ↔ photodetector” are
swapped to “ ↔ photodetector” and “
↔ photogate”.
Extend: Focusing on one correspondence, the
source or target phrase is extended to include
its neighboring (parent or child) NULL-aligned
word.
Add: A new correspondence is added between a
source word and a target word both of which
are aligned to NULL.
Reject: A correspondence is rejected and the source
and target phrase are aligned to NULL.
Figure 2 shows an illustrative example of hill
climbing. The alignment is revised only if the align-
ment probability gets increased. It is repeated un-
til no operation can improve the alignment probabil-
ity, and the final state is the best approximate align-
ment. As a by-product of hill-climbing, pseudo n-
best alignment can be acquired. It is used in collect-
ing fractional counts.
</bodyText>
<subsectionHeader confidence="0.737486">
3.2.3 Phrase Generation
</subsectionHeader>
<bodyText confidence="0.990298076923077">
If there is a word which is aligned to NULL in the
best approximate alignment, a new possible phrase
is generated by merging the word into a neighbor-
ing phrase which is not aligned to NULL. In the last
alignment result in Figure 2, for example, “ ”
is treated as being included in the correspondence
between “ ” and “photodetector” and the cor-
respondence between “ ” and “for”. As a result,
we consider the correspondence between “
” and “photodetector” and the correspondence be-
tween “ ” and “for” existing in parallel sen-
tences. The new possible phrase is taken into con-
sideration from the next iteration.
</bodyText>
<subsubsectionHeader confidence="0.509963">
3.2.4 Model Estimation
</subsubsectionHeader>
<bodyText confidence="0.9989165">
Collecting all the alignment results, we estimate
phrase alignment probabilities and dependency rela-
tion probabilities.
One way of estimating parameters of phrase
alignment probabilities is using the following equa-
tions:
</bodyText>
<equation confidence="0.9960756">
C(Fj, Ei)
p(Fj|Ei) =
C(Fj, Ei)
p(Ei|Fj) = (14)
Ek C(Ek, Fj)
</equation>
<bodyText confidence="0.999797875">
where C(Fj, Ei) is a frequency of Fj and Ei is
aligned.
However, if we use this in our model, the phrase
translation probability of the new possible phrase
can become extremely high (often it becomes 1).
To avoid this problem, we use the equations below
for the estimation of phrase translation probability
in place of Equation 14:
</bodyText>
<equation confidence="0.997951">
C(Fj, Ei) C(Fj, Ei)
p(Fj|Ei) = C(Ei) , p(Ei|Fj) = C(Fj) (15)
</equation>
<bodyText confidence="0.968311181818182">
C(Ei) is the frequency of the phrase Ei in the train-
ing corpus which can be pre-counted. This definition
can resolve the problem where the phrase translation
probability of the new possible phrase becomes too
high.
As for the NULL, we use Equation 14 because we
cannot pre-count the frequency of NULL.
Using the estimated phrase alignment probabil-
ities and dependency relation probabilities, we go
back to the initial alignment described in Section
3.2.1 iteratively.
</bodyText>
<equation confidence="0.36448">
Ek C(Fk, Ei)
</equation>
<page confidence="0.625725">
14
</page>
<figure confidence="0.999910152173913">
Initial alignment
(�n
䛿
(h.)
䝣䜷䝖
used
for
used
for
used
for
䛿
䝣䜷䝖
䝀䞊䝖
䛿
䝣䜷䝖
䝀䞊䝖
(photo)
䝀䞊䝖
(gate)
(•a)
䜢
⏝䛔䛯
(used)
䜢
⏝䛔䛯
䚹
䚹
䜢
⏝䛔䛯
䚹
the
photodetector
ཷ
ཷ
ཷ
(accept)
ග
⣲Ꮚ
ග
A
photogate
A
photogate
(light)
⣲Ꮚ
(device)
䛻
䛻
䛻
is
is
is
ග A
Swap⣲Ꮚ photogate Reject
the
photodetector
the
photodetector
ග
⣲Ꮚ
A
photogate
䛻
is
is
䛿
䝣䜷䝖
䝀䞊䝖
䛿
䝣䜷䝖
䝀䞊䝖
used
for
used
for
the
photodetector
the
photodetector
䜢
⏝䛔䛯
䚹
䜢
⏝䛔䛯
䚹
ගA
⣲Ꮚ photogate
Add Extend
䛻
ཷ
ཷ
</figure>
<figureCaption confidence="0.992403">
Figure 2: An example of hill-climbing.
</figureCaption>
<sectionHeader confidence="0.996948" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.99999425">
We conducted alignment experiments. A JST1
Japanese-English paper abstract corpus consisting
of 1M parallel sentences was used for the model
training. This corpus was constructed from a 2M
Japanese-English paper abstract corpus by NICT2
using the method of Uchiyama and Isahara (2007).
As gold-standard data, we used 475 sentence pairs
which were annotated by hand. The annotations
were only sure (5) alignments (there were no possi-
ble (P) alignments) (Och and Ney, 2003). The unit
of evaluation was word-base for both Japanese and
English. We used precision, recall, and F-measure
as evaluation criteria.
We conducted two experiments to reveal 1) the
contribution of our proposed model compared to the
existing models, and 2) the effectiveness of using
dependency tree structure and phrases, which are
larger alignment units than words. Trainings were
run on the original forms of words for both the pro-
posed model and the models used for comparison.
</bodyText>
<subsectionHeader confidence="0.999836">
4.1 Comparison with Word Sequential Model
</subsectionHeader>
<bodyText confidence="0.999651571428571">
For comparison, we used GIZA++ (Och and Ney,
2003) which implements the prominent sequential
word-base statistical alignment model of IBM Mod-
els. We conducted word alignment bidirectionally
with its default parameters and merged them using
three types of symmetrization heuristics (Koehn et
al., 2003). The results are shown in Table 2.
</bodyText>
<footnote confidence="0.6212575">
lhttp://www.jst.go.jp/
zhttp://www.nict.go.jp/
</footnote>
<bodyText confidence="0.999883421052632">
The result of ’Step 1’ uses parameters estimated
after 5 iterations of Step 1. The alignment is ob-
tained by the method of initial alignment shown in
Section 3.2.1. In ’Step 2-1’, the phrase translation
probabilities are the same as those in ’Step 1’. In ad-
dition, dependency relation probabilities estimated
from the ’Step 1’ alignment result are used. By com-
paring ’Step 1’ and ’Step 2-1’, we can see the ef-
fectiveness of dependency relation probability. We
performed 5 iterations for Step 2 and calculated the
alignment accuracy each time. As a result, the pro-
posed model could achieve a higher F-measure by
1.7 points compared to the sequential model. ’In-
tersection’ achieved best Precision, but its Recall is
quite low. ’grow-diag-final-and’ achieved best Re-
call, but its Precision is lower than our best result
where the Recall is almost same. Thus, we can say
our result is better than sequential word alignment
models.
</bodyText>
<subsectionHeader confidence="0.9992485">
4.2 Effectiveness of Dependency Trees and
Phrases
</subsectionHeader>
<bodyText confidence="0.999865">
To confirm the effectiveness of dependency trees and
phrases, we conducted alignment experiments on the
following four conditions:
</bodyText>
<listItem confidence="0.984089">
• Using both dependency trees and phrases (re-
ferred to as ’proposed’).
• Using dependency trees only.
• Using phrases only.
• Not using dependency trees or phrases (referred
to as ’none’)
</listItem>
<bodyText confidence="0.9885535">
For the conditions which do not use dependency
trees, we used positional relations of a sentence as
</bodyText>
<page confidence="0.998891">
15
</page>
<tableCaption confidence="0.995129">
Table 2: Results of alignment experiment.
</tableCaption>
<table confidence="0.967712076923077">
Precision Recall F
Step 1 77.55 33.92 47.20
Step 2-1 83.46 40.03 54.11
Step 2-2 87.74 45.37 59.81
Step 2-3 87.62 48.92 62.79
Step 2-4 86.87 50.42 63.81
Step 2-5 85.90 50.75 63.80
Step 2-6 85.54 51.00 63.90
Step 2-7 85.18 50.87 63.70
Step 2-8 84.66 50.75 63.46
intersection 90.34 34.28 49.71
grow-final-and 81.32 48.85 61.04
grow-diag-final-and 79.39 51.15 62.22
</table>
<figure confidence="0.969479904761905">
���
exhibited ■ ■
a
strong ■
inhibitory ■
effect ■
on ■ ■
tumor ■
growth ■
in ■
the ■
castrated ■
mice ■
as ■ ■
in ■
the
non-castrated ■
mice ■
� r,v i6 t Nr,vd) ad) ik IrMiz 0 fm �0VPAt-�
࣭࣭࣭ o rt; iz N rt; It L jA L
x x 5 t_
</figure>
<figureCaption confidence="0.991997">
Figure 3: An alignment example of the word sequential
model (grow-diag-final-and).
</figureCaption>
<tableCaption confidence="0.933571">
Table 3: Effectiveness of dependency trees and phrases
(results after 5 iterations in Step 2.)
</tableCaption>
<table confidence="0.9511544">
Precision Recall F
proposed 85.54 51.00 63.90
dependency tree only 89.77 39.47 54.83
phrase only 84.41 47.33 60.65
none 85.07 38.06 52.59
</table>
<bodyText confidence="0.981812">
a sequence of words instead of dependency tree re-
lations. The results are shown in Table 3. All the
results are the alignment accuracy after 5 iterations
of Step 2.
</bodyText>
<sectionHeader confidence="0.998053" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.977968523809524">
Table 2 shows that our proposed model could
achieve reasonably high accuracy of alignment, and
is better than sequential word-base models. As
an example, alignment results of a word sequen-
tial model are shown in Figure 3. The gray col-
ored cells are the gold-standard alignments, and the
black boxes are the outputs of the sequential model.
The model failed to resolve the correspondence am-
biguities between “ (not) (castrated)
(mice)”, and “ ”; and “non-castrated
mice”, and “castrated mice” respectively. This is
because these words are placed close to each other
and are also close to the correspondence “
H as” which can be a clue to the word order. Us-
ing the tree structure in Figure 4, these words were
correctly aligned. This is because in the English
tree, the phrase “castrated mice” does not depend
on “as”, and “non-castrated mice” does. Similarly
in the Japanese tree, “ ” depends on “
” and “ ” does not.
As mentioned in Section 1, sequential statistical
</bodyText>
<figure confidence="0.670580269230769">
���
exhibited ■ ■
ؒ ؜ؐa
ؒ جؐstrong ■
ؒ جؐinhibitory ■
جؐeffect ■
k_0
1-on
ؒ ؒ ؜ؐtumor ■
ؒ ؤؐgrowth ■
جؐin ■
ؒ ؒ ؜ؐthe
ؒ ؒجؐcastrated ■
ؒ ؤؐmice ■
ؤؐas ■
ؤؐin ■
ؒ ؜ؐthe
ؒ جؐnon-castrated ■ ■
ؤؐmice ■
ؐ ؒ� ؒ ؐ ؐ ؒ � ؜0) ؐ ؼ ؐ ؐ ؐ ؼ �
࣭ ؐ ؐ ؜in! ؒ ؜0) ؐ ؐ ؒ ؜ ؼ ؒL
؜ ؜ ؒ 41 ؜z ؐ ؐ ؜ ؜pt ؒؒS ؜pt_
� ؒ r 1= ؒ� ؒ ؒ ؒ L isa ؒ
؜z -1x ؜ � ؜lit 1= �IN SCJ
-1x � ؒ� ؒ �
ؐ ؼ �
</figure>
<figureCaption confidence="0.99837">
Figure 4: An alignment example of the proposed model.
</figureCaption>
<bodyText confidence="0.999860266666667">
methods, which regard a sentence as a sequence of
words, work well for language pairs that are not too
different in their language structure. Japanese and
English have significantly different structures. One
of the issues is that Japanese sentences have a SOV
word order, but in English, the word order is SVO, so
the dependency relations are often turned over. For
language pairs such as Japanese and English, deeper
sentence analysis using NLP resources is necessary
and useful. Our method is therefore suitable for such
language pairs.
As another example of an alignment failure by
the sequential model, Figure 5 shows the phrase cor-
respondence “ H photodetector”, which
was correctly found as shown in Figure 6. The pro-
</bodyText>
<page confidence="0.967706">
16
</page>
<figure confidence="0.8696579">
A
photogate ■ ■
is ■
used ■
for ■
the ■
photodetector ■
� � � � � � � � �
T h — LN
h t
</figure>
<figureCaption confidence="0.9748625">
Figure 5: An unsuccessful example of phrase detection in
the sequential model (grow-diag-final-and).
</figureCaption>
<figure confidence="0.731615083333334">
؜ؐA ■ ■
؜ؐphotogate ■ ■
is ■ ■
ؤؐused ■ ■
ؤؐfor ■ ■
ؒ ؜ؐthe
ؤؐphotodetector ■ ■ ■
؜ iti ؒ++ 17 ؜��F ؐ ؐ ؼ pt
ؒ I ؜�؜� � ؜ ؒ �
R ؒؒ� F ؒ �
؜
ؒ
</figure>
<figureCaption confidence="0.99367">
Figure 6: An example of phrase detection in the proposed
model.
</figureCaption>
<bodyText confidence="0.9953985">
posed method of generating possible phrases during
iterations works well and improves alignment.
From the result of our second experiment, we can
see the following points:
</bodyText>
<listItem confidence="0.963103666666667">
1. Phrasal alignment improves the recall, but low-
ers the precision.
2. By using dependency trees, precision can be
improved.
3. We can find a balance point by using both
phrasal alignment and dependency trees.
</listItem>
<bodyText confidence="0.99996175862069">
The causes of alignment errors in our model can
be summarized into categories. The biggest one is
parsing errors. Since our model is highly dependent
on the parsing result, the alignments would easily
turn out wrong if the parsing result was incorrect.
Sometimes the hill-climbing algorithm could not
revise the initial alignment. Most of these cases
would happen when one word occurred several
times on one side, but some of those occurrences
were omitted on the other side. Let’s suppose there
are two identical words on the source side, but the
target side has only one corresponding word. Initial
alignment is created without considering the depen-
dencies at all, so it cannot judge which source word
should be aligned to the corresponding target word.
In this case, the best alignment searching sometimes
gets the local solution. This problem could be re-
solved by considering local dependencies for am-
biguous words.
One difficulty is how to handle function words.
Function words often do not have exactly corre-
sponding words in the opposite language. Japanese
case markers such as “ (ha)”, “ (ga)” (subjec-
tive case), “ (wo)” (objective case) and so on, and
English articles are typical examples of words, that
do not have corresponding parts. There is a differ-
ence between alignment criteria for function words
of gold-standard and our outputs, and it is somewhat
difficult to improve alignment accuracy.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981185185185">
In this paper, we have proposed a linguistically-
motivated probabilistic phrase alignment model
based on dependency tree structures. The model in-
corporates the tree-based reordering model. Experi-
mental results show that the word sequential model
does not work well for linguistically different lan-
guage pairs, and this can be resolved by using syn-
tactic information. We have conducted the experi-
ments only on Japanese-English corpora. To firmly
support our claim that syntactic information is im-
portant, it is necessary to do more investigation on
other language pairs.
Most frequent alignment errors are derived from
parsing errors. Because our method depends heavily
on structural information, parsing errors easily make
the alignment accuracy worse. Although the parsing
accuracy is high in general for both Japanese and
English, it sometimes outputs incorrect dependency
structures because technical or unknown words of-
ten appears in scientific papers. This problem could
be resolved by introducing parsing probabilities into
our model using parsing tools which can output n-
best parsing with their parsing probabilities. This
will not only improve the alignment accuracy, it will
allow revision of the parsing result. Moreover, we
need to investigate the contribution of our alignment
result to the translation quality.
</bodyText>
<page confidence="0.998689">
17
</page>
<sectionHeader confidence="0.993888" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999826776315789">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Association for Computational Linguistics,
19(2):263–312.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association of Compu-
tational Linguistics, pages 88–95.
Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins.
2006. A discriminative model for tree-to-tree trans-
lation. In Proceedings of the 2006 Conference on
EMNLP, pages 232–241, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting on ACL, pages 80–87.
Daisuke Kawahara and Sadao Kurohashi. 2006. A fully-
lexicalized probabilistic model for japanese syntactic
and case structure analysis. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 176–183, New York City,
USA, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL
2003: Main Proceedings, pages 127–133.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proceedings of
The International Workshop on Sharable Natural Lan-
guage, pages 22–28.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104–111, New York City, USA,
June. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 523–530, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Arul Menezes and Stephen D. Richardson. 2001. A best-
first alignment algorithm for automatic extraction of
transfer mappings from bilingual corpora. In Proceed-
ings of the 39th Annual Meeting of the Association for
Computational Linguistics (ACL) Workshop on Data-
Driven Machine Translation, pages 39–46.
Toshiaki Nakazawa and Sadao Kurohashi. 2008.
Linguistically-motivated tree-based probabilistic
phrase alignment. In In Proceedings of the Eighth
Conference of the Association for Machine Translation
in the Americas (AMTA2008).
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Association for Computational Linguistics, 29(1):19–
51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL’05), pages 271–279.
Masao Utiyama and Hitoshi Isahara. 2007. A japanese-
english patent parallel corpus. In MT summit XI, pages
475–482.
Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki.
2000. Finding structural correspondences from bilin-
gual parsed corpus for corpus-based translation. In
Proceedings of the 18th International Conference on
Computational Linguistics, pages 906–912.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of 39th
Annual Meeting of the ACL, pages 523–530.
</reference>
<page confidence="0.999285">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.579773">
<title confidence="0.9989515">Statistical Phrase Alignment Using Dependency Relation Probability</title>
<author confidence="0.997533">Toshiaki Nakazawa Sadao Kurohashi</author>
<affiliation confidence="0.99997">Graduate School of Informatics, Kyoto University</affiliation>
<address confidence="0.841512">Yoshida-honmachi, Sakyo-ku Kyoto, 606-8501, Japan</address>
<email confidence="0.700115">nakazawa@nlp.kuee.kyoto-u.ac.jpkuro@i.kyoto-u.ac.jp</email>
<abstract confidence="0.9979671">When aligning very different language pairs, the most important needs are the use of structural information and the capability of generating one-to-many or many-to-many correspondences. In this paper, we propose a novel phrase alignment method which models word or phrase dependency relations in dependency tree structures of source and target languages. The dependency relation model is a kind of tree-based reordering model, and can handle non-local reorderings which sequential word-based models often cannot handle properly. The model is also capable of estimating phrase correspondences automatically without any heuristic rules. Experimental results of alignment show that our model could achieve F-measure 1.7 points higher than the conventional word alignment model with symmetrization algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Association for Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1609" citStr="Brown et al., 1993" startWordPosition="227" endWordPosition="230">s higher than the conventional word alignment model with symmetrization algorithms. 1 Introduction We consider that there are two important needs in aligning parallel sentences written in very different languages such as Japanese and English. One is to adopt structural or dependency analysis into the alignment process to overcome the difference in word order. The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving corr</context>
<context position="5496" citStr="Brown et al., 1993" startWordPosition="848" endWordPosition="851"> sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment aˆ between a given source sentence f and its target sentence e is acquired by the following equation: a� = argmax p(f, a|e) a = argmax p(f|e, a) - p(a|e) (1) a Figure 1: An example of a dependency tree and its alignment. where p(f|e,a) is called lexicon probability and p(a|e) is called alignment probability. Suppose f consists of n words f1, f2,..., fn, and e consists of m words e1, e2,..., en and a NULL word (e0). The alignment mapping a consists of associations j —* i = aj from source position j to target position i = aj. The two probabilities above are broken down as: J</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Association for Computational Linguistics, 19(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A probability model to improve word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="2623" citStr="Cherry and Lin (2003)" startWordPosition="389" endWordPosition="392">nformation from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model, </context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A probability model to improve word alignment. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Ivona Ku˘cerov´a</author>
<author>Michael Collins</author>
</authors>
<title>A discriminative model for tree-to-tree translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on EMNLP,</booktitle>
<pages>232--241</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Cowan, Ku˘cerov´a, Collins, 2006</marker>
<rawString>Brooke Cowan, Ivona Ku˘cerov´a, and Michael Collins. 2006. A discriminative model for tree-to-tree translation. In Proceedings of the 2006 Conference on EMNLP, pages 232–241, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Loosely tree-based alignment for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on ACL,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="2275" citStr="Gildea (2003)" startWordPosition="336" endWordPosition="337"> rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also pro</context>
</contexts>
<marker>Gildea, 2003</marker>
<rawString>Daniel Gildea. 2003. Loosely tree-based alignment for machine translation. In Proceedings of the 41st Annual Meeting on ACL, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fullylexicalized probabilistic model for japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>176--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="5098" citStr="Kawahara and Kurohashi, 2006" startWordPosition="779" endWordPosition="782">thm. During the Step 2 iterations, word correspondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment aˆ between a given source sentence f and its target sentence e is acquired by the following equation: a� = argmax p(f, a|e) a = argmax p(f|e, a) - p(a|e) (1) a Figure 1: An example </context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006. A fullylexicalized probabilistic model for japanese syntactic and case structure analysis. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 176–183, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003: Main Proceedings,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1689" citStr="Koehn et al., 2003" startWordPosition="239" endWordPosition="242">ms. 1 Introduction We consider that there are two important needs in aligning parallel sentences written in very different languages such as Japanese and English. One is to adopt structural or dependency analysis into the alignment process to overcome the difference in word order. The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tr</context>
<context position="19321" citStr="Koehn et al., 2003" startWordPosition="3187" endWordPosition="3190">osed model compared to the existing models, and 2) the effectiveness of using dependency tree structure and phrases, which are larger alignment units than words. Trainings were run on the original forms of words for both the proposed model and the models used for comparison. 4.1 Comparison with Word Sequential Model For comparison, we used GIZA++ (Och and Ney, 2003) which implements the prominent sequential word-base statistical alignment model of IBM Models. We conducted word alignment bidirectionally with its default parameters and merged them using three types of symmetrization heuristics (Koehn et al., 2003). The results are shown in Table 2. lhttp://www.jst.go.jp/ zhttp://www.nict.go.jp/ The result of ’Step 1’ uses parameters estimated after 5 iterations of Step 1. The alignment is obtained by the method of initial alignment shown in Section 3.2.1. In ’Step 2-1’, the phrase translation probabilities are the same as those in ’Step 1’. In addition, dependency relation probabilities estimated from the ’Step 1’ alignment result are used. By comparing ’Step 1’ and ’Step 2-1’, we can see the effectiveness of dependency relation probability. We performed 5 iterations for Step 2 and calculated the align</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLT-NAACL 2003: Main Proceedings, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proceedings of The International Workshop on Sharable Natural Language,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="5034" citStr="Kurohashi et al., 1994" startWordPosition="769" endWordPosition="772">ep 1 and Step 2 are performed iteratively by the EM algorithm. During the Step 2 iterations, word correspondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment aˆ between a given source sentence f and its target sentence e is acquired by the following equation: a� = argmax p(f</context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proceedings of The International Workshop on Sharable Natural Language, pages 22–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="7332" citStr="Liang et al., 2006" startWordPosition="1178" endWordPosition="1181">n 1 is modified as follows: a� = argmax p(f|e, a) - p(a|e) - a p(e|f, a) - p(a|f) (4) Since our model regards a phrase as a basic unit, the above formula is calculated in a straightforward way. In contrast, the IBM models can consider a many-to-one alignment by combining one-to-one Nk (device) A-T (light) .46 (accept) A photogate I;: is (ni) (ha) lj: 7-th used for (photo) —h the (gate) � photodetector (wo) Mot_- (used) � p(a|e) = p(Aj|ez) (3) 11 alignments, but they cannot consider a one-to-many or many-to-many alignment. The models are estimated by EM-like algorithm which is very similar to (Liang et al., 2006). The important difference is that we are using tree structures. We maximize the data likelihood: � max (log pef(f, e; Bef) + log pfe(f, e; Bfe)) θef,θfe f,e (5) In the E-step, we compute the posterior distribution of the alignments with the current parameter B: q(a; f, e) pef(a|f, e; Bef) · pfe(a|f, e; Bfe) (6) In the M-step, we update the parameter B: EB&apos; := argmax θ a,f,e q(a; f, e) log pef(a, f, e; Bef) + E q(a; f, e) logpfe(a, f, e; Bfe) a,f,e q(a; f, e) logp(e) - pef(a, f|e; Bef) q(a; f, e) logp(f) - pfe(a, e|f; Bfe) (7) Note that p(e) and p(f) have no effect on maximization, and pef(a, </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 104–111, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="5133" citStr="McDonald et al., 2005" startWordPosition="784" endWordPosition="787">rrespondences are grown into phrase correspondences. 2 Proposed Model We suppose that Japanese is the source language and English is the target language in the description of our model. Note that the model is not specialized for this language pair, and it can be applied to any language pair. Because our model uses dependency tree structures, both source and target sentences are parsed beforehand. Japanese sentences are converted into dependency structures using the morphological analyzer JUMAN (Kurohashi et al., 1994), and the dependency analyzer KNP (Kawahara and Kurohashi, 2006). MSTparser (McDonald et al., 2005) is used to convert English sentences. Figure 1 shows an example of dependency structures. The root of a tree is placed at the extreme left and words are placed from top to bottom. 2.1 Overview This section outlines our proposed model in comparison to the IBM models, which are the conventional statistical alignment models. In the IBM models (Brown et al., 1993), the best alignment aˆ between a given source sentence f and its target sentence e is acquired by the following equation: a� = argmax p(f, a|e) a = argmax p(f|e, a) - p(a|e) (1) a Figure 1: An example of a dependency tree and its alignm</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arul Menezes</author>
<author>Stephen D Richardson</author>
</authors>
<title>A bestfirst alignment algorithm for automatic extraction of transfer mappings from bilingual corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL) Workshop on DataDriven Machine Translation,</booktitle>
<pages>39--46</pages>
<contexts>
<context position="2114" citStr="Menezes and Richardson (2001)" startWordPosition="310" endWordPosition="313">word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discr</context>
</contexts>
<marker>Menezes, Richardson, 2001</marker>
<rawString>Arul Menezes and Stephen D. Richardson. 2001. A bestfirst alignment algorithm for automatic extraction of transfer mappings from bilingual corpora. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL) Workshop on DataDriven Machine Translation, pages 39–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiaki Nakazawa</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Linguistically-motivated tree-based probabilistic phrase alignment. In</title>
<date>2008</date>
<booktitle>In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas (AMTA2008).</booktitle>
<contexts>
<context position="2866" citStr="Nakazawa and Kurohashi (2008)" startWordPosition="430" endWordPosition="433">da and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohashi (2008) also proposed a model focusing on the dependency relations. Their model has the constraint that content words can only correspond to content words on the other side, and the same applies for function words. This sometimes leads to an incorrect alignment. We have removed this constraint to make more flexible alignments possible. Moreover, in their model, some function words are brought together, and thus they cannot handle the situation where each function word corresponds to a different part. The smallest unit of our model is a single word, which should solve this problem. Proceedings of SSST</context>
</contexts>
<marker>Nakazawa, Kurohashi, 2008</marker>
<rawString>Toshiaki Nakazawa and Sadao Kurohashi. 2008. Linguistically-motivated tree-based probabilistic phrase alignment. In In Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas (AMTA2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Association for Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>51</pages>
<contexts>
<context position="18497" citStr="Och and Ney, 2003" startWordPosition="3060" endWordPosition="3063">the photodetector the photodetector 䜢 ⏝䛔䛯 䚹 䜢 ⏝䛔䛯 䚹 ගA ⣲Ꮚ photogate Add Extend 䛻 ཷ ཷ Figure 2: An example of hill-climbing. 4 Experimental Results We conducted alignment experiments. A JST1 Japanese-English paper abstract corpus consisting of 1M parallel sentences was used for the model training. This corpus was constructed from a 2M Japanese-English paper abstract corpus by NICT2 using the method of Uchiyama and Isahara (2007). As gold-standard data, we used 475 sentence pairs which were annotated by hand. The annotations were only sure (5) alignments (there were no possible (P) alignments) (Och and Ney, 2003). The unit of evaluation was word-base for both Japanese and English. We used precision, recall, and F-measure as evaluation criteria. We conducted two experiments to reveal 1) the contribution of our proposed model compared to the existing models, and 2) the effectiveness of using dependency tree structure and phrases, which are larger alignment units than words. Trainings were run on the original forms of words for both the proposed model and the models used for comparison. 4.1 Comparison with Word Sequential Model For comparison, we used GIZA++ (Och and Ney, 2003) which implements the promi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Association for Computational Linguistics, 29(1):19– 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>271--279</pages>
<contexts>
<context position="1821" citStr="Quirk et al., 2005" startWordPosition="259" endWordPosition="262">such as Japanese and English. One is to adopt structural or dependency analysis into the alignment process to overcome the difference in word order. The other is that the method needs to have the capability of generating phrase correspondences, that is, one-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side,</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 271–279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A japaneseenglish patent parallel corpus.</title>
<date>2007</date>
<booktitle>In MT summit XI,</booktitle>
<pages>475--482</pages>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A japaneseenglish patent parallel corpus. In MT summit XI, pages 475–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
<author>Sadao Kurohashi</author>
<author>Eiji Aramaki</author>
</authors>
<title>Finding structural correspondences from bilingual parsed corpus for corpus-based translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>906--912</pages>
<contexts>
<context position="2080" citStr="Watanabe et al. (2000)" startWordPosition="305" endWordPosition="308">ne-to-many or many-to-many word correspondences. Most existing alignment methods simply consider a sentence as a sequence of words (Brown et al., 1993), and generate phrase correspondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tre</context>
</contexts>
<marker>Watanabe, Kurohashi, Aramaki, 2000</marker>
<rawString>Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki. 2000. Finding structural correspondences from bilingual parsed corpus for corpus-based translation. In Proceedings of the 18th International Conference on Computational Linguistics, pages 906–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the ACL,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2257" citStr="Yamada and Knight (2001)" startWordPosition="331" endWordPosition="334">rrespondences using heuristic rules (Koehn et al., 2003). Some studies incorporate structural information into the alignment process after this simple word align10 ment (Quirk et al., 2005; Cowan et al., 2006). However, this is not sufficient because the basic word alignment itself is not good. On the other hand, a few models have been proposed which use structural information from the beginning of the alignment process. Watanabe et al. (2000) and Menezes and Richardson (2001) proposed a structural alignment methods. These methods use heuristic rules when resolving correspondence ambiguities. Yamada and Knight (2001) and Gildea (2003) proposed a tree-based probabilistic alignment methods. These methods reorder, insert or delete sub-trees on one side to reproduce the other side, but the constraints of using syntactic information is often too rigid. Yamada and Knight flattened the trees by collapsing nodes. Gildea cloned sub-trees to deal with the problem. Cherry and Lin (2003) proposed a model which uses a source side dependency tree structure and constructs a discriminative model. However, there is the defect that its alignment unit is a word, so it can only find oneto-one alignments. Nakazawa and Kurohas</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the ACL, pages 523–530.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>