<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9984345">
Multi-level Association Graphs –
A New Graph-Based Model for Information Retrieval
</title>
<author confidence="0.834994">
Hans Friedrich Witschel
</author>
<affiliation confidence="0.9008835">
NLP department
University of Leipzig
</affiliation>
<address confidence="0.90571">
04009 Leipzig, P.O. Box 100920
</address>
<email confidence="0.9963">
witschel@informatik.uni-leipzig.de
</email>
<sectionHeader confidence="0.998581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997252">
This paper introduces multi-level associa-
tion graphs (MLAGs), a new graph-based
framework for information retrieval (IR).
The goal of that framework is twofold:
First, it is meant to be a meta model of
IR, i.e. it subsumes various IR models
under one common representation. Sec-
ond, it allows to model different forms of
search, such as feedback, associative re-
trieval and browsing at the same time. It
is shown how the new integrated model
gives insights and stimulates new ideas for
IR algorithms. One of these new ideas is
presented and evaluated, yielding promis-
ing experimental results.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999817846153846">
Developing formal models for information retrieval
has a long history. A model of information retrieval
“predicts and explains what a user will find relevant
given the user query” (Hiemstra, 2001). Most IR
models are firmly grounded in mathematics and thus
provide a formalisation of ideas that facilitates dis-
cussion and makes sure that the ideas can be imple-
mented. More specifically, most IR models provide
a so-called retrieval function f(q, d) , which returns
– for given representations of a document d and of a
user information need q – a so-called retrieval status
value by which documents can be ranked according
to their presumed relevance w.r.t. to the query q.
In order to understand the commonalities and dif-
ferences among IR models, this paper introduces the
notion of meta modeling. Since the word “meta
model” is perhaps not standard terminology in IR,
it should be explained what is meant by it: a meta
model is a model or framework that subsumes other
IR models, such that they are derived by specifying
certain parameters of the meta model.
In terms of IR theory, such a framework conveys
what is common to all IR models by subsuming
them. At the same time, the differences between
models are highlighted in a conceptually simple
way by the different values of parameters that have
to be set in order to arrive at this subsumption. It
will be shown that a graph-based representation of
IR data is very well suited to this problem.
IR models concentrate on the matching process,
i.e. on measuring the degree of overlap between a
query q and a document representation d. On the
other hand, there are the problems of finding suit-
able representations for documents (indexing) and
for users’ information needs (query formulation).
Since users are often not able to adequately state
their information need, some interactive and asso-
ciative procedures have been developed by IR re-
searchers that help to overcome this problem:
</bodyText>
<listItem confidence="0.870006625">
• Associative retrieval, i.e. retrieving informa-
tion which is associated to objects known or
suspected to be relevant to the user – e.g. query
terms or documents that have been retrieved al-
ready.
• Feedback, another method for boosting recall,
either relies on relevance information given
by the user (relevance feedback) or assumes
</listItem>
<page confidence="0.703685">
9
</page>
<subsubsectionHeader confidence="0.341209">
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 9–16,
</subsubsectionHeader>
<bodyText confidence="0.9374211">
Rochester, April 2007 c�2007 Association for Computational Linguistics
top-ranked documents to be relevant (pseudo
feedback) and learns better query formulations
from this information.
• Browsing, i.e. exploring a document collec-
tion interactively by following links between
objects such as documents, terms or concepts.
Again, it will be shown that – using a graph-based
representation – these forms of search can be sub-
sumed easily.
</bodyText>
<sectionHeader confidence="0.999909" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.950113">
2.1 Meta modeling
</subsectionHeader>
<bodyText confidence="0.999962681818182">
In the literal sense of the definition above, there is
a rather limited number of meta models for IR, the
most important of which will be described here very
shortly.
Most research about how to subsume various IR
models in a common framework has been done in
the context of Bayesian networks and probabilistic
inference (Turtle and Croft, 1990). In this approach,
models are subsumed by specifying certain proba-
bility distributions. In (Wong and Yao, 1995), the
authors elaborately show how all major IR models
known at that time can be subsumed using proba-
bilistic inference. Language modeling, which was
not known then was later added to the list by (Met-
zler and Croft, 2004).
Another graph-based meta modeling approach
uses the paradigm of spreading activation (SA) as a
simple unifying framework. Given semantic knowl-
edge in the form of a (directed) graph, the idea of
spreading activation is that a measure of relevance
– w.r.t. a current focus of attention – is spread over
the graph’s edges in the form of activation energy,
yielding for each vertex in the graph a degree of re-
latedness with that focus (cf. (Anderson and Pirolli,
1984)). It is easy to see how this relates to IR: us-
ing a graph that contains vertices for both terms and
documents and appropriate links between the two,
we can interpret a query as a focus of attention and
spread that over the network in order to rank docu-
ments by their degree of relatedness to that focus.
A very general introduction of spreading activa-
tion as a meta model is given in the early work by
(Preece, 1981) All later models are hence special
cases of Preece’s work, including the multi-level as-
sociation graphs introduced in section 3. Preece’s
model subsumes the Boolean retrieval model, coor-
dination level matching and vector space processing.
Finally, an interesting meta model is described by
(van Rijsbergen, 2004) who uses a Hilbert space as
an information space and connects the geometry of
that space to probability and logics. In particular,
he manages to give the familiar dot product between
query and document vector a probabilistic interpre-
tation.
</bodyText>
<subsectionHeader confidence="0.9551745">
2.2 Graph-based models for associative
retrieval and browsing
</subsectionHeader>
<bodyText confidence="0.999816764705882">
The spreading activation paradigm is also often used
for associative retrieval. The idea is to reach vertices
in the graph that are not necessarily directly linked to
query nodes, but are reachable from query nodes via
a large number of short paths along highly weighted
edges.
Besides (Preece, 1981), much more work on SA
was done, a good survey of which can be found in
(Crestani, 1997). A renewed interest in SA was later
triggered with the advent of the WWW where hyper-
links form a directed graph. In particular, variants of
the PageRank (Brin and Page, 1998) algorithm that
bias a random searcher towards some starting nodes
(e.g. an initial result set of documents) bear close re-
semblance to SA (Richardson and Domingos, 2002;
White and Smyth, 2003).
Turning to browsing, we can distinguish three
types of browsing w.r.t. to the vertices of the graph:
index term browsing, which supports the user in for-
mulating his query by picking related terms (Doyle,
1961; Beaulieu, 1997), document browsing which
serves to expand result sets by allowing access to
similar documents or by supporting web browsing
(Smucker and Allan, 2006; Olston and Chi, 2003)
and combined approaches where both index terms
and documents are used simultaneously for brows-
ing.
In this last category, many different possibilities
arise for designing interfaces. A common guiding
principle of many graph-based browsing approahces
is that of interactive spreading activation (Oddy,
1977; Croft and Thompson, 1987). Another ap-
proach, which is very closely related to MLAGs,
is a multi-level hypertext (MLHT), as proposed in
</bodyText>
<page confidence="0.996942">
10
</page>
<bodyText confidence="0.999879333333333">
(Agosti and Crestani, 1993) – a data structure con-
sisting of three levels, for documents, index terms
and concepts. Each level contains objects and links
among them. There are also connections between
objects of two adjacent levels. An MLHT is meant
to be used for interactive query formulation, brows-
ing and search, although (Agosti and Crestani, 1993)
give no precise specification of the processing pro-
cedures.
</bodyText>
<subsectionHeader confidence="0.999948">
2.3 Contribution of this work
</subsectionHeader>
<bodyText confidence="0.999973259259259">
Compared to Preece’s work, the MLAG framework
makes two sorts of modifications in order to reach
the goals formulated in the introduction: in order to
subsume more IR models, the flexibility and power
of Preece’s model is increased by adding real-valued
edge weights. On the other hand, a clearer distinc-
tion is made between local and global information
through the explicit introduction of “level graphs”.
With the introduction of levels, the MLAG data
structure becomes very closely related to the MLHT
paradigm of (Agosti and Crestani, 1993), MLAGs,
however, generalise MLHTs by allowing arbitrary
types of levels, not only the three types proposed in
(Agosti and Crestani, 1993). Additionally, links in
MLAGs are weighted and the spreading activation
processing defined in the next section makes exten-
sive use of these weights.
All in all, the new model combines the data struc-
ture of multi-level hypertexts (Agosti and Crestani,
1993) with the processing paradigm of spreading ac-
tivation as proposed by Preece (Preece, 1981), re-
fining both with an adequate edge weighting. The
framework is an attempt to be as general as neces-
sary for subsuming all models and allowing for dif-
ferent forms of search, while at the same time being
as specific as possible about the things that are really
common to all IR models.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="method">
3 The MLAG model
</sectionHeader>
<subsectionHeader confidence="0.999878">
3.1 Data structure
</subsectionHeader>
<bodyText confidence="0.999208642857143">
Formally, the basis of a multi-level association
graph (MLAG) is a union of n level graphs
L1, ..., Ln. Each of these n directed graphs Li =
G(VLi, ELi, WLi) consists of a set of vertices
VLi, a set ELi C V Li x V Li of edges and a func-
tion WLi : ELi —* R returning edge weights.
In order to connect the levels, there are n −
1 connecting bipartite graphs (or inverted lists)
I1,2, ..., In−1,n where each inverted list Ij,j+1 con-
sists of vertices V Ij,j+1 = V Lj U V Lj+1, edges
EIj,j+1 C (V Lj x V Lj+1) U (V Lj+1 x V Lj) and
weights WIj,j+1 : EIj,j+1 —* R. Figure 1 depicts
a simple example multi-level association graph with
two levels Ld and Lt for documents and terms.
</bodyText>
<equation confidence="0.664217777777778">
d4�
d2 d d6Ad
Id d5
Inverted Nist
❇
/❚✁�t7 t t4
tt t
t �t1 � t8�� �Term level
�
</equation>
<figureCaption confidence="0.999609">
Figure 1: A simple example MLAG
</figureCaption>
<bodyText confidence="0.9999376">
Assuming that the vertices on a given level cor-
respond to objects of the same type and vertices
in different levels to objects of different types, this
data structure has the following general interpreta-
tion: Each level represents associations between ob-
jects of a given type, e.g. term-term or document-
document similarities. The inverted lists, on the
other hand, represent associations between different
types of objects, e.g. occurrences of terms in docu-
ments.
</bodyText>
<subsectionHeader confidence="0.992991">
3.2 Examples
3.2.1 Standard retrieval
</subsectionHeader>
<bodyText confidence="0.999734363636364">
The simplest version of a multi-level association
graph consists of just two levels – a term level Lt and
a document level Ld. This is the variant depicted in
figure 1.
The graph Itd that connects Lt and Ld is an in-
verted list in the traditional sense of the word, i.e. a
term is connected to all documents that it occurs in
and the weight WI(t, d) of an edge (t, d) connect-
ing term t and document d conveys the degree to
which t is representative of d’s content, or to which
d is about t.
</bodyText>
<figure confidence="0.8349495">
d9 nt level
ocume
</figure>
<page confidence="0.992673">
11
</page>
<bodyText confidence="0.9999374">
The level graphs Lt and Ld can be computed in
various ways. As for documents, a straight-forward
way would be to calculate document similarities,
e.g. based on the number of terms shared by two
documents. However, other forms of edges are pos-
sible, such as hyperlinks or citations – if available.
Term associations, on the other hand, can be com-
puted using co-occurrence information. An alterna-
tive would be to use relations from manually created
thesauri or ontologies.
</bodyText>
<subsectionHeader confidence="0.868139">
3.2.2 More levels
</subsectionHeader>
<bodyText confidence="0.999957">
In order to (partly) take document structure into
account, it can be useful to introduce a level for doc-
ument parts (e.g. headlines and/or passages) in be-
tween the term and the document level. This can
be combined with text summarisation methods (cf.
e.g. (Brandow et al., 1995)) in order to give higher
weights to more important passages in the inverted
list connecting passages to documents.
In distributed or peer-to-peer environments,
databases or peers may be modeled in a separate
layer above the document level, inverted lists in-
dicating where documents are held. Additionally,
a peer’s neighbours in an overlay network may be
modeled by directed edges in the peer level graph.
More extensions are possible and the flexibility of
the MLAG framework allows for the insertion of ar-
bitrary layers.
</bodyText>
<subsectionHeader confidence="0.999722">
3.3 Processing paradigm
</subsectionHeader>
<bodyText confidence="0.9912972">
The operating mode of an MLAG is based on the
spreading activation principle. However, the spread
of activation between nodes of two different levels
is not iterated. Rather, it is carefully controlled, yet
allowing non-linear modifications at some points.
In order to model spreading activation in an
MLAG, we introduce an activation function Ai :
V Li —* R which returns the so-called activation en-
ergies of vertices on a given level Li. The default
value of the activation function is Ai(v) = 0 for all
vertices v E V Li.
In the following, it is assumed that the MLAG
processing is invoked by activating a set of vertices
A on a given level Li of the MLAG by modifying the
activation function of that level so that Ai(v) = wv
for each v E A.
A common example of such activation is a query
being issued by a user. The initial activation is the
result of the query formulation process, which se-
lects some vertices v E A and weights them accord-
ing to their presumed importance wv. This weight is
then the initial activation energy of the vertex.
Once we have an initial set of activated vertices,
the following general procedure is executed until
some stopping criterion is met:
</bodyText>
<listItem confidence="0.998204">
1. Collect activation values on current level Li,
i.e. determine Ai(u) for all u E V Li.
2. (Optionally) apply a transformation to the acti-
vation energies of Li-nodes, i.e. alter Ai(u) by
using a – possibly non-linear – transformation
function or procedure.
3. Spread activation to the next level Li+1 along
the links connecting the two levels:
</listItem>
<equation confidence="0.9928695">
�Ai+1(v) = Ai(u) · WI(u, v) (1)
(u,v)EIi,i+1
</equation>
<listItem confidence="0.991274">
4. Set Ai(u) = 0 for all u E V Li, i.e. “forget”
about the old activation energies
5. (Optionally) apply a transformation to the acti-
vation energies of Li+1-nodes (see step 2).
6. Go to 1, increment i (or decrement, depend-
ing on its value and the configuration of the
MLAG)
</listItem>
<bodyText confidence="0.998726588235294">
If we take a vector space view of this process-
ing mode and if we identify level Li with terms and
level Li+1 with documents, we can interpret the ac-
tivation energies Ai(u) as a query vector and the
edge weights WI(u, v) of edges arriving at vertex
v E V Li+1 as a document vector for document v.
This shows that the basic retrieval function re-
alised by steps 1, 3 and 4 of this process is a simple
dot product. We will later see that retrieval functions
of most IR models can actually be written in that
form, provided that the initial activation of query
terms and the edge weights of Ii,i+1 are chosen cor-
rectly (section 4).
For some models, however, we additionally need
the possibility to perform nonlinear transformations
on result sets in order to subsume them. Steps 2 and
5 of the algorithm allow for arbitrary modifications
</bodyText>
<page confidence="0.993996">
12
</page>
<bodyText confidence="0.9984504">
of the activation values based on whatever evidence
may be available on the current level or globally –
but not in the inverted list. This will later also allow
to include feedback and associative retrieval tech-
niques.
</bodyText>
<sectionHeader confidence="0.961018" genericHeader="method">
4 The MLAG as a meta model
</sectionHeader>
<bodyText confidence="0.999808833333333">
In this section, examples will be shown that demon-
strate how existing IR models of ranked retrieval 1
can be subsumed using the simple MLAG of figure
1 and the processing paradigm from the last section.
This is done by specifying the following parameters
of that paradigm:
</bodyText>
<listItem confidence="0.999750666666667">
1. How nodes are activated in the very first step
2. How edges of the inverted list are weighted
3. Which transformation is used in 2 and 5.
</listItem>
<bodyText confidence="0.9983378">
For each model, the corresponding retrieval func-
tion will be given and the parameter specification
will be discussed shortly. The specification of the
above parameters will be given in the form of triplets
(activationinit, edge weights, transform).
</bodyText>
<subsectionHeader confidence="0.936524">
4.1 Vector space model
</subsectionHeader>
<bodyText confidence="0.999960333333333">
In the case of the vector space model (Salton et al.,
1975), the retrieval function to be mimicked is as
follows:
</bodyText>
<equation confidence="0.94648">
f(q, d) = � wtqwtd (2)
tEqnd
</equation>
<bodyText confidence="0.999986166666667">
where wtq and wtd are a term’s weight in the query
q and the current document d, respectively. This
can be achieved by specifying the parameter triplet
(wtq, wtd, none). This simple representation reflects
the closeness of the MLAG paradigm to the vector
space model that has been hinted at above.
</bodyText>
<subsectionHeader confidence="0.949046">
4.2 Probabilistic model
</subsectionHeader>
<bodyText confidence="0.999816">
For the probabilistic relevance model (Robertson
and Sparck-Jones, 1976), the MLAG has to realise
the following retrieval function
</bodyText>
<equation confidence="0.9418415">
di log pi(1 − ri) (3)
ri(1 − pi)
</equation>
<footnote confidence="0.9573325">
1This excludes the Boolean model, which can, however, also
be subsumed as shown in section 5.5 of (Preece, 1981)
</footnote>
<bodyText confidence="0.919908090909091">
where di E 10, 11 indicates whether term i is con-
tained in document d, pi is the probability that a rele-
vant document will contain term i and ri is the prob-
ability that an irrelevant document will contain it.
This retrieval function is realised by the parameter
triplet (log p.��−r.�
r.��−p.�, di, none).
Now there is still the question of how the esti-
mates of pi and ri are derived. This task involves the
use of relevance information which can be gained
via feedback, described in section 6.1.
</bodyText>
<subsectionHeader confidence="0.998974">
4.3 Language models
</subsectionHeader>
<bodyText confidence="0.999858">
The general language modeling retrieval function
(cf. e.g. (Zhai and Lafferty, 2001)) is – admittedly –
not in the linear form of equation 1. But using log-
arithms, products can be turned into sums without
changing the ranking – the logarithm being a mono-
tonic function (note that this is what also happened
in the case of the probabilistic relevance models).
In particular, we will use the approach of com-
paring query and document language models by
Kullback-Leibler divergence (KLD) (Lafferty and
Zhai, 2001) which results in the equation
</bodyText>
<equation confidence="0.9896844">
� P (t|Mq)
KLD(Mq||Md) = P(t|Mq) log P(t|Md)
tEq
�OC − P(t|Mq) log P(t|Md)
tEq
</equation>
<bodyText confidence="0.999214642857143">
where P(t|Mq) and P(t|Md) refer to the probabil-
ity that term t will be generated by the unigram lan-
guage model of query q or document d, respectively.
Note that we have simplified the equation by drop-
ping a term &amp; P(t|Mq) log P(t|Mq), which de-
pends only on the query, not on the documents to
be ranked.
Now, the triplet (P(t|Mq), − log P(t|Md), t)
can be used to realise this retrieval func-
tion where t stands for a procedure that adds
−P(t|Mq) log P(t|Md) to the document node’s
activation level for terms t not occurring in d and
sorts documents by increasing activation values
afterwards.
</bodyText>
<sectionHeader confidence="0.993194" genericHeader="method">
5 Combining IR models
</sectionHeader>
<bodyText confidence="0.996747333333333">
As can be seen from the last equation above, the lan-
guage model retrieval function sums over all terms
in the query. Each term – regardless of whether it
</bodyText>
<equation confidence="0.881715333333333">
�
f(q, d) =
i
</equation>
<page confidence="0.99392">
13
</page>
<bodyText confidence="0.99886">
appears in the document d or not – contributes some-
thing that may be interpreted as a “penalty” for the
document. The magnitude of this penalty depends
on the smoothing method used (cf. (Zhai and Laf-
ferty, 2001)). A popular smoothing method uses
so-called Dirichlet priors to estimate document lan-
guage models:
</bodyText>
<equation confidence="0.998468666666667">
tf + µp(t|C)
P(t|Md) = (4)
µ + |d|
</equation>
<bodyText confidence="0.9997221">
where tf is t’s frequency in d, p(t|C) is the term’s
relative frequency in the whole collection and µ is
a free parameter. This indicates that if a rare term
is missing from a document, the penalty will be
large, P(t|Md) being very small because tf = 0
and p(t|C) small.
Conceptually, it is unproblematic to model the
retrieval function by making Itd a complete bipar-
tite graph, i.e. specifying a (non-zero) value for
P(t|Md), even if t does not occur in d. In a practical
implementation, this is not feasible, which is why
we add the contribution of terms not contained in
a document, i.e. −P(t|Mq) log P(t|Md), for terms
that do not occur in d. 2
This transformation indicates an important differ-
ence between language modeling and all other IR
models: language models penalise documents for
the absence of rare (i.e. informative) terms whereas
the other models reward them for the presence of
these terms.
These considerations suggest a combination of
both approaches: starting with an arbitrary “pres-
ence rewarding” model – e.g. the vector space model
– we may integrate the “absence penalising” philos-
ophy by subtracting from a document’s score, for
each missing term, the contribution that one occur-
rence of that term would have earned (cf. (Witschel,
2006)).
For the vector space model, this yields the follow-
ing retrieval function:
</bodyText>
<equation confidence="0.984239333333333">
f(q, d) = 1: wtqwtd
tEqnd
wtd(tf = 1)wtq
</equation>
<footnote confidence="0.497864666666667">
2In order to do this, we only need to know |d |and the relative
frequency of t in the collection p(tIC), i.e. information that is
available outside the inverted list.
</footnote>
<bodyText confidence="0.999960333333333">
where α is a free parameter regulating the relative
influence of penalties, comparable to the µ parame-
ter of language models above.
</bodyText>
<subsectionHeader confidence="0.9951">
5.1 Experimental results
</subsectionHeader>
<bodyText confidence="0.999958318181818">
Table 2 shows retrieval results for combining two
weighting schemes, BM25 (Robertson et al., 1992)
and Lnu.ltn (Singhal et al., 1996), with penalties.
Both of them belong to the family of tf.idf weight-
ing schemes and can hence be regarded as represent-
ing the vector space model, although BM25 was de-
veloped out of the probabilistic model.
Combining them with the idea of “absence penal-
ties” works as indicated above, i.e. weights are ac-
cumulated for each document using the tf.idf-like
retrieval functions. Then, from each score, the con-
tributions that one occurrence of each missing term
would have earned is subtracted. More precisely,
what is subtracted consists of the usual tf.idf weight
for the missing term, where tf = 1 is substituted in
the tf part of the formula.
Experiments were run with queries from TREC-7
and TREC-8. In order to study the effect of query
length, very short queries (using only the title field
of TREC queries), medium ones (using title and de-
scription fields) and long ones (using all fields) were
used. Table 1 shows an example TREC query.
</bodyText>
<table confidence="0.800087">
&lt; top&gt;
&lt; num&gt; Number: 441
&lt; title&gt; Lyme disease
&lt; desc&gt; Description:
How do you prevent and treat Lyme disease?
&lt; narr&gt; Narrative:
Documents that discuss current prevention and
treatment techniques for Lyme disease are relevant [...]
&lt; /top&gt;
</table>
<tableCaption confidence="0.999641">
Table 1: A sample TREC query
</tableCaption>
<bodyText confidence="0.976389125">
Table 2 shows that both weighting schemes can
be significantly improved by using penalties, espe-
cially for short queries, reaching and sometimes sur-
passing the performance of retrieval with language
models. This holds even when the parameter α is
not tuned and confirms that interesting insights are
gained from a common representation of IR models
in a graph-based environment. 3
</bodyText>
<footnote confidence="0.8171495">
3Note that these figures were obtained without any refine-
ments such as query expansion and are hence substantially
</footnote>
<equation confidence="0.527506333333333">
α
−
|q |tEq\d
</equation>
<page confidence="0.981262">
14
</page>
<table confidence="0.997371454545455">
TREC-7 TREC-8
Weighting very short medium long very short medium long
BM25 0.1770 0.2120 0.2141 0.2268 0.2514 0.2332
+ P (α = 1) 0.1867* 0.2194* 0.2178* 0.2380* 0.2593* 0.2335
+ P (best α) 0.1896* 0.2220* 0.2185* 0.2411* 0.2625* 0.2337
best α value 2 2 1.5 2 2 0.25
Lnu.ltn 0.1521 0.1837 0.1920 0.1984 0.2226 0.2013
+ P (α = 1) 0.1714* 0.1972* 0.1946* 0.2176* 0.2305* 0.2040
+ P (best α) 0.1873* 0.2106* 0.1977 0.2394* 0.2396* 0.2064*
best α value 5 5 3 5 4 1.5
LM 0.1856 0.2163 0.2016 0.2505 0.2578 0.2307
</table>
<tableCaption confidence="0.981646">
Table 2: Mean average precision of BM25 and Lnu.ltn and their corresponding penalty schemes (+ P) for
</tableCaption>
<bodyText confidence="0.6223555">
TREC-7 and TREC-8. Asterisks indicate statistically significant deviations (using a paired Wilcoxon test
on a 95% confidence level) from each baseline, whereas the best run for each query length is marked with
bold font. Performance of language models (LM) is given for reference, where the value of the smoothing
parameter µ was set to the average document length.
</bodyText>
<sectionHeader confidence="0.99713" genericHeader="method">
6 Different forms of search with MLAGs
</sectionHeader>
<bodyText confidence="0.999914">
In order to complete the goals stated in the intro-
duction of this paper, this section will briefly ex-
plain how feedback, associative retrieval and brows-
ing can be modeled within the MLAG framework.
</bodyText>
<subsectionHeader confidence="0.951228">
6.1 Feedback
</subsectionHeader>
<bodyText confidence="0.991326333333333">
Using the simple term-document MLAG of figure
1, feedback can be implemented by the following
procedure:
</bodyText>
<listItem confidence="0.955566133333333">
1. Perform steps 1 – 4 of the basic processing.
2. Apply a transformation to the activation values
of Ld-nodes, e.g. let the user pick relevant doc-
uments and set their activation to some positive
constant Q.
3. Perform step 3 of the basic processing with
Li = Ld and Li+1 = Lt, i.e. let activation
flow back to term level.
4. Forget about activation levels of documents.
5. Apply transformation on the term level Lt, e.g.
apply thresholding to obtain a fixed number of
expansion terms.
6. Spread activation back to the document level to
obtain the final retrieval status values of docu-
ments.
</listItem>
<bodyText confidence="0.92149525">
lower than MAP scores achieved by systems actually partici-
pating in TREC.
In order to instantiate a particular feedback algo-
rithm, there are three parameters to be specified:
</bodyText>
<listItem confidence="0.99563725">
• The transformation to be applied in step 2
• The weighting of document-term edges (if dif-
ferent from term-document edges) and
• The transformation applied in step 5.
</listItem>
<bodyText confidence="0.9993994">
Unfortunately, due to space constraints, it is out of
the scope of this paper to show how different spec-
ifications lead to well-known feedback algorithms
such as Rocchio (Rocchio, 1971) or the probabilistic
model above.
</bodyText>
<subsectionHeader confidence="0.998839">
6.2 Associative retrieval
</subsectionHeader>
<bodyText confidence="0.999980083333333">
Associative retrieval in MLAGs exploits the infor-
mation encoded in level graphs: expanding queries
with related terms can be realised by using the term
level graph Lt of a simple MLAG (cf. figure 1) in
step 2 of the basic processing, whereas the expan-
sion of document result sets takes place in step 5 on
the document level Ld. In order to exploit the rela-
tions encoded in the level graphs, one may again use
spreading activation, but also simpler mechanisms.
Since relations are used directly, dimensionality re-
duction techniques such as LSI cannot and need not
be modeled.
</bodyText>
<subsectionHeader confidence="0.992321">
6.3 Browsing
</subsectionHeader>
<bodyText confidence="0.996284">
Since the MLAG framework is graph-based, it is
easy to grasp and to be visualised, which makes it
</bodyText>
<page confidence="0.992377">
15
</page>
<bodyText confidence="0.999250777777778">
a suitable data structure for browsing. The level
graphs can be used as a flat graphical representa-
tion of the data, which can be exploited directly
for browsing. Depending on their information need,
users can choose to browse either on the term level
Lt or on the document level Ld and they can switch
between both types of levels at any time using the
inverted list Itd. This applies, of course, also to pas-
sage or any other type of levels if they exist.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999746">
In this paper, a new graph-based framework for in-
formation retrieval has been introduced that allows
to subsume a wide range of IR models and algo-
rithms. It has been shown how this common rep-
resentation can be an inspiration and lead to new
insights and algorithms that outperform the origi-
nal ones. Future work will aim at finding similar
forms of synergies for the different forms of search,
e.g. new combinations of feedback and associative
retrieval algorithms.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999675853333333">
M. Agosti and F. Crestani. 1993. A methodology for the au-
tomatic construction of a hypertext for information retrieval.
In Proceedings of SAC 1993, pages 745–753.
J. R. Anderson and P. L. Pirolli. 1984. Spread of activa-
tion. Journal of Experimental Psychology: Learning, Mem-
ory and Cognition, 10:791–799.
M. Beaulieu. 1997. Experiments of interfaces to support query
expansion. Journal of Documentation, 1(53):8–19.
R. Brandow, K. Mitze, and L. F. Rau. 1995. Automatic con-
densation of electronic publications by sentence selection.
Information Processing and Management, 31(5):675–685.
S. Brin and L. Page. 1998. The anatomy of a large-scale hyper-
textual Web search engine. In Proceedings of WWW7, pages
107–117.
F. Crestani. 1997. Application of spreading activation tech-
niques in information retrieval. Artificial Intelligence Re-
view, 11(6):453–482.
W. B. Croft and R. H. Thompson. 1987. I3R : a new approach
to the design of document retrieval systems. Journal of the
american society for information science, 38(6):389–404.
L. B. Doyle. 1961. Semantic Road Maps for Literature
Searchers. Journal of the ACM, 8(4):553–578.
D. Hiemstra. 2001. Using language models for information
retrieval. Ph.D. thesis, University of Twente.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for information re-
trieval. In Proceedings of SIGIR 2001, pages 111–119.
D. Metzler and W. B. Croft. 2004. Combining the language
model and inference network approaches to retrieval. Infor-
mation Processing and Management, 40(5):735–750.
R. N. Oddy. 1977. Information retrieval through man-machine
dialogue. Journal of Documentation, 33(1):1–14.
C. Olston and E. H. Chi. 2003. ScentTrails: Integrating
browsing and searching on the Web. ACM Transactions on
Computer-Human Interaction, 10(3):177–197.
S. E. Preece. 1981. A spreading activation network model for
information retrieval. Ph.D. thesis, Universtiy of Illinois at
Urbana-Champaign.
M. Richardson and P. Domingos. 2002. The intelligent surfer:
Probabilistic combination of link and content information in
pagerank. In Proceedings of Advances in Neural Informa-
tion Processing Systems.
S. E. Robertson and K. Sparck-Jones. 1976. Relevance Weight-
ing of Search Terms. JASIS, 27(3):129–146.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu, A. Gull, and
M. Lau. 1992. Okapi at TREC-3. In Proceedings of TREC,
pages 21–30.
J.J. Rocchio. 1971. Relevance feedback in information re-
trieval. In G. Salton, editor, The SMART Retrieval System
: Experiments in Automatic Document Processing. Prentice
Hall Inc., Englewood Cliffs, New Jersey.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space
model for automatic indexing. Communications of the ACM,
18(11):613–620.
A. Singhal, C. Buckley, and M. Mitra. 1996. Pivoted document
length normalization. In Proceedings of SIGIR 1996, pages
21–29.
M. D. Smucker and J. Allan. 2006. Find-similar: similarity
browsing as a search tool. In Proceedings of SIGIR 2006,
pages 461–468.
H. Turtle and W. B. Croft. 1990. Inference networks for docu-
ment retrieval. In Proceedings of SIGIR 1990, pages 1–24.
C. J. van Rijsbergen. 2004. The Geometry of Information Re-
trieval. Cambridge University Press.
Scott White and Padhraic Smyth. 2003. Algorithms for esti-
mating relative importance in networks. In Proceedings of
KDD 2003, pages 266–275.
H. F. Witschel. 2006. Carrot and stick: combining information
retrieval models. In Proceedings of DocEng 2006, page 32.
S. K. M. Wong and Y. Y. Yao. 1995. On modeling information
retrieval with probabilistic inference. ACM Transactions on
Information Systems, 13(1):38–68.
C. Zhai and J. Lafferty. 2001. A study of smoothing methods
for language models applied to Ad Hoc information retrieval.
In Proceedings of SIGIR 2001, pages 334–342.
</reference>
<page confidence="0.998686">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921527">
<title confidence="0.9997745">Multi-level Association Graphs – A New Graph-Based Model for Information Retrieval</title>
<author confidence="0.999567">Hans Friedrich</author>
<affiliation confidence="0.983876">NLP University of</affiliation>
<address confidence="0.99591">04009 Leipzig, P.O. Box</address>
<email confidence="0.995378">witschel@informatik.uni-leipzig.de</email>
<abstract confidence="0.9972620625">paper introduces associagraphs a new graph-based framework for information retrieval (IR). The goal of that framework is twofold: First, it is meant to be a meta model of IR, i.e. it subsumes various IR models under one common representation. Second, it allows to model different forms of search, such as feedback, associative retrieval and browsing at the same time. It is shown how the new integrated model gives insights and stimulates new ideas for IR algorithms. One of these new ideas is presented and evaluated, yielding promising experimental results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Agosti</author>
<author>F Crestani</author>
</authors>
<title>A methodology for the automatic construction of a hypertext for information retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of SAC</booktitle>
<pages>745--753</pages>
<contexts>
<context position="7452" citStr="Agosti and Crestani, 1993" startWordPosition="1208" endWordPosition="1211"> which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), as proposed in 10 (Agosti and Crestani, 1993) – a data structure consisting of three levels, for documents, index terms and concepts. Each level contains objects and links among them. There are also connections between objects of two adjacent levels. An MLHT is meant to be used for interactive query formulation, browsing and search, although (Agosti and Crestani, 1993) give no precise specification of the processing procedures. 2.3 Contribution of this work Compared to Preece’s work, the MLAG framework makes two sorts of modifications in order to reach the goals formulated in the introduction: in order to subsume more IR models, the flex</context>
<context position="8808" citStr="Agosti and Crestani, 1993" startWordPosition="1424" endWordPosition="1427">ade between local and global information through the explicit introduction of “level graphs”. With the introduction of levels, the MLAG data structure becomes very closely related to the MLHT paradigm of (Agosti and Crestani, 1993), MLAGs, however, generalise MLHTs by allowing arbitrary types of levels, not only the three types proposed in (Agosti and Crestani, 1993). Additionally, links in MLAGs are weighted and the spreading activation processing defined in the next section makes extensive use of these weights. All in all, the new model combines the data structure of multi-level hypertexts (Agosti and Crestani, 1993) with the processing paradigm of spreading activation as proposed by Preece (Preece, 1981), refining both with an adequate edge weighting. The framework is an attempt to be as general as necessary for subsuming all models and allowing for different forms of search, while at the same time being as specific as possible about the things that are really common to all IR models. 3 The MLAG model 3.1 Data structure Formally, the basis of a multi-level association graph (MLAG) is a union of n level graphs L1, ..., Ln. Each of these n directed graphs Li = G(VLi, ELi, WLi) consists of a set of vertices</context>
</contexts>
<marker>Agosti, Crestani, 1993</marker>
<rawString>M. Agosti and F. Crestani. 1993. A methodology for the automatic construction of a hypertext for information retrieval. In Proceedings of SAC 1993, pages 745–753.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
<author>P L Pirolli</author>
</authors>
<title>Spread of activation.</title>
<date>1984</date>
<journal>Journal of Experimental Psychology: Learning, Memory and Cognition,</journal>
<pages>10--791</pages>
<contexts>
<context position="4791" citStr="Anderson and Pirolli, 1984" startWordPosition="773" endWordPosition="776"> at that time can be subsumed using probabilistic inference. Language modeling, which was not known then was later added to the list by (Metzler and Croft, 2004). Another graph-based meta modeling approach uses the paradigm of spreading activation (SA) as a simple unifying framework. Given semantic knowledge in the form of a (directed) graph, the idea of spreading activation is that a measure of relevance – w.r.t. a current focus of attention – is spread over the graph’s edges in the form of activation energy, yielding for each vertex in the graph a degree of relatedness with that focus (cf. (Anderson and Pirolli, 1984)). It is easy to see how this relates to IR: using a graph that contains vertices for both terms and documents and appropriate links between the two, we can interpret a query as a focus of attention and spread that over the network in order to rank documents by their degree of relatedness to that focus. A very general introduction of spreading activation as a meta model is given in the early work by (Preece, 1981) All later models are hence special cases of Preece’s work, including the multi-level association graphs introduced in section 3. Preece’s model subsumes the Boolean retrieval model, </context>
</contexts>
<marker>Anderson, Pirolli, 1984</marker>
<rawString>J. R. Anderson and P. L. Pirolli. 1984. Spread of activation. Journal of Experimental Psychology: Learning, Memory and Cognition, 10:791–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beaulieu</author>
</authors>
<title>Experiments of interfaces to support query expansion.</title>
<date>1997</date>
<journal>Journal of Documentation,</journal>
<volume>1</volume>
<issue>53</issue>
<contexts>
<context position="6807" citStr="Beaulieu, 1997" startWordPosition="1113" endWordPosition="1114">d in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), </context>
</contexts>
<marker>Beaulieu, 1997</marker>
<rawString>M. Beaulieu. 1997. Experiments of interfaces to support query expansion. Journal of Documentation, 1(53):8–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brandow</author>
<author>K Mitze</author>
<author>L F Rau</author>
</authors>
<title>Automatic condensation of electronic publications by sentence selection.</title>
<date>1995</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>31--5</pages>
<contexts>
<context position="11787" citStr="Brandow et al., 1995" startWordPosition="1972" endWordPosition="1975">e.g. based on the number of terms shared by two documents. However, other forms of edges are possible, such as hyperlinks or citations – if available. Term associations, on the other hand, can be computed using co-occurrence information. An alternative would be to use relations from manually created thesauri or ontologies. 3.2.2 More levels In order to (partly) take document structure into account, it can be useful to introduce a level for document parts (e.g. headlines and/or passages) in between the term and the document level. This can be combined with text summarisation methods (cf. e.g. (Brandow et al., 1995)) in order to give higher weights to more important passages in the inverted list connecting passages to documents. In distributed or peer-to-peer environments, databases or peers may be modeled in a separate layer above the document level, inverted lists indicating where documents are held. Additionally, a peer’s neighbours in an overlay network may be modeled by directed edges in the peer level graph. More extensions are possible and the flexibility of the MLAG framework allows for the insertion of arbitrary layers. 3.3 Processing paradigm The operating mode of an MLAG is based on the spread</context>
</contexts>
<marker>Brandow, Mitze, Rau, 1995</marker>
<rawString>R. Brandow, K. Mitze, and L. F. Rau. 1995. Automatic condensation of electronic publications by sentence selection. Information Processing and Management, 31(5):675–685.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual Web search engine.</title>
<date>1998</date>
<booktitle>In Proceedings of WWW7,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="6388" citStr="Brin and Page, 1998" startWordPosition="1043" endWordPosition="1046">raph-based models for associative retrieval and browsing The spreading activation paradigm is also often used for associative retrieval. The idea is to reach vertices in the graph that are not necessarily directly linked to query nodes, but are reachable from query nodes via a large number of short paths along highly weighted edges. Besides (Preece, 1981), much more work on SA was done, a good survey of which can be found in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and comb</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>S. Brin and L. Page. 1998. The anatomy of a large-scale hypertextual Web search engine. In Proceedings of WWW7, pages 107–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Crestani</author>
</authors>
<title>Application of spreading activation techniques in information retrieval.</title>
<date>1997</date>
<journal>Artificial Intelligence Review,</journal>
<volume>11</volume>
<issue>6</issue>
<contexts>
<context position="6213" citStr="Crestani, 1997" startWordPosition="1014" endWordPosition="1015">f that space to probability and logics. In particular, he manages to give the familiar dot product between query and document vector a probabilistic interpretation. 2.2 Graph-based models for associative retrieval and browsing The spreading activation paradigm is also often used for associative retrieval. The idea is to reach vertices in the graph that are not necessarily directly linked to query nodes, but are reachable from query nodes via a large number of short paths along highly weighted edges. Besides (Preece, 1981), much more work on SA was done, a good survey of which can be found in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), docu</context>
</contexts>
<marker>Crestani, 1997</marker>
<rawString>F. Crestani. 1997. Application of spreading activation techniques in information retrieval. Artificial Intelligence Review, 11(6):453–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Croft</author>
<author>R H Thompson</author>
</authors>
<title>I3R : a new approach to the design of document retrieval systems.</title>
<date>1987</date>
<journal>Journal of the</journal>
<pages>38--6</pages>
<contexts>
<context position="7312" citStr="Croft and Thompson, 1987" startWordPosition="1185" endWordPosition="1188">x term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), as proposed in 10 (Agosti and Crestani, 1993) – a data structure consisting of three levels, for documents, index terms and concepts. Each level contains objects and links among them. There are also connections between objects of two adjacent levels. An MLHT is meant to be used for interactive query formulation, browsing and search, although (Agosti and Crestani, 1993) give no precise specification of the processing procedures. 2.3 Contribution of this work Compared to Preece’s work, the MLAG framewo</context>
</contexts>
<marker>Croft, Thompson, 1987</marker>
<rawString>W. B. Croft and R. H. Thompson. 1987. I3R : a new approach to the design of document retrieval systems. Journal of the american society for information science, 38(6):389–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L B Doyle</author>
</authors>
<title>Semantic Road Maps for Literature Searchers.</title>
<date>1961</date>
<journal>Journal of the ACM,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="6790" citStr="Doyle, 1961" startWordPosition="1111" endWordPosition="1112">h can be found in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level h</context>
</contexts>
<marker>Doyle, 1961</marker>
<rawString>L. B. Doyle. 1961. Semantic Road Maps for Literature Searchers. Journal of the ACM, 8(4):553–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
</authors>
<title>Using language models for information retrieval.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Twente.</institution>
<contexts>
<context position="1021" citStr="Hiemstra, 2001" startWordPosition="153" endWordPosition="154"> meta model of IR, i.e. it subsumes various IR models under one common representation. Second, it allows to model different forms of search, such as feedback, associative retrieval and browsing at the same time. It is shown how the new integrated model gives insights and stimulates new ideas for IR algorithms. One of these new ideas is presented and evaluated, yielding promising experimental results. 1 Introduction Developing formal models for information retrieval has a long history. A model of information retrieval “predicts and explains what a user will find relevant given the user query” (Hiemstra, 2001). Most IR models are firmly grounded in mathematics and thus provide a formalisation of ideas that facilitates discussion and makes sure that the ideas can be implemented. More specifically, most IR models provide a so-called retrieval function f(q, d) , which returns – for given representations of a document d and of a user information need q – a so-called retrieval status value by which documents can be ranked according to their presumed relevance w.r.t. to the query q. In order to understand the commonalities and differences among IR models, this paper introduces the notion of meta modeling</context>
</contexts>
<marker>Hiemstra, 2001</marker>
<rawString>D. Hiemstra. 2001. Using language models for information retrieval. Ph.D. thesis, University of Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document language models, query models, and risk minimization for information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>111--119</pages>
<contexts>
<context position="17713" citStr="Lafferty and Zhai, 2001" startWordPosition="3005" endWordPosition="3008">s the use of relevance information which can be gained via feedback, described in section 6.1. 4.3 Language models The general language modeling retrieval function (cf. e.g. (Zhai and Lafferty, 2001)) is – admittedly – not in the linear form of equation 1. But using logarithms, products can be turned into sums without changing the ranking – the logarithm being a monotonic function (note that this is what also happened in the case of the probabilistic relevance models). In particular, we will use the approach of comparing query and document language models by Kullback-Leibler divergence (KLD) (Lafferty and Zhai, 2001) which results in the equation � P (t|Mq) KLD(Mq||Md) = P(t|Mq) log P(t|Md) tEq �OC − P(t|Mq) log P(t|Md) tEq where P(t|Mq) and P(t|Md) refer to the probability that term t will be generated by the unigram language model of query q or document d, respectively. Note that we have simplified the equation by dropping a term &amp; P(t|Mq) log P(t|Mq), which depends only on the query, not on the documents to be ranked. Now, the triplet (P(t|Mq), − log P(t|Md), t) can be used to realise this retrieval function where t stands for a procedure that adds −P(t|Mq) log P(t|Md) to the document node’s activation</context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>J. Lafferty and C. Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR 2001, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Metzler</author>
<author>W B Croft</author>
</authors>
<title>Combining the language model and inference network approaches to retrieval.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>40--5</pages>
<contexts>
<context position="4325" citStr="Metzler and Croft, 2004" startWordPosition="693" endWordPosition="697">s a rather limited number of meta models for IR, the most important of which will be described here very shortly. Most research about how to subsume various IR models in a common framework has been done in the context of Bayesian networks and probabilistic inference (Turtle and Croft, 1990). In this approach, models are subsumed by specifying certain probability distributions. In (Wong and Yao, 1995), the authors elaborately show how all major IR models known at that time can be subsumed using probabilistic inference. Language modeling, which was not known then was later added to the list by (Metzler and Croft, 2004). Another graph-based meta modeling approach uses the paradigm of spreading activation (SA) as a simple unifying framework. Given semantic knowledge in the form of a (directed) graph, the idea of spreading activation is that a measure of relevance – w.r.t. a current focus of attention – is spread over the graph’s edges in the form of activation energy, yielding for each vertex in the graph a degree of relatedness with that focus (cf. (Anderson and Pirolli, 1984)). It is easy to see how this relates to IR: using a graph that contains vertices for both terms and documents and appropriate links b</context>
</contexts>
<marker>Metzler, Croft, 2004</marker>
<rawString>D. Metzler and W. B. Croft. 2004. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R N Oddy</author>
</authors>
<title>Information retrieval through man-machine dialogue.</title>
<date>1977</date>
<journal>Journal of Documentation,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="7285" citStr="Oddy, 1977" startWordPosition="1183" endWordPosition="1184"> graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), as proposed in 10 (Agosti and Crestani, 1993) – a data structure consisting of three levels, for documents, index terms and concepts. Each level contains objects and links among them. There are also connections between objects of two adjacent levels. An MLHT is meant to be used for interactive query formulation, browsing and search, although (Agosti and Crestani, 1993) give no precise specification of the processing procedures. 2.3 Contribution of this work Compared to Pree</context>
</contexts>
<marker>Oddy, 1977</marker>
<rawString>R. N. Oddy. 1977. Information retrieval through man-machine dialogue. Journal of Documentation, 33(1):1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Olston</author>
<author>E H Chi</author>
</authors>
<title>ScentTrails: Integrating browsing and searching on the Web.</title>
<date>2003</date>
<journal>ACM Transactions on Computer-Human Interaction,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="6979" citStr="Olston and Chi, 2003" startWordPosition="1138" endWordPosition="1141">PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), as proposed in 10 (Agosti and Crestani, 1993) – a data structure consisting of three levels, for documents, index terms and concepts. Each level contains objects and links </context>
</contexts>
<marker>Olston, Chi, 2003</marker>
<rawString>C. Olston and E. H. Chi. 2003. ScentTrails: Integrating browsing and searching on the Web. ACM Transactions on Computer-Human Interaction, 10(3):177–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Preece</author>
</authors>
<title>A spreading activation network model for information retrieval.</title>
<date>1981</date>
<tech>Ph.D. thesis,</tech>
<institution>Universtiy of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="5208" citStr="Preece, 1981" startWordPosition="854" endWordPosition="855">us of attention – is spread over the graph’s edges in the form of activation energy, yielding for each vertex in the graph a degree of relatedness with that focus (cf. (Anderson and Pirolli, 1984)). It is easy to see how this relates to IR: using a graph that contains vertices for both terms and documents and appropriate links between the two, we can interpret a query as a focus of attention and spread that over the network in order to rank documents by their degree of relatedness to that focus. A very general introduction of spreading activation as a meta model is given in the early work by (Preece, 1981) All later models are hence special cases of Preece’s work, including the multi-level association graphs introduced in section 3. Preece’s model subsumes the Boolean retrieval model, coordination level matching and vector space processing. Finally, an interesting meta model is described by (van Rijsbergen, 2004) who uses a Hilbert space as an information space and connects the geometry of that space to probability and logics. In particular, he manages to give the familiar dot product between query and document vector a probabilistic interpretation. 2.2 Graph-based models for associative retrie</context>
<context position="8898" citStr="Preece, 1981" startWordPosition="1440" endWordPosition="1441">introduction of levels, the MLAG data structure becomes very closely related to the MLHT paradigm of (Agosti and Crestani, 1993), MLAGs, however, generalise MLHTs by allowing arbitrary types of levels, not only the three types proposed in (Agosti and Crestani, 1993). Additionally, links in MLAGs are weighted and the spreading activation processing defined in the next section makes extensive use of these weights. All in all, the new model combines the data structure of multi-level hypertexts (Agosti and Crestani, 1993) with the processing paradigm of spreading activation as proposed by Preece (Preece, 1981), refining both with an adequate edge weighting. The framework is an attempt to be as general as necessary for subsuming all models and allowing for different forms of search, while at the same time being as specific as possible about the things that are really common to all IR models. 3 The MLAG model 3.1 Data structure Formally, the basis of a multi-level association graph (MLAG) is a union of n level graphs L1, ..., Ln. Each of these n directed graphs Li = G(VLi, ELi, WLi) consists of a set of vertices VLi, a set ELi C V Li x V Li of edges and a function WLi : ELi —* R returning edge weight</context>
<context position="16687" citStr="Preece, 1981" startWordPosition="2830" endWordPosition="2831">qwtd (2) tEqnd where wtq and wtd are a term’s weight in the query q and the current document d, respectively. This can be achieved by specifying the parameter triplet (wtq, wtd, none). This simple representation reflects the closeness of the MLAG paradigm to the vector space model that has been hinted at above. 4.2 Probabilistic model For the probabilistic relevance model (Robertson and Sparck-Jones, 1976), the MLAG has to realise the following retrieval function di log pi(1 − ri) (3) ri(1 − pi) 1This excludes the Boolean model, which can, however, also be subsumed as shown in section 5.5 of (Preece, 1981) where di E 10, 11 indicates whether term i is contained in document d, pi is the probability that a relevant document will contain term i and ri is the probability that an irrelevant document will contain it. This retrieval function is realised by the parameter triplet (log p.��−r.� r.��−p.�, di, none). Now there is still the question of how the estimates of pi and ri are derived. This task involves the use of relevance information which can be gained via feedback, described in section 6.1. 4.3 Language models The general language modeling retrieval function (cf. e.g. (Zhai and Lafferty, 2001</context>
</contexts>
<marker>Preece, 1981</marker>
<rawString>S. E. Preece. 1981. A spreading activation network model for information retrieval. Ph.D. thesis, Universtiy of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>The intelligent surfer: Probabilistic combination of link and content information in pagerank.</title>
<date>2002</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="6556" citStr="Richardson and Domingos, 2002" startWordPosition="1070" endWordPosition="1073">ertices in the graph that are not necessarily directly linked to query nodes, but are reachable from query nodes via a large number of short paths along highly weighted edges. Besides (Preece, 1981), much more work on SA was done, a good survey of which can be found in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing inter</context>
</contexts>
<marker>Richardson, Domingos, 2002</marker>
<rawString>M. Richardson and P. Domingos. 2002. The intelligent surfer: Probabilistic combination of link and content information in pagerank. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparck-Jones</author>
</authors>
<title>Relevance Weighting of Search Terms.</title>
<date>1976</date>
<journal>JASIS,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="16483" citStr="Robertson and Sparck-Jones, 1976" startWordPosition="2791" endWordPosition="2794">given in the form of triplets (activationinit, edge weights, transform). 4.1 Vector space model In the case of the vector space model (Salton et al., 1975), the retrieval function to be mimicked is as follows: f(q, d) = � wtqwtd (2) tEqnd where wtq and wtd are a term’s weight in the query q and the current document d, respectively. This can be achieved by specifying the parameter triplet (wtq, wtd, none). This simple representation reflects the closeness of the MLAG paradigm to the vector space model that has been hinted at above. 4.2 Probabilistic model For the probabilistic relevance model (Robertson and Sparck-Jones, 1976), the MLAG has to realise the following retrieval function di log pi(1 − ri) (3) ri(1 − pi) 1This excludes the Boolean model, which can, however, also be subsumed as shown in section 5.5 of (Preece, 1981) where di E 10, 11 indicates whether term i is contained in document d, pi is the probability that a relevant document will contain term i and ri is the probability that an irrelevant document will contain it. This retrieval function is realised by the parameter triplet (log p.��−r.� r.��−p.�, di, none). Now there is still the question of how the estimates of pi and ri are derived. This task i</context>
</contexts>
<marker>Robertson, Sparck-Jones, 1976</marker>
<rawString>S. E. Robertson and K. Sparck-Jones. 1976. Relevance Weighting of Search Terms. JASIS, 27(3):129–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M Hancock-Beaulieu</author>
<author>A Gull</author>
<author>M Lau</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1992</date>
<booktitle>In Proceedings of TREC,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="20756" citStr="Robertson et al., 1992" startWordPosition="3536" endWordPosition="3539">ntribution that one occurrence of that term would have earned (cf. (Witschel, 2006)). For the vector space model, this yields the following retrieval function: f(q, d) = 1: wtqwtd tEqnd wtd(tf = 1)wtq 2In order to do this, we only need to know |d |and the relative frequency of t in the collection p(tIC), i.e. information that is available outside the inverted list. where α is a free parameter regulating the relative influence of penalties, comparable to the µ parameter of language models above. 5.1 Experimental results Table 2 shows retrieval results for combining two weighting schemes, BM25 (Robertson et al., 1992) and Lnu.ltn (Singhal et al., 1996), with penalties. Both of them belong to the family of tf.idf weighting schemes and can hence be regarded as representing the vector space model, although BM25 was developed out of the probabilistic model. Combining them with the idea of “absence penalties” works as indicated above, i.e. weights are accumulated for each document using the tf.idf-like retrieval functions. Then, from each score, the contributions that one occurrence of each missing term would have earned is subtracted. More precisely, what is subtracted consists of the usual tf.idf weight for t</context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>S. E. Robertson, S. Walker, M. Hancock-Beaulieu, A. Gull, and M. Lau. 1992. Okapi at TREC-3. In Proceedings of TREC, pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Rocchio</author>
</authors>
<title>Relevance feedback in information retrieval.</title>
<date>1971</date>
<booktitle>The SMART Retrieval System : Experiments in Automatic Document Processing.</booktitle>
<editor>In G. Salton, editor,</editor>
<publisher>Prentice Hall Inc.,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="24965" citStr="Rocchio, 1971" startWordPosition="4252" endWordPosition="4253"> back to the document level to obtain the final retrieval status values of documents. lower than MAP scores achieved by systems actually participating in TREC. In order to instantiate a particular feedback algorithm, there are three parameters to be specified: • The transformation to be applied in step 2 • The weighting of document-term edges (if different from term-document edges) and • The transformation applied in step 5. Unfortunately, due to space constraints, it is out of the scope of this paper to show how different specifications lead to well-known feedback algorithms such as Rocchio (Rocchio, 1971) or the probabilistic model above. 6.2 Associative retrieval Associative retrieval in MLAGs exploits the information encoded in level graphs: expanding queries with related terms can be realised by using the term level graph Lt of a simple MLAG (cf. figure 1) in step 2 of the basic processing, whereas the expansion of document result sets takes place in step 5 on the document level Ld. In order to exploit the relations encoded in the level graphs, one may again use spreading activation, but also simpler mechanisms. Since relations are used directly, dimensionality reduction techniques such as </context>
</contexts>
<marker>Rocchio, 1971</marker>
<rawString>J.J. Rocchio. 1971. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System : Experiments in Automatic Document Processing. Prentice Hall Inc., Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Communications of the ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="16005" citStr="Salton et al., 1975" startWordPosition="2712" endWordPosition="2715">he simple MLAG of figure 1 and the processing paradigm from the last section. This is done by specifying the following parameters of that paradigm: 1. How nodes are activated in the very first step 2. How edges of the inverted list are weighted 3. Which transformation is used in 2 and 5. For each model, the corresponding retrieval function will be given and the parameter specification will be discussed shortly. The specification of the above parameters will be given in the form of triplets (activationinit, edge weights, transform). 4.1 Vector space model In the case of the vector space model (Salton et al., 1975), the retrieval function to be mimicked is as follows: f(q, d) = � wtqwtd (2) tEqnd where wtq and wtd are a term’s weight in the query q and the current document d, respectively. This can be achieved by specifying the parameter triplet (wtq, wtd, none). This simple representation reflects the closeness of the MLAG paradigm to the vector space model that has been hinted at above. 4.2 Probabilistic model For the probabilistic relevance model (Robertson and Sparck-Jones, 1976), the MLAG has to realise the following retrieval function di log pi(1 − ri) (3) ri(1 − pi) 1This excludes the Boolean mod</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
<author>C Buckley</author>
<author>M Mitra</author>
</authors>
<title>Pivoted document length normalization.</title>
<date>1996</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>21--29</pages>
<contexts>
<context position="20791" citStr="Singhal et al., 1996" startWordPosition="3542" endWordPosition="3545">t term would have earned (cf. (Witschel, 2006)). For the vector space model, this yields the following retrieval function: f(q, d) = 1: wtqwtd tEqnd wtd(tf = 1)wtq 2In order to do this, we only need to know |d |and the relative frequency of t in the collection p(tIC), i.e. information that is available outside the inverted list. where α is a free parameter regulating the relative influence of penalties, comparable to the µ parameter of language models above. 5.1 Experimental results Table 2 shows retrieval results for combining two weighting schemes, BM25 (Robertson et al., 1992) and Lnu.ltn (Singhal et al., 1996), with penalties. Both of them belong to the family of tf.idf weighting schemes and can hence be regarded as representing the vector space model, although BM25 was developed out of the probabilistic model. Combining them with the idea of “absence penalties” works as indicated above, i.e. weights are accumulated for each document using the tf.idf-like retrieval functions. Then, from each score, the contributions that one occurrence of each missing term would have earned is subtracted. More precisely, what is subtracted consists of the usual tf.idf weight for the missing term, where tf = 1 is su</context>
</contexts>
<marker>Singhal, Buckley, Mitra, 1996</marker>
<rawString>A. Singhal, C. Buckley, and M. Mitra. 1996. Pivoted document length normalization. In Proceedings of SIGIR 1996, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Smucker</author>
<author>J Allan</author>
</authors>
<title>Find-similar: similarity browsing as a search tool.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>461--468</pages>
<contexts>
<context position="6956" citStr="Smucker and Allan, 2006" startWordPosition="1134" endWordPosition="1137">ticular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding principle of many graph-based browsing approahces is that of interactive spreading activation (Oddy, 1977; Croft and Thompson, 1987). Another approach, which is very closely related to MLAGs, is a multi-level hypertext (MLHT), as proposed in 10 (Agosti and Crestani, 1993) – a data structure consisting of three levels, for documents, index terms and concepts. Each level cont</context>
</contexts>
<marker>Smucker, Allan, 2006</marker>
<rawString>M. D. Smucker and J. Allan. 2006. Find-similar: similarity browsing as a search tool. In Proceedings of SIGIR 2006, pages 461–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Turtle</author>
<author>W B Croft</author>
</authors>
<title>Inference networks for document retrieval.</title>
<date>1990</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>1--24</pages>
<contexts>
<context position="3992" citStr="Turtle and Croft, 1990" startWordPosition="638" endWordPosition="641">owsing, i.e. exploring a document collection interactively by following links between objects such as documents, terms or concepts. Again, it will be shown that – using a graph-based representation – these forms of search can be subsumed easily. 2 Related work 2.1 Meta modeling In the literal sense of the definition above, there is a rather limited number of meta models for IR, the most important of which will be described here very shortly. Most research about how to subsume various IR models in a common framework has been done in the context of Bayesian networks and probabilistic inference (Turtle and Croft, 1990). In this approach, models are subsumed by specifying certain probability distributions. In (Wong and Yao, 1995), the authors elaborately show how all major IR models known at that time can be subsumed using probabilistic inference. Language modeling, which was not known then was later added to the list by (Metzler and Croft, 2004). Another graph-based meta modeling approach uses the paradigm of spreading activation (SA) as a simple unifying framework. Given semantic knowledge in the form of a (directed) graph, the idea of spreading activation is that a measure of relevance – w.r.t. a current </context>
</contexts>
<marker>Turtle, Croft, 1990</marker>
<rawString>H. Turtle and W. B. Croft. 1990. Inference networks for document retrieval. In Proceedings of SIGIR 1990, pages 1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>The Geometry of Information Retrieval.</title>
<date>2004</date>
<publisher>Cambridge University Press.</publisher>
<marker>van Rijsbergen, 2004</marker>
<rawString>C. J. van Rijsbergen. 2004. The Geometry of Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott White</author>
<author>Padhraic Smyth</author>
</authors>
<title>Algorithms for estimating relative importance in networks.</title>
<date>2003</date>
<booktitle>In Proceedings of KDD</booktitle>
<pages>266--275</pages>
<contexts>
<context position="6580" citStr="White and Smyth, 2003" startWordPosition="1074" endWordPosition="1077">ot necessarily directly linked to query nodes, but are reachable from query nodes via a large number of short paths along highly weighted edges. Besides (Preece, 1981), much more work on SA was done, a good survey of which can be found in (Crestani, 1997). A renewed interest in SA was later triggered with the advent of the WWW where hyperlinks form a directed graph. In particular, variants of the PageRank (Brin and Page, 1998) algorithm that bias a random searcher towards some starting nodes (e.g. an initial result set of documents) bear close resemblance to SA (Richardson and Domingos, 2002; White and Smyth, 2003). Turning to browsing, we can distinguish three types of browsing w.r.t. to the vertices of the graph: index term browsing, which supports the user in formulating his query by picking related terms (Doyle, 1961; Beaulieu, 1997), document browsing which serves to expand result sets by allowing access to similar documents or by supporting web browsing (Smucker and Allan, 2006; Olston and Chi, 2003) and combined approaches where both index terms and documents are used simultaneously for browsing. In this last category, many different possibilities arise for designing interfaces. A common guiding </context>
</contexts>
<marker>White, Smyth, 2003</marker>
<rawString>Scott White and Padhraic Smyth. 2003. Algorithms for estimating relative importance in networks. In Proceedings of KDD 2003, pages 266–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H F Witschel</author>
</authors>
<title>Carrot and stick: combining information retrieval models.</title>
<date>2006</date>
<booktitle>In Proceedings of DocEng</booktitle>
<pages>32</pages>
<contexts>
<context position="20216" citStr="Witschel, 2006" startWordPosition="3447" endWordPosition="3448">2 This transformation indicates an important difference between language modeling and all other IR models: language models penalise documents for the absence of rare (i.e. informative) terms whereas the other models reward them for the presence of these terms. These considerations suggest a combination of both approaches: starting with an arbitrary “presence rewarding” model – e.g. the vector space model – we may integrate the “absence penalising” philosophy by subtracting from a document’s score, for each missing term, the contribution that one occurrence of that term would have earned (cf. (Witschel, 2006)). For the vector space model, this yields the following retrieval function: f(q, d) = 1: wtqwtd tEqnd wtd(tf = 1)wtq 2In order to do this, we only need to know |d |and the relative frequency of t in the collection p(tIC), i.e. information that is available outside the inverted list. where α is a free parameter regulating the relative influence of penalties, comparable to the µ parameter of language models above. 5.1 Experimental results Table 2 shows retrieval results for combining two weighting schemes, BM25 (Robertson et al., 1992) and Lnu.ltn (Singhal et al., 1996), with penalties. Both of</context>
</contexts>
<marker>Witschel, 2006</marker>
<rawString>H. F. Witschel. 2006. Carrot and stick: combining information retrieval models. In Proceedings of DocEng 2006, page 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>Y Y Yao</author>
</authors>
<title>On modeling information retrieval with probabilistic inference.</title>
<date>1995</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="4104" citStr="Wong and Yao, 1995" startWordPosition="655" endWordPosition="658">ms or concepts. Again, it will be shown that – using a graph-based representation – these forms of search can be subsumed easily. 2 Related work 2.1 Meta modeling In the literal sense of the definition above, there is a rather limited number of meta models for IR, the most important of which will be described here very shortly. Most research about how to subsume various IR models in a common framework has been done in the context of Bayesian networks and probabilistic inference (Turtle and Croft, 1990). In this approach, models are subsumed by specifying certain probability distributions. In (Wong and Yao, 1995), the authors elaborately show how all major IR models known at that time can be subsumed using probabilistic inference. Language modeling, which was not known then was later added to the list by (Metzler and Croft, 2004). Another graph-based meta modeling approach uses the paradigm of spreading activation (SA) as a simple unifying framework. Given semantic knowledge in the form of a (directed) graph, the idea of spreading activation is that a measure of relevance – w.r.t. a current focus of attention – is spread over the graph’s edges in the form of activation energy, yielding for each vertex</context>
</contexts>
<marker>Wong, Yao, 1995</marker>
<rawString>S. K. M. Wong and Y. Y. Yao. 1995. On modeling information retrieval with probabilistic inference. ACM Transactions on Information Systems, 13(1):38–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Zhai</author>
<author>J Lafferty</author>
</authors>
<title>A study of smoothing methods for language models applied to Ad Hoc information retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>334--342</pages>
<contexts>
<context position="17288" citStr="Zhai and Lafferty, 2001" startWordPosition="2933" endWordPosition="2936">n 5.5 of (Preece, 1981) where di E 10, 11 indicates whether term i is contained in document d, pi is the probability that a relevant document will contain term i and ri is the probability that an irrelevant document will contain it. This retrieval function is realised by the parameter triplet (log p.��−r.� r.��−p.�, di, none). Now there is still the question of how the estimates of pi and ri are derived. This task involves the use of relevance information which can be gained via feedback, described in section 6.1. 4.3 Language models The general language modeling retrieval function (cf. e.g. (Zhai and Lafferty, 2001)) is – admittedly – not in the linear form of equation 1. But using logarithms, products can be turned into sums without changing the ranking – the logarithm being a monotonic function (note that this is what also happened in the case of the probabilistic relevance models). In particular, we will use the approach of comparing query and document language models by Kullback-Leibler divergence (KLD) (Lafferty and Zhai, 2001) which results in the equation � P (t|Mq) KLD(Mq||Md) = P(t|Mq) log P(t|Md) tEq �OC − P(t|Mq) log P(t|Md) tEq where P(t|Mq) and P(t|Md) refer to the probability that term t wi</context>
<context position="18819" citStr="Zhai and Lafferty, 2001" startWordPosition="3208" endWordPosition="3212">his retrieval function where t stands for a procedure that adds −P(t|Mq) log P(t|Md) to the document node’s activation level for terms t not occurring in d and sorts documents by increasing activation values afterwards. 5 Combining IR models As can be seen from the last equation above, the language model retrieval function sums over all terms in the query. Each term – regardless of whether it � f(q, d) = i 13 appears in the document d or not – contributes something that may be interpreted as a “penalty” for the document. The magnitude of this penalty depends on the smoothing method used (cf. (Zhai and Lafferty, 2001)). A popular smoothing method uses so-called Dirichlet priors to estimate document language models: tf + µp(t|C) P(t|Md) = (4) µ + |d| where tf is t’s frequency in d, p(t|C) is the term’s relative frequency in the whole collection and µ is a free parameter. This indicates that if a rare term is missing from a document, the penalty will be large, P(t|Md) being very small because tf = 0 and p(t|C) small. Conceptually, it is unproblematic to model the retrieval function by making Itd a complete bipartite graph, i.e. specifying a (non-zero) value for P(t|Md), even if t does not occur in d. In a pr</context>
</contexts>
<marker>Zhai, Lafferty, 2001</marker>
<rawString>C. Zhai and J. Lafferty. 2001. A study of smoothing methods for language models applied to Ad Hoc information retrieval. In Proceedings of SIGIR 2001, pages 334–342.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>