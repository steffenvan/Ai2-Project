<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001731">
<title confidence="0.997391">
A New Model for Generating Multimodal Referring Expressions
</title>
<author confidence="0.924809">
Emiel Krahmer Ielka van der Sluis
</author>
<affiliation confidence="0.971381">
Communication and Cognition Computational Linguistics and AT
Tilburg University Tilburg University
</affiliation>
<email confidence="0.983883">
E.J.Krahmer@uvt.n1 I.F.vdrSluis@uvt.n1
</email>
<sectionHeader confidence="0.995251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999575">
We present a new algorithm for the gen-
eration of multimodal referring expres-
sions (combining language and deictic
gestures)) The approach differs from
earlier work in that we allow for various
gradations of preciseness in pointing,
ranging from unambiguous to vague
pointing gestures. The model predicts
that linguistic properties realized in the
generated expression are co-dependent
on the kind of pointing gesture included.
The decision to point is based on a trade-
off between the costs of pointing and the
costs of linguistic properties, where both
kinds of costs are computed in empir-
ically motivated ways. The model has
been implemented using a graph-based
generation algorithm.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9985275">
The generation of referring expressions is a cen-
tral task in Natural Language Generation (NLG),
and various useful algorithms which automatically
produce referring expressions have been devel-
oped (recent examples are van Deemter 2002, Gar-
dent 2002 and Krahmer et al. 2003). A typical al-
</bodyText>
<footnote confidence="0.745959714285714">
paper greatly benefitted from discussions with
Mariet Theune and Kees van Deemter. Thanks are also due
to Sebastiaan van Erk, Fons Maes, Paul Piwek and André
Verleg. Krahmer&apos;s work was done within the context of the
TUNA project, funded by Engineering and Physical Sciences
Research Council (EPSRC) in the UK, under grant reference
GR/S13330/01.
</footnote>
<bodyText confidence="0.999957096774194">
gorithm takes as input a single object v (the tar-
get object) and a set of objects (the distractors)
from which the target object needs to be distin-
guished (borrowing terminology from Dale and
Reiter 1995). The task of the algorithm is to de-
termine which set of properties is needed to single
out the target object from the distractors. This is
known as the content determination problem for
referring expressions. On the basis of this set of
properties a distinguishing description in natu-
ral language can be generated; a description which
applies to v but not to any of the distractors.
We describe a new algorithm which aims at pro-
ducing multimodal referring expressions: natural
language referring expressions which may include
deictic pointing gestures. There are at least two
motivations for such an extension. First, in vari-
ous situations a purely linguistic description may
simply be too complex, e.g., because the domain
contains many highly similar objects. In those
cases, including a deictic pointing gesture may be
the most efficient way to single out the intended
referent. Second, if we look at human commu-
nication it soon becomes apparent that referring
expressions which include pointing gestures are
rather common (Beun and Cremers 1998). Various
algorithms for the generation of multimodal re-
ferring expressions have been proposed (e.g., Co-
hen 1984, Claassen 1992, Huls et al. 1995, André
and Rist 1996, Lester et al. 1999, van der Sluis
and Krahmer 2001).2 Most of these are based
</bodyText>
<footnote confidence="0.996590333333333">
2These algorithms all operate on domains which are in the
direct visual field of both speaker and hearer. Throughout this
paper we will make this assumption as well.
</footnote>
<page confidence="0.999323">
47
</page>
<bodyText confidence="0.998633833333334">
on the assumption that a pointing gesture is pre-
cise and unambiguous. As soon as a pointing
gesture is included, it directly eliminates the dis-
tractors and singles out the intended referent. As
a consequence, the generated expressions tend to
be relatively simple and usually contain no more
than a head noun (this block) in combination with
a pointing gesture. Moreover, most algorithms
tend to be based on relatively simple, context-
independent criteria for the decision whether a
pointing gesture should be included or not. For
instance, Claassen 1992 only generates a pointing
gesture when referring to an object for which no
distinguishing linguistic description can be pro-
duced. Lester et al. 1999 generate pointing ges-
tures for all objects which cannot be referred to
with a pronoun. Van der Sluis and Krahmer (2001)
use pointing if the object is close or when a purely
linguistic description is too complex, where both
closeness and complexity are measured with re-
spect to a predefined threshold.
The approach described in this paper differs
from these earlier proposals in a number of ways.
We do not assume that pointing is always pre-
cise and unambiguous. Rather we allow for var-
ious gradations of preciseness in pointing, rang-
ing from unambiguous to vague pointing gestures.
Precise pointing has a high precision. Its scope
is restricted to the target object, and this directly
rules out the distractors. But, arguably, precise
pointing is &apos;expensive&apos;; the speaker has to make
sure she points precisely to the target object in
such a way that the hearer will be able to unam-
biguously interpret the referring expression. Im-
precise pointing, on the other hand, has a lower
precision —it generally includes some distractors
in its scope— but is intuitively less &apos;expensive&apos; .3
The model for pointing we propose may be
likened to a flashlight.4 If one holds a flashlight
just above a surface, it will cover only a small area
(the target object). Moving the flashlight away
Thi s intuition is in line with the alleged existence of neu-
rological differences between precise and imprecise pointing.
The former is argued to be monitored by a slow and con-
scious feedback control system, while the latter is governed
by a faster and non-conscious control system located in the
center and lower-back parts of the brain (see e.g., Smyth and
Wing 1984, Bizzi and Mussa-Ivaldi 1990).
</bodyText>
<footnote confidence="0.824144">
4This analogy was suggested by Mariet Theune (p.c.)
</footnote>
<bodyText confidence="0.999961166666667">
will enlarge the cone of light (shining on the tar-
get object but probably also on one or more dis-
tractors). A direct consequence of this &amp;quot;Flash-
light model for pointing&amp;quot; is that we predict that the
amount of linguistic properties required to gener-
ate a distinguishing multimodal referring expres-
sion is dependent on the kind of pointing gesture.
Imprecise pointing will require more additional
linguistic properties to single out the intended ref-
erent than precise pointing.
In our proposal, the decision to point is based
on a trade-off between the costs of pointing and
the costs of a linguistic description. The latter are
determined by summing over the costs of the indi-
vidual linguistic properties used in the description.
Arguably, the costs of precise pointing are deter-
mined by two factors: the size of the target object
(a big object is easier to point at than a small ob-
jects) and the distance between the target object
and the pointing device (objects which are near
are easier to point to than objects that are further
away). As we shall see, Fitts&apos; law —a fundamental
empirical law about the human motor-system due
to Fitts (1954)— can be used to model the costs of
precise pointing. In addition, we shall argue that
Fitts&apos; law allows us to capture the intuition that im-
precise pointing is cheaper than precise pointing.
The algorithm we describe in this paper is a
variant of the graph-based generation algorithm
described in Krahmer et al. (2003). It models
scenes as labelled directed graphs, in which ob-
jects are represented as vertices (or nodes) and the
properties and relations of these objects are rep-
resented as edges (or arcs). Cost functions are
used to assign weights to edges. The problem
of finding a referring expression for an object is
treated as finding the cheapest subgraph of the
scene graph which uniquely characterizes the in-
tended referent. For the generation of multimodal
referring expressions, the scene graph is enriched
with edges representing the various kinds of point-
ing gestures. Since the algorithm looks for the
cheapest subgraph, pointing edges will only be se-
lected when linguistic edges are relatively expen-
sive or when pointing is relatively cheap.
The rest of this paper is organized as follows. In
section 2 we describe the ingredients of the mul-
timodal graph-based approach to the generation
</bodyText>
<page confidence="0.997404">
48
</page>
<figure confidence="0.975749">
• • gh ot ogbrot rightrigh gh right of right
dl d2 d3 d4 dl d6 d7 dl eft o Iettt le left of left o eft of cit 0
</figure>
<figureCaption confidence="0.999834">
Figure 1: An example scene.
</figureCaption>
<bodyText confidence="0.990933975">
of referring expressions. Section 3 is devoted to
determining the costs of linguistic properties and
gestures. Section 4 describes the algorithm, and
illustrates it with a worked example. In section 5,
we summarize and discuss some of the properties
and predictions of the model.
2 Generating multimodal referring
expressions
2.1 Scene graphs Consider the visual scene de-
picted in Figure 1, consisting of a set of objects
with various properties and relations. In this par-
ticular scene M = {d1,....d8} is the set of enti-
ties, Prop = { small, large, black, white, block
} is the set of properties of these objects and Rel =
{ left-of, right-of } the set of relations. We repre-
sent a scene as a labelled directed graph. Let
L = Prop U Rel be the set of labels with Prop
and Rel disjoint, then G = (Vs, EG) is a labelled
directed graph, where VG C Al is the set of ver-
tices and EG C VG X L x VG is the set of la-
belled directed edges.5 Two other notions that we
use in this paper are graph union and graph exten-
sion. The union of graphs F = KVF, EF) and
G = (VG. EG) is the graph F U G = (VF U
VG, EF U Er). If G = KV, E) is a graph and
e = (v. 1,w) is an edge between vertices v and w
and with label 1 e L, then the extension of G with
e (notated G e) is the graph VU {v, w}, E U e).
Figure 2 contains a graph representation of the
scene depicted in Figure 1.6 Notice that proper-
ties are represented as loops, while relations are
modelled as edges between different vertices.
2.2 Referring graphs Suppose we want to gen-
erate a distinguishing description referring to d4.
Then we have to determine which properties
5Here and elsewhere subscripts are omitted when this can
be done without creating confusion.
(We only model the direct spatial relations under the as-
sumption that a distinguishing description would not use a
distant object as a relatum when a closer one can be selected.
</bodyText>
<figureCaption confidence="0.997128">
Figure 2: Example scene as a graph.
</figureCaption>
<bodyText confidence="0.999975461538462">
and/or relations are required to single out d4 from
its distractors This is done by creating referring
graphs, which at least include a vertex represent-
ing the target object. Informally, a vertex v (the
target object) in a referring graph H refers to a
given entity in the scene graph G iff the graph H
can be &amp;quot;placed&amp;quot; over the scene graph G in such
a way that v can be placed over the vertex of the
given entity in G and each edge from H with label
1 can be &amp;quot;placed over&amp;quot; a corresponding edge in G
with the same label. Furthermore, a vertex-graph
pair is distinguishing iff it refers to exactly one
vertex in the scene graph.7
Consider Figure 3, containing a number of po-
tential referring graphs for d4, each time with a
circle around the intended referent. The first one,
H1 has all the properties of d4 and hence can refer
to dzi. It is not distinguishing, however: it fails to
rule out d7 (the other large black block). Graph
H2 is distinguishing. Here, the circled vertex can
only be &amp;quot;placed over&amp;quot; the intended referent c/4 in
the scene graph. A straightforward linguistic re-
alization (expressing properties as adjectives and
relations as prepositional phrases) would be some-
thing like &amp;quot;the large black block to the left of a
small white block and to the right of another small
</bodyText>
<construct confidence="0.9557322">
7The informal notion of one graph being &amp;quot;placed over&amp;quot;
another corresponds with a well-known mathematical con-
struction on graphs, namely subgraph isomorphism. H =
(VH E can be &amp;quot;placed over&amp;quot; G = (VG, EG) iff there ex-
ists a subgraph G&apos; of G such that H is isomorphic to G&apos; H
is isomorphic to G&apos; iff there exists a bijection ii : VH VG&apos;,
such that for all vertices v, w E VI/ and all 1E L:
(vi, w) E E (7r 3 1,1, 7r E E GI
Given a graph H and a vertex v in H, and a graph G and a
vertex w in G, we define that the pair (v, H) refers to the pair
</construct>
<figure confidence="0.916402714285714">
(a) ,G) iff H is connected and H is mapped to a subgraph of
G by an isomorphism 7 and 7.v = w.
block
,„ii
the
white
49
dl
d2 d3 d4 d5 d5
d7 d8
HI
H2
VIP
• IP
</figure>
<figureCaption confidence="0.999955">
Figure 3: Three potential referring graphs for c/4.
</figureCaption>
<bodyText confidence="0.990107363636364">
white block&amp;quot;.8 Generally there is more than one
distinguishing graph referring to an object. In fact,
I/2 is not the smallest distinguishing graph refer-
ring to d4. This is 1/3. It might be realized as &amp;quot;the
large black block to the right of a white block&amp;quot;.
This is a distinguishing description but not a par-
ticular natural one; it is complex and arguably dif-
ficult for the hearer to interpret. In such cases, hav-
ing the possibility to simply point to the intended
referent would be very useful.
2.3 Gesture graphs Suppose we want to point
to c/4. Clearly this can be done from various
distances and under various angles. The various
hands in Figure 4 illustrate three levels of deic-
tic pointing gestures, all under the same angle but
each with different distances to the target object:
precise pointing (P), imprecise pointing ( I P) and
very imprecise pointing (VIP). We shall limit the
presentation here to these three levels of precision
and a fixed angle, although nothing hinges on this.
Naturally, the respective positions of the speaker
and the target object co-determine the angle un-
der which the pointing gesture occurs; this in turn
fixes the &apos;scope&apos; of the pointing gesture and thus
which objects are ruled out by it.9 If these respec-
8A somewhat more involved lexicalization module (us-
ing aggregation) might realize this graph as &amp;quot;The large black
block in between the two small white blocks&amp;quot;.
91-lere, for the sake of simplicity, we assume that an object
falls inside the scope of a pointing gesture if the &apos;cone&apos; shines
on part of it. A more fine-grained approach might distinguish
between objects in the center (where the light shines brightly)
and objects in the periphery (where the light is more blurred).
</bodyText>
<figureCaption confidence="0.984119">
Figure 4: Pointing into the scene
</figureCaption>
<bodyText confidence="0.998929">
tive positions are known, computing the scope of a
pointing gesture is straightforward, but the actual
mathematics falls outside the scope of this paper.
Just as properties and relations of objects can
be expressed in a graph, so can various pointing
gestures to these objects. All objects in the scope
of a potential pointing gesture (with a certain de-
gree of precision) are associated with an edge la-
belled with an indexed pointing gesture. Selecting
this edge implies that all objects which fall out-
side the scope of the gesture are ruled out. We
represent this information using a gesture graph.
Let PG„ = {P„, I P„, VI PO be the set of point-
ing gestures to a target object v. Then, given a
scene graph G = (VG, E s) , a gesture graph A, =
(VG, ED) is a labelled directed graph, where VG
is the set of vertices from the scene graph and
ED = Vs x PG„ x Vs the set of pointing edges.
Figure 5 displays a graph modelling the various
pointing gestures in Figure 4. Notice that there is
one gesture edge which is only associated with d4,
the one representing precise pointing to the target
object (modelled by edge P4). No other pointing
gesture eliminates all distractors.
2.4 Multimodal graphs Now the generation of
multimodal referring graphs is based on the union
of the scene graph G (which is relatively fixed)
with the deictic gesture graph D (which varies
with the target object). Figure 6 shows three dis-
tinguishing multimodal referring graphs for our
target object dzi. H1 is the smallest, only consist-
ing of an edge modelling a precise pointing ges-
</bodyText>
<page confidence="0.951727">
50
</page>
<figure confidence="0.911282333333333">
dl d2 d3 d4 d5 d6 d7 d8
• • Figure 6: Three distinguishing multimodal refer-
ring graphs for 4.
</figure>
<figureCaption confidence="0.999532">
Figure 5: Deictic gesture graph
</figureCaption>
<bodyText confidence="0.980506">
ture. It might be realized as &amp;quot;this one&amp;quot; combined
with a precise pointing gesture. H2 incorporates
an imprecise pointing gesture (of the kind shown
in Figure 4). Since this imprecise pointing ges-
ture does not eliminate the distractors d3 and d5,
a further edge is required, expressing that d4 is
black. This graph could be realized as &amp;quot;this black
one&amp;quot; combined with an imprecise pointing ges-
ture. Finally, H3 is a distinguishing graph which
incorporates a very imprecise pointing gesture. In-
cluding such an edge only rules out the distractors
d1, d7 and d8. At least two additional edges are
required for the construction of a distinguishing
graph, expressing that d4 is both large and black.
The resulting graph might be realized as &amp;quot;this large
black one&amp;quot; in combination with a very imprecise
pointing gesture. Arguably, in the scene of inter-
est these multimodal referring expressions seem
preferable to the linguistic expression from section
2 (the large black block to the right of a white one).
</bodyText>
<sectionHeader confidence="0.971405" genericHeader="method">
3 Cost functions
</sectionHeader>
<bodyText confidence="0.999657909090909">
We now have many ways to generate a distinguish-
ing referring expression for an object. Cost func-
tions are used to give preference to some solutions
over others. Costs are associated with subgraphs
H of the scene graph G. We require the cost func-
tion to be monotonic. This implies that extending
a graph H with an edge e can never result in a
graph which is cheaper than H.&apos;° We assume that
if H is a subgraph of G, the costs of H (notated
cost(H)) can be determined by summing over the
costs associated with the edges of H.
</bodyText>
<subsectionHeader confidence="0.929485">
3.1 The costs of properties The idea that cer-
</subsectionHeader>
<bodyText confidence="0.9943254">
tain linguistic properties are &apos;cheaper&apos; than others
luForrnally, VH C G Ve E EG : cost(H) cost(H e).
is already implicit in the notion of preferred at-
tributes in the incremental algorithm of Dale and
Reiter (1995), and is based on psycholinguistic ev-
idence. If someone wants to describe an object,
(s)he will first describe the &amp;quot;type&amp;quot; (what kind of
object it is; a block, an animal or whatever). If
that does not suffice, first absolute properties like
color may be used, followed by relative ones such
as size. In terms of costs, we assume that type
properties (block) are for free. Other properties
are more expensive. Absolute properties (colors
such as black and white) are cheaper than relative
ones (representing size, such as small or large).
There is little empirical work on the costs of rela-
tions, but it seems safe to assume that for our ex-
ample scene atomic relations are more expensive
than atomic properties. First, relations are compa-
rable to relative properties (they can not be verified
on the basis of the intended referent alone). In ad-
dition, using a relation implies that a second object
(the relatum) needs to be described as well and
describing two objects generally requires more ef-
fort than describing a single object.
</bodyText>
<subsectionHeader confidence="0.983707">
3.2 The costs of pointing Arguably, at least two
</subsectionHeader>
<bodyText confidence="0.996766333333333">
factors co-determine the costs of pointing: (i) the
size S of the target object (the bigger the object,
the easier, and hence cheaper, the reference), and
(ii) the distance D which the pointing device (in
our case the hand) has to travel in the direction of
the target object (a short distance is cheaper than a
long one).11 Interestingly, the pioneering work of
Fitts (1954) captures these two factors in the In-
dex of Difficulty, which states that the difficulty to
reach a target is a function of the size of and the
distance to a target: ID = log2( 2+), ). Thus with
each doubling of distance and with each halving
11A third factor which seems to be relevant is the salience
of the target. For a detailed discussion of this aspect we refer
to van der Sluis and Krahmer (2001). See also Section 5.
</bodyText>
<page confidence="0.996836">
51
</page>
<bodyText confidence="0.999988384615384">
of size the index of difficulty increases with 1 bit.
The addition of the factor 2 in the numerator is
unmotivated; Fitts added it to make sure that in his
experimental conditions the ID was always posi-
tive. He performed three experiments (a tapping,
a disk transfer and a pin transfer task) and in all
three found a high correlation between the time
subjects required to perform the task and the index
of difficulty. In recent years various alternatives
for the original /D have been proposed. MacKen-
zie&apos;s (1991) alternative removes the unmotivated
2 from the numerator and starts counting from 1
assuring that the ID is always positive.
</bodyText>
<equation confidence="0.909716">
ID = log2( —s 1)
</equation>
<bodyText confidence="0.999907933333333">
MacKenzie shows that this version of the ID fits
the experimental data slightly better. Below we
derive the costs of pointing from this index of dif-
ficulty. As argued, it seems a reasonable assump-
tion that imprecise pointing is cheaper than precise
pointing; it rules out fewer distractors, but also re-
quires less motoric precision and effort from the
speaker. The index of difficulty allows us to cap-
ture this intuition. We do not interpret the distance
D as the distance from the neutral, current position
of the hand to the target object, but rather as the
distance from the current position of the hand to
the target position of the hand. For the imprecise
variants of pointing this distance will be smaller
and hence the index of difficulty will be lower.
</bodyText>
<sectionHeader confidence="0.485604" genericHeader="method">
4 Sketch of the algorithm
</sectionHeader>
<bodyText confidence="0.9994504">
In this section we describe an algorithm which
outputs the cheapest distinguishing graph for a
target object, and illustrate it with an example.
Whether this cheapest graph will include pointing
edges, and if so, of what level of precision, is de-
termined by a trade-off between the costs of the
linguistic edges representing properties and rela-
tions of the target object and the costs of pointing.
The algorithm is a multimodal extension of the
algorithm described in Krahmer et al. (2003), to
which paper we refer for more details about com-
plexity, motivation and implementation.
Suppose we want to generate a description for
d4 from the scene graph G in Figure 2. Before we
illustrate the workings of this function we need to
</bodyText>
<equation confidence="0.8550163125">
makeReferringExpression(v, G)
construct D,;
111 := D, U G;
bestGraph := _L;
H :=
return findGraph(v, bestGraph, H, M); }
findGraph(v, bestGraph, H, M) {
if [bestGraph 1 and cost(bestGraph) &lt; cost(H)]
then return bestGraph;
distr := {n V fl E VA&amp;quot; A (v, H) refers to (n, M)};
if distr = 0 then return H;
for each adjacent edge e do
I := findGraph(v, bestGraph, H e, AI);
if [bestGraph = 1 or cost(I) &lt; cost(bestGraph)]
then bestGraph := I;
return bestGraph;}
</equation>
<figureCaption confidence="0.999228">
Figure 7: Sketch of the algorithm.
</figureCaption>
<bodyText confidence="0.999954678571429">
specify a cost function. Let us assume that c/4 is
a cube with sides of 1 inch, and that 31 inches is
the distance from the current neutral position of
the hand to the target position required for pre-
cise pointing, 15 inches for imprecise pointing and
7 inches for very imprecise pointing. Some easy
calculations will show that the index of difficulty
in the three cases is 5 bits, 4 bits and 3 bits re-
spectively. Thus, precise pointing (P) costs 5.00
points, imprecise pointing (IP) 4.00 and very im-
precise pointing (VIP) 3.00. The preferred order
for attributes in the current domain is (1) type, (2)
color, (3) size and (4) relations. In terms of costs,
let us assume for the sake of illustration that type
edges (block) are for free, color edges cost 0.75,
size edges cost 1.50 and relational edges 2.25.
We call the function makeReferringExpres-
sion (d4, G), outlined in figure 7. First of all the
deictic gesture graph Dd4, adding pointing edges
of various levels of precision to c14, is constructed
(see Figure 4), and merged with G. This gives us
a multi-modal graph M. The variable bestGraph,
for the cheapest solution found so far, is initialized
as the undefined graph! (no solution was found
yet), and the referring graph under construction H
is initialized as the graph only consisting of the
vertex c/4. We call the function findGraph with as
parameters the target object c/4, the best graph so
</bodyText>
<page confidence="0.996297">
52
</page>
<bodyText confidence="0.999983861111111">
far (I), the graph under construction H and the
multi-modal graph /V/. Now the algorithm sys-
tematically tries all relevant subgraphs H of AI .
It starts from the graph which only contains the
vertex d4 and the algorithm recursively tries to ex-
tend this graph by adding adjacent edges (that is
edges which start in d4 or possibly in any of the
other vertices added later on to the H under con-
struction). For each graph H it checks to which
objects in Al (different from d4) the vertex-graph
pair (d4, H) may refer; these are the distractors.
As soon as this set is empty we have found a distin-
guishing graph referring to 4. This graph is stored
in the variable bestGraph for the cheapest distin-
guishing graph found so far. In the end the al-
gorithm returns the cheapest distinguishing graph
which refers to the target object, if one exists, oth-
erwise it returns the undefined null graph I. In the
current set up the latter possibility will never arise
due to the presence of unambiguous pointing ges-
tures (expensive though they may be). Which re-
ferring graph is the first to be found depends on the
order in which the edges are tried (clearly this is a
place where heuristics are helpful, e.g., it will gen-
erally be beneficial to try cheap edges before ex-
pensive ones). Let us say, for the sake of argument,
that the first distinguishing graph which the algo-
rithm finds is H3 from Figure 3. This graph costs
5.25. At this point, graphs which are as expensive
as this graph can be discarded (since due to the
monotonicity constraint they will never end up be-
ing cheaper than the best solution found so far). In
the current situation, the cheapest solution is H2
from Figure 6, which costs a mere 4.75.12 The re-
sulting graph could be realized as &amp;quot;this black one&amp;quot;
combined with an imprecise pointing gesture.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999889827586207">
We have described a new model for the genera-
tion of multimodal referring expressions. The ap-
proach is based on only a few, independently mo-
&apos;Note that if pointing would have been cheaper (because
the distance between the current position of the hand and the
required position for precise pointing was, say, 3 inches), the
algorithm would output &amp;quot;this one&amp;quot; plus a precise pointing
edge (i.e., H1 from Figure 6, for 2.00). If pointing would
be more expensive (because even for very imprecise point-
ing the distance would be substantial), the algorithm would
output H3 from Figure 3, for 5.25.
tivated assumptions. The starting point is a graph-
based algorithm which tries to find the cheapest
referring expression for a particular target object
(Krahmer et al. 2003). We assume that linguis-
tic properties have certain costs (c.f., the preferred
attributes from Dale &amp; Reiter 1995). And, finally,
we propose a &amp;quot;flashlight&amp;quot; model of pointing allow-
ing for different gradations of pointing precision,
ranging from precise and unambiguous to impre-
cise and ambiguous. The costs of these various
pointing gestures are derived from an empirically
motivated adaptation of Fitts&apos; (1954) law.
The model has a number of nice consequences.
We have described two in detail: (1) we do not
need an a priori criterion to decide when to in-
clude a pointing gesture in a distinguishing de-
scription. Rather the decision to point is based
on a trade-off between the costs of pointing and
the costs of a linguistic description. And (2)
we predict that the amount of linguistic proper-
ties required to generate a distinguishing multi-
modal referring expression is dependent on the
kind of pointing gesture. One further neat conse-
quence of the model is that an isolated object does
not require precise pointing; there will always
be a graph containing a less precise (and hence
cheaper) pointing edge which has the same ob-
jects in its scope as the more precise pointing act.
Notice also that the algorithm will never output
a graph with multiple pointing edges, since there
would always be a cheaper graph which omits the
less precise one. In most situations, it will also
not happen that a distinguishing graph will include
both an imprecise pointing gesture and a relational
edge. Under most cost functions it will be more
&apos;cost effective&apos; to include a precise pointing edge
than an imprecise pointing edge plus a relational
edge plus the edges associated with the relatum.
The algorithm we have described has been im-
plemented in Java 2 (J2SE, version 1.4). The com-
putation described in section 4 requires 110 ms.
on a PC with a 900 mHz AMD Athlon Processor
and 128 Mb RAM. Due to the presence of precise
pointing edges it will always be possible to single
out one object from the others. As a side effect
of this we obtain a polynomial upperbound for the
theoretical complexity. 13 It has been argued that
</bodyText>
<footnote confidence="0.755207">
13We know the costs of at least one distinguishing graph
</footnote>
<page confidence="0.998083">
53
</page>
<bodyText confidence="0.999986867924529">
some notion of focus of attention could be used to
tackle the computational complexity. We may as-
sume that objects which are currently in the focus
of attention are more salient than objects which are
not in focus. Now the distractor set for a target ob-
ject need not include all objects in the domain, but
only those that are at least as salient as the target
object. A distinguishing description only needs to
rule out those objects. There are two interesting
connections between focus of attention and multi-
modality. First, pointing gestures typically serve
to demarcate the focus of attention. Second, the
model described in this paper predicts that a distin-
guishing description for an object which is salient
is less likely to contain a pointing gesture. If an
object is salient, this generally implies that its dis-
tractor set is relatively small (typically, only a few
objects are somehow salient). This in turn implies
that fewer (or less expensive) edges are required to
rule out the distractors, hence there is less need for
deictic pointing gestures.
It is interesting to observe that, even though we
borrow the idea of preferred attributes from the
Incremental Algorithm (arguably the most influ-
ential algorithm for the generation of referring ex-
pressions), an incremental approach to multimodal
descriptions does not seem to be straightforward.
One might consider extending the list of preferred
attributes with VIP, IP and P (in that preference
order, modelling the increase in costs). On this
approach, we would first select a number of lin-
guistic edges (independent of the kind of pointing
gesture) followed by one or more pointing edges.
But that would not work, since the lack of back-
tracking (which is inherent to incrementality) en-
tails that all selected properties will be realized.
This seems to suggest that the model outlined in
this paper is inherently non-incremental.
We are currently running an experimental eval-
uation of the model, particularly addressing the
for our target object; the graph consisting of only a vertex
for the target object and a precise pointing edge. This means
that we do not have to inspect all subgraphs of the merged
multimodal graph Al, but only those subgraphs which do not
cost more than the precise pointing graph. Thus, we only
need to inspect graphs with less than K edges (for some K
depending on the costs of precise pointing), which requires
in the worst case 0(n&apos;), with n the number of edges in the
graph Al. It should be added that this worst case complexity
is computationally rather unattractive for larger values of K.
vague pointing gestures and their interaction with
linguistic realization. We hope to present the re-
sults of this evaluation in a sequel to this paper.
</bodyText>
<sectionHeader confidence="0.991902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991977065217391">
André, E. and T. Rist (1996), Coping with Temporal Con-
straints in Multimedia Presentation Planning, Proceedings
of the 13th AAAI, 142-147.
Beun, R.J. &amp; A. Cremers (1998), Object reference in a
shared domain of conversation, Pragmatics &amp; Cognition
6(1/2):121-152.
Bizzi, E. and F. Mussa-Ivaldi (1990), Muscle properties and
the control of arm movement, In: Visual Cognition and
Action (vol 2), D. Osherson, et al. (eds.), MIT Press.
Cohen, P. (1984), The pragmatics of referring and the
modality of communication, Computational Linguistics
10(2):97-125.
Claassen, W. (1992), Generating referring expressions in a
multimodal environment, in: Aspects of Automated Natu-
ral Language Generation, R. Dale et al. (eds.), Springer
Verlag, Berlin.
Dale, R. and E. Reiter (1995), Computational interpretations
of the Gricean maxims in the generation of referring ex-
pressions, Cognitive Science 18:233-263.
van Deemter, K. (2002), Generating referring expressions:
Beyond the Incremental Algorithm, Computational Lin-
guistics 28(1):37-52.
Fitts, P. (1954), The information capacity of the human motor
system in controlling amplitude of movement, Journal of
Experimental Psychology 47:381-391.
Gardent, C. (2002), Generating minimal definite descriptions,
Proceedings of the 40th ACL, Philadelphia, USA.
Huls, C. E. Bos &amp; W. Claassen (1995), Automatic referent
resolution of deictic and anaphoric expressions, Computa-
tional Linguistics 21(1):59-79.
Krahmer, E. S. van Erk &amp; A. Verleg (2003), Graph-based
Generation of Referring Expressions, Computational Lin-
guistics, 29(1): 53-72.
Lester, J., J. Voerman, S. Towns and C. Callaway (1999),
Deictic Believability: Coordinating gesture, locomotion,
and speech in lifelike pedagogical agents, Applied Artifi-
cial Intelligence 13(4-5):383-414.
Smyth, M. and Wing, A. (1984), The Psychology of Human
Movement, New York: Academic Press.
MacKenzie, I.S. (1991), Fitts&apos; law as a performance model in
human-computer interaction, doctoral dissertation, Uni-
versity of Toronto, Canada.
van der Sluis, I. and E. Krahmer (2001), Generating Refer-
ring Expressions in a Multimodal Context: An empirically
motivated approach. Selected Papers from the 11th CLIN
Meeting, W. Daelemans et al. (eds.), Rodopi, Amsterdam.
</reference>
<page confidence="0.998861">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651214">
<title confidence="0.999939">A New Model for Generating Multimodal Referring Expressions</title>
<author confidence="0.94685">Emiel Krahmer Ielka van_der_Sluis</author>
<affiliation confidence="0.946247">Communication and Cognition Computational Linguistics and AT Tilburg University Tilburg University</affiliation>
<address confidence="0.797788">E.J.Krahmer@uvt.n1 I.F.vdrSluis@uvt.n1</address>
<abstract confidence="0.995738052631579">We present a new algorithm for the generation of multimodal referring expressions (combining language and deictic gestures)) The approach differs from earlier work in that we allow for various gradations of preciseness in pointing, ranging from unambiguous to vague pointing gestures. The model predicts that linguistic properties realized in the generated expression are co-dependent on the kind of pointing gesture included. The decision to point is based on a tradeoff between the costs of pointing and the costs of linguistic properties, where both kinds of costs are computed in empirically motivated ways. The model has been implemented using a graph-based generation algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E André</author>
<author>T Rist</author>
</authors>
<title>Coping with Temporal Constraints in Multimedia Presentation Planning,</title>
<date>1996</date>
<booktitle>Proceedings of the 13th AAAI,</booktitle>
<pages>142--147</pages>
<contexts>
<context position="2993" citStr="André and Rist 1996" startWordPosition="462" endWordPosition="465"> an extension. First, in various situations a purely linguistic description may simply be too complex, e.g., because the domain contains many highly similar objects. In those cases, including a deictic pointing gesture may be the most efficient way to single out the intended referent. Second, if we look at human communication it soon becomes apparent that referring expressions which include pointing gestures are rather common (Beun and Cremers 1998). Various algorithms for the generation of multimodal referring expressions have been proposed (e.g., Cohen 1984, Claassen 1992, Huls et al. 1995, André and Rist 1996, Lester et al. 1999, van der Sluis and Krahmer 2001).2 Most of these are based 2These algorithms all operate on domains which are in the direct visual field of both speaker and hearer. Throughout this paper we will make this assumption as well. 47 on the assumption that a pointing gesture is precise and unambiguous. As soon as a pointing gesture is included, it directly eliminates the distractors and singles out the intended referent. As a consequence, the generated expressions tend to be relatively simple and usually contain no more than a head noun (this block) in combination with a pointin</context>
</contexts>
<marker>André, Rist, 1996</marker>
<rawString>André, E. and T. Rist (1996), Coping with Temporal Constraints in Multimedia Presentation Planning, Proceedings of the 13th AAAI, 142-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Beun</author>
<author>A Cremers</author>
</authors>
<title>Object reference in a shared domain of conversation,</title>
<date>1998</date>
<journal>Pragmatics &amp; Cognition</journal>
<pages>6--1</pages>
<contexts>
<context position="2827" citStr="Beun and Cremers 1998" startWordPosition="435" endWordPosition="438">oducing multimodal referring expressions: natural language referring expressions which may include deictic pointing gestures. There are at least two motivations for such an extension. First, in various situations a purely linguistic description may simply be too complex, e.g., because the domain contains many highly similar objects. In those cases, including a deictic pointing gesture may be the most efficient way to single out the intended referent. Second, if we look at human communication it soon becomes apparent that referring expressions which include pointing gestures are rather common (Beun and Cremers 1998). Various algorithms for the generation of multimodal referring expressions have been proposed (e.g., Cohen 1984, Claassen 1992, Huls et al. 1995, André and Rist 1996, Lester et al. 1999, van der Sluis and Krahmer 2001).2 Most of these are based 2These algorithms all operate on domains which are in the direct visual field of both speaker and hearer. Throughout this paper we will make this assumption as well. 47 on the assumption that a pointing gesture is precise and unambiguous. As soon as a pointing gesture is included, it directly eliminates the distractors and singles out the intended refe</context>
</contexts>
<marker>Beun, Cremers, 1998</marker>
<rawString>Beun, R.J. &amp; A. Cremers (1998), Object reference in a shared domain of conversation, Pragmatics &amp; Cognition 6(1/2):121-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bizzi</author>
<author>F Mussa-Ivaldi</author>
</authors>
<title>Muscle properties and the control of arm movement,</title>
<date>1990</date>
<journal>In: Visual Cognition and Action</journal>
<volume>(vol</volume>
<editor>2), D. Osherson, et al. (eds.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5619" citStr="Bizzi and Mussa-Ivaldi 1990" startWordPosition="901" endWordPosition="904">is intuitively less &apos;expensive&apos; .3 The model for pointing we propose may be likened to a flashlight.4 If one holds a flashlight just above a surface, it will cover only a small area (the target object). Moving the flashlight away Thi s intuition is in line with the alleged existence of neurological differences between precise and imprecise pointing. The former is argued to be monitored by a slow and conscious feedback control system, while the latter is governed by a faster and non-conscious control system located in the center and lower-back parts of the brain (see e.g., Smyth and Wing 1984, Bizzi and Mussa-Ivaldi 1990). 4This analogy was suggested by Mariet Theune (p.c.) will enlarge the cone of light (shining on the target object but probably also on one or more distractors). A direct consequence of this &amp;quot;Flashlight model for pointing&amp;quot; is that we predict that the amount of linguistic properties required to generate a distinguishing multimodal referring expression is dependent on the kind of pointing gesture. Imprecise pointing will require more additional linguistic properties to single out the intended referent than precise pointing. In our proposal, the decision to point is based on a trade-off between t</context>
</contexts>
<marker>Bizzi, Mussa-Ivaldi, 1990</marker>
<rawString>Bizzi, E. and F. Mussa-Ivaldi (1990), Muscle properties and the control of arm movement, In: Visual Cognition and Action (vol 2), D. Osherson, et al. (eds.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cohen</author>
</authors>
<title>The pragmatics of referring and the modality of communication,</title>
<date>1984</date>
<journal>Computational Linguistics</journal>
<pages>10--2</pages>
<contexts>
<context position="2939" citStr="Cohen 1984" startWordPosition="453" endWordPosition="455">. There are at least two motivations for such an extension. First, in various situations a purely linguistic description may simply be too complex, e.g., because the domain contains many highly similar objects. In those cases, including a deictic pointing gesture may be the most efficient way to single out the intended referent. Second, if we look at human communication it soon becomes apparent that referring expressions which include pointing gestures are rather common (Beun and Cremers 1998). Various algorithms for the generation of multimodal referring expressions have been proposed (e.g., Cohen 1984, Claassen 1992, Huls et al. 1995, André and Rist 1996, Lester et al. 1999, van der Sluis and Krahmer 2001).2 Most of these are based 2These algorithms all operate on domains which are in the direct visual field of both speaker and hearer. Throughout this paper we will make this assumption as well. 47 on the assumption that a pointing gesture is precise and unambiguous. As soon as a pointing gesture is included, it directly eliminates the distractors and singles out the intended referent. As a consequence, the generated expressions tend to be relatively simple and usually contain no more than </context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Cohen, P. (1984), The pragmatics of referring and the modality of communication, Computational Linguistics 10(2):97-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Claassen</author>
</authors>
<title>Generating referring expressions in a multimodal environment, in:</title>
<date>1992</date>
<journal>Aspects of Automated Natural Language Generation,</journal>
<editor>R. Dale et al. (eds.),</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="2954" citStr="Claassen 1992" startWordPosition="456" endWordPosition="457">at least two motivations for such an extension. First, in various situations a purely linguistic description may simply be too complex, e.g., because the domain contains many highly similar objects. In those cases, including a deictic pointing gesture may be the most efficient way to single out the intended referent. Second, if we look at human communication it soon becomes apparent that referring expressions which include pointing gestures are rather common (Beun and Cremers 1998). Various algorithms for the generation of multimodal referring expressions have been proposed (e.g., Cohen 1984, Claassen 1992, Huls et al. 1995, André and Rist 1996, Lester et al. 1999, van der Sluis and Krahmer 2001).2 Most of these are based 2These algorithms all operate on domains which are in the direct visual field of both speaker and hearer. Throughout this paper we will make this assumption as well. 47 on the assumption that a pointing gesture is precise and unambiguous. As soon as a pointing gesture is included, it directly eliminates the distractors and singles out the intended referent. As a consequence, the generated expressions tend to be relatively simple and usually contain no more than a head noun (th</context>
</contexts>
<marker>Claassen, 1992</marker>
<rawString>Claassen, W. (1992), Generating referring expressions in a multimodal environment, in: Aspects of Automated Natural Language Generation, R. Dale et al. (eds.), Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions,</title>
<date>1995</date>
<journal>Cognitive Science</journal>
<pages>18--233</pages>
<contexts>
<context position="1777" citStr="Dale and Reiter 1995" startWordPosition="267" endWordPosition="270">e van Deemter 2002, Gardent 2002 and Krahmer et al. 2003). A typical alpaper greatly benefitted from discussions with Mariet Theune and Kees van Deemter. Thanks are also due to Sebastiaan van Erk, Fons Maes, Paul Piwek and André Verleg. Krahmer&apos;s work was done within the context of the TUNA project, funded by Engineering and Physical Sciences Research Council (EPSRC) in the UK, under grant reference GR/S13330/01. gorithm takes as input a single object v (the target object) and a set of objects (the distractors) from which the target object needs to be distinguished (borrowing terminology from Dale and Reiter 1995). The task of the algorithm is to determine which set of properties is needed to single out the target object from the distractors. This is known as the content determination problem for referring expressions. On the basis of this set of properties a distinguishing description in natural language can be generated; a description which applies to v but not to any of the distractors. We describe a new algorithm which aims at producing multimodal referring expressions: natural language referring expressions which may include deictic pointing gestures. There are at least two motivations for such an</context>
<context position="17355" citStr="Dale and Reiter (1995)" startWordPosition="3026" endWordPosition="3029">Costs are associated with subgraphs H of the scene graph G. We require the cost function to be monotonic. This implies that extending a graph H with an edge e can never result in a graph which is cheaper than H.&apos;° We assume that if H is a subgraph of G, the costs of H (notated cost(H)) can be determined by summing over the costs associated with the edges of H. 3.1 The costs of properties The idea that certain linguistic properties are &apos;cheaper&apos; than others luForrnally, VH C G Ve E EG : cost(H) cost(H e). is already implicit in the notion of preferred attributes in the incremental algorithm of Dale and Reiter (1995), and is based on psycholinguistic evidence. If someone wants to describe an object, (s)he will first describe the &amp;quot;type&amp;quot; (what kind of object it is; a block, an animal or whatever). If that does not suffice, first absolute properties like color may be used, followed by relative ones such as size. In terms of costs, we assume that type properties (block) are for free. Other properties are more expensive. Absolute properties (colors such as black and white) are cheaper than relative ones (representing size, such as small or large). There is little empirical work on the costs of relations, but i</context>
<context position="26004" citStr="Dale &amp; Reiter 1995" startWordPosition="4541" endWordPosition="4544"> required position for precise pointing was, say, 3 inches), the algorithm would output &amp;quot;this one&amp;quot; plus a precise pointing edge (i.e., H1 from Figure 6, for 2.00). If pointing would be more expensive (because even for very imprecise pointing the distance would be substantial), the algorithm would output H3 from Figure 3, for 5.25. tivated assumptions. The starting point is a graphbased algorithm which tries to find the cheapest referring expression for a particular target object (Krahmer et al. 2003). We assume that linguistic properties have certain costs (c.f., the preferred attributes from Dale &amp; Reiter 1995). And, finally, we propose a &amp;quot;flashlight&amp;quot; model of pointing allowing for different gradations of pointing precision, ranging from precise and unambiguous to imprecise and ambiguous. The costs of these various pointing gestures are derived from an empirically motivated adaptation of Fitts&apos; (1954) law. The model has a number of nice consequences. We have described two in detail: (1) we do not need an a priori criterion to decide when to include a pointing gesture in a distinguishing description. Rather the decision to point is based on a trade-off between the costs of pointing and the costs of a</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Dale, R. and E. Reiter (1995), Computational interpretations of the Gricean maxims in the generation of referring expressions, Cognitive Science 18:233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating referring expressions: Beyond the Incremental Algorithm,</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<pages>28--1</pages>
<marker>van Deemter, 2002</marker>
<rawString>van Deemter, K. (2002), Generating referring expressions: Beyond the Incremental Algorithm, Computational Linguistics 28(1):37-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fitts</author>
</authors>
<title>The information capacity of the human motor system in controlling amplitude of movement,</title>
<date>1954</date>
<journal>Journal of Experimental Psychology</journal>
<pages>47--381</pages>
<contexts>
<context position="6815" citStr="Fitts (1954)" startWordPosition="1106" endWordPosition="1107">de-off between the costs of pointing and the costs of a linguistic description. The latter are determined by summing over the costs of the individual linguistic properties used in the description. Arguably, the costs of precise pointing are determined by two factors: the size of the target object (a big object is easier to point at than a small objects) and the distance between the target object and the pointing device (objects which are near are easier to point to than objects that are further away). As we shall see, Fitts&apos; law —a fundamental empirical law about the human motor-system due to Fitts (1954)— can be used to model the costs of precise pointing. In addition, we shall argue that Fitts&apos; law allows us to capture the intuition that imprecise pointing is cheaper than precise pointing. The algorithm we describe in this paper is a variant of the graph-based generation algorithm described in Krahmer et al. (2003). It models scenes as labelled directed graphs, in which objects are represented as vertices (or nodes) and the properties and relations of these objects are represented as edges (or arcs). Cost functions are used to assign weights to edges. The problem of finding a referring expre</context>
<context position="18810" citStr="Fitts (1954)" startWordPosition="3277" endWordPosition="3278">ition, using a relation implies that a second object (the relatum) needs to be described as well and describing two objects generally requires more effort than describing a single object. 3.2 The costs of pointing Arguably, at least two factors co-determine the costs of pointing: (i) the size S of the target object (the bigger the object, the easier, and hence cheaper, the reference), and (ii) the distance D which the pointing device (in our case the hand) has to travel in the direction of the target object (a short distance is cheaper than a long one).11 Interestingly, the pioneering work of Fitts (1954) captures these two factors in the Index of Difficulty, which states that the difficulty to reach a target is a function of the size of and the distance to a target: ID = log2( 2+), ). Thus with each doubling of distance and with each halving 11A third factor which seems to be relevant is the salience of the target. For a detailed discussion of this aspect we refer to van der Sluis and Krahmer (2001). See also Section 5. 51 of size the index of difficulty increases with 1 bit. The addition of the factor 2 in the numerator is unmotivated; Fitts added it to make sure that in his experimental con</context>
</contexts>
<marker>Fitts, 1954</marker>
<rawString>Fitts, P. (1954), The information capacity of the human motor system in controlling amplitude of movement, Journal of Experimental Psychology 47:381-391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
</authors>
<title>Generating minimal definite descriptions,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th ACL,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1188" citStr="Gardent 2002" startWordPosition="170" endWordPosition="172">ized in the generated expression are co-dependent on the kind of pointing gesture included. The decision to point is based on a tradeoff between the costs of pointing and the costs of linguistic properties, where both kinds of costs are computed in empirically motivated ways. The model has been implemented using a graph-based generation algorithm. 1 Introduction The generation of referring expressions is a central task in Natural Language Generation (NLG), and various useful algorithms which automatically produce referring expressions have been developed (recent examples are van Deemter 2002, Gardent 2002 and Krahmer et al. 2003). A typical alpaper greatly benefitted from discussions with Mariet Theune and Kees van Deemter. Thanks are also due to Sebastiaan van Erk, Fons Maes, Paul Piwek and André Verleg. Krahmer&apos;s work was done within the context of the TUNA project, funded by Engineering and Physical Sciences Research Council (EPSRC) in the UK, under grant reference GR/S13330/01. gorithm takes as input a single object v (the target object) and a set of objects (the distractors) from which the target object needs to be distinguished (borrowing terminology from Dale and Reiter 1995). The task </context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>Gardent, C. (2002), Generating minimal definite descriptions, Proceedings of the 40th ACL, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Bos Huls</author>
<author>W Claassen</author>
</authors>
<title>Automatic referent resolution of deictic and anaphoric expressions,</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<pages>21--1</pages>
<marker>Huls, Claassen, 1995</marker>
<rawString>Huls, C. E. Bos &amp; W. Claassen (1995), Automatic referent resolution of deictic and anaphoric expressions, Computational Linguistics 21(1):59-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S van Erk Krahmer</author>
<author>A Verleg</author>
</authors>
<date>2003</date>
<journal>Graph-based Generation of Referring Expressions, Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>53--72</pages>
<marker>Krahmer, Verleg, 2003</marker>
<rawString>Krahmer, E. S. van Erk &amp; A. Verleg (2003), Graph-based Generation of Referring Expressions, Computational Linguistics, 29(1): 53-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lester</author>
<author>J Voerman</author>
<author>S Towns</author>
<author>C Callaway</author>
</authors>
<title>Deictic Believability: Coordinating gesture, locomotion, and speech in lifelike pedagogical agents,</title>
<date>1999</date>
<journal>Applied Artificial Intelligence</journal>
<pages>13--4</pages>
<contexts>
<context position="3013" citStr="Lester et al. 1999" startWordPosition="466" endWordPosition="469"> in various situations a purely linguistic description may simply be too complex, e.g., because the domain contains many highly similar objects. In those cases, including a deictic pointing gesture may be the most efficient way to single out the intended referent. Second, if we look at human communication it soon becomes apparent that referring expressions which include pointing gestures are rather common (Beun and Cremers 1998). Various algorithms for the generation of multimodal referring expressions have been proposed (e.g., Cohen 1984, Claassen 1992, Huls et al. 1995, André and Rist 1996, Lester et al. 1999, van der Sluis and Krahmer 2001).2 Most of these are based 2These algorithms all operate on domains which are in the direct visual field of both speaker and hearer. Throughout this paper we will make this assumption as well. 47 on the assumption that a pointing gesture is precise and unambiguous. As soon as a pointing gesture is included, it directly eliminates the distractors and singles out the intended referent. As a consequence, the generated expressions tend to be relatively simple and usually contain no more than a head noun (this block) in combination with a pointing gesture. Moreover,</context>
</contexts>
<marker>Lester, Voerman, Towns, Callaway, 1999</marker>
<rawString>Lester, J., J. Voerman, S. Towns and C. Callaway (1999), Deictic Believability: Coordinating gesture, locomotion, and speech in lifelike pedagogical agents, Applied Artificial Intelligence 13(4-5):383-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Smyth</author>
<author>A Wing</author>
</authors>
<title>The Psychology of Human Movement,</title>
<date>1984</date>
<publisher>Academic Press.</publisher>
<location>New York:</location>
<contexts>
<context position="5589" citStr="Smyth and Wing 1984" startWordPosition="897" endWordPosition="900">rs in its scope— but is intuitively less &apos;expensive&apos; .3 The model for pointing we propose may be likened to a flashlight.4 If one holds a flashlight just above a surface, it will cover only a small area (the target object). Moving the flashlight away Thi s intuition is in line with the alleged existence of neurological differences between precise and imprecise pointing. The former is argued to be monitored by a slow and conscious feedback control system, while the latter is governed by a faster and non-conscious control system located in the center and lower-back parts of the brain (see e.g., Smyth and Wing 1984, Bizzi and Mussa-Ivaldi 1990). 4This analogy was suggested by Mariet Theune (p.c.) will enlarge the cone of light (shining on the target object but probably also on one or more distractors). A direct consequence of this &amp;quot;Flashlight model for pointing&amp;quot; is that we predict that the amount of linguistic properties required to generate a distinguishing multimodal referring expression is dependent on the kind of pointing gesture. Imprecise pointing will require more additional linguistic properties to single out the intended referent than precise pointing. In our proposal, the decision to point is </context>
</contexts>
<marker>Smyth, Wing, 1984</marker>
<rawString>Smyth, M. and Wing, A. (1984), The Psychology of Human Movement, New York: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S MacKenzie</author>
</authors>
<title>Fitts&apos; law as a performance model in human-computer interaction, doctoral dissertation,</title>
<date>1991</date>
<institution>University of Toronto, Canada.</institution>
<marker>MacKenzie, 1991</marker>
<rawString>MacKenzie, I.S. (1991), Fitts&apos; law as a performance model in human-computer interaction, doctoral dissertation, University of Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I van der Sluis</author>
<author>E Krahmer</author>
</authors>
<title>Generating Referring Expressions in a Multimodal Context: An empirically motivated approach.</title>
<date>2001</date>
<booktitle>Selected Papers from the 11th CLIN Meeting, W. Daelemans</booktitle>
<editor>et al. (eds.),</editor>
<location>Rodopi, Amsterdam.</location>
<marker>van der Sluis, Krahmer, 2001</marker>
<rawString>van der Sluis, I. and E. Krahmer (2001), Generating Referring Expressions in a Multimodal Context: An empirically motivated approach. Selected Papers from the 11th CLIN Meeting, W. Daelemans et al. (eds.), Rodopi, Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>