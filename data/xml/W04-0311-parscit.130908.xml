<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002003">
<title confidence="0.995448">
Dynamic Dependency Parsing
</title>
<author confidence="0.996691">
Michael Daum
</author>
<affiliation confidence="0.997931333333333">
Natural Language Systems Group
Department of Computer Science
University of Hamburg
</affiliation>
<email confidence="0.989251">
micha@nats.informatik.uni-hamburg.de
</email>
<sectionHeader confidence="0.993732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999637666666667">
The inherent robustness of a system might be an
important prerequisite for an incremental pars-
ing model to the effect that grammaticality re-
quirements on full sentences may be suspended
or allowed to be violated transiently. How-
ever, we present additional means that allow the
grammarian to model prefix-analyses by alter-
ing a grammar for non-incremental parsing in a
controlled way. This is done by introducing un-
derspecified dependency edges that model the
expected relation between already seen and yet
unseen words during parsing. Thus the basic
framework of weighted constraint dependency
parsing is extended by the notion of dynamic
dependency parsing.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993395">
In an incremental mode of operation, a parser
works on a prefix of a prolonging utterance, try-
ing to compute prefix-analyses while having to
cope with a growing computational effort. This
situation gives rise at least to the following ques-
tions:
</bodyText>
<listItem confidence="0.835237714285714">
(1) Which provisions can be made to accept
prefix-analyses transiently given a model
of language that describes complete sen-
tences?
(2) How shall prefix-analyses look like?
(3) How can the complexity of incremental
parsing be bounded?
</listItem>
<bodyText confidence="0.9756829375">
We will introduce underspecified dependency
edges, called nonspec dependency edges, to the
framework of weighted constraint dependency
grammar (WCDG) (Schr¨oder, 2002). These are
used to encode an expected function of a word
already seen but not yet integrated into the rest
of the parse tree during incremental parsing.
In WCDG, parse trees are annotated by
constraint violations that pinpoint deviations
from grammatical requirements or preferences.
Hence weighted constraints are a means to de-
scribe a graded grammaticality discretion by de-
scribing the inherent ‘costs’ of accepting an im-
perfect parse tree. Thus parsing follows princi-
ples of economy when repairing constraint vio-
lations as long as reducing costs any further is
justified by its effort.
The following sections revise the basic ideas
of applying constraint optimization to natural
language parsing and extend it to dynamic de-
pendency parsing.
2 From static to dynamic constraint
satisfaction
We begin by describing the standard constraint
satisfaction problem (CSP), then extend it in
two different directions commonly found in the
literature: (a) to constraint optimization prob-
lems (COP) and (b) to dynamic constraint sat-
isfaction problems (DynCSP) aggregating both
to dynamic constraint optimization problems
(DynCOP) which is motivated by our current
application to incremental parsingi.
</bodyText>
<subsectionHeader confidence="0.971799">
2.1 Constraint Satisfaction
</subsectionHeader>
<bodyText confidence="0.995823">
Constraint satisfaction is defined as being the
problem of finding consistent values for a fixed
set of variables given all constraints between
those values. Formally, a constraint satisfac-
tion problem (CSP) can be viewed as a triple
(X, D, C) where X = {x1,... , xn} is a fi-
nite set of variables with respective domains
</bodyText>
<equation confidence="0.9158065">
D = {Dl, ... , Dn}, and a set of constraints
C = {Cl, ... , Ct}. A constraint Ci is defined as
</equation>
<bodyText confidence="0.9842174">
a relation defined on a subset of variables, called
&apos;Note that we don’t use the common abbreviations
for dynamic constraint satisfaction problems DCSP in fa-
vor of DynCSP in order to distinguish if from distributed
constraint satisfaction problems which are called DCSPs
also. Likewise we use DynCOP instead of DCOP, the
latter of which is commonly known as distributed con-
straint optimization problems.
the scope, restricting their simultaneous assign-
ment. Constraints defined on one variable are
called unary; constraints on two variables are
binary. We call unary and binary constraints lo-
cal constraints as their scope is very restricted.
Constraints of wider scope are classified non-
local. Especially those involving a full scope
over all variables are called context constraints.
The ‘local knowledge’ of a CSP is encoded in
a constraint network (CN) consisting of nodes
bundling all values of a variable consistent with
all unary constraints. The edges of a CN de-
pict binary constraints between the connected
variables. So a CN is a compact representation
(of a superset) of all possible instantiations. A
solution of a CSP is a complete instantiation of
variables (x1, ... , xn) with values (di1, ... , din)
with dik E Dk found in a CN that is consistent
with all constraints.
Principles of processing CSPs have been de-
veloped in (Montanari, 1974), (Waltz, 1975)
and (Mackworth, 1977).
</bodyText>
<subsectionHeader confidence="0.99844">
2.2 Constraint Optimization
</subsectionHeader>
<bodyText confidence="0.99922805">
In many problem cases no complete instantia-
tion exists that satisfies all constraints: either
we get stuck by solving only a part of the prob-
lem or constraints need to be considered defea-
sible for a certain penalty. Thus finding a so-
lution becomes a constraint optimization prob-
lem (COP). A COP is denoted as a quadruple
(X, D, C, f), where (X, D, C) is a CSP and f
is a cost function on (partial) variable instan-
tiations. f might be computed by multiply-
ing the penalties of all violated constraints. A
solution of a COP is a complete instantiation,
where f((di1, ... , din)) is optimal. This term
becomes zero if the penalty of at least one vio-
lated constraint is zero. These constraints are
called hard, those with a penalty greater zero
are called soft.
An more precise formulation of COPs (also
called partial constraint satisfaction problems),
can be found in (Freuder and Wallace, 1989).
</bodyText>
<subsectionHeader confidence="0.99177">
2.3 Dynamic Constraint Satisfaction
</subsectionHeader>
<bodyText confidence="0.99591035">
The traditional CSP and COP framework is
only applicable to static problems, where the
number of variables, the values in their domains
and the constraints are all known in advance. In
a dynamically changing environment these as-
sumptions don’t hold any more as new variables,
new values or new constraints become available
over time. A dynamic constraint satisfaction
problem (DynCSP) is construed as a series of
CSPs P0, P1,... that change periodically over
time by loss of gain of values, variables or con-
straints (Pi+1 = Pi + OPi+1). For each problem
change OPi+1 we try to find a solution change
OSi+1 such that Si+1 = Si + ASi+1 is a solution
to Pi+1. The legitimate hope is that this is more
efficient than solving Pi+1 the naive way from
scratch whenever things change.
This notation is consistent with previous ones
found in in (Dechter and Dechter, 1988) and
(Wir´en, 1993).
</bodyText>
<subsectionHeader confidence="0.975381">
2.4 Dynamic Constraint Optimization
</subsectionHeader>
<bodyText confidence="0.99817835">
Most notions of DynCSPs in the literature are
an extension of the classical CSP that use hard
constraints exclusively. To model the aimed
application of incremental parsing however, we
still like to use weighted constraints. There-
fore we define dynamic constraint optimization
problems (DynCOP) the same way DynCSPs
were defined on the basis of CSPs as a series
of COPs P0, P1,... that change over time. In
addition to changing variables, values and con-
straints we are concerned with changes of the
cost function as well. In particular, variable in-
stantiations evaluated formerly might now be
judged differently. As this could entail seri-
ous computational problems we try to keep
changes in the cost function monotonic, that
is re-evaluation shall only give lower penalties
than before, i.e. instantiations that become in-
consistent once don’t get consistent later on
again.
</bodyText>
<sectionHeader confidence="0.962559" genericHeader="introduction">
3 Basic Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999478285714286">
Using constraint satisfaction techniques for nat-
ural language parsing was introduced first in
(Maruyama, 1990) by defining a constraint de-
pendency grammar (CDG) that maps nicely on
the notion of a CSP. A CDG is a quadruple
(E, R, L, C), where E is a lexicon of known
words, R is a set of roles of a word. A role rep-
resents a level of language like ‘SYN’ or ‘SEM’.
L is a set of labels for each role (e.g. I‘SUBJ’,
’OBJ’}, I‘AGENT’,‘PATIENT’}), and C is a
constraint grammar consisting of atomic logical
formulas. Now, the only thing that is left in
order to match a CDGs to a CSPs is to define
variables and their possible values. For each
word of an utterance and for each role we al-
locate one variable that can take values of the
form ei,j = (r, wi, l, wj) with r E R, wi, wj E E
and l E L. ei,j is called the dependency edge
between word form wi and wj labeled with l on
the description level r. A dependency edge of
the form ei,root is called the root edge. Hence
a dependency tree of an utterance of length n
is a set of dependency edges s = {ei,j  |i E
{1, ... , n} , j E {1, ... , n} U {root} , i =� j}.
From this point on parsing natural language
has become a matter of constraint processing
as can be found in the CSP literature (Dechter,
2001).
</bodyText>
<sectionHeader confidence="0.985928" genericHeader="method">
4 Weighted Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999989545454545">
In (Schr¨oder, 2002) the foundations of depen-
dency parsing have been carried over to COPs
using weighted constraint dependency grammars
(WCDG), a framework to model language using
all-quantified logical formulas on dependency
structures. Penalties for constraint violations
aren’t necessarily static once, but can be lexi-
calized or computed arithmetically on the ba-
sis of the structure under consideration. The
following constraints are rather typical once re-
stricting the properties of subject edges:
</bodyText>
<equation confidence="0.998999142857143">
{X:SYN} : SUBJ-init : 0.0 :
X.label = SUBJ -&gt;
( X@cat = NN  |X@cat = NE |
X@cat = FM  |X@cat = PPER ) &amp;
X&amp;quot;cat = VVFIN;
{X:SYN} : SUBJ-dist : 2.9 / X.length :
X.label = SUBJ -&gt; X.length &lt; 3;
</equation>
<bodyText confidence="0.9996935">
Both constraints have a scope of one depen-
dency edge on the syntax level ({X:SYN}). The
constraint SUBJ-init is a hard constraint stat-
ing that every dependency edge labeled SUBJ
shall have a nominal modifier and a finite
verb as its modifiee. The second constraint
SUBJ-dist is a soft one, such as every edge
with label SUBJ attached more than two words
away induces a penalty calculated by the term
2.9 / X.length. Note, that the maximal edge
length in SUBJ-dist is quite arbitrary and
should be extracted from a corpus automati-
cally as well as the grade of increasing penal-
ization. A realistic grammar consists of about
500 such handwritten constraints like the cur-
rently developed grammar for German (Daum
et al., 2003).
The notation used for constraints in this pa-
per is expressing valid formulas interpretable by
the WCDG constraint system. The following
definitions explain some of the primitives that
are part of the constraint language:
</bodyText>
<listItem confidence="0.9704615">
• X is a variable for a dependency edge of the
form ei,j = (r, wi,l, wj),
</listItem>
<figureCaption confidence="0.951432">
Figure 1: Architecture of WCDG
</figureCaption>
<listItem confidence="0.9998865">
• X@word (X&amp;quot;word) refers to the word form
wi E E (wj E E)
• X@id (X@id) refers to the position i (j)
• X.label refers to the label l E L
• X@cat (X&amp;quot;cat) refers to the POS-tag of the
modifier (modifiee)
• root(X&amp;quot;id) - true iff wj = root
• X.length is defined as |i − j|.
</listItem>
<bodyText confidence="0.999384285714286">
A complete definition can be found in (Schr¨oder
et al., 1999).
Figure (1) outlines the overall architecture of
the system consisting of a lexicon component,
ontologies and other external shallow parsing
components, all of which are accessed via con-
straints affecting the internal constraint net-
work as far as variables are in their scope. While
a static parsing model injects all variables into
the ring in Figure (1) once and then waits for
all constraints to let the variable instantiations
settle in a state of equilibrium, a dynamic opti-
mization process will add and remove variables
from the current scope repeatedly.
</bodyText>
<sectionHeader confidence="0.996322" genericHeader="method">
5 Dynamic Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999777428571429">
As indicated, the basic parsing model in WCDG
is a two stage process: first building up a con-
straint network given an utterance and second
constructing an optimal dependency parse. In a
dynamic system like an incremental dependency
parser these two steps are repeated in a loop
while consuming all bits from the input that
</bodyText>
<figure confidence="0.99812555">
Lexicon
Chunker
semantic
syntax
constraints
constraints
semantic–
reference
constraints
SEM SYN
syntax–semantic
constraints
reference
constraints
REF
syntax–
reference
constraints
Ontology
Tagger
</figure>
<bodyText confidence="0.99829362962963">
complete a sentence over time. In principle, the
problem of converting the static parsing model
into a dynamic one should only be a question
of repetitive updating the constraint network in
a subtle way. Additionally, information about
the internal state of the ‘constraint optimizer’
itself, which is not stored in the constraint net,
shall not get lost during consecutive iterations
as it (a) might participate in the update heuris-
tics of the first phase and (b) the parsing effort
during all previous loops might affect the cur-
rent computation substantially. We will come
back to this argument in Section 8.
Basically, changes to the constraint network
are only allowed after the parser has emitted a
parse tree. This is acceptable if the parser it-
self is interruptible providing the best parse tree
found so far. An interrupt may occur either
from ‘outside’ or from ‘inside’ by the parser it-
self taking into account the number of pending
new words not yet added. So it either may in-
tegrate a new word as soon as it arrives or wait
until further hypotheses have been checked. As
transformation based parsing has strong any-
time properties, these heuristics can be imple-
mented as part of a termination criterion be-
tween increments easily.
</bodyText>
<sectionHeader confidence="0.7318485" genericHeader="method">
6 Modeling expectations using
nonspec
</sectionHeader>
<subsectionHeader confidence="0.98512">
6.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999787714285714">
Analyzing sentence prefixes with a static parser,
that i.e. is not aware of the sentence being a pre-
fix, will yield at least a penalty for a fragmen-
tary representation. To get such a result at all,
the parser must allow partial parse trees. The
constraints S-init and frag illustrate modeling
of normal and fragmentary dependency trees.
</bodyText>
<equation confidence="0.997937666666667">
{X:SYN} : S-init : 0.0 :
X.label = S -&gt; root(X&amp;quot;id) &amp;
(X@cat = VVFIN  |X@cat = VMFIN |
X@cat = VAFIN  |... );
{X:SYN} : frag : 0.001 :
root(X&amp;quot;id) -&gt; X.label = S;
</equation>
<bodyText confidence="0.999449633333333">
Constraint S-init restricts all edges with la-
bel S to be finite verbs pointing to root. But if
some dependency edge is pointing to root and
is not labeled with S then constraint frag is vi-
olated and induces a penalty of 0.001. So every
fragment in sentence (2a) that can not be inte-
grated into the rest of the dependency tree will
increase the penalty of the structure by three or-
ders of magnitude. A constraint optimizer will
try to avoid an analysis with an overall penalty
of at least 1−12 and will search for another struc-
ture better than that. Modeling language in
a way that (2a) in fact turns out as the op-
timal solution is therefore difficult. Moreover,
the computational effort could be avoided if a
partial tree is promised to be integrated later
with fewer costs.
The only way to prevent a violation of frag
in WCDG is either by temporarily switching it
off completely or, preferably, by replacing the
root attachment with a nonspec dependency as
shown in (2b), thereby preventing the prerequi-
sites of frag in the first place while remaining
relevant for ‘normal’ dependency edges.
A prefix-analysis like (2a) might turn out to
be cognitively implausible as well, as humans
expect a proper NP-head to appear as long as
no other evidence forces an adjective to be nom-
inalized. Such a thesis can be modeled using
nonspec dependency edges.
</bodyText>
<subsectionHeader confidence="0.988521">
6.2 Definition
</subsectionHeader>
<bodyText confidence="0.988263846153846">
We now extend the original definition of
WCDG, so that a dependency tree is devised as
s = {ei,j  |i E {1, ... , n} U {*}, j E {1, ... , n} U
{root, *}, i =� j}. We will use the notation w*
to denote any unseen word. A dependency edge
modifying w* is written as ei,*, and an edge of
the form e*,i denotes a dependency edge of w*
modifying an already seen word. ei,* and e*,i
are called nonspec dependency edges.
Selective changes to the semantics of the con-
straint language have been made to accom-
plish nonspec dependency edges. So given two
edges ei1,i2 = (r, wi1, l�, wi2) and ej1,j2 =
</bodyText>
<figureCaption confidence="0.994324">
Figure 2: Example sentence prefix
</figureCaption>
<figure confidence="0.981906230769231">
the big blue bouncing ball w*
the big blue bouncing w*
DET
the big blue bouncing
DET
DET ADJ ADJ ADJ
ADJ
ADJ
ADJ
ADJ
ADJ SUBJ
ADJ
(a)
</figure>
<bodyText confidence="0.401045">
(r, wj1, l&apos;&apos;, wj2) with X - ei1,i2 and Y - ej1,j2:
</bodyText>
<listItem confidence="0.99071875">
• X&amp;quot;id = Y&amp;quot;id - false iff wi2 =� wj2 E E V
wi2 = wj2 = w*, and true otherwise
• X.length - |i1 − i2 |iff wi1, wi2 E E, and
n+1 iff wi2 = w*, (n: length of the current
sentence prefix)
• X&amp;quot;cat = (POS − tag) - false iff wi2 = w*
• nonspec(X&amp;quot;id) - true iff wi2 = w*
• spec(X&amp;quot;id) - true iff wi2 E E
</listItem>
<subsectionHeader confidence="0.954476">
6.3 Properties
</subsectionHeader>
<bodyText confidence="0.999906517241379">
Although every nonspec dependency in Figure
(2b) points to the same word w*, two nonspec
dependency edges are not taken to be connected
at the top (X&amp;quot;id = Y&amp;quot;id - false) as we don’t
know yet whether wi and wj will be modifying
the same word in the future.
In general, the above extension to the con-
straint language is reasonable enough to fit into
the notion of static parsing, that is a grammar
tailored for incremental parsing can still be used
for static parsing. An unpleasant consequence
of nonspec is, that more error-cases might occur
in an already existing constraint grammar for
static parsing that was not written with nonspec
dependency edges in mind. Therefore we intro-
duced guard-predicates nonspec() and spec()
that complete those guard-predicates already
part of the language (e.g. root() and exists()).
These must be added by the grammarian to pre-
vent logical error-cases in a constraint formula.
Looking back at the constraints we’ve dis-
cussed so far, the constraints SUBJ-init and
S-init have to be adjusted to become nonspec-
aware because referring to the POS-tag is not
possible if the dependency edge under consider-
ation is of the form ei,* or e*,i. Thus a prefix-
analysis like (2c) is inducing a hard violation
of SUBJ-init. We have to rewrite SUBJ-init to
allow (2c) as follows:
</bodyText>
<equation confidence="0.997573833333333">
{X:SYN} : SUBJ-init : 0.0 :
X.label = SUBJ -&gt;
( nonspec(X@id) |
X@cat = NN  |X@cat = NE |
X@cat = FM  |X@cat = PPER) &amp;
( nonspec(X&amp;quot;id)  |X&amp;quot;cat = VVFIN );
</equation>
<bodyText confidence="0.999768357142857">
When all constraints have been checked for
logical errors due to a possible nonspec depen-
dency edge, performance of the modified gram-
mar will not have changed for static parsing but
will accept nonspec dependency edges.
Using the nonspec() predicate, we are able to
write constraints that are triggered by nonspec
edges only being not pertinent for ‘normal’
edges. For example we like to penalize nonspec
dependency edges the older they become dur-
ing the incremental course and thereby allow a
cheaper structure to take over seamlessly. This
can easily be achieved with a constraint like
nonspec-dist, similar to SUBJ-dist:
</bodyText>
<equation confidence="0.9672085">
{X:SYN} : nonspec-dist : 1.9 / X.length :
nonspec(X&amp;quot;id) -&gt; X.length &lt; 2;
</equation>
<bodyText confidence="0.999830333333333">
The effect of nonspec-dist is, that a certain
amount of penalty is caused by (SYN, the, DET,
w*) and (SYN, big, ADJ, w*) in (2b). Figure (2c)
illustrates the desired prefix-analysis in the next
loop when nonspec edges become pricey due to
their increased attachment length. In a real-
life constraint grammar (2c) will be optimal ba-
sically because the head of the NP occurred,
therefore overruling every alternative nonspec
dependency edges that crosses the head. The
latter alternative structure will either cause a
projectivity violation with all other non-head
components of the NP that are still linked to the
head or cause an alternative head to be elected
when becoming available.
</bodyText>
<sectionHeader confidence="0.991297" genericHeader="method">
7 Dynamic Constraint Networks
</sectionHeader>
<bodyText confidence="0.9955806">
nonspec dependency edges play an important
role when updating a constraint network to re-
flect the problem change OP,. Maintaining the
constraint network in the first phase is crucial
for the overall performance as a more sophisti-
cated strategy to prune edges might compensate
computational effort in the second phase.
Figure (3) illustrates a sentence of three
words being processed one word wi per time-
point ti as follows:
</bodyText>
<listItem confidence="0.910867666666667">
1. for each edge e of the form ej,* or e*,j, (j &lt;
i) recompute penalty f((e)). If its penalty
drops below a, then remove e. Otherwise
derive edge e&apos; on the basis of e
2. add new edges ei,* and e*,i to the CN as far
as f((ei,*)) &lt; a and f((e*,i)) &lt; a
3. remove each edge e from the CN if it’s lo-
cal penalty is lower than the penalty of the
best parse so far.
</listItem>
<bodyText confidence="0.9999525">
The parameter a is a penalty threshold that
determines the amount of nonspec edges being
pruned. Any remaining nonspec edge indicates
where the constraint network remains extensible
</bodyText>
<figure confidence="0.989615769230769">
w1 w2 w3 w*
e1,2
e2,1
e1,3
e3,1
e1,*
e*,1
e2,3
e3,2
e2,*
e*,2
e3,*
e*,3
t3
w1 w2 w*
e1,2
e2,1
e1,*
e*,1
e2,*
e*,2
t2
e1,*
w1 w*
e*,1
t1
</figure>
<figureCaption confidence="0.999786">
Figure 3: Incremental update of a constraint network
</figureCaption>
<bodyText confidence="0.933821857142857">
and provides an upper estimate of any future
edge derived from it. This holds only if some
prerequisites of monotony are guaranteed:
• The penalty of a parse will always be lower
than each of the penalties on its depen-
dency edges (guaranteed by the multiplica-
tive cost function).
</bodyText>
<listItem confidence="0.951548">
• Each nonspec edge must have a penalty
that is an upper boundary of each depen-
dency edge that will be derived from it:
</listItem>
<equation confidence="0.638324">
f((e*,i1)) &gt;= f((ei2,i1)) and
f((ei1,*)) &gt;= f((ei1,i2)) with (i1 &lt; i2).
</equation>
<bodyText confidence="0.9941195">
Only then will pruning of nonspec depen-
dency edges be correct.
</bodyText>
<listItem confidence="0.919246666666667">
• As a consequence the overall penalties of
prefix-analyses degrade monotonically over
time: f(si) &gt;= f(si+1)
</listItem>
<bodyText confidence="0.9998819">
Note, that the given strategy to update the
constraint network does not take the struc-
ture of the previous prefix-analysis into account
but only works on the basis of the complete
constraint network. Nevertheless, the previous
parse tree is used as a starting point for the
next optimization step, so that near-by parse
trees will be constructed within a few transfor-
mation steps using the alternatives licensed by
the constraint network.
</bodyText>
<sectionHeader confidence="0.986433" genericHeader="method">
8 The Optimizer
</sectionHeader>
<bodyText confidence="0.998609178571429">
So far we discussed the first phase of a dynamic
dependency parser building up a series of prob-
lems P0, P1,... changing Pi using OPz+1 in terms
of maintaining a dynamic constraint network.
In the second phase ‘the optimizer’ tries to ac-
commodate to those changes by constructing
Si+1 on the basis of Si and Pi+1.
WCDG offers a decent set of methods to com-
pute the second phase, one of which implements
a guided local search (Daum and Menzel, 2002).
The key idea of GLS is to add a heuristics
sitting on top of a local search procedure by in-
troducing weights for each possible dependency
edge in the constraint network. Initially being
zero, weights are increased steadily if a local
search settles in a local optimum. By augment-
ing the cost function f with these extra weights,
further transformations are initiated along the
gradient of f. Thus every weight of a depen-
dency edge resembles an custom-tailored con-
straint whose penalty is learned during search.
The question now to be asked is, how weights
acquired during the incremental course of pars-
ing influence GLS. The interesting property
is that the weights of dependency edges inte-
grated earlier will always tend to be higher than
weights of most recently introduced dependency
edges as a matter of saturation. Thus keeping
old weights will prevent GLS from changing old
dependency edges and encourage transforming
newer dependency edges first. Old dependency
edges will not be transformed until more re-
cent constraint violations have been removed or
old structures are strongly deprecated recently.
This is a desirable behavior as it stabilizes for-
mer dependency structures with no extra provi-
sions to the base mechanism. Transformations
will be focused on the most recently added de-
pendency edges. This approach is comparable
to a simulated annealing heuristics where trans-
formations are getting more infrequent due to a
declining ‘temperature’.
Another very successful implementation of
‘the optimizer’ in WCDG is called Frobbing
(Foth et al., 2000) which is a transformation
based parsing technique similar to taboo search.
One interesting feature of Frobbing is its ability
to estimate an upper boundary of the penalty of
any structure using a certain dependency edge
and a certain word form. In an incremental
parsing mode the penalty limit of a nonspec de-
pendency edge will then be an estimate of any
structure derived from it and thereby provide a
good heuristics to prune nonspec edges falling
beyond a during the maintenance of the con-
straint network.
</bodyText>
<sectionHeader confidence="0.995419" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999933888888889">
Incremental parsing using weighted constraint
optimization has been classified as a special case
of dynamic dependency parsing.
The idea of nonspec dependency edges has
been described as a means of expressing ex-
pectations during the incremental process. We
have argued that (a) nonspec dependency edges
are more adequate to model prefix-analyses and
(b) offer a computational advantage compared
to a parser that models the special situation of
a sentence prefix only by means of violated con-
straints.
While completing the notion of dynamic de-
pendency parsing, we assessed the consequences
of an incremental parsing mode to the most
commonly used optimization methods used in
WCDG.
Further research will need to add the notion
of DynCSP to the WCDG system as well as
an adaption and completion of an existing con-
straint grammar. This will allow an in-depth
evaluation of dynamic dependency parsing with
and without nonspec dependency edges given
the optimization methods currently available.
Experiments will be conducted to acquire pars-
ing times per increment that are then compared
to human reading times.
</bodyText>
<sectionHeader confidence="0.998258" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.971214333333333">
This research has been partially supported by
Deutsche Forschungsgemeinschaft under grant
Me 1472/4-1.
</bodyText>
<sectionHeader confidence="0.998586" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99899779245283">
Michael Daum and Wolfgang Menzel. 2002.
Parsing natural language using guided local
search. In F. van Harmelen, editor, Proc. 15th
European Conference on Artificial Intelli-
gence, Amsterdam. IOS Press.
Michael Daum, Kilian Foth, and Wolfgang Men-
zel. 2003. Constraint based integration of
deep and shallow parsing techniques. In Pro-
ceedings 11th Conference of the European
Chapter of the ACL, Budapest, Hungary.
Rina Dechter and Avi Dechter. 1988. Be-
lief Maintenance in Dynamic Constraint Net-
works. In 7th Annual Conference of the
American Association of Artificial Intelli-
gence, pages 37–42.
Rina Dechter. 2001. Constraint Processing.
Morgan Kaufmann, September.
Kilian Foth, Wolfgang Menzel, and Ingo
Schr¨oder. 2000. A transformation-based
parsing technique with anytime properties.
In Proc. 4th International Workshop on Pars-
ing Technologies, pages 89–100, Trento, Italy.
Eugene C. Freuder and Richard J. Wallace.
1989. Partial constraint satisfaction. In
Proc. 11th International Joint Conference on
Artificial Intelligence (IJCAI-89), volume 58,
pages 278–283, Detroit, Michigan, USA.
A. K. Mackworth. 1977. Consistency in net-
works of relations. Artificial Intelligence.
8:99-118.
Hiroshi Maruyama. 1990. Structure disam-
biguation with constraint propagation. In
Proc. the 28th Annual Meeting of the ACL,
pages 31–38, Pittsburgh.
U. Montanari. 1974. Networks of constraints:
Fundamental properties and applications to
picture processing. Inform. Sci., 7:95-132.
Ingo Schr¨oder, Kilian A. Foth, and Michael
Schulz. 1999. [X]cdg Benutzerhandbuch.
Technical Report Dawai-HH-13, Universit¨at
Hamburg.
Ingo Schr¨oder. 2002. Natural Language Parsing
with Graded Constraints. Ph.D. thesis, Dept.
of Computer Science, University of Hamburg,
Germany.
David Waltz. 1975. Understanding line draw-
ings of scenes with shadows. In P. H. Win-
ston, editor, The Psychology of Computer Vi-
sion. McGraw–Hill, New York.
Mats Wir´en. 1993. Bounded incremental pars-
ing. In Proc. 6th Twente Workshop on
Language Technology, pages 145–156, En-
schede/Netherlands.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.553964">
<title confidence="0.999982">Dynamic Dependency Parsing</title>
<author confidence="0.666663">Michael</author>
<affiliation confidence="0.867102">Natural Language Systems Department of Computer University of</affiliation>
<email confidence="0.965828">micha@nats.informatik.uni-hamburg.de</email>
<abstract confidence="0.997935">The inherent robustness of a system might be an important prerequisite for an incremental parsing model to the effect that grammaticality requirements on full sentences may be suspended or allowed to be violated transiently. However, we present additional means that allow the grammarian to model prefix-analyses by altering a grammar for non-incremental parsing in a controlled way. This is done by introducing underspecified dependency edges that model the expected relation between already seen and yet unseen words during parsing. Thus the basic framework of weighted constraint dependency parsing is extended by the notion of dynamic dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Daum</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Parsing natural language using guided local search.</title>
<date>2002</date>
<booktitle>Proc. 15th European Conference on Artificial Intelligence,</booktitle>
<editor>In F. van Harmelen, editor,</editor>
<publisher>IOS Press.</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="21598" citStr="Daum and Menzel, 2002" startWordPosition="3706" endWordPosition="3709">ptimization step, so that near-by parse trees will be constructed within a few transformation steps using the alternatives licensed by the constraint network. 8 The Optimizer So far we discussed the first phase of a dynamic dependency parser building up a series of problems P0, P1,... changing Pi using OPz+1 in terms of maintaining a dynamic constraint network. In the second phase ‘the optimizer’ tries to accommodate to those changes by constructing Si+1 on the basis of Si and Pi+1. WCDG offers a decent set of methods to compute the second phase, one of which implements a guided local search (Daum and Menzel, 2002). The key idea of GLS is to add a heuristics sitting on top of a local search procedure by introducing weights for each possible dependency edge in the constraint network. Initially being zero, weights are increased steadily if a local search settles in a local optimum. By augmenting the cost function f with these extra weights, further transformations are initiated along the gradient of f. Thus every weight of a dependency edge resembles an custom-tailored constraint whose penalty is learned during search. The question now to be asked is, how weights acquired during the incremental course of </context>
</contexts>
<marker>Daum, Menzel, 2002</marker>
<rawString>Michael Daum and Wolfgang Menzel. 2002. Parsing natural language using guided local search. In F. van Harmelen, editor, Proc. 15th European Conference on Artificial Intelligence, Amsterdam. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Daum</author>
<author>Kilian Foth</author>
<author>Wolfgang Menzel</author>
</authors>
<title>Constraint based integration of deep and shallow parsing techniques.</title>
<date>2003</date>
<booktitle>In Proceedings 11th Conference of the European Chapter of the ACL,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="9998" citStr="Daum et al., 2003" startWordPosition="1662" endWordPosition="1665">init is a hard constraint stating that every dependency edge labeled SUBJ shall have a nominal modifier and a finite verb as its modifiee. The second constraint SUBJ-dist is a soft one, such as every edge with label SUBJ attached more than two words away induces a penalty calculated by the term 2.9 / X.length. Note, that the maximal edge length in SUBJ-dist is quite arbitrary and should be extracted from a corpus automatically as well as the grade of increasing penalization. A realistic grammar consists of about 500 such handwritten constraints like the currently developed grammar for German (Daum et al., 2003). The notation used for constraints in this paper is expressing valid formulas interpretable by the WCDG constraint system. The following definitions explain some of the primitives that are part of the constraint language: • X is a variable for a dependency edge of the form ei,j = (r, wi,l, wj), Figure 1: Architecture of WCDG • X@word (X&amp;quot;word) refers to the word form wi E E (wj E E) • X@id (X@id) refers to the position i (j) • X.label refers to the label l E L • X@cat (X&amp;quot;cat) refers to the POS-tag of the modifier (modifiee) • root(X&amp;quot;id) - true iff wj = root • X.length is defined as |i − j|. A </context>
</contexts>
<marker>Daum, Foth, Menzel, 2003</marker>
<rawString>Michael Daum, Kilian Foth, and Wolfgang Menzel. 2003. Constraint based integration of deep and shallow parsing techniques. In Proceedings 11th Conference of the European Chapter of the ACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rina Dechter</author>
<author>Avi Dechter</author>
</authors>
<title>Belief Maintenance in Dynamic Constraint Networks.</title>
<date>1988</date>
<booktitle>In 7th Annual Conference of the American Association of Artificial Intelligence,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="6344" citStr="Dechter and Dechter, 1988" startWordPosition="1016" endWordPosition="1019"> don’t hold any more as new variables, new values or new constraints become available over time. A dynamic constraint satisfaction problem (DynCSP) is construed as a series of CSPs P0, P1,... that change periodically over time by loss of gain of values, variables or constraints (Pi+1 = Pi + OPi+1). For each problem change OPi+1 we try to find a solution change OSi+1 such that Si+1 = Si + ASi+1 is a solution to Pi+1. The legitimate hope is that this is more efficient than solving Pi+1 the naive way from scratch whenever things change. This notation is consistent with previous ones found in in (Dechter and Dechter, 1988) and (Wir´en, 1993). 2.4 Dynamic Constraint Optimization Most notions of DynCSPs in the literature are an extension of the classical CSP that use hard constraints exclusively. To model the aimed application of incremental parsing however, we still like to use weighted constraints. Therefore we define dynamic constraint optimization problems (DynCOP) the same way DynCSPs were defined on the basis of CSPs as a series of COPs P0, P1,... that change over time. In addition to changing variables, values and constraints we are concerned with changes of the cost function as well. In particular, variab</context>
</contexts>
<marker>Dechter, Dechter, 1988</marker>
<rawString>Rina Dechter and Avi Dechter. 1988. Belief Maintenance in Dynamic Constraint Networks. In 7th Annual Conference of the American Association of Artificial Intelligence, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rina Dechter</author>
</authors>
<title>Constraint Processing.</title>
<date>2001</date>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="8554" citStr="Dechter, 2001" startWordPosition="1420" endWordPosition="1421">h word of an utterance and for each role we allocate one variable that can take values of the form ei,j = (r, wi, l, wj) with r E R, wi, wj E E and l E L. ei,j is called the dependency edge between word form wi and wj labeled with l on the description level r. A dependency edge of the form ei,root is called the root edge. Hence a dependency tree of an utterance of length n is a set of dependency edges s = {ei,j |i E {1, ... , n} , j E {1, ... , n} U {root} , i =� j}. From this point on parsing natural language has become a matter of constraint processing as can be found in the CSP literature (Dechter, 2001). 4 Weighted Dependency Parsing In (Schr¨oder, 2002) the foundations of dependency parsing have been carried over to COPs using weighted constraint dependency grammars (WCDG), a framework to model language using all-quantified logical formulas on dependency structures. Penalties for constraint violations aren’t necessarily static once, but can be lexicalized or computed arithmetically on the basis of the structure under consideration. The following constraints are rather typical once restricting the properties of subject edges: {X:SYN} : SUBJ-init : 0.0 : X.label = SUBJ -&gt; ( X@cat = NN |X@cat </context>
</contexts>
<marker>Dechter, 2001</marker>
<rawString>Rina Dechter. 2001. Constraint Processing. Morgan Kaufmann, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Foth</author>
<author>Wolfgang Menzel</author>
<author>Ingo Schr¨oder</author>
</authors>
<title>A transformation-based parsing technique with anytime properties.</title>
<date>2000</date>
<booktitle>In Proc. 4th International Workshop on Parsing Technologies,</booktitle>
<pages>89--100</pages>
<location>Trento, Italy.</location>
<marker>Foth, Menzel, Schr¨oder, 2000</marker>
<rawString>Kilian Foth, Wolfgang Menzel, and Ingo Schr¨oder. 2000. A transformation-based parsing technique with anytime properties. In Proc. 4th International Workshop on Parsing Technologies, pages 89–100, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene C Freuder</author>
<author>Richard J Wallace</author>
</authors>
<title>Partial constraint satisfaction.</title>
<date>1989</date>
<booktitle>In Proc. 11th International Joint Conference on Artificial Intelligence (IJCAI-89),</booktitle>
<volume>58</volume>
<pages>278--283</pages>
<location>Detroit, Michigan, USA.</location>
<contexts>
<context position="5443" citStr="Freuder and Wallace, 1989" startWordPosition="864" endWordPosition="867">ization problem (COP). A COP is denoted as a quadruple (X, D, C, f), where (X, D, C) is a CSP and f is a cost function on (partial) variable instantiations. f might be computed by multiplying the penalties of all violated constraints. A solution of a COP is a complete instantiation, where f((di1, ... , din)) is optimal. This term becomes zero if the penalty of at least one violated constraint is zero. These constraints are called hard, those with a penalty greater zero are called soft. An more precise formulation of COPs (also called partial constraint satisfaction problems), can be found in (Freuder and Wallace, 1989). 2.3 Dynamic Constraint Satisfaction The traditional CSP and COP framework is only applicable to static problems, where the number of variables, the values in their domains and the constraints are all known in advance. In a dynamically changing environment these assumptions don’t hold any more as new variables, new values or new constraints become available over time. A dynamic constraint satisfaction problem (DynCSP) is construed as a series of CSPs P0, P1,... that change periodically over time by loss of gain of values, variables or constraints (Pi+1 = Pi + OPi+1). For each problem change O</context>
</contexts>
<marker>Freuder, Wallace, 1989</marker>
<rawString>Eugene C. Freuder and Richard J. Wallace. 1989. Partial constraint satisfaction. In Proc. 11th International Joint Conference on Artificial Intelligence (IJCAI-89), volume 58, pages 278–283, Detroit, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Mackworth</author>
</authors>
<title>Consistency in networks of relations.</title>
<date>1977</date>
<journal>Artificial Intelligence.</journal>
<pages>8--99</pages>
<contexts>
<context position="4521" citStr="Mackworth, 1977" startWordPosition="705" endWordPosition="706">straints. The ‘local knowledge’ of a CSP is encoded in a constraint network (CN) consisting of nodes bundling all values of a variable consistent with all unary constraints. The edges of a CN depict binary constraints between the connected variables. So a CN is a compact representation (of a superset) of all possible instantiations. A solution of a CSP is a complete instantiation of variables (x1, ... , xn) with values (di1, ... , din) with dik E Dk found in a CN that is consistent with all constraints. Principles of processing CSPs have been developed in (Montanari, 1974), (Waltz, 1975) and (Mackworth, 1977). 2.2 Constraint Optimization In many problem cases no complete instantiation exists that satisfies all constraints: either we get stuck by solving only a part of the problem or constraints need to be considered defeasible for a certain penalty. Thus finding a solution becomes a constraint optimization problem (COP). A COP is denoted as a quadruple (X, D, C, f), where (X, D, C) is a CSP and f is a cost function on (partial) variable instantiations. f might be computed by multiplying the penalties of all violated constraints. A solution of a COP is a complete instantiation, where f((di1, ... , </context>
</contexts>
<marker>Mackworth, 1977</marker>
<rawString>A. K. Mackworth. 1977. Consistency in networks of relations. Artificial Intelligence. 8:99-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
</authors>
<title>Structure disambiguation with constraint propagation.</title>
<date>1990</date>
<booktitle>In Proc. the 28th Annual Meeting of the ACL,</booktitle>
<pages>31--38</pages>
<location>Pittsburgh.</location>
<contexts>
<context position="7412" citStr="Maruyama, 1990" startWordPosition="1183" endWordPosition="1184">r time. In addition to changing variables, values and constraints we are concerned with changes of the cost function as well. In particular, variable instantiations evaluated formerly might now be judged differently. As this could entail serious computational problems we try to keep changes in the cost function monotonic, that is re-evaluation shall only give lower penalties than before, i.e. instantiations that become inconsistent once don’t get consistent later on again. 3 Basic Dependency Parsing Using constraint satisfaction techniques for natural language parsing was introduced first in (Maruyama, 1990) by defining a constraint dependency grammar (CDG) that maps nicely on the notion of a CSP. A CDG is a quadruple (E, R, L, C), where E is a lexicon of known words, R is a set of roles of a word. A role represents a level of language like ‘SYN’ or ‘SEM’. L is a set of labels for each role (e.g. I‘SUBJ’, ’OBJ’}, I‘AGENT’,‘PATIENT’}), and C is a constraint grammar consisting of atomic logical formulas. Now, the only thing that is left in order to match a CDGs to a CSPs is to define variables and their possible values. For each word of an utterance and for each role we allocate one variable that c</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Hiroshi Maruyama. 1990. Structure disambiguation with constraint propagation. In Proc. the 28th Annual Meeting of the ACL, pages 31–38, Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Montanari</author>
</authors>
<title>Networks of constraints: Fundamental properties and applications to picture processing.</title>
<date>1974</date>
<journal>Inform. Sci.,</journal>
<pages>7--95</pages>
<contexts>
<context position="4484" citStr="Montanari, 1974" startWordPosition="700" endWordPosition="701"> all variables are called context constraints. The ‘local knowledge’ of a CSP is encoded in a constraint network (CN) consisting of nodes bundling all values of a variable consistent with all unary constraints. The edges of a CN depict binary constraints between the connected variables. So a CN is a compact representation (of a superset) of all possible instantiations. A solution of a CSP is a complete instantiation of variables (x1, ... , xn) with values (di1, ... , din) with dik E Dk found in a CN that is consistent with all constraints. Principles of processing CSPs have been developed in (Montanari, 1974), (Waltz, 1975) and (Mackworth, 1977). 2.2 Constraint Optimization In many problem cases no complete instantiation exists that satisfies all constraints: either we get stuck by solving only a part of the problem or constraints need to be considered defeasible for a certain penalty. Thus finding a solution becomes a constraint optimization problem (COP). A COP is denoted as a quadruple (X, D, C, f), where (X, D, C) is a CSP and f is a cost function on (partial) variable instantiations. f might be computed by multiplying the penalties of all violated constraints. A solution of a COP is a complet</context>
</contexts>
<marker>Montanari, 1974</marker>
<rawString>U. Montanari. 1974. Networks of constraints: Fundamental properties and applications to picture processing. Inform. Sci., 7:95-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Schr¨oder</author>
<author>Kilian A Foth</author>
<author>Michael Schulz</author>
</authors>
<title>[X]cdg Benutzerhandbuch.</title>
<date>1999</date>
<tech>Technical Report Dawai-HH-13,</tech>
<institution>Universit¨at Hamburg.</institution>
<marker>Schr¨oder, Foth, Schulz, 1999</marker>
<rawString>Ingo Schr¨oder, Kilian A. Foth, and Michael Schulz. 1999. [X]cdg Benutzerhandbuch. Technical Report Dawai-HH-13, Universit¨at Hamburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Schr¨oder</author>
</authors>
<title>Natural Language Parsing with Graded Constraints.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. of Computer Science, University of Hamburg,</institution>
<marker>Schr¨oder, 2002</marker>
<rawString>Ingo Schr¨oder. 2002. Natural Language Parsing with Graded Constraints. Ph.D. thesis, Dept. of Computer Science, University of Hamburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Waltz</author>
</authors>
<title>Understanding line drawings of scenes with shadows.</title>
<date>1975</date>
<booktitle>The Psychology of Computer Vision.</booktitle>
<editor>In P. H. Winston, editor,</editor>
<publisher>McGraw–Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="4499" citStr="Waltz, 1975" startWordPosition="702" endWordPosition="703">called context constraints. The ‘local knowledge’ of a CSP is encoded in a constraint network (CN) consisting of nodes bundling all values of a variable consistent with all unary constraints. The edges of a CN depict binary constraints between the connected variables. So a CN is a compact representation (of a superset) of all possible instantiations. A solution of a CSP is a complete instantiation of variables (x1, ... , xn) with values (di1, ... , din) with dik E Dk found in a CN that is consistent with all constraints. Principles of processing CSPs have been developed in (Montanari, 1974), (Waltz, 1975) and (Mackworth, 1977). 2.2 Constraint Optimization In many problem cases no complete instantiation exists that satisfies all constraints: either we get stuck by solving only a part of the problem or constraints need to be considered defeasible for a certain penalty. Thus finding a solution becomes a constraint optimization problem (COP). A COP is denoted as a quadruple (X, D, C, f), where (X, D, C) is a CSP and f is a cost function on (partial) variable instantiations. f might be computed by multiplying the penalties of all violated constraints. A solution of a COP is a complete instantiation</context>
</contexts>
<marker>Waltz, 1975</marker>
<rawString>David Waltz. 1975. Understanding line drawings of scenes with shadows. In P. H. Winston, editor, The Psychology of Computer Vision. McGraw–Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Wir´en</author>
</authors>
<title>Bounded incremental parsing.</title>
<date>1993</date>
<booktitle>In Proc. 6th Twente Workshop on Language Technology,</booktitle>
<pages>145--156</pages>
<marker>Wir´en, 1993</marker>
<rawString>Mats Wir´en. 1993. Bounded incremental parsing. In Proc. 6th Twente Workshop on Language Technology, pages 145–156, Enschede/Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>