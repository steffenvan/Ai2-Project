<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008514">
<title confidence="0.848974">
Learning Bilingual Word Representations by Marginalizing Alignments
</title>
<author confidence="0.992568">
Tom´aˇs Koˇcisk´y Karl Moritz Hermann Phil Blunsom
</author>
<affiliation confidence="0.997421">
Department of Computer Science
University of Oxford
</affiliation>
<address confidence="0.99679">
Oxford, OX1 3QD, UK
</address>
<email confidence="0.999">
{tomas.kocisky,karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.994804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999744">
We present a probabilistic model that si-
multaneously learns alignments and dis-
tributed representations for bilingual data.
By marginalizing over word alignments
the model captures a larger semantic con-
text than prior work relying on hard align-
ments. The advantage of this approach is
demonstrated in a cross-lingual classifica-
tion task, where we outperform the prior
published state of the art.
</bodyText>
<sectionHeader confidence="0.998423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937793650794">
Distributed representations have become an in-
creasingly important tool in machine learning.
Such representations—typically continuous vec-
tors learned in an unsupervised setting—can fre-
quently be used in place of hand-crafted, and thus
expensive, features. By providing a richer rep-
resentation than what can be encoded in discrete
settings, distributed representations have been suc-
cessfully used in many areas. This includes AI and
reinforcement learning (Mnih et al., 2013), image
retrieval (Kiros et al., 2013), language modelling
(Bengio et al., 2003), sentiment analysis (Socher
et al., 2011; Hermann and Blunsom, 2013), frame-
semantic parsing (Hermann et al., 2014), and doc-
ument classification (Klementiev et al., 2012).
In Natural Language Processing (NLP), the use
of distributed representations is motivated by the
idea that they could capture semantics and/or syn-
tax, as well as encoding a continuous notion of
similarity, thereby enabling information sharing
between similar words and other units. The suc-
cess of distributed approaches to a number of
tasks, such as listed above, supports this notion
and its implied benefits (see also Turian et al.
(2010) and Collobert and Weston (2008)).
While most work employing distributed repre-
sentations has focused on monolingual tasks, mul-
tilingual representations would also be useful for
several NLP-related tasks. Such problems include
document classification, machine translation, and
cross-lingual information retrieval, where multi-
lingual data is frequently the norm. Furthermore,
learning multilingual representations can also be
useful for cross-lingual information transfer, that
is exploiting resource-fortunate languages to gen-
erate supervised data in resource-poor ones.
We propose a probabilistic model that simulta-
neously learns word alignments and bilingual dis-
tributed word representations. As opposed to pre-
vious work in this field, which has relied on hard
alignments or bilingual lexica (Klementiev et al.,
2012; Mikolov et al., 2013), we marginalize out
the alignments, thus capturing more bilingual se-
mantic context. Further, this results in our dis-
tributed word alignment (DWA) model being the
first probabilistic account of bilingual word repre-
sentations. This is desirable as it allows better rea-
soning about the derived representations and fur-
thermore, makes the model suitable for inclusion
in higher-level tasks such as machine translation.
The contributions of this paper are as follows.
We present a new probabilistic similarity measure
which is based on an alignment model and prior
language modeling work which learns and relates
word representations across languages. Subse-
quently, we apply these embeddings to a standard
document classification task and show that they
outperform the current published state of the art
(Hermann and Blunsom, 2014b). As a by-product
we develop a distributed version of FASTALIGN
(Dyer et al., 2013), which performs on par with
the original model, thereby demonstrating the ef-
ficacy of the learned bilingual representations.
</bodyText>
<sectionHeader confidence="0.983832" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999857">
The IBM alignment models, introduced by Brown
et al. (1993), form the basis of most statistical ma-
chine translation systems. In this paper we base
our alignment model on FASTALIGN (FA), a vari-
</bodyText>
<page confidence="0.972601">
224
</page>
<bodyText confidence="0.9552765">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
ation of IBM model 2 introduced by Dyer et al.
(2013). This model is both fast and produces
alignments on par with the state of the art. Further,
to induce the distributed representations we incor-
porate ideas from the log-bilinear language model
presented by Mnih and Hinton (2007).
</bodyText>
<sectionHeader confidence="0.55135" genericHeader="method">
2.1 IBM Model 2
</sectionHeader>
<bodyText confidence="0.999906727272727">
Given a parallel corpus with aligned sentences, an
alignment model can be used to discover matching
words and phrases across languages. Such mod-
els are an integral part of most machine translation
pipelines. An alignment model learns p(f, a|e) (or
p(e, a&apos;|f)) for the source and target sentences e
and f (sequences of words). a represents the word
alignment across these two sentences from source
to target. IBM model 2 (Brown et al., 1993) learns
alignment and translation probabilities in a gener-
ative style as follows:
</bodyText>
<equation confidence="0.996190333333333">
J
p(f, a|e) = p(J|I) p(aj|j, I, J) p(fj|eap) ,
j=1
</equation>
<bodyText confidence="0.998923">
where p(J|I) captures the two sentence lengths;
p(aj|j, I, J) the alignment and p(fj|eap) the
translation probability. Sentence likelihood is
given by marginalizing out the alignments, which
results in the following equation:
</bodyText>
<equation confidence="0.788571">
p(i|j, I, J) p(fj|ei) .
</equation>
<bodyText confidence="0.999809363636364">
We use FASTALIGN (FA) (Dyer et al., 2013), a
log-linear reparametrization of IBM model 2. This
model uses an alignment distribution defined by
a single parameter that measures how close the
alignment is to the diagonal. This replaces the
original multinomial alignment distribution which
often suffered from sparse counts. This improved
model was shown to run an order of magnitude
faster than IBM model 4 and yet still outperformed
it in terms of the BLEU score and, on Chinese-
English data, in alignment error rate (AER).
</bodyText>
<subsectionHeader confidence="0.998007">
2.2 Log-Bilinear Language Model
</subsectionHeader>
<bodyText confidence="0.9461935">
Language models assign a probability measure
to sequences of words. We use the log-bilinear
language model proposed by Mnih and Hinton
(2007). It is an n-gram based model defined in
terms of an energy function E(wn; w1:n−1). The
probability for predicting the next word wn given
its preceding context of n − 1 words is expressed
using the energy function
</bodyText>
<equation confidence="0.93983">
T T
rwi Ci I rwn −br rwn−bwn
</equation>
<bodyText confidence="0.998075294117647">
as p(wn|w1:n−1) = Zc 1 exp(−E(wn;w1:n−1))
where Zc = Ewn exp (−E(wn; w1:n−1)) is the
normalizer, rwi E Rd are word representations,
Ci E Rdxd are context transformation matrices,
and br E Rd, bwn E R are representation and word
biases respectively. Here, the sum of the trans-
formed context-word vectors endeavors to be close
to the word we want to predict, since the likelihood
in the model is maximized when the energy of the
observed data is minimized.
This model can be considered a variant of a
log-linear language model in which, instead of
defining binary n-gram features, the model learns
the features of the input and output words, and
a transformation between them. This provides a
vastly more compact parameterization of a lan-
guage model as n-gram features are not stored.
</bodyText>
<subsectionHeader confidence="0.999194">
2.3 Multilingual Representation Learning
</subsectionHeader>
<bodyText confidence="0.999897892857143">
There is some recent prior work on multilin-
gual distributed representation learning. Simi-
lar to the model presented here, Klementiev et
al. (2012) and Zou et al. (2013) learn bilingual
embeddings using word alignments. These two
models are non-probabilistic and conditioned on
the output of a separate alignment model, un-
like our model, which defines a probability dis-
tribution over translations and marginalizes over
all alignments. These models are also highly re-
lated to prior work on bilingual lexicon induc-
tion (Haghighi et al., 2008). Other recent ap-
proaches include Sarath Chandar et al. (2013),
Lauly et al. (2013) and Hermann and Blunsom
(2014a, 2014b). These models avoid word align-
ment by transferring information across languages
using a composed sentence-level representation.
While all of these approaches are related to the
model proposed in this paper, it is important to
note that our approach is novel by providing a
probabilistic account of these word embeddings.
Further, we learn word alignments and simultane-
ously use these alignments to guide the represen-
tation learning, which could be advantageous par-
ticularly for rare tokens, where a sentence based
approach might fail to transfer information.
Related work also includes Mikolov et al.
(2013), who learn a transformation matrix to
</bodyText>
<equation confidence="0.999033571428571">
p(f|e) = p(J|I)
I
i=0
J
j=1
E(wn; w1:n−1)=− n−1�
i=1
</equation>
<page confidence="0.980967">
225
</page>
<bodyText confidence="0.99995675">
reconcile monolingual embedding spaces, in an
l2 norm sense, using dictionary entries instead of
alignments, as well as Schwenk et al. (2007) and
Schwenk (2012), who also use distributed repre-
sentations for estimating translation probabilities.
Faruqui and Dyer (2014) use a technique based on
CCA and alignments to project monolingual word
representations to a common vector space.
</bodyText>
<sectionHeader confidence="0.988247" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999995964285714">
Here we describe our distributed word alignment
(DWA) model. The DWA model can be viewed
as a distributed extension of the FA model in that
it uses a similarity measure over distributed word
representations instead of the standard multino-
mial translation probability employed by FA. We
do this using a modified version of the log-bilinear
language model in place of the translation proba-
bilities p(fj|ei) at the heart of the FA model. This
allows us to learn word representations for both
languages, a translation matrix relating these vec-
tor spaces, as well as alignments at the same time.
Our modifications to the log-bilinear model are
as follows. Where the original log-bilinear lan-
guage model uses context words to predict the next
word—this is simply the distributed extension of
an n-gram language model—we use a word from
the source language in a parallel sentence to pre-
dict a target word. An additional aspect of our
model, which demonstrates its flexibility, is that it
is simple to include further context from the source
sentence, such as words around the aligned word
or syntactic and semantic annotations. In this pa-
per we experiment with a transformed sum over
k context words to each side of the aligned source
word. We evaluate different context sizes and re-
port the results in Section 5. We define the energy
function for the translation probabilities to be
</bodyText>
<equation confidence="0.92673">
�rT ez+�Ts rf−bT r rf−bf (1)
</equation>
<bodyText confidence="0.997400142857143">
where rez, rf E Rd are vector representations for
source and target words ei+s E VE, f E VF in
their respective vocabularies, Ts E Rdxd is the
transformation matrix for each surrounding con-
text position, br E Rd are the representation bi-
ases, and bf E R is a bias for each word f E VF.
The translation probability is given by
</bodyText>
<equation confidence="0.9837665">
p(f|ei) = Zz exp (−E(f, ei)) , where
Zez = E f exp (−E(f, ei)) is the normalizer.
</equation>
<bodyText confidence="0.99931275">
In addition to these translation probabilities, we
have parameterized the translation probabilities
for the null word using a softmax over an addi-
tional weight vector.
</bodyText>
<subsectionHeader confidence="0.998817">
3.1 Class Factorization
</subsectionHeader>
<bodyText confidence="0.999809157894737">
We improve training performance using a class
factorization strategy (Morin and Bengio, 2005)
as follows. We augment the translation probabil-
ity to be p(f|e) = p(cf|e) p(f|cf, e) where cf
is a unique predetermined class of f; the class
probability is modeled using a similar log-bilinear
model as above, but instead of predicting a word
representation rf we predict the class representa-
tion rcf (which is learned with the model) and we
add respective new context matrices and biases.
Note that the probability of the word f depends
on both the class and the given context words: it is
normalized only over words in the class cf.
In our training we create classes based on word
frequencies in the corpus as follows. Considering
words in the order of their decreasing frequency,
we add word types into a class until the total fre-
quency of the word types in the currently consid-
ered class is less than total tokens and the class size is
</bodyText>
<equation confidence="0.922864">
|V1
|
�
</equation>
<bodyText confidence="0.949793">
less than |VF |. We have found that the maximal
class size affects the speed the most.
</bodyText>
<sectionHeader confidence="0.994648" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.9999748">
The original FA model optimizes the likelihood
using the expectation maximization (EM) algo-
rithm where, in the M-step, the parameter update
is analytically solvable, except for the λ parameter
(the diagonal tension), which is optimized using
gradient descent (Dyer et al., 2013). We modified
the implementations provided with CDEC (Dyer et
al., 2010), retaining its default parameters.
In our model, DWA, we optimize the likelihood
using the EM as well. However, while training we
fix the counts of the E-step to those computed by
FA, trained for the default 5 iterations, to aid the
convergence rate, and optimize the M-step only.
Let θ be the parameters for our model. Then the
gradient for each sentence is given by
</bodyText>
<equation confidence="0.994457090909091">
∂ ∂θ logp(f|e) =
�
I p(l|k, I, J)p(fk|el)
EIi=0 p(i|k, I, J) p(fk|ei)
l=0
�∂θ∂ log(p(l|k, I, J)p(fk|el))
k
E(f, ei) = −
s=−k
J
k=1
</equation>
<page confidence="0.991151">
226
</page>
<bodyText confidence="0.99998275">
where the first part are the counts from the FA
model and second part comes from our model.
We compute the gradient for the alignment
probabilities in the same way as in the FA model,
and the gradient for the translation probabilities
using back-propagation (Rumelhart et al., 1986).
For parameter update, we use ADAGRAD as the
gradient descent algorithm (Duchi et al., 2011).
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99999725">
We first evaluate the alignment error rate of our
approach, which establishes the model’s ability to
both learn alignments as well as word representa-
tions that explain these alignments. Next, we use
a cross-lingual document classification task to ver-
ify that the representations are semantically useful.
We also inspect the embedding space qualitatively
to get some insight into the learned structure.
</bodyText>
<subsectionHeader confidence="0.991599">
5.1 Alignment Evaluation
</subsectionHeader>
<bodyText confidence="0.999979857142857">
We compare the alignments learned here with
those of the FASTALIGN model which produces
very good alignments and translation BLEU
scores. We use the same language pairs and
datasets as in Dyer et al. (2013), that is the FBIS
Chinese-English corpus, and the French-English
section of the Europarl corpus (Koehn, 2005). We
used the preprocessing tools from CDEC and fur-
ther replaced all unique tokens with UNK. We
trained our models with 100 dimensional repre-
sentations for up to 40 iterations, and the FA
model for 5 iterations as is the default.
Table 1 shows that our model learns alignments
on part with those of the FA model. This is in line
with expectation as our model was trained using
the FA expectations. However, it confirms that
the learned word representations are able to ex-
plain translation probabilities. Surprisingly, con-
text seems to have little impact on the alignment
error, suggesting that the model receives sufficient
information from the aligned words themselves.
</bodyText>
<subsectionHeader confidence="0.995797">
5.2 Document Classification
</subsectionHeader>
<bodyText confidence="0.999718875">
A standard task for evaluating cross-lingual word
representations is document classification where
training is performed in one and evaluation in an-
other language. This tasks require semantically
plausible embeddings (for classification) which
are valid across two languages (for the semantic
transfer). Hence this task requires more of the
word embeddings than the previous task.
</bodyText>
<table confidence="0.966347714285714">
Languages Model
FA DWA DWA
k = 0 k = 3
ZHIEN 49.4 48.4 48.7
ENIZH 44.9 45.3 45.9
FRIEN 17.1 17.2 17.0
ENIFR 16.6 16.3 16.1
</table>
<tableCaption confidence="0.999266">
Table 1: Alignment error rate (AER) compar-
</tableCaption>
<bodyText confidence="0.981525081081081">
ison, in both directions, between the FASTAL-
IGN (FA) alignment model and our model (DWA)
with k context words (see Equation 1). Lower
numbers indicate better performance.
We mainly follow the setup of Klementiev et al.
(2012) and use the German-English parallel cor-
pus of the European Parliament proceedings to
train the word representations. We perform the
classification task on the Reuters RCV1/2 corpus.
Unlike Klementiev et al. (2012), we do not use that
corpus during the representation learning phase.
We remove all words occurring less than five times
in the data and learn 40 dimensional word embed-
dings in line with prior work.
To train a classifier on English data and test it
on German documents we first project word rep-
resentations from English into German: we select
the most probable German word according to the
learned translation probabilities, and then compute
document representations by averaging the word
representations in each document. We use these
projected representations for training and subse-
quently test using the original German data and
representations. We use an averaged perceptron
classifier as in prior work, with the number of
epochs (3) tuned on a subset of the training set.
Table 2 shows baselines from previous work
and classification accuracies. Our model outper-
forms the model by Klementiev et al. (2012), and
it also outperforms the most comparable models
by Hermann and Blunsom (2014b) when training
on German data and performs on par with it when
training on English data.1 It seems that our model
learns more informative representations towards
document classification, even without additional
monolingual language models or context informa-
tion. Again the impact of context is inconclusive.
</bodyText>
<footnote confidence="0.994346">
1From Hermann and Blunsom (2014a, 2014b) we only
compare with models equivalent with respect to embedding
dimensionality and training data. They still achieve the state
of the art when using additional training data.
</footnote>
<page confidence="0.984964">
227
</page>
<table confidence="0.999835888888889">
Model en → de de → en
Majority class 46.8 46.8
Glossed 65.1 68.6
MT 68.1 67.4
Klementiev et al. 77.6 71.1
BiCVM ADD 83.7 71.4
BiCVM BI 83.4 69.2
DWA (k = 0) 82.8 76.0
DWA (k = 3) 83.1 75.4
</table>
<tableCaption confidence="0.775006666666667">
Table 2: Document classification accuracy when
trained on 1,000 training examples of the RCV1/2
corpus (train→test). Baselines are the majority
</tableCaption>
<bodyText confidence="0.991699">
class, glossed, and MT (Klementiev et al., 2012).
Further, we are comparing to Klementiev et al.
(2012), BiCVM ADD (Hermann and Blunsom,
2014a), and BiCVM BI (Hermann and Blunsom,
2014b). k is the context size, see Equation 1.
</bodyText>
<subsectionHeader confidence="0.998106">
5.3 Representation Visualization
</subsectionHeader>
<bodyText confidence="0.999944888888889">
Following the document classification task we
want to gain further insight into the types of fea-
tures our embeddings learn. For this we visu-
alize word representations using t-SNE projec-
tions (van der Maaten and Hinton, 2008). Fig-
ure 1 shows an extract from our projection of the
2,000 most frequent German words, together with
an expected representation of a translated English
word given translation probabilities. Here, it is
interesting to see that the model is able to learn
related representations for words chair and rat-
spr¨asidentschaft (presidency) even though these
words were not aligned by our model. Figure 2
shows an extract from the visualization of the
10,000 most frequent English words trained on an-
other corpus. Here again, it is evident that the em-
beddings are semantically plausible with similar
words being closely aligned.
</bodyText>
<sectionHeader confidence="0.993233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998475333333333">
We presented a new probabilistic model for learn-
ing bilingual word representations. This dis-
tributed word alignment model (DWA) learns both
representations and alignments at the same time.
We have shown that the DWA model is able
to learn alignments on par with the FASTALIGN
alignment model which produces very good align-
ments, thereby determining the efficacy of the
learned representations which are used to calculate
</bodyText>
<figureCaption confidence="0.9995276">
Figure 1: A visualization of the expected represen-
tation of the translated English word chair among
the nearest German words: words never aligned
(green), and those seen aligned (blue) with it.
Figure 2: A cluster of English words from the
</figureCaption>
<bodyText confidence="0.979682217391304">
10,000 most frequent English words visualized us-
ing t-SNE. Word representations were optimized
for p(zh|en) (k = 0).
word translation probabilities for the alignment
task. Subsequently, we have demonstrated that
our model can effectively be used to project doc-
uments from one language to another. The word
representations our model learns as part of the
alignment process are semantically plausible and
useful. We highlighted this by applying these em-
beddings to a cross-lingual document classifica-
tion task where we outperform prior work, achieve
results on par with the current state of the art and
provide new state-of-the-art results on one of the
tasks. Having provided a probabilistic account of
word representations across multiple languages,
future work will focus on applying this model to
machine translation and related tasks, for which
previous approaches of learning such embeddings
are less suited. Another avenue for further study
is to combine this method with monolingual lan-
guage models, particularly in the context of se-
mantic transfer into resource-poor languages.
</bodyText>
<sectionHeader confidence="0.996182" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998051666666667">
This work was supported by a Xerox Foundation
Award and EPSRC grant number EP/K036580/1.
We acknowledge the use of the Oxford ARC.
</bodyText>
<page confidence="0.991942">
228
</page>
<note confidence="0.764574090909091">
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the 10th Machine Translation Summit.
References
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155, February.
</note>
<reference confidence="0.994013382978723">
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311, June.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of ICML.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121–2159, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of NAACL-
HLT.
Manaal Faruqui and Chris Dyer. 2014. Improving Vec-
tor Space Word Representations Using Multilingual
Correlation. In Proceedings of EACL.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
HLT.
Karl Moritz Hermann and Phil Blunsom. 2013. The
Role of Syntax in Vector Space Models of Compo-
sitional Semantics. In Proceedings of ACL.
Karl Moritz Hermann and Phil Blunsom. 2014a. Mul-
tilingual Distributed Representations without Word
Alignment. In Proceedings of ICLR.
Karl Moritz Hermann and Phil Blunsom. 2014b. Mul-
tilingual Models for Compositional Distributional
Semantics. In Proceedings of ACL.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic Frame Iden-
tification with Distributed Word Representations. In
Proceedings of ACL.
Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdi-
nov. 2013. Multimodal neural language models. In
NIPS Deep Learning Workshop.
Stanislas Lauly, Alex Boulanger, and Hugo Larochelle.
2013. Learning multilingual word representations
using a bag-of-words autoencoder. In NIPS Deep
Learning Workshop.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for ma-
chine translation. CoRR, abs/1309.4168.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of ICML.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Alex Graves, Ioannis Antonoglou, Daan Wierstra,
and Martin Riedmiller. 2013. Playing atari with
deep reinforcement learning. In NIPS Deep Learn-
ing Workshop.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics, pages 246–252.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning representations by back-
propagating errors. Nature, 323:533–536, October.
A P Sarath Chandar, M Khapra Mitesh, B Ravindran,
Vikas Raykar, and Amrita Saha. 2013. Multilingual
deep learning. In Deep Learning Workshop at NIPS.
Holger Schwenk, Marta R. Costa-jussa, and Jose A.
R. Fonollosa. 2007. Smooth bilingual n-gram trans-
lation. In Proceedings of EMNLP-CoNLL.
Holger Schwenk. 2012. Continuous space translation
models for phrase-based statistical machine transla-
tion. In Proceedings of COLING: Posters.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visual-
izing high-dimensional data using t-sne. Journal of
Machine Learning Research, 9:2579–2605.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual Word Embed-
dings for Phrase-Based Machine Translation. In
Proceedings of EMNLP.
</reference>
<page confidence="0.998915">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915787">
<title confidence="0.999988">Learning Bilingual Word Representations by Marginalizing Alignments</title>
<author confidence="0.999081">Tom´aˇs Koˇcisk´y Karl Moritz Hermann Phil Blunsom</author>
<affiliation confidence="0.99981">Department of Computer University of</affiliation>
<address confidence="0.996329">Oxford, OX1 3QD, UK</address>
<abstract confidence="0.992382363636364">We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3785" citStr="Brown et al. (1993)" startWordPosition="545" endWordPosition="548">stic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations. 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bilinear language model presen</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="1869" citStr="Collobert and Weston (2008)" startWordPosition="266" endWordPosition="269">sis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for several NLP-related tasks. Such problems include document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones. We propose a probabilistic model that simultaneously learns wo</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="13049" citStr="Duchi et al., 2011" startWordPosition="2091" endWordPosition="2094">mize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by ∂ ∂θ logp(f|e) = � I p(l|k, I, J)p(fk|el) EIi=0 p(i|k, I, J) p(fk|ei) l=0 �∂θ∂ log(p(l|k, I, J)p(fk|el)) k E(f, ei) = − s=−k J k=1 226 where the first part are the counts from the FA model and second part comes from our model. We compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation (Rumelhart et al., 1986). For parameter update, we use ADAGRAD as the gradient descent algorithm (Duchi et al., 2011). 5 Experiments We first evaluate the alignment error rate of our approach, which establishes the model’s ability to both learn alignments as well as word representations that explain these alignments. Next, we use a cross-lingual document classification task to verify that the representations are semantically useful. We also inspect the embedding space qualitatively to get some insight into the learned structure. 5.1 Alignment Evaluation We compare the alignments learned here with those of the FASTALIGN model which produces very good alignments and translation BLEU scores. We use the same lan</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Jonathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL System Demonstrations.</booktitle>
<contexts>
<context position="12170" citStr="Dyer et al., 2010" startWordPosition="1935" endWordPosition="1938">ncy, we add word types into a class until the total frequency of the word types in the currently considered class is less than total tokens and the class size is |V1 | � less than |VF |. We have found that the maximal class size affects the speed the most. 4 Learning The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013). We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters. In our model, DWA, we optimize the likelihood using the EM as well. However, while training we fix the counts of the E-step to those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by ∂ ∂θ logp(f|e) = � I p(l|k, I, J)p(fk|el) EIi=0 p(i|k, I, J) p(fk|ei) l=0 �∂θ∂ log(p(l|k, I, J)p(fk|el)) k E(f, ei) = − s=−k J k=1 226 where the first part are the counts from the FA model and second part comes from our model. We c</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACLHLT.</booktitle>
<contexts>
<context position="3587" citStr="Dyer et al., 2013" startWordPosition="515" endWordPosition="518">representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations. 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ation of IBM model 2 introduced by Dyer et al. (2013).</context>
<context position="5296" citStr="Dyer et al., 2013" startWordPosition="790" endWordPosition="793">r p(e, a&apos;|f)) for the source and target sentences e and f (sequences of words). a represents the word alignment across these two sentences from source to target. IBM model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows: J p(f, a|e) = p(J|I) p(aj|j, I, J) p(fj|eap) , j=1 where p(J|I) captures the two sentence lengths; p(aj|j, I, J) the alignment and p(fj|eap) the translation probability. Sentence likelihood is given by marginalizing out the alignments, which results in the following equation: p(i|j, I, J) p(fj|ei) . We use FASTALIGN (FA) (Dyer et al., 2013), a log-linear reparametrization of IBM model 2. This model uses an alignment distribution defined by a single parameter that measures how close the alignment is to the diagonal. This replaces the original multinomial alignment distribution which often suffered from sparse counts. This improved model was shown to run an order of magnitude faster than IBM model 4 and yet still outperformed it in terms of the BLEU score and, on ChineseEnglish data, in alignment error rate (AER). 2.2 Log-Bilinear Language Model Language models assign a probability measure to sequences of words. We use the log-bil</context>
<context position="12098" citStr="Dyer et al., 2013" startWordPosition="1924" endWordPosition="1927">us as follows. Considering words in the order of their decreasing frequency, we add word types into a class until the total frequency of the word types in the currently considered class is less than total tokens and the class size is |V1 | � less than |VF |. We have found that the maximal class size affects the speed the most. 4 Learning The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013). We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters. In our model, DWA, we optimize the likelihood using the EM as well. However, while training we fix the counts of the E-step to those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by ∂ ∂θ logp(f|e) = � I p(l|k, I, J)p(fk|el) EIi=0 p(i|k, I, J) p(fk|ei) l=0 �∂θ∂ log(p(l|k, I, J)p(fk|el)) k E(f, ei) = − s=−k J k=1 226 where the first part are</context>
<context position="13698" citStr="Dyer et al. (2013)" startWordPosition="2192" endWordPosition="2195">te the alignment error rate of our approach, which establishes the model’s ability to both learn alignments as well as word representations that explain these alignments. Next, we use a cross-lingual document classification task to verify that the representations are semantically useful. We also inspect the embedding space qualitatively to get some insight into the learned structure. 5.1 Alignment Evaluation We compare the alignments learned here with those of the FASTALIGN model which produces very good alignments and translation BLEU scores. We use the same language pairs and datasets as in Dyer et al. (2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005). We used the preprocessing tools from CDEC and further replaced all unique tokens with UNK. We trained our models with 100 dimensional representations for up to 40 iterations, and the FA model for 5 iterations as is the default. Table 1 shows that our model learns alignments on part with those of the FA model. This is in line with expectation as our model was trained using the FA expectations. However, it confirms that the learned word representations are able to explain translation p</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving Vector Space Word Representations Using Multilingual Correlation.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="8651" citStr="Faruqui and Dyer (2014)" startWordPosition="1332" endWordPosition="1335">nments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to p(f|e) = p(J|I) I i=0 J j=1 E(wn; w1:n−1)=− n−1� i=1 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 Model Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extension of the FA model in that it uses a similarity measure over distributed word representations instead of the standard multinomial translation probability employed by FA. We do this using a modified version of the log-bilinear language model in place of the translation probabilities p(fj|ei) at the heart of the FA model. This allows us to learn word representatio</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving Vector Space Word Representations Using Multilingual Correlation. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACLHLT.</booktitle>
<contexts>
<context position="7557" citStr="Haghighi et al., 2008" startWordPosition="1162" endWordPosition="1165"> model as n-gram features are not stored. 2.3 Multilingual Representation Learning There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The Role of Syntax in Vector Space Models of Compositional Semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1294" citStr="Hermann and Blunsom, 2013" startWordPosition="176" endWordPosition="179">on Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employ</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The Role of Syntax in Vector Space Models of Compositional Semantics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Distributed Representations without Word Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<contexts>
<context position="3502" citStr="Hermann and Blunsom, 2014" startWordPosition="501" endWordPosition="504">ual word representations. This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations. 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association</context>
<context position="7670" citStr="Hermann and Blunsom (2014" startWordPosition="1181" endWordPosition="1184">work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Miko</context>
<context position="16483" citStr="Hermann and Blunsom (2014" startWordPosition="2639" endWordPosition="2642">n word according to the learned translation probabilities, and then compute document representations by averaging the word representations in each document. We use these projected representations for training and subsequently test using the original German data and representations. We use an averaged perceptron classifier as in prior work, with the number of epochs (3) tuned on a subset of the training set. Table 2 shows baselines from previous work and classification accuracies. Our model outperforms the model by Klementiev et al. (2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.1 It seems that our model learns more informative representations towards document classification, even without additional monolingual language models or context information. Again the impact of context is inconclusive. 1From Hermann and Blunsom (2014a, 2014b) we only compare with models equivalent with respect to embedding dimensionality and training data. They still achieve the state of the art when using additional training data. 227 Model en → de de → en Majority class 46.8 46.8 Glossed 65.1 68.6 MT 6</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014a. Multilingual Distributed Representations without Word Alignment. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual Models for Compositional Distributional Semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3502" citStr="Hermann and Blunsom, 2014" startWordPosition="501" endWordPosition="504">ual word representations. This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTALIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations. 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association</context>
<context position="7670" citStr="Hermann and Blunsom (2014" startWordPosition="1181" endWordPosition="1184">work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Miko</context>
<context position="16483" citStr="Hermann and Blunsom (2014" startWordPosition="2639" endWordPosition="2642">n word according to the learned translation probabilities, and then compute document representations by averaging the word representations in each document. We use these projected representations for training and subsequently test using the original German data and representations. We use an averaged perceptron classifier as in prior work, with the number of epochs (3) tuned on a subset of the training set. Table 2 shows baselines from previous work and classification accuracies. Our model outperforms the model by Klementiev et al. (2012), and it also outperforms the most comparable models by Hermann and Blunsom (2014b) when training on German data and performs on par with it when training on English data.1 It seems that our model learns more informative representations towards document classification, even without additional monolingual language models or context information. Again the impact of context is inconclusive. 1From Hermann and Blunsom (2014a, 2014b) we only compare with models equivalent with respect to embedding dimensionality and training data. They still achieve the state of the art when using additional training data. 227 Model en → de de → en Majority class 46.8 46.8 Glossed 65.1 68.6 MT 6</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014b. Multilingual Models for Compositional Distributional Semantics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic Frame Identification with Distributed Word Representations.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1340" citStr="Hermann et al., 2014" startWordPosition="183" endWordPosition="186">asingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on</context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic Frame Identification with Distributed Word Representations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Richard S Zemel</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal neural language models.</title>
<date>2013</date>
<booktitle>In NIPS Deep Learning Workshop.</booktitle>
<contexts>
<context position="1183" citStr="Kiros et al., 2013" startWordPosition="160" endWordPosition="163">ross-lingual classification task, where we outperform the prior published state of the art. 1 Introduction Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion a</context>
</contexts>
<marker>Kiros, Zemel, Salakhutdinov, 2013</marker>
<rawString>Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdinov. 2013. Multimodal neural language models. In NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanislas Lauly</author>
<author>Alex Boulanger</author>
<author>Hugo Larochelle</author>
</authors>
<title>Learning multilingual word representations using a bag-of-words autoencoder.</title>
<date>2013</date>
<booktitle>In NIPS Deep Learning Workshop.</booktitle>
<contexts>
<context position="7640" citStr="Lauly et al. (2013)" startWordPosition="1176" endWordPosition="1179">re is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. R</context>
</contexts>
<marker>Lauly, Boulanger, Larochelle, 2013</marker>
<rawString>Stanislas Lauly, Alex Boulanger, and Hugo Larochelle. 2013. Learning multilingual word representations using a bag-of-words autoencoder. In NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<location>CoRR, abs/1309.4168.</location>
<contexts>
<context position="2677" citStr="Mikolov et al., 2013" startWordPosition="376" endWordPosition="379">nclude document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones. We propose a probabilistic model that simultaneously learns word alignments and bilingual distributed word representations. As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context. Further, this results in our distributed word alignment (DWA) model being the first probabilistic account of bilingual word representations. This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and </context>
<context position="8287" citStr="Mikolov et al. (2013)" startWordPosition="1275" endWordPosition="1278">2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to p(f|e) = p(J|I) I i=0 J j=1 E(wn; w1:n−1)=− n−1� i=1 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 Model Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extensi</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Three new graphical models for statistical language modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="4414" citStr="Mnih and Hinton (2007)" startWordPosition="644" endWordPosition="647">the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTALIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bilinear language model presented by Mnih and Hinton (2007). 2.1 IBM Model 2 Given a parallel corpus with aligned sentences, an alignment model can be used to discover matching words and phrases across languages. Such models are an integral part of most machine translation pipelines. An alignment model learns p(f, a|e) (or p(e, a&apos;|f)) for the source and target sentences e and f (sequences of words). a represents the word alignment across these two sentences from source to target. IBM model 2 (Brown et al., 1993) learns alignment and translation probabilities in a generative style as follows: J p(f, a|e) = p(J|I) p(aj|j, I, J) p(fj|eap) , j=1 where p(J</context>
<context position="5951" citStr="Mnih and Hinton (2007)" startWordPosition="894" endWordPosition="897">of IBM model 2. This model uses an alignment distribution defined by a single parameter that measures how close the alignment is to the diagonal. This replaces the original multinomial alignment distribution which often suffered from sparse counts. This improved model was shown to run an order of magnitude faster than IBM model 4 and yet still outperformed it in terms of the BLEU score and, on ChineseEnglish data, in alignment error rate (AER). 2.2 Log-Bilinear Language Model Language models assign a probability measure to sequences of words. We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn; w1:n−1). The probability for predicting the next word wn given its preceding context of n − 1 words is expressed using the energy function T T rwi Ci I rwn −br rwn−bwn as p(wn|w1:n−1) = Zc 1 exp(−E(wn;w1:n−1)) where Zc = Ewn exp (−E(wn; w1:n−1)) is the normalizer, rwi E Rd are word representations, Ci E Rdxd are context transformation matrices, and br E Rd, bwn E R are representation and word biases respectively. Here, the sum of the transformed context-word vectors endeavors to be close to the word we want to predict, </context>
</contexts>
<marker>Mnih, Hinton, 2007</marker>
<rawString>Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volodymyr Mnih</author>
<author>Koray Kavukcuoglu</author>
<author>David Silver</author>
</authors>
<title>Alex Graves, Ioannis Antonoglou,</title>
<date>2013</date>
<booktitle>In NIPS Deep Learning Workshop.</booktitle>
<location>Daan</location>
<contexts>
<context position="1145" citStr="Mnih et al., 2013" startWordPosition="154" endWordPosition="157"> this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art. 1 Introduction Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such a</context>
</contexts>
<marker>Mnih, Kavukcuoglu, Silver, 2013</marker>
<rawString>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederic Morin</author>
<author>Yoshua Bengio</author>
</authors>
<title>Hierarchical probabilistic neural network language model.</title>
<date>2005</date>
<booktitle>In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,</booktitle>
<pages>246--252</pages>
<contexts>
<context position="10873" citStr="Morin and Bengio, 2005" startWordPosition="1710" endWordPosition="1713">rds ei+s E VE, f E VF in their respective vocabularies, Ts E Rdxd is the transformation matrix for each surrounding context position, br E Rd are the representation biases, and bf E R is a bias for each word f E VF. The translation probability is given by p(f|ei) = Zz exp (−E(f, ei)) , where Zez = E f exp (−E(f, ei)) is the normalizer. In addition to these translation probabilities, we have parameterized the translation probabilities for the null word using a softmax over an additional weight vector. 3.1 Class Factorization We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows. We augment the translation probability to be p(f|e) = p(cf|e) p(f|cf, e) where cf is a unique predetermined class of f; the class probability is modeled using a similar log-bilinear model as above, but instead of predicting a word representation rf we predict the class representation rcf (which is learned with the model) and we add respective new context matrices and biases. Note that the probability of the word f depends on both the class and the given context words: it is normalized only over words in the class cf. In our training we create classes based on word frequencies in t</context>
</contexts>
<marker>Morin, Bengio, 2005</marker>
<rawString>Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Robert G. Cowell and Zoubin Ghahramani, editors, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning representations by backpropagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>323--533</pages>
<contexts>
<context position="12956" citStr="Rumelhart et al., 1986" startWordPosition="2076" endWordPosition="2079">those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by ∂ ∂θ logp(f|e) = � I p(l|k, I, J)p(fk|el) EIi=0 p(i|k, I, J) p(fk|ei) l=0 �∂θ∂ log(p(l|k, I, J)p(fk|el)) k E(f, ei) = − s=−k J k=1 226 where the first part are the counts from the FA model and second part comes from our model. We compute the gradient for the alignment probabilities in the same way as in the FA model, and the gradient for the translation probabilities using back-propagation (Rumelhart et al., 1986). For parameter update, we use ADAGRAD as the gradient descent algorithm (Duchi et al., 2011). 5 Experiments We first evaluate the alignment error rate of our approach, which establishes the model’s ability to both learn alignments as well as word representations that explain these alignments. Next, we use a cross-lingual document classification task to verify that the representations are semantically useful. We also inspect the embedding space qualitatively to get some insight into the learned structure. 5.1 Alignment Evaluation We compare the alignments learned here with those of the FASTALI</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by backpropagating errors. Nature, 323:533–536, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Sarath Chandar</author>
<author>M Khapra Mitesh</author>
<author>B Ravindran</author>
<author>Vikas Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>Multilingual deep learning.</title>
<date>2013</date>
<booktitle>In Deep Learning Workshop at NIPS.</booktitle>
<contexts>
<context position="7619" citStr="Chandar et al. (2013)" startWordPosition="1172" endWordPosition="1175">esentation Learning There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to tr</context>
</contexts>
<marker>Chandar, Mitesh, Ravindran, Raykar, Saha, 2013</marker>
<rawString>A P Sarath Chandar, M Khapra Mitesh, B Ravindran, Vikas Raykar, and Amrita Saha. 2013. Multilingual deep learning. In Deep Learning Workshop at NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Marta R Costa-jussa</author>
<author>Jose A R Fonollosa</author>
</authors>
<title>Smooth bilingual n-gram translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="8524" citStr="Schwenk et al. (2007)" startWordPosition="1315" endWordPosition="1318">to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to p(f|e) = p(J|I) I i=0 J j=1 E(wn; w1:n−1)=− n−1� i=1 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 Model Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extension of the FA model in that it uses a similarity measure over distributed word representations instead of the standard multinomial translation probability employed by FA. We do this using a modified version of the log-bilinear language mo</context>
</contexts>
<marker>Schwenk, Costa-jussa, Fonollosa, 2007</marker>
<rawString>Holger Schwenk, Marta R. Costa-jussa, and Jose A. R. Fonollosa. 2007. Smooth bilingual n-gram translation. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING:</booktitle>
<publisher>Posters.</publisher>
<contexts>
<context position="8543" citStr="Schwenk (2012)" startWordPosition="1320" endWordPosition="1321">is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for rare tokens, where a sentence based approach might fail to transfer information. Related work also includes Mikolov et al. (2013), who learn a transformation matrix to p(f|e) = p(J|I) I i=0 J j=1 E(wn; w1:n−1)=− n−1� i=1 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 Model Here we describe our distributed word alignment (DWA) model. The DWA model can be viewed as a distributed extension of the FA model in that it uses a similarity measure over distributed word representations instead of the standard multinomial translation probability employed by FA. We do this using a modified version of the log-bilinear language model in place of the</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. 2012. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of COLING: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1266" citStr="Socher et al., 2011" startWordPosition="172" endWordPosition="175">the art. 1 Introduction Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (20</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1837" citStr="Turian et al. (2010)" startWordPosition="261" endWordPosition="264">., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for several NLP-related tasks. Such problems include document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones. We propose a probabilistic mod</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L J P van der Maaten</author>
<author>G E Hinton</author>
</authors>
<title>Visualizing high-dimensional data using t-sne.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2579</pages>
<marker>van der Maaten, Hinton, 2008</marker>
<rawString>L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing high-dimensional data using t-sne. Journal of Machine Learning Research, 9:2579–2605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual Word Embeddings for Phrase-Based Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7186" citStr="Zou et al. (2013)" startWordPosition="1105" endWordPosition="1108">od in the model is maximized when the energy of the observed data is minimized. This model can be considered a variant of a log-linear language model in which, instead of defining binary n-gram features, the model learns the features of the input and output words, and a transformation between them. This provides a vastly more compact parameterization of a language model as n-gram features are not stored. 2.3 Multilingual Representation Learning There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y. Zou, Richard Socher, Daniel Cer, and Christopher D. Manning. 2013. Bilingual Word Embeddings for Phrase-Based Machine Translation. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>