<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.059826">
<title confidence="0.981957">
Learning Semantics and Selectional Preference of Adjective-Noun Pairs
</title>
<author confidence="0.999212">
Karl Moritz Hermann Chris Dyer
</author>
<affiliation confidence="0.999275">
Department of Computer Science Language Technologies Institute
University of Oxford Carnegie Mellon University
</affiliation>
<address confidence="0.9945">
Oxford OX1 3QD, UK Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.997417">
karl.moritz.hermann@cs.ox.ac.uk cdyer@cs.cmu.edu
</email>
<author confidence="0.997822">
Phil Blunsom Stephen Pulman
</author>
<affiliation confidence="0.9998005">
Department of Computer Science Department of Computer Science
University of Oxford University of Oxford
</affiliation>
<address confidence="0.996115">
Oxford OX1 3QD, UK Oxford OX1 3QD, UK
</address>
<email confidence="0.998845">
phil.blunsom@cs.ox.ac.uk stephen.pulman@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999457615384616">
We investigate the semantic relationship be-
tween a noun and its adjectival modifiers.
We introduce a class of probabilistic mod-
els that enable us to to simultaneously cap-
ture both the semantic similarity of nouns
and modifiers, and adjective-noun selectional
preference. Through a combination of novel
and existing evaluations we test the degree to
which adjective-noun relationships can be cat-
egorised. We analyse the effect of lexical con-
text on these relationships, and the efficacy of
the latent semantic representation for disam-
biguating word meaning.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923216216216">
Developing models of the meanings of words and
phrases is a key challenge for computational linguis-
tics. Distributed representations are useful in captur-
ing such meaning for individual words (Sato et al.,
2008; Maas and Ng, 2010; Curran, 2005). How-
ever, finding a compelling account of semantic com-
positionality that utilises such representations has
proven more difficult and is an active research topic
(Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). It is in
this area that our paper makes its contribution.
The dominant approaches to distributional se-
mantics have relied on relatively simple frequency
counting techniques. However, such approaches fail
to generalise to the much sparser distributions en-
countered when modeling compositional processes
and provide no account of selectional preference.
We propose a probabilistic model of the semantic
representations for nouns and modifiers. The foun-
dation of this model is a latent variable representa-
tion of noun and adjective semantics together with
their compositional probabilities. We employ this
formulation to give a dual view of noun-modifier
semantics: the induced latent variables provide an
explicit account of selectional preference while the
marginal distributions of the latent variables for each
word implicitly produce a distributed representation.
Most related work on selectional preference uses
class-based probabilities to approximate (sparse)
individual probabilities. Relevant papers include
O´ S´eaghdha (2010), who evaluates several topic
models adapted to learning selectional preference
using co-occurence and Baroni and Zamparelli
(2010), who represent nouns as vectors and adjec-
tives as matrices, thus treating them as functions
over noun meaning. Again, inference is achieved
using co-occurrence and dimensionality reduction.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="method">
2 Adjective-Noun Model
</sectionHeader>
<bodyText confidence="0.999987777777778">
We hypothesize that semantic classes determine the
semantic characteristics of nouns and adjectives, and
that the distribution of either with respect to other
components of the sentences they occur in is also
mediated by these classes (i.e., not by the words
themselves). We assume that in general nouns select
for adjectives,1 and that this selection is dependent
on both their latent semantic classes. In the next sec-
tion, we describe a model encoding our hypotheses.
</bodyText>
<sectionHeader confidence="0.595482" genericHeader="method">
2.1 Generative Process
</sectionHeader>
<bodyText confidence="0.97555125">
We model a corpus D of tuples of the form
(n, m, cl ... ck) consisting of a noun n, an adjective
m (modifier), and k words of context. The context
variables (cl ... ck) are treated as a bag of words and
</bodyText>
<footnote confidence="0.988982">
1We evaluate this hypothesis as well as its inverse.
</footnote>
<page confidence="0.968065">
70
</page>
<note confidence="0.8247015">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70–74,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.993087666666667">
Figure 1: Plate diagram illustrating our model of noun
and modifier semantic classes (designated N and M, re-
spectively), a modifier-noun pair (m,n), and its context.
</figureCaption>
<bodyText confidence="0.999957181818182">
include the words to the left and right of the noun,
its siblings and governing verbs. We designate the
vocabulary Vn for nouns, Vm for modifiers and Vc
for context. We use zi to refer to the ith tuple in D
and refer to variables within that tuple by subscript-
ing them with i, e.g., ni and c3,i are the noun and
the third context variable of zi. The latent noun and
adjective class variables are designated Ni and Mi.
The corpus D is generated according to the plate
diagram in figure 1. First, a set of parameters is
drawn. A multinomial TN representing the distribu-
tion of noun semantic classes in the corpus is drawn
from a Dirichlet distribution with parameter αN. For
each noun class i we have distributions TMi over
adjective classes, Tn i over Vn and Tc i over Vc, also
drawn from Dirichlet distributions. Finally, for each
adjective class j, we have distributions Tm j over Vm.
Next, the contents of the corpus are generated by
first drawing the length of the corpus (we do not
parametrise this since we never generate from this
model). Then, for each i, we generate noun class
Ni, adjective class Mi, and the tuple zi as follows:
</bodyText>
<equation confidence="0.953128">
Ni  |TN ∼ Multi(TN)
Mi  |TMNi ∼ Multi(TMNi)
ni  |TnNi ∼ Multi(TnNi)
mi  |TmMi∼ Multi(TmMi)
∀k: ck,i  |TcNi ∼ Multi(TcNi)
</equation>
<subsectionHeader confidence="0.998557">
2.2 Parameterization and Inference
</subsectionHeader>
<bodyText confidence="0.999959">
We use Gibbs sampling to estimate the distributions
of N and M, integrating out the multinomial param-
eters Tx (Griffiths and Steyvers, 2004). The Dirich-
let parameters α are drawn independently from a
r(1,1) distribution, and are resampled using slice
sampling at frequent intervals throughout the sam-
pling process (Johnson and Goldwater, 2009). This
“vague” prior encourages sparse draws from the
Dirichlet distribution. The number of noun and ad-
jective classes N and M was set to 50 each; other
sizes (100,150) did not significantly alter results.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999925583333333">
As our model was developed on the basis of several
hypotheses, we design the experiments and evalu-
ation so that these hypotheses can be examined on
their individual merit. We test the first hypothesis,
that nouns and adjectives can be represented by se-
mantic classes, recoverable using co-occurence, us-
ing a sense clustering evaluation by Ciaramita and
Johnson (2003). The second hypothesis, that the dis-
tribution with respect to context and to each other is
governed by these semantic classes is evaluated us-
ing pseudo-disambiguation (Clark and Weir, 2002;
Pereira et al., 1993; Rooth et al., 1999) and bigram
plausibility (Keller and Lapata, 2003) tests.
To test whether noun classes indeed select for ad-
jective classes, we also evaluate an inverse model
(Modi), where the adjective class is drawn first, in
turn generating both context and the noun class. In
addition, we evaluate copies of both models ignoring
context (Modnc and Modinc).
We use the British National Corpus (BNC), train-
ing on 90 percent and testing on 10 percent of the
corpus. Results are reported after 2,000 iterations
including a burn-in period of 200 iterations. Classes
are marginalised over every 10th iteration.
</bodyText>
<sectionHeader confidence="0.999599" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999886">
4.1 Supersense Tagging
</subsectionHeader>
<bodyText confidence="0.999791875">
Supersense tagging (Ciaramita and Johnson, 2003;
Curran, 2005) evaluates a model’s ability to clus-
ter words by their semantics. The task of this eval-
uation is to determine the WORDNET supersenses
of a given list of nouns. We report results on the
WN1.6 test set as defined by Ciaramita and John-
son (2003), who used 755 randomly selected nouns
with a unique supersense from the WORDNET 1.6
</bodyText>
<figure confidence="0.997782166666667">
Wc
αc
c
|N|
k
WN
Wn
αN
αn
N M
n m
|N ||M|
WM
Wm
αM
αm
|D|
|N|
</figure>
<page confidence="0.989175">
71
</page>
<bodyText confidence="0.998876833333333">
corpus. As their test set was random, results weren’t
exactly replicable. For a fair comparison, we select
all suitable nouns from the corpus that also appeared
in the training corpus. We report results on type and
token level (52314 tokens with 1119 types). The
baseline2 chooses the most common supersense.
</bodyText>
<table confidence="0.999588">
k Token Type
Baseline .241 .210
Ciaramita &amp; Johnson .523 .534
Curran - .680
Mod 10 .592 .517
Mod,,,, 10 .473 .410
</table>
<tableCaption confidence="0.994789666666667">
Table 1: Supersense evaluation results. Values are the
percentage of correctly assigned supersenses. k indicates
the number of nearest neighbours considered.
</tableCaption>
<bodyText confidence="0.999551571428572">
We use cosine-similarity on the marginal noun
class vectors to measure distance between nouns.
Each noun in the test set is then assigned a su-
persense by performing a distance-weighted voting
among its k nearest neighbours. Results of this eval-
uation are shown in Table 1, with Figure 2 showing
scores for model Mod across different values for k.
</bodyText>
<figureCaption confidence="0.996561">
Figure 2: Scores of Mod on the supersense task. The up-
per line denotes token-, the lower type-level scores. The
y-axis is the percentage of correct assignments, the x-axis
denotes the number of neighbours included in the vote.
</figureCaption>
<bodyText confidence="0.9999864">
The results demonstrate that nouns can semanti-
cally be represented as members of latent classes,
while the superiority of Mod over Mod,,,, supports
our hypothesis that context co-occurence is a key
feature for learning these classes.
</bodyText>
<subsectionHeader confidence="0.9945">
4.2 Pseudo-Disambiguation
</subsectionHeader>
<bodyText confidence="0.99996625">
Pseudo-disambiguation was introduced by Clark
and Weir (2002) to evaluate models of selectional
preference. The task is to select the more probable
of two candidate arguments to associate with a given
</bodyText>
<footnote confidence="0.641638666666667">
2The baseline results are from Ciaramita and Johnson
(2003). Using the majority baseline on the full test set, we only
get .176 and .160 for token and type respectively.
</footnote>
<bodyText confidence="0.996037444444445">
predicate. For us, this is to decide which adjective,
a1 or a2, is more likely to modify a noun n.
We follow the approach by Clark and Weir (2002)
to create the test data. To improve the quality of
the data, we filtered using bigram counts from the
Web1T corpus, setting a lower bound on the proba-
ble bigram (a1, n) and chosing a2 from five candi-
dates, picking the lowest count for bigram (a2, n).
We report results for all variants of our model in
</bodyText>
<tableCaption confidence="0.8406435">
Table 2. As baseline we use unigram counts in our
training data, chosing the more frequent adjective.
</tableCaption>
<table confidence="0.999081714285714">
L-bound 0 100 500 1000
Size 5714 5253 3741 2789
Baseline .543 .543 .539 .550
Mod .783 .792 .810 .816
Modi .781 .787 .800 .810
Modc,,, .720 .728 .746 .750
Modic,,, .722 .730 .747 .752
</table>
<tableCaption confidence="0.994390666666667">
Table 2: Pseudo-disambiguation: Percentage of correct
choices made. L-bound denotes the Web1T lower bound
on the (al, n) bigram, size the number of decisions made.
</tableCaption>
<bodyText confidence="0.996967625">
While all models decisively beat the baseline, the
models using context strongly outperform those that
do not. This supports our hypothesis regarding the
importance of context in semantic clustering.
The similarity between the normal and inverse
models implies that the direction of the noun-
adjective relationship has negligible impact for this
evaluation.
</bodyText>
<subsectionHeader confidence="0.99865">
4.3 Bigram Plausibility
</subsectionHeader>
<bodyText confidence="0.9997571">
Bigram plausibility (Keller and Lapata, 2003) is a
second evaluation for selectional preference. Unlike
the frequency-based pseudo-disambiguation task, it
evaluates how well a model matches human judge-
ment of the plausibility of adjective-noun pairs.
Keller and Lapata (2003) demonstrated a correlation
between frequencies and plausibility, but this does
not sufficiently explain human judgement. An ex-
ample taken from their unseen data set illustrates the
dissociation between frequency and plausibility:
</bodyText>
<listItem confidence="0.999319166666667">
• Frequent, implausible: “educational water”
• Infrequent, plausible: “difficult foreigner”3
The plausibility evaluation has two data sets of 90
adjective-noun pairs each. The first set (seen) con-
tains random bigrams from the BNC. The second set
(unseen) are bigrams not contained in the BNC.
</listItem>
<footnote confidence="0.61777775">
3At the time of writing, Google estimates 56,900 hits for
“educational water” and 575 hits for “difficult foreigner”. “Ed-
ucational water” ranks bottom in the gold standard of the unseen
set, “difficult foreigner” ranks in the top ten.
</footnote>
<page confidence="0.997765">
72
</page>
<bodyText confidence="0.998402166666667">
Recent work ( O´ S´eaghdha, 2010; Erk et al.,
2010) approximated plausibility with joint probabil-
ity (JP). We believe that for semantic plausibility
(not probability!) mutual information (MI), which
factors out acutal frequencies, is a better metric.4 We
report results using JP, MI and MI&amp;quot;2.
</bodyText>
<table confidence="0.999160642857143">
Seen Unseen
r p r p
AltaVista .650 — .480 —
BNC (Rasp) .543 .622 .135 .102
Pad´o et al. .479 .570 .120 .138
LDA .594 .558 .468 .459
ROOTH-LDA .575 .599 .501 .469
DUAL-LDA .460 .400 .334 .278
Mod (JP) .495 .413 .286 .276
Mod (MI) .394 .425 .471 .457
Mod (MI&amp;quot;2) .575 .501 .430 .408
Modnc (JP) .626 .505 .357 .369
Modnc (MI) .628 .574 .427 .385
Modnc (MI&amp;quot;2) .701 .623 .423 .394
</table>
<tableCaption confidence="0.96454325">
Table 3: Results (Pearson r and Spearman p correlations)
on the Keller and Lapata (2003) plausibility data. Bold
indicates best scores, underlining our best scores. High
values indicate high correlation with the gold standard.
</tableCaption>
<bodyText confidence="0.9989238">
Table 3 shows the performance of our models
compared to results reported in O´ S´eaghdha (2010).
As before, results between the normal and the in-
verse model (omitted due to space) are very simi-
lar. Surprisingly, the no-context models consistently
outperform the models using context on the seen
data set. This suggests that the seen data set can
quite precisely be ranked using frequency estimates,
which the no-context models might be better at cap-
turing without the ‘noise’ introduced by context.
</bodyText>
<table confidence="0.999349125">
Standard Inverse (i)
r p r p
Mod (JP) .286 .276 .243 .245
Mod (MI) .471 .457 .409 .383
Mod (MI&amp;quot;2) .430 .408 .362 .347
Modnc (JP) .357 .369 .181 .161
Modnc (MI) .427 .385 .220 .209
Modnc (MI&amp;quot;2) .423 .394 .218 .185
</table>
<tableCaption confidence="0.999898">
Table 4: Results on the unseen plausibility dataset.
</tableCaption>
<bodyText confidence="0.99866875">
The results on the unseen data set (Table 4)
prove interesting as well. The inverse no-context
model is performing significantly poorer than any
of the other models. To understand this result we
must investigate the differences between the unseen
data set and the seen data set and to the pseudo-
disambiguation evaluation. The key difference to
pseudo-disambiguation is that we measure a human
</bodyText>
<footnote confidence="0.893616">
4See (Evert, 2005) for a discussion of these metrics.
</footnote>
<bodyText confidence="0.9996388">
plausibility judgement, which — as we have demon-
strated — only partially correlates with bigram fre-
quencies. Our models were trained on the BNC,
hence they could only learn frequency estimates for
the seen data set, but not for the unseen data.
Based on our hypothesis about the role of con-
text, we expect Mod and Modi to learn semantic
classes based on the distribution of context. Without
the access to that context, we argued that Modnc and
Modinc would instead learn frequency estimates.5
The hypothesis that nouns generally select for ad-
jectives rather than vice versa further suggests that
Mod and Modnc would learn semantic properties
that Modi and Modinc could not learn so well.
In summary, we hence expected Mod to perform
best on the unseen data, learning semantics from
both context and noun-adjective selection. Also, as
supported by the results, we expected Modinc to
performs poorly, as it is the model least capable of
learning semantics according to our hypotheses.
</bodyText>
<sectionHeader confidence="0.993861" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998612">
We have presented a class of probabilistic mod-
els which successfully learn semantic clusterings of
nouns and a representation of adjective-noun selec-
tional preference. These models encoded our beliefs
about how adjective-noun pairs relate to each other
and to the other words in the sentence. The perfor-
mance of our models on estimating selectional pref-
erence strongly supported these initial hypotheses.
We discussed plausibility judgements from a the-
oretical perspective and argued that frequency esti-
mates and JP are imperfect approximations for plau-
sibility. While models can perform well on some
evaluations by using either frequency estimates or
semantic knowledge, we explained why this does
not apply to the unseen plausibility test. The perfor-
mance on that task demonstrates both the success of
our model and the shortcomings of frequency-based
approaches to human plausibility judgements.
Finally, this paper demonstrated that it is feasi-
ble to learn semantic representations of words while
concurrently learning how they relate to one another.
Future work will explore learning words from
broader classes of semantic relations and the role of
context in greater detail. Also, we will evaluate the
system applied to higher level tasks.
</bodyText>
<footnote confidence="0.993262333333333">
5This could also explain their weaker performance on
pseudo-disambiguation in the previous section, where the neg-
ative examples had zero frequency in the training corpus.
</footnote>
<page confidence="0.998624">
73
</page>
<sectionHeader confidence="0.980732" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997855674698795">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
’10, pages 1183–1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP ’03,
pages 168–175, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Comput.
Linguist., 28:187–206, June.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’05, pages 26–33, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36:723–763.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universit¨at Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’11, pages 1394–1404,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009Annual Conference of the NorthAmeri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 317–325, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, pages 459–484.
Andrew L. Maas and Andrew Y. Ng. 2010. A probabilis-
tic model for semantic word vectors. In Workshop on
Deep Learning and Unsupervised Feature Learning,
NIPS ’10.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL-HLT’08,
pages 236 – 244.
Diarmuid O´ S´eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 435–444, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting on Association for
Computational Linguistics, ACL ’93, pages 183–190,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics on Computational Lin-
guistics, ACL ’99, pages 104–111, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa.
2008. Knowledge discovery of semantic relationships
between words using nonparametric bayesian graph
model. In Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ’08, pages 587–595, New York,
NY, USA. ACM.
</reference>
<page confidence="0.99913">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871137">
<title confidence="0.999984">Learning Semantics and Selectional Preference of Adjective-Noun Pairs</title>
<author confidence="0.99989">Karl Moritz Hermann Chris Dyer</author>
<affiliation confidence="0.999953">Department of Computer Science Language Technologies Institute University of Oxford Carnegie Mellon University</affiliation>
<address confidence="0.998089">Oxford OX1 3QD, UK Pittsburgh, PA, 15213, USA</address>
<email confidence="0.997875">karl.moritz.hermann@cs.ox.ac.ukcdyer@cs.cmu.edu</email>
<author confidence="0.999917">Phil Blunsom Stephen Pulman</author>
<affiliation confidence="0.9999605">Department of Computer Science Department of Computer Science University of Oxford University of Oxford</affiliation>
<address confidence="0.969834">Oxford OX1 3QD, UK Oxford OX1 3QD, UK</address>
<email confidence="0.960352">phil.blunsom@cs.ox.ac.ukstephen.pulman@cs.ox.ac.uk</email>
<abstract confidence="0.995482857142857">We investigate the semantic relationship between a noun and its adjectival modifiers. We introduce a class of probabilistic models that enable us to to simultaneously capture both the semantic similarity of nouns and modifiers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1570" citStr="Baroni and Zamparelli, 2010" startWordPosition="219" endWordPosition="222">e analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective semantics together with their comp</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1183–1193, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in wordnet.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03,</booktitle>
<pages>168--175</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6291" citStr="Ciaramita and Johnson (2003)" startWordPosition="972" endWordPosition="975">ing process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British Natio</context>
<context position="9280" citStr="Ciaramita and Johnson (2003)" startWordPosition="1467" endWordPosition="1470">is the percentage of correct assignments, the x-axis denotes the number of neighbours included in the vote. The results demonstrate that nouns can semantically be represented as members of latent classes, while the superiority of Mod over Mod,,,, supports our hypothesis that context co-occurence is a key feature for learning these classes. 4.2 Pseudo-Disambiguation Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. The task is to select the more probable of two candidate arguments to associate with a given 2The baseline results are from Ciaramita and Johnson (2003). Using the majority baseline on the full test set, we only get .176 and .160 for token and type respectively. predicate. For us, this is to decide which adjective, a1 or a2, is more likely to modify a noun n. We follow the approach by Clark and Weir (2002) to create the test data. To improve the quality of the data, we filtered using bigram counts from the Web1T corpus, setting a lower bound on the probable bigram (a1, n) and chosing a2 from five candidates, picking the lowest count for bigram (a2, n). We report results for all variants of our model in Table 2. As baseline we use unigram coun</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense tagging of unknown nouns in wordnet. In Proceedings of the 2003 conference on Empirical methods in natural language processing, EMNLP ’03, pages 168–175, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<pages>28--187</pages>
<contexts>
<context position="6480" citStr="Clark and Weir, 2002" startWordPosition="1002" endWordPosition="1005">es (100,150) did not significantly alter results. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are margi</context>
<context position="9081" citStr="Clark and Weir (2002)" startWordPosition="1435" endWordPosition="1438">ith Figure 2 showing scores for model Mod across different values for k. Figure 2: Scores of Mod on the supersense task. The upper line denotes token-, the lower type-level scores. The y-axis is the percentage of correct assignments, the x-axis denotes the number of neighbours included in the vote. The results demonstrate that nouns can semantically be represented as members of latent classes, while the superiority of Mod over Mod,,,, supports our hypothesis that context co-occurence is a key feature for learning these classes. 4.2 Pseudo-Disambiguation Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. The task is to select the more probable of two candidate arguments to associate with a given 2The baseline results are from Ciaramita and Johnson (2003). Using the majority baseline on the full test set, we only get .176 and .160 for token and type respectively. predicate. For us, this is to decide which adjective, a1 or a2, is more likely to modify a noun n. We follow the approach by Clark and Weir (2002) to create the test data. To improve the quality of the data, we filtered using bigram counts from the Web1T corpus, setting a lower bound on th</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Comput. Linguist., 28:187–206, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>Supersense tagging of unknown nouns using semantic similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1353" citStr="Curran, 2005" startWordPosition="189" endWordPosition="190">y of nouns and modifiers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional prefer</context>
<context position="7213" citStr="Curran, 2005" startWordPosition="1120" endWordPosition="1121">n classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 Evaluation 4.1 Supersense Tagging Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a model’s ability to cluster words by their semantics. The task of this evaluation is to determine the WORDNET supersenses of a given list of nouns. We report results on the WN1.6 test set as defined by Ciaramita and Johnson (2003), who used 755 randomly selected nouns with a unique supersense from the WORDNET 1.6 Wc αc c |N| k WN Wn αN αn N M n m |N ||M| WM Wm αM αm |D| |N| 71 corpus. As their test set was random, results weren’t exactly replicable. For a fair comparison, we select all suitable nouns from the corpus that also appeared in the training corpus. We report results on ty</context>
</contexts>
<marker>Curran, 2005</marker>
<rawString>James R. Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 26–33, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Ulrike Pad´o</author>
</authors>
<title>A flexible, corpus-driven model of regular and inverse selectional preferences.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<pages>36--723</pages>
<marker>Erk, Pad´o, Pad´o, 2010</marker>
<rawString>Katrin Erk, Sebastian Pad´o, and Ulrike Pad´o. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36:723–763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The statistics of word cooccurrences: word pairs and collocations.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<volume>16</volume>
<pages>70174</pages>
<institution>Universit¨at Stuttgart, Holzgartenstr.</institution>
<location>Stuttgart.</location>
<contexts>
<context position="13778" citStr="Evert, 2005" startWordPosition="2211" endWordPosition="2212">) .471 .457 .409 .383 Mod (MI&amp;quot;2) .430 .408 .362 .347 Modnc (JP) .357 .369 .181 .161 Modnc (MI) .427 .385 .220 .209 Modnc (MI&amp;quot;2) .423 .394 .218 .185 Table 4: Results on the unseen plausibility dataset. The results on the unseen data set (Table 4) prove interesting as well. The inverse no-context model is performing significantly poorer than any of the other models. To understand this result we must investigate the differences between the unseen data set and the seen data set and to the pseudodisambiguation evaluation. The key difference to pseudo-disambiguation is that we measure a human 4See (Evert, 2005) for a discussion of these metrics. plausibility judgement, which — as we have demonstrated — only partially correlates with bigram frequencies. Our models were trained on the BNC, hence they could only learn frequency estimates for the seen data set, but not for the unseen data. Based on our hypothesis about the role of context, we expect Mod and Modi to learn semantic classes based on the distribution of context. Without the access to that context, we argued that Modnc and Modinc would instead learn frequency estimates.5 The hypothesis that nouns generally select for adjectives rather than v</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The statistics of word cooccurrences: word pairs and collocations. Ph.D. thesis, Universit¨at Stuttgart, Holzgartenstr. 16, 70174 Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1394--1404</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1605" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="223" endWordPosition="226">al context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective semantics together with their compositional probabilities. We employ </context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1394–1404, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="5501" citStr="Griffiths and Steyvers, 2004" startWordPosition="847" endWordPosition="850">tributions. Finally, for each adjective class j, we have distributions Tm j over Vm. Next, the contents of the corpus are generated by first drawing the length of the corpus (we do not parametrise this since we never generate from this model). Then, for each i, we generate noun class Ni, adjective class Mi, and the tuple zi as follows: Ni |TN ∼ Multi(TN) Mi |TMNi ∼ Multi(TMNi) ni |TnNi ∼ Multi(TnNi) mi |TmMi∼ Multi(TmMi) ∀k: ck,i |TcNi ∼ Multi(TcNi) 2.2 Parameterization and Inference We use Gibbs sampling to estimate the distributions of N and M, integrating out the multinomial parameters Tx (Griffiths and Steyvers, 2004). The Dirichlet parameters α are drawn independently from a r(1,1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We te</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5704" citStr="Johnson and Goldwater, 2009" startWordPosition="877" endWordPosition="880">nce we never generate from this model). Then, for each i, we generate noun class Ni, adjective class Mi, and the tuple zi as follows: Ni |TN ∼ Multi(TN) Mi |TMNi ∼ Multi(TMNi) ni |TnNi ∼ Multi(TnNi) mi |TmMi∼ Multi(TmMi) ∀k: ck,i |TcNi ∼ Multi(TcNi) 2.2 Parameterization and Inference We use Gibbs sampling to estimate the distributions of N and M, integrating out the multinomial parameters Tx (Griffiths and Steyvers, 2004). The Dirichlet parameters α are drawn independently from a r(1,1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second </context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009Annual Conference of the NorthAmerican Chapter of the Association for Computational Linguistics, NAACL ’09, pages 317–325, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the web to obtain frequencies for unseen bigrams. Computational Linguistics,</title>
<date>2003</date>
<pages>459--484</pages>
<contexts>
<context position="6573" citStr="Keller and Lapata, 2003" startWordPosition="1017" endWordPosition="1020">d on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 Evaluation 4.1 Supersense Tagging Supersense tagging (Ci</context>
<context position="10715" citStr="Keller and Lapata, 2003" startWordPosition="1711" endWordPosition="1714">.750 Modic,,, .722 .730 .747 .752 Table 2: Pseudo-disambiguation: Percentage of correct choices made. L-bound denotes the Web1T lower bound on the (al, n) bigram, size the number of decisions made. While all models decisively beat the baseline, the models using context strongly outperform those that do not. This supports our hypothesis regarding the importance of context in semantic clustering. The similarity between the normal and inverse models implies that the direction of the nounadjective relationship has negligible impact for this evaluation. 4.3 Bigram Plausibility Bigram plausibility (Keller and Lapata, 2003) is a second evaluation for selectional preference. Unlike the frequency-based pseudo-disambiguation task, it evaluates how well a model matches human judgement of the plausibility of adjective-noun pairs. Keller and Lapata (2003) demonstrated a correlation between frequencies and plausibility, but this does not sufficiently explain human judgement. An example taken from their unseen data set illustrates the dissociation between frequency and plausibility: • Frequent, implausible: “educational water” • Infrequent, plausible: “difficult foreigner”3 The plausibility evaluation has two data sets </context>
<context position="12463" citStr="Keller and Lapata (2003)" startWordPosition="1992" endWordPosition="1995">tic plausibility (not probability!) mutual information (MI), which factors out acutal frequencies, is a better metric.4 We report results using JP, MI and MI&amp;quot;2. Seen Unseen r p r p AltaVista .650 — .480 — BNC (Rasp) .543 .622 .135 .102 Pad´o et al. .479 .570 .120 .138 LDA .594 .558 .468 .459 ROOTH-LDA .575 .599 .501 .469 DUAL-LDA .460 .400 .334 .278 Mod (JP) .495 .413 .286 .276 Mod (MI) .394 .425 .471 .457 Mod (MI&amp;quot;2) .575 .501 .430 .408 Modnc (JP) .626 .505 .357 .369 Modnc (MI) .628 .574 .427 .385 Modnc (MI&amp;quot;2) .701 .623 .423 .394 Table 3: Results (Pearson r and Spearman p correlations) on the Keller and Lapata (2003) plausibility data. Bold indicates best scores, underlining our best scores. High values indicate high correlation with the gold standard. Table 3 shows the performance of our models compared to results reported in O´ S´eaghdha (2010). As before, results between the normal and the inverse model (omitted due to space) are very similar. Surprisingly, the no-context models consistently outperform the models using context on the seen data set. This suggests that the seen data set can quite precisely be ranked using frequency estimates, which the no-context models might be better at capturing witho</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, pages 459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Andrew Y Ng</author>
</authors>
<title>A probabilistic model for semantic word vectors.</title>
<date>2010</date>
<booktitle>In Workshop on Deep Learning and Unsupervised Feature Learning, NIPS ’10.</booktitle>
<contexts>
<context position="1338" citStr="Maas and Ng, 2010" startWordPosition="185" endWordPosition="188"> semantic similarity of nouns and modifiers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of sel</context>
</contexts>
<marker>Maas, Ng, 2010</marker>
<rawString>Andrew L. Maas and Andrew Y. Ng. 2010. A probabilistic model for semantic word vectors. In Workshop on Deep Learning and Unsupervised Feature Learning, NIPS ’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In ACL-HLT’08,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="1541" citStr="Mitchell and Lapata, 2008" startWordPosition="215" endWordPosition="218">ships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective seman</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In ACL-HLT’08, pages 236 – 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>435--444</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid O´ S´eaghdha. 2010. Latent variable models of selectional preference. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 435–444, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics, ACL ’93,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6502" citStr="Pereira et al., 1993" startWordPosition="1006" endWordPosition="1009">ignificantly alter results. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10t</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, ACL ’93, pages 183–190, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via EM-based clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6523" citStr="Rooth et al., 1999" startWordPosition="1010" endWordPosition="1013">ults. 3 Experiments As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (Modi), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (Modnc and Modinc). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 Evalua</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 104–111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issei Sato</author>
<author>Minoru Yoshida</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Knowledge discovery of semantic relationships between words using nonparametric bayesian graph model.</title>
<date>2008</date>
<booktitle>In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08,</booktitle>
<pages>587--595</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1319" citStr="Sato et al., 2008" startWordPosition="181" endWordPosition="184">ly capture both the semantic similarity of nouns and modifiers, and adjective-noun selectional preference. Through a combination of novel and existing evaluations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provid</context>
</contexts>
<marker>Sato, Yoshida, Nakagawa, 2008</marker>
<rawString>Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa. 2008. Knowledge discovery of semantic relationships between words using nonparametric bayesian graph model. In Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages 587–595, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>