<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999683">
An Integrated Approach to Robust Processing
of Situated Spoken Dialogue
</title>
<author confidence="0.980569">
Pierre Lison
</author>
<affiliation confidence="0.8413985">
Language Technology Lab,
DFKI GmbH,
</affiliation>
<address confidence="0.59305">
Saarbr¨ucken, Germany
</address>
<email confidence="0.993962">
pierre.lison@dfki.de
</email>
<sectionHeader confidence="0.993802" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963689655173">
Spoken dialogue is notoriously hard to
process with standard NLP technologies.
Natural spoken dialogue is replete with
disfluent, partial, elided or ungrammatical
utterances, all of which are very hard to
accommodate in a dialogue system. Fur-
thermore, speech recognition is known to
be a highly error-prone task, especially for
complex, open-ended discourse domains.
The combination of these two problems
– ill-formed and/or misrecognised speech
inputs – raises a major challenge to the de-
velopment of robust dialogue systems.
We present an integrated approach for ad-
dressing these two issues, based on a in-
cremental parser for Combinatory Cate-
gorial Grammar. The parser takes word
lattices as input and is able to handle ill-
formed and misrecognised utterances by
selectively relaxing its set of grammati-
cal rules. The choice of the most rele-
vant interpretation is then realised via a
discriminative model augmented with con-
textual information. The approach is fully
implemented in a dialogue system for au-
tonomous robots. Evaluation results on a
Wizard of Oz test suite demonstrate very
significant improvements in accuracy and
robustness compared to the baseline.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999750375">
Spoken dialogue is often considered to be one of
the most natural means of interaction between a
human and a robot. It is, however, notoriously
hard to process with standard language process-
ing technologies. Dialogue utterances are often in-
complete or ungrammatical, and may contain nu-
merous disfluencies like fillers (err, uh, mm), rep-
etitions, self-corrections, etc. Rather than getting
</bodyText>
<note confidence="0.759719">
Geert-Jan M. Kruijff
Language Technology Lab,
DFKI GmbH,
Saarbr¨ucken, Germany
</note>
<email confidence="0.946011">
gj@dfki.de
</email>
<bodyText confidence="0.999981585365854">
crisp-and-clear commands such as ”Put the red
ball inside the box!”, it is more likely the robot
will hear such kind of utterance: ”right, now, could
you, uh, put the red ball, yeah, inside the ba/ box!”.
This is natural behaviour in human-human interac-
tion (Fern´andez and Ginzburg, 2002) and can also
be observed in several domain-specific corpora for
human-robot interaction (Topp et al., 2006).
Moreover, even in the (rare) case where the ut-
terance is perfectly well-formed and does not con-
tain any kind of disfluencies, the dialogue sys-
tem still needs to accomodate the various speech
recognition errors thay may arise. This problem
is particularly acute for robots operating in real-
world noisy environments and deal with utterances
pertaining to complex, open-ended domains.
The paper presents a new approach to address
these two difficult issues. Our starting point is the
work done by Zettlemoyer and Collins on parsing
using relaxed CCG grammars (Zettlemoyer and
Collins, 2007) (ZC07). In order to account for
natural spoken language phenomena (more flex-
ible word order, missing words, etc.), they aug-
ment their grammar framework with a small set
of non-standard combinatory rules, leading to a
relaxation of the grammatical constraints. A dis-
criminative model over the parses is coupled with
the parser, and is responsible for selecting the most
likely interpretation(s) among the possible ones.
In this paper, we extend their approach in two
important ways. First, ZC07 focused on the treat-
ment of ill-formed input, and ignored the speech
recognition issues. Our system, to the contrary,
is able to deal with both ill-formed and misrec-
ognized input, in an integrated fashion. This is
done by augmenting the set of non-standard com-
binators with new rules specifically tailored to deal
with speech recognition errors.
Second, the only features used by ZC07 are syn-
tactic features (see 3.4 for details). We signifi-
cantly extend the range of features included in the
</bodyText>
<subsubsectionHeader confidence="0.571512">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 58–65,
</subsubsectionHeader>
<bodyText confidence="0.258201">
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.998625">
58
</page>
<bodyText confidence="0.999929222222222">
discriminative model, by incorporating not only
syntactic, but also acoustic, semantic and contex-
tual information into the model.
An overview of the paper is as follows. We first
describe in Section 2 the cognitive architecture in
which our system has been integrated. We then
discuss the approach in detail in Section 3. Fi-
nally, we present in Section 4 the quantitative eval-
uations on a WOZ test suite, and conclude.
</bodyText>
<sectionHeader confidence="0.959081" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.9998088">
The approach we present in this paper is fully im-
plemented and integrated into a cognitive architec-
ture for autonomous robots. A recent version of
this system is described in (Hawes et al., 2007). It
is capable of building up visuo-spatial models of
a dynamic local scene, continuously plan and exe-
cute manipulation actions on objects within that
scene. The robot can discuss objects and their
material- and spatial properties for the purpose of
visual learning and manipulation tasks.
</bodyText>
<figureCaption confidence="0.904285">
Figure 1: Architecture schema of the communica-
tion subsystem (only for comprehension).
Figure 2 illustrates the architecture schema for
</figureCaption>
<bodyText confidence="0.922574214285714">
the communication subsystem incorporated in the
cognitive architecture (only the comprehension
part is shown).
Starting with ASR, we process the audio signal
to establish a word lattice containing statistically
ranked hypotheses about word sequences. Subse-
quently, parsing constructs grammatical analyses
for the given word lattice. A grammatical analy-
sis constructs both a syntactic analysis of the ut-
terance, and a representation of its meaning. The
analysis is based on an incremental chart parser1
for Combinatory Categorial Grammar (Steedman
and Baldridge, 2009). These meaning represen-
tations are ontologically richly sorted, relational
</bodyText>
<footnote confidence="0.9660815">
1Built on top of the OpenCCG NLP library:
http://openccg.sf.net
</footnote>
<bodyText confidence="0.99988975">
structures, formulated in a (propositional) descrip-
tion logic, more precisely in the HLDS formal-
ism (Baldridge and Kruijff, 2002). The parser
compacts all meaning representations into a sin-
gle packed logical form (Carroll and Oepen, 2005;
Kruijff et al., 2007). A packed LF represents con-
tent similar across the different analyses as a single
graph, using over- and underspecification of how
different nodes can be connected to capture lexical
and syntactic forms of ambiguity.
At the level of dialogue interpretation, a packed
logical form is resolved against a SDRS-like di-
alogue model (Asher and Lascarides, 2003) to
establish contextual co-reference and dialogue
moves.
Linguistic interpretations must finally be associ-
ated with extra-linguistic knowledge about the en-
vironment – dialogue comprehension hence needs
to connect with other subarchitectures like vision,
spatial reasoning or planning. We realise this
information binding between different modalities
via a specific module, called the “binder”, which is
responsible for the ontology-based mediation ac-
cross modalities (Jacobsson et al., 2008).
</bodyText>
<subsectionHeader confidence="0.998631">
2.1 Context-sensitivity
</subsectionHeader>
<bodyText confidence="0.999724615384615">
The combinatorial nature of language provides
virtually unlimited ways in which we can commu-
nicate meaning. This, of course, raises the ques-
tion of how precisely an utterance should then be
understood as it is being heard. Empirical stud-
ies have investigated what information humans use
when comprehending spoken utterances. An im-
portant observation is that interpretation in con-
text plays a crucial role in the comprehension of
utterance as it unfolds (Knoeferle and Crocker,
2006). During utterance comprehension, humans
combine linguistic information with scene under-
standing and “world knowledge”.
</bodyText>
<figureCaption confidence="0.9608965">
Figure 2: Context-sensitivity in processing situ-
ated dialogue understanding
</figureCaption>
<bodyText confidence="0.936913">
Several approaches in situated dialogue for
human-robot interaction have made similar obser-
</bodyText>
<page confidence="0.998036">
59
</page>
<bodyText confidence="0.999656">
vations (Roy, 2005; Roy and Mukherjee, 2005;
Brick and Scheutz, 2007; Kruijff et al., 2007): A
robot’s understanding can be improved by relating
utterances to the situated context. As we will see
in the next section, by incorporating contextual in-
formation into our model, our approach to robust
processing of spoken dialogue seeks to exploit this
important insight.
</bodyText>
<sectionHeader confidence="0.999034" genericHeader="method">
3 Approach
</sectionHeader>
<subsectionHeader confidence="0.999873">
3.1 Grammar relaxation
</subsectionHeader>
<bodyText confidence="0.999951157894737">
Our approach to robust processing of spoken di-
alogue rests on the idea of grammar relaxation:
the grammatical constraints specified in the gram-
mar are “relaxed” to handle slightly ill-formed or
misrecognised utterances.
Practically, the grammar relaxation is done
via the introduction of non-standard CCG rules
(Zettlemoyer and Collins, 2007). In Combinatory
Categorial Grammar, the rules are used to assem-
ble categories to form larger pieces of syntactic
and semantic structure. The standard rules are ap-
plication (&lt;, &gt;), composition (B), and type rais-
ing (T) (Steedman and Baldridge, 2009).
Several types of non-standard rules have been
introduced. We describe here the two most impor-
tant ones: the discourse-level composition rules,
and the ASR correction rules. We invite the reader
to consult (Lison, 2008) for more details on the
complete set of grammar relaxation rules.
</bodyText>
<subsectionHeader confidence="0.861613">
3.1.1 Discourse-level composition rules
</subsectionHeader>
<bodyText confidence="0.99995825">
In natural spoken dialogue, we may encounter ut-
terances containing several independent “chunks”
without any explicit separation (or only a short
pause or a slight change in intonation), such as
</bodyText>
<listItem confidence="0.6074005">
(1) “yes take the ball no the other one on your
left right and now put it in the box.”
</listItem>
<bodyText confidence="0.998646923076923">
Even if retrieving a fully structured parse for
this utterance is difficult to achieve, it would be
useful to have access to a list of smaller “discourse
units”. Syntactically speaking, a discourse unit
can be any type of saturated atomic categories -
from a simple discourse marker to a full sentence.
The type raising rule Tdu allows the conversion
of atomic categories into discourse units:
A : @if ==&gt;- du : @if (Tdu)
where A represents an arbitrary saturated
atomic category (s, np, pp, etc.).
The rule &gt;C is responsible for the integration of
two discourse units into a single structure:
</bodyText>
<equation confidence="0.9374225">
du:@if, du:@jg==&gt;-
du : @{d:d-unitsj(list∧
((FIRST) i ∧ f)∧
((NEXT) j ∧ g)) (&gt;C)
</equation>
<subsectionHeader confidence="0.924102">
3.1.2 ASR error correction rules
</subsectionHeader>
<bodyText confidence="0.9999462">
Speech recognition is a highly error-prone task. It
is however possible to partially alleviate this prob-
lem by inserting new error-correction rules (more
precisely, new lexical entries) for the most fre-
quently misrecognised words.
If we notice e.g. that the ASR system frequently
substitutes the word “wrong” for the word “round”
during the recognition (because of their phonolog-
ical proximity), we can introduce a new lexical en-
try in the lexicon in order to correct this error:
</bodyText>
<equation confidence="0.761249">
round �- adj : @attitude(wrong) (2)
</equation>
<bodyText confidence="0.999743666666667">
A set of thirteen new lexical entries of this type
have been added to our lexicon to account for the
most frequent recognition errors.
</bodyText>
<subsectionHeader confidence="0.999816">
3.2 Parse selection
</subsectionHeader>
<bodyText confidence="0.99985">
Using more powerful grammar rules to relax the
grammatical analysis tends to increase the number
of parses. We hence need a a mechanism to dis-
criminate among the possible parses. The task of
selecting the most likely interpretation among a set
of possible ones is called parse selection. Once all
the possible parses for a given utterance are com-
puted, they are subsequently filtered or selected
in order to retain only the most likely interpreta-
tion(s). This is done via a (discriminative) statisti-
cal model covering a large number of features.
Formally, the task is defined as a function F :
X —* Y where the domain X is the set of possible
inputs (in our case, X is the set of possible word
lattices), and Y the set of parses. We assume:
</bodyText>
<listItem confidence="0.9986385">
1. A function GEN(x) which enumerates all
possible parses for an input x. In our case,
this function simply represents the set of
parses of x which are admissible according
to the CCG grammar.
2. A d-dimensional feature vector f(x, y) E
</listItem>
<bodyText confidence="0.465207">
fid, representing specific features of the pair
(x, y). It can include various acoustic, syn-
tactic, semantic or contextual features which
can be relevant in discriminating the parses.
</bodyText>
<page confidence="0.90811">
60
</page>
<listItem confidence="0.482515">
3. A parameter vector w E ltd.
</listItem>
<bodyText confidence="0.9996615">
The function F, mapping a word lattice to its
most likely parse, is then defined as:
</bodyText>
<equation confidence="0.9978135">
F(x) = argmax wT · f(x, y) (3)
yEGEN(x)
</equation>
<bodyText confidence="0.999985928571429">
where wT · f (x, y) is the inner product
Ed s1 ws fs(x, y), and can be seen as a measure
of the “quality” of the parse. Given the parameters
w, the optimal parse of a given utterance x can be
therefore easily determined by enumerating all the
parses generated by the grammar, extracting their
features, computing the inner product wT ·f(x, y),
and selecting the parse with the highest score.
The task of parse selection is an example of
structured classification problem, which is the
problem of predicting an output y from an input
x, where the output y has a rich internal structure.
In the specific case of parse selection, x is a word
lattice, and y a logical form.
</bodyText>
<subsectionHeader confidence="0.991963">
3.3 Learning
3.3.1 Training data
</subsectionHeader>
<bodyText confidence="0.999992617647059">
In order to estimate the parameters w, we need a
set of training examples. Unfortunately, no corpus
of situated dialogue adapted to our task domain is
available to this day, let alone semantically anno-
tated. The collection of in-domain data via Wizard
of Oz experiments being a very costly and time-
consuming process, we followed the approach ad-
vocated in (Weilhammer et al., 2006) and gener-
ated a corpus from a hand-written task grammar.
To this end, we first collected a small set of
WoZ data, totalling about a thousand utterances.
This set is too small to be directly used as a cor-
pus for statistical training, but sufficient to cap-
ture the most frequent linguistic constructions in
this particular context. Based on it, we designed
a domain-specific CFG grammar covering most of
the utterances. Each rule is associated to a seman-
tic HLDS representation. Weights are automati-
cally assigned to each grammar rule by parsing our
corpus, hence leading to a small stochastic CFG
grammar augmented with semantic information.
Once the grammar is specified, it is randomly
traversed a large number of times, resulting in a
larger set (about 25.000) of utterances along with
their semantic representations. Since we are inter-
ested in handling errors arising from speech recog-
nition, we also need to “simulate” the most fre-
quent recognition errors. To this end, we synthe-
sise each string generated by the domain-specific
CFG grammar, using a text-to-speech engine2,
feed the audio stream to the speech recogniser,
and retrieve the recognition result. Via this tech-
nique, we are able to easily collect a large amount
of training data3.
</bodyText>
<subsectionHeader confidence="0.849612">
3.3.2 Perceptron learning
</subsectionHeader>
<bodyText confidence="0.999782736842105">
The algorithm we use to estimate the parameters
w using the training data is a perceptron. The al-
gorithm is fully online - it visits each example in
turn and updates w if necessary. Albeit simple,
the algorithm has proven to be very efficient and
accurate for the task of parse selection (Collins
and Roark, 2004; Collins, 2004; Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007).
The pseudo-code for the online learning algo-
rithm is detailed in [Algorithm 1].
It works as follows: the parameters w are first
initialised to some arbitrary values. Then, for
each pair (xi, zi) in the training set, the algorithm
searchs for the parse y&apos; with the highest score ac-
cording to the current model. If this parse happens
to match the best parse which generates zi (which
we shall denote y*), we move to the next example.
Else, we perform a simple perceptron update on
the parameters:
</bodyText>
<equation confidence="0.992392">
w = w + f(xi, y*) − f(xi, y&apos;) (4)
</equation>
<bodyText confidence="0.999192818181818">
The iteration on the training set is repeated T
times, or until convergence.
The most expensive step in this algorithm is
the calculation of y&apos; = argmaxyEGEN(xz) wT ·
f(xi, y) - this is the decoding problem.
It is possible to prove that, provided the train-
ing set (xi, zi) is separable with margin 6 &gt; 0, the
algorithm is assured to converge after a finite num-
ber of iterations to a model with zero training er-
rors (Collins and Roark, 2004). See also (Collins,
2004) for convergence theorems and proofs.
</bodyText>
<subsectionHeader confidence="0.606032">
3.4 Features
</subsectionHeader>
<bodyText confidence="0.998135">
As we have seen, the parse selection operates by
enumerating the possible parses and selecting the
</bodyText>
<footnote confidence="0.8277554">
2We used MARY (http://mary.dfki.de) for the
text-to-speech engine.
3Because of its relatively artificial character, the quality
of such training data is naturally lower than what could be
obtained with a genuine corpus. But, as the experimental re-
sults will show, it remains sufficient to train the perceptron
for the parse selection task, and achieve significant improve-
ments in accuracy and robustness. In a near future, we plan
to progressively replace this generated training data by a real
spoken dialogue corpus adapted to our task domain.
</footnote>
<page confidence="0.997617">
61
</page>
<figure confidence="0.664728576923077">
Algorithm 1 Online perceptron learning
Require: - set of n training examples {(xi, zi) : i = 1...n}
- T: number of iterations over the training set
- GEN(x): function enumerating possible parses
for an input x, according to the CCG grammar.
- GEN(x, z): function enumerating possible parses
for an input x and which have semantics z,
according to the CCG grammar.
- L(y) maps a parse tree y to its logical form.
- Initial parameter vector w0
% Initialise
w +— w0
% Loop T times on the training examples
fort = 1...T do
for i = 1...n do
% Compute best parse accordin� to current model
Let y&apos; = argmaxyEGEN(xi) w · f(xi, y)
% If the decoded parse =� expected parse, update the
parameters
if L(y&apos;) =� zi then
% Search the best parse for utterance xi with se-
mantics zi
Let y* = argmaxyEGEN(xi,zi) wT · f(xi, y)
% Update parameter vector w
Set w = w + f(xi, y*) − f(xi, y&apos;)
end if
</figure>
<subsectionHeader confidence="0.388917">
end for
end for
</subsectionHeader>
<bodyText confidence="0.9744544">
return parameter vector w
one with the highest score according to the linear
model parametrised by w.
The accuracy of our method crucially relies on
the selection of “good” features f(x, y) for our
model - that is, features which help discriminat-
ing the parses. They must also be relatively cheap
to compute. In our model, the features are of four
types: semantic features, syntactic features, con-
textual features, and speech recognition features.
</bodyText>
<subsectionHeader confidence="0.866717">
3.4.1 Semantic features
</subsectionHeader>
<bodyText confidence="0.999835666666667">
What are the substructures of a logical form which
may be relevant to discriminate the parses? We de-
fine features on the following information sources:
</bodyText>
<listItem confidence="0.9801931">
1. Nominals: for each possible pair
(prop, sort), we include a feature fi in
f(x, y) counting the number of nominals
with ontological sort sort and proposition
prop in the logical form.
2. Ontological sorts: occurrences of specific
ontological sorts in the logical form.
Figure 3: graphical representation of the HLDS
logical form for “I want you to take the mug”.
3. Dependency relations: following (Clark and
Curran, 2003), we also model the depen-
dency structure of the logical form. Each
dependency relation is defined as a triple
(sorta, sortb, label), where sorta denotes
the sort of the incoming nominal, sortb the
sort of the outgoing nominal, and label is the
relation label.
4. Sequences of dependency relations: number
of occurrences of particular sequences (ie. bi-
gram counts) of dependency relations.
</listItem>
<bodyText confidence="0.999935875">
The features on nominals and ontological sorts
aim at modeling (aspects of) lexical semantics -
e.g. which meanings are the most frequent for a
given word -, whereas the features on relations and
sequence of relations focus on sentential seman-
tics - which dependencies are the most frequent.
These features therefore help us handle lexical and
syntactic ambiguities.
</bodyText>
<subsectionHeader confidence="0.856299">
3.4.2 Syntactic features
</subsectionHeader>
<bodyText confidence="0.999886333333334">
By “syntactic features”, we mean features associ-
ated to the derivational history of a specific parse.
The main use of these features is to penalise to a
correct extent the application of the non-standard
rules introduced into the grammar.
To this end, we include in the feature vector
f(x, y) a new feature for each non-standard rule,
which counts the number of times the rule was ap-
plied in the parse.
</bodyText>
<page confidence="0.991419">
62
</page>
<figure confidence="0.9887155">
cup
pick up corr
particle
&gt;
&gt;
s
</figure>
<figureCaption confidence="0.999865">
Figure 4: CCG derivation of “pick cup the ball”.
</figureCaption>
<bodyText confidence="0.999963066666667">
In the derivation shown in the figure 4, the rule
corr (correction of a speech recognition error) is
applied once, so the corresponding feature value is
set to 1. The feature values for the remaining rules
are set to 0, since they are absent from the parse.
These syntactic features can be seen as a penalty
given to the parses using these non-standard rules,
thereby giving a preference to the “normal” parses
over them. This mechanism ensures that the gram-
mar relaxation is only applied “as a last resort”
when the usual grammatical analysis fails to pro-
vide a full parse. Of course, depending on the
relative frequency of occurrence of these rules in
the training corpus, some of them will be more
strongly penalised than others.
</bodyText>
<subsectionHeader confidence="0.944011">
3.4.3 Contextual features
</subsectionHeader>
<bodyText confidence="0.966219333333333">
As we have already outlined in the background
section, one striking characteristic of spoken dia-
logue is the importance of context. Understanding
the visual and discourse contexts is crucial to re-
solve potential ambiguities and compute the most
likely interpretation(s) of a given utterance.
The feature vector f(x, y) therefore includes
various features related to the context:
1. Activated words: our dialogue system main-
tains in its working memory a list of contex-
tually activated words (cfr. (Lison and Krui-
jff, 2008)). This list is continuously updated
as the dialogue and the environment evolves.
For each context-dependent word, we include
one feature counting the number of times it
appears in the utterance string.
2. Expected dialogue moves: for each possible
dialogue move, we include one feature indi-
cating if the dialogue move is consistent with
the current discourse model. These features
ensure for instance that the dialogue move
following a QuestionYN is a Accept, Re-
ject or another question (e.g. for clarification
requests), but almost never an Opening.
3. Expected syntactic categories: for each
atomic syntactic category in the CCG gram-
mar, we include one feature indicating if the
category is consistent with the current dis-
course model. These features can be used to
handle sentence fragments.
</bodyText>
<subsectionHeader confidence="0.944584">
3.4.4 Speech recognition features
</subsectionHeader>
<bodyText confidence="0.999977111111111">
Finally, the feature vector f(x, y) also includes
features related to the speech recognition. The
ASR module outputs a set of (partial) recognition
hypotheses, packed in a word lattice. One exam-
ple of such a structure is given in Figure 5. Each
recognition hypothesis is provided with an asso-
ciated confidence score, and we want to favour
the hypotheses with high confidence scores, which
are, according to the statistical models incorpo-
rated in the ASR, more likely to reflect what was
uttered.
To this end, we introduce three features: the
acoustic confidence score (confidence score pro-
vided by the statistical models included in the
ASR), the semantic confidence score (based on a
“concept model” also provided by the ASR), and
the ASR ranking (hypothesis rank in the word lat-
tice, from best to worst).
</bodyText>
<figureCaption confidence="0.994171">
Figure 5: Example of word lattice
</figureCaption>
<sectionHeader confidence="0.956808" genericHeader="evaluation">
4 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.999954666666667">
We performed a quantitative evaluation of our ap-
proach, using its implementation in a fully inte-
grated system (cf. Section 2). To set up the ex-
periments for the evaluation, we have gathered a
corpus of human-robot spoken dialogue for our
task-domain, which we segmented and annotated
manually with their expected semantic interpreta-
tion. The data set contains 195 individual utter-
ances along with their complete logical forms.
</bodyText>
<subsectionHeader confidence="0.764774">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.9998645">
Three types of quantitative results are extracted
from the evaluation results: exact-match, partial-
match, and word error rate. Tables 1, 2 and 3 illus-
trate the results, broken down by use of grammar
relaxation, use of parse selection, and number of
recognition hypotheses considered.
</bodyText>
<figure confidence="0.973787857142857">
s/particle/np
s/np
ball
np/n n
&gt;
np
the
</figure>
<page confidence="0.995817">
63
</page>
<table confidence="0.999678875">
Size of word lattice Grammar Parse Precision Recall Fl-value
(number of NBests) relaxation selection
(Baseline) 1 No No 40.9 45.2 43.0
. 1 No Yes 59.0 54.3 56.6
. 1 Yes Yes 52.7 70.8 60.4
. 3 Yes Yes 55.3 82.9 66.3
. 5 Yes Yes 55.6 84.0 66.9
(Full approach) 10 Yes Yes 55.6 84.9 67.2
</table>
<tableCaption confidence="0.996931">
Table 1: Exact-match accuracy results (in percents).
</tableCaption>
<table confidence="0.999904625">
Size of word lattice Grammar Parse Precision Recall Fl-value
(number of NBests) relaxation selection
(Baseline) 1 No No 86.2 56.2 68.0
. 1 No Yes 87.4 56.6 68.7
. 1 Yes Yes 88.1 76.2 81.7
. 3 Yes Yes 87.6 85.2 86.4
. 5 Yes Yes 87.6 86.0 86.8
(Full approach) 10 Yes Yes 87.7 87.0 87.3
</table>
<tableCaption confidence="0.999251">
Table 2: Partial-match accuracy results (in percents).
</tableCaption>
<bodyText confidence="0.995262181818182">
Each line in the tables corresponds to a possible
configuration. Tables 1 and 2 give the precision,
recall and F1 value for each configuration (respec-
tively for the exact- and partial-match), and Table
3 gives the Word Error Rate [WER].
The first line corresponds to the baseline: no
grammar relaxation, no parse selection, and use of
the first NBest recognition hypothesis. The last
line corresponds to the results with the full ap-
proach: grammar relaxation, parse selection, and
use of 10 recognition hypotheses.
</bodyText>
<table confidence="0.999121857142857">
Size of word Grammar Parse WER
lattice (NBests) relaxation selection
1 No No 20.5
1 Yes Yes 19.4
3 Yes Yes 16.5
5 Yes Yes 15.7
10 Yes Yes 15.7
</table>
<tableCaption confidence="0.999478">
Table 3: Word error rate (in percents).
</tableCaption>
<subsectionHeader confidence="0.998467">
4.2 Comparison with baseline
</subsectionHeader>
<bodyText confidence="0.603072533333333">
Here are the comparative results we obtained:
• Regarding the exact-match results between
the baseline and our approach (grammar re-
laxation and parse selection with all fea-
tures activated for NBest 10), the F1-measure
climbs from 43.0 % to 67.2 %, which means
a relative difference of 56.3 %.
• For the partial-match, the F1-measure goes
from 68.0 % for the baseline to 87.3 % for
our approach – a relative increase of 28.4 %.
• We obverse a significant decrease in WER:
we go from 20.5 % for the baseline to 15.7 %
with our approach. The difference is statisti-
cally significant (p-value for t-tests is 0.036),
and the relative decrease of 23.4 %.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999990384615385">
We presented an integrated approach to the pro-
cessing of (situated) spoken dialogue, suited to
the specific needs and challenges encountered in
human-robot interaction.
In order to handle disfluent, partial, ill-formed
or misrecognized utterances, the grammar used by
the parser is “relaxed” via the introduction of a
set of non-standard combinators which allow for
the insertion/deletion of specific words, the com-
bination of discourse fragments or the correction
of speech recognition errors.
The relaxed parser yields a (potentially large)
set of parses, which are then packed and retrieved
by the parse selection module. The parse selec-
tion is based on a discriminative model exploring a
set of relevant semantic, syntactic, contextual and
acoustic features extracted for each parse. The pa-
rameters of this model are estimated against an au-
tomatically generated corpus of (utterance, logical
form) pairs. The learning algorithm is an percep-
tron, a simple albeit efficient technique for param-
eter estimation.
As forthcoming work, we shall examine the po-
tential extension of our approach in new direc-
tions, such as the exploitation of parse selection
for incremental scoring/pruning of the parse chart,
</bodyText>
<page confidence="0.997931">
64
</page>
<bodyText confidence="0.99968">
the introduction of more refined contextual fea-
tures, or the use of more sophisticated learning al-
gorithms, such as Support Vector Machines.
</bodyText>
<sectionHeader confidence="0.989693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756642857143">
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Cambridge University Press.
J. Baldridge and G.-J. M. Kruijff. 2002. Coupling
CCG and hybrid logic dependency semantics. In
ACL’02: Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics,
pages 319–326, Philadelphia, PA. Association for
Computational Linguistics.
T. Brick and M. Scheutz. 2007. Incremental natu-
ral language processing for HRI. In Proceeding of
the ACM/IEEE international conference on Human-
Robot Interaction (HRI’07), pages 263 – 270.
J. Carroll and S. Oepen. 2005. High efficiency re-
alization for a wide-coverage unification grammar.
In Proceedings of the International Joint Confer-
ence on Natural Language Processing (IJCNLP’05),
pages 165–176.
Stephen Clark and James R. Curran. 2003. Log-linear
models for wide-coverage ccg parsing. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 97–104, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In ACL
’04: Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics, page
111, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Michael Collins. 2004. Parameter estimation for
statistical parsing models: theory and practice of
distribution-free methods. In New developments in
parsing technology, pages 19–55. Kluwer Academic
Publishers.
R. Fern´andez and J. Ginzburg. 2002. A corpus study
of non-sentential utterances in dialogue. Traitement
Automatique des Langues, 43(2):12–43.
N. Hawes, A. Sloman, J. Wyatt, M. Zillich, H. Jacob-
sson, G.J. M. Kruijff, M. Brenner, G. Berginc, and
D. Skocaj. 2007. Towards an integrated robot with
multiple cognitive functions. In AAAI, pages 1548–
1553. AAAI Press.
Henrik Jacobsson, Nick Hawes, Geert-Jan Kruijff, and
Jeremy Wyatt. 2008. Crossmodal content bind-
ing in information-processing architectures. In Pro-
ceedings of the 3rd ACM/IEEE International Con-
ference on Human-Robot Interaction (HRI), Amster-
dam, The Netherlands, March 12–15.
P. Knoeferle and M.C. Crocker. 2006. The coordinated
interplay of scene, utterance, and world knowledge:
evidence from eye tracking. Cognitive Science.
G.J.M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson,
and N.A. Hawes. 2007. Incremental, multi-level
processing for comprehending situated dialogue in
human-robot interaction. In Language and Robots:
Proceedings from the Symposium (LangRo’2007),
pages 55–64, Aveiro, Portugal, December.
Pierre Lison and Geert-Jan M. Kruijff. 2008. Salience-
driven contextual priming of speech recognition for
human-robot interaction. In Proceedings of the 18th
European Conference on Artificial Intelligence, Pa-
tras (Greece).
Pierre Lison. 2008. Robust processing of situated spo-
ken dialogue. Master’s thesis, Universit¨at des Saar-
landes, Saarbr¨ucken.
D. Roy and N. Mukherjee. 2005. Towards situated
speech understanding: visual context priming of
language models. Computer Speech &amp; Language,
19(2):227–248, April.
D.K. Roy. 2005. Semiotic schemas: A framework for
grounding language in action and perception. Artifi-
cialIntelligence, 167(1-2):170–205.
Mark Steedman and Jason Baldridge. 2009. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti B¨orjars, editors, Nontransformational Syntax: A
Guide to Current Models. Blackwell, Oxford.
E. A. Topp, H. H¨uttenrauch, H.I. Christensen, and
K. Severinson Eklundh. 2006. Bringing together
human and robotic environment representations –
a pilot study. In Proc. of the IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems
(IROS), Beijing, China, October.
Karl Weilhammer, Matthew N. Stuttle, and Steve
Young. 2006. Bootstrapping language models
for dialogue systems. In Proceedings of INTER-
SPEECH 2006, Pittsburgh, PA.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In UAI ’05, Proceedings of the 21st Con-
ference in Uncertainty in Artificial Intelligence, July
2005, pages 658–666.
Luke Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 678–687.
</reference>
<page confidence="0.999611">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.243558">
<title confidence="0.9775465">An Integrated Approach to Robust of Situated Spoken Dialogue</title>
<author confidence="0.784413">Pierre</author>
<affiliation confidence="0.619617333333333">Language Technology DFKI Saarbr¨ucken,</affiliation>
<email confidence="0.991151">pierre.lison@dfki.de</email>
<abstract confidence="0.9985091">Spoken dialogue is notoriously hard to process with standard NLP technologies. Natural spoken dialogue is replete with disfluent, partial, elided or ungrammatical utterances, all of which are very hard to accommodate in a dialogue system. Furthermore, speech recognition is known to be a highly error-prone task, especially for complex, open-ended discourse domains. The combination of these two problems – ill-formed and/or misrecognised speech inputs – raises a major challenge to the development of robust dialogue systems. We present an integrated approach for addressing these two issues, based on a incremental parser for Combinatory Categorial Grammar. The parser takes word lattices as input and is able to handle illformed and misrecognised utterances by selectively relaxing its set of grammatical rules. The choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information. The approach is fully implemented in a dialogue system for autonomous robots. Evaluation results on a Wizard of Oz test suite demonstrate very significant improvements in accuracy and robustness compared to the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6375" citStr="Asher and Lascarides, 2003" startWordPosition="980" endWordPosition="983">ccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002). The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007). A packed LF represents content similar across the different analyses as a single graph, using over- and underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity. At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves. Linguistic interpretations must finally be associated with extra-linguistic knowledge about the environment – dialogue comprehension hence needs to connect with other subarchitectures like vision, spatial reasoning or planning. We realise this information binding between different modalities via a specific module, called the “binder”, which is responsible for the ontology-based mediation accross modalities (Jacobsson et al., 2008). 2.1 Context-sensitivity The combinatorial nature of language provides virtually unlimited ways in which we</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>G-J M Kruijff</author>
</authors>
<title>Coupling CCG and hybrid logic dependency semantics.</title>
<date>2002</date>
<booktitle>In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>319--326</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5888" citStr="Baldridge and Kruijff, 2002" startWordPosition="903" endWordPosition="906">ly ranked hypotheses about word sequences. Subsequently, parsing constructs grammatical analyses for the given word lattice. A grammatical analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning. The analysis is based on an incremental chart parser1 for Combinatory Categorial Grammar (Steedman and Baldridge, 2009). These meaning representations are ontologically richly sorted, relational 1Built on top of the OpenCCG NLP library: http://openccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002). The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007). A packed LF represents content similar across the different analyses as a single graph, using over- and underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity. At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves. Linguistic interpretations must finally be associated w</context>
</contexts>
<marker>Baldridge, Kruijff, 2002</marker>
<rawString>J. Baldridge and G.-J. M. Kruijff. 2002. Coupling CCG and hybrid logic dependency semantics. In ACL’02: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 319–326, Philadelphia, PA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brick</author>
<author>M Scheutz</author>
</authors>
<title>Incremental natural language processing for HRI.</title>
<date>2007</date>
<booktitle>In Proceeding of the ACM/IEEE international conference on HumanRobot Interaction (HRI’07),</booktitle>
<pages>263--270</pages>
<contexts>
<context position="7733" citStr="Brick and Scheutz, 2007" startWordPosition="1171" endWordPosition="1174">eard. Empirical studies have investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understanding and “world knowledge”. Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robot’s understanding can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterances. Practically, the grammar relaxation is done via</context>
</contexts>
<marker>Brick, Scheutz, 2007</marker>
<rawString>T. Brick and M. Scheutz. 2007. Incremental natural language processing for HRI. In Proceeding of the ACM/IEEE international conference on HumanRobot Interaction (HRI’07), pages 263 – 270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>S Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP’05),</booktitle>
<pages>165--176</pages>
<contexts>
<context position="5996" citStr="Carroll and Oepen, 2005" startWordPosition="920" endWordPosition="923">rd lattice. A grammatical analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning. The analysis is based on an incremental chart parser1 for Combinatory Categorial Grammar (Steedman and Baldridge, 2009). These meaning representations are ontologically richly sorted, relational 1Built on top of the OpenCCG NLP library: http://openccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002). The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007). A packed LF represents content similar across the different analyses as a single graph, using over- and underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity. At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves. Linguistic interpretations must finally be associated with extra-linguistic knowledge about the environment – dialogue comprehension hence needs to connect with ot</context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>J. Carroll and S. Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proceedings of the International Joint Conference on Natural Language Processing (IJCNLP’05), pages 165–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage ccg parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 conference on Empirical methods in natural language processing,</booktitle>
<pages>97--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18348" citStr="Clark and Curran, 2003" startWordPosition="2980" endWordPosition="2983"> recognition features. 3.4.1 Semantic features What are the substructures of a logical form which may be relevant to discriminate the parses? We define features on the following information sources: 1. Nominals: for each possible pair (prop, sort), we include a feature fi in f(x, y) counting the number of nominals with ontological sort sort and proposition prop in the logical form. 2. Ontological sorts: occurrences of specific ontological sorts in the logical form. Figure 3: graphical representation of the HLDS logical form for “I want you to take the mug”. 3. Dependency relations: following (Clark and Curran, 2003), we also model the dependency structure of the logical form. Each dependency relation is defined as a triple (sorta, sortb, label), where sorta denotes the sort of the incoming nominal, sortb the sort of the outgoing nominal, and label is the relation label. 4. Sequences of dependency relations: number of occurrences of particular sequences (ie. bigram counts) of dependency relations. The features on nominals and ontological sorts aim at modeling (aspects of) lexical semantics - e.g. which meanings are the most frequent for a given word -, whereas the features on relations and sequence of rel</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Stephen Clark and James R. Curran. 2003. Log-linear models for wide-coverage ccg parsing. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 97–104, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="14634" citStr="Collins and Roark, 2004" startWordPosition="2334" endWordPosition="2337">o this end, we synthesise each string generated by the domain-specific CFG grammar, using a text-to-speech engine2, feed the audio stream to the speech recogniser, and retrieve the recognition result. Via this technique, we are able to easily collect a large amount of training data3. 3.3.2 Perceptron learning The algorithm we use to estimate the parameters w using the training data is a perceptron. The algorithm is fully online - it visits each example in turn and updates w if necessary. Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). The pseudo-code for the online learning algorithm is detailed in [Algorithm 1]. It works as follows: the parameters w are first initialised to some arbitrary values. Then, for each pair (xi, zi) in the training set, the algorithm searchs for the parse y&apos; with the highest score according to the current model. If this parse happens to match the best parse which generates zi (which we shall denote y*), we move to the next example. Else, we perform a simple perceptron update on the parameters: w = w + f(xi, y*) − f(xi,</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>Michael Collins and Brian Roark. 2004. Incremental parsing with the perceptron algorithm. In ACL ’04: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, page 111, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Parameter estimation for statistical parsing models: theory and practice of distribution-free methods.</title>
<date>2004</date>
<booktitle>In New developments in parsing technology,</booktitle>
<pages>pages</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="14649" citStr="Collins, 2004" startWordPosition="2338" endWordPosition="2339"> each string generated by the domain-specific CFG grammar, using a text-to-speech engine2, feed the audio stream to the speech recogniser, and retrieve the recognition result. Via this technique, we are able to easily collect a large amount of training data3. 3.3.2 Perceptron learning The algorithm we use to estimate the parameters w using the training data is a perceptron. The algorithm is fully online - it visits each example in turn and updates w if necessary. Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). The pseudo-code for the online learning algorithm is detailed in [Algorithm 1]. It works as follows: the parameters w are first initialised to some arbitrary values. Then, for each pair (xi, zi) in the training set, the algorithm searchs for the parse y&apos; with the highest score according to the current model. If this parse happens to match the best parse which generates zi (which we shall denote y*), we move to the next example. Else, we perform a simple perceptron update on the parameters: w = w + f(xi, y*) − f(xi, y&apos;) (4) The it</context>
</contexts>
<marker>Collins, 2004</marker>
<rawString>Michael Collins. 2004. Parameter estimation for statistical parsing models: theory and practice of distribution-free methods. In New developments in parsing technology, pages 19–55. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fern´andez</author>
<author>J Ginzburg</author>
</authors>
<title>A corpus study of non-sentential utterances in dialogue.</title>
<date>2002</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<pages>43--2</pages>
<marker>Fern´andez, Ginzburg, 2002</marker>
<rawString>R. Fern´andez and J. Ginzburg. 2002. A corpus study of non-sentential utterances in dialogue. Traitement Automatique des Langues, 43(2):12–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hawes</author>
<author>A Sloman</author>
<author>J Wyatt</author>
<author>M Zillich</author>
<author>H Jacobsson</author>
<author>G J M Kruijff</author>
<author>M Brenner</author>
<author>G Berginc</author>
<author>D Skocaj</author>
</authors>
<title>Towards an integrated robot with multiple cognitive functions.</title>
<date>2007</date>
<booktitle>In AAAI,</booktitle>
<pages>1548--1553</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4625" citStr="Hawes et al., 2007" startWordPosition="721" endWordPosition="724">tive model, by incorporating not only syntactic, but also acoustic, semantic and contextual information into the model. An overview of the paper is as follows. We first describe in Section 2 the cognitive architecture in which our system has been integrated. We then discuss the approach in detail in Section 3. Finally, we present in Section 4 the quantitative evaluations on a WOZ test suite, and conclude. 2 Architecture The approach we present in this paper is fully implemented and integrated into a cognitive architecture for autonomous robots. A recent version of this system is described in (Hawes et al., 2007). It is capable of building up visuo-spatial models of a dynamic local scene, continuously plan and execute manipulation actions on objects within that scene. The robot can discuss objects and their material- and spatial properties for the purpose of visual learning and manipulation tasks. Figure 1: Architecture schema of the communication subsystem (only for comprehension). Figure 2 illustrates the architecture schema for the communication subsystem incorporated in the cognitive architecture (only the comprehension part is shown). Starting with ASR, we process the audio signal to establish a </context>
</contexts>
<marker>Hawes, Sloman, Wyatt, Zillich, Jacobsson, Kruijff, Brenner, Berginc, Skocaj, 2007</marker>
<rawString>N. Hawes, A. Sloman, J. Wyatt, M. Zillich, H. Jacobsson, G.J. M. Kruijff, M. Brenner, G. Berginc, and D. Skocaj. 2007. Towards an integrated robot with multiple cognitive functions. In AAAI, pages 1548– 1553. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henrik Jacobsson</author>
<author>Nick Hawes</author>
<author>Geert-Jan Kruijff</author>
<author>Jeremy Wyatt</author>
</authors>
<title>Crossmodal content binding in information-processing architectures.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI),</booktitle>
<location>Amsterdam, The Netherlands,</location>
<contexts>
<context position="6867" citStr="Jacobsson et al., 2008" startWordPosition="1046" endWordPosition="1049">level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves. Linguistic interpretations must finally be associated with extra-linguistic knowledge about the environment – dialogue comprehension hence needs to connect with other subarchitectures like vision, spatial reasoning or planning. We realise this information binding between different modalities via a specific module, called the “binder”, which is responsible for the ontology-based mediation accross modalities (Jacobsson et al., 2008). 2.1 Context-sensitivity The combinatorial nature of language provides virtually unlimited ways in which we can communicate meaning. This, of course, raises the question of how precisely an utterance should then be understood as it is being heard. Empirical studies have investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understan</context>
</contexts>
<marker>Jacobsson, Hawes, Kruijff, Wyatt, 2008</marker>
<rawString>Henrik Jacobsson, Nick Hawes, Geert-Jan Kruijff, and Jeremy Wyatt. 2008. Crossmodal content binding in information-processing architectures. In Proceedings of the 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI), Amsterdam, The Netherlands, March 12–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Knoeferle</author>
<author>M C Crocker</author>
</authors>
<title>The coordinated interplay of scene, utterance, and world knowledge: evidence from eye tracking. Cognitive Science.</title>
<date>2006</date>
<contexts>
<context position="7375" citStr="Knoeferle and Crocker, 2006" startWordPosition="1124" endWordPosition="1127">, called the “binder”, which is responsible for the ontology-based mediation accross modalities (Jacobsson et al., 2008). 2.1 Context-sensitivity The combinatorial nature of language provides virtually unlimited ways in which we can communicate meaning. This, of course, raises the question of how precisely an utterance should then be understood as it is being heard. Empirical studies have investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understanding and “world knowledge”. Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robot’s understanding can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of </context>
</contexts>
<marker>Knoeferle, Crocker, 2006</marker>
<rawString>P. Knoeferle and M.C. Crocker. 2006. The coordinated interplay of scene, utterance, and world knowledge: evidence from eye tracking. Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M Kruijff</author>
<author>P Lison</author>
<author>T Benjamin</author>
<author>H Jacobsson</author>
<author>N A Hawes</author>
</authors>
<title>Incremental, multi-level processing for comprehending situated dialogue in human-robot interaction.</title>
<date>2007</date>
<booktitle>In Language and Robots: Proceedings from the Symposium (LangRo’2007),</booktitle>
<pages>55--64</pages>
<location>Aveiro, Portugal,</location>
<contexts>
<context position="6019" citStr="Kruijff et al., 2007" startWordPosition="924" endWordPosition="927"> analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning. The analysis is based on an incremental chart parser1 for Combinatory Categorial Grammar (Steedman and Baldridge, 2009). These meaning representations are ontologically richly sorted, relational 1Built on top of the OpenCCG NLP library: http://openccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002). The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007). A packed LF represents content similar across the different analyses as a single graph, using over- and underspecification of how different nodes can be connected to capture lexical and syntactic forms of ambiguity. At the level of dialogue interpretation, a packed logical form is resolved against a SDRS-like dialogue model (Asher and Lascarides, 2003) to establish contextual co-reference and dialogue moves. Linguistic interpretations must finally be associated with extra-linguistic knowledge about the environment – dialogue comprehension hence needs to connect with other subarchitectures li</context>
<context position="7756" citStr="Kruijff et al., 2007" startWordPosition="1175" endWordPosition="1178">ave investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understanding and “world knowledge”. Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robot’s understanding can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterances. Practically, the grammar relaxation is done via the introduction of no</context>
</contexts>
<marker>Kruijff, Lison, Benjamin, Jacobsson, Hawes, 2007</marker>
<rawString>G.J.M. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, and N.A. Hawes. 2007. Incremental, multi-level processing for comprehending situated dialogue in human-robot interaction. In Language and Robots: Proceedings from the Symposium (LangRo’2007), pages 55–64, Aveiro, Portugal, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Lison</author>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>Saliencedriven contextual priming of speech recognition for human-robot interaction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence,</booktitle>
<location>Patras</location>
<contexts>
<context position="20896" citStr="Lison and Kruijff, 2008" startWordPosition="3403" endWordPosition="3407">e training corpus, some of them will be more strongly penalised than others. 3.4.3 Contextual features As we have already outlined in the background section, one striking characteristic of spoken dialogue is the importance of context. Understanding the visual and discourse contexts is crucial to resolve potential ambiguities and compute the most likely interpretation(s) of a given utterance. The feature vector f(x, y) therefore includes various features related to the context: 1. Activated words: our dialogue system maintains in its working memory a list of contextually activated words (cfr. (Lison and Kruijff, 2008)). This list is continuously updated as the dialogue and the environment evolves. For each context-dependent word, we include one feature counting the number of times it appears in the utterance string. 2. Expected dialogue moves: for each possible dialogue move, we include one feature indicating if the dialogue move is consistent with the current discourse model. These features ensure for instance that the dialogue move following a QuestionYN is a Accept, Reject or another question (e.g. for clarification requests), but almost never an Opening. 3. Expected syntactic categories: for each atomi</context>
</contexts>
<marker>Lison, Kruijff, 2008</marker>
<rawString>Pierre Lison and Geert-Jan M. Kruijff. 2008. Saliencedriven contextual priming of speech recognition for human-robot interaction. In Proceedings of the 18th European Conference on Artificial Intelligence, Patras (Greece).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Lison</author>
</authors>
<title>Robust processing of situated spoken dialogue.</title>
<date>2008</date>
<booktitle>Master’s thesis, Universit¨at des Saarlandes, Saarbr¨ucken.</booktitle>
<contexts>
<context position="8877" citStr="Lison, 2008" startWordPosition="1349" endWordPosition="1350">cognised utterances. Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). In Combinatory Categorial Grammar, the rules are used to assemble categories to form larger pieces of syntactic and semantic structure. The standard rules are application (&lt;, &gt;), composition (B), and type raising (T) (Steedman and Baldridge, 2009). Several types of non-standard rules have been introduced. We describe here the two most important ones: the discourse-level composition rules, and the ASR correction rules. We invite the reader to consult (Lison, 2008) for more details on the complete set of grammar relaxation rules. 3.1.1 Discourse-level composition rules In natural spoken dialogue, we may encounter utterances containing several independent “chunks” without any explicit separation (or only a short pause or a slight change in intonation), such as (1) “yes take the ball no the other one on your left right and now put it in the box.” Even if retrieving a fully structured parse for this utterance is difficult to achieve, it would be useful to have access to a list of smaller “discourse units”. Syntactically speaking, a discourse unit can be an</context>
</contexts>
<marker>Lison, 2008</marker>
<rawString>Pierre Lison. 2008. Robust processing of situated spoken dialogue. Master’s thesis, Universit¨at des Saarlandes, Saarbr¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roy</author>
<author>N Mukherjee</author>
</authors>
<title>Towards situated speech understanding: visual context priming of language models.</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7708" citStr="Roy and Mukherjee, 2005" startWordPosition="1167" endWordPosition="1170">derstood as it is being heard. Empirical studies have investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understanding and “world knowledge”. Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robot’s understanding can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterances. Practically, the gramm</context>
</contexts>
<marker>Roy, Mukherjee, 2005</marker>
<rawString>D. Roy and N. Mukherjee. 2005. Towards situated speech understanding: visual context priming of language models. Computer Speech &amp; Language, 19(2):227–248, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Roy</author>
</authors>
<title>Semiotic schemas: A framework for grounding language in action and perception.</title>
<date>2005</date>
<journal>ArtificialIntelligence,</journal>
<pages>167--1</pages>
<contexts>
<context position="7683" citStr="Roy, 2005" startWordPosition="1165" endWordPosition="1166"> then be understood as it is being heard. Empirical studies have investigated what information humans use when comprehending spoken utterances. An important observation is that interpretation in context plays a crucial role in the comprehension of utterance as it unfolds (Knoeferle and Crocker, 2006). During utterance comprehension, humans combine linguistic information with scene understanding and “world knowledge”. Figure 2: Context-sensitivity in processing situated dialogue understanding Several approaches in situated dialogue for human-robot interaction have made similar obser59 vations (Roy, 2005; Roy and Mukherjee, 2005; Brick and Scheutz, 2007; Kruijff et al., 2007): A robot’s understanding can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterance</context>
</contexts>
<marker>Roy, 2005</marker>
<rawString>D.K. Roy. 2005. Semiotic schemas: A framework for grounding language in action and perception. ArtificialIntelligence, 167(1-2):170–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Jason Baldridge</author>
</authors>
<title>Combinatory categorial grammar.</title>
<date>2009</date>
<booktitle>In Robert Borsley and Kersti B¨orjars, editors, Nontransformational Syntax: A Guide to Current Models.</booktitle>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="5619" citStr="Steedman and Baldridge, 2009" startWordPosition="866" endWordPosition="869">prehension). Figure 2 illustrates the architecture schema for the communication subsystem incorporated in the cognitive architecture (only the comprehension part is shown). Starting with ASR, we process the audio signal to establish a word lattice containing statistically ranked hypotheses about word sequences. Subsequently, parsing constructs grammatical analyses for the given word lattice. A grammatical analysis constructs both a syntactic analysis of the utterance, and a representation of its meaning. The analysis is based on an incremental chart parser1 for Combinatory Categorial Grammar (Steedman and Baldridge, 2009). These meaning representations are ontologically richly sorted, relational 1Built on top of the OpenCCG NLP library: http://openccg.sf.net structures, formulated in a (propositional) description logic, more precisely in the HLDS formalism (Baldridge and Kruijff, 2002). The parser compacts all meaning representations into a single packed logical form (Carroll and Oepen, 2005; Kruijff et al., 2007). A packed LF represents content similar across the different analyses as a single graph, using over- and underspecification of how different nodes can be connected to capture lexical and syntactic fo</context>
<context position="8657" citStr="Steedman and Baldridge, 2009" startWordPosition="1313" endWordPosition="1316">ht. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterances. Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). In Combinatory Categorial Grammar, the rules are used to assemble categories to form larger pieces of syntactic and semantic structure. The standard rules are application (&lt;, &gt;), composition (B), and type raising (T) (Steedman and Baldridge, 2009). Several types of non-standard rules have been introduced. We describe here the two most important ones: the discourse-level composition rules, and the ASR correction rules. We invite the reader to consult (Lison, 2008) for more details on the complete set of grammar relaxation rules. 3.1.1 Discourse-level composition rules In natural spoken dialogue, we may encounter utterances containing several independent “chunks” without any explicit separation (or only a short pause or a slight change in intonation), such as (1) “yes take the ball no the other one on your left right and now put it in th</context>
</contexts>
<marker>Steedman, Baldridge, 2009</marker>
<rawString>Mark Steedman and Jason Baldridge. 2009. Combinatory categorial grammar. In Robert Borsley and Kersti B¨orjars, editors, Nontransformational Syntax: A Guide to Current Models. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Topp</author>
<author>H H¨uttenrauch</author>
<author>H I Christensen</author>
<author>K Severinson Eklundh</author>
</authors>
<title>Bringing together human and robotic environment representations – a pilot study.</title>
<date>2006</date>
<booktitle>In Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),</booktitle>
<location>Beijing, China,</location>
<marker>Topp, H¨uttenrauch, Christensen, Eklundh, 2006</marker>
<rawString>E. A. Topp, H. H¨uttenrauch, H.I. Christensen, and K. Severinson Eklundh. 2006. Bringing together human and robotic environment representations – a pilot study. In Proc. of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Beijing, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Weilhammer</author>
<author>Matthew N Stuttle</author>
<author>Steve Young</author>
</authors>
<title>Bootstrapping language models for dialogue systems.</title>
<date>2006</date>
<booktitle>In Proceedings of INTERSPEECH 2006,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="13048" citStr="Weilhammer et al., 2006" startWordPosition="2070" endWordPosition="2073">ion problem, which is the problem of predicting an output y from an input x, where the output y has a rich internal structure. In the specific case of parse selection, x is a word lattice, and y a logical form. 3.3 Learning 3.3.1 Training data In order to estimate the parameters w, we need a set of training examples. Unfortunately, no corpus of situated dialogue adapted to our task domain is available to this day, let alone semantically annotated. The collection of in-domain data via Wizard of Oz experiments being a very costly and timeconsuming process, we followed the approach advocated in (Weilhammer et al., 2006) and generated a corpus from a hand-written task grammar. To this end, we first collected a small set of WoZ data, totalling about a thousand utterances. This set is too small to be directly used as a corpus for statistical training, but sufficient to capture the most frequent linguistic constructions in this particular context. Based on it, we designed a domain-specific CFG grammar covering most of the utterances. Each rule is associated to a semantic HLDS representation. Weights are automatically assigned to each grammar rule by parsing our corpus, hence leading to a small stochastic CFG gra</context>
</contexts>
<marker>Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Karl Weilhammer, Matthew N. Stuttle, and Steve Young. 2006. Bootstrapping language models for dialogue systems. In Proceedings of INTERSPEECH 2006, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<contexts>
<context position="14680" citStr="Zettlemoyer and Collins, 2005" startWordPosition="2340" endWordPosition="2343">nerated by the domain-specific CFG grammar, using a text-to-speech engine2, feed the audio stream to the speech recogniser, and retrieve the recognition result. Via this technique, we are able to easily collect a large amount of training data3. 3.3.2 Perceptron learning The algorithm we use to estimate the parameters w using the training data is a perceptron. The algorithm is fully online - it visits each example in turn and updates w if necessary. Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). The pseudo-code for the online learning algorithm is detailed in [Algorithm 1]. It works as follows: the parameters w are first initialised to some arbitrary values. Then, for each pair (xi, zi) in the training set, the algorithm searchs for the parse y&apos; with the highest score according to the current model. If this parse happens to match the best parse which generates zi (which we shall denote y*), we move to the next example. Else, we perform a simple perceptron update on the parameters: w = w + f(xi, y*) − f(xi, y&apos;) (4) The iteration on the training set is </context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, July 2005, pages 658–666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>678--687</pages>
<contexts>
<context position="2819" citStr="Zettlemoyer and Collins, 2007" startWordPosition="428" endWordPosition="431">man-robot interaction (Topp et al., 2006). Moreover, even in the (rare) case where the utterance is perfectly well-formed and does not contain any kind of disfluencies, the dialogue system still needs to accomodate the various speech recognition errors thay may arise. This problem is particularly acute for robots operating in realworld noisy environments and deal with utterances pertaining to complex, open-ended domains. The paper presents a new approach to address these two difficult issues. Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). In order to account for natural spoken language phenomena (more flexible word order, missing words, etc.), they augment their grammar framework with a small set of non-standard combinatory rules, leading to a relaxation of the grammatical constraints. A discriminative model over the parses is coupled with the parser, and is responsible for selecting the most likely interpretation(s) among the possible ones. In this paper, we extend their approach in two important ways. First, ZC07 focused on the treatment of ill-formed input, and ignored the speech recognition issues. Our system, to t</context>
<context position="8408" citStr="Zettlemoyer and Collins, 2007" startWordPosition="1273" endWordPosition="1276">ing can be improved by relating utterances to the situated context. As we will see in the next section, by incorporating contextual information into our model, our approach to robust processing of spoken dialogue seeks to exploit this important insight. 3 Approach 3.1 Grammar relaxation Our approach to robust processing of spoken dialogue rests on the idea of grammar relaxation: the grammatical constraints specified in the grammar are “relaxed” to handle slightly ill-formed or misrecognised utterances. Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). In Combinatory Categorial Grammar, the rules are used to assemble categories to form larger pieces of syntactic and semantic structure. The standard rules are application (&lt;, &gt;), composition (B), and type raising (T) (Steedman and Baldridge, 2009). Several types of non-standard rules have been introduced. We describe here the two most important ones: the discourse-level composition rules, and the ASR correction rules. We invite the reader to consult (Lison, 2008) for more details on the complete set of grammar relaxation rules. 3.1.1 Discourse-level composition rules In natural spoken dialog</context>
<context position="14712" citStr="Zettlemoyer and Collins, 2007" startWordPosition="2344" endWordPosition="2347">CFG grammar, using a text-to-speech engine2, feed the audio stream to the speech recogniser, and retrieve the recognition result. Via this technique, we are able to easily collect a large amount of training data3. 3.3.2 Perceptron learning The algorithm we use to estimate the parameters w using the training data is a perceptron. The algorithm is fully online - it visits each example in turn and updates w if necessary. Albeit simple, the algorithm has proven to be very efficient and accurate for the task of parse selection (Collins and Roark, 2004; Collins, 2004; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). The pseudo-code for the online learning algorithm is detailed in [Algorithm 1]. It works as follows: the parameters w are first initialised to some arbitrary values. Then, for each pair (xi, zi) in the training set, the algorithm searchs for the parse y&apos; with the highest score according to the current model. If this parse happens to match the best parse which generates zi (which we shall denote y*), we move to the next example. Else, we perform a simple perceptron update on the parameters: w = w + f(xi, y*) − f(xi, y&apos;) (4) The iteration on the training set is repeated T times, or until conve</context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 678–687.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>