<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014865">
<title confidence="0.993708">
UNIBA-CORE: Combining Strategies for Semantic Textual Similarity
</title>
<author confidence="0.992315">
Annalina Caputo Pierpaolo Basile Giovanni Semeraro
</author>
<affiliation confidence="0.996998">
Department of Computer Science
University of Bari Aldo Moro
</affiliation>
<address confidence="0.894077">
Via E. Orabona, 4 - 70125 Bari, Italy
</address>
<email confidence="0.997972">
{annalina.caputo, pierpaolo.basile, giovanni.semeraro}@uniba.it
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988985">
This paper describes the UNIBA participation
in the Semantic Textual Similarity (STS) core
task 2013. We exploited three different sys-
tems for computing the similarity between two
texts. A system is used as baseline, which rep-
resents the best model emerged from our pre-
vious participation in STS 2012. Such system
is based on a distributional model of seman-
tics capable of taking into account also syn-
tactic structures that glue words together. In
addition, we investigated the use of two dif-
ferent learning strategies exploiting both syn-
tactic and semantic features. The former uses
ensemble learning in order to combine the
best machine learning techniques trained on
2012 training and test sets. The latter tries to
overcome the limit of working with different
datasets with varying characteristics by select-
ing only the more suitable dataset for the train-
ing purpose.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849564102564">
Semantic Textual Similarity is the task of comput-
ing the similarity between any two given texts. The
task, in its core formulation, aims at capturing the
different kinds of similarity that emerge from texts.
Machine translation, paraphrasing, synonym substi-
tution or text entailment are some fruitful methods
exploited for this purpose. These techniques, along
with other methods for estimating the text similar-
ity, were successfully employed via machine learn-
ing approaches during the 2012 task.
However, the STS 2013 core task (Agirre et al.,
2013) differs from the 2012 formulation in that it
provides a test set which is similar to the training,
but not drawn from the same set of data. Hence,
in order to generalize the machine learning models
trained on a group of datasets, we investigate the use
of combination strategies. The objective of combi-
nation strategies, known under the name of ensem-
ble learning, is that of reducing the bias-variance
decomposition through reducing the variance error.
Hence, this class of methods should be more ro-
bust with respect to previously unseen data. Among
the several ensemble learning alternatives, we ex-
ploit the stacked generalization (STACKING) algo-
rithm (Wolpert, 1992). Moreover, we investigate the
use of a two-steps learning algorithm (2STEPSML).
In this method the learning algorithm is trained us-
ing only the dataset most similar to the instance to
be predicted. The first step aims at predicting the
dataset more similar to the given pair of texts. Then
the second step makes use of the previously trained
algorithm to predict the similarity value. The base-
line for the evaluation is represented by our best sys-
tem (DSM PERM) resulting from our participation
in the 2012 task. After introducing the general mod-
els behind our systems in Section 2, Section 3 de-
scribes the evaluation setting of our systems along
with the experimental results. Then, some conclu-
sions and remarks close the paper.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="method">
2 General Models
</sectionHeader>
<subsectionHeader confidence="0.9267115">
2.1 Dependency Encoding via Vector
Permutations
</subsectionHeader>
<bodyText confidence="0.99868">
Distributional models are effective methods for rep-
resenting word paradigmatic relations in a simple
</bodyText>
<page confidence="0.976788">
169
</page>
<subsubsectionHeader confidence="0.252213">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.982876757575758">
and the Shared Task, pages 169–175, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
way through vector spaces (Mitchell and Lapata,
2008). These spaces are built taking into account
the word context, hence the resulting vector repre-
sentation is such that the distance between vectors
reflects their similarity. Although several definitions
of context are possible (e.g. a sliding window of
text, the word order or syntactic dependencies), in
their plain definition these kinds of models account
for just one type of context at a time. To overcome
this limitation, we exploit a method to encode more
definitions of context in the same vector exploiting
the vector permutations (Caputo et al., 2012). This
technique, which is based on Random Indexing as
a means for computing the distributional model, is
based on the idea that when the components of a
highly sparse vector are shuffled, the resulting vec-
tor is nearly orthogonal to the original one. Hence,
vector permutation represents a way for generat-
ing new random vectors in a predetermined manner.
Different word contexts can be encoded using dif-
ferent types of permutations. In our distributional
model system (DSM PERM), we encode the syn-
tactic dependencies between words rather than the
mere co-occurrence information. In this way, word-
vector components bear the information about both
co-occurring and syntactically related words. In this
distributional space, a text can be easily represented
as the superposition of its words. Then, the vec-
tor representation of a text is given by adding the
vector representation of its words, and the similarity
between texts come through the cosine of the angle
between their vector representations.
</bodyText>
<subsectionHeader confidence="0.99942">
2.2 Stacking
</subsectionHeader>
<bodyText confidence="0.972787529411765">
Stacking algorithms (Wolpert, 1992) are a way of
combining different types of learning algorithms re-
ducing the variance of the system. In this model,
the meta-learner tries to predict the real value of
an instance combining the outputs of other machine
learning methods.
Figure 1 shows how the learning process takes
place. The level-0 represents the ensemble of dif-
ferent models to be trained on the same dataset. The
level-0 outputs build up the level-1 dataset: an in-
stance at this level is represented by the numeric
values predicted by each level-0 model along with
the gold standard value. Then, the objective of the
level-1 learning model is to learn how to combine
the level-0 outputs in order to provide the best pre-
diction.
prediction
</bodyText>
<figureCaption confidence="0.998977">
Figure 1: Stacking algorithm
</figureCaption>
<subsectionHeader confidence="0.997397">
2.3 Two steps learning algorithm
</subsectionHeader>
<bodyText confidence="0.999839555555556">
Given an ensemble of datasets with different charac-
teristics, this method is based on the idea that when
instances come from a specific dataset, the learn-
ing algorithm trained on that dataset outperforms the
same algorithm trained on the whole ensemble.
Hence, the two steps algorithm tries to overcome
the problem of dealing with different datasets hav-
ing different characteristics through a classification
model.
</bodyText>
<figureCaption confidence="0.986354">
Figure 2: Two steps machine learning algorithm
</figureCaption>
<bodyText confidence="0.9967435">
In the first step (Figure 2), a different class is as-
signed to each dataset. The classifier is trained on
</bodyText>
<figure confidence="0.890064578947368">
level-0
model1
level-1
meta-learner
model2
� � �
modeln
input
� � �
dataset2
classifier
output: dataset class
dataset1
datasetn
predicted dataset
step-1
step-2
learning algorithm
prediction
</figure>
<page confidence="0.973872">
170
</page>
<bodyText confidence="0.999798">
a set of instances whose classes correspond to the
dataset numbers. Then, given a new instance the
output of this step will be the dataset to be used
for training the learning algorithm in the step 2. In
the second step, the learning algorithm is trained on
the dataset choose in the first step. The output of
this step is the predicted similarity between the two
texts. Through these steps, it is possible to select
the dataset with the characteristics more similar to
a given instance, and exploit just this set of data for
learning the algorithm.
</bodyText>
<subsectionHeader confidence="0.926709">
2.4 Features
</subsectionHeader>
<bodyText confidence="0.999607">
Both STACKING and 2STEPSML systems rely on
several kinds of features, which vary from lexical to
semantic ones. Features are grouped in seven main
classes, as follows:
</bodyText>
<listItem confidence="0.956081571428571">
1. Character/string/annotation-based features:
the length of the longest common contiguous
substring between the texts; the Jaccard index
of both tokens and lemmas; the Levenshtein
distance between texts; the normalized number
of common 2-grams, 3-grams and 4-grams; the
total number of tokens and characters; the dif-
ference in tokens and characters between texts;
the normalized difference with respect to the
max text length in tokens and characters be-
tween texts. Exploiting other linguistic anno-
tations extracted by Stanford CoreNLP1, we
compute the Jaccard index between PoS-tags
and named entities. Using WordNet we extract
the Jaccard index between the first sense and its
super-sense tag.
2. Textual Similarity-based features: a set of fea-
tures based on the textual similarity proposed
by Mihalcea (Mihalcea et al., 2006). Given two
texts T1 and T2 the similarity is computed as
follows:
</listItem>
<footnote confidence="0.962049">
1Available at: http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<bodyText confidence="0.999191625">
We adopt several similarity measures using
semantic distributional models (see Section
2.5), the Resnik’s knowledge-based approach
(Resnik, 1995) and the point-wise mutual infor-
mation as suggested by Turney (Turney, 2001)
computed on British National Corpus2. For all
the features, the idf is computed relying on
UKWaC corpus3 (Baroni et al., 2009).
</bodyText>
<listItem confidence="0.981476533333333">
3. Head similarity-based features: this measure
takes into account the maximum similarity be-
tween the roots of each text. The roots are ex-
tracted using the dependency parser provided
by Stanford CoreNLP. The similarity is com-
puted according to the distributional semantic
models proposed in Section 2.5.
4. ESA similarity: computes the similarity
between texts using the Explicit Semantic
Analysis (ESA) approach (Gabrilovich and
Markovitch, 2007). For each text we extract the
ESA vector built using the English Wikipedia,
and then we compute the similarity as the co-
sine similarity between the two ESA vectors.
5. Paraphrasing features: this is a very simple
</listItem>
<bodyText confidence="0.996509">
measure which counts the number of possi-
ble paraphrasings belonging to the two texts.
Given two texts T1 and T2, for each token in T1
a list of paraphrasings is extracted using a dic-
tionary4. If T2 contains one of the paraphrasing
in the list, the score is incremented by one. The
final score is divided by the number of tokens
in T1. The same score is computed taking into
account T2. Finally, the two score are added
and divided by 2.
</bodyText>
<sectionHeader confidence="0.784946" genericHeader="method">
6. Greedy Lemma Aligning Overlap features:
</sectionHeader>
<bodyText confidence="0.864455666666667">
this measure computes the similarity between
texts using the semantic alignment of lemmas
as proposed by ˇSari´c et al. (2012). In order
to compute the similarity between lemmas, we
exploit the distributional semantic models de-
scribed in Section 2.5.
</bodyText>
<footnote confidence="0.9957">
2Available at: http://www.natcorp.ox.ac.uk/
3Available at: http://wacky.sslmit.unibo.it/
4English Thesaurus for StarDict available at
https://aur.archlinux.org/packages/stardict-thesaurus-ee/
</footnote>
<equation confidence="0.9990785">
sim(T1,T2) = 1(EwCT1 maxSim/ (w, T2)
2 EwCT1 idf (w)
EwET2 maxSim(w,Tl)
+
EwCT2 idf(w) )
(1)
</equation>
<page confidence="0.951209">
171
</page>
<bodyText confidence="0.9407414">
7. Compositional features: we build several simi-
larity features using the distributional semantic
models described in Section 2.5 and a compo-
sitional operator based on sum. This approach
is thoroughly explained in Section 2.6
</bodyText>
<subsectionHeader confidence="0.976992">
2.5 Distributional semantic models
</subsectionHeader>
<bodyText confidence="0.999847375">
In several features proposed in our approaches, the
similarity between words is computed using Dis-
tributional Semantic Models. These models repre-
sent word meanings through contexts: the different
meanings of a word can be accounted for by look-
ing at the different contexts wherein the word oc-
curs. This insight can beautifully be expressed by
the geometrical representation of words as vectors
in a semantic space. Each term is represented as a
vector whose components are contexts surrounding
the term. In this way, the meaning of a term across
a corpus is thoroughly conveyed by the contexts it
appears in, where a context may typically be the set
of co-occurring words in a document, in a sentence
or in a window of surrounding terms.
In particular, we take into account two main
classes of models: Simple Distributional Spaces and
Structured Semantic Spaces. The former considers
as context the co-occurring words, the latter takes
into account both co-occurrence and syntactic de-
pendency between words.
Simple Distributional Spaces rely on Latent
Semantic Analysis (LSA) and Random Indexing
(RI) in order to reduce the dimension of the co-
occurrences matrix. Moreover, we use an approach
which applies LSA to the matrix produced by RI.
Structured Semantic Spaces are based on two
techniques to encode syntactic information into the
vector space. The first approach uses the vector per-
mutation of random vector in RI to encode the syn-
tactic role (head or dependent) of a word. The sec-
ond method is based on Holographic Reduced Rep-
resentation, in particular using convolution between
vectors, to encode syntactic information.
Adopting distributional semantic models, each
word can be represented as a vector in a geomet-
ric space. The similarity between two words can be
easily computed taking into account the cosine sim-
ilarity between word vectors.
All models are described in Basile et al. (2012).
</bodyText>
<subsectionHeader confidence="0.987405">
2.6 Compositional features
</subsectionHeader>
<bodyText confidence="0.996966666666667">
In Distributional Semantic Models, given the vector
representations of two words, it is always possible
to compute their similarity as the cosine of the angle
between them.
However, texts are composed by several terms,
so in order to compute the similarity between them
we need a method to compose words occurring in
these texts. It is possible to combine words through
the vector addition (+). This operator is similar to
the superposition defined in connectionist systems
(Smolensky, 1990), and corresponds to the point-
wise sum of components:
</bodyText>
<equation confidence="0.999038">
p = u + v (2)
</equation>
<bodyText confidence="0.999772631578947">
where pi = ui + vi
The addition is a commutative operator, which
means that it does not take into account any order
or underlying structures existing between words. In
this first study, we do not exploit more complex
methods to combine word vectors. We plan to in-
vestigate them in future work.
Given a text p, we denote with p its vector repre-
sentation obtained applying addition operator (+) to
the vector representation of terms it is composed of.
Furthermore, it is possible to compute the similar-
ity between two texts exploiting the cosine similarity
between vectors.
Formally, if a = a1, a2...an and b = b1, b2...bm
are two texts, we build two vectors a and b which
represent respectively the two texts in a semantic
space. Vector representations for the two texts are
built applying the addition operator to the vector rep-
resentation of words belonging to them:
</bodyText>
<equation confidence="0.989779">
a = a1 + a2 + ... + ar,, (3)
b = b1 + b2 ... + bm
</equation>
<bodyText confidence="0.998624">
The similarity between a and b is computed as the
cosine similarity between them.
</bodyText>
<sectionHeader confidence="0.997191" genericHeader="method">
3 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.9651188">
SemEval-2013 STS is the second attempt to provide
a “unified framework for the evaluation of modular
semantic textual similarity and to characterize their
impact on NLP applications”. The task consists
in computing the similarity between pair of texts,
</bodyText>
<page confidence="0.995804">
172
</page>
<bodyText confidence="0.999971227272727">
returning a similarity score. The test set is com-
posed by data coming from the following datasets:
news headlines (headlines); mapping of lexical re-
sources from Ontonotes to Wordnet (OnWN) and
from FrameNet to WordNet (FNWN); and evalua-
tion of machine translation (SMT).
The training data for STS-2013 is made up by
training and testing data from the previous edition
of STS-2012 task. During the 2012 edition, STS
provided participants with three training data: MSR-
Paraphrase, MSR-Video, STMeuropar; and five test-
ing data: MSR-Paraphrase, MSR-Video, STMeu-
ropar, SMTnews and OnWN. It is important to note
that part of 2012 test sets were made up from the
same sources of the training sets. On the other
hand, STS-2013 training and testing are very differ-
ent, making the prediction task a bit harder.
Humans rated each pair of texts with values from
0 to 5. The evaluation is performed by compar-
ing the humans scores against system performance
through Pearson’s correlation with the gold standard
for the four datasets.
</bodyText>
<subsectionHeader confidence="0.998426">
3.1 System setup
</subsectionHeader>
<bodyText confidence="0.99946019047619">
For the evaluation, we built the distributional spaces
using the WaCkypedia EN corpus5. WaCkype-
dia EN is based on a 2009 dump of the English
Wikipedia (about 800 million tokens) and includes
information about: part-of-speech, lemma and a full
dependency parsing performed by MaltParser (Nivre
et al., 2007). The structured spaces described in
Subsections 2.1 and 2.5 are built exploiting infor-
mation about term windows and dependency pars-
ing supplied by WaCkypedia. The total number of
dependencies amounts to about 200 million.
The RI system is implemented in Java and re-
lies on some portions of code publicly available in
the Semantic Vectors package (Widdows and Fer-
raro, 2008), while for LSA we exploited the publicly
available C library SVDLIBC6.
We restricted the vocabulary to the 50,000 most
frequent terms, with stop words removal and forc-
ing the system to include terms which occur in the
dataset.
Semantic space building involves some parame-
</bodyText>
<footnote confidence="0.999969">
5http://wacky.sslmit.unibo.it/doku.php?id=corpora
6http://tedlab.mit.edu/ dr/SVDLIBC/
</footnote>
<bodyText confidence="0.999600113636363">
ters. In particular, each semantic space needs to set
up the dimension k of the space. All spaces use a
dimension of 500 (resulting in a 50,000×500 ma-
trix). The number of non-zero elements in the ran-
dom vector is set to 10. When we apply LSA to the
output space generated by the Random Indexing we
hold all the 500 dimensions, since during the tuning
we observed a drop in performance when a lower
dimension was set. The co-occurrence distance w
between terms was set up to 4.
In order to compute the similarity between
the vector representations of text using UNIBA-
DSM PERM, we used the cosine similarity, and
then we multiplied by 5 the obtained value.
The two supervised methods, UNIBA-2STEPML
and UNIBA-STACKING, are developed in Java
using Weka7 to implement the learning algo-
rithms. Regarding the stacking approach (UNIBA-
STACKING) we used for the level-0 the follow-
ing models: Gaussian Process with polynomial ker-
nel, Gaussian Process with RBF kernel, Linear Re-
gression, Support Vector regression with polynomial
kernel, and decision tree. The level-1 model uses
a Gaussian Process with RBF kernel. In the first
step of UNIBA-2STEPML we adopt Support Vec-
tor Machine, while in the second one we use Sup-
port Vector Machine for regression. In both steps,
the RBF-Kernel is used. Features are normalized
removing non alphanumerics characters. In all the
learning algorithms, we use the default parameters
set by Weka. As future work, we plan to perform a
tuning step in order to set the best parameters.
The choice of the learning algorithms for both
UNIBA-STACKING and UNIBA-2STEPSML sys-
tems was performed after a tuning phase where only
the STS-2012 training datasets were exploited. Ta-
ble 1 reports the values obtained by our three sys-
tems on the STS-2012 test sets. After the tuning,
we came up with the learning algorithms to employ
in the level-0 and level-1 of UNIBA-STACKING
and in step-1 and step-2 of UNIBA-2STEPSML.
Then, the training of both UNIBA-STACKING and
UNIBA-2STEPSML was performed on all STS-
2012 datasets (training and test data).
</bodyText>
<page confidence="0.996324">
173
</page>
<table confidence="0.99751225">
MSRpar MSRvid SMTeuroparl OnWN SMTnews mean
UNIBA-2STEPSML .6056 .8573 .6233 .5079 .4533 .7016
UNIBA-DSM PERM .4349 .7592 .5324 .6593 .4559 .6172
UNIBA-STACKING .6473 .8727 .5344 .6646 .4604 .7714
</table>
<tableCaption confidence="0.999634">
Table 1: STS-2012 test results of Pearson’s correlation.
</tableCaption>
<table confidence="0.9998095">
headlines OnWN FNWN SMT mean rank
UNIBA- 2STEPSML .4255 .4801 .1832 .2710 .3673 71
UNIBA- DSM PERM .6319 4910 .2717 .3155 .4610 54
UNIBA- STACKING .6275 .4658 .2111 .2588 .4293 61
</table>
<tableCaption confidence="0.999271">
Table 2: Evaluation results of Pearson’s correlation for individual datasets.
</tableCaption>
<subsectionHeader confidence="0.99616">
3.2 Evaluation results
</subsectionHeader>
<bodyText confidence="0.999966034482759">
Evaluation results on the STS-2013 data are reported
in Table 2. Among the three systems, UNIBA-
DSM PERM obtained the best performances on
both individual datasets and in the overall evalua-
tion metric (mean), which computes the Pearson’s
correlation considering all datasets combined in a
single one. The best system ranked 54 over a to-
tal of 90 submissions, while UNIBA-STACKING
and UNIBA-2STEPSML ranked 61 and 71 re-
spectively. These results are at odds with those
reported in Table 1. During the test on 2012
dataset, UNIBA-STACKING gave the best result,
followed by UNIBA-2STEPSML, while UNIBA-
DSM PERM gave the worst performance. The
UNIBA-STACKING system corroborated our hy-
pothesis giving also the best results on those datasets
not exploited during the training phase of the sys-
tem (OnWN, SMTnews). Conversely, UNIBA-
2STEPSML reported a different trend showing its
weakness with respect to a high variance in the data,
and performing worse than UNIBA-DSM PERM on
the OnWN and SMTnews datasets.
However, the evaluation results have refuted our
hypothesis, even with the use of the stacking sys-
tem. The independence from a training set makes
the UNIBA-DSM PERM system more robust than
other supervised algorithms, even though it is not
able to give always the best performance on individ-
ual datasets, as highlighted by results in Table 1.
</bodyText>
<footnote confidence="0.885234">
7http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<sectionHeader confidence="0.999256" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999967666666667">
This paper reports on UNIBA participation in Se-
mantic Textual Similarity 2013 core task. In this
task edition, we exploited both distributional mod-
els and machine learning techniques to build three
systems. A distributional model, which takes into
account the syntactic structure that relates words in a
corpus, has been used as baseline. Moreover, we in-
vestigate the use of two machine learning techniques
as a means to make our systems more independent
from the training data. However, the evaluation re-
sults have highlighted the higher robustness of the
distributional model with respect to these systems.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999778">
This work fulfils the research objectives of the PON
02 00563 3470993 project ”VINCENTE - A Virtual
collective INtelligenCe ENvironment to develop
sustainable Technology Entrepreneurship ecosys-
tems” funded by the Italian Ministry of University
and Research (MIUR).
</bodyText>
<sectionHeader confidence="0.998587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993521454545455">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
</reference>
<page confidence="0.993637">
174
</page>
<reference confidence="0.999640232876713">
Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-
aro. 2012. A study on compositional semantics of
words in distributional spaces. In Sixth IEEE Inter-
national Conference on Semantic Computing, ICSC
2012, Palermo, Italy, September 19-21, 2012, pages
154–161. IEEE Computer Society.
Annalina Caputo, Pierpaolo Basile, and Giovanni Semer-
aro. 2012. Uniba: Distributional semantics for tex-
tual similarity. In *SEM 2012: The First Joint Confer-
ence on Lexical and Computational Semantics – Vol-
ume 1: Proceedings of the main conference and the
shared task, and Volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation (Se-
mEval 2012), pages 591–596, Montr´eal, Canada, 7-8
June. Association for Computational Linguistics.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th in-
ternational joint conference on artificial intelligence,
volume 6, page 12.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
pages 775–780. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Kathleen McKe-
own, Johanna D. Moore, Simone Teufel, James Allan,
and Sadaoki Furui, editors, ACL 2008, Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, June 15-20, 2008, Columbus,
Ohio, USA, pages 236–244. The Association for Com-
puter Linguistics.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95–135.
Philip Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
pages 448–453.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159–216, November.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelfth European Conference on Machine Learn-
ing (ECML-2001), pages 491–502.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems
for measuring semantic text similarity. In *SEM 2012:
The First Joint Conference on Lexical and Computa-
tional Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 441–448,
Montr´eal, Canada, 7-8 June. Association for Compu-
tational Linguistics.
Dominic Widdows and Kathleen Ferraro. 2008. Se-
mantic Vectors: A Scalable Open Source Package
and Online Technology Management Application. In
Nicoletta Calzolari, Khalid Choukri, Bente Maegaard,
Joseph Mariani, Jan Odjik, Stelios Piperidis, and
Daniel Tapias, editors, Proceedings of the 6th Interna-
tional Conference on Language Resources and Eval-
uation (LREC2008), pages 1183–1190, Marrakech,
Morocco. European Language Resources Association
(ELRA).
David H. Wolpert. 1992. Stacked generalization. Neural
networks, 5(2):241–259, February.
</reference>
<page confidence="0.998722">
175
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.501690">
<title confidence="0.999126">UNIBA-CORE: Combining Strategies for Semantic Textual Similarity</title>
<author confidence="0.999893">Annalina Caputo Pierpaolo Basile Giovanni Semeraro</author>
<affiliation confidence="0.999627">Department of Computer University of Bari Aldo</affiliation>
<address confidence="0.525688">Via E. Orabona, 4 - 70125 Bari,</address>
<email confidence="0.988203">pierpaolo.basile,</email>
<abstract confidence="0.998177095238095">This paper describes the UNIBA participation in the Semantic Textual Similarity (STS) core task 2013. We exploited three different systems for computing the similarity between two texts. A system is used as baseline, which represents the best model emerged from our previous participation in STS 2012. Such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together. In addition, we investigated the use of two different learning strategies exploiting both syntactic and semantic features. The former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets. The latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1724" citStr="Agirre et al., 2013" startWordPosition="258" endWordPosition="261">ly the more suitable dataset for the training purpose. 1 Introduction Semantic Textual Similarity is the task of computing the similarity between any two given texts. The task, in its core formulation, aims at capturing the different kinds of similarity that emerge from texts. Machine translation, paraphrasing, synonym substitution or text entailment are some fruitful methods exploited for this purpose. These techniques, along with other methods for estimating the text similarity, were successfully employed via machine learning approaches during the 2012 task. However, the STS 2013 core task (Agirre et al., 2013) differs from the 2012 formulation in that it provides a test set which is similar to the training, but not drawn from the same set of data. Hence, in order to generalize the machine learning models trained on a group of datasets, we investigate the use of combination strategies. The objective of combination strategies, known under the name of ensemble learning, is that of reducing the bias-variance decomposition through reducing the variance error. Hence, this class of methods should be more robust with respect to previously unseen data. Among the several ensemble learning alternatives, we ex</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="8763" citStr="Baroni et al., 2009" startWordPosition="1371" endWordPosition="1374">sense tag. 2. Textual Similarity-based features: a set of features based on the textual similarity proposed by Mihalcea (Mihalcea et al., 2006). Given two texts T1 and T2 the similarity is computed as follows: 1Available at: http://nlp.stanford.edu/software/corenlp.shtml We adopt several similarity measures using semantic distributional models (see Section 2.5), the Resnik’s knowledge-based approach (Resnik, 1995) and the point-wise mutual information as suggested by Turney (Turney, 2001) computed on British National Corpus2. For all the features, the idf is computed relying on UKWaC corpus3 (Baroni et al., 2009). 3. Head similarity-based features: this measure takes into account the maximum similarity between the roots of each text. The roots are extracted using the dependency parser provided by Stanford CoreNLP. The similarity is computed according to the distributional semantic models proposed in Section 2.5. 4. ESA similarity: computes the similarity between texts using the Explicit Semantic Analysis (ESA) approach (Gabrilovich and Markovitch, 2007). For each text we extract the ESA vector built using the English Wikipedia, and then we compute the similarity as the cosine similarity between the tw</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>A study on compositional semantics of words in distributional spaces.</title>
<date>2012</date>
<booktitle>In Sixth IEEE International Conference on Semantic Computing, ICSC 2012,</booktitle>
<pages>154--161</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Palermo, Italy,</location>
<contexts>
<context position="12607" citStr="Basile et al. (2012)" startWordPosition="1978" endWordPosition="1981">two techniques to encode syntactic information into the vector space. The first approach uses the vector permutation of random vector in RI to encode the syntactic role (head or dependent) of a word. The second method is based on Holographic Reduced Representation, in particular using convolution between vectors, to encode syntactic information. Adopting distributional semantic models, each word can be represented as a vector in a geometric space. The similarity between two words can be easily computed taking into account the cosine similarity between word vectors. All models are described in Basile et al. (2012). 2.6 Compositional features In Distributional Semantic Models, given the vector representations of two words, it is always possible to compute their similarity as the cosine of the angle between them. However, texts are composed by several terms, so in order to compute the similarity between them we need a method to compose words occurring in these texts. It is possible to combine words through the vector addition (+). This operator is similar to the superposition defined in connectionist systems (Smolensky, 1990), and corresponds to the pointwise sum of components: p = u + v (2) where pi = u</context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2012</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2012. A study on compositional semantics of words in distributional spaces. In Sixth IEEE International Conference on Semantic Computing, ICSC 2012, Palermo, Italy, September 19-21, 2012, pages 154–161. IEEE Computer Society.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Annalina Caputo</author>
<author>Pierpaolo Basile</author>
<author>Giovanni Semeraro</author>
</authors>
<title>Uniba: Distributional semantics for textual similarity.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>591--596</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="4144" citStr="Caputo et al., 2012" startWordPosition="642" endWordPosition="645">istics way through vector spaces (Mitchell and Lapata, 2008). These spaces are built taking into account the word context, hence the resulting vector representation is such that the distance between vectors reflects their similarity. Although several definitions of context are possible (e.g. a sliding window of text, the word order or syntactic dependencies), in their plain definition these kinds of models account for just one type of context at a time. To overcome this limitation, we exploit a method to encode more definitions of context in the same vector exploiting the vector permutations (Caputo et al., 2012). This technique, which is based on Random Indexing as a means for computing the distributional model, is based on the idea that when the components of a highly sparse vector are shuffled, the resulting vector is nearly orthogonal to the original one. Hence, vector permutation represents a way for generating new random vectors in a predetermined manner. Different word contexts can be encoded using different types of permutations. In our distributional model system (DSM PERM), we encode the syntactic dependencies between words rather than the mere co-occurrence information. In this way, wordvec</context>
</contexts>
<marker>Caputo, Basile, Semeraro, 2012</marker>
<rawString>Annalina Caputo, Pierpaolo Basile, and Giovanni Semeraro. 2012. Uniba: Distributional semantics for textual similarity. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 591–596, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th international joint conference on artificial intelligence,</booktitle>
<volume>6</volume>
<pages>12</pages>
<contexts>
<context position="9212" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1437" endWordPosition="1440"> mutual information as suggested by Turney (Turney, 2001) computed on British National Corpus2. For all the features, the idf is computed relying on UKWaC corpus3 (Baroni et al., 2009). 3. Head similarity-based features: this measure takes into account the maximum similarity between the roots of each text. The roots are extracted using the dependency parser provided by Stanford CoreNLP. The similarity is computed according to the distributional semantic models proposed in Section 2.5. 4. ESA similarity: computes the similarity between texts using the Explicit Semantic Analysis (ESA) approach (Gabrilovich and Markovitch, 2007). For each text we extract the ESA vector built using the English Wikipedia, and then we compute the similarity as the cosine similarity between the two ESA vectors. 5. Paraphrasing features: this is a very simple measure which counts the number of possible paraphrasings belonging to the two texts. Given two texts T1 and T2, for each token in T1 a list of paraphrasings is extracted using a dictionary4. If T2 contains one of the paraphrasing in the list, the score is incremented by one. The final score is divided by the number of tokens in T1. The same score is computed taking into account T2. </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on artificial intelligence, volume 6, page 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the national conference on artificial intelligence,</booktitle>
<volume>21</volume>
<pages>775--780</pages>
<publisher>AAAI Press; MIT Press.</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="8286" citStr="Mihalcea et al., 2006" startWordPosition="1305" endWordPosition="1308">e normalized number of common 2-grams, 3-grams and 4-grams; the total number of tokens and characters; the difference in tokens and characters between texts; the normalized difference with respect to the max text length in tokens and characters between texts. Exploiting other linguistic annotations extracted by Stanford CoreNLP1, we compute the Jaccard index between PoS-tags and named entities. Using WordNet we extract the Jaccard index between the first sense and its super-sense tag. 2. Textual Similarity-based features: a set of features based on the textual similarity proposed by Mihalcea (Mihalcea et al., 2006). Given two texts T1 and T2 the similarity is computed as follows: 1Available at: http://nlp.stanford.edu/software/corenlp.shtml We adopt several similarity measures using semantic distributional models (see Section 2.5), the Resnik’s knowledge-based approach (Resnik, 1995) and the point-wise mutual information as suggested by Turney (Turney, 2001) computed on British National Corpus2. For all the features, the idf is computed relying on UKWaC corpus3 (Baroni et al., 2009). 3. Head similarity-based features: this measure takes into account the maximum similarity between the roots of each text.</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the national conference on artificial intelligence, volume 21, pages 775–780. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>236--244</pages>
<editor>In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors,</editor>
<publisher>The Association for Computer Linguistics.</publisher>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="3584" citStr="Mitchell and Lapata, 2008" startWordPosition="553" endWordPosition="556">tems in Section 2, Section 3 describes the evaluation setting of our systems along with the experimental results. Then, some conclusions and remarks close the paper. 2 General Models 2.1 Dependency Encoding via Vector Permutations Distributional models are effective methods for representing word paradigmatic relations in a simple 169 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 169–175, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics way through vector spaces (Mitchell and Lapata, 2008). These spaces are built taking into account the word context, hence the resulting vector representation is such that the distance between vectors reflects their similarity. Although several definitions of context are possible (e.g. a sliding window of text, the word order or syntactic dependencies), in their plain definition these kinds of models account for just one type of context at a time. To overcome this limitation, we exploit a method to encode more definitions of context in the same vector exploiting the vector permutations (Caputo et al., 2012). This technique, which is based on Rand</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Kathleen McKeown, Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui, editors, ACL 2008, Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, June 15-20, 2008, Columbus, Ohio, USA, pages 236–244. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="15820" citStr="Nivre et al., 2007" startWordPosition="2517" endWordPosition="2520">ing and testing are very different, making the prediction task a bit harder. Humans rated each pair of texts with values from 0 to 5. The evaluation is performed by comparing the humans scores against system performance through Pearson’s correlation with the gold standard for the four datasets. 3.1 System setup For the evaluation, we built the distributional spaces using the WaCkypedia EN corpus5. WaCkypedia EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-of-speech, lemma and a full dependency parsing performed by MaltParser (Nivre et al., 2007). The structured spaces described in Subsections 2.1 and 2.5 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia. The total number of dependencies amounts to about 200 million. The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package (Widdows and Ferraro, 2008), while for LSA we exploited the publicly available C library SVDLIBC6. We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Erwin Marsi. 2007. Maltparser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="8560" citStr="Resnik, 1995" startWordPosition="1340" endWordPosition="1341">stic annotations extracted by Stanford CoreNLP1, we compute the Jaccard index between PoS-tags and named entities. Using WordNet we extract the Jaccard index between the first sense and its super-sense tag. 2. Textual Similarity-based features: a set of features based on the textual similarity proposed by Mihalcea (Mihalcea et al., 2006). Given two texts T1 and T2 the similarity is computed as follows: 1Available at: http://nlp.stanford.edu/software/corenlp.shtml We adopt several similarity measures using semantic distributional models (see Section 2.5), the Resnik’s knowledge-based approach (Resnik, 1995) and the point-wise mutual information as suggested by Turney (Turney, 2001) computed on British National Corpus2. For all the features, the idf is computed relying on UKWaC corpus3 (Baroni et al., 2009). 3. Head similarity-based features: this measure takes into account the maximum similarity between the roots of each text. The roots are extracted using the dependency parser provided by Stanford CoreNLP. The similarity is computed according to the distributional semantic models proposed in Section 2.5. 4. ESA similarity: computes the similarity between texts using the Explicit Semantic Analys</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Smolensky</author>
</authors>
<title>Tensor product variable binding and the representation of symbolic structures in connectionist systems.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<pages>46--1</pages>
<contexts>
<context position="13127" citStr="Smolensky, 1990" startWordPosition="2061" endWordPosition="2062">unt the cosine similarity between word vectors. All models are described in Basile et al. (2012). 2.6 Compositional features In Distributional Semantic Models, given the vector representations of two words, it is always possible to compute their similarity as the cosine of the angle between them. However, texts are composed by several terms, so in order to compute the similarity between them we need a method to compose words occurring in these texts. It is possible to combine words through the vector addition (+). This operator is similar to the superposition defined in connectionist systems (Smolensky, 1990), and corresponds to the pointwise sum of components: p = u + v (2) where pi = ui + vi The addition is a commutative operator, which means that it does not take into account any order or underlying structures existing between words. In this first study, we do not exploit more complex methods to combine word vectors. We plan to investigate them in future work. Given a text p, we denote with p its vector representation obtained applying addition operator (+) to the vector representation of terms it is composed of. Furthermore, it is possible to compute the similarity between two texts exploiting</context>
</contexts>
<marker>Smolensky, 1990</marker>
<rawString>Paul Smolensky. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial Intelligence, 46(1-2):159–216, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<contexts>
<context position="8636" citStr="Turney, 2001" startWordPosition="1352" endWordPosition="1353">x between PoS-tags and named entities. Using WordNet we extract the Jaccard index between the first sense and its super-sense tag. 2. Textual Similarity-based features: a set of features based on the textual similarity proposed by Mihalcea (Mihalcea et al., 2006). Given two texts T1 and T2 the similarity is computed as follows: 1Available at: http://nlp.stanford.edu/software/corenlp.shtml We adopt several similarity measures using semantic distributional models (see Section 2.5), the Resnik’s knowledge-based approach (Resnik, 1995) and the point-wise mutual information as suggested by Turney (Turney, 2001) computed on British National Corpus2. For all the features, the idf is computed relying on UKWaC corpus3 (Baroni et al., 2009). 3. Head similarity-based features: this measure takes into account the maximum similarity between the roots of each text. The roots are extracted using the dependency parser provided by Stanford CoreNLP. The similarity is computed according to the distributional semantic models proposed in Section 2.5. 4. ESA similarity: computes the similarity between texts using the Explicit Semantic Analysis (ESA) approach (Gabrilovich and Markovitch, 2007). For each text we extra</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c.</title>
<date></date>
<booktitle>In *SEM</booktitle>
<marker>ˇSari´c, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In *SEM 2012:</rawString>
</citation>
<citation valid="true">
<date>2012</date>
<booktitle>The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>441--448</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="10032" citStr="(2012)" startWordPosition="1585" endWordPosition="1585">measure which counts the number of possible paraphrasings belonging to the two texts. Given two texts T1 and T2, for each token in T1 a list of paraphrasings is extracted using a dictionary4. If T2 contains one of the paraphrasing in the list, the score is incremented by one. The final score is divided by the number of tokens in T1. The same score is computed taking into account T2. Finally, the two score are added and divided by 2. 6. Greedy Lemma Aligning Overlap features: this measure computes the similarity between texts using the semantic alignment of lemmas as proposed by ˇSari´c et al. (2012). In order to compute the similarity between lemmas, we exploit the distributional semantic models described in Section 2.5. 2Available at: http://www.natcorp.ox.ac.uk/ 3Available at: http://wacky.sslmit.unibo.it/ 4English Thesaurus for StarDict available at https://aur.archlinux.org/packages/stardict-thesaurus-ee/ sim(T1,T2) = 1(EwCT1 maxSim/ (w, T2) 2 EwCT1 idf (w) EwET2 maxSim(w,Tl) + EwCT2 idf(w) ) (1) 171 7. Compositional features: we build several similarity features using the distributional semantic models described in Section 2.5 and a compositional operator based on sum. This approach</context>
<context position="12607" citStr="(2012)" startWordPosition="1981" endWordPosition="1981"> to encode syntactic information into the vector space. The first approach uses the vector permutation of random vector in RI to encode the syntactic role (head or dependent) of a word. The second method is based on Holographic Reduced Representation, in particular using convolution between vectors, to encode syntactic information. Adopting distributional semantic models, each word can be represented as a vector in a geometric space. The similarity between two words can be easily computed taking into account the cosine similarity between word vectors. All models are described in Basile et al. (2012). 2.6 Compositional features In Distributional Semantic Models, given the vector representations of two words, it is always possible to compute their similarity as the cosine of the angle between them. However, texts are composed by several terms, so in order to compute the similarity between them we need a method to compose words occurring in these texts. It is possible to combine words through the vector addition (+). This operator is similar to the superposition defined in connectionist systems (Smolensky, 1990), and corresponds to the pointwise sum of components: p = u + v (2) where pi = u</context>
</contexts>
<marker>2012</marker>
<rawString>The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 441–448, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dominic Widdows</author>
<author>Kathleen Ferraro</author>
</authors>
<title>Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC2008),</booktitle>
<pages>1183--1190</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors,</editor>
<location>Marrakech,</location>
<contexts>
<context position="16194" citStr="Widdows and Ferraro, 2008" startWordPosition="2577" endWordPosition="2581"> using the WaCkypedia EN corpus5. WaCkypedia EN is based on a 2009 dump of the English Wikipedia (about 800 million tokens) and includes information about: part-of-speech, lemma and a full dependency parsing performed by MaltParser (Nivre et al., 2007). The structured spaces described in Subsections 2.1 and 2.5 are built exploiting information about term windows and dependency parsing supplied by WaCkypedia. The total number of dependencies amounts to about 200 million. The RI system is implemented in Java and relies on some portions of code publicly available in the Semantic Vectors package (Widdows and Ferraro, 2008), while for LSA we exploited the publicly available C library SVDLIBC6. We restricted the vocabulary to the 50,000 most frequent terms, with stop words removal and forcing the system to include terms which occur in the dataset. Semantic space building involves some parame5http://wacky.sslmit.unibo.it/doku.php?id=corpora 6http://tedlab.mit.edu/ dr/SVDLIBC/ ters. In particular, each semantic space needs to set up the dimension k of the space. All spaces use a dimension of 500 (resulting in a 50,000×500 matrix). The number of non-zero elements in the random vector is set to 10. When we apply LSA </context>
</contexts>
<marker>Widdows, Ferraro, 2008</marker>
<rawString>Dominic Widdows and Kathleen Ferraro. 2008. Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, and Daniel Tapias, editors, Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC2008), pages 1183–1190, Marrakech, Morocco. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural networks,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="2393" citStr="Wolpert, 1992" startWordPosition="368" endWordPosition="369">a test set which is similar to the training, but not drawn from the same set of data. Hence, in order to generalize the machine learning models trained on a group of datasets, we investigate the use of combination strategies. The objective of combination strategies, known under the name of ensemble learning, is that of reducing the bias-variance decomposition through reducing the variance error. Hence, this class of methods should be more robust with respect to previously unseen data. Among the several ensemble learning alternatives, we exploit the stacked generalization (STACKING) algorithm (Wolpert, 1992). Moreover, we investigate the use of a two-steps learning algorithm (2STEPSML). In this method the learning algorithm is trained using only the dataset most similar to the instance to be predicted. The first step aims at predicting the dataset more similar to the given pair of texts. Then the second step makes use of the previously trained algorithm to predict the similarity value. The baseline for the evaluation is represented by our best system (DSM PERM) resulting from our participation in the 2012 task. After introducing the general models behind our systems in Section 2, Section 3 descri</context>
<context position="5192" citStr="Wolpert, 1992" startWordPosition="808" endWordPosition="809">In our distributional model system (DSM PERM), we encode the syntactic dependencies between words rather than the mere co-occurrence information. In this way, wordvector components bear the information about both co-occurring and syntactically related words. In this distributional space, a text can be easily represented as the superposition of its words. Then, the vector representation of a text is given by adding the vector representation of its words, and the similarity between texts come through the cosine of the angle between their vector representations. 2.2 Stacking Stacking algorithms (Wolpert, 1992) are a way of combining different types of learning algorithms reducing the variance of the system. In this model, the meta-learner tries to predict the real value of an instance combining the outputs of other machine learning methods. Figure 1 shows how the learning process takes place. The level-0 represents the ensemble of different models to be trained on the same dataset. The level-0 outputs build up the level-1 dataset: an instance at this level is represented by the numeric values predicted by each level-0 model along with the gold standard value. Then, the objective of the level-1 lear</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural networks, 5(2):241–259, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>