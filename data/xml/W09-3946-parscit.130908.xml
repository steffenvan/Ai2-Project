<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000349">
<title confidence="0.9978855">
Ranking Help Message Candidates Based on Robust Grammar
Verification Results and Utterance History in Spoken Dialogue Systems
</title>
<author confidence="0.333396">
Kazunori Komatani Satoshi Ikeda Yuichiro Fukubayashi
Tetsuya Ogata Hiroshi G. Okuno
</author>
<affiliation confidence="0.405549">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.477705">
Yoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan
</address>
<email confidence="0.998038">
{komatani,sikeda,fukubaya,ogata,okuno}@kuis.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916476190476">
We address an issue of out-of-grammar
(OOG) utterances in spoken dialogue sys-
tems by generating help messages for
novice users. Help generation for OOG
utterances is a challenging problem be-
cause language understanding (LU) re-
sults based on automatic speech recogni-
tion (ASR) results for such utterances are
always erroneous as important words are
often misrecognized or missed from such
utterances. We first develop grammar ver-
ification for OOG utterances on the ba-
sis of a Weighted Finite-State Transducer
(WFST). It robustly identifies a grammar
rule that a user intends to utter, even when
some important words are missed from the
ASR result. We then adopt a ranking algo-
rithm, RankBoost, whose features include
the grammar verification results and the
utterance history representing the user’s
experience.
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997533542857143">
Studies on spoken dialogue systems have recently
proceeded from in-laboratory systems to ones de-
ployed to the open public (Raux et al., 2006; Ko-
matani et al., 2007; Nisimura et al., 2005). Ac-
cordingly, opportunities are increasing as general
citizens use the systems. This situation means
that novice users directly access the systems with
no instruction, which is quite different from in-
laboratory experiments where some instructions
can be given. In such cases, users often experi-
ence situations where their utterances are not cor-
rectly recognized. This is because of a gap be-
tween the actual system and a user’s mental model,
that is, a user’s expectation of the system. Ac-
tually, a user’s utterance often cannot be inter-
preted by the system because of the system’s lim-
ited grammar for language understanding (LU).
We call such an unacceptable utterance an “out-
of-grammar (OOG) utterance.” When users’ ut-
terances are OOG, they cannot change their ut-
terances into acceptable ones unless they are in-
formed what expressions are acceptable by the
system.
We aim to manage the problem of OOG utter-
ances by providing help messages showing an ex-
ample of acceptable language expressions when a
user utterance is not acceptable. We prepare help
messages corresponding to each grammar rule the
system has. We therefore assume that appropri-
ate help messages can be provided if a user’s in-
tention, i.e., a grammar rule the user originally
intends to use by his utterance, is correctly esti-
mated.
Issues for generating such help messages in-
clude:
</bodyText>
<listItem confidence="0.990707">
1. Estimating a grammar rule corresponding to
user intention even from OOG utterances,
and
2. Complementing missing information in a sin-
gle utterance.
</listItem>
<bodyText confidence="0.9997106">
The first issue focuses on the fact that automatic
speech recognition (ASR) results, used as main in-
put data, are erroneous for OOG utterances. Es-
timating a grammar rule that the user intends to
use becomes accordingly difficult especially when
content words, which correspond to database en-
tries such as place names and their attributes, are
not correctly recognized. That is, any type of ASR
error in any position should be taken into consid-
eration in ASR results of OOG utterances. On the
</bodyText>
<note confidence="0.709782">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 314–321,
</note>
<affiliation confidence="0.662745">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998911">
314
</page>
<bodyText confidence="0.992069106382979">
other hand, the second issue focuses on the fact U1: Tell me your recommending sites.
that an ASR result for an OOG utterance does not Underlined parts are not in-vocabulary and no
necessarily contain sufficient information to esti- valid LU result is obtained. The estimated gram-
mate the user intention. This is because of ASR mar is [Obtaining info on a site] although the most
errors or that users may omit some elements from appropriate help message is that corresponding to
their utterances because they are in context. [Searching tourist sites].
We develop a grammar verification method S1: I did not understand. You can say “Tell me
based on Weighted Finite-State Transducer the address of Kiyomizu Temple” for example,
(WFST) as a solution to the first issue. The if getting information on a site.
grammar verification method robustly estimates The help message corresponding to [Obtaining info
which a grammar rule is intended to use by a on a site] is provided.
user’s utterance. The WFST is automatically U2: Tell me your recommending sites.
generated to represent an ASR result in which any The user repeats the same utterance probably be-
possibility of error is taken into consideration. We cause the help message (S1) was not helpful. The
furthermore adopt a boosting algorithm, Rank- estimated grammar is [Obtaining info on a site]
Boost (Freund et al., 2003), to put help messages again.
in order of probability to address the second issue. S2: I did not understand. You can say “Search
Because it is difficult even for human annotators shrines or museums” for example, if searching
to uniquely determine which help message should tourist sites.
be provided for each case, we adopt an algorithm Another help message corresponding to [Searching
that can be used for training on several data tourist sites] is provided after ranking candidates
examples that have a certain order of priority. by also using the user’s utterance history.
We also incorporate features representing the
user’s utterance history for preventing message
repetition.
2 Related Work
Various studies have been done on generating help
messages in spoken dialogue systems. Gorrell et
al. (2002) trained a decision tree to classify causes
of errors for OOG utterances. Hockey et al. (2003)
also classified OOG utterances into the three cate-
gories of endpointing errors, unknown vocabulary,
and subcategorization mistakes, by comparing two
kinds of ASR results. This was called Targeted
Help and provided a user with immediate feedback
tailored to what the user said. Lee et al. (2007) also
addressed error recovery by generating help mes-
sages in an example-based dialog modeling frame-
work. These studies, however, determined what
help messages should be provided mainly on the
basis of literal ASR results. Therefore, help mes-
sages would be degraded by ASR results in which
a lot of information was missing, especially for
OOG utterances. The same help messages would
be repeated when the same ASR results were ob-
tained.
An example dialogue enabled by our method,
</bodyText>
<figureCaption confidence="0.956371">
especially the part of the method described in Sec-
tion 4, is shown in Figure 1. Here, user utter-
ances are transcriptions, and utterance numbers
315
[] denotes grammar rules.
Figure 1: Example dialogue enabled by our
method
start with “S” and “U” denote system and user
utterances, respectively. In this example, ASR
results for the user utterances (U1 and U2) do
not contain sufficient information because the ut-
terances are short and contain out-of-vocabulary
words. These two results are similar, and ac-
cordingly, the help message after U2 provided by
methods like Targeted Help (Gorrell et al., 2002;
Hockey et al., 2003) is the same as Utterance S1
because they are only based on ASR results. Our
method can provide different help messages as Ut-
terance S2 after ranking candidates by consider-
ing the utterance history and grammar verification
results. Because the candidates are arranged in
the order of probability, the most appropriate help
message can be provided in fewer attempts.
This ranking method for help message candi-
dates is also useful in multimodal interfaces with
speech input. Help messages are necessary when
ASR is used as its input modality, and such mes-
sages were actually implemented in City Browser
(Gruenstein and Seneff, 2007), for example. This
system lists template-based help messages on the
screen by using ASR results and internal states of
the system. The order of help messages is impor-
tant, especially in portable devices with a small
screen, on which the number of help messages dis-
</figureCaption>
<bodyText confidence="0.9998362">
played at one time is limited, as Hartmann and
Schreiber (2008) pointed out. Even in cases where
sufficiently large screens are available, too many
help messages without any order will distract the
user’s attention and thus spoil its usability.
</bodyText>
<sectionHeader confidence="0.981638" genericHeader="method">
3 Grammar Verification based on WFST
</sectionHeader>
<bodyText confidence="0.999988">
We estimate a user’s intention even from OOG ut-
terances as a grammar rule that the user intends
to use by his utterance. We call this estimation
grammar verification. This process is applied to
ASR outputs based on a statistical language model
(LM) in this paper. We use two transducers: a
finite-state transducer (FST) representing the task
grammar, and weighted FST (WFST) representing
an ASR result and its confidence score. Hereafter,
we denote these two as “grammar FST” and “input
WFST” and depict examples in Figure 2.
A strong point of our method is that it takes
all three types of ASR error into consideration.
The input WFST is designed to represent all cases
where any word in an ASR result is an inserted or
substituted error, or any word is deleted. Its weight
is designed to reflect confidence scores of ASR re-
sults. By composing this WFST and the gram-
mar FST, we can obtain all possible sequences
and their accumulated weights when arbitrary se-
quences represented by the input WFST are input
into the grammar FST. The optimal results having
the maximum accumulated weight consist of the
LU result and the grammar rule that is the nearest
to the ASR result. The result can be obtained even
when any element in it is misrecognized or absent
from the ASR result.
An LU result is a set of concepts that consist
of slots and their values corresponding to database
entries the system handles. For example, an LU
result “month=2, day=22” consists of two con-
cepts, such as the value of slot month is 2, and the
value of slot day is 22.
</bodyText>
<subsectionHeader confidence="0.999701">
3.1 Design of input WFST and grammar FST
</subsectionHeader>
<bodyText confidence="0.99995158">
In input WFSTs and grammar FSTs, each arc rep-
resenting state transitions has a label in the form of
“a:b/c” denoting its input symbol, output symbol,
and weight, in this order. Input symbol ε means a
state transition without any input symbol, that is,
an epsilon transition. Output symbol ε means no
output in the state transition. For example, a state
transition “please:ε/1.0” is executed when an in-
put symbol is “please,” no output symbol is gen-
erated, and 1.0 is added to the accumulated weight.
Weights are omitted in the grammar FST because
no weight is given in it.
An input WFST is automatically constructed
from an ASR result. Sequential state transitions
are assigned to each word in the ASR result, and
each of them is paralleled by filler transitions, as
shown in Figure 2 where the ASR result was “Ev-
ery Monday please” for example. Filler transitions
such as INS, DEL, and SUB are assigned to each
state for representing every kind of error such as
insertion, deletion, and substitution errors. All in-
put symbols in the input WFST are ε, by which the
WFST represents all possible sequences contain-
ing arbitrary errors. For example, the input WFST
in Figure 2 represents all possible sequences such
as “Every Monday please,” “Every Monday F,” “F
Monday F,” and so on. Here, every word can be
replaced by the symbol F that represents an inser-
tion or substitution error. Moreover, the error sym-
bol DEL can be inserted into its output symbol se-
quence at any position, which corresponds to dele-
tion errors in ASR results. Each weight per state
transition is summed up and then the optimal re-
sult is determined. The weights will be explained
in Section 3.2.
A grammar FST is generated from a task gram-
mar, which is written by a system developer for
each task. It determines whether an input se-
quence conforms to the task grammar. We also
assign filler transitions to each state for handling
each type of error of ASR results considered in
the input WFST. A filler transition, either of INS,
DEL, or SUB, is added to each state in the FST
except for states within keyphrases, which are ex-
plicitly indicated by a system developer. In the
example shown in Figure 2, “SUB $ Monday
date-repeat=Mon please” is output for an input
sequence “SUB Monday please”. Here, date-
repeat=Mon denotes an LU result, and $ is a sym-
bol for marking words corresponding to a concept.
</bodyText>
<subsectionHeader confidence="0.673543">
3.2 Weights assigned to input WFST
</subsectionHeader>
<bodyText confidence="0.995597">
We defined two kinds of weights:
</bodyText>
<listItem confidence="0.817514">
1. Rewards for accepted words (wacc), and
2. Penalties for each kind of error (wsub, wdel,
wins).
</listItem>
<bodyText confidence="0.998798">
An accumulated weight for a single utterance is
defined as the sum of these weights as shown be-
</bodyText>
<page confidence="0.997696">
316
</page>
<figureCaption confidence="0.999486">
Figure 2: Example of input WFST and grammar FST
</figureCaption>
<equation confidence="0.750711666666667">
low.
�w = �wacc + (wsub + wdel + wins)
Eaccepted Eerror
</equation>
<bodyText confidence="0.999971666666667">
Here, Eaccepted denotes a set of accepted words
corresponding to elements of each grammar rule,
and Eerror denotes a set of words that are not ac-
cepted and that have either error symbol. Note that
the weights are not given beforehand but are cal-
culated and given to the input WFST in runtime
according to each ASR result.
A weight for an accepted word easr is defined
by using its confidence score CM(easr) (Lee et
al., 2004) and its word length. A word length in
mora is denoted as l(·), which is normalized by
that of the longest word in the vocabulary.
</bodyText>
<equation confidence="0.858823">
wacc = CM(easr)l(easr)
</equation>
<bodyText confidence="0.999865">
This weight wacc gives preference to sequences
containing longer words with higher confidence
scores.
Weights for each type of error have negative val-
ues because they are penalties:
</bodyText>
<equation confidence="0.999939">
wsub = −{CM(easr)l(easr) + l(egram)}/2
wdel = −{l(e) + l(egram)}/2
wins = −{CM(easr)l(easr) + l(e)}/2
</equation>
<bodyText confidence="0.999773">
where l(e) is the average word length in the vocab-
ulary and egram is a grammar element i.e., either a
word or a class. A deletion error is a case when a
grammar element does not correspond to any word
in the ASR result. A substitution error is a case
when an element is replaced by another word in
the ASR result. An insertion error is a case when
no grammar element corresponds to the ASR re-
sult. Every weight is defined as an average of a
word length of a grammar element and the corre-
sponding one in the ASR result multiplied by its
confidence score. When correspondences cannot
be defined in insertion and deletion errors, l(e) is
used instead. In the case when egram is a class in
the grammar, the average word length in that class
is used as l(egram).
</bodyText>
<subsectionHeader confidence="0.999488">
3.3 Example of calculating the weights
</subsectionHeader>
<bodyText confidence="0.999850434782609">
We show how a weight is calculated by using the
example in Figure 3. In this example, the user ut-
terance was “Tell me a liaison of Koetsu-ji (a tem-
ple name).” The word “liaison” was not in the sys-
tem vocabulary. The ASR result accordingly con-
tained errors for that part; the result was “Tell me
all Sakyo-ward Koetsu-ji.”
Weights are calculated for each grammar rule
the system has. This example shows calcula-
tions for two grammar rules: [get info] accept-
ing “Tell me (item name) of (temple name),” and
[search ward] accepting “Tell me (facility name)
of (ward name).” Here, [] and () denote a gram-
mar rule and a class in grammars. Two alignment
results are also shown for grammar [get info] in
this example. Weights are calculated for any align-
ment as shown here, and the alignment result with
the largest weight is selected. In this example,
weight +0.16 for the grammar [get info] was the
largest.
We consequently obtained the result that gram-
mar rule [get info] had the highest score for this
OOG utterance and its accumulated weight was
</bodyText>
<page confidence="0.991421">
317
</page>
<table confidence="0.981770083333333">
User utterance: “Tell me a liaison of Koetsu-ji”. (Underlined parts denote OOG.)
ASR result tell me all Sakyo-ward Koetsu-ji
(ward) (temple)
grammar [get info] tell me INS (item name) of (temple name) +0.16
WFST output tell me −0.04 SUB DEL Koetsu-ji
weights +0.09 +0.06 −0.11 −0.02 +0.18
grammar [get info] tell me (item name) of (temple name) +0.02
WFST output tell me SUB SUB Koetsu-ji
weights +0.09 +0.06 −0.21 −0.10 +0.18
grammar [search ward] tell me INS (facility type) in (ward name) −0.24
WFST output tell me −0.04 SUB DEL SUB
weights +0.09 +0.06 −0.12 −0.02 −0.21
</table>
<figureCaption confidence="0.99818">
Figure 3: Example of calculating weights in our grammar verification
</figureCaption>
<bodyText confidence="0.9999508">
+0.16. The result also indicated each type of er-
ror as a result of the alignment: (item name) was
substituted by “Sakyo-ward”, “of” in the grammar
[get info] was deleted, and “all” in the ASR result
was inserted.
</bodyText>
<sectionHeader confidence="0.9970035" genericHeader="method">
4 Ranking Help Message Candidates by
Integrating Dialogue Context
</sectionHeader>
<bodyText confidence="0.999984625">
We furthermore develop a method to rank help
message candidates per grammar rule by integrat-
ing the grammar verification result and the user’s
utterance history. This complements information
that is often absent from utterances or misrecog-
nized in ASR and prevents that the same help mes-
sages are repeated. An outline of the method is
depicted in Figure 4.
</bodyText>
<subsectionHeader confidence="0.993128">
4.1 Features used in Ranking
</subsectionHeader>
<bodyText confidence="0.999303142857143">
Features used in our methods are listed in Table
1. These features are calculated for each help
message candidate corresponding to each gram-
mar rule. Features H1 to H5 represent how reli-
able a grammar verification result is. Feature H1 is
a grammar verification score, that is, the resulting
accumulated weight described in Section 3. Fea-
ture H2 is calculated by normalizing H1 by the
total score of all grammar rules. This represents
how reliable the grammar verification result is rel-
atively compared to others. Features H3 to H5
represent how partially the user utterance matches
with the grammar rule.
Features H6 and H7 correspond to a dialogue
context. Feature H6 reflects the case in which
users tend to repeat similar utterances when their
utterances were not understood by the system.
Feature H7 represents whether and how the user
knows about the language expression of the gram-
mar rule. This feature corresponds to the known
degree we previously proposed (Fukubayashi et
</bodyText>
<tableCaption confidence="0.958191894736842">
Table 1: Features of each instance (help message
candidate)
H1: accumulated weight of GV (GV score)
H2: GV score normalized by the total GV score of other
instances
H3: ratio of # of accepted words in GV result to # of all
words
H4: maximum number of successively accepted words
in GV result
H5: number of accepted slots in GV result
H6: how before the grammar rule was selected as GV
result (in # of utterances)
H7: maximum GV score for the grammar rule until then
H8: whether it belongs to the “command” class
H9: whether it belongs to the “query” class
H10: whether it belongs to the “request-info” class
H11-H17: products of H8 and each of H1 to H7
H18-H24: products of H9 and each of H1 to H7
H25-H31: products of H10 and each of H1 to H7
</tableCaption>
<bodyText confidence="0.98531975">
GV: grammar verification
al., 2006), and prevents a help message the user
already knows from being provided repeatedly.
Features H8 to H10 represent properties of
utterances corresponding to the grammar rules,
which are categorized into three classes such as
“command,” “query,” and “request-info.” In the
sightseeing task, the numbers of grammar rules for
the three classes were 8, 4, and 11, respectively.
More specifically, utterances in either “query” or
“request-info” class tend to appear successively
because they are used when users try and com-
pare several query conditions; on the other hand,
utterances in “command” class tend to appear in-
dependently of the context. Features H11 to H31
are the products of features H8, H9, and H10 and
each feature from H1 to H7. These were defined to
consider combinations of properties of utterances
represented by H8, H9, and H10 and their reliabil-
ity represented by H1 to H7, because RankBoost
</bodyText>
<page confidence="0.998054">
318
</page>
<figureCaption confidence="0.999132">
Figure 4: Outline of our ranking method for help message candidates
</figureCaption>
<subsectionHeader confidence="0.923635">
does not consider them.
4.2 Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.999885941176471">
We adopt RankBoost (Freund et al., 2003), a
boosting algorithm based on machine learning, to
rank help message candidates. This algorithm can
be used for training on several data examples hav-
ing a certain order of priority. This attribute fits
for the problem in this paper; it is difficult even
for human annotators to determine the unique ap-
propriate help message to be provided. Target in-
stances x of the algorithm are help message can-
didates corresponding to grammar rules in this pa-
per.
RankBoost trains a score function H(x) and ar-
ranges instances x in the order. Here, H(x′) &lt;
H(x′′) means x′′ is ranked higher than x′. This
score function is defined as a linear combination
of weak rankers giving partial information regard-
ing the order:
</bodyText>
<equation confidence="0.984006333333333">
T
H(x) = E αtht(x)
t
</equation>
<bodyText confidence="0.999493">
where T, ht(), and αt denote the number of boost-
ing iterations, a weak ranker, and its associated
weight, respectively. The weak ranker ht is de-
fined by comparing the value of a feature fi of an
instance x with a threshold θ. That is,
</bodyText>
<equation confidence="0.998825666666667">
1 if fi(x) &gt; θ
0 if fi(x) &lt; θ (1)
qdef if fi(x) = L
</equation>
<bodyText confidence="0.999932">
where qdef E 10, 11. Here, fi(x) denotes the
value of the i-th feature of instance x, and L de-
notes that no value is given in fi(x).
</bodyText>
<sectionHeader confidence="0.992268" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.988499">
5.1 Target Data
</subsectionHeader>
<bodyText confidence="0.999976268292683">
Data were collected by 30 subjects in total by us-
ing a multi-domain spoken dialogue system that
handles five domains such as restaurant, hotel,
sightseeing, bus, and weather (Komatani et al.,
2008). The data consisted of 180 dialogues and
11,733 utterances. Data from five subjects were
used to determine the number of boosting iter-
ations and to improve LMs for ASR. We used
utterances in the restaurant, hotel, and sightsee-
ing domains because the remaining two, bus and
weather, did not have many grammar rules. We
then extracted OOG utterances on the basis of the
grammar verification results to evaluate the per-
formance of our method for such utterances. We
regarded an utterance whose accumulated weight
was negative as OOG. As a result, 1,349 OOG ut-
terances by 25 subjects were used for evaluation,
hereafter. These consisted of 363 utterances in the
restaurant domain, 563 in the hotel domain, and
423 in the sightseeing domain. These data were
collected under the following conditions: subjects
were given no instructions on concrete language
expressions the system accepts. System responses
were made only by speech, and no screen for dis-
playing outputs was used. Subjects were given six
scenarios describing tasks to be completed.
We used Julius1 that is a statistical-LM-based
ASR engine. We constructed class 3-gram LMs
for ASR by using 10,000 sentences generated
from the task grammars and the 600 utterances
collected by the five subjects. The vocabulary
sizes for the restaurant, hotel, and sightseeing do-
mains were 3,456, 2,625, and 3,593, and ASR ac-
curacies for them were 45.8%, 57.1%, and 43.5%,
respectively. These ASR accuracies were not very
high because the target utterances were all OOG.
A set of possible thresholds in the weak rankers
described in Section 4.2 consisted of all feature
values that appeared in the training data. The num-
bers of boosting iterations were determined on the
basis of accuracies for the data by the five sub-
</bodyText>
<footnote confidence="0.869132">
1http://julius.sourceforge.jp/
</footnote>
<equation confidence="0.961059">
ht(x) = I
</equation>
<page confidence="0.985802">
319
</page>
<figure confidence="0.992388">
100%
90%
Accuracy
80%
70%
60%
50%
40%
30%
1-hest 2-hest 3-hest 4-hest 5-hest
N-hest
</figure>
<tableCaption confidence="0.876222">
Table 2: Sum of absolute values of weight α for
each feature
</tableCaption>
<figure confidence="0.6933618">
H7 H17 H19 H2 H6
(H7*H8) (H2*H9)
9.58 6.91 6.61 6.02 6.01
baseline
our method
</figure>
<figureCaption confidence="0.958468">
Figure 5: Accuracy when N candidates were pro-
vided in sightseeing domain (1 &lt; N &lt; 5)
</figureCaption>
<bodyText confidence="0.9988615">
jects. The numbers were 400, 100, and 500 for the
restaurant, hotel, and sightseeing domains.
</bodyText>
<subsectionHeader confidence="0.990891">
5.2 Evaluation Criterion
</subsectionHeader>
<bodyText confidence="0.999775066666667">
We manually gave five help messages correspond-
ing to grammar rules as reference labels per ut-
terance in the order of having a strong relation to
the utterance. The numbers of candidate help mes-
sages were 28, 27, and 23 for the restaurant, hotel
and sightseeing domains, respectively.
We evaluated our ranking method as the accu-
racy where at least one of the reference labels was
contained in its top N candidates. This corre-
sponds to a probability where at least one appro-
priate help message was contained in a list of N
candidates. The accuracy was calculated by 5-fold
cross validation. In the baseline method we set,
help messages were provided only by using the
grammar verification scores.
</bodyText>
<sectionHeader confidence="0.641611" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.994652641025641">
Results in the sightseeing domain are plotted in
Figure 5. We can see that our method outper-
formed the baseline in the accuracies for all N
values. All these differences were statistically sig-
nificant (p &lt; 0.05) by the McNemar test. The ac-
curacies were also better in the other two domains
for all N values, and the average differences for
the three domains were 11.7 points for N=1, 9.7
points for N=2, and 6.7 points for N=3. The dif-
ferences were large especially for small N values.
This result indicates that we can successfully re-
duce the number of help messages when providing
several ones for users. The improvements were
derived from the features we incorporated such as
the estimated user knowledge in addition to gram-
mar verification results. The baseline method was
only based on grammar verification results for sin-
gle utterances, which contained insufficient infor-
mation because OOG utterances were often mis-
recognized or misunderstood.
We also investigated dominant features by cal-
culating the sum of absolute values of final weight
α for each feature in RankBoost. Five dominant
features based on the sums are shown in Table
2. These five features include a feature obtained
from grammar verification result (H2), a feature
about the user’s utterance history (H6), a feature
representing estimated user knowledge (H7), and
features representing properties of the utterances.
The most dominant feature was H7, which ap-
peared twice in this table. This was because user
utterances were not likely to be OOG utterances
again after the user had already known an expres-
sion corresponding to the grammar rule, which can
be detected when user utterances for it were cor-
rectly accepted, that is, its grammar verification
score was high. The second dominant feature was
H2, which showed that grammar verification re-
sults worked effectively.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999831238095238">
We addressed an issue of OOG utterances in spo-
ken dialogue systems by generating help mes-
sages. To manage situations when a user utter-
ance could not be accepted, we robustly estimated
a user’s intention as a grammar rule that the user
intends to use. We furthermore integrated various
information as well as the grammar verification
results for complementing missing information in
single utterances, and then ranked help message
candidates corresponding to the grammar rules for
efficiently providing them.
Our future work includes the following. The
evaluation in this paper was taken place only on
the basis of utterances collected beforehand. Pro-
viding help messages itself should be evaluated by
another experiment through dialogues. Further-
more, we assumed that language expressions of
help messages to show an example language ex-
pression were fixed. We also need to investigate
what kind of expression is more helpful to novice
users.
</bodyText>
<page confidence="0.996048">
320
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999937819672131">
Yoav Freund, Raj D. Iyer, Robert E. Schapire, and
Yoram Singer. 2003. An efficient boosting algo-
rithm for combining preferences. Journal of Ma-
chine Learning Research, 4:933–969.
Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya
Ogata, and Hiroshi G. Okuno. 2006. Dynamic
help generation by estimating user’s mental model in
spoken dialogue systems. In Proc. Int’l Conf. Spo-
ken Language Processing (INTERSPEECH), pages
1946–1949.
Genevieve Gorrell, Ian Lewin, and Manny Rayner.
2002. Adding intelligent help to mixed-initiative
spoken dialogue systems. In Proc. Int’l Conf. Spo-
ken Language Processing (ICSLP), pages 2065–
2068.
Alexander Gruenstein and Stephanie Seneff. 2007.
Releasing a multimodal dialogue system into the
wild: User support mechanisms. In Proc. 8th SIG-
dial Workshop on Discourse and Dialogue, pages
111–119.
Melanie Hartmann and Daniel Schreiber. 2008. Proac-
tively adapting interfaces to individual users for mo-
bile devices. In Adaptive Hypermedia and Adap-
tive Web-Based Systems, 5th International Confer-
ence (AH 2008), volume 5149 of Lecture Notes in
Computer Science, pages 300–303. Springer.
Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, James Hieronymus, Alexander
Gruenstein, and John Dowding. 2003. Targeted
help for spoken dialogue systems: intelligent feed-
back improves naive users’ performance. In Proc.
10th Conf. of the European Chapter of the ACL
(EACL2003), pages 147–154.
Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G.
Okuno. 2007. Analyzing temporal transition of real
user’s behaviors in a spoken dialogue system. In
Proc. INTERSPEECH, pages 142–145.
Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata,
and Hiroshi G. Okuno. 2008. Managing out-of-
grammar utterances by topic estimation with domain
extensibility in multi-domain spoken dialogue sys-
tems. Speech Communication, 50(10):863–870.
Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawa-
hara. 2004. Real-time word confidence scoring us-
ing local posterior probabilities on tree trellis search.
In IEEE Int’l Conf. Acoust., Speech &amp; Signal Pro-
cessing (ICASSP), volume 1, pages 793–796.
Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and
Gary Guenbae Lee. 2007. Example-based error re-
covery strategy for spoken dialog system. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU), pages 538–543.
Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and
Kiyohiro Shikano. 2005. Operating a public spo-
ken guidance system in real environment. In Proc.
European Conf. Speech Commun. &amp; Tech. (EU-
ROSPEECH), pages 845–848.
Antoine Raux, Dan Bohus, Brian Langner, Alan W.
Black, and Maxine Eskenazi. 2006. Doing research
on a deployed spoken dialogue system: One year of
Let’s Go! experience. In Proc. INTERSPEECH.
</reference>
<page confidence="0.998768">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239840">
<title confidence="0.9996245">Ranking Help Message Candidates Based on Robust Verification Results and Utterance History in Spoken Dialogue Systems</title>
<author confidence="0.97416">Kazunori Komatani Satoshi Ikeda Yuichiro</author>
<affiliation confidence="0.614637666666667">Tetsuya Ogata Hiroshi G. Graduate School of Kyoto</affiliation>
<address confidence="0.435002">Yoshida-Hommachi, Sakyo, Kyoto 606-8501,</address>
<abstract confidence="0.996636590909091">We address an issue of out-of-grammar (OOG) utterances in spoken dialogue systems by generating help messages for novice users. Help generation for OOG utterances is a challenging problem because language understanding (LU) results based on automatic speech recognition (ASR) results for such utterances are always erroneous as important words are often misrecognized or missed from such utterances. We first develop grammar verification for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identifies a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar verification results and the utterance history representing the user’s experience.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj D Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--933</pages>
<contexts>
<context position="4978" citStr="Freund et al., 2003" startWordPosition="783" endWordPosition="786">e first issue. The if getting information on a site. grammar verification method robustly estimates The help message corresponding to [Obtaining info which a grammar rule is intended to use by a on a site] is provided. user’s utterance. The WFST is automatically U2: Tell me your recommending sites. generated to represent an ASR result in which any The user repeats the same utterance probably bepossibility of error is taken into consideration. We cause the help message (S1) was not helpful. The furthermore adopt a boosting algorithm, Rank- estimated grammar is [Obtaining info on a site] Boost (Freund et al., 2003), to put help messages again. in order of probability to address the second issue. S2: I did not understand. You can say “Search Because it is difficult even for human annotators shrines or museums” for example, if searching to uniquely determine which help message should tourist sites. be provided for each case, we adopt an algorithm Another help message corresponding to [Searching that can be used for training on several data tourist sites] is provided after ranking candidates examples that have a certain order of priority. by also using the user’s utterance history. We also incorporate feat</context>
<context position="19615" citStr="Freund et al., 2003" startWordPosition="3289" endWordPosition="3292">tend to appear successively because they are used when users try and compare several query conditions; on the other hand, utterances in “command” class tend to appear independently of the context. Features H11 to H31 are the products of features H8, H9, and H10 and each feature from H1 to H7. These were defined to consider combinations of properties of utterances represented by H8, H9, and H10 and their reliability represented by H1 to H7, because RankBoost 318 Figure 4: Outline of our ranking method for help message candidates does not consider them. 4.2 Ranking Algorithm We adopt RankBoost (Freund et al., 2003), a boosting algorithm based on machine learning, to rank help message candidates. This algorithm can be used for training on several data examples having a certain order of priority. This attribute fits for the problem in this paper; it is difficult even for human annotators to determine the unique appropriate help message to be provided. Target instances x of the algorithm are help message candidates corresponding to grammar rules in this paper. RankBoost trains a score function H(x) and arranges instances x in the order. Here, H(x′) &lt; H(x′′) means x′′ is ranked higher than x′. This score fu</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 2003</marker>
<rawString>Yoav Freund, Raj D. Iyer, Robert E. Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuichiro Fukubayashi</author>
<author>Kazunori Komatani</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Dynamic help generation by estimating user’s mental model in spoken dialogue systems.</title>
<date>2006</date>
<booktitle>In Proc. Int’l Conf. Spoken Language Processing (INTERSPEECH),</booktitle>
<pages>1946--1949</pages>
<marker>Fukubayashi, Komatani, Ogata, Okuno, 2006</marker>
<rawString>Yuichiro Fukubayashi, Kazunori Komatani, Tetsuya Ogata, and Hiroshi G. Okuno. 2006. Dynamic help generation by estimating user’s mental model in spoken dialogue systems. In Proc. Int’l Conf. Spoken Language Processing (INTERSPEECH), pages 1946–1949.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genevieve Gorrell</author>
<author>Ian Lewin</author>
<author>Manny Rayner</author>
</authors>
<title>Adding intelligent help to mixed-initiative spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In Proc. Int’l Conf. Spoken Language Processing (ICSLP),</booktitle>
<pages>pages</pages>
<contexts>
<context position="5783" citStr="Gorrell et al. (2002)" startWordPosition="909" endWordPosition="912">es or museums” for example, if searching to uniquely determine which help message should tourist sites. be provided for each case, we adopt an algorithm Another help message corresponding to [Searching that can be used for training on several data tourist sites] is provided after ranking candidates examples that have a certain order of priority. by also using the user’s utterance history. We also incorporate features representing the user’s utterance history for preventing message repetition. 2 Related Work Various studies have been done on generating help messages in spoken dialogue systems. Gorrell et al. (2002) trained a decision tree to classify causes of errors for OOG utterances. Hockey et al. (2003) also classified OOG utterances into the three categories of endpointing errors, unknown vocabulary, and subcategorization mistakes, by comparing two kinds of ASR results. This was called Targeted Help and provided a user with immediate feedback tailored to what the user said. Lee et al. (2007) also addressed error recovery by generating help messages in an example-based dialog modeling framework. These studies, however, determined what help messages should be provided mainly on the basis of literal A</context>
<context position="7254" citStr="Gorrell et al., 2002" startWordPosition="1148" endWordPosition="1151">ed by our method, especially the part of the method described in Section 4, is shown in Figure 1. Here, user utterances are transcriptions, and utterance numbers 315 [] denotes grammar rules. Figure 1: Example dialogue enabled by our method start with “S” and “U” denote system and user utterances, respectively. In this example, ASR results for the user utterances (U1 and U2) do not contain sufficient information because the utterances are short and contain out-of-vocabulary words. These two results are similar, and accordingly, the help message after U2 provided by methods like Targeted Help (Gorrell et al., 2002; Hockey et al., 2003) is the same as Utterance S1 because they are only based on ASR results. Our method can provide different help messages as Utterance S2 after ranking candidates by considering the utterance history and grammar verification results. Because the candidates are arranged in the order of probability, the most appropriate help message can be provided in fewer attempts. This ranking method for help message candidates is also useful in multimodal interfaces with speech input. Help messages are necessary when ASR is used as its input modality, and such messages were actually imple</context>
</contexts>
<marker>Gorrell, Lewin, Rayner, 2002</marker>
<rawString>Genevieve Gorrell, Ian Lewin, and Manny Rayner. 2002. Adding intelligent help to mixed-initiative spoken dialogue systems. In Proc. Int’l Conf. Spoken Language Processing (ICSLP), pages 2065– 2068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>Stephanie Seneff</author>
</authors>
<title>Releasing a multimodal dialogue system into the wild: User support mechanisms.</title>
<date>2007</date>
<booktitle>In Proc. 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>111--119</pages>
<contexts>
<context position="7906" citStr="Gruenstein and Seneff, 2007" startWordPosition="1254" endWordPosition="1257">s the same as Utterance S1 because they are only based on ASR results. Our method can provide different help messages as Utterance S2 after ranking candidates by considering the utterance history and grammar verification results. Because the candidates are arranged in the order of probability, the most appropriate help message can be provided in fewer attempts. This ranking method for help message candidates is also useful in multimodal interfaces with speech input. Help messages are necessary when ASR is used as its input modality, and such messages were actually implemented in City Browser (Gruenstein and Seneff, 2007), for example. This system lists template-based help messages on the screen by using ASR results and internal states of the system. The order of help messages is important, especially in portable devices with a small screen, on which the number of help messages displayed at one time is limited, as Hartmann and Schreiber (2008) pointed out. Even in cases where sufficiently large screens are available, too many help messages without any order will distract the user’s attention and thus spoil its usability. 3 Grammar Verification based on WFST We estimate a user’s intention even from OOG utteranc</context>
</contexts>
<marker>Gruenstein, Seneff, 2007</marker>
<rawString>Alexander Gruenstein and Stephanie Seneff. 2007. Releasing a multimodal dialogue system into the wild: User support mechanisms. In Proc. 8th SIGdial Workshop on Discourse and Dialogue, pages 111–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Hartmann</author>
<author>Daniel Schreiber</author>
</authors>
<title>Proactively adapting interfaces to individual users for mobile devices.</title>
<date>2008</date>
<booktitle>In Adaptive Hypermedia and Adaptive Web-Based Systems, 5th International Conference (AH</booktitle>
<volume>5149</volume>
<pages>300--303</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8234" citStr="Hartmann and Schreiber (2008)" startWordPosition="1310" endWordPosition="1313">e can be provided in fewer attempts. This ranking method for help message candidates is also useful in multimodal interfaces with speech input. Help messages are necessary when ASR is used as its input modality, and such messages were actually implemented in City Browser (Gruenstein and Seneff, 2007), for example. This system lists template-based help messages on the screen by using ASR results and internal states of the system. The order of help messages is important, especially in portable devices with a small screen, on which the number of help messages displayed at one time is limited, as Hartmann and Schreiber (2008) pointed out. Even in cases where sufficiently large screens are available, too many help messages without any order will distract the user’s attention and thus spoil its usability. 3 Grammar Verification based on WFST We estimate a user’s intention even from OOG utterances as a grammar rule that the user intends to use by his utterance. We call this estimation grammar verification. This process is applied to ASR outputs based on a statistical language model (LM) in this paper. We use two transducers: a finite-state transducer (FST) representing the task grammar, and weighted FST (WFST) repres</context>
</contexts>
<marker>Hartmann, Schreiber, 2008</marker>
<rawString>Melanie Hartmann and Daniel Schreiber. 2008. Proactively adapting interfaces to individual users for mobile devices. In Adaptive Hypermedia and Adaptive Web-Based Systems, 5th International Conference (AH 2008), volume 5149 of Lecture Notes in Computer Science, pages 300–303. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth A Hockey</author>
<author>Oliver Lemon</author>
<author>Ellen Campana</author>
<author>Laura Hiatt</author>
<author>Gregory Aist</author>
<author>James Hieronymus</author>
<author>Alexander Gruenstein</author>
<author>John Dowding</author>
</authors>
<title>Targeted help for spoken dialogue systems: intelligent feedback improves naive users’ performance.</title>
<date>2003</date>
<booktitle>In Proc. 10th Conf. of the European Chapter of the ACL (EACL2003),</booktitle>
<pages>147--154</pages>
<contexts>
<context position="5877" citStr="Hockey et al. (2003)" startWordPosition="925" endWordPosition="928">t sites. be provided for each case, we adopt an algorithm Another help message corresponding to [Searching that can be used for training on several data tourist sites] is provided after ranking candidates examples that have a certain order of priority. by also using the user’s utterance history. We also incorporate features representing the user’s utterance history for preventing message repetition. 2 Related Work Various studies have been done on generating help messages in spoken dialogue systems. Gorrell et al. (2002) trained a decision tree to classify causes of errors for OOG utterances. Hockey et al. (2003) also classified OOG utterances into the three categories of endpointing errors, unknown vocabulary, and subcategorization mistakes, by comparing two kinds of ASR results. This was called Targeted Help and provided a user with immediate feedback tailored to what the user said. Lee et al. (2007) also addressed error recovery by generating help messages in an example-based dialog modeling framework. These studies, however, determined what help messages should be provided mainly on the basis of literal ASR results. Therefore, help messages would be degraded by ASR results in which a lot of inform</context>
<context position="7276" citStr="Hockey et al., 2003" startWordPosition="1152" endWordPosition="1155">cially the part of the method described in Section 4, is shown in Figure 1. Here, user utterances are transcriptions, and utterance numbers 315 [] denotes grammar rules. Figure 1: Example dialogue enabled by our method start with “S” and “U” denote system and user utterances, respectively. In this example, ASR results for the user utterances (U1 and U2) do not contain sufficient information because the utterances are short and contain out-of-vocabulary words. These two results are similar, and accordingly, the help message after U2 provided by methods like Targeted Help (Gorrell et al., 2002; Hockey et al., 2003) is the same as Utterance S1 because they are only based on ASR results. Our method can provide different help messages as Utterance S2 after ranking candidates by considering the utterance history and grammar verification results. Because the candidates are arranged in the order of probability, the most appropriate help message can be provided in fewer attempts. This ranking method for help message candidates is also useful in multimodal interfaces with speech input. Help messages are necessary when ASR is used as its input modality, and such messages were actually implemented in City Browser</context>
</contexts>
<marker>Hockey, Lemon, Campana, Hiatt, Aist, Hieronymus, Gruenstein, Dowding, 2003</marker>
<rawString>Beth A. Hockey, Oliver Lemon, Ellen Campana, Laura Hiatt, Gregory Aist, James Hieronymus, Alexander Gruenstein, and John Dowding. 2003. Targeted help for spoken dialogue systems: intelligent feedback improves naive users’ performance. In Proc. 10th Conf. of the European Chapter of the ACL (EACL2003), pages 147–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Tatsuya Kawahara</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Analyzing temporal transition of real user’s behaviors in a spoken dialogue system.</title>
<date>2007</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>142--145</pages>
<contexts>
<context position="1361" citStr="Komatani et al., 2007" startWordPosition="192" endWordPosition="196">or missed from such utterances. We first develop grammar verification for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identifies a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar verification results and the utterance history representing the user’s experience. 1 Introduction Studies on spoken dialogue systems have recently proceeded from in-laboratory systems to ones deployed to the open public (Raux et al., 2006; Komatani et al., 2007; Nisimura et al., 2005). Accordingly, opportunities are increasing as general citizens use the systems. This situation means that novice users directly access the systems with no instruction, which is quite different from inlaboratory experiments where some instructions can be given. In such cases, users often experience situations where their utterances are not correctly recognized. This is because of a gap between the actual system and a user’s mental model, that is, a user’s expectation of the system. Actually, a user’s utterance often cannot be interpreted by the system because of the sys</context>
</contexts>
<marker>Komatani, Kawahara, Okuno, 2007</marker>
<rawString>Kazunori Komatani, Tatsuya Kawahara, and Hiroshi G. Okuno. 2007. Analyzing temporal transition of real user’s behaviors in a spoken dialogue system. In Proc. INTERSPEECH, pages 142–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Satoshi Ikeda</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Managing out-ofgrammar utterances by topic estimation with domain extensibility in multi-domain spoken dialogue systems.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<issue>10</issue>
<contexts>
<context position="21001" citStr="Komatani et al., 2008" startWordPosition="3543" endWordPosition="3546">er of boosting iterations, a weak ranker, and its associated weight, respectively. The weak ranker ht is defined by comparing the value of a feature fi of an instance x with a threshold θ. That is, 1 if fi(x) &gt; θ 0 if fi(x) &lt; θ (1) qdef if fi(x) = L where qdef E 10, 11. Here, fi(x) denotes the value of the i-th feature of instance x, and L denotes that no value is given in fi(x). 5 Experimental Evaluation 5.1 Target Data Data were collected by 30 subjects in total by using a multi-domain spoken dialogue system that handles five domains such as restaurant, hotel, sightseeing, bus, and weather (Komatani et al., 2008). The data consisted of 180 dialogues and 11,733 utterances. Data from five subjects were used to determine the number of boosting iterations and to improve LMs for ASR. We used utterances in the restaurant, hotel, and sightseeing domains because the remaining two, bus and weather, did not have many grammar rules. We then extracted OOG utterances on the basis of the grammar verification results to evaluate the performance of our method for such utterances. We regarded an utterance whose accumulated weight was negative as OOG. As a result, 1,349 OOG utterances by 25 subjects were used for evalu</context>
</contexts>
<marker>Komatani, Ikeda, Ogata, Okuno, 2008</marker>
<rawString>Kazunori Komatani, Satoshi Ikeda, Tetsuya Ogata, and Hiroshi G. Okuno. 2008. Managing out-ofgrammar utterances by topic estimation with domain extensibility in multi-domain spoken dialogue systems. Speech Communication, 50(10):863–870.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akinobu Lee</author>
<author>Kiyohiro Shikano</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Real-time word confidence scoring using local posterior probabilities on tree trellis search.</title>
<date>2004</date>
<booktitle>In IEEE Int’l Conf. Acoust., Speech &amp; Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>793--796</pages>
<contexts>
<context position="13200" citStr="Lee et al., 2004" startWordPosition="2187" endWordPosition="2190">ted weight for a single utterance is defined as the sum of these weights as shown be316 Figure 2: Example of input WFST and grammar FST low. �w = �wacc + (wsub + wdel + wins) Eaccepted Eerror Here, Eaccepted denotes a set of accepted words corresponding to elements of each grammar rule, and Eerror denotes a set of words that are not accepted and that have either error symbol. Note that the weights are not given beforehand but are calculated and given to the input WFST in runtime according to each ASR result. A weight for an accepted word easr is defined by using its confidence score CM(easr) (Lee et al., 2004) and its word length. A word length in mora is denoted as l(·), which is normalized by that of the longest word in the vocabulary. wacc = CM(easr)l(easr) This weight wacc gives preference to sequences containing longer words with higher confidence scores. Weights for each type of error have negative values because they are penalties: wsub = −{CM(easr)l(easr) + l(egram)}/2 wdel = −{l(e) + l(egram)}/2 wins = −{CM(easr)l(easr) + l(e)}/2 where l(e) is the average word length in the vocabulary and egram is a grammar element i.e., either a word or a class. A deletion error is a case when a grammar e</context>
</contexts>
<marker>Lee, Shikano, Kawahara, 2004</marker>
<rawString>Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara. 2004. Real-time word confidence scoring using local posterior probabilities on tree trellis search. In IEEE Int’l Conf. Acoust., Speech &amp; Signal Processing (ICASSP), volume 1, pages 793–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cheongjae Lee</author>
<author>Sangkeun Jung</author>
<author>Donghyeon Lee</author>
<author>Gary Guenbae Lee</author>
</authors>
<title>Example-based error recovery strategy for spoken dialog system.</title>
<date>2007</date>
<booktitle>In Proc. of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),</booktitle>
<pages>538--543</pages>
<contexts>
<context position="6172" citStr="Lee et al. (2007)" startWordPosition="971" endWordPosition="974">e also incorporate features representing the user’s utterance history for preventing message repetition. 2 Related Work Various studies have been done on generating help messages in spoken dialogue systems. Gorrell et al. (2002) trained a decision tree to classify causes of errors for OOG utterances. Hockey et al. (2003) also classified OOG utterances into the three categories of endpointing errors, unknown vocabulary, and subcategorization mistakes, by comparing two kinds of ASR results. This was called Targeted Help and provided a user with immediate feedback tailored to what the user said. Lee et al. (2007) also addressed error recovery by generating help messages in an example-based dialog modeling framework. These studies, however, determined what help messages should be provided mainly on the basis of literal ASR results. Therefore, help messages would be degraded by ASR results in which a lot of information was missing, especially for OOG utterances. The same help messages would be repeated when the same ASR results were obtained. An example dialogue enabled by our method, especially the part of the method described in Section 4, is shown in Figure 1. Here, user utterances are transcriptions</context>
</contexts>
<marker>Lee, Jung, Lee, Lee, 2007</marker>
<rawString>Cheongjae Lee, Sangkeun Jung, Donghyeon Lee, and Gary Guenbae Lee. 2007. Example-based error recovery strategy for spoken dialog system. In Proc. of IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 538–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichi Nisimura</author>
<author>Akinobu Lee</author>
<author>Masashi Yamada</author>
<author>Kiyohiro Shikano</author>
</authors>
<title>Operating a public spoken guidance system in real environment. In</title>
<date>2005</date>
<booktitle>Proc. European Conf. Speech Commun. &amp; Tech. (EUROSPEECH),</booktitle>
<pages>845--848</pages>
<contexts>
<context position="1385" citStr="Nisimura et al., 2005" startWordPosition="197" endWordPosition="200">erances. We first develop grammar verification for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identifies a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar verification results and the utterance history representing the user’s experience. 1 Introduction Studies on spoken dialogue systems have recently proceeded from in-laboratory systems to ones deployed to the open public (Raux et al., 2006; Komatani et al., 2007; Nisimura et al., 2005). Accordingly, opportunities are increasing as general citizens use the systems. This situation means that novice users directly access the systems with no instruction, which is quite different from inlaboratory experiments where some instructions can be given. In such cases, users often experience situations where their utterances are not correctly recognized. This is because of a gap between the actual system and a user’s mental model, that is, a user’s expectation of the system. Actually, a user’s utterance often cannot be interpreted by the system because of the system’s limited grammar fo</context>
</contexts>
<marker>Nisimura, Lee, Yamada, Shikano, 2005</marker>
<rawString>Ryuichi Nisimura, Akinobu Lee, Masashi Yamada, and Kiyohiro Shikano. 2005. Operating a public spoken guidance system in real environment. In Proc. European Conf. Speech Commun. &amp; Tech. (EUROSPEECH), pages 845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Dan Bohus</author>
<author>Brian Langner</author>
<author>Alan W Black</author>
<author>Maxine Eskenazi</author>
</authors>
<title>Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience.</title>
<date>2006</date>
<booktitle>In Proc. INTERSPEECH.</booktitle>
<contexts>
<context position="1338" citStr="Raux et al., 2006" startWordPosition="188" endWordPosition="191">ften misrecognized or missed from such utterances. We first develop grammar verification for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identifies a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar verification results and the utterance history representing the user’s experience. 1 Introduction Studies on spoken dialogue systems have recently proceeded from in-laboratory systems to ones deployed to the open public (Raux et al., 2006; Komatani et al., 2007; Nisimura et al., 2005). Accordingly, opportunities are increasing as general citizens use the systems. This situation means that novice users directly access the systems with no instruction, which is quite different from inlaboratory experiments where some instructions can be given. In such cases, users often experience situations where their utterances are not correctly recognized. This is because of a gap between the actual system and a user’s mental model, that is, a user’s expectation of the system. Actually, a user’s utterance often cannot be interpreted by the sy</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>Antoine Raux, Dan Bohus, Brian Langner, Alan W. Black, and Maxine Eskenazi. 2006. Doing research on a deployed spoken dialogue system: One year of Let’s Go! experience. In Proc. INTERSPEECH.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>