<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.998158">
Boosting Transition-based AMR Parsing with Refined Actions and
Auxiliary Analyzers
</title>
<author confidence="0.996342">
Chuan Wang Nianwen Xue Sameer Pradhan
</author>
<affiliation confidence="0.998129">
Brandeis University Brandeis University Boulder Language Technologies
</affiliation>
<email confidence="0.988273">
cwang24@brandeis.edu xuen@brandeis.edu pradhan@bltek.com
</email>
<sectionHeader confidence="0.993645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99968675">
We report improved AMR parsing results
by adding a new action to a transition-
based AMR parser to infer abstract con-
cepts and by incorporating richer features
produced by auxiliary analyzers such as
a semantic role labeler and a coreference
resolver. We report final AMR parsing
results that show an improvement of 7%
absolute in F1 score over the best pre-
viously reported result. Our parser is
available at: https://github.com/
Juicechuan/AMRParsing
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945411764706">
AMR parsing is the task of taking a sentence as
input and producing as output an Abstract Mean-
ing Representation (AMR) that is a rooted, di-
rected, edge-labeled and leaf-labeled graph that is
used to represent the meaning of a sentence (Ba-
narescu et al., 2013). AMR parsing has drawn
an increasing amount of attention recently. The
first published AMR parser, JAMR (Flanigan et
al., 2014), performs AMR parsing in two stages:
concept identification and relation identification.
Flanigan et al. (2014) treat concept identification
as a sequence labeling task and utilize a semi-
Markov model to map spans of words in a sen-
tence to concept graph fragments. For relation
identification, they adopt graph-based techniques
similar to those used in dependency parsing (Mc-
Donald et al., 2005). Instead of finding maximum
spanning trees (MST) over words, they propose an
algorithm that finds the maximum spanning con-
nected subgraph (MSCG) over concept fragments
identified in the first stage.
A competitive alternative to the MSCG ap-
proach is transition-based AMR parsing. Our
previous work (Wang et al., 2015) describes a
transition-based system that also involves two
stages. In the first step, an input sentence is
parsed into a dependency tree with a dependency
parser. In the second step, the transition-based
AMR parser transforms the dependency tree into
an AMR graph by performing a series of actions.
Note that the dependency parser used in the first
step can be any off-the-shelf dependency parser
and does not have to trained on the same data set
as used in the second step.
</bodyText>
<figure confidence="0.521059">
ARG0 ARG1
</figure>
<figureCaption confidence="0.716495">
Figure 1: An example showing abstract concept
have-org-role-91 for the sentence “Israel
foreign minister visits South Korea.”
</figureCaption>
<bodyText confidence="0.997728333333333">
Unlike a dependency parse where each leaf
node corresponds to a word in a sentence and there
is an inherent alignment between the words in a
sentence and the leaf nodes in the parse tree, the
alignment between the word tokens in a sentence
and the concepts in an AMR graph is non-trivial.
Both JAMR and our transition-based parser rely
on a heuristics based aligner that can align the
words in a sentence and concepts in its AMR with
a 90% F1 score, but there are some concepts in
the AMR that cannot be aligned to any word in a
sentence.
This is illustrated in Figure 1 where the concept
have-org-role-91 is not aligned to any word
or word sequence. We refer to these concepts as
abstract concepts, and existing AMR parsers do
not have a systematic way of inferring such ab-
stract concepts.
</bodyText>
<figure confidence="0.998285789473684">
visit-01
ARG0-of
name
have-org-role-91
name
op1 op2
“South”
ARG1
ARG2
country
name
name
minister
mod
“Korea”
op1
foreign
“Israel”
person country
</figure>
<page confidence="0.799076">
857
</page>
<bodyText confidence="0.965343911111111">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 857–862,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
Current AMR parsers are in their early stages of
development, and their features are not yet fully
developed. For example, the AMR makes heavy
use of the framesets and semantic role labels used
in the Proposition Bank (Palmer et al., 2005), and
it would seem that information produced by a se-
mantic role labeling system trained on the Prop-
Bank can be used as features to improve the AMR
parsing accuracy. Similarly, since AMR repre-
sents limited within-sentence coreference, coref-
erence information produced by an off-the-shelf
coreference system should benefit the AMR parser
as well.
In this paper, we describe an extension to our
transition-based AMR parser (Wang et al., 2015)
by adding a new action to infer the abstract
concepts in an AMR, and new features derived
from an off-the-shelf semantic role labeling sys-
tem (Pradhan et al., 2004) and coreference system
(Lee et al., 2013). We also experimented with
adding Brown clusters as features to the AMR
parser. Additionally, we experimented with us-
ing different syntactic parsers in the first stage.
Following our previous work, we use the aver-
aged perceptron algorithm (Collins, 2002) to train
the parameters of the model and use the greedy
parsing strategy during decoding to determine the
best action sequence to apply for each training in-
stance. Our results show that (i) the transition-
based AMR parser is very stable across the dif-
ferent parsers used in the first stage, (ii) adding
the new action significantly improves the parser
performance, and (iii) semantic role information
is beneficial to AMR parsing when used as fea-
tures, while the Brown clusters do not make a dif-
ference and coreference information slightly hurts
the AMR parsing performance.
The rest of the paper is organized as follows. In
Section 2 we briefly describe the transition-based
parser, and in Section 3 we describe our exten-
sions. We report experimental results in Section
4 and conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.95991" genericHeader="method">
2 Transition-based AMR Parser
</sectionHeader>
<bodyText confidence="0.9894601">
The transition-based parser first uses a depen-
dency parser to parse an input sentence, and then
performs a limited number of highly general ac-
tions to transform the dependency tree to an AMR
graph. The transition actions are briefly described
below but due to the limited space, we cannot
describe the full details of these actions, and the
reader is referred to our previous work (Wang et
al., 2015) for detailed descriptions of these ac-
tions:
</bodyText>
<listItem confidence="0.9847395">
• NEXT-EDGE-lr (ned): Assign the current
edge with edge label lr and go to next edge.
• SWAP-lr (sw): Swap the current edge, make
the current dependent as the new head, and
assign edge label lr to the swapped edge.
• REATTACHk-lr (reat): Reattach current de-
pendent to node k and assign edge label lr.
• REPLACE-HEAD (rph): Replace current head
node with current dependent node.
• REENTRANCEk-lr (reen): Add another head
node k to current dependent and assign label
lr to edge between k and current dependent.
• MERGE (mrg): Merge two nodes connected
by the edge into one node.
</listItem>
<bodyText confidence="0.892069">
From each node in the dependency tree, the parser
performs the following 2 actions:
</bodyText>
<listItem confidence="0.997906333333333">
• NEXT-NODE-lc (nnd): Assign the current
node with concept label lc and go to next
node.
• DELETE-NODE (dnd): Delete the current
node and all edges associated with current
node.
</listItem>
<bodyText confidence="0.995700666666667">
Crucially, none of these actions can infer the
types of abstract concepts illustrated in Figure 1.
And this serves as our baseline parser.
</bodyText>
<figure confidence="0.43838">
ARG0-of
</figure>
<figureCaption confidence="0.887054">
Figure 2: Enhanced Span Graph for AMR in Fig-
ure 1, “Israel foreign minister visits South Korea.”
sx,y corresponds to sentence span (x, y).
</figureCaption>
<sectionHeader confidence="0.996842" genericHeader="method">
3 Parser Extensions
</sectionHeader>
<subsectionHeader confidence="0.999261">
3.1 Inferring Abstract Concepts
</subsectionHeader>
<bodyText confidence="0.999897333333333">
We previously create the learning target by repre-
senting an AMR graph as a Span Graph, where
each AMR concept is annotated with the text span
</bodyText>
<figure confidence="0.990701222222222">
ARG0 ARG1
person s5,7:country+name
s4,5:visit-01
s0,1:ROOT
ARG1 ARG2
s1,2:country+name s3,4:minister
mod
s2,3:foreign
have-org-role-91
</figure>
<page confidence="0.990109">
858
</page>
<bodyText confidence="0.999395588235294">
of the word or the (contiguous) word sequence it is
aligned to. However, abstract concepts that are not
aligned to any word or word sequence are simply
ignored and are unreachable during training. To
address this, we construct the span graph by keep-
ing the abstract concepts as they are in the AMR
graph, as illustrated in Figure 2.
In order to predict these abstract concepts, we
design an INFER-l, action that is applied in the fol-
lowing way: when the parser visits an node in de-
pendency tree, it inserts an abstract node with con-
cept label l, right between the current node and its
parent. For example in Figure 3, after applying ac-
tion INFER-have-org-role-91 on node min-
ister, the abstract concept is recovered and subse-
quent actions can be applied to transform the sub-
graph to its correct AMR.
</bodyText>
<figureCaption confidence="0.978611">
Figure 3: INFER-have-org-role-91 action
</figureCaption>
<subsectionHeader confidence="0.997307">
3.2 Feature Enrichment
</subsectionHeader>
<bodyText confidence="0.998269916666667">
In our previous work, we only use simple lexi-
cal features and structural features. We extend the
feature set to include (i) features generated by a
semantic role labeling system—ASSERT (Prad-
han et al., 2004), including a frameset disam-
biguator trained using a word sense disambigua-
tion system—IMS (Zhong and Ng, 2010) and a
coreference system (Lee et al., 2013) and (ii) fea-
tures generated using semi-supervised word clus-
ters (Turian et al., 2010; Koo et al., 2008).
Coreference features Coreference is typically
represented as a chain of mentions realized as
noun phrases or pronouns. AMR, on the other
hand, represents coreference as re-entrance and
uses one concept to represent all co-referring enti-
ties. To use the coreference information to inform
AMR parsing actions, we design the following two
features: 1) SHARE DEPENDENT. When applying
REENTRANCEk-lr action on edge (a, b), we check
whether the corresponding head node k of a candi-
date concept has any dependent node that co-refers
with current dependent b. 2) DEPENDENT LABEL.
If SHARE DEPENDENT is true for head node k and
assuming k’s dependent m co-refers with the cur-
rent dependent, the value of this feature is set to
the dependency label between k and m.
For example, for the partial graph shown in Fig-
ure 4, when examining edge (wants, boy), we
may consider REENTRANCEbelieve-ARG1 as one
of the candidate actions. The candidate head
believe has dependent him which is co-referred
with current dependent boy, therefore the value of
feature SHARE DEPENDENT is set to true for this
candidate action. Also the value of feature DE-
PENDENT LABEL is dobj given the dependency la-
bel between (believe, him).
</bodyText>
<table confidence="0.2324904">
semantic role labeling:
wants, want-01, ARG0:the boy, ARG1:the girl to believe him
coreference chain: {boy, him}
For action NEXT-NODE-want-01
EQ FRAMESET: true
</table>
<figureCaption confidence="0.94296">
Figure 4: An example of coreference feature and
</figureCaption>
<bodyText confidence="0.95252888">
semantic role labeling feature in partial parsing
graph of sentence,“The boy wants the girl to be-
lieve him.”
Semantic role labeling features We use the
following semantic role labeling features: 1)
EQ FRAMESET. For action that predicts the con-
cept label (NEXT-NODE-l,), we check whether the
candidate concept label l, matches the frameset
predicted by the semantic role labeler. For ex-
ample, for partial graph in Figure 4, when we
examine node wants, one of the candidate ac-
tions would be NEXT-NODE-want-01. Since
the candidate concept label want-01 is equal
to node wants’s frameset want-01 as predicted
by the semantic role labeler, the value of feature
EQ FRAMESET is set to true. 2) IS ARGUMENT.
For actions that predicts the edge label, we check
whether the semantic role labeler predicts that the
current dependent is an argument of the current
head. Note that the arguments in semantic role la-
beler output are non-terminals which corresponds
to a sentence span. Here we simply take the head
word in the sentence span as the argument.
Word Clusters For the semi-supervised word
cluster feature, we use Brown clusters, more
</bodyText>
<figure confidence="0.985021285714286">
Israel foreign
visits
minister
Israel foreign
visits
have-org-role-91
minister
boy believe
wants
ARG1
girl him
For action REENTRANCEbelieve-ARG1
SHARE DEPENDENT: true
DEPENDENT LABEL: dobj
</figure>
<page confidence="0.997159">
859
</page>
<bodyText confidence="0.990689">
specifically, 1000 classes word cluster trained
by Turian et al. (2010). We use prefixes of lengths
4,6,10,20 of the word’s bit-string as features.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99930775">
We first tune and evaluate our system on the
newswire section of LDC2013E117 dataset. Then
we show our parser’s performance on the recent
LDC2014T12 dataset.
</bodyText>
<subsectionHeader confidence="0.996269">
4.1 Experiments on LDC2013E117
</subsectionHeader>
<bodyText confidence="0.998738714285714">
We first conduct our experiments on the
newswire section of AMR annotation corpus
(LDC2013E117). The train/dev/test split of
dataset is 4.0K/2.1K/2.1K, which is identical to
the settings of JAMR. We evaluate our parser with
Smatch v2.0 (Cai and Knight, 2013) on all the
experiments.
</bodyText>
<table confidence="0.9997255">
System P R F1
Charniak (ON) .67 .64 .65
Charniak .66 .62 .64
Stanford .64 .62 .63
Malt .65 .61 .63
Turbo .65 .61 .63
</table>
<tableCaption confidence="0.99519">
Table 1: AMR parsing performance on develop-
ment set using different syntactic parsers.
</tableCaption>
<table confidence="0.999944666666667">
System P R F1
Charniak (ON) .67 .64 .65
+INFER .71 .67 .69
+INFER+BROWN .71 .68 .69
+INFER+BROWN+SRL .72 .69 .71
+INFER+BROWN+SRL+COREF .72 .69 .70
</table>
<tableCaption confidence="0.991151">
Table 2: AMR parsing performance on the devel-
opment set.
</tableCaption>
<subsectionHeader confidence="0.935909">
4.1.1 Impact of different syntactic parsers
</subsectionHeader>
<bodyText confidence="0.99995692">
We experimented with four different parsers: the
Stanford parser (Manning et al., 2014), the Char-
niak parser (Charniak and Johnson, 2005) (Its
phrase structure output is converted to dependency
structure using the Stanford CoreNLP converter),
the Malt Parser (Nivre et al., 2006), and the Turbo
Parser (Martins et al., 2013). All the parsers we
used are trained on the 02-22 sections of the Penn
Treebank, except for CHARNIAK(ON), which is
trained on the OntoNotes corpus (Hovy et al.,
2006) on the training and development partitions
used by Pradhan et al. (2013) after excluding a few
documents that overlapped with the AMR corpus1.
All the different dependency trees are then used as
input to our baseline system and we evaluate AMR
parsing performance on the development set.
From Table 1, we can see that the perfor-
mance of the baseline transition-based system
remains very stable when different dependency
parsers used are trained on same data set. How-
ever, the Charniak parser that is trained on a
much larger and more diverse dataset (CHARNIAK
(ON)) yields the best overall AMR parsing perfor-
mance. Subsequent experiments are all based on
this version of the Charniak parser.
</bodyText>
<subsectionHeader confidence="0.919956">
4.1.2 Impact of parser extensions
</subsectionHeader>
<bodyText confidence="0.999986583333333">
In Table 2 we present results from extending the
transition-based AMR parser. All experiments are
conducted on the development set. From Table
2, we can see that the INFER action yields a 4
point improvement in F1 score over the CHAR-
NIAK(ON) system. Adding Brown clusters im-
proves the recall by 1 point, but the F1 score re-
mains unchanged. Adding semantic role features
on top of the Brown clusters leads to an improve-
ment of another 2 points in F1 score, and gives us
the best result. Adding coreference features actu-
ally slightly hurts the performance.
</bodyText>
<subsectionHeader confidence="0.897758">
4.1.3 Final Result on Test Set
</subsectionHeader>
<bodyText confidence="0.762530625">
We evaluate the best model we get from §4.1 on
the test set, as shown in Table 3. For comparison
purposes, we also include results of all published
parsers on the same dataset: the updated version of
JAMR, the old version of JAMR (Flanigan et al.,
2014), the Stanford AMR parser (Werling et al.,
2015), the SHRG-based AMR parser (Peng et al.,
2015) and our baseline parser (Wang et al., 2015).
1Documents in the AMR corpus have some overlap with
the documents in the OntoNotes corpus. We excluded these
documents (which are primarily from Xinhua newswirte)
from the training data while retraining the Charniak parser,
ASSERT semantic role labeler, and IMS frameset
disambiguation tool). The full list of overlapping documents
is available at http://cemantix.org/ontonotes/ontonotes-
amr-document-overlap.txt
</bodyText>
<page confidence="0.98463">
860
</page>
<table confidence="0.999707714285714">
System P R F1
Our system .71 .69 .70
JAMR (GitHub)2 .69 .58 .63
JAMR (Flanigan et al., 2014) .66 .52 .58
Stanford .66 .59 .62
SHRG-based .59 .58 .58
Wang et al. (2015) .64 .62 .63
</table>
<tableCaption confidence="0.8470045">
Table 3: AMR parsing performance on the news
wire test set of LDC2013E117.
</tableCaption>
<bodyText confidence="0.99527025">
From Table 3 we can see that our parser has sig-
nificant improvement over all the other parsers and
outperforms the previous best parser by 7% points
in Smatch score.
</bodyText>
<subsectionHeader confidence="0.995384">
4.2 Experiments on LDC2014T12
</subsectionHeader>
<bodyText confidence="0.999623789473684">
In this section, we conduct experiments on
the AMR annotation release 1.0 (LDC2014T12),
which contains 13,051 AMRs from newswire,
weblogs and web discussion forums. We use
the training/development/test split recommended
in the release: 10,312 sentences for training,
1,368 sentences for development and 1,371 sen-
tences for testing. We re-train the parser on the
LDC2014T12 training set with the best parser con-
figuration given in §4.1, and test the parser on the
test set. The result is shown in Table 4. For com-
parison purposes, we also include the results of the
updated version of JAMR and our baseline parser
in (Wang et al., 2015) which are also trained on
the same dataset. There is a significant drop-off
in performance compared with the results on the
LDC2013E117 test set for all the parsers, but our
parser outperforms the other parsers by a similar
margin on both test sets.
</bodyText>
<table confidence="0.999715">
System P R F
Our system .70 .62 .66
Wang et al. (2015) .63 .56 .59
JAMR (GitHub) .64 .53 .58
</table>
<tableCaption confidence="0.890121">
Table 4: AMR parsing performance on the full test
set of LDC2014T12.
</tableCaption>
<bodyText confidence="0.9992355">
We also evaluate our parser on the newswire
section of LDC2014T12 dataset. Table 5 com-
pares the performance of JAMR, the Stanford
AMR parser and our system on the same dataset.
</bodyText>
<footnote confidence="0.92185">
2This is the updated JAMR from
https://github.com/jflanigan/jamr
</footnote>
<table confidence="0.9998335">
System P R F
Our system .72 .67 .70
Stanford .67 .58 .62
JAMR (GitHub) .67 .53 .59
</table>
<tableCaption confidence="0.9826255">
Table 5: AMR parsing performance on newswire
section of LDC2014T12 test set
</tableCaption>
<bodyText confidence="0.5988825">
And our system still outperforms the other
parsers by a similar margin.
</bodyText>
<sectionHeader confidence="0.997978" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999985857142857">
We presented extensions to a transition-based
AMR parser that leads to an improvement of 7%
in absolute F1 score over the best previously pub-
lished results. The extensions include designing a
new action to infer abstract concepts and training
the parser with additional semantic role labeling
and coreference based features.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999574375">
We want to thank the anonymous reviewers for
their suggestions.. This work was partially sup-
ported by the National Science Foundation via
Grant No.0910532 entitled Richer Representa-
tions for Machine Translation. All views ex-
pressed in this paper are those of the authors and
do not necessarily represent the view of the Na-
tional Science Foundation.
</bodyText>
<sectionHeader confidence="0.998555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984720434782609">
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse, pages 178–186. Association for Compu-
tational Linguistics.
Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 748–752. Association for Computa-
tional Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ’05, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
</reference>
<page confidence="0.992399">
861
</page>
<reference confidence="0.996003131868132">
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract mean-
ing representation. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1426–
1436, Baltimore, Maryland, June. Association for
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers, pages 57–60. Associ-
ation for Computational Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
ACL-08: HLT, page 595.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617–622, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the Conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, HLT ’05, pages 523–530, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, vol-
ume 6, pages 2216–2219.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational linguistics,
31(1):71–106.
Xiaochang Peng, Linfeng Song, and Daniel Gildea.
2015. A synchronous hyperedge replacement gram-
mar based approach for AMR parsing. In Proceed-
ings of the Nineteenth Conference on Computational
Natural Language Learning.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of the Human Language Technology Confer-
ence/North American chapter of the Association of
Computational Linguistics (HLT/NAACL), Boston,
MA, May.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
bust linguistic analysis using OntoNotes. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 143–
152, Sofia, Bulgaria, August.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015. A transition-based algorithm for AMR pars-
ing. In North American Association for Computa-
tional Linguistics, Denver, Colorado.
Keenon Werling, Gabor Angeli, Melvin Johnson
Premkumar, and Christopher D. Manning. 2015.
Robust subgraph generation improves abstract
meaning representation parsing. In ACL.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78–83, Uppsala, Sweden.
</reference>
<page confidence="0.997858">
862
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886847">
<title confidence="0.998423">Boosting Transition-based AMR Parsing with Refined Actions Auxiliary Analyzers</title>
<author confidence="0.997159">Chuan Wang Nianwen Xue Sameer Pradhan</author>
<affiliation confidence="0.986215">Brandeis University Brandeis University Boulder Language</affiliation>
<email confidence="0.995974">cwang24@brandeis.eduxuen@brandeis.edupradhan@bltek.com</email>
<abstract confidence="0.994302333333334">We report improved AMR parsing results by adding a new action to a transitionbased AMR parser to infer abstract concepts and by incorporating richer features produced by auxiliary analyzers such as a semantic role labeler and a coreference resolver. We report final AMR parsing results that show an improvement of 7% in over the best previously reported result. Our parser is at:</abstract>
<intro confidence="0.964279">Juicechuan/AMRParsing</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>178--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="981" citStr="Banarescu et al., 2013" startWordPosition="141" endWordPosition="145">r abstract concepts and by incorporating richer features produced by auxiliary analyzers such as a semantic role labeler and a coreference resolver. We report final AMR parsing results that show an improvement of 7% absolute in F1 score over the best previously reported result. Our parser is available at: https://github.com/ Juicechuan/AMRParsing 1 Introduction AMR parsing is the task of taking a sentence as input and producing as output an Abstract Meaning Representation (AMR) that is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence (Banarescu et al., 2013). AMR parsing has drawn an increasing amount of attention recently. The first published AMR parser, JAMR (Flanigan et al., 2014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: an evaluation metric for semantic feature structures.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>748--752</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12201" citStr="Cai and Knight, 2013" startWordPosition="1975" endWordPosition="1978">L: dobj 859 specifically, 1000 classes word cluster trained by Turian et al. (2010). We use prefixes of lengths 4,6,10,20 of the word’s bit-string as features. 4 Experiments We first tune and evaluate our system on the newswire section of LDC2013E117 dataset. Then we show our parser’s performance on the recent LDC2014T12 dataset. 4.1 Experiments on LDC2013E117 We first conduct our experiments on the newswire section of AMR annotation corpus (LDC2013E117). The train/dev/test split of dataset is 4.0K/2.1K/2.1K, which is identical to the settings of JAMR. We evaluate our parser with Smatch v2.0 (Cai and Knight, 2013) on all the experiments. System P R F1 Charniak (ON) .67 .64 .65 Charniak .66 .62 .64 Stanford .64 .62 .63 Malt .65 .61 .63 Turbo .65 .61 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System P R F1 Charniak (ON) .67 .64 .65 +INFER .71 .67 .69 +INFER+BROWN .71 .68 .69 +INFER+BROWN+SRL .72 .69 .71 +INFER+BROWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and</context>
</contexts>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 748–752. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12816" citStr="Charniak and Johnson, 2005" startWordPosition="2079" endWordPosition="2082">night, 2013) on all the experiments. System P R F1 Charniak (ON) .67 .64 .65 Charniak .66 .62 .64 Stanford .64 .62 .63 Malt .65 .61 .63 Turbo .65 .61 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System P R F1 Charniak (ON) .67 .64 .65 +INFER .71 .67 .69 +INFER+BROWN .71 .68 .69 +INFER+BROWN+SRL .72 .69 .71 +INFER+BROWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for CHARNIAK(ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few documents that overlapped with the AMR corpus1. All the different dependency trees are then used as input to our baseline system and we evaluate AMR p</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4790" citStr="Collins, 2002" startWordPosition="762" endWordPosition="763">coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the parser performance, and (iii) semantic role information is beneficial to AMR parsing when used as features, while the Brown clusters do not make a difference and coreference information slightly hurts the AMR parsing performance. The rest of the paper is </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1426--1436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1109" citStr="Flanigan et al., 2014" startWordPosition="162" endWordPosition="165">reference resolver. We report final AMR parsing results that show an improvement of 7% absolute in F1 score over the best previously reported result. Our parser is available at: https://github.com/ Juicechuan/AMRParsing 1 Introduction AMR parsing is the task of taking a sentence as input and producing as output an Abstract Meaning Representation (AMR) that is a rooted, directed, edge-labeled and leaf-labeled graph that is used to represent the meaning of a sentence (Banarescu et al., 2013). AMR parsing has drawn an increasing amount of attention recently. The first published AMR parser, JAMR (Flanigan et al., 2014), performs AMR parsing in two stages: concept identification and relation identification. Flanigan et al. (2014) treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. A com</context>
<context position="14736" citStr="Flanigan et al., 2014" startWordPosition="2406" endWordPosition="2409">re over the CHARNIAK(ON) system. Adding Brown clusters improves the recall by 1 point, but the F1 score remains unchanged. Adding semantic role features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us the best result. Adding coreference features actually slightly hurts the performance. 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). 1Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System P R F1 Our system .71 .69 .70 JAMR (</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1426– 1436, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13170" citStr="Hovy et al., 2006" startWordPosition="2137" endWordPosition="2140">OWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for CHARNIAK(ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few documents that overlapped with the AMR corpus1. All the different dependency trees are then used as input to our baseline system and we evaluate AMR parsing performance on the development set. From Table 1, we can see that the performance of the baseline transition-based system remains very stable when different dependency parsers used are trained on same data set. However, the Charniak parser that is trained on a much larger and more diverse dataset (CHARNIAK (ON)) yields the best overall AMR parsi</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% solution. In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing. ACL-08: HLT,</title>
<date>2008</date>
<pages>595</pages>
<contexts>
<context position="8860" citStr="Koo et al., 2008" startWordPosition="1443" endWordPosition="1446">ered and subsequent actions can be applied to transform the subgraph to its correct AMR. Figure 3: INFER-have-org-role-91 action 3.2 Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Coreference features Coreference is typically represented as a chain of mentions realized as noun phrases or pronouns. AMR, on the other hand, represents coreference as re-entrance and uses one concept to represent all co-referring entities. To use the coreference information to inform AMR parsing actions, we design the following two features: 1) SHARE DEPENDENT. When applying REENTRANCEk-lr action on edge (a, b), we check whether the corresponding head node k of a candidate concept has any dependent node that co-refers with current dependent b. 2) DEPENDENT LABEL. If SHARE DEPENDENT is true</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. ACL-08: HLT, page 595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="4535" citStr="Lee et al., 2013" startWordPosition="721" endWordPosition="724">mation produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding the new action significantly improves the </context>
<context position="8756" citStr="Lee et al., 2013" startWordPosition="1425" endWordPosition="1428">n Figure 3, after applying action INFER-have-org-role-91 on node minister, the abstract concept is recovered and subsequent actions can be applied to transform the subgraph to its correct AMR. Figure 3: INFER-have-org-role-91 action 3.2 Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Coreference features Coreference is typically represented as a chain of mentions realized as noun phrases or pronouns. AMR, on the other hand, represents coreference as re-entrance and uses one concept to represent all co-referring entities. To use the coreference information to inform AMR parsing actions, we design the following two features: 1) SHARE DEPENDENT. When applying REENTRANCEk-lr action on edge (a, b), we check whether the corresponding head node k of a candidate concept has an</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="12766" citStr="Manning et al., 2014" startWordPosition="2071" endWordPosition="2074">luate our parser with Smatch v2.0 (Cai and Knight, 2013) on all the experiments. System P R F1 Charniak (ON) .67 .64 .65 Charniak .66 .62 .64 Stanford .64 .62 .63 Malt .65 .61 .63 Turbo .65 .61 .63 Table 1: AMR parsing performance on development set using different syntactic parsers. System P R F1 Charniak (ON) .67 .64 .65 +INFER .71 .67 .69 +INFER+BROWN .71 .68 .69 +INFER+BROWN+SRL .72 .69 .71 +INFER+BROWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for CHARNIAK(ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few documents that overlapped with the AMR corpus1. All the different dependency trees are then used as </context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>617--622</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="13003" citStr="Martins et al., 2013" startWordPosition="2108" endWordPosition="2111">elopment set using different syntactic parsers. System P R F1 Charniak (ON) .67 .64 .65 +INFER .71 .67 .69 +INFER+BROWN .71 .68 .69 +INFER+BROWN+SRL .72 .69 .71 +INFER+BROWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for CHARNIAK(ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few documents that overlapped with the AMR corpus1. All the different dependency trees are then used as input to our baseline system and we evaluate AMR parsing performance on the development set. From Table 1, we can see that the performance of the baseline transition-based system remains very stable when different dependency parsers used</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617–622, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 523–530, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>2216--2219</pages>
<contexts>
<context position="12958" citStr="Nivre et al., 2006" startWordPosition="2100" endWordPosition="2103">.63 Table 1: AMR parsing performance on development set using different syntactic parsers. System P R F1 Charniak (ON) .67 .64 .65 +INFER .71 .67 .69 +INFER+BROWN .71 .68 .69 +INFER+BROWN+SRL .72 .69 .71 +INFER+BROWN+SRL+COREF .72 .69 .70 Table 2: AMR parsing performance on the development set. 4.1.1 Impact of different syntactic parsers We experimented with four different parsers: the Stanford parser (Manning et al., 2014), the Charniak parser (Charniak and Johnson, 2005) (Its phrase structure output is converted to dependency structure using the Stanford CoreNLP converter), the Malt Parser (Nivre et al., 2006), and the Turbo Parser (Martins et al., 2013). All the parsers we used are trained on the 02-22 sections of the Penn Treebank, except for CHARNIAK(ON), which is trained on the OntoNotes corpus (Hovy et al., 2006) on the training and development partitions used by Pradhan et al. (2013) after excluding a few documents that overlapped with the AMR corpus1. All the different dependency trees are then used as input to our baseline system and we evaluate AMR parsing performance on the development set. From Table 1, we can see that the performance of the baseline transition-based system remains very </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, volume 6, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational linguistics,</journal>
<pages>31--1</pages>
<contexts>
<context position="3888" citStr="Palmer et al., 2005" startWordPosition="615" endWordPosition="618">op1 op2 “South” ARG1 ARG2 country name name minister mod “Korea” op1 foreign “Israel” person country 857 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 857–862, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Current AMR parsers are in their early stages of development, and their features are not yet fully developed. For example, the AMR makes heavy use of the framesets and semantic role labels used in the Proposition Bank (Palmer et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochang Peng</author>
<author>Linfeng Song</author>
<author>Daniel Gildea</author>
</authors>
<title>A synchronous hyperedge replacement grammar based approach for AMR parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the Nineteenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="14831" citStr="Peng et al., 2015" startWordPosition="2422" endWordPosition="2425">core remains unchanged. Adding semantic role features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us the best result. Adding coreference features actually slightly hurts the performance. 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). 1Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System P R F1 Our system .71 .69 .70 JAMR (GitHub)2 .69 .58 .63 JAMR (Flanigan et al., 2014) .66 .52 .58 Stanford .66 .59 .62 SHRG-based .</context>
</contexts>
<marker>Peng, Song, Gildea, 2015</marker>
<rawString>Xiaochang Peng, Linfeng Song, and Daniel Gildea. 2015. A synchronous hyperedge replacement grammar based approach for AMR parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics (HLT/NAACL),</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="4493" citStr="Pradhan et al., 2004" startWordPosition="714" endWordPosition="717">er et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply for each training instance. Our results show that (i) the transitionbased AMR parser is very stable across the different parsers used in the first stage, (ii) adding </context>
<context position="8602" citStr="Pradhan et al., 2004" startWordPosition="1399" endWordPosition="1403">n the parser visits an node in dependency tree, it inserts an abstract node with concept label l, right between the current node and its parent. For example in Figure 3, after applying action INFER-have-org-role-91 on node minister, the abstract concept is recovered and subsequent actions can be applied to transform the subgraph to its correct AMR. Figure 3: INFER-have-org-role-91 action 3.2 Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Coreference features Coreference is typically represented as a chain of mentions realized as noun phrases or pronouns. AMR, on the other hand, represents coreference as re-entrance and uses one concept to represent all co-referring entities. To use the coreference information to inform AMR parsing actions, we design the following two feat</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference/North American chapter of the Association of Computational Linguistics (HLT/NAACL), Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Hwee Tou Ng</author>
<author>Anders Bj¨orkelund</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
<author>Zhi Zhong</author>
</authors>
<title>Towards robust linguistic analysis using OntoNotes.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>143--152</pages>
<location>Sofia, Bulgaria,</location>
<marker>Pradhan, Moschitti, Xue, Ng, Bj¨orkelund, Uryupina, Zhang, Zhong, 2013</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou Ng, Anders Bj¨orkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143– 152, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8841" citStr="Turian et al., 2010" startWordPosition="1439" endWordPosition="1442">ract concept is recovered and subsequent actions can be applied to transform the subgraph to its correct AMR. Figure 3: INFER-have-org-role-91 action 3.2 Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Coreference features Coreference is typically represented as a chain of mentions realized as noun phrases or pronouns. AMR, on the other hand, represents coreference as re-entrance and uses one concept to represent all co-referring entities. To use the coreference information to inform AMR parsing actions, we design the following two features: 1) SHARE DEPENDENT. When applying REENTRANCEk-lr action on edge (a, b), we check whether the corresponding head node k of a candidate concept has any dependent node that co-refers with current dependent b. 2) DEPENDENT LABEL. If SHAR</context>
<context position="11663" citStr="Turian et al. (2010)" startWordPosition="1892" endWordPosition="1895">ole labeler predicts that the current dependent is an argument of the current head. Note that the arguments in semantic role labeler output are non-terminals which corresponds to a sentence span. Here we simply take the head word in the sentence span as the argument. Word Clusters For the semi-supervised word cluster feature, we use Brown clusters, more Israel foreign visits minister Israel foreign visits have-org-role-91 minister boy believe wants ARG1 girl him For action REENTRANCEbelieve-ARG1 SHARE DEPENDENT: true DEPENDENT LABEL: dobj 859 specifically, 1000 classes word cluster trained by Turian et al. (2010). We use prefixes of lengths 4,6,10,20 of the word’s bit-string as features. 4 Experiments We first tune and evaluate our system on the newswire section of LDC2013E117 dataset. Then we show our parser’s performance on the recent LDC2014T12 dataset. 4.1 Experiments on LDC2013E117 We first conduct our experiments on the newswire section of AMR annotation corpus (LDC2013E117). The train/dev/test split of dataset is 4.0K/2.1K/2.1K, which is identical to the settings of JAMR. We evaluate our parser with Smatch v2.0 (Cai and Knight, 2013) on all the experiments. System P R F1 Charniak (ON) .67 .64 .</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan Wang</author>
<author>Nianwen Xue</author>
<author>Sameer Pradhan</author>
</authors>
<title>A transition-based algorithm for AMR parsing.</title>
<date>2015</date>
<booktitle>In North American Association for Computational Linguistics,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="1821" citStr="Wang et al., 2015" startWordPosition="272" endWordPosition="275">igan et al. (2014) treat concept identification as a sequence labeling task and utilize a semiMarkov model to map spans of words in a sentence to concept graph fragments. For relation identification, they adopt graph-based techniques similar to those used in dependency parsing (McDonald et al., 2005). Instead of finding maximum spanning trees (MST) over words, they propose an algorithm that finds the maximum spanning connected subgraph (MSCG) over concept fragments identified in the first stage. A competitive alternative to the MSCG approach is transition-based AMR parsing. Our previous work (Wang et al., 2015) describes a transition-based system that also involves two stages. In the first step, an input sentence is parsed into a dependency tree with a dependency parser. In the second step, the transition-based AMR parser transforms the dependency tree into an AMR graph by performing a series of actions. Note that the dependency parser used in the first step can be any off-the-shelf dependency parser and does not have to trained on the same data set as used in the second step. ARG0 ARG1 Figure 1: An example showing abstract concept have-org-role-91 for the sentence “Israel foreign minister visits So</context>
<context position="4328" citStr="Wang et al., 2015" startWordPosition="685" endWordPosition="688"> and their features are not yet fully developed. For example, the AMR makes heavy use of the framesets and semantic role labels used in the Proposition Bank (Palmer et al., 2005), and it would seem that information produced by a semantic role labeling system trained on the PropBank can be used as features to improve the AMR parsing accuracy. Similarly, since AMR represents limited within-sentence coreference, coreference information produced by an off-the-shelf coreference system should benefit the AMR parser as well. In this paper, we describe an extension to our transition-based AMR parser (Wang et al., 2015) by adding a new action to infer the abstract concepts in an AMR, and new features derived from an off-the-shelf semantic role labeling system (Pradhan et al., 2004) and coreference system (Lee et al., 2013). We also experimented with adding Brown clusters as features to the AMR parser. Additionally, we experimented with using different syntactic parsers in the first stage. Following our previous work, we use the averaged perceptron algorithm (Collins, 2002) to train the parameters of the model and use the greedy parsing strategy during decoding to determine the best action sequence to apply f</context>
<context position="6030" citStr="Wang et al., 2015" startWordPosition="969" endWordPosition="972">In Section 2 we briefly describe the transition-based parser, and in Section 3 we describe our extensions. We report experimental results in Section 4 and conclude the paper in Section 5. 2 Transition-based AMR Parser The transition-based parser first uses a dependency parser to parse an input sentence, and then performs a limited number of highly general actions to transform the dependency tree to an AMR graph. The transition actions are briefly described below but due to the limited space, we cannot describe the full details of these actions, and the reader is referred to our previous work (Wang et al., 2015) for detailed descriptions of these actions: • NEXT-EDGE-lr (ned): Assign the current edge with edge label lr and go to next edge. • SWAP-lr (sw): Swap the current edge, make the current dependent as the new head, and assign edge label lr to the swapped edge. • REATTACHk-lr (reat): Reattach current dependent to node k and assign edge label lr. • REPLACE-HEAD (rph): Replace current head node with current dependent node. • REENTRANCEk-lr (reen): Add another head node k to current dependent and assign label lr to edge between k and current dependent. • MERGE (mrg): Merge two nodes connected by th</context>
<context position="14875" citStr="Wang et al., 2015" startWordPosition="2430" endWordPosition="2433"> features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us the best result. Adding coreference features actually slightly hurts the performance. 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). 1Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System P R F1 Our system .71 .69 .70 JAMR (GitHub)2 .69 .58 .63 JAMR (Flanigan et al., 2014) .66 .52 .58 Stanford .66 .59 .62 SHRG-based .59 .58 .58 Wang et al. (2015) .64 .62 .63 Ta</context>
<context position="16379" citStr="Wang et al., 2015" startWordPosition="2673" endWordPosition="2676">we conduct experiments on the AMR annotation release 1.0 (LDC2014T12), which contains 13,051 AMRs from newswire, weblogs and web discussion forums. We use the training/development/test split recommended in the release: 10,312 sentences for training, 1,368 sentences for development and 1,371 sentences for testing. We re-train the parser on the LDC2014T12 training set with the best parser configuration given in §4.1, and test the parser on the test set. The result is shown in Table 4. For comparison purposes, we also include the results of the updated version of JAMR and our baseline parser in (Wang et al., 2015) which are also trained on the same dataset. There is a significant drop-off in performance compared with the results on the LDC2013E117 test set for all the parsers, but our parser outperforms the other parsers by a similar margin on both test sets. System P R F Our system .70 .62 .66 Wang et al. (2015) .63 .56 .59 JAMR (GitHub) .64 .53 .58 Table 4: AMR parsing performance on the full test set of LDC2014T12. We also evaluate our parser on the newswire section of LDC2014T12 dataset. Table 5 compares the performance of JAMR, the Stanford AMR parser and our system on the same dataset. 2This is t</context>
</contexts>
<marker>Wang, Xue, Pradhan, 2015</marker>
<rawString>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for AMR parsing. In North American Association for Computational Linguistics, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keenon Werling</author>
<author>Gabor Angeli</author>
<author>Melvin Johnson Premkumar</author>
<author>Christopher D Manning</author>
</authors>
<title>Robust subgraph generation improves abstract meaning representation parsing.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="14784" citStr="Werling et al., 2015" startWordPosition="2414" endWordPosition="2417">sters improves the recall by 1 point, but the F1 score remains unchanged. Adding semantic role features on top of the Brown clusters leads to an improvement of another 2 points in F1 score, and gives us the best result. Adding coreference features actually slightly hurts the performance. 4.1.3 Final Result on Test Set We evaluate the best model we get from §4.1 on the test set, as shown in Table 3. For comparison purposes, we also include results of all published parsers on the same dataset: the updated version of JAMR, the old version of JAMR (Flanigan et al., 2014), the Stanford AMR parser (Werling et al., 2015), the SHRG-based AMR parser (Peng et al., 2015) and our baseline parser (Wang et al., 2015). 1Documents in the AMR corpus have some overlap with the documents in the OntoNotes corpus. We excluded these documents (which are primarily from Xinhua newswirte) from the training data while retraining the Charniak parser, ASSERT semantic role labeler, and IMS frameset disambiguation tool). The full list of overlapping documents is available at http://cemantix.org/ontonotes/ontonotesamr-document-overlap.txt 860 System P R F1 Our system .71 .69 .70 JAMR (GitHub)2 .69 .58 .63 JAMR (Flanigan et al., 2014</context>
</contexts>
<marker>Werling, Angeli, Premkumar, Manning, 2015</marker>
<rawString>Keenon Werling, Gabor Angeli, Melvin Johnson Premkumar, and Christopher D. Manning. 2015. Robust subgraph generation improves abstract meaning representation parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<location>Uppsala,</location>
<contexts>
<context position="8712" citStr="Zhong and Ng, 2010" startWordPosition="1417" endWordPosition="1420">the current node and its parent. For example in Figure 3, after applying action INFER-have-org-role-91 on node minister, the abstract concept is recovered and subsequent actions can be applied to transform the subgraph to its correct AMR. Figure 3: INFER-have-org-role-91 action 3.2 Feature Enrichment In our previous work, we only use simple lexical features and structural features. We extend the feature set to include (i) features generated by a semantic role labeling system—ASSERT (Pradhan et al., 2004), including a frameset disambiguator trained using a word sense disambiguation system—IMS (Zhong and Ng, 2010) and a coreference system (Lee et al., 2013) and (ii) features generated using semi-supervised word clusters (Turian et al., 2010; Koo et al., 2008). Coreference features Coreference is typically represented as a chain of mentions realized as noun phrases or pronouns. AMR, on the other hand, represents coreference as re-entrance and uses one concept to represent all co-referring entities. To use the coreference information to inform AMR parsing actions, we design the following two features: 1) SHARE DEPENDENT. When applying REENTRANCEk-lr action on edge (a, b), we check whether the correspondi</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>