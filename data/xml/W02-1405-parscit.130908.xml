<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.993473">
Improving a general-purpose Statistical Translation Engine by
Terminological lexicons
</title>
<author confidence="0.507258">
Philippe Langlais
</author>
<note confidence="0.448267">
RALI / DIRO / Universit´e de Montr´eal
C.P. 6128, succursale Centre-ville
</note>
<address confidence="0.824404">
Montr´eal (Qu´ebec)
Canada, H3C 3J7
</address>
<email confidence="0.997946">
email:felipe@iro.umontreal.ca
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979285714286">
The past decade has witnessed exciting work
in the field of Statistical Machine Translation
(SMT). However, accurate evaluation of its po-
tential in real-life contexts is still a questionable
issue.
In this study, we investigate the behavior of
an SMT engine faced with a corpus far differ-
ent from the one it has been trained on. We
show that terminological databases are obvious
resources that should be used to boost the per-
formance of a statistical engine. We propose
and evaluate a way of integrating terminology
into a SMT engine which yields a significant re-
duction in word error rate.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998239375">
SMT mainly became known to the linguistic
community as a result of the seminal work of
Brown et al. (1993b). Since then, many re-
searchers have invested effort into designing bet-
ter models than the ones proposed in the afore-
mentioned article and several new exciting ways
have been suggested to attack the problem.
For instance, Vogel et al. (1996) succeeded in
overcoming the independence assumption made
by IBM models by introducing order-1 Hidden
Markov alignment models. Och et al. (1999) de-
scribed an elegant way of integrating automat-
ically acquired probabilistic templates into the
translation process, and Nießen and Ney (2001)
did the same for morphological information.
Radically different statistical models have
also been proposed. (Foster, 2000) investigated
maximum entropy models as an alternative to
the so-called noisy-channel approach. Very re-
cently, Yamada and Knight (2001) described a
model in which the noisy-channel takes as input
a parsed sentence rather than simple words.
While many of these studies include intensive
evaluation sections, it is not always easy to de-
termine exactly how well statistical translation
can do on a given task. We know that on a spe-
cific task of spoken language translation, Wang
(1998) provided evidence that SMT compared
favorably to a symbolic translation system; but
as mentioned by the author, the comparison was
not totally fair.
We do not know of any studies that describe
extensive experiments evaluating the adequacy
of SMT in a real translation environment. We
prefer not to commit ourselves to defining what
a real translation task is; instead, we adopt the
conservative point of view that a viable transla-
tion engine (statistical or not) is one that copes
with texts that may be very different in nature
from those used to train it.
This fairly general definition suggests that
adaptativity is a cornerstone of a successful
SMT engine. Curiously enough, we are not
aware of much work on adaptative SMT, de-
spite the tremendous amount of work done on
adaptative statistical language modeling.
In this paper, we propose to evaluate how
a statistical engine behaves when translating a
very domain specific text which is far different
from the corpus used to trained both our trans-
lation and language models. We first describe
our translation engine. In section 3, we quantify
and analyse the performance deterioration of an
SMT engine trained on a broad-based corpus
(the Hansard) when used to translate a domain
specific text (in this study, a manual for military
snipers). In section 4, We then suggest a sim-
ple but natural way of improving a broad-based
SMT engine; that is, by opening the engine to
available terminological resources. In section 5,
we report on the improvement we observed by
implementing our proposed approach. Finally,
in section 6 we discuss other approaches we feel
can lead to more robust translation.
</bodyText>
<sectionHeader confidence="0.881643" genericHeader="method">
2 Our statistical engine
</sectionHeader>
<subsectionHeader confidence="0.999571">
2.1 The statistical models
</subsectionHeader>
<bodyText confidence="0.877234166666667">
In this study, we built an SMT engine designed
to translate from French to English, following
the noisy-channel paradigm first described by
(Brown et al., 1993b). This engine is based on
equation 1, where e1 stands for the sequence
ˆI
</bodyText>
<subsectionHeader confidence="0.7689385">
of Iˆ English target words to be found, given a
French source sentence of J words fJ1 :
</subsectionHeader>
<bodyText confidence="0.981510575757576">
To train our statistical models, we assembled
a bitext composed of 1.6 million pairs of sen-
tences that were automatically aligned at the
sentence level. In this experiment, every token
was converted into lowercase before training.
The language model we used is an interpo-
lated trigram we trained on the English sen-
tences of our bitext. The perplexity of the re-
sulting model is fairly low – 65 –, which actually
reflects the fact that this corpus contains many
fixed expressions (e.g pursuant to standing
order).
The inverted translation model we used is
an IBM2-like model: 10 iterations of IBM1-
training were run (reducing the perplexity of
the training corpus from 7776 to 90), followed
by 10 iterations of IBM2-training (yielding a
final perplexity of 54). We further reduced
the number of transfer parameters (originally
34 969 331) by applying an algorithm described
in Foster (2000); this algorithm basically filters
in the pairs of words with the best gain, where
gain is defined as the difference in perplexity —
measured on a held-out corpus — of a model
trained with this pair of words and a model
trained without. In this experiment, we worked
with a model containing exactly the first gain-
ranked million parameters. It is interesting to
note that by doing this, we not only save mem-
ory, and therefore time, but also obtain improv-
ments in terms of perplexity and overall perfor-
mance1.
&apos;On a translation task from French to English on
</bodyText>
<subsectionHeader confidence="0.998774">
2.2 The search algorithm
</subsectionHeader>
<bodyText confidence="0.999473058823529">
The maximum operation in equation 1, also
called search or decoding, involves a length
model. We assume that the length (counted in
words) of French sentences that translate an En-
glish sentence of a given length follow a normal
distribution.
We extended the decoder described by Nießen
et al. (1998) to a trigram language model. The
basic idea of this search algorithm is to expand
hypotheses along the positions of the target
string while progressively covering the source
ones. We refer the reader to the original paper
for the recursion on which it relies, and instead
give in Figure 1 a sketch of how a translation
is built. An hypothesis h is fully determined
by four parameters: its source (j) and target
(i) positions of the last word (e), and its cov-
erage (c). Therefore, the search space can be
represented as a 4-dimension table, each item
of which contains backtracking information (f
for the fertility of e, bj and bw for the source
position and the target word we should look at
to backtrack) and the hypothesis score (prob).
We know that better alignment models have
been proposed and extensively compared (Och
and Ney, 2000). We must however point
out that the performance we obtained on the
HANSARD corpus (see Section 3) is comparable
to the rates published elsewhere on the same
kind of corpus. In any case, our goal in this
study is to compare the behavior of a SMT en-
gine in both friendly and adverse situations. In
our view, the present SMT engine is suitable for
such a comparative study.
</bodyText>
<subsectionHeader confidence="0.999796">
2.3 Tuning the decoder
</subsectionHeader>
<bodyText confidence="0.999751714285714">
The decoder has been tuned in several ways in
order to reduce its computations without detri-
mentally affecting the quality of its output. The
first thing we do when the decoder receives a
sentence is to compute what we call an active
vocabulary; that is, a collection of words which
are likely to occur in the translation. This is
done by ranking for each source word the tar-
get words according to their non normalized
posterior likelihood (that is argmaxe p(f|e)p(e),
where p(e) is given by a unigram target lan-
guage model, and p(f|e) is given by the transfer
Hansard sentences, we observed a reduction in word er-
ror rate of more than 3% with the reduced model.
</bodyText>
<figure confidence="0.45141025">
e1 = argmax P(eI1) . P(fJ1 |eI1) (1)
ˆI � Y J � Y J
I,ei language translation
Input: f1 ... fj ... fJ
Initialize the search space table Space
Select a maximum target length: Imax
Compute the active vocabulary
// Fill the search table recursively:
for all target position i = 1, 2, ... , Imax do
prune(i − 1);
for all alive hyp. h = Space(i, j, c, e) do
uv +– History(h);
zones +– FreeSrcPositions(h);
bestWords +– NBestTgtWords(uv);
for all w in bestWords do
prob +– Score(h) + log p(w|uv);
setIfBetter(i, j, c, b, prob, 0, j, v);
for all free source position d do
s +– prob;
for all f E [1, fmax] / d + f − 1 is
free do
s+ = log a(i|d, J) + log t(fd|ei);
setIfBetter(i, d, c+ f, w, s, f, j, w);
// Find and return the best hypothesis if any
max, +– −oc
for all i E [1, Imax] do
for all alive hyp. h = Space(i, j, c, e) do
s +– Score(h) + log p(i|J);
</figure>
<construct confidence="0.8561462">
if ((c == J) and (s &gt; max,)) then
max, +– s
(maxi, maxj, maxe) +– (i, j, e)
if (max,! = oc) then
Return Space(maxi, maxj, J, maxe);
</construct>
<figure confidence="0.436489333333333">
else
Failure
Output: e1 ... ei ... emaxi
</figure>
<figureCaption confidence="0.972989">
Figure 1: Sketch of our decoder.
</figureCaption>
<bodyText confidence="0.968537925925926">
FreeSrcPositions returns the source posi-
tions not already associated to words of h;
NBestTgtWords returns the list of words
that are likely to follow the last bigram uv
preceeding e according to the language model;
and setIfBetter(i, j, c, e, p, f, bj, bw) is an
operator that memorizes an hypothesis if its
score (p) is greater than the hypothesis already
stored in Space(i, j, c, e). a and t stands for the
alignment and transfert distributions used by
IBM2 models.
probabilities of our inverted translation model)
and keeping for each source word at most a tar-
get words.
Increasing a raises the coverage of the active
vocabulary, but also slows down the translation
process and increases the risk of admitting a
word that has nothing to do with the transla-
tion. We have conducted experiments with var-
ious a-values, and found that an a-value of 10
offers a good compromise.
As mentioned in the block diagram, we also
prune the space to make the search tractable.
This is done with relative filtering as well as ab-
solute thresholding. The details of all the filter-
ing strategies we implemented are however not
relevant to the present study.
</bodyText>
<sectionHeader confidence="0.829913" genericHeader="method">
3 Performances of our SMT engine
</sectionHeader>
<subsectionHeader confidence="0.999687">
3.1 Test corpora
</subsectionHeader>
<bodyText confidence="0.999910875">
In this section we provide a comparison of the
translation performances we measured on two
corpora. The first one (namely, the HANSARD)
is a collection of sentences extracted from a part
of the Hansard corpus we did not use for train-
ing. In particular, we did not use any specific
strategy to select these sentences so that they
would be closely related to the ones that were
used for training.
Our second corpus (here called SNIPER) is
an excerpt of an army manual on sniper train-
ing and deployment that was used in an EAR-
LIER study (Macklovitch, 1995). This corpus is
highly specific to the military domain and would
certainly prove difficult for any translation en-
gine not specifically tuned to such material.
</bodyText>
<subsectionHeader confidence="0.999725">
3.2 Overall performance
</subsectionHeader>
<bodyText confidence="0.998810909090909">
In this section, we evaluate the performance of
our engine in terms of sentence- and word- error
rates according to an oracle translation2. The
first rate is the percentage of sentences for which
the decoder found the exact translation (that is,
the one of our oracle), and the word error rate
is computed by a Levenstein distance (count-
ing the same penalty for both insertion, dele-
tion and substitution edition operations). We
realize that these measures alone are not suffi-
cient for a serious evaluation, but we were re-
</bodyText>
<footnote confidence="0.961414333333333">
2Both corpora have been published in both French
and English, and we took the English part as the gold
standard.
</footnote>
<bodyText confidence="0.999479538461538">
luctant in this experiment to resort to manual
judgments, following for instance the protocol
described in (Wang, 1998). Actually a quick
look at the degradation in performance we ob-
served on SNIPER is so clear that we feel these
two rates are informative enough !
Table 1 summarizes the performance rates we
measured. The WER is close to 60% on the
HANSARD corpus and close to 74% on SNIPER;
source sentences in the latter corpus being
slightly longer on average (21 words). Not a
single sentence was found to be identical to the
gold standard translation on the SNIPER corpus
</bodyText>
<table confidence="0.7999805">
3.
corpus nbs |length |SER WER
HANSARD 1038 (16.2, 7.8) 95.6 59.6
SNIPER 203 (20.8, 6.8) 100 74.6
</table>
<tableCaption confidence="0.999215">
Table 1: Main characteristics of our test cor-
</tableCaption>
<bodyText confidence="0.912940333333333">
pora and global performance of our statistical
translator without any adjustments. |length|
reports the average length (counted in words)
of the source sentences and the standard de-
viation; nbs is the number of sentences in the
corpus.
</bodyText>
<subsectionHeader confidence="0.999893">
3.3 Analyzing the performance drop
</subsectionHeader>
<bodyText confidence="0.999845368421053">
As expected, the poor performance observed on
the SNIPER text is mainly due to two reasons:
the presence of out of vocabulary (OOV) words
and the incorrect translations of terminological
units.
In the SNIPER corpus, 3.5% of the source to-
kens and 6.5% of the target ones are unknown
to the statistical models. 44% of the source sen-
tences and 77% of the target sentences contain
at least one unknown word. In the HANSARD
text, the OOV rates are much lower: around
0.5% of the source and target tokens are un-
known and close to 5% of the source and target
sentences contain at least one OOV words.
These OOV rates have a clear impact on
the coverage of our active vocabulary. On the
SNIPER text, 72% of the oracle tokens are in the
active vocabulary (only 0.5% of the target sen-
tences are fully covered); whilst on HANSARD,
</bodyText>
<footnote confidence="0.969317333333333">
3The full output of our translation sessions
is available at www-iro.umontreal.ca/felipe/
ResearchOutput/Computerm2002
</footnote>
<bodyText confidence="0.994100789473684">
86% of the oracle’s tokens are covered (24% of
the target sentences are fully covered).
Another source of disturbance is the presence
of terminological units (TU) within the text to
translate. Table 2 provides some examples of
mistranslated TU from the SNIPER text. We
also observed that many words within termino-
logical units are not even known by the statisti-
cal models. Therefore accounting for terminol-
ogy is one of the ways that should be considered
to reduce the impact of OOV words.
&lt; source term / oracle / translation&gt;
&lt;ˆame / bore / heart&gt;
&lt;huile polyvalente / general purpose oil / oil
polyvalente&gt;
&lt;chambre / chamber / house of common&gt;
&lt;tireur d’ ´elite / sniper / issuer of elite&gt;
&lt;la longueur de la crosse / butt length / the
length of the crosse&gt;
</bodyText>
<tableCaption confidence="0.7544925">
Table 2: Examples of mistranslated terminolog-
ical entries of the SNIPER corpus.
</tableCaption>
<sectionHeader confidence="0.915336" genericHeader="method">
4 Integrating non-probabilistic
terminological resources
</sectionHeader>
<bodyText confidence="0.999758520000001">
Using terminological resources to improve the
quality of an automatic translation engine is not
at all a new idea. However, we know of very few
studies that actually investigated this avenue
in the field of statistical machine translation.
Among them, (Brown et al., 1993a) have pro-
posed a way to exploit bilingual dictionnaries at
training time. There may also be cases where
domain-specific corpora are available which al-
low for the training of specialized models that
can be combined with the general ones.
Another approach that would not require
such material at training time consists in de-
signing an adaptative translation engine. For
instance, a cache-based language model could
be used instead of our static trigram model.
However, the design of a truly adaptative trans-
lation model remains a more speculative enter-
prise. At the very least, it would require a fairly
precise location of errors in previously trans-
lated sentences; and we know from the AR-
CADE campaign on bilingual alignments, that
accurate word alignments are difficult to obtain
(V´eronis and Langlais, 2000). This may be even
more difficult in situations where errors will in-
volve OOV words.
We investigated a third option, which involves
taking advantage – at run time – of existing ter-
minological resources, such as Termium4. As
mentioned by Langlais et al. (2001), one of
a translator’s first tasks is often terminological
research; and many translation companies em-
ploy specialized terminologists. Actually, aside
from the infrequent cases where, in a given
thematic context, a word is likely to have a
clearly preferred translation (e.g. bill/facture
vs bill/projet de loi), lexicons are often the
only means for a user to influence the transla-
tion engine.
Merging such lexicons at run time offers
a complementary solution to those mentioned
above and it should be a fruitful strategy in sit-
uations where terminological resources are not
available at training time (which may often be
the case). Unfortunately, integrating termino-
logical (or user) lexicons into a probabilistic en-
gine is not a straightforward operation, since
we cannot expect them to come with attached
probabilities. Several strategies do come to
mind, however. For instance, we could credit a
translation of a sentence that contains a source
lexicon entry in cases it contains an authorized
translation. But this strategy may prouve dif-
ficult to tune since decoding usually involves
many filtering strategies.
The approach we adopted consists in view-
ing a terminological lexicon as a set of con-
straints that are employed to reduce the search
space. For instance, knowing that sniper is a
sanctioned translation of tireur d’elite, we may
require that current hypotheses in the search
space associate the target word sniper with the
three source French words.
In our implementation, we had to slightly
modify the block diagram of Figure 1 in order
to: 1) forbid a given word ei from being asso-
ciated with a word belonging to a source ter-
minological unit, if it is not sanctioned by the
lexicon; and 2) add at any target position an
hypothesis linking a target lexicon entry to its
source counterpart. Whether these hypotheses
will survive intact will depend on constraints
imposed by the maximum operation (of equa-
tion 1) over the full translation.
The score associated with a target entry ei&apos;
</bodyText>
<footnote confidence="0.731523">
i
4See http://www.termium.com/site/.
</footnote>
<bodyText confidence="0.820833">
when linked to its source counterpart fj&apos;
j in the
latter case is given by:
</bodyText>
<equation confidence="0.985865">
log p(ek|ek−2ek−1) + max
lE[j,j&apos;]
</equation>
<bodyText confidence="0.999973833333333">
The rationale behind this equation is that
both the language (p) and the alignment (a)
models have some information that can help
to decide the appropriateness of an extension:
the former knows how likely it is that a word
(known or not) will follow the current history5;
and the latter knows to some extent where the
target unit should be (regardless of its identity).
In the absence of a better mechanism (e.g. a
cache-model should be worth a try) We hope
that this will be sufficient to determine the final
position of the target unit in a given hypothesis.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9986232">
We considered three terminological lexicons
whose characteristics are summarized in Table
3; they essentially differ in terms of number
of entries and therefore coverage of the text to
translate.
</bodyText>
<table confidence="0.97835225">
lexicon nb coverage SER WER
sniper-1 33 20/247 99 67.4
sniper-2 59 47/299 98 66.2
sniper-3 146 132/456 98 64.3
</table>
<tableCaption confidence="0.998382">
Table 3: Translation performance with differ-
</tableCaption>
<bodyText confidence="0.988485">
ent terminological lexicons. nb is the number of
entries in the lexicon and coverage reports the
number of different source entries from the lex-
icon belonging to the text to translate and the
total number of their occurrences.
The first lexicon (namely sniper-1) contains
the 33 entries used in the study of terminological
consistency checking described in (Macklovitch,
1995). The second and third lexicons (namely
sniper-2 and sniper-3) contain those entries
plus other ones added manually after an incre-
mental inspection of the SNIPER corpus.
As can be observed from Table 3, introduc-
ing terminological lexicons into the translation
engine does improve performance, measured in
terms of WER, and this even with lexicons that
</bodyText>
<footnote confidence="0.924899">
5Our trigram model has been trained to provide pa-
rameters such as p(UNKjab).
</footnote>
<equation confidence="0.981623">
E
kE[i,i&apos;]
log(a(kll, J))
</equation>
<bodyText confidence="0.99538225">
Source le tireur d’ elite voit simultan´ement les fils croises et l’ image ( l’ objectif ) .
Target the sniper sees the crosshairs and the image - target - at the same time .
without the gunman being same son sit and picture of the hon. members : agreed .
with the sniper simultaneously see the crosshairs and the image (objective . )
Source contrˆole de la detente .
Target exercising trigger control .
without the control of d´etente .
with control of the trigger .
</bodyText>
<tableCaption confidence="0.5779865">
Table 4: Two examples of translation with and without a terminological lexicon; TU appear in
bold.
</tableCaption>
<bodyText confidence="0.999834416666667">
cover only a small portion of the text to trans-
late. With the narrow coverage lexicon, we ob-
serve an absolute reduction of 7%, and a reduc-
tion of 10% with the broader lexicon sniper-3.
This suggests that adding more entries into the
lexicon is likely to decrease WER. In another
study (Carl and Langlais, 2002), we investigated
whether an automatic procedure designed to de-
tect term variants could improve these perfor-
mances futher.
Table 4 provides two examples of translation
outputs, with and without the help of termino-
logical units. The first one clearly shows that
EVEN A few TU (two in this case) may sub-
stantially improve the quality of the translation
output; (the translation produced without the
lexicon was particularly poor in this very case.
Even though terminological lexicons do im-
prove the overall WER figure, a systematic in-
spection of the outputs produced with TU re-
veals that the translations are still less faithful
to the source text than the translations pro-
duced for the HANSARD text. OOV words re-
main a serious problem.
</bodyText>
<sectionHeader confidence="0.999768" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999962107142857">
In this study, we have shown that translat-
ing texts in specific domains with a general[-
]purpose statistical engine is difficult. This sug-
gests the need to implementing an adaptative
strategy. Among the possible scenarios, we have
shown that opening the engine to terminologi-
cal resources is a natural and efficient way of
softening the decoder.
In a similar vein, Marcu (2001) investigated
how to combine Example Based Machine Trans-
lation (EBMT) and SMT approaches. The au-
thor automatically derived from the Hansard
corpus what he calls a translation memory: ac-
tually a collection of pairs of source and target
word sequences that are in a translation rela-
tion according to the viterbi alignment run with
an IBM4 model that was also trained on the
Hansard corpus. This collection of phrases was
then merged with a greedy statistical decoder to
improve the overall performance of the system.
What this study suggests is that translation
memories collected from a given corpus can im-
prove the performance of a statistical engine
trained on the same corpus, which in itself is
an interesting result. A very similar study but
with weaker results is derscribed in (Langlais et
al., 2000), in the framework of the TRANSTYPE
project. Besides the different metrics the au-
thors used, the discrepancy in performance in
these two studies may be explained by the na-
ture of the test corpora used. The test corpus
in the latter study was more representative of a
real translation task, while the test corpus that
Marcu used was a set of around 500 French sen-
tences of no more than 10 words.
Our present study is close in spirit to these
last two, except that we do not attack the prob-
lem of automatically acquiring bilingual lexi-
cons; instead, we consider it a part of the trans-
lator’s task to provide such lexicons. Actually,
we feel this may be one of the only ways a user
has of retaining some control over the engine’s
output, a fact that professional translators seem
to appreciate (Langlais et al., 2001).
As a final remark, we want to stress that we
see the present study as a first step toward the
eventual unification of EBMT and SMT, and in
this respect we agree with (Marcu, 2001). Po-
tentially, of course, EBMT can offer much more
than just a simple list of equivalences, like those
we used in this study. However, the basic ap-
proach we describe here still holds, as long as
we can extend the notion of constraint used in
this study to include non-consecutive sequences
of words. This is a problem we we plan to in-
vestigate in future research.
</bodyText>
<sectionHeader confidence="0.995515" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.966016333333333">
I am indebted to Elliott Macklovitch and
George Foster for the fruitful orientation they
gave to this work.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981045882353">
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, Meredith J. Goldsmith,
Jan Hajic, Robert L. Mercer, and Surya
Mohanty. 1993a. But dictionaries are data
too. In Human Language Technology (HLT),
pages 202–205, Princeton, NJ, march.
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, and Robert L. Mer-
cer. 1993b. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
Michael Carl and Philippe Langlais. 2002. To-
ward an intelligent terminology database as
a front-and backend for statistical machine
translation. In COMPUTERM 2002, Taipei.
George Foster. 2000. A Maximum Entropy /
Minimum Divergence translation model. In
Proceedings of the 38th Annual Meeting of the
ACL, pages 37–44, Hong Kong, October.
Philippe Langlais, George Foster, and Guy
Lapalme. 2000. Unit completion for a
computer-aided translation typing system. In
Proceedings of the 5th Conference on Applied
Natural Language Processing (ANLP), pages
135–141, Seattle, Washington, May.
Philippe Langlais, George Foster, and Guy La-
palme. 2001. Integrating bilingual lexicons in
a probabilistic translation assistant. In Pro-
ceedings of the 8th Machine Translation Sum-
mit, pages 197–202, Santiago de Compostela,
Galicia, Spain, September. IAMT.
Elliott Macklovitch. 1995. Can terminologi-
cal consistency be validated automatically
? Technical report, CITI/RALI, Montr´eal,
Canada.
Daniel Marcu. 2001. Towards a unified ap-
proach to memory- and statistical-based ma-
chine translation. In Proceedings of the 39th
Annual Meeting of the ACL, pages 378–385,
Toulouse, France.
Sonja Nießen and Hermann Ney. 2001. Toward
hierarchical models for statistical machine
translation of inflected languages. In Proceed-
ings of the Workshop on Data Driven Ma-
chine Translation yielded at the 39th Annual
Meeting of the ACL, pages 47–54, Toulouse,
France.
Sonja Nießen, Stephan Vogel, Hermann Ney,
and Christoph Tillmann. 1998. A dp based
search algorithm for statistical machine trans-
lation. In Proceedings of the 36th Annual
Meeting of the ACL and the 17th COLING,
pages 960–966, Montr´eal, Canada, August.
Franz Joseph Och and Hermann Ney. 2000.
A comparison of alignement models for sta-
tistical machine translation. In Proceed-
ings of the International Conference on
Computational Linguistics (COLING) 2000,
pages 1086–1090, Saarbrucken, Luxembourg,
Nancy, August.
Franz Josef Och, Christoph Tillman, and Her-
man Ney. 1999. Improved alignment models
for statistical machine translation. In Pro-
ceedings of the 4nd Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP), pages 20–28, College Park, Mary-
land.
Jean V´eronis and Philippe Langlais, 2000. Eval-
uation of parallel text alignment systems: The
ARCADE project, volume 13, chapter 19,
pages 369–388. Parallel Text Processing,
Kluwer.
Stephan Vogel, Hermann Ney, and Christoph
Tillmann. 1996. Hmm-based word aligne-
ment in statistical translation. In Proceedings
of the International Conference on Compu-
tational Linguistics (COLING) 1996, pages
836–841, Copenhagen, Denmark, August.
Ye-Yi Wang. 1998. Grammar Inference and
Statistical Machine Translation. Ph.D. the-
sis, CMU-LTI, Carnegie Mellon University.
Kenji Yamada and Kevin Knight. 2001. A
syntax-based statistical translation model. In
Proceedings of the 39th Annual Meeting of the
ACL, pages 531–538, Toulouse, France.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212749">
<title confidence="0.698635">Improving a general-purpose Statistical Translation Engine Terminological lexicons Philippe RALI / DIRO / Universit´e de</title>
<author confidence="0.405591">C P</author>
<affiliation confidence="0.648033">Montr´eal</affiliation>
<address confidence="0.925243">Canada, H3C</address>
<email confidence="0.994247">felipe@iro.umontreal.ca</email>
<abstract confidence="0.994045533333333">The past decade has witnessed exciting work in the field of Statistical Machine Translation (SMT). However, accurate evaluation of its potential in real-life contexts is still a questionable issue. In this study, we investigate the behavior of an SMT engine faced with a corpus far different from the one it has been trained on. We show that terminological databases are obvious resources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Meredith J Goldsmith</author>
<author>Jan Hajic</author>
<author>Robert L Mercer</author>
<author>Surya Mohanty</author>
</authors>
<title>But dictionaries are data too.</title>
<date>1993</date>
<booktitle>In Human Language Technology (HLT),</booktitle>
<pages>202--205</pages>
<location>Princeton, NJ,</location>
<contexts>
<context position="964" citStr="Brown et al. (1993" startWordPosition="147" endWordPosition="150">SMT). However, accurate evaluation of its potential in real-life contexts is still a questionable issue. In this study, we investigate the behavior of an SMT engine faced with a corpus far different from the one it has been trained on. We show that terminological databases are obvious resources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate. 1 Introduction SMT mainly became known to the linguistic community as a result of the seminal work of Brown et al. (1993b). Since then, many researchers have invested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statis</context>
<context position="3959" citStr="Brown et al., 1993" startWordPosition="629" endWordPosition="632"> text (in this study, a manual for military snipers). In section 4, We then suggest a simple but natural way of improving a broad-based SMT engine; that is, by opening the engine to available terminological resources. In section 5, we report on the improvement we observed by implementing our proposed approach. Finally, in section 6 we discuss other approaches we feel can lead to more robust translation. 2 Our statistical engine 2.1 The statistical models In this study, we built an SMT engine designed to translate from French to English, following the noisy-channel paradigm first described by (Brown et al., 1993b). This engine is based on equation 1, where e1 stands for the sequence ˆI of Iˆ English target words to be found, given a French source sentence of J words fJ1 : To train our statistical models, we assembled a bitext composed of 1.6 million pairs of sentences that were automatically aligned at the sentence level. In this experiment, every token was converted into lowercase before training. The language model we used is an interpolated trigram we trained on the English sentences of our bitext. The perplexity of the resulting model is fairly low – 65 –, which actually reflects the fact that th</context>
<context position="14513" citStr="Brown et al., 1993" startWordPosition="2484" endWordPosition="2487">uile polyvalente / general purpose oil / oil polyvalente&gt; &lt;chambre / chamber / house of common&gt; &lt;tireur d’ ´elite / sniper / issuer of elite&gt; &lt;la longueur de la crosse / butt length / the length of the crosse&gt; Table 2: Examples of mistranslated terminological entries of the SNIPER corpus. 4 Integrating non-probabilistic terminological resources Using terminological resources to improve the quality of an automatic translation engine is not at all a new idea. However, we know of very few studies that actually investigated this avenue in the field of statistical machine translation. Among them, (Brown et al., 1993a) have proposed a way to exploit bilingual dictionnaries at training time. There may also be cases where domain-specific corpora are available which allow for the training of specialized models that can be combined with the general ones. Another approach that would not require such material at training time consists in designing an adaptative translation engine. For instance, a cache-based language model could be used instead of our static trigram model. However, the design of a truly adaptative translation model remains a more speculative enterprise. At the very least, it would require a fai</context>
</contexts>
<marker>Brown, Pietra, Pietra, Goldsmith, Hajic, Mercer, Mohanty, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, Meredith J. Goldsmith, Jan Hajic, Robert L. Mercer, and Surya Mohanty. 1993a. But dictionaries are data too. In Human Language Technology (HLT), pages 202–205, Princeton, NJ, march.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="964" citStr="Brown et al. (1993" startWordPosition="147" endWordPosition="150">SMT). However, accurate evaluation of its potential in real-life contexts is still a questionable issue. In this study, we investigate the behavior of an SMT engine faced with a corpus far different from the one it has been trained on. We show that terminological databases are obvious resources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate. 1 Introduction SMT mainly became known to the linguistic community as a result of the seminal work of Brown et al. (1993b). Since then, many researchers have invested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statis</context>
<context position="3959" citStr="Brown et al., 1993" startWordPosition="629" endWordPosition="632"> text (in this study, a manual for military snipers). In section 4, We then suggest a simple but natural way of improving a broad-based SMT engine; that is, by opening the engine to available terminological resources. In section 5, we report on the improvement we observed by implementing our proposed approach. Finally, in section 6 we discuss other approaches we feel can lead to more robust translation. 2 Our statistical engine 2.1 The statistical models In this study, we built an SMT engine designed to translate from French to English, following the noisy-channel paradigm first described by (Brown et al., 1993b). This engine is based on equation 1, where e1 stands for the sequence ˆI of Iˆ English target words to be found, given a French source sentence of J words fJ1 : To train our statistical models, we assembled a bitext composed of 1.6 million pairs of sentences that were automatically aligned at the sentence level. In this experiment, every token was converted into lowercase before training. The language model we used is an interpolated trigram we trained on the English sentences of our bitext. The perplexity of the resulting model is fairly low – 65 –, which actually reflects the fact that th</context>
<context position="14513" citStr="Brown et al., 1993" startWordPosition="2484" endWordPosition="2487">uile polyvalente / general purpose oil / oil polyvalente&gt; &lt;chambre / chamber / house of common&gt; &lt;tireur d’ ´elite / sniper / issuer of elite&gt; &lt;la longueur de la crosse / butt length / the length of the crosse&gt; Table 2: Examples of mistranslated terminological entries of the SNIPER corpus. 4 Integrating non-probabilistic terminological resources Using terminological resources to improve the quality of an automatic translation engine is not at all a new idea. However, we know of very few studies that actually investigated this avenue in the field of statistical machine translation. Among them, (Brown et al., 1993a) have proposed a way to exploit bilingual dictionnaries at training time. There may also be cases where domain-specific corpora are available which allow for the training of specialized models that can be combined with the general ones. Another approach that would not require such material at training time consists in designing an adaptative translation engine. For instance, a cache-based language model could be used instead of our static trigram model. However, the design of a truly adaptative translation model remains a more speculative enterprise. At the very least, it would require a fai</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993b. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
<author>Philippe Langlais</author>
</authors>
<title>Toward an intelligent terminology database as a front-and backend for statistical machine translation.</title>
<date>2002</date>
<booktitle>In COMPUTERM 2002,</booktitle>
<location>Taipei.</location>
<contexts>
<context position="20362" citStr="Carl and Langlais, 2002" startWordPosition="3452" endWordPosition="3455"> . with the sniper simultaneously see the crosshairs and the image (objective . ) Source contrˆole de la detente . Target exercising trigger control . without the control of d´etente . with control of the trigger . Table 4: Two examples of translation with and without a terminological lexicon; TU appear in bold. cover only a small portion of the text to translate. With the narrow coverage lexicon, we observe an absolute reduction of 7%, and a reduction of 10% with the broader lexicon sniper-3. This suggests that adding more entries into the lexicon is likely to decrease WER. In another study (Carl and Langlais, 2002), we investigated whether an automatic procedure designed to detect term variants could improve these performances futher. Table 4 provides two examples of translation outputs, with and without the help of terminological units. The first one clearly shows that EVEN A few TU (two in this case) may substantially improve the quality of the translation output; (the translation produced without the lexicon was particularly poor in this very case. Even though terminological lexicons do improve the overall WER figure, a systematic inspection of the outputs produced with TU reveals that the translatio</context>
</contexts>
<marker>Carl, Langlais, 2002</marker>
<rawString>Michael Carl and Philippe Langlais. 2002. Toward an intelligent terminology database as a front-and backend for statistical machine translation. In COMPUTERM 2002, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>A Maximum Entropy / Minimum Divergence translation model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>37--44</pages>
<location>Hong Kong,</location>
<contexts>
<context position="1616" citStr="Foster, 2000" startWordPosition="247" endWordPosition="248">nvested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. While many of these studies include intensive evaluation sections, it is not always easy to determine exactly how well statistical translation can do on a given task. We know that on a specific task of spoken language translation, Wang (1998) provided evidence that SMT compared favorably to a symbolic translation system; but as mentioned by the author, the c</context>
<context position="5012" citStr="Foster (2000)" startWordPosition="809" endWordPosition="810"> trigram we trained on the English sentences of our bitext. The perplexity of the resulting model is fairly low – 65 –, which actually reflects the fact that this corpus contains many fixed expressions (e.g pursuant to standing order). The inverted translation model we used is an IBM2-like model: 10 iterations of IBM1- training were run (reducing the perplexity of the training corpus from 7776 to 90), followed by 10 iterations of IBM2-training (yielding a final perplexity of 54). We further reduced the number of transfer parameters (originally 34 969 331) by applying an algorithm described in Foster (2000); this algorithm basically filters in the pairs of words with the best gain, where gain is defined as the difference in perplexity — measured on a held-out corpus — of a model trained with this pair of words and a model trained without. In this experiment, we worked with a model containing exactly the first gainranked million parameters. It is interesting to note that by doing this, we not only save memory, and therefore time, but also obtain improvments in terms of perplexity and overall performance1. &apos;On a translation task from French to English on 2.2 The search algorithm The maximum operat</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. A Maximum Entropy / Minimum Divergence translation model. In Proceedings of the 38th Annual Meeting of the ACL, pages 37–44, Hong Kong, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>Unit completion for a computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP),</booktitle>
<pages>135--141</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="22294" citStr="Langlais et al., 2000" startWordPosition="3772" endWordPosition="3775">ctually a collection of pairs of source and target word sequences that are in a translation relation according to the viterbi alignment run with an IBM4 model that was also trained on the Hansard corpus. This collection of phrases was then merged with a greedy statistical decoder to improve the overall performance of the system. What this study suggests is that translation memories collected from a given corpus can improve the performance of a statistical engine trained on the same corpus, which in itself is an interesting result. A very similar study but with weaker results is derscribed in (Langlais et al., 2000), in the framework of the TRANSTYPE project. Besides the different metrics the authors used, the discrepancy in performance in these two studies may be explained by the nature of the test corpora used. The test corpus in the latter study was more representative of a real translation task, while the test corpus that Marcu used was a set of around 500 French sentences of no more than 10 words. Our present study is close in spirit to these last two, except that we do not attack the problem of automatically acquiring bilingual lexicons; instead, we consider it a part of the translator’s task to pr</context>
</contexts>
<marker>Langlais, Foster, Lapalme, 2000</marker>
<rawString>Philippe Langlais, George Foster, and Guy Lapalme. 2000. Unit completion for a computer-aided translation typing system. In Proceedings of the 5th Conference on Applied Natural Language Processing (ANLP), pages 135–141, Seattle, Washington, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>Integrating bilingual lexicons in a probabilistic translation assistant.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th Machine Translation Summit,</booktitle>
<pages>197--202</pages>
<publisher>IAMT.</publisher>
<location>Santiago de Compostela, Galicia, Spain,</location>
<contexts>
<context position="15584" citStr="Langlais et al. (2001)" startWordPosition="2658" endWordPosition="2661">ram model. However, the design of a truly adaptative translation model remains a more speculative enterprise. At the very least, it would require a fairly precise location of errors in previously translated sentences; and we know from the ARCADE campaign on bilingual alignments, that accurate word alignments are difficult to obtain (V´eronis and Langlais, 2000). This may be even more difficult in situations where errors will involve OOV words. We investigated a third option, which involves taking advantage – at run time – of existing terminological resources, such as Termium4. As mentioned by Langlais et al. (2001), one of a translator’s first tasks is often terminological research; and many translation companies employ specialized terminologists. Actually, aside from the infrequent cases where, in a given thematic context, a word is likely to have a clearly preferred translation (e.g. bill/facture vs bill/projet de loi), lexicons are often the only means for a user to influence the translation engine. Merging such lexicons at run time offers a complementary solution to those mentioned above and it should be a fruitful strategy in situations where terminological resources are not available at training t</context>
<context position="23108" citStr="Langlais et al., 2001" startWordPosition="3919" endWordPosition="3922">ora used. The test corpus in the latter study was more representative of a real translation task, while the test corpus that Marcu used was a set of around 500 French sentences of no more than 10 words. Our present study is close in spirit to these last two, except that we do not attack the problem of automatically acquiring bilingual lexicons; instead, we consider it a part of the translator’s task to provide such lexicons. Actually, we feel this may be one of the only ways a user has of retaining some control over the engine’s output, a fact that professional translators seem to appreciate (Langlais et al., 2001). As a final remark, we want to stress that we see the present study as a first step toward the eventual unification of EBMT and SMT, and in this respect we agree with (Marcu, 2001). Potentially, of course, EBMT can offer much more than just a simple list of equivalences, like those we used in this study. However, the basic approach we describe here still holds, as long as we can extend the notion of constraint used in this study to include non-consecutive sequences of words. This is a problem we we plan to investigate in future research. Acknowledgments I am indebted to Elliott Macklovitch an</context>
</contexts>
<marker>Langlais, Foster, Lapalme, 2001</marker>
<rawString>Philippe Langlais, George Foster, and Guy Lapalme. 2001. Integrating bilingual lexicons in a probabilistic translation assistant. In Proceedings of the 8th Machine Translation Summit, pages 197–202, Santiago de Compostela, Galicia, Spain, September. IAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliott Macklovitch</author>
</authors>
<title>Can terminological consistency be validated automatically ?</title>
<date>1995</date>
<tech>Technical report, CITI/RALI,</tech>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="10593" citStr="Macklovitch, 1995" startWordPosition="1825" endWordPosition="1826">present study. 3 Performances of our SMT engine 3.1 Test corpora In this section we provide a comparison of the translation performances we measured on two corpora. The first one (namely, the HANSARD) is a collection of sentences extracted from a part of the Hansard corpus we did not use for training. In particular, we did not use any specific strategy to select these sentences so that they would be closely related to the ones that were used for training. Our second corpus (here called SNIPER) is an excerpt of an army manual on sniper training and deployment that was used in an EARLIER study (Macklovitch, 1995). This corpus is highly specific to the military domain and would certainly prove difficult for any translation engine not specifically tuned to such material. 3.2 Overall performance In this section, we evaluate the performance of our engine in terms of sentence- and word- error rates according to an oracle translation2. The first rate is the percentage of sentences for which the decoder found the exact translation (that is, the one of our oracle), and the word error rate is computed by a Levenstein distance (counting the same penalty for both insertion, deletion and substitution edition oper</context>
<context position="19030" citStr="Macklovitch, 1995" startWordPosition="3223" endWordPosition="3224"> differ in terms of number of entries and therefore coverage of the text to translate. lexicon nb coverage SER WER sniper-1 33 20/247 99 67.4 sniper-2 59 47/299 98 66.2 sniper-3 146 132/456 98 64.3 Table 3: Translation performance with different terminological lexicons. nb is the number of entries in the lexicon and coverage reports the number of different source entries from the lexicon belonging to the text to translate and the total number of their occurrences. The first lexicon (namely sniper-1) contains the 33 entries used in the study of terminological consistency checking described in (Macklovitch, 1995). The second and third lexicons (namely sniper-2 and sniper-3) contain those entries plus other ones added manually after an incremental inspection of the SNIPER corpus. As can be observed from Table 3, introducing terminological lexicons into the translation engine does improve performance, measured in terms of WER, and this even with lexicons that 5Our trigram model has been trained to provide parameters such as p(UNKjab). E kE[i,i&apos;] log(a(kll, J)) Source le tireur d’ elite voit simultan´ement les fils croises et l’ image ( l’ objectif ) . Target the sniper sees the crosshairs and the image </context>
</contexts>
<marker>Macklovitch, 1995</marker>
<rawString>Elliott Macklovitch. 1995. Can terminological consistency be validated automatically ? Technical report, CITI/RALI, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Towards a unified approach to memory- and statistical-based machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>378--385</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="21488" citStr="Marcu (2001)" startWordPosition="3640" endWordPosition="3641">e, a systematic inspection of the outputs produced with TU reveals that the translations are still less faithful to the source text than the translations produced for the HANSARD text. OOV words remain a serious problem. 6 Discussion In this study, we have shown that translating texts in specific domains with a general[- ]purpose statistical engine is difficult. This suggests the need to implementing an adaptative strategy. Among the possible scenarios, we have shown that opening the engine to terminological resources is a natural and efficient way of softening the decoder. In a similar vein, Marcu (2001) investigated how to combine Example Based Machine Translation (EBMT) and SMT approaches. The author automatically derived from the Hansard corpus what he calls a translation memory: actually a collection of pairs of source and target word sequences that are in a translation relation according to the viterbi alignment run with an IBM4 model that was also trained on the Hansard corpus. This collection of phrases was then merged with a greedy statistical decoder to improve the overall performance of the system. What this study suggests is that translation memories collected from a given corpus c</context>
</contexts>
<marker>Marcu, 2001</marker>
<rawString>Daniel Marcu. 2001. Towards a unified approach to memory- and statistical-based machine translation. In Proceedings of the 39th Annual Meeting of the ACL, pages 378–385, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Toward hierarchical models for statistical machine translation of inflected languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Data Driven Machine Translation yielded at the 39th Annual Meeting of the ACL,</booktitle>
<pages>47--54</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1493" citStr="Nießen and Ney (2001)" startWordPosition="229" endWordPosition="232">came known to the linguistic community as a result of the seminal work of Brown et al. (1993b). Since then, many researchers have invested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. While many of these studies include intensive evaluation sections, it is not always easy to determine exactly how well statistical translation can do on a given task. We know that on a specific task of spoken language translation, Wang (</context>
</contexts>
<marker>Nießen, Ney, 2001</marker>
<rawString>Sonja Nießen and Hermann Ney. 2001. Toward hierarchical models for statistical machine translation of inflected languages. In Proceedings of the Workshop on Data Driven Machine Translation yielded at the 39th Annual Meeting of the ACL, pages 47–54, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>A dp based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and the 17th COLING,</booktitle>
<pages>960--966</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5893" citStr="Nießen et al. (1998)" startWordPosition="960" endWordPosition="963">orked with a model containing exactly the first gainranked million parameters. It is interesting to note that by doing this, we not only save memory, and therefore time, but also obtain improvments in terms of perplexity and overall performance1. &apos;On a translation task from French to English on 2.2 The search algorithm The maximum operation in equation 1, also called search or decoding, involves a length model. We assume that the length (counted in words) of French sentences that translate an English sentence of a given length follow a normal distribution. We extended the decoder described by Nießen et al. (1998) to a trigram language model. The basic idea of this search algorithm is to expand hypotheses along the positions of the target string while progressively covering the source ones. We refer the reader to the original paper for the recursion on which it relies, and instead give in Figure 1 a sketch of how a translation is built. An hypothesis h is fully determined by four parameters: its source (j) and target (i) positions of the last word (e), and its coverage (c). Therefore, the search space can be represented as a 4-dimension table, each item of which contains backtracking information (f for</context>
</contexts>
<marker>Nießen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>Sonja Nießen, Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1998. A dp based search algorithm for statistical machine translation. In Proceedings of the 36th Annual Meeting of the ACL and the 17th COLING, pages 960–966, Montr´eal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignement models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)</booktitle>
<pages>1086--1090</pages>
<location>Saarbrucken, Luxembourg, Nancy,</location>
<contexts>
<context position="6733" citStr="Och and Ney, 2000" startWordPosition="1106" endWordPosition="1109"> for the recursion on which it relies, and instead give in Figure 1 a sketch of how a translation is built. An hypothesis h is fully determined by four parameters: its source (j) and target (i) positions of the last word (e), and its coverage (c). Therefore, the search space can be represented as a 4-dimension table, each item of which contains backtracking information (f for the fertility of e, bj and bw for the source position and the target word we should look at to backtrack) and the hypothesis score (prob). We know that better alignment models have been proposed and extensively compared (Och and Ney, 2000). We must however point out that the performance we obtained on the HANSARD corpus (see Section 3) is comparable to the rates published elsewhere on the same kind of corpus. In any case, our goal in this study is to compare the behavior of a SMT engine in both friendly and adverse situations. In our view, the present SMT engine is suitable for such a comparative study. 2.3 Tuning the decoder The decoder has been tuned in several ways in order to reduce its computations without detrimentally affecting the quality of its output. The first thing we do when the decoder receives a sentence is to co</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2000. A comparison of alignement models for statistical machine translation. In Proceedings of the International Conference on Computational Linguistics (COLING) 2000, pages 1086–1090, Saarbrucken, Luxembourg, Nancy, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Christoph Tillman</author>
<author>Herman Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>20--28</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="1350" citStr="Och et al. (1999)" startWordPosition="208" endWordPosition="211">te a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate. 1 Introduction SMT mainly became known to the linguistic community as a result of the seminal work of Brown et al. (1993b). Since then, many researchers have invested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. While many of these studies include intensive evaluation sections, it is not always easy to de</context>
</contexts>
<marker>Och, Tillman, Ney, 1999</marker>
<rawString>Franz Josef Och, Christoph Tillman, and Herman Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 4nd Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 20–28, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
<author>Philippe Langlais</author>
</authors>
<title>Evaluation of parallel text alignment systems: The ARCADE project,</title>
<date>2000</date>
<booktitle>Parallel Text Processing,</booktitle>
<volume>13</volume>
<pages>369--388</pages>
<publisher>Kluwer.</publisher>
<marker>V´eronis, Langlais, 2000</marker>
<rawString>Jean V´eronis and Philippe Langlais, 2000. Evaluation of parallel text alignment systems: The ARCADE project, volume 13, chapter 19, pages 369–388. Parallel Text Processing, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignement in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics (COLING)</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="1206" citStr="Vogel et al. (1996)" startWordPosition="187" endWordPosition="190">ow that terminological databases are obvious resources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate. 1 Introduction SMT mainly became known to the linguistic community as a result of the seminal work of Brown et al. (1993b). Since then, many researchers have invested effort into designing better models than the ones proposed in the aforementioned article and several new exciting ways have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignement in statistical translation. In Proceedings of the International Conference on Computational Linguistics (COLING) 1996, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
</authors>
<title>Grammar Inference and Statistical Machine Translation.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>CMU-LTI, Carnegie Mellon University.</institution>
<contexts>
<context position="2098" citStr="Wang (1998)" startWordPosition="325" endWordPosition="326">(2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. While many of these studies include intensive evaluation sections, it is not always easy to determine exactly how well statistical translation can do on a given task. We know that on a specific task of spoken language translation, Wang (1998) provided evidence that SMT compared favorably to a symbolic translation system; but as mentioned by the author, the comparison was not totally fair. We do not know of any studies that describe extensive experiments evaluating the adequacy of SMT in a real translation environment. We prefer not to commit ourselves to defining what a real translation task is; instead, we adopt the conservative point of view that a viable translation engine (statistical or not) is one that copes with texts that may be very different in nature from those used to train it. This fairly general definition suggests t</context>
<context position="11530" citStr="Wang, 1998" startWordPosition="1980" endWordPosition="1981">e first rate is the percentage of sentences for which the decoder found the exact translation (that is, the one of our oracle), and the word error rate is computed by a Levenstein distance (counting the same penalty for both insertion, deletion and substitution edition operations). We realize that these measures alone are not sufficient for a serious evaluation, but we were re2Both corpora have been published in both French and English, and we took the English part as the gold standard. luctant in this experiment to resort to manual judgments, following for instance the protocol described in (Wang, 1998). Actually a quick look at the degradation in performance we observed on SNIPER is so clear that we feel these two rates are informative enough ! Table 1 summarizes the performance rates we measured. The WER is close to 60% on the HANSARD corpus and close to 74% on SNIPER; source sentences in the latter corpus being slightly longer on average (21 words). Not a single sentence was found to be identical to the gold standard translation on the SNIPER corpus 3. corpus nbs |length |SER WER HANSARD 1038 (16.2, 7.8) 95.6 59.6 SNIPER 203 (20.8, 6.8) 100 74.6 Table 1: Main characteristics of our test c</context>
</contexts>
<marker>Wang, 1998</marker>
<rawString>Ye-Yi Wang. 1998. Grammar Inference and Statistical Machine Translation. Ph.D. thesis, CMU-LTI, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the ACL,</booktitle>
<pages>531--538</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1751" citStr="Yamada and Knight (2001)" startWordPosition="264" endWordPosition="267">ys have been suggested to attack the problem. For instance, Vogel et al. (1996) succeeded in overcoming the independence assumption made by IBM models by introducing order-1 Hidden Markov alignment models. Och et al. (1999) described an elegant way of integrating automatically acquired probabilistic templates into the translation process, and Nießen and Ney (2001) did the same for morphological information. Radically different statistical models have also been proposed. (Foster, 2000) investigated maximum entropy models as an alternative to the so-called noisy-channel approach. Very recently, Yamada and Knight (2001) described a model in which the noisy-channel takes as input a parsed sentence rather than simple words. While many of these studies include intensive evaluation sections, it is not always easy to determine exactly how well statistical translation can do on a given task. We know that on a specific task of spoken language translation, Wang (1998) provided evidence that SMT compared favorably to a symbolic translation system; but as mentioned by the author, the comparison was not totally fair. We do not know of any studies that describe extensive experiments evaluating the adequacy of SMT in a r</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th Annual Meeting of the ACL, pages 531–538, Toulouse, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>