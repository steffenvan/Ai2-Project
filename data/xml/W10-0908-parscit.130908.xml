<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9151">
Empirical Studies in Learning to Read
</title>
<author confidence="0.773101">
Marjorie Freedman, Edward Loper, Elizabeth Boschee, Ralph Weischedel
</author>
<affiliation confidence="0.675167">
BBN Raytheon Technologies
</affiliation>
<address confidence="0.8227525">
10 Moulton St
Cambridge, MA 02139
</address>
<email confidence="0.986323">
{mfreedma, eloper, eboschee, weischedel}@bbn.com
</email>
<sectionHeader confidence="0.996542" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801769230769">
In this paper, we present empirical results on
the challenge of learning to read. That is, giv-
en a handful of examples of the concepts and
relations in an ontology and a large corpus,
the system should learn to map from text to
the concepts/relations of the ontology. In this
paper, we report contrastive experiments on
the recall, precision, and F-measure (F) of the
mapping in the following conditions: (1) em-
ploying word-based patterns, employing se-
mantic structure, and combining the two; and
(2) fully automatic learning versus allowing
minimal questions of a human informant.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988435">
This paper reports empirical results with an algo-
rithm that “learns to read” text and map that text
into concepts and relations in an ontology specified
by the user. Our approach uses unsupervised and
semi-supervised algorithms to harness the diversity
and redundancy of the ways concepts and relations
are expressed in document collections. Diversity
can be used to automatically generate patterns and
paraphrases for new concepts and relations to
boost recall. Redundancy can be exploited to au-
tomatically check and improve the accuracy of
those patterns, allowing for system learning with-
out human supervision.
For example, the system learns how to recog-
nize a new relation (e.g. invent), starting from 5-20
instances (e.g. Thomas Edison + the light bulb).
The system iteratively searches a collection of
documents to find sentences where those instances
are expressed (e.g. “Thomas Edison’s patent for
the light bulb”), induces patterns over textual fea-
</bodyText>
<page confidence="0.988881">
61
</page>
<bodyText confidence="0.999532676470588">
tures found in those instances (e.g. pa-
tent(possessive:A, for:B)), and repeats the cycle by
applying the generated patterns to find additional
instances followed by inducing more patterns from
those instances. Unsupervised measures of redun-
dancy and coverage are used to estimate the relia-
bility of the induced patterns and learned instances;
only the most reliable are added, which minimizes
the amount of noise introduced at each step.
There have been two approaches to evaluation
of mapping text to concepts and relations: Auto-
matic Content Extraction (ACE)1 and Knowledge
Base Population (KBP)2. In ACE, complete ma-
nual annotation for a small corpus (~25k words)
was possible; thus, both recall and precision could
be measured across every instance in the test set.
This evaluation can be termed micro reading in
that it evaluates every concept/relation mention in
the corpus. In ACE, learning algorithms had
roughly 300k words of training data.
By contrast, in KBP, the corpus of documents
in the test set was too large for a complete answer
key. Rather than a complete answer key, relations
were extracted for a list of entities; system output
was pooled and judged manually. This type of
reading has been termed macro reading3, since
finding any instance of the relation in the 1.3M
document corpus is measured success, rather than
finding every instance. Only 118 queries were pro-
vided, though several hundred were created and
distributed by participants.
In the study in this paper, recall, precision, and
F are measured for 11 relations under the following
contrastive conditions
</bodyText>
<footnote confidence="0.996053666666667">
1 http://www.nist.gov/speech/tests/ace/
2 http://apl.jhu.edu/~paulmac/kbp.html
3 See http://rtw.ml.cmu.edu/papers/mitchell-iswc09.pdf
</footnote>
<note confidence="0.985863">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 61–69,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<listItem confidence="0.988957">
1. Patterns based on words vs. predicate-
argument structure vs. combining both.
2. Fully automatic vs. a few periodic res-
ponses by humans to specific queries.
</listItem>
<bodyText confidence="0.999922">
Though many prior studies have focused on
precision, e.g., to find any text justification to an-
swer a question, we focus equally on recall and
report recall performance as well as precision. This
addresses the challenge of finding information on
rarely mentioned entities (no matter how challeng-
ing the expression). We believe the effect will be
improved technology overall. We evaluate our sys-
tem in a micro-reading context on 11 relations. In a
fully automatic configuration, the system achieves
an F of .48 (Recall=.37, Precision=.68). With li-
mited human intervention, F rises to .58 (Re-
call=.49, Precision=.70). We see that patterns
based on predicate-argument structure (text
graphs) outperform patterns based on surface
strings with respect to both precision and recall.
Section 2 describes our approach; section 3,
some challenges; section 4, the implementation;
section 5, evaluation; section 6, empirical results
on extraction type; section 7, the effect of periodic,
limited human feedback; section 8, related work;
and section 9, lessons learned and conclusions.
</bodyText>
<sectionHeader confidence="0.983175" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999903708333333">
Our approach for learning patterns that can be used
to detect relations is depicted in Figure 1. Initially,
a few instances of the relation tuples are provided,
along with a massive corpus, e.g., the web or the
gigaword corpus from the Linguistic Data Consor-
tium (LDC). The diagram shows three inventor-
invention pairs, beginning with Thomas Edi-
son...light bulb. From these, we find candidate
sentences in the massive corpus, e.g., Thomas Edi-
son invented the light bulb. Features extracted from
the sentences retrieved, for example features of the
text-graph (the predicate-argument structure con-
necting the two arguments), provide a training in-
stance for pattern induction. The induced patterns
are added to the collection (database) of patterns.
Running the extended pattern collection over the
corpus finds new, previously unseen relation
tuples. From these new tuples, additional sentences
which express those tuples can be retrieved, and
the cycle of learning can continue.
There is an analogous cycle of learning con-
cepts from instances and the large corpus; the ex-
periments in this paper do not report on that paral-
lel learning cycle.
</bodyText>
<figureCaption confidence="0.9577">
Figure 1: Approach to Learning Relations
</figureCaption>
<bodyText confidence="0.616955666666667">
At the ith iteration, the steps are
1. Given the set of hypothesized instances of the
relation (triples HTi), find instances of such
triples in the corpus. (On the first iteration,
“hypothesized” triples are manually-generated
seed examples.)
</bodyText>
<listItem confidence="0.986625310344827">
2. Induce possible patterns. For each proposed
pattern P:
a. Apply pattern P to the corpus to generate a
set of triples TP
b. Estimate precision as the confidence-
weighted average of the scores of the
triples in TP. Reduce precision score by the
percentage of triples in TP that violate us-
er-specified relation constraints (e.g. arity
constraints described in 4.3)
c. Estimate recall as the confidence-weighted
percentage of triples in HTi found by the
pattern
3. Identify a set of high-confidence patterns HPi
using cutoffs automatically derived from rank-
based curves for precision, recall, and F-
measure (α=0.7)
4. Apply high-confidence patterns to a Web-scale
corpus to hypothesize new triples. For each
proposed triple T
a. Estimate score(T) as the expected proba-
bility that T is correct, calculated by com-
bining the respective precision and recall
scores of all of the patterns that did or did
not return it (using the Naïve Bayes as-
sumption that all patterns are independent)
b. Estimate confidence(T) as the percentage
of patterns in HPi by which T was found
5. Identify a set of high-confidence triples HTi+1
</listItem>
<figure confidence="0.959101064516129">
prune
and add
INVENTOR
retrieve from
carpus
pattern
database
lobi
granted
proposed
patterns
patent
obi
INVENTION
proposed
pairs
Tor
retrieve from carpus
proposed
instances
Edison invented the light bulb
Bell built the first telephone
Edison was granted a U.S. patent
for the light bulb
Franklin invented the lightning rod
induce
Thomas Edison... light bulb
Alexander G. Bell ... telephone
Ben Franklin ... lightning rod
example pairs
instances
</figure>
<page confidence="0.951968">
62
</page>
<bodyText confidence="0.887576">
using cutoffs automatically derived from rank-
based cu
rves; use these triples to begin the
next iteration
</bodyText>
<sectionHeader confidence="0.984964" genericHeader="method">
3 Challenges
</sectionHeader>
<bodyText confidence="0.998284">
The iterative cycle of learning we describe above
has most frequently been applied for macro-
reading tasks.
However, here we are interested in
measuring performance for micro-reading. There
are several reasons for wanting to measure perfor-
mance in a micro-reading task:
</bodyText>
<listItem confidence="0.598576">
•
</listItem>
<bodyText confidence="0.7333476">
For information that is rare (e.g. relations
about infrequently mentioned entities), a mi-
cro-
reading paradigm may more accurately
predict results.
</bodyText>
<listItem confidence="0.999946">
• For some tasks or domains micro-reading may
be all that we can do-- the actual corpus of in-
terest may not be macro-scaled.
• Macro-
</listItem>
<bodyText confidence="0.9751272">
reading frequently utilizes statistics of
extraction targets from the whole corpus to
improve its precision. Therefore, improving
micro-reading could improve the precision of
macro-reading.
</bodyText>
<listItem confidence="0.577309">
•
</listItem>
<bodyText confidence="0.996896428571429">
Because we are measuring performance in a
micro-
reading context, recall at the instance
level is as important as precision. Consequent-
ly, our learning system must learn to predict
patterns that incorporate nominal and prono-
minal mentions.
</bodyText>
<listItem confidence="0.489683">
•
</listItem>
<bodyText confidence="0.996425">
Furthermore, while the approach we describe
makes use of corpus-
wide statistics during the
learning process, during pattern application we
limit ourselves to only information from within
a single document (and in practice primarily
within a single sentence). Our evaluation
measures performance at the instance-level4.
</bodyText>
<sectionHeader confidence="0.992676" genericHeader="method">
4 Implementation
</sectionHeader>
<subsectionHeader confidence="0.984892">
4.1 Pattern Types
</subsectionHeader>
<bodyText confidence="0.961971651162791">
Boschee et al.(2008) describes two types of pat-
terns: patterns that rely on surface strings and pat-
terns that rely only on two types of syntactic-
structure. We diverge from that early work by al-
4Our evaluation measures only performance in extracting the
relation: that is if the text of sentence implies to an annotator
that the relation is present, then the annotator has been in-
structed to mark the sentence as correct (regardless of whether
or not outside knowledge contradicts this fact).
lowing more expressive surface-
string patterns: our
surface-string patterns can include wild-cards (al-
lowing the system to make matches which require
omitting words). For example for kill(agent, vic-
tim), the system learns the pattern &lt;AGENT&gt; &lt;...&gt;
assassinated &lt;VICTIM&gt;, which correctly matches
Booth, with hopes of a resurgent Confederacy in
mind, cruelly assassinated Lincoln.
We also diverge from the earlier work by only
making use of patterns based on the normalizedpredicate-
argument structure and not dependency
parses. The normalized predicate-argument struc-
tures (text-
graphs) are built by performing a set of
rule-
based transformations on the syntactic parse
of a sentence. These transformations include find-
ing the logical subject and object for each verb,
resolving some traces, identifying temporal argu-
ments, and attaching other verb arguments with
lexicalized roles (e.g. ‘of’ in Figure 2). The result-
ing graphs allow both noun and verb predicates.
Manually created patterns using this structure
have been successfully used for event detection
and template-based question answering. The text
graph structures have also served as useful features
in supervised, discriminative models for relation
and event extraction. While
the experiments described
here do not include depen-
dency tree paths, we do
allow arbitrarily large text
graph patterns.
</bodyText>
<subsectionHeader confidence="0.999097">
4.2 Co-Reference
</subsectionHeader>
<bodyText confidence="0.9933408">
Non-named mentions are essential for allowing
high instance-level recall. In certain cases, a rela-
tion is most clearly and frequently expressed with
pronouns and descriptions (e.g her father for the
relation child).5 Because non-named instances ap-
pear in different constructions than named in-
stances, we need to learn patterns that will appear
in non-named contexts. Thus, co-reference infor-
mation is used during pattern induction to extract
patterns from sentences where the hypothesized
triples are not explicitly mentioned. In particular,
any mention that is co-
referent with the desired
entity can be used to induce a pattern. Co-reference
for 7 types of entities is produced by SERIF, a
</bodyText>
<footnote confidence="0.6497772">
5 The structure of our noun-predicates allows us to learn lexi-
calized patterns in cases like this. For her father we would
induce the pattern n
:father:&lt;ref&gt;PARENT, &lt;pos&gt;CHILD.
Text Graph Pattern
</footnote>
<page confidence="0.995625">
63
</page>
<bodyText confidence="0.995525821428571">
state of the art information extraction engine. A
manually determined confidence threshold is used
to discard mentions where co-
reference certainty is
too low.
During pattern matching, co-
reference is used
to fi
nd the “best string” for each element of the
matched triple. In particular, pronouns and descrip-
tors are replaced by their referents; and abbreviated
names are replaced by full names. If any pronoun
cannot be resolved to a description or a name, or if
the co-
reference threshold falls below a manually
determined threshold, then the match is discarded.
Pattern scoring requires that we compare in-
stances of triples across the whole corpus. If these
instances were compared purely on the basis of
strings, in many cases the same entity would ap-
pear as distinct (e.g. US, United States). This
would interfere with the arity constraints described
below. To alleviate this challenge, we use a data-
base of name strings that have been shown to be
equivalent with a combination of edit-distance and
extraction statistics (Baron &amp; Freedman, 2008).
Thus, for triple(tP) and hypothesized triples (HTi),
if tP∉HTi
</bodyText>
<listItem confidence="0.86136025">
, but can be mapped via the equivalent
names database to some triple tP ’∈HTi, then its
score and confidence are adjusted towards that of
tP ’, weighted by the confidence of the equivalence.
</listItem>
<subsectionHeader confidence="0.937761">
4.3 Relation Set and Constraints
</subsectionHeader>
<bodyText confidence="0.9787124">
We ran experiments using 11 relation types. The
relation types were selected as a subset of the rela-
tions that have been proposed for DARPA’s ma-
chine reading program. In addition to seed
examples, we provided the learning system with
three types of constraints for each relation:
Symmetry: For relations where R(X,Y) =
R(Y,X), the learning (and scoring process), norma-
lizes instances of the relation so that R(X,Y) and
R(Y,X) are equivalent.
Arity: For each argument of the relation, pro-
vide an expected maximum number of instances
per unique instance of the other argument. These
numbers are intentionally higher than the expected
true value to account for co-reference mistakes.
Patterns that violate the arity constraint (e.g
v:accompanied(&lt;obj&gt;=&lt;X&gt;, &lt;sub&gt;=&lt;Y&gt; as a
pattern for spouse) are penalized. This is one way
of providing negative feedback during the unsu-
pervised training process.
Argument Type:
For each argument, specify
the expected class of entities for this argument.
Entity types are one of the 7 ACE types (Person,
Organization, Geo-political entity, Location, Facil-
ity, Weapon, Vehicle) or Date. Currently the sys-
tem only allows instance propo
sals when the types
are correct. Potentially, the system could use pat-
tern matches that violate type constraints as an ad-
ditional type of negative example. Any
implementation would need to account for the fact
that in some cases, potentially too general patterns
are quite effective when the type constraints are
met. For example, for the relation em-
ployed(PERSON, ORGANIZATION), &lt;ORG&gt;’s
&lt;PER&gt; is a fairly precise pattern, despite clearly
being overly general without the type constraints.
In our relation set, only two relations (sibling
and spouse) are symmetric. Table 1 below includes
the other constraints. ACE types/dates are in col-
umns labeled with the first letter of the name of the
type (A is arity)
. We have only included those
types that are an argument for some relation.
</bodyText>
<tableCaption confidence="0.97671">
Table 1 : Argument types of the test relations
</tableCaption>
<subsectionHeader confidence="0.993233">
4.4 Corpus and Seed Examples
</subsectionHeader>
<bodyText confidence="0.998868">
While many other experiments using this approach
have used web-
scale corpora, we chose to include
Wikipedia articles as well as Gigaword-3 to pro-
vide additional instances of information (e.g.
birthDate and sibling) that is uncommon in news.
For each relation type, 20 seed-examples were
selected randomly from the corpus by using a
combination of keyword search and an ACE ex-
traction system to identify passages that were like-
ly to contain the relations of interest. As such, each
seed example was guaranteed to occur at least once
in a context that indicated the relation was present.
</bodyText>
<sectionHeader confidence="0.997933" genericHeader="method">
5 Evaluation Framework
</sectionHeader>
<bodyText confidence="0.952645">
To evaluate system performance, we ran two sepa-
</bodyText>
<page confidence="0.99725">
64
</page>
<bodyText confidence="0.997475704545454">
rate annotation-based evaluations, the first meas-
ured precision, and the second measured recall.
To measure overall precision, we ran each sys-
tem’s patterns over the web-
scale corpora, and
randomly sampled 200 of the instances it found6.
These instances were then manually assessed as to
whether they conveyed the intended relation or not.
The system precision is simply the percentage of
instances that were judged t o be correct.
To measure recall, we began by randomly se-
lecting 20 test-
examples from the corpus, using the
same process that we used to select the training
seeds (but guaranteed to be distinct from the train-
ing seeds). We then searched the web-scale corpus
for sentences that might possibly link these test
examples together (whether directly or via co-
reference). We randomly sampled this set of sen-
tences, choosing 10 sentences for each test-
example, to form a collection of 200 sentences
which were likely to convey the desired relation.
These sentences were then manually annotated to
indicate which sentences actually convey the de-
sired relation; this set of sentences forms the recall
test set. Once a recall set had been created for each
relation, a system’s recall could be evaluated by
running that system’s patterns over the documents
in the recall set, and checking what percentage of
the recall test sentences it correctly identified.
We intentionally chose to sample 10 sentences
from each test exam
ple, rather than sampling from
the set of all sentences found for any of the test
examples, in order to prevent one or two very
common instances from dominating the recall set.
As a result, the recall test set is somewhat biased
away from “true” recall, si
nce it places a higher
weight on the “long tail” of instances. However,
we believe that this gives a more accurate indica-
tion of the system’s ability to find novel instances
of a relation (as opposed to novel ways of talking
about known instances).
</bodyText>
<sectionHeader confidence="0.885909" genericHeader="method">
6 Effect of Pattern Type
</sectionHeader>
<bodyText confidence="0.947150272727273">
As described in 4.1 , our system is capable of learn-
ing two classes of patterns: surface-strings and
text-graphs. We measured our system’s perfor-
m
ance on each of the relation types
6 While the system provides estimated precision for each pat
tern, we do not evaluate over the n-
with estimated confidence above 50% are treated equally.
sample from the set of matches produced by these patterns.
iterations. In each iteration, the system can learn
multiple patterns of either type. There is currently
no penalty for learning overlapping pattern types.
For example, in the first iteration for the relation
killed(), the system learns both the surface-string
pattern &lt;AGENT&gt; killed &lt;VICTIM&gt; and the text-
graph pattern: v:killed(&lt;sub&gt;=&lt;AGENT&gt;,
&lt;obj&gt;=&lt;VICTIM&gt;). During decoding, if multiple
patterns match the same relation instance, the sys-
tem accepts the relatio
n instance, but does not
make use of the additional information that there
were multiple supporting patterns.
</bodyText>
<figureCaption confidence="0.972986666666667">
Figure 5: F-Score of Pattern Type by Relation
Figure 3, Figure 4, and Figure 5 plot precision,
recall, and F-
</figureCaption>
<bodyText confidence="0.984494">
score for each of the 11 relations
showing performance of all patterns vs. only text-
graph patterns vs. only surface-string patterns.
</bodyText>
<listItem confidence="0.975878666666667">
• For most relations, the text-graph patterns pro-
vide both higher precision and higher recall
than the surface-
</listItem>
<bodyText confidence="0.760459833333333">
string patterns. The lower pre-
cision of the text-
graph patterns for attackOn is
the result of the system learning a number of
overly general patterns that correlate with at-
tacks, but do not themselves indicate the pres-
</bodyText>
<figureCaption confidence="0.993898">
Figure 4: Recall of Pattern Type by Relation
Figure 3
</figureCaption>
<bodyText confidence="0.5655832">
: Precision of Pattern Types by Relation
after twenty
-
We
best matches. All patterns
</bodyText>
<page confidence="0.997327">
65
</page>
<bodyText confidence="0.849543636363636">
ence of an attack. For instance, the system
learns patterns with predicates said and arrive.
Certainly, governments often make statements
on the date of an attack and troops arrive in a
location before attacking, but both patterns will
produce a large number of spurious instances.
• While text-graph patterns typically have higher
precision than the combined pattern set, sur-
face-string patterns provide enough improve-
ment in recall that typically the all-pattern F-
score is higher than the text-graph F-score.
</bodyText>
<figureCaption confidence="0.997972">
Figure 6: Text-Graph and Surface-
</figureCaption>
<subsectionHeader confidence="0.657459">
String Patterns
</subsectionHeader>
<bodyText confidence="0.988640471698113">
A partial explanation for the higher recall and
precision of the text-
graph patterns is illustrated in
Figure 6 which presents a simple surface-string
pattern and a simple text-
graph pattern that appear
to represent the same information. On the right of
the figure are three sentences. The text-graph pat-
tern correctly identifies the agent and victim in
each sentence. However, the surface-string pattern,
misses the killed
() relation in the first sentence and
misidentifies the victim
in the second sentence. The
false-alarm in the second sentence would have
been avoided by a system that restricted itself to
matching named instances, but as described above
in section 4.2, for the micro-
reading task described
here, det
ecting relations with pronouns is critical.
While we allowed the creation of text-graph
patterns with arbitrarily long paths between the
arguments, in practice, the system rarely learned
such patterns. For the relation killed(Agent, Vic-
tim), we learned 8 patterns that have more than one
predicate (compared to 22 that only have a single
predicate). For the relation killedInLoca-
tion(Victim, Location), the system learned 28 pat-
terns with more than 1 predicate (compared with
20 containing only 1 predicate). In both cases, the
precision of the longer patterns was higher, but
their recall was significantly lower. In the case of
killedInLocation, none of the longer path patterns
matched any of the examples in our recall set.
One strength of text-graph patterns is allowing
for intelligent omission of overly specific text, for
example, ignoring ‘ during a buglary’ in Figure 6.
Surface string patterns can include wild-cards, but
for surface string patterns, the omission is not syn-
tactically defined. Approximately 30% of surface
string patterns included one wild-card. An addi-
tional 17% included two. Figure 7 presents aver-
aged precision and recall for text-graph and
surface-string patterns. The final three columns
break the surface-string patterns down by the num-
ber of wild-cards. It appears that with one, the pat-
terns remain reasonably precise, but the addition of
a second wild-card drops precision by more than
50%. The presence of wild-card patterns improves
recall, but surface-
string patterns do not reach the
level of recall of text-graph patterns.
</bodyText>
<table confidence="0.99922925">
Text Graph Surface String
All No-* 1-* 2-*
Precision 0.75 0.61 0.72 0.69 0.30
Recall 0.32 0.22 0.16 0.10 0.09
</table>
<figureCaption confidence="0.923534">
Figure 7: Performance by Number of WildCards (*)
</figureCaption>
<sectionHeader confidence="0.605018" genericHeader="method">
7 Effect of Human Review
</sectionHeader>
<bodyText confidence="0.9997797">
In addition to allowing the system to self-train in a
completely unsupervised manner, we ran a parallel
set of experiments where the system was given
limited human guidance. At the end of iterations 1,
5, 10, and 20
, a person provided under 10 minutes
of feedback (on average 5 minutes). The person
was presented with five patterns, and five sample
matched instances for each pattern. The person was
able to provide two types of feedback:
</bodyText>
<figure confidence="0.458666166666667">
•
The pattern is correct/incorrect (e.g.
&lt;EMPLOYEE&gt; said
&lt;ORGANIZATION&gt; is
an incorrect pattern for employ(X,Y))
•
</figure>
<bodyText confidence="0.801294684210527">
The matched instances are correct/incorrect
(e.g. ‘Bob
received a diploma from MIT’ is a
correct instance, even if the pattern that pro-
duced it is debatable (e.g. v:&lt;received&gt;
subj:PERSON, from:ORGANIZATION). A cor-
rect instance can also produce a new known-
to-be correct seed.
Pattern judgments are stored in the database and
incorporated as absolute truth.
Instance judgments
provide useful input into pattern scoring. Patterns
were selected for annotation using a score that
combines their estimated f-measure; their frequen-
cy; and their dissimilarity to patterns that were
previously chosen for annotation. The matched
instances for each pattern are randomly sampled, to
ensure that the resulting annotation can be used to
derive an unbiased precision estimate.
</bodyText>
<page confidence="0.891157">
66
</page>
<bodyText confidence="0.980611944444445">
Figure 8, Figure 9, and Figure 10 plot precision, ter iteration 5, but a much larger improvement after
recall, and F- iteration 20. For child, there is actually a small de-
score at iterations 5 and 20 for the crease in recall after 5 iterations, but after 20 itera-
system running in a fully unsupervised manner and tions, the system has dramatically improved.
one allowing human intervention. The effect on precision is similarly varied. For
9 of the 11, human intervention improves preci-
sion; but the improvement is never as dramatic as
the improvement in recall. For precision, the
strongest improvements in performance appear in
the early iterations. It is unclear whether this mere-
ly reflects that bootstrapping is likely to become
less precise over time (as it learns more patterns),
or if early feedback is truly better for improving
precision.
In the case of attackOn, even with human inter-
vention, after iteration 10, the system begins to
learn very general patterns of the type described in
the previous section (e.g. &lt;said in:LOCATION
on:DATE&gt; as a pattern indicating an attack. These
patterns may be correlated with experiencing an
attack but are not themselves evidence of an attack.
Because the overly general patterns do in fact cor-
relate with the presence of an attack, the positive
examples provided by human intervention may in
fact produce more such patterns.
There is an interaction between improved preci-
sion and improved recall. If a system is very im-
precise at iteration n
, the additional instances that it
proposes may not reflect the relation and be so dif-
ferent from each other that the system becomes
unable to produce good patterns that improve re-
call. Conversely, if recall at iteration n does not
produce a sufficiently diverse set of instances, it
will be difficult for the system to generate in-
stances that are used to estimate pattern precision.
</bodyText>
<sectionHeader confidence="0.604094" genericHeader="related work">
8 Related Work
</sectionHeader>
<figureCaption confidence="0.713085260869565">
Much research has been done on concept and
relation detection using large amounts of super-
vised training. This is the typical approach in pro-
grams like Automatic Content Extraction (ACE),
which eva
luates system performance in detecting a
fixed set of concepts and relations in text. In ACE,
all participating researchers are given access to a
substantial amount of supervised training, e.g.,
250k words of annotated data. Researchers have
typically used this data to incorporate a great deal
of structural syntactic information in their models
(e.g. Ramshaw 2001), but the obvious weakness of
these approaches is the resulting reliance on the
Figure 8 : Precision at Iterations 5 and 20 for the Unsu-
pervised System and the System with Intervention
Figure 9: Recall at Iterations 5 and 20 for the Unsuper-
vised System and the System with Intervention
Figure 10: F-Score at Iterations 5 and 20 for the Unsu-
pervised System and the System with Intervention
For two relations: child and sibling, recall im-
proved dram
atically with human intervention. By
</figureCaption>
<bodyText confidence="0.994712071428571">
inspecting the patterns produced by the system, we
see that in case of sibling without intervention, the
system only learned the relation ‘ brother’ and not
the relation ‘sister.’
The limited feedback from a
person was e
nough to allow the system to learn
patterns for sister
as well, causing the significantly
improved recall. We see smaller, but frequently
significant improvements in recall in a number of
other relations. Interestingly, for different relations,
the recall improvements are seen at different itera-
tions. For sibling, the jump in recall appears within
the first five iterations. Contrastingly, for attend-
School, there is a minor improvement in recall af-
67
manually annotated examples, which are expensive
and time-consuming to create.
Co-training circumvents this weakness by play-
ing off two sufficiently different views of a data set
to leverage large quantities of unlabeled data
(along with a few examples of labeled data), in
order to improve the performance of a learning
algorithm (Mitchell and Blum, 1998). Co-training
will offer our approach to simultaneously learn the
patterns of expressing a relation and its arguments.
Other researchers have also previously explored
automatic pattern generation from unsupervised
text, classically in (Riloff &amp; Jones 1999). Ravi-
chandran and Hovy (2002) reported experimental
results for automatically generating surface pat-
terns for relation identification; others have ex-
plored similar approaches (e.g. Agichtein &amp;
Gravano 2000 or Pantel &amp; Pennacchiotti, 2006).
More recently (Mitchell et al., 2009) has shown
that for macro-reading, precision and recall can be
improved by learning a large set of interconnected
relations and concepts simultaneously.
We depart from this work by learning patterns
that use the structural features of text-graph pat-
terns and our particular approach to pattern and
pair scoring and selection.
Most approaches to automatic pattern genera-
tion have focused on precision, e.g., Ravichandran
and Hovy report results in the Text Retrieval Con-
ference (TREC) Question Answering track, where
extracting one instance of a relation can be suffi-
cient, rather than detecting all instances. This study
has also emphasized recall. Information about an
entity may only be mentioned once, especially for
rarely mentioned entities. A primary focus on pre-
cision allows one to ignore many instances that
require co-reference or long-distance dependen-
cies; one primary goal of our work is to measure
system performance in exactly those areas.
</bodyText>
<sectionHeader confidence="0.996483" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999992272727273">
We have shown that bootstrapping approaches can
be successfully applied to micro-reading tasks.
Most prior work with this approach has focused on
macro-reading, and thus emphasized precision.
Clearly, the task becomes much more challenging
when the system must detect every instance. De-
spite the challenge, with very limited human inter-
vention, we achieved F-scores of &gt;.65 on 6 of the
11 relations (average F on the relation set was .58).
We have also replicated an earlier preliminary
result (Boschee, 2008) showing that for a micro-
reading task, patterns that utilize seman-
tic/syntactic information outperform patterns that
make use of only surface strings. Our result covers
a larger inventory of relation types and attempts to
provide a more precise measure of recall than the
earlier preliminary study.
Analysis of our system’s output provides in-
sights into challenges that such a system may face.
One challenge for bootstrapping systems is that
it is easy for the system to learn just a subset of
relations. We observed this in both sibling where
we learned the relation brother and for employed
where we only learned patterns for leaders of an
organization. For sibling human intervention al-
lowed us to correct for this mistake. However for
employed even with human intervention, our recall
remains low. The difference between these two
relations may be that for sibling there are only two
sub-relations to learn, while there is a rich hie-
rarchy of potential sub-relations under the general
relation employed. The challenge is quite possibly
exacerbated by the fact that the distribution of em-
ployment relations in the news is heavily biased
towards top officials, but our recall test set inten-
tionally does not reflect this skew.
Another challenge for this approach is continu-
ing to learn in successive iterations. As we saw in
the figures in Section 7, for many relations perfor-
mance at iteration 20 is not significantly greater
than performance at iteration 5. Note that seeing
improvements on the long tail of ways to express a
relation may require a larger recall set than the test
set used here. This is exemplified by the existence
of the highly precise 2-predicate patterns which in
some cases never fired in our recall test set.
In future, we wish to address the subset prob-
lem and the problem of stalled improvements. Both
could potentially be addressed by improved inter-
nal rescoring. For example, the system scoring
could try to guarantee coverage over the whole
seed-set thus promoting patterns with low recall,
but high value for reflecting different information.
A complementary set of improvements could ex-
plore improved uses of human intervention.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.718224">
This work was supported, in part, by BBN under
AFRL Contract FA8750-09-C-179.
</bodyText>
<page confidence="0.999311">
68
</page>
<sectionHeader confidence="0.998345" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928867924529">
E. Agichtein and L. Gravano. Snowball: extracting rela-
tions from large plain-text collections. In Proceed-
ings of the ACM Conference on Digital Libraries, pp.
85-94, 2000.
A. Baron and M. Freedman, “Who is Who and What is
What: Experiments in Cross Document Co-
Reference”. Empirical Methods in Natural Language
Processing. 2008.
A. Blum and T. Mitchell. Combining Labeled and Un-
labeled Data with Co-Training. In Proceedings of the
1998 Conference on Computational Learning
Theory, July 1998.
E. Boschee, V. Punyakanok, R. Weischedel. An Explo-
ratory Study Towards ‘Machines that Learn to Read’.
Proceedings of AAAI BICA Fall Symposium, No-
vember 2008.
M. Collins and Y Singer. Unsupervised Models for
Named Entity Classification. EMNLP/VLC. (1999).
M Mintz, S Bills, R Snow, and D Jurafsky.. Distant su-
pervision for relation extraction without labeled data.
Proceedings of ACL-IJCNLP 200. 2009..
T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and
R. Wang. “Populating the Semantic Web by Macro-
Reading Internet Text. Invited paper, Proceedings of
the 8th International Semantic Web Conference
(ISWC 2009).
P. Pantel and M. Pennacchiotti. Espresso: Leveraging
Generic Patterns for Automatically Harvesting Se-
mantic Relations. In Proceedings of Conference on
Computational Linguistics / Association for Compu-
tational Linguistics (COLING/ACL-06). pp. 113-120.
Sydney, Australia, 2006.
L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R.
Stone, R. Weischedel, A. Zamanian, “Experiments in
multi-modal automatic content extraction”, Proceed-
ings of Human Technology Conference, March 2001.
D. Ravichandran and E. Hovy. Learning surface text
patterns for a question answering system. In Pro-
ceedings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL 2002),
pages 41–47, Philadelphia, PA, 2002.
E. Riloff. Automatically generating extraction patterns
from untagged text. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pages
1044-1049, 1996.
E. Rilof and Jones, R &amp;quot;Learning Dictionaries for In-
formation Extraction by Multi-Level Bootstrapping&amp;quot;,
Proceedings of the Sixteenth National Conference on
Artificial Intelligence (AAAI-99) , 1999, pp. 474-
479. 1999.
R Snow, D Jurafsky, and A Y. Ng.. Learning syntactic
patterns for automatic hypernym discovery . Proceed-
ings of NIPS 17. 2005.
</reference>
<page confidence="0.999318">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850130">
<title confidence="0.969158">Empirical Studies in Learning to Read</title>
<author confidence="0.999015">Marjorie Freedman</author>
<author confidence="0.999015">Edward Loper</author>
<author confidence="0.999015">Elizabeth Boschee</author>
<author confidence="0.999015">Ralph</author>
<affiliation confidence="0.966884">BBN Raytheon</affiliation>
<address confidence="0.953579">10 Moulton Cambridge, MA 02139</address>
<email confidence="0.999609">mfreedma@bbn.com</email>
<email confidence="0.999609">eloper@bbn.com</email>
<email confidence="0.999609">eboschee@bbn.com</email>
<email confidence="0.999609">weischedel@bbn.com</email>
<abstract confidence="0.999394785714286">In this paper, we present empirical results on challenge of to That is, given a handful of examples of the concepts and relations in an ontology and a large corpus, the system should learn to map from text to the concepts/relations of the ontology. In this paper, we report contrastive experiments on the recall, precision, and F-measure (F) of the mapping in the following conditions: (1) employing word-based patterns, employing semantic structure, and combining the two; and (2) fully automatic learning versus allowing minimal questions of a human informant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACM Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<contexts>
<context position="28601" citStr="Agichtein &amp; Gravano 2000" startWordPosition="4526" endWordPosition="4529">ge quantities of unlabeled data (along with a few examples of labeled data), in order to improve the performance of a learning algorithm (Mitchell and Blum, 1998). Co-training will offer our approach to simultaneously learn the patterns of expressing a relation and its arguments. Other researchers have also previously explored automatic pattern generation from unsupervised text, classically in (Riloff &amp; Jones 1999). Ravichandran and Hovy (2002) reported experimental results for automatically generating surface patterns for relation identification; others have explored similar approaches (e.g. Agichtein &amp; Gravano 2000 or Pantel &amp; Pennacchiotti, 2006). More recently (Mitchell et al., 2009) has shown that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. We depart from this work by learning patterns that use the structural features of text-graph patterns and our particular approach to pattern and pair scoring and selection. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting o</context>
</contexts>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>E. Agichtein and L. Gravano. Snowball: extracting relations from large plain-text collections. In Proceedings of the ACM Conference on Digital Libraries, pp. 85-94, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Baron</author>
<author>M Freedman</author>
</authors>
<title>Who is Who and What is What: Experiments</title>
<date>2008</date>
<booktitle>in Cross Document CoReference”. Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="13205" citStr="Baron &amp; Freedman, 2008" startWordPosition="2041" endWordPosition="2044">ved to a description or a name, or if the coreference threshold falls below a manually determined threshold, then the match is discarded. Pattern scoring requires that we compare instances of triples across the whole corpus. If these instances were compared purely on the basis of strings, in many cases the same entity would appear as distinct (e.g. US, United States). This would interfere with the arity constraints described below. To alleviate this challenge, we use a database of name strings that have been shown to be equivalent with a combination of edit-distance and extraction statistics (Baron &amp; Freedman, 2008). Thus, for triple(tP) and hypothesized triples (HTi), if tP∉HTi , but can be mapped via the equivalent names database to some triple tP ’∈HTi, then its score and confidence are adjusted towards that of tP ’, weighted by the confidence of the equivalence. 4.3 Relation Set and Constraints We ran experiments using 11 relation types. The relation types were selected as a subset of the relations that have been proposed for DARPA’s machine reading program. In addition to seed examples, we provided the learning system with three types of constraints for each relation: Symmetry: For relations where R</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>A. Baron and M. Freedman, “Who is Who and What is What: Experiments in Cross Document CoReference”. Empirical Methods in Natural Language Processing. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining Labeled and Unlabeled Data with Co-Training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 Conference on Computational Learning Theory,</booktitle>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A. Blum and T. Mitchell. Combining Labeled and Unlabeled Data with Co-Training. In Proceedings of the 1998 Conference on Computational Learning Theory, July 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boschee</author>
<author>V Punyakanok</author>
<author>R Weischedel</author>
</authors>
<title>An Exploratory Study Towards ‘Machines that Learn to Read’.</title>
<date>2008</date>
<booktitle>Proceedings of AAAI BICA Fall Symposium,</booktitle>
<marker>Boschee, Punyakanok, Weischedel, 2008</marker>
<rawString>E. Boschee, V. Punyakanok, R. Weischedel. An Exploratory Study Towards ‘Machines that Learn to Read’. Proceedings of AAAI BICA Fall Symposium, November 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification.</title>
<date>1999</date>
<location>EMNLP/VLC.</location>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y Singer. Unsupervised Models for Named Entity Classification. EMNLP/VLC. (1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<journal>Proceedings of ACL-IJCNLP</journal>
<volume>200</volume>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>M Mintz, S Bills, R Snow, and D Jurafsky.. Distant supervision for relation extraction without labeled data. Proceedings of ACL-IJCNLP 200. 2009..</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
<author>J Betteridge</author>
<author>A Carlson</author>
<author>E Hruschka</author>
<author>R Wang</author>
</authors>
<title>Populating the Semantic Web by MacroReading Internet Text. Invited paper,</title>
<date>2009</date>
<booktitle>Proceedings of the 8th International Semantic Web Conference (ISWC</booktitle>
<contexts>
<context position="28673" citStr="Mitchell et al., 2009" startWordPosition="4537" endWordPosition="4540">), in order to improve the performance of a learning algorithm (Mitchell and Blum, 1998). Co-training will offer our approach to simultaneously learn the patterns of expressing a relation and its arguments. Other researchers have also previously explored automatic pattern generation from unsupervised text, classically in (Riloff &amp; Jones 1999). Ravichandran and Hovy (2002) reported experimental results for automatically generating surface patterns for relation identification; others have explored similar approaches (e.g. Agichtein &amp; Gravano 2000 or Pantel &amp; Pennacchiotti, 2006). More recently (Mitchell et al., 2009) has shown that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. We depart from this work by learning patterns that use the structural features of text-graph patterns and our particular approach to pattern and pair scoring and selection. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one instance of a relation can be sufficient, rather than detecting all i</context>
</contexts>
<marker>Mitchell, Betteridge, Carlson, Hruschka, Wang, 2009</marker>
<rawString>T. Mitchell, J. Betteridge, A. Carlson, E. Hruschka, and R. Wang. “Populating the Semantic Web by MacroReading Internet Text. Invited paper, Proceedings of the 8th International Semantic Web Conference (ISWC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics (COLING/ACL-06).</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="28634" citStr="Pantel &amp; Pennacchiotti, 2006" startWordPosition="4531" endWordPosition="4534">ata (along with a few examples of labeled data), in order to improve the performance of a learning algorithm (Mitchell and Blum, 1998). Co-training will offer our approach to simultaneously learn the patterns of expressing a relation and its arguments. Other researchers have also previously explored automatic pattern generation from unsupervised text, classically in (Riloff &amp; Jones 1999). Ravichandran and Hovy (2002) reported experimental results for automatically generating surface patterns for relation identification; others have explored similar approaches (e.g. Agichtein &amp; Gravano 2000 or Pantel &amp; Pennacchiotti, 2006). More recently (Mitchell et al., 2009) has shown that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. We depart from this work by learning patterns that use the structural features of text-graph patterns and our particular approach to pattern and pair scoring and selection. Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one instance of a relation can be </context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics (COLING/ACL-06). pp. 113-120. Sydney, Australia, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Boschee</author>
<author>S Bratus</author>
<author>S Miller</author>
<author>R Stone</author>
<author>R Weischedel</author>
<author>A Zamanian</author>
</authors>
<title>Experiments in multi-modal automatic content extraction”,</title>
<date>2001</date>
<booktitle>Proceedings of Human Technology Conference,</booktitle>
<marker>Boschee, Bratus, Miller, Stone, Weischedel, Zamanian, 2001</marker>
<rawString>L. Ramshaw , E. Boschee, S. Bratus, S. Miller, R. Stone, R. Weischedel, A. Zamanian, “Experiments in multi-modal automatic content extraction”, Proceedings of Human Technology Conference, March 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="28425" citStr="Ravichandran and Hovy (2002)" startWordPosition="4502" endWordPosition="4506">ated examples, which are expensive and time-consuming to create. Co-training circumvents this weakness by playing off two sufficiently different views of a data set to leverage large quantities of unlabeled data (along with a few examples of labeled data), in order to improve the performance of a learning algorithm (Mitchell and Blum, 1998). Co-training will offer our approach to simultaneously learn the patterns of expressing a relation and its arguments. Other researchers have also previously explored automatic pattern generation from unsupervised text, classically in (Riloff &amp; Jones 1999). Ravichandran and Hovy (2002) reported experimental results for automatically generating surface patterns for relation identification; others have explored similar approaches (e.g. Agichtein &amp; Gravano 2000 or Pantel &amp; Pennacchiotti, 2006). More recently (Mitchell et al., 2009) has shown that for macro-reading, precision and recall can be improved by learning a large set of interconnected relations and concepts simultaneously. We depart from this work by learning patterns that use the structural features of text-graph patterns and our particular approach to pattern and pair scoring and selection. Most approaches to automat</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and E. Hovy. Learning surface text patterns for a question answering system. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 41–47, Philadelphia, PA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044-1049, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rilof</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping&amp;quot;,</title>
<date>1999</date>
<booktitle>Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99) ,</booktitle>
<pages>474--479</pages>
<marker>Rilof, Jones, 1999</marker>
<rawString>E. Rilof and Jones, R &amp;quot;Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping&amp;quot;, Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99) , 1999, pp. 474-479. 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery .</title>
<date>2005</date>
<journal>Proceedings of NIPS</journal>
<volume>17</volume>
<marker>Snow, Jurafsky, Ng, 2005</marker>
<rawString>R Snow, D Jurafsky, and A Y. Ng.. Learning syntactic patterns for automatic hypernym discovery . Proceedings of NIPS 17. 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>