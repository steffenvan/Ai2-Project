<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001555">
<title confidence="0.988208">
Frame-Semantic Role Labeling with Heterogeneous Annotations
</title>
<author confidence="0.859206">
Meghana Kshirsagar* Sam Thomson* Nathan Schneider†
Jaime Carbonell* Noah A. Smith* Chris Dyer*
</author>
<affiliation confidence="0.999896">
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA
†School of Informatics, University of Edinburgh, Edinburgh, Scotland, UK
</affiliation>
<sectionHeader confidence="0.989354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994636363636">
We consider the task of identifying and la-
beling the semantic arguments of a predi-
cate that evokes a FrameNet frame. This
task is challenging because there are only
a few thousand fully annotated sentences
for supervised training. Our approach aug-
ments an existing model with features de-
rived from FrameNet and PropBank and
with partially annotated exemplars from
FrameNet. We observe a 4% absolute in-
crease in F1 versus the original model.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913">
Paucity of data resources is a challenge for
semantic analyses like frame-semantic parsing
(Gildea and Jurafsky, 2002; Das et al., 2014) using
the FrameNet lexicon (Baker et al., 1998; Fillmore
and Baker, 2009).1 Given a sentence, a frame-
semantic parse maps word tokens to frames they
evoke, and for each frame, finds and labels its ar-
gument phrases with frame-specific roles. An ex-
ample appears in figure 1.
In this paper, we address this argument iden-
tification subtask, a form of semantic role label-
ing (SRL), a task introduced by Gildea and Juraf-
sky (2002) using an earlier version of FrameNet.
Our contribution addresses the paucity of annotated
data for training using standard domain adaptation
techniques. We exploit three annotation sources:
</bodyText>
<listItem confidence="0.981142166666667">
• the frame-to-frame relations in FrameNet, by
using hierarchical features to share statistical
strength among related roles (§3.2),
• FrameNet’s corpus of partially-annotated ex-
emplar sentences, by using “frustratingly
easy” domain adaptation (§3.3), and
</listItem>
<footnote confidence="0.8633345">
‡ Corresponding author: mkshirsa@cs.cmu.edu
1http://framenet.icsi.berkeley.edu
</footnote>
<figureCaption confidence="0.999789">
Figure 1: Part of a sentence from FrameNet full-text an-
</figureCaption>
<figure confidence="0.9865974375">
DESIRING: eagr.a hope.n
Et hopev intereseda ihv
notation. 3 frames and their arguments are shown: DESIR-
want.v wishn wihv ...
yu m o h
A0 A1 Jly gu
ING is evoked by want, ACTIVITY_FINISH by finish, and HOLD-
AMADV wav01
ACTIVITY_FNISH: cmplete.v
the pople rally wan us to stay te course and finish th job .
ING_OFF_ON by hold off. Thin horizontal lines representing
gen cy coclude.v finis.v ...
HOF hld ffv
argument spans are labeled with role_names. (Notnshown: July
and August evoke CALENDRIC_UNIT and fill its Unit role.)
A A3 stay-v-01
</figure>
<figureCaption confidence="0.853">
Figure 2: A PropBank-annotated sentence from OntoNotes
(Hovy et al., 2006). The PB lexicon defines rolesets (verb
sense–specific frames) and their core roles: e.g., finish-v-01
</figureCaption>
<bodyText confidence="0.964522714285714">
‘cause to stop’, A0 ‘intentional agent’, A1 ‘thing finishing’, and
A2 ‘explicit instrument, thing finished with’. (finish-v-03,by
contrast, means ‘apply a finish, as to wood’.) Clear similarities
to the FrameNet annotations in figure 1 are evident, though
PB uses lexical frames rather than deep frames and makes
some different decisions about roles (e.g., want-v-01 has no
analogue to Focal_participant).
</bodyText>
<listItem confidence="0.9753315">
• a PropBank-style SRL system, by using guide
features (§3.4).2
</listItem>
<bodyText confidence="0.999719">
These expansions of the training corpus and the
feature set for supervised argument identification
are integrated into SEMAFOR (Das et al., 2014),
the leading open-source frame-semantic parser
for English. We observe a 4% F1 improvement
in argument identification on the FrameNet test
set, leading to a 1% F1 improvement on the full
frame-semantic parsing task. Our code and mod-
els are available at http://www.ark.cs.cmu.edu/
SEMAFOR/.
</bodyText>
<sectionHeader confidence="0.997876" genericHeader="introduction">
2 FrameNet
</sectionHeader>
<bodyText confidence="0.92893">
FrameNet represents events, scenarios, and rela-
tionships with an inventory of frames (such as
</bodyText>
<footnote confidence="0.98212875">
2Preliminary experiments training on PropBank annota-
tions mapped to FrameNet via SemLink 1.2.2c (Bonial et al.,
2013) hurt performance, likely due to errors and coverage
gaps in the mappings.
</footnote>
<figure confidence="0.99423795">
Event
DESIRING: eager.a hope.n
hope.v interested.a itch.v
want.v wish.n wish.v ...
do you want me to hold off until I finish July and August ?
ACTIVITY_FINISH: complete.v
conclude.v finish.v ...
Activity
Agent
End_point
Desirable_action: ∅
HOLDING_OFF_ON: hold off.v
wait.v
Agent
A0 AM-ADV A1 want-v-01
the people really want us to stay the course and finish the job .
A0 A1 finish-v-01
stay-v-01
A1
A3
</figure>
<page confidence="0.767557">
218
</page>
<bodyText confidence="0.884420785714286">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 218–224,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
SHOPPING and SCARCITY). Each frame is associ-
ated with a set of roles (or frame elements) called
to mind in order to understand the scenario, and
lexical predicates (verbs, nouns, adjectives, and
adverbs) capable of evoking the scenario. For ex-
ample, the BODY—MOVEMENT frame has Agent and
Body_part as its core roles, and lexical entries in-
cluding verbs such as bend, blink, crane, and curtsy,
plus the noun use of curtsy. In FrameNet 1.5, there
are over 1,000 frames and 12,000 lexical predi-
cates.
</bodyText>
<subsectionHeader confidence="0.973988">
2.1 Hierarchy
</subsectionHeader>
<bodyText confidence="0.999521242424242">
The FrameNet lexicon is organized as a network,
with several kinds of frame-to-frame relations
linking pairs of frames and (subsets of) their ar-
guments (Ruppenhofer et al., 2010). In this work,
we consider two kinds of frame-to-frame relations:
Inheritance: E.g., ROBBERY inherits from
COMMITTING—CRIME, which inherits from MIS-
DEED. Crucially, roles in inheriting frames
are mapped to corresponding roles in inher-
ited frames: ROBBERY.Perpetrator links to
COMMITTING—CRIME.Perpetrator, which links to
MISDEED.Wrongdoer, and so forth.
Subframe: This indicates a subevent within a
complex event. E.g., the CRIMINAL—PROCESS frame
groups together subframes ARREST, ARRAIGN-
MENT and TRIAL. CRIMINAL—PROCESS.Defendant,
for instance, is mapped to ARREST.Suspect,
TRIAL.Defendant, and SENTENCING.Convict.
We say that a parent of a role is one that has
either the Inheritance or Subframe relation to it.
There are 4,138 Inheritance and 589 Subframe
links among role types in FrameNet 1.5.
Prior work has considered various ways of group-
ing role labels together in order to share statisti-
cal strength. Matsubayashi et al. (2009) observed
small gains from using the Inheritance relation-
ships and also from grouping by the role name
(SEMAFOR already incorporates such features).
Johansson (2012) reports improvements in SRL for
Swedish, by exploiting relationships between both
frames and roles. Baldewein et al. (2004) learn
latent clusters of roles and role-fillers, reporting
mixed results. Our approach is described in §3.2.
</bodyText>
<subsectionHeader confidence="0.998189">
2.2 Annotations
</subsectionHeader>
<bodyText confidence="0.9974395">
Statistics for the annotations appear in table 1.
Full-text (FT): This portion of the FrameNet cor-
pus consists of documents and has about 5,000
sentences for which annotators assigned frames
</bodyText>
<table confidence="0.998550333333333">
Full-Text Exemplars
train test train test
Sentences 2,780 2,420 137,515 4,132
Frames 15,019 4,458 137,515 4,132
Overt arguments 25,918 7,210 278,985 8,417
TYPES
Frames 642 470 862 562
Roles 2,644 1,420 4,821 1,224
Unseen frames vs. train: 46 0
Roles in unseen frames vs. train: 178 0
Unseen roles vs. train: 289 38
Unseen roles vs. combined train: 103 32
</table>
<tableCaption confidence="0.99107">
Table 1: Characteristics of the training and test data. (These
statistics exclude the development set, which contains 4,463
frames over 746 sentences.)
</tableCaption>
<bodyText confidence="0.999975653846154">
and arguments to as many words as possible. Be-
ginning with the SemEval-2007 shared task on
FrameNet analysis, frame-semantic parsers have
been trained and evaluated on the full-text data
(Baker et al., 2007; Das et al., 2014).3 The full-text
documents represent a mix of genres, prominently
including travel guides and bureaucratic reports
about weapons stockpiles.
Exemplars: To document a given predicate, lexi-
cographers manually select corpus examples and
annotate them only with respect to the predicate
in question. These singly-annotated sentences
from FrameNet are called lexicographic exem-
plars. There are over 140,000 sentences containing
argument annotations and relative to the FT dataset,
these contain an order of magnitude more frame
annotations and over two orders of magnitude more
sentences. As these were manually selected, the
rate of overt arguments per frame is noticeably
higher than in the FT data. The exemplars formed
the basis of early studies of frame-semantic role
labeling (e.g., Gildea and Jurafsky, 2002; Thomp-
son et al., 2003; Fleischman et al., 2003; Litkowski,
2004; Kwon et al., 2004). Exemplars have not yet
been exploited successfully to improve role label-
ing performance on the more realistic FT task.4
</bodyText>
<subsectionHeader confidence="0.996842">
2.3 PropBank
</subsectionHeader>
<bodyText confidence="0.756162166666667">
PropBank (PB; Palmer et al., 2005) is a lexicon and
corpus of predicate–argument structures that takes
a shallower approach than FrameNet. FrameNet
frames cluster lexical predicates that evoke sim-
3Though these were annotated at the document level,
and train/development/test splits are by document, the frame-
semantic parsing is currently restricted to the sentence level.
4Das and Smith (2011, 2012) investigated semi-supervised
techniques using the exemplars and WordNet for frame iden-
tification. Hermann et al. (2014) also improve frame iden-
tification by mapping frames and predicates into the same
continuous vector space, allowing statistical sharing.
</bodyText>
<page confidence="0.995703">
219
</page>
<bodyText confidence="0.999850666666667">
ilar kinds of scenarios In comparison, PropBank
frames are purely lexical and there are no formal
relations between different predicates or their roles.
PropBank’s sense distinctions are generally coarser-
grained than FrameNet’s. Moreover, FrameNet lex-
ical entries cover many different parts of speech,
while PropBank focuses on verbs and (as of re-
cently) eventive noun and adjective predicates. An
example with PB annotations is shown in figure 2.
</bodyText>
<sectionHeader confidence="0.992436" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99997225">
We use the model from SEMAFOR (Das et al.,
2014), detailed in §3.1, as a starting point. We ex-
periment with techniques that augment the model’s
training data (§3.3) and feature set (§3.2, §3.4).
</bodyText>
<subsectionHeader confidence="0.98456">
3.1 Baseline
</subsectionHeader>
<bodyText confidence="0.998990647058824">
In SEMAFOR, the argument identification task is
treated as a structured prediction problem. Let
the classification input be a dependency-parsed
sentence x, the token(s) p constituting the pred-
icate in question, and the frame f evoked by p (as
determined by frame identification). We use the
heuristic procedure described by (Das et al., 2014)
for extracting candidate argument spans for the
predicate; call this spans(x,p, f). spans always
includes a special span denoting an empty or non-
overt role, denoted 0. For each candidate argument
a ∈ spans(x, p, f) and each role r, a binary feature
vector φ(a,x, p, f,r) is extracted. We use the fea-
ture extractors from (Das et al., 2014) as a baseline,
adding additional ones in our experiments (§3.2–
§3.4). Each a is given a real-valued score by a
linear model:
</bodyText>
<equation confidence="0.729341">
scorew(a I x, p, f,r) = w&apos;φ(a,x, p, f,r) (1)
</equation>
<bodyText confidence="0.999445428571429">
The model parameters w are learned from data (§4).
Prediction requires choosing a joint assignment
of all arguments of a frame, respecting the con-
straints that a role may be assigned to at most one
span, and spans of overt arguments must not over-
lap. Beam search, with a beam size of 100, is used
to find this argmax.5
</bodyText>
<subsectionHeader confidence="0.99823">
3.2 Hierarchy Features
</subsectionHeader>
<bodyText confidence="0.9995735">
We experiment with features shared between re-
lated roles of related frames in order to capture
</bodyText>
<footnote confidence="0.940014">
5Recent work has improved upon global decoding tech-
niques (Das et al., 2012; Täckström et al., 2015). We expect
such improvements to be complementary to the gains due to
the added features and data reported here.
</footnote>
<bodyText confidence="0.999861266666667">
statistical generalizations about the kinds of argu-
ments seen in those roles. Our hypothesis is that
this will be beneficial given the small number of
training examples for individual roles.
All roles that have a common parent based on
the Inheritance and Subframe relations will share
a set of features in common. Specifically, for each
base feature φ which is conjoined with the role r
in the baseline model (φ ∧&amp;quot;role=r&amp;quot;), and for each
parent r′ of r, we add a new copy of the feature
that is the base feature conjoined with the parent
role, (φ ∧&amp;quot;parent_role=r′&amp;quot;). We experimented with
using more than one level of the hierarchy (e.g.,
grandparents), but the additional levels did not im-
prove performance.
</bodyText>
<subsectionHeader confidence="0.998814">
3.3 Domain Adaptation and Exemplars
</subsectionHeader>
<bodyText confidence="0.999892733333333">
Daumé (2007) proposed a feature augmentation
approach that is now widely used in supervised
domain adaptation scenarios. We use a variant
of this approach. Let Dex denote the exemplars
training data, and Dft denote the full text training
data. For every feature φ(a,x, p, f,r) in the base
model, we add a new feature φft(⋅) that fires only
if φ(⋅) fires and x ∈ Dft. The intuition is that each
base feature contributes both a “general” weight
and a “domain-specific” weight to the model; thus,
it can exhibit a general preference for specific roles,
but this general preference can be fine-tuned for
the domain. Regularization encourages the model
to use the general version over the domain-specific,
if possible.
</bodyText>
<subsectionHeader confidence="0.966163">
3.4 Guide Features
</subsectionHeader>
<bodyText confidence="0.999808">
Another approach to domain adaptation is to train a
supervised model on a source domain, make predic-
tions using that model on the target domain, then
use those predictions as additional features while
training a new model on the target domain. The
source domain model is effectively a form of pre-
processing, and the features from its output are
known as guide features (Johansson, 2013; Kong
et al., 2014).6
In our case, the full text data is our target do-
main, and PropBank and the exemplars data are our
source domains, respectively. For PropBank, we
run the SRL system of Illinois Curator 1.1.4 (Pun-
</bodyText>
<footnote confidence="0.931771">
6This is related to the technique of model stacking, where
successively richer models are trained by cross-validation on
the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and
McDonald, 2008; Martins et al., 2008).
</footnote>
<page confidence="0.99429">
220
</page>
<bodyText confidence="0.999570777777778">
yakanok et al., 2008)7 on verbs in the full-text data.
For the exemplars, we train baseline SEMAFOR
on the exemplars and run it on the full-text data.
We use two types of guide features: one encodes
the role label predicted by the source model, and
the other indicates that a span a was assigned some
role. For the exemplars, we use an additional fea-
ture to indicate that the predicted role matches the
role being filled.
</bodyText>
<sectionHeader confidence="0.99636" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.997783">
Following SEMAFOR, we train using a local ob-
jective, treating each role and span pair as an in-
dependent training instance. We have made two
modifications to training which had negligible im-
pact on full-text accuracy, but decreased training
time significantly:8
</bodyText>
<listItem confidence="0.9928388">
• We use the online optimization method
AdaDelta (Zeiler, 2012) with minibatches, in-
stead of the batch method L-BFGS (Liu and
Nocedal, 1989). We use minibatches of size
4,000 on the full text data, and 40,000 on the
exemplar data.
• We minimize squared structured hinge loss
instead of a log-linear loss. Let ((x, p, f,r),a)
be the ith training example. Then the squared
hinge loss is given by Lw(i) =
</listItem>
<bodyText confidence="0.944314666666667">
(mr
We learn w by minimizing the `2-regularized aver-
age loss on the dataset:
</bodyText>
<equation confidence="0.79146">
Lw(i)+2X11w1122 (2)
</equation>
<sectionHeader confidence="0.998777" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999992777777778">
We use the same FrameNet 1.5 data and train/test
splits as Das et al. (2014). Automatic syntactic de-
pendency parses from MSTParserStacked (Martins
et al., 2008) are used, as in Das et al. (2014).
Preprocessing. Out of 145,838 exemplar sen-
tences, we removed 4,191 sentences which had
no role annotations. We removed sentences that ap-
peared in the full-text data. We also merged spans
which were adjacent and had the same role label.
</bodyText>
<footnote confidence="0.9609245">
7http://cogcomp.cs.illinois.edu/page/software_
view/SRL
8With SEMAFOR’s original features and training data, the
result of the above changes is that full-text F1 decreases from
59.3% to 59.1%, while training time (running optimization to
convergence) decreases from 729 minutes to 82 minutes.
</footnote>
<table confidence="0.985207">
Training Configuration Model P R F1
(Features) Size (%) (%) (%)
FT (Baseline) 1.1 65.6 53.8 59.1
FT (Hierarchy) 1.9 67.2 54.8 60.4
guide
Exemplars ÐÐÐ→ FT 1.2 65.2 55.9 60.2
FT+Exemplars (Basic) 5.0 66.0 58.2 61.9
FT+Exemplars (DA) 5.8 65.7 59.0 62.2
PB-SRL guide
ÐÐÐ→ FT 1.2 65.0 54.8 59.5
Combining the best methods
PB-SRL guide
ÐÐÐ→ FT+Exemplars 5.5 67.4 58.8 62.8
FT+Exemplars (Hierarchy) 9.3 66.0 60.4 63.1
</table>
<tableCaption confidence="0.996627">
Table 2: Argument identification results on the full-text test
set. Model size is in millions of features.
</tableCaption>
<bodyText confidence="0.999649428571429">
Hyperparameter tuning. We determined the
stopping criterion and the `2 regularization parame-
ter X by tuning on the FT development set, search-
ing over the following values for X: 10−5, 10−7,
10−9, 10−12.
Evaluation. A complete frame-semantic parsing
system involves frame identification and argument
identification. We perform two evaluations: one as-
suming gold-standard frames are given, to evaluate
argument identification alone; and one using the
output of the system described by Hermann et al.
(2014), the current state-of-the-art in frame identi-
fication, to demonstrate that our improvements are
retained when incorporated into a full system.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999916714285714">
Argument Identification. We present precision,
recall, and F1-measure microaveraged across the
test instances in table 2, for all approaches. The
evaluation used in Das et al. (2014) assesses both
frames and arguments; since our focus is on SRL,
we only report performance for arguments, ren-
dering our scores more interpretable. Under our
argument-only evaluation, the system of Das et al.
(2014) gets 59.3% F1.
The first block shows baseline performance. The
next block shows the benefit of FrameNet hierarchy
features (+1.2% F1). The third block shows that
using exemplars as training data, especially with
domain adaptation, is preferable to using them as
guide features (2.8% F1 vs. 0.9% F1). PropBank
SRL as guide features offers a small (0.4% F1) gain.
The last two rows of table 2 show the perfor-
mance upon combining the best approaches. Both
use full-text and exemplars for training; the first
uses PropBank SRL as guide features, and the sec-
ond adds hierarchy features. The best result is the
</bodyText>
<figure confidence="0.899432333333333">
1
{wTo1
,x, , ,
)}
wT
(a,
,
f
r)2
)
′
x
−
φ
x
p,
,
w∗ = argmin
w
1
N
N
�
i=1
221
0 200 400 600 800 1000 1200 1400
Frame Element, ordered by test set frequency
(a) Frequency of each role appearing in the test set.
Frame Element, ordered by test set frequency
(b) F1 of the best methods compared with the baseline.
</figure>
<figureCaption confidence="0.9916525">
Figure 3: F1 for each role appearing in the test set, ranked by
frequency. F1 values have been smoothed with loess, with
a smoothing parameter of 0.2. “Siblings” refers to hierarchy
features.
</figureCaption>
<bodyText confidence="0.999515166666667">
latter, gaining 3.95% F1 over the baseline.
Role-level evaluation. Figure 3(b) shows F1 per
frame element, for the baseline and the three best
models. Each x-axis value is one role, sorted by
decreasing frequency (the distribution of role fre-
quencies is shown in figure 3(a)). For frequent
roles, performance is similar; our models achieve
gains on rarer roles.
Full system. When using the frame output of
Hermann et al. (2014), F1 improves by 1.1%, from
66.8% for the baseline, to 67.9% for our combined
model (from the last row in table 2).
</bodyText>
<sectionHeader confidence="0.992493" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999977769230769">
We have empirically shown that auxiliary semantic
resources can benefit the challenging task of frame-
semantic role labeling. The significant gains come
from the FrameNet exemplars and the FrameNet hi-
erarchy, with some signs that the PropBank scheme
can be leveraged as well.
We are optimistic that future improvements to
lexical semantic resources, such as crowdsourced
lexical expansion of FrameNet (Pavlick et al., 2015)
as well as ongoing/planned changes for PropBank
(Bonial et al., 2014) and SemLink (Bonial et al.,
2013), will lead to further gains in this task. More-
over, the techniques discussed here could be further
explored using semi-automatic mappings between
lexical resources (such as UBY; Gurevych et al.,
2012), and correspondingly, this task could be used
to extrinsically validate those mappings.
Ours is not the only study to show benefit from
heterogeneous annotations for semantic analysis
tasks. Feizabadi and Padó (2015), for example,
successfully applied similar techniques for SRL of
implicit arguments.9 Ultimately, given the diversity
of semantic resources, we expect that learning from
heterogeneous annotations in different corpora will
be necessary to build automatic semantic analyzers
that are both accurate and robust.
</bodyText>
<sectionHeader confidence="0.994348" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999888333333333">
The authors are grateful to Dipanjan Das for his as-
sistance, and to anonymous reviewers for their help-
ful feedback. This research has been supported by
the Richard King Mellon Foundation and DARPA
grant FA8750-12-2-0342 funded under the DEFT
program.
</bodyText>
<sectionHeader confidence="0.993656" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.954377566666667">
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
SemEval-2007 Task 19: frame semantic structure
extraction. In Proc. of SemEval, pages 99–104.
Prague, Czech Republic.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc.
of COLING-ACL, pages 86–90. Montreal, Quebec,
Canada. URL http://framenet.icsi.berkeley.
edu.
Ulrike Baldewein, Katrin Erk, Sebastian Padó, and
Detlef Prescher. 2004. Semantic role labelling
with similarity-based generalization using EM-
based clustering. In Rada Mihalcea and Phil Ed-
monds, editors, Proc. of SENSEUAL-3, the Third
International Workshop on the Evaluation of Sys-
tems for the Semantic Analysis of Text, pages 64–68.
Barcelona, Spain.
Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang, and Martha Palmer. 2014. PropBank: se-
mantics of new predicate types. In Nicoletta Calzo-
lari, Khalid Choukri, Thierry Declerck, Hrafn Lofts-
son, Bente Maegaard, Joseph Mariani, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis, editors,
Proc. of LREC, pages 3013–3019. Reykjavík, Ice-
land.
Claire Bonial, Kevin Stowe, and Martha Palmer. 2013.
Renewing and revising SemLink. In Proc. of the
9They applied frustratingly easy domain adaptation to
learn from FrameNet along with a PropBank-like dataset of
nominal frames.
</reference>
<figure confidence="0.999424666666667">
0 200 400 600 800 1000 1200 1400
F1
0.0 0.2 0.4 0.6 0.8
Baseline (FT)
FT + Exemplars
FT + Exemplars + PB
FT + Exemplars + Siblings
0 50 100 150
Test Examples
</figure>
<page confidence="0.962349">
222
</page>
<reference confidence="0.996829151785714">
2nd Workshop on Linked Data in Linguistics (LDL-
2013): Representing and linking lexicons, terminolo-
gies and other language data, pages 9–17. Pisa,
Italy.
William W. Cohen and Vitor R. Carvalho. 2005.
Stacked sequential learning. In Proc. ofIJCAI, pages
671–676. Edinburgh, Scotland, UK.
Dipanjan Das, Desai Chen, André F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9–56. URL http://www.ark.cs.cmu.
edu/SEMAFOR.
Dipanjan Das, André F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Proc.
of *SEM, pages 209–217. Montréal, Canada.
Dipanjan Das and Noah A. Smith. 2011. Semi-
supervised frame-semantic parsing for unknown
predicates. In Proc. of ACL-HLT, pages 1435–1444.
Portland, Oregon, USA.
Dipanjan Das and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proc. of NAACL-HLT, pages 677–687. Montréal,
Canada.
Hal Daumé, III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256–263. Prague,
Czech Republic.
Parvin Sadat Feizabadi and Sebastian Padó. 2015.
Combining seemingly incompatible corpora for im-
plicit semantic role labeling. In Proc. of *SEM,
pages 40–50. Denver, Colorado, USA.
Charles J. Fillmore and Collin Baker. 2009. A frames
approach to semantic analysis. In Bernd Heine and
Heiko Narrog, editors, The Oxford Handbook of Lin-
guistic Analysis, pages 791–816. Oxford University
Press, Oxford, UK.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet clas-
sification. In Michael Collins and Mark Steedman,
editors, Proc. of EMNLP, pages 49–56.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-
mann, Michael Matuschek, Christian M. Meyer, and
Christian Wirth. 2012. UBY - a large-scale unified
lexical-semantic resource based on LMF. In Proc. of
EACL, pages 580–590. Avignon, France.
Karl Moritz Hermann, Dipanjan Das, Jason Weston,
and Kuzman Ganchev. 2014. Semantic frame iden-
tification with distributed word representations. In
Proc. of ACL, pages 1448–1458. Baltimore, Mary-
land, USA.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
the 90% solution. In Proc. of HLT-NAACL, pages
57–60. New York City, USA.
Richard Johansson. 2012. Non-atomic classification to
improve a semantic role labeler for a low-resource
language. In Proc. of *SEM, pages 95–99. Montréal,
Canada.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT, pages
127–137. Atlanta, Georgia, USA.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah A. Smith. 2014. A dependency parser for
tweets. In Proc. of EMNLP, pages 1001–1012.
Doha, Qatar.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. FrameNet-based semantic parsing using max-
imum entropy models. In Proc. of Coling, pages
1233–1239. Geneva, Switzerland.
Ken Litkowski. 2004. SENSEVAL-3 task: Automatic
labeling of semantic roles. In Rada Mihalcea and
Phil Edmonds, editors, Proc. of SENSEVAL-3, the
Third International Workshop on the Evaluation of
Systems for the Semantic Analysis of Text, pages 9–
12. Barcelona, Spain.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Program., 45(3):503–528.
André F. T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP, pages 157–166. Honolulu, Hawaii.
Yuichiroh Matsubayashi, Naoaki Okazaki, and Jun’ichi
Tsujii. 2009. A comparative study on generalization
of semantic roles in FrameNet. In Proc. of ACL-
IJCNLP, pages 19–27. Suntec, Singapore.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT, pages 950–958.
Columbus, Ohio, USA.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: an annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi,
Chris Callison-Burch, Mark Drezde, and Benjamin
Van Durme. 2015. FrameNet+: Fast paraphrastic
tripling of FrameNet. In Proc. of ACL-IJCNLP. Bei-
jing, China.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference
in semantic role labeling. Computational Linguis-
tics, 34(2):257–287. URL http://cogcomp.cs.
illinois.edu/page/software_view/SRL.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: extended theory and prac-
tice. URL https://framenet2.icsi.berkeley.
edu/docs/r1.5/book.pdf.
</reference>
<page confidence="0.98478">
223
</page>
<reference confidence="0.99947">
Oscar Täckström, Kuzman Ganchev, and Dipanjan Das.
2015. Efficient inference and structured learning for
semantic role labeling. Transactions of the Associa-
tion for Computational Linguistics, 3:29–41.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic
role labeling. In Machine Learning: ECML 2003,
pages 397–408.
Matthew D. Zeiler. 2012. ADADELTA: An adap-
tive learning rate method. arXiv:1212.5701 [cs].
URL http://arxiv.org/abs/1212.5701, arXiv:
1212.5701.
</reference>
<page confidence="0.998709">
224
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.217333">
<title confidence="0.984223">Frame-Semantic Role Labeling with Heterogeneous Annotations</title>
<author confidence="0.304787">A</author>
<affiliation confidence="0.441547">of Computer Science, Carnegie Mellon University, Pittsburgh, PA,</affiliation>
<address confidence="0.720745">of Informatics, University of Edinburgh, Edinburgh, Scotland, UK</address>
<abstract confidence="0.99762525">We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute inin the original model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>SemEval-2007 Task 19: frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proc. of SemEval,</booktitle>
<pages>99--104</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7458" citStr="Baker et al., 2007" startWordPosition="1130" endWordPosition="1133">58 137,515 4,132 Overt arguments 25,918 7,210 278,985 8,417 TYPES Frames 642 470 862 562 Roles 2,644 1,420 4,821 1,224 Unseen frames vs. train: 46 0 Roles in unseen frames vs. train: 178 0 Unseen roles vs. train: 289 38 Unseen roles vs. combined train: 103 32 Table 1: Characteristics of the training and test data. (These statistics exclude the development set, which contains 4,463 frames over 746 sentences.) and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentence</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. SemEval-2007 Task 19: frame semantic structure extraction. In Proc. of SemEval, pages 99–104. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>86--90</pages>
<location>Montreal, Quebec, Canada.</location>
<note>URL http://framenet.icsi.berkeley. edu.</note>
<contexts>
<context position="953" citStr="Baker et al., 1998" startWordPosition="139" endWordPosition="142">of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model. 1 Introduction Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: • the frame-to-frame relations i</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proc. of COLING-ACL, pages 86–90. Montreal, Quebec, Canada. URL http://framenet.icsi.berkeley. edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrike Baldewein</author>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
<author>Detlef Prescher</author>
</authors>
<title>Semantic role labelling with similarity-based generalization using EMbased clustering.</title>
<date>2004</date>
<booktitle>In Rada Mihalcea and Phil Edmonds, editors, Proc. of SENSEUAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>64--68</pages>
<location>Barcelona, Spain.</location>
<contexts>
<context position="6428" citStr="Baldewein et al. (2004)" startWordPosition="967" endWordPosition="970">ING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames Full-Text Exemplars train test train test Sentences 2,780 2,420 137,515 4,132 Frames 15,019 4,458 137,515 4,132 Overt arguments 25,918 7,210 278,985 8,417 TYPES Frames 642 470 862 562 Roles 2,644 1,420 4,821 1,224 Unseen frames vs. train: 46 0 Roles in unseen frames vs. train: 178 0</context>
</contexts>
<marker>Baldewein, Erk, Padó, Prescher, 2004</marker>
<rawString>Ulrike Baldewein, Katrin Erk, Sebastian Padó, and Detlef Prescher. 2004. Semantic role labelling with similarity-based generalization using EMbased clustering. In Rada Mihalcea and Phil Edmonds, editors, Proc. of SENSEUAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 64–68. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Bonial</author>
<author>Julia Bonn</author>
<author>Kathryn Conger</author>
<author>Jena D Hwang</author>
<author>Martha Palmer</author>
</authors>
<title>PropBank: semantics of new predicate types.</title>
<date>2014</date>
<booktitle>Proc. of LREC,</booktitle>
<pages>3013--3019</pages>
<editor>In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Reykjavík, Iceland.</location>
<contexts>
<context position="19307" citStr="Bonial et al., 2014" startWordPosition="3073" endWordPosition="3076"> F1 improves by 1.1%, from 66.8% for the baseline, to 67.9% for our combined model (from the last row in table 2). 7 Conclusion We have empirically shown that auxiliary semantic resources can benefit the challenging task of framesemantic role labeling. The significant gains come from the FrameNet exemplars and the FrameNet hierarchy, with some signs that the PropBank scheme can be leveraged as well. We are optimistic that future improvements to lexical semantic resources, such as crowdsourced lexical expansion of FrameNet (Pavlick et al., 2015) as well as ongoing/planned changes for PropBank (Bonial et al., 2014) and SemLink (Bonial et al., 2013), will lead to further gains in this task. Moreover, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we exp</context>
</contexts>
<marker>Bonial, Bonn, Conger, Hwang, Palmer, 2014</marker>
<rawString>Claire Bonial, Julia Bonn, Kathryn Conger, Jena D. Hwang, and Martha Palmer. 2014. PropBank: semantics of new predicate types. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proc. of LREC, pages 3013–3019. Reykjavík, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Bonial</author>
<author>Kevin Stowe</author>
<author>Martha Palmer</author>
</authors>
<title>Renewing and revising SemLink.</title>
<date>2013</date>
<booktitle>In Proc. of the 9They</booktitle>
<contexts>
<context position="3753" citStr="Bonial et al., 2013" startWordPosition="564" endWordPosition="567">s and the feature set for supervised argument identification are integrated into SEMAFOR (Das et al., 2014), the leading open-source frame-semantic parser for English. We observe a 4% F1 improvement in argument identification on the FrameNet test set, leading to a 1% F1 improvement on the full frame-semantic parsing task. Our code and models are available at http://www.ark.cs.cmu.edu/ SEMAFOR/. 2 FrameNet FrameNet represents events, scenarios, and relationships with an inventory of frames (such as 2Preliminary experiments training on PropBank annotations mapped to FrameNet via SemLink 1.2.2c (Bonial et al., 2013) hurt performance, likely due to errors and coverage gaps in the mappings. Event DESIRING: eager.a hope.n hope.v interested.a itch.v want.v wish.n wish.v ... do you want me to hold off until I finish July and August ? ACTIVITY_FINISH: complete.v conclude.v finish.v ... Activity Agent End_point Desirable_action: ∅ HOLDING_OFF_ON: hold off.v wait.v Agent A0 AM-ADV A1 want-v-01 the people really want us to stay the course and finish the job . A0 A1 finish-v-01 stay-v-01 A1 A3 218 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Join</context>
<context position="19341" citStr="Bonial et al., 2013" startWordPosition="3079" endWordPosition="3082">or the baseline, to 67.9% for our combined model (from the last row in table 2). 7 Conclusion We have empirically shown that auxiliary semantic resources can benefit the challenging task of framesemantic role labeling. The significant gains come from the FrameNet exemplars and the FrameNet hierarchy, with some signs that the PropBank scheme can be leveraged as well. We are optimistic that future improvements to lexical semantic resources, such as crowdsourced lexical expansion of FrameNet (Pavlick et al., 2015) as well as ongoing/planned changes for PropBank (Bonial et al., 2014) and SemLink (Bonial et al., 2013), will lead to further gains in this task. Moreover, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneo</context>
</contexts>
<marker>Bonial, Stowe, Palmer, 2013</marker>
<rawString>Claire Bonial, Kevin Stowe, and Martha Palmer. 2013. Renewing and revising SemLink. In Proc. of the 9They applied frustratingly easy domain adaptation to learn from FrameNet along with a PropBank-like dataset of nominal frames.</rawString>
</citation>
<citation valid="false">
<title>2nd Workshop on Linked Data in Linguistics (LDL2013): Representing and linking lexicons, terminologies and other language data,</title>
<pages>9--17</pages>
<location>Pisa, Italy.</location>
<marker></marker>
<rawString>2nd Workshop on Linked Data in Linguistics (LDL2013): Representing and linking lexicons, terminologies and other language data, pages 9–17. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
</authors>
<title>Stacked sequential learning.</title>
<date>2005</date>
<booktitle>In Proc. ofIJCAI,</booktitle>
<pages>671--676</pages>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="13581" citStr="Cohen and Carvalho, 2005" startWordPosition="2126" endWordPosition="2129">en use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun6This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the role label predicted by the source model, and the other indicates that a span a was assigned some role. For the exemplars, we use an additional feature to indicate that the predicted role matches the role being filled. 4 Learning Following SEMAFOR, we train using a local objective, treating each role and span pair as an independent training</context>
</contexts>
<marker>Cohen, Carvalho, 2005</marker>
<rawString>William W. Cohen and Vitor R. Carvalho. 2005. Stacked sequential learning. In Proc. ofIJCAI, pages 671–676. Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>André F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<note>URL http://www.ark.cs.cmu. edu/SEMAFOR.</note>
<contexts>
<context position="906" citStr="Das et al., 2014" startWordPosition="131" endWordPosition="134">h, Scotland, UK Abstract We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model. 1 Introduction Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annot</context>
<context position="3240" citStr="Das et al., 2014" startWordPosition="488" endWordPosition="491">1 ‘cause to stop’, A0 ‘intentional agent’, A1 ‘thing finishing’, and A2 ‘explicit instrument, thing finished with’. (finish-v-03,by contrast, means ‘apply a finish, as to wood’.) Clear similarities to the FrameNet annotations in figure 1 are evident, though PB uses lexical frames rather than deep frames and makes some different decisions about roles (e.g., want-v-01 has no analogue to Focal_participant). • a PropBank-style SRL system, by using guide features (§3.4).2 These expansions of the training corpus and the feature set for supervised argument identification are integrated into SEMAFOR (Das et al., 2014), the leading open-source frame-semantic parser for English. We observe a 4% F1 improvement in argument identification on the FrameNet test set, leading to a 1% F1 improvement on the full frame-semantic parsing task. Our code and models are available at http://www.ark.cs.cmu.edu/ SEMAFOR/. 2 FrameNet FrameNet represents events, scenarios, and relationships with an inventory of frames (such as 2Preliminary experiments training on PropBank annotations mapped to FrameNet via SemLink 1.2.2c (Bonial et al., 2013) hurt performance, likely due to errors and coverage gaps in the mappings. Event DESIRI</context>
<context position="7477" citStr="Das et al., 2014" startWordPosition="1134" endWordPosition="1137">rt arguments 25,918 7,210 278,985 8,417 TYPES Frames 642 470 862 562 Roles 2,644 1,420 4,821 1,224 Unseen frames vs. train: 46 0 Roles in unseen frames vs. train: 178 0 Unseen roles vs. train: 289 38 Unseen roles vs. combined train: 103 32 Table 1: Characteristics of the training and test data. (These statistics exclude the development set, which contains 4,463 frames over 746 sentences.) and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were ma</context>
<context position="9670" citStr="Das et al., 2014" startWordPosition="1464" endWordPosition="1467">cation by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. 219 ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. 3 Model We use the model from SEMAFOR (Das et al., 2014), detailed in §3.1, as a starting point. We experiment with techniques that augment the model’s training data (§3.3) and feature set (§3.2, §3.4). 3.1 Baseline In SEMAFOR, the argument identification task is treated as a structured prediction problem. Let the classification input be a dependency-parsed sentence x, the token(s) p constituting the predicate in question, and the frame f evoked by p (as determined by frame identification). We use the heuristic procedure described by (Das et al., 2014) for extracting candidate argument spans for the predicate; call this spans(x,p, f). spans always </context>
<context position="14926" citStr="Das et al. (2014)" startWordPosition="2359" endWordPosition="2362">time significantly:8 • We use the online optimization method AdaDelta (Zeiler, 2012) with minibatches, instead of the batch method L-BFGS (Liu and Nocedal, 1989). We use minibatches of size 4,000 on the full text data, and 40,000 on the exemplar data. • We minimize squared structured hinge loss instead of a log-linear loss. Let ((x, p, f,r),a) be the ith training example. Then the squared hinge loss is given by Lw(i) = (mr We learn w by minimizing the `2-regularized average loss on the dataset: Lw(i)+2X11w1122 (2) 5 Experimental Setup We use the same FrameNet 1.5 data and train/test splits as Das et al. (2014). Automatic syntactic dependency parses from MSTParserStacked (Martins et al., 2008) are used, as in Das et al. (2014). Preprocessing. Out of 145,838 exemplar sentences, we removed 4,191 sentences which had no role annotations. We removed sentences that appeared in the full-text data. We also merged spans which were adjacent and had the same role label. 7http://cogcomp.cs.illinois.edu/page/software_ view/SRL 8With SEMAFOR’s original features and training data, the result of the above changes is that full-text F1 decreases from 59.3% to 59.1%, while training time (running optimization to conver</context>
<context position="16934" citStr="Das et al. (2014)" startWordPosition="2665" endWordPosition="2668">te frame-semantic parsing system involves frame identification and argument identification. We perform two evaluations: one assuming gold-standard frames are given, to evaluate argument identification alone; and one using the output of the system described by Hermann et al. (2014), the current state-of-the-art in frame identification, to demonstrate that our improvements are retained when incorporated into a full system. 6 Results Argument Identification. We present precision, recall, and F1-measure microaveraged across the test instances in table 2, for all approaches. The evaluation used in Das et al. (2014) assesses both frames and arguments; since our focus is on SRL, we only report performance for arguments, rendering our scores more interpretable. Under our argument-only evaluation, the system of Das et al. (2014) gets 59.3% F1. The first block shows baseline performance. The next block shows the benefit of FrameNet hierarchy features (+1.2% F1). The third block shows that using exemplars as training data, especially with domain adaptation, is preferable to using them as guide features (2.8% F1 vs. 0.9% F1). PropBank SRL as guide features offers a small (0.4% F1) gain. The last two rows of ta</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, André F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56. URL http://www.ark.cs.cmu. edu/SEMAFOR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>André F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>An exact dual decomposition algorithm for shallow semantic parsing with constraints.</title>
<date>2012</date>
<booktitle>In Proc. of *SEM,</booktitle>
<pages>209--217</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="11195" citStr="Das et al., 2012" startWordPosition="1727" endWordPosition="1730">4). Each a is given a real-valued score by a linear model: scorew(a I x, p, f,r) = w&apos;φ(a,x, p, f,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features We experiment with features shared between related roles of related frames in order to capture 5Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. statistical generalizations about the kinds of arguments seen in those roles. Our hypothesis is that this will be beneficial given the small number of training examples for individual roles. All roles that have a common parent based on the Inheritance and Subframe relations will share a set of features in common. Specifically, for each base feature φ which is conjoined with the role r in the baseline model (φ ∧&amp;quot;role=r&amp;quot;), and for each parent r′ of r, we add a</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>Dipanjan Das, André F. T. Martins, and Noah A. Smith. 2012. An exact dual decomposition algorithm for shallow semantic parsing with constraints. In Proc. of *SEM, pages 209–217. Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semisupervised frame-semantic parsing for unknown predicates.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>1435--1444</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="8898" citStr="Das and Smith (2011" startWordPosition="1347" endWordPosition="1350">y, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. 219 ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verb</context>
</contexts>
<marker>Das, Smith, 2011</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2011. Semisupervised frame-semantic parsing for unknown predicates. In Proc. of ACL-HLT, pages 1435–1444. Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Graph-based lexicon expansion with sparsity-inducing penalties.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>677--687</pages>
<location>Montréal, Canada.</location>
<marker>Das, Smith, 2012</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2012. Graph-based lexicon expansion with sparsity-inducing penalties. In Proc. of NAACL-HLT, pages 677–687. Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>256--263</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12089" citStr="Daumé (2007)" startWordPosition="1879" endWordPosition="1880">ber of training examples for individual roles. All roles that have a common parent based on the Inheritance and Subframe relations will share a set of features in common. Specifically, for each base feature φ which is conjoined with the role r in the baseline model (φ ∧&amp;quot;role=r&amp;quot;), and for each parent r′ of r, we add a new copy of the feature that is the base feature conjoined with the parent role, (φ ∧&amp;quot;parent_role=r′&amp;quot;). We experimented with using more than one level of the hierarchy (e.g., grandparents), but the additional levels did not improve performance. 3.3 Domain Adaptation and Exemplars Daumé (2007) proposed a feature augmentation approach that is now widely used in supervised domain adaptation scenarios. We use a variant of this approach. Let Dex denote the exemplars training data, and Dft denote the full text training data. For every feature φ(a,x, p, f,r) in the base model, we add a new feature φft(⋅) that fires only if φ(⋅) fires and x ∈ Dft. The intuition is that each base feature contributes both a “general” weight and a “domain-specific” weight to the model; thus, it can exhibit a general preference for specific roles, but this general preference can be fine-tuned for the domain. </context>
</contexts>
<marker>Daumé, 2007</marker>
<rawString>Hal Daumé, III. 2007. Frustratingly easy domain adaptation. In Proc. of ACL, pages 256–263. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parvin Sadat Feizabadi</author>
<author>Sebastian Padó</author>
</authors>
<title>Combining seemingly incompatible corpora for implicit semantic role labeling.</title>
<date>2015</date>
<booktitle>In Proc. of *SEM,</booktitle>
<pages>40--50</pages>
<location>Denver, Colorado, USA.</location>
<marker>Feizabadi, Padó, 2015</marker>
<rawString>Parvin Sadat Feizabadi and Sebastian Padó. 2015. Combining seemingly incompatible corpora for implicit semantic role labeling. In Proc. of *SEM, pages 40–50. Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Collin Baker</author>
</authors>
<title>A frames approach to semantic analysis.</title>
<date>2009</date>
<booktitle>In Bernd Heine and Heiko Narrog, editors, The Oxford Handbook of Linguistic Analysis,</booktitle>
<pages>791--816</pages>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="980" citStr="Fillmore and Baker, 2009" startWordPosition="143" endWordPosition="146">abeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model. 1 Introduction Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: • the frame-to-frame relations in FrameNet, by using hierar</context>
</contexts>
<marker>Fillmore, Baker, 2009</marker>
<rawString>Charles J. Fillmore and Collin Baker. 2009. A frames approach to semantic analysis. In Bernd Heine and Heiko Narrog, editors, The Oxford Handbook of Linguistic Analysis, pages 791–816. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Namhee Kwon</author>
<author>Eduard Hovy</author>
</authors>
<title>Maximum entropy models for FrameNet classification.</title>
<date>2003</date>
<booktitle>Proc. of EMNLP,</booktitle>
<pages>49--56</pages>
<editor>In Michael Collins and Mark Steedman, editors,</editor>
<contexts>
<context position="8334" citStr="Fleischman et al., 2003" startWordPosition="1262" endWordPosition="1265">annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>Michael Fleischman, Namhee Kwon, and Eduard Hovy. 2003. Maximum entropy models for FrameNet classification. In Michael Collins and Mark Steedman, editors, Proc. of EMNLP, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="887" citStr="Gildea and Jurafsky, 2002" startWordPosition="127" endWordPosition="130">sity of Edinburgh, Edinburgh, Scotland, UK Abstract We consider the task of identifying and labeling the semantic arguments of a predicate that evokes a FrameNet frame. This task is challenging because there are only a few thousand fully annotated sentences for supervised training. Our approach augments an existing model with features derived from FrameNet and PropBank and with partially annotated exemplars from FrameNet. We observe a 4% absolute increase in F1 versus the original model. 1 Introduction Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We </context>
<context position="8286" citStr="Gildea and Jurafsky, 2002" startWordPosition="1253" endWordPosition="1256">exicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Judith Eckle-Kohler</author>
<author>Silvana Hartmann</author>
<author>Michael Matuschek</author>
<author>Christian M Meyer</author>
<author>Christian Wirth</author>
</authors>
<title>UBY - a large-scale unified lexical-semantic resource based on LMF.</title>
<date>2012</date>
<booktitle>In Proc. of EACL,</booktitle>
<pages>580--590</pages>
<location>Avignon, France.</location>
<contexts>
<context position="19542" citStr="Gurevych et al., 2012" startWordPosition="3110" endWordPosition="3113">ntic role labeling. The significant gains come from the FrameNet exemplars and the FrameNet hierarchy, with some signs that the PropBank scheme can be leveraged as well. We are optimistic that future improvements to lexical semantic resources, such as crowdsourced lexical expansion of FrameNet (Pavlick et al., 2015) as well as ongoing/planned changes for PropBank (Bonial et al., 2014) and SemLink (Bonial et al., 2013), will lead to further gains in this task. Moreover, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneous annotations in different corpora will be necessary to build automatic semantic analyzers that are both accurate and robust. Acknowledgments The authors are grateful to Dipanjan Das for his assistanc</context>
</contexts>
<marker>Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, Wirth, 2012</marker>
<rawString>Iryna Gurevych, Judith Eckle-Kohler, Silvana Hartmann, Michael Matuschek, Christian M. Meyer, and Christian Wirth. 2012. UBY - a large-scale unified lexical-semantic resource based on LMF. In Proc. of EACL, pages 580–590. Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic frame identification with distributed word representations.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1448--1458</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="9025" citStr="Hermann et al. (2014)" startWordPosition="1364" endWordPosition="1367">loited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. 219 ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. 3 Model We </context>
<context position="16598" citStr="Hermann et al. (2014)" startWordPosition="2616" endWordPosition="2619">60.4 63.1 Table 2: Argument identification results on the full-text test set. Model size is in millions of features. Hyperparameter tuning. We determined the stopping criterion and the `2 regularization parameter X by tuning on the FT development set, searching over the following values for X: 10−5, 10−7, 10−9, 10−12. Evaluation. A complete frame-semantic parsing system involves frame identification and argument identification. We perform two evaluations: one assuming gold-standard frames are given, to evaluate argument identification alone; and one using the output of the system described by Hermann et al. (2014), the current state-of-the-art in frame identification, to demonstrate that our improvements are retained when incorporated into a full system. 6 Results Argument Identification. We present precision, recall, and F1-measure microaveraged across the test instances in table 2, for all approaches. The evaluation used in Das et al. (2014) assesses both frames and arguments; since our focus is on SRL, we only report performance for arguments, rendering our scores more interpretable. Under our argument-only evaluation, the system of Das et al. (2014) gets 59.3% F1. The first block shows baseline per</context>
<context position="18686" citStr="Hermann et al. (2014)" startWordPosition="2974" endWordPosition="2977">. Figure 3: F1 for each role appearing in the test set, ranked by frequency. F1 values have been smoothed with loess, with a smoothing parameter of 0.2. “Siblings” refers to hierarchy features. latter, gaining 3.95% F1 over the baseline. Role-level evaluation. Figure 3(b) shows F1 per frame element, for the baseline and the three best models. Each x-axis value is one role, sorted by decreasing frequency (the distribution of role frequencies is shown in figure 3(a)). For frequent roles, performance is similar; our models achieve gains on rarer roles. Full system. When using the frame output of Hermann et al. (2014), F1 improves by 1.1%, from 66.8% for the baseline, to 67.9% for our combined model (from the last row in table 2). 7 Conclusion We have empirically shown that auxiliary semantic resources can benefit the challenging task of framesemantic role labeling. The significant gains come from the FrameNet exemplars and the FrameNet hierarchy, with some signs that the PropBank scheme can be leveraged as well. We are optimistic that future improvements to lexical semantic resources, such as crowdsourced lexical expansion of FrameNet (Pavlick et al., 2015) as well as ongoing/planned changes for PropBank </context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame identification with distributed word representations. In Proc. of ACL, pages 1448–1458. Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>57--60</pages>
<location>New York City, USA.</location>
<contexts>
<context position="2522" citStr="Hovy et al., 2006" startWordPosition="383" endWordPosition="386">text anDESIRING: eagr.a hope.n Et hopev intereseda ihv notation. 3 frames and their arguments are shown: DESIRwant.v wishn wihv ... yu m o h A0 A1 Jly gu ING is evoked by want, ACTIVITY_FINISH by finish, and HOLDAMADV wav01 ACTIVITY_FNISH: cmplete.v the pople rally wan us to stay te course and finish th job . ING_OFF_ON by hold off. Thin horizontal lines representing gen cy coclude.v finis.v ... HOF hld ffv argument spans are labeled with role_names. (Notnshown: July and August evoke CALENDRIC_UNIT and fill its Unit role.) A A3 stay-v-01 Figure 2: A PropBank-annotated sentence from OntoNotes (Hovy et al., 2006). The PB lexicon defines rolesets (verb sense–specific frames) and their core roles: e.g., finish-v-01 ‘cause to stop’, A0 ‘intentional agent’, A1 ‘thing finishing’, and A2 ‘explicit instrument, thing finished with’. (finish-v-03,by contrast, means ‘apply a finish, as to wood’.) Clear similarities to the FrameNet annotations in figure 1 are evident, though PB uses lexical frames rather than deep frames and makes some different decisions about roles (e.g., want-v-01 has no analogue to Focal_participant). • a PropBank-style SRL system, by using guide features (§3.4).2 These expansions of the tra</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proc. of HLT-NAACL, pages 57–60. New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Non-atomic classification to improve a semantic role labeler for a low-resource language. In</title>
<date>2012</date>
<booktitle>Proc. of *SEM,</booktitle>
<pages>95--99</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="6304" citStr="Johansson (2012)" startWordPosition="951" endWordPosition="952">GNMENT and TRIAL. CRIMINAL—PROCESS.Defendant, for instance, is mapped to ARREST.Suspect, TRIAL.Defendant, and SENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames Full-Text Exemplars train test train test Sentences 2,780 2,420 137,515 4,132 Frames 15,019 4,458 137,515 4,132 Overt arguments 25,918 7,210 278,985 8,417 TYPE</context>
</contexts>
<marker>Johansson, 2012</marker>
<rawString>Richard Johansson. 2012. Non-atomic classification to improve a semantic role labeler for a low-resource language. In Proc. of *SEM, pages 95–99. Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Training parsers on incompatible treebanks.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>127--137</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="13195" citStr="Johansson, 2013" startWordPosition="2063" endWordPosition="2064">hibit a general preference for specific roles, but this general preference can be fine-tuned for the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun6This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use t</context>
</contexts>
<marker>Johansson, 2013</marker>
<rawString>Richard Johansson. 2013. Training parsers on incompatible treebanks. In Proc. of NAACL-HLT, pages 127–137. Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lingpeng Kong</author>
<author>Nathan Schneider</author>
<author>Swabha Swayamdipta</author>
<author>Archna Bhatia</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A dependency parser for tweets.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1001--1012</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="13215" citStr="Kong et al., 2014" startWordPosition="2065" endWordPosition="2068">reference for specific roles, but this general preference can be fine-tuned for the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun6This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide fe</context>
</contexts>
<marker>Kong, Schneider, Swayamdipta, Bhatia, Dyer, Smith, 2014</marker>
<rawString>Lingpeng Kong, Nathan Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer, and Noah A. Smith. 2014. A dependency parser for tweets. In Proc. of EMNLP, pages 1001–1012. Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Namhee Kwon</author>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>FrameNet-based semantic parsing using maximum entropy models.</title>
<date>2004</date>
<booktitle>In Proc. of Coling,</booktitle>
<pages>1233--1239</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="8371" citStr="Kwon et al., 2004" startWordPosition="1268" endWordPosition="1271">dicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and W</context>
</contexts>
<marker>Kwon, Fleischman, Hovy, 2004</marker>
<rawString>Namhee Kwon, Michael Fleischman, and Eduard Hovy. 2004. FrameNet-based semantic parsing using maximum entropy models. In Proc. of Coling, pages 1233–1239. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Litkowski</author>
</authors>
<title>SENSEVAL-3 task: Automatic labeling of semantic roles.</title>
<date>2004</date>
<booktitle>Proc. of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>9--12</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<location>Barcelona, Spain.</location>
<contexts>
<context position="8351" citStr="Litkowski, 2004" startWordPosition="1266" endWordPosition="1267">espect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised techniques using</context>
</contexts>
<marker>Litkowski, 2004</marker>
<rawString>Ken Litkowski. 2004. SENSEVAL-3 task: Automatic labeling of semantic roles. In Rada Mihalcea and Phil Edmonds, editors, Proc. of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 9– 12. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the Limited Memory BFGS Method for Large Scale Optimization.</title>
<date>1989</date>
<journal>Math. Program.,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="14470" citStr="Liu and Nocedal, 1989" startWordPosition="2277" endWordPosition="2280">l predicted by the source model, and the other indicates that a span a was assigned some role. For the exemplars, we use an additional feature to indicate that the predicted role matches the role being filled. 4 Learning Following SEMAFOR, we train using a local objective, treating each role and span pair as an independent training instance. We have made two modifications to training which had negligible impact on full-text accuracy, but decreased training time significantly:8 • We use the online optimization method AdaDelta (Zeiler, 2012) with minibatches, instead of the batch method L-BFGS (Liu and Nocedal, 1989). We use minibatches of size 4,000 on the full text data, and 40,000 on the exemplar data. • We minimize squared structured hinge loss instead of a log-linear loss. Let ((x, p, f,r),a) be the ith training example. Then the squared hinge loss is given by Lw(i) = (mr We learn w by minimizing the `2-regularized average loss on the dataset: Lw(i)+2X11w1122 (2) 5 Experimental Setup We use the same FrameNet 1.5 data and train/test splits as Das et al. (2014). Automatic syntactic dependency parses from MSTParserStacked (Martins et al., 2008) are used, as in Das et al. (2014). Preprocessing. Out of 14</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the Limited Memory BFGS Method for Large Scale Optimization. Math. Program., 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>157--166</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="13630" citStr="Martins et al., 2008" startWordPosition="2134" endWordPosition="2137">e training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun6This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the role label predicted by the source model, and the other indicates that a span a was assigned some role. For the exemplars, we use an additional feature to indicate that the predicted role matches the role being filled. 4 Learning Following SEMAFOR, we train using a local objective, treating each role and span pair as an independent training instance. We have made two modifications to trai</context>
<context position="15010" citStr="Martins et al., 2008" startWordPosition="2370" endWordPosition="2373">012) with minibatches, instead of the batch method L-BFGS (Liu and Nocedal, 1989). We use minibatches of size 4,000 on the full text data, and 40,000 on the exemplar data. • We minimize squared structured hinge loss instead of a log-linear loss. Let ((x, p, f,r),a) be the ith training example. Then the squared hinge loss is given by Lw(i) = (mr We learn w by minimizing the `2-regularized average loss on the dataset: Lw(i)+2X11w1122 (2) 5 Experimental Setup We use the same FrameNet 1.5 data and train/test splits as Das et al. (2014). Automatic syntactic dependency parses from MSTParserStacked (Martins et al., 2008) are used, as in Das et al. (2014). Preprocessing. Out of 145,838 exemplar sentences, we removed 4,191 sentences which had no role annotations. We removed sentences that appeared in the full-text data. We also merged spans which were adjacent and had the same role label. 7http://cogcomp.cs.illinois.edu/page/software_ view/SRL 8With SEMAFOR’s original features and training data, the result of the above changes is that full-text F1 decreases from 59.3% to 59.1%, while training time (running optimization to convergence) decreases from 729 minutes to 82 minutes. Training Configuration Model P R F1</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>André F. T. Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proc. of EMNLP, pages 157–166. Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuichiroh Matsubayashi</author>
<author>Naoaki Okazaki</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A comparative study on generalization of semantic roles in FrameNet.</title>
<date>2009</date>
<booktitle>In Proc. of ACLIJCNLP,</booktitle>
<pages>pages</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6139" citStr="Matsubayashi et al. (2009)" startWordPosition="925" endWordPosition="928"> links to MISDEED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the CRIMINAL—PROCESS frame groups together subframes ARREST, ARRAIGNMENT and TRIAL. CRIMINAL—PROCESS.Defendant, for instance, is mapped to ARREST.Suspect, TRIAL.Defendant, and SENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned f</context>
</contexts>
<marker>Matsubayashi, Okazaki, Tsujii, 2009</marker>
<rawString>Yuichiroh Matsubayashi, Naoaki Okazaki, and Jun’ichi Tsujii. 2009. A comparative study on generalization of semantic roles in FrameNet. In Proc. of ACLIJCNLP, pages 19–27. Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="13607" citStr="Nivre and McDonald, 2008" startWordPosition="2130" endWordPosition="2133">s additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun6This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the role label predicted by the source model, and the other indicates that a span a was assigned some role. For the exemplars, we use an additional feature to indicate that the predicted role matches the role being filled. 4 Learning Following SEMAFOR, we train using a local objective, treating each role and span pair as an independent training instance. We have made tw</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proc. of ACL-HLT, pages 950–958. Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: an annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="8540" citStr="Palmer et al., 2005" startWordPosition="1295" endWordPosition="1298"> and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: an annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellie Pavlick</author>
<author>Travis Wolfe</author>
<author>Pushpendre Rastogi</author>
<author>Chris Callison-Burch</author>
<author>Mark Drezde</author>
<author>Benjamin Van Durme</author>
</authors>
<title>FrameNet+: Fast paraphrastic tripling of FrameNet.</title>
<date>2015</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<location>Beijing, China.</location>
<marker>Pavlick, Wolfe, Rastogi, Callison-Burch, Drezde, Van Durme, 2015</marker>
<rawString>Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Drezde, and Benjamin Van Durme. 2015. FrameNet+: Fast paraphrastic tripling of FrameNet. In Proc. of ACL-IJCNLP. Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<note>URL http://cogcomp.cs. illinois.edu/page/software_view/SRL.</note>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287. URL http://cogcomp.cs. illinois.edu/page/software_view/SRL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: extended theory and practice. URL</title>
<date>2010</date>
<note>https://framenet2.icsi.berkeley. edu/docs/r1.5/book.pdf.</note>
<contexts>
<context position="5201" citStr="Ruppenhofer et al., 2010" startWordPosition="791" endWordPosition="794">les (or frame elements) called to mind in order to understand the scenario, and lexical predicates (verbs, nouns, adjectives, and adverbs) capable of evoking the scenario. For example, the BODY—MOVEMENT frame has Agent and Body_part as its core roles, and lexical entries including verbs such as bend, blink, crane, and curtsy, plus the noun use of curtsy. In FrameNet 1.5, there are over 1,000 frames and 12,000 lexical predicates. 2.1 Hierarchy The FrameNet lexicon is organized as a network, with several kinds of frame-to-frame relations linking pairs of frames and (subsets of) their arguments (Ruppenhofer et al., 2010). In this work, we consider two kinds of frame-to-frame relations: Inheritance: E.g., ROBBERY inherits from COMMITTING—CRIME, which inherits from MISDEED. Crucially, roles in inheriting frames are mapped to corresponding roles in inherited frames: ROBBERY.Perpetrator links to COMMITTING—CRIME.Perpetrator, which links to MISDEED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the CRIMINAL—PROCESS frame groups together subframes ARREST, ARRAIGNMENT and TRIAL. CRIMINAL—PROCESS.Defendant, for instance, is mapped to ARREST.Suspect, TRIAL.Defendant, and SEN</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2010</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2010. FrameNet II: extended theory and practice. URL https://framenet2.icsi.berkeley. edu/docs/r1.5/book.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar Täckström</author>
<author>Kuzman Ganchev</author>
<author>Dipanjan Das</author>
</authors>
<title>Efficient inference and structured learning for semantic role labeling.</title>
<date>2015</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>3--29</pages>
<contexts>
<context position="11220" citStr="Täckström et al., 2015" startWordPosition="1731" endWordPosition="1734">n a real-valued score by a linear model: scorew(a I x, p, f,r) = w&apos;φ(a,x, p, f,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features We experiment with features shared between related roles of related frames in order to capture 5Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. statistical generalizations about the kinds of arguments seen in those roles. Our hypothesis is that this will be beneficial given the small number of training examples for individual roles. All roles that have a common parent based on the Inheritance and Subframe relations will share a set of features in common. Specifically, for each base feature φ which is conjoined with the role r in the baseline model (φ ∧&amp;quot;role=r&amp;quot;), and for each parent r′ of r, we add a new copy of the feature </context>
</contexts>
<marker>Täckström, Ganchev, Das, 2015</marker>
<rawString>Oscar Täckström, Kuzman Ganchev, and Dipanjan Das. 2015. Efficient inference and structured learning for semantic role labeling. Transactions of the Association for Computational Linguistics, 3:29–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>397--408</pages>
<contexts>
<context position="8309" citStr="Thompson et al., 2003" startWordPosition="1257" endWordPosition="1261">ct corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4Das and Smith (2011, 2012) inv</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labeling. In Machine Learning: ECML 2003, pages 397–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: An adaptive learning rate method.</title>
<date>2012</date>
<booktitle>arXiv:1212.5701 [cs]. URL http://arxiv.org/abs/1212.5701,</booktitle>
<pages>1212--5701</pages>
<contexts>
<context position="14393" citStr="Zeiler, 2012" startWordPosition="2266" endWordPosition="2267"> data. We use two types of guide features: one encodes the role label predicted by the source model, and the other indicates that a span a was assigned some role. For the exemplars, we use an additional feature to indicate that the predicted role matches the role being filled. 4 Learning Following SEMAFOR, we train using a local objective, treating each role and span pair as an independent training instance. We have made two modifications to training which had negligible impact on full-text accuracy, but decreased training time significantly:8 • We use the online optimization method AdaDelta (Zeiler, 2012) with minibatches, instead of the batch method L-BFGS (Liu and Nocedal, 1989). We use minibatches of size 4,000 on the full text data, and 40,000 on the exemplar data. • We minimize squared structured hinge loss instead of a log-linear loss. Let ((x, p, f,r),a) be the ith training example. Then the squared hinge loss is given by Lw(i) = (mr We learn w by minimizing the `2-regularized average loss on the dataset: Lw(i)+2X11w1122 (2) 5 Experimental Setup We use the same FrameNet 1.5 data and train/test splits as Das et al. (2014). Automatic syntactic dependency parses from MSTParserStacked (Mart</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: An adaptive learning rate method. arXiv:1212.5701 [cs]. URL http://arxiv.org/abs/1212.5701, arXiv: 1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>