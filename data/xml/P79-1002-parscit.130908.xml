<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.738548">
TOWARDS A SELF-EXTENDING PARSER
</title>
<author confidence="0.510427">
Jaime G. Carbonell
</author>
<affiliation confidence="0.649561">
Department Of Computer Science
Carnegie-Mellon University
</affiliation>
<note confidence="0.460752">
Pittsburgh, PA 15213
</note>
<sectionHeader confidence="0.883494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990013">
This paper discusses an approach to incremental
learning in natural language processing. The
technique of projecting and Integrating semantic
constraints to learn word definitions is analyzed
as implemented in the POLITICS system.
Extensions and improvements of this technique
are developed. The problem of generalizing
existing word meanings and understanding
metaphorical uses of words Is addressed in terms
of semantic constraint integration.
</bodyText>
<sectionHeader confidence="0.992107" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99276075862069">
Natural language analysis, like most other subfields of
Artificial Intelligence and Computational Linguistics, suffers
from the fact that computer systems are unable to
automatically better themselves. Automated learning la
considered a very difficult problem, especially when applied
to natural language understanding. Consequently, little effort
has been focused on this problem. Some pioneering work in
Artificial Intelligence, such as AM [1] and Winston&apos;s learning
system [2] strove to learn or discover concept descriptions
in well-defined domains. Although their efforts produced
interesting ideas and techniques, these techniques do not
fully extend to a domain as complex as natural language
analysis.
Rather than attempting the formidable task of creating a
language learning system, I will discuss techniques for
Incrementally increasing the abilities of a flexible language
analyzer. There are many tasks that can be considered
&amp;quot;Incremental language learning&amp;quot;. Initially the learning domain
is restricted to learning the meaning of new words and
generalizing existing word definitions. There are a number of
A.I. techniques, and combinations of these techniques
capable of exhibiting Incremental learning behavior. I first
discuss FOULUP and POLITICS, two programs that exhibit a
limited capability for incremental word learning. Secondly, the
technique of semantic constraint projection and integration,
as implemented In POLITICS, is analyzed In some detail.
Finally, I discuss the application of some general learning
techniques to the problem of generalizing word definitions
and understanding metaphors.
</bodyText>
<sectionHeader confidence="0.856416" genericHeader="introduction">
2. Learning From Script Expectations
</sectionHeader>
<bodyText confidence="0.982845666666667">
Learning word definitions in semantically-rich contexts is
perhaps one of the simpler tasks of incremental learning.
Initially I confine my discussion to situations where the
meaning of a word can be learned from the Immediately
surrounding context. Later I relax this criterion to see how
global context and multiple examples can help to learn the
</bodyText>
<subsectionHeader confidence="0.511202">
meaning of unknown words.
</subsectionHeader>
<bodyText confidence="0.999836272727272">
The FOULUP program [3] learned the meaning of some
unknown words In the context of applying a script to
understand a story. Scripts [4, 5] are frame-like knowledge
representations abstracting the Important features and
causal structure of mundane events. Scripts have general
expectations of the actions and objects that will be
encountered in processing a story. For instance, the
restaurant script expects to see menus, waitresses, and
customers ordering and eating food (at different
pre-specified times in the story).
FOULUP took advantage of these script expectations to
conclude that Items referenced in the story, which were part
of expected actions, were indeed names of objects that the
script expected to see. These expectations were used to
form definitions of new words. For instance, FOULUP induced
the meaning of &amp;quot;Rabbit&amp;quot; in, &amp;quot;A Rabbit veered off the road
and struck a tree,&amp;quot; to be a self-propelled vehicle. The
system used information about the automobile accident script
to match the unknown word with the script-role &amp;quot;VEHICLE&amp;quot;,
because the script knows that the only objects that veer off
roads to smash into road-side obstructions are self propelled
vehicles.
</bodyText>
<sectionHeader confidence="0.848586" genericHeader="method">
3. Constraint Projection in POLITICS
</sectionHeader>
<bodyText confidence="0.997091538461538">
The POLITICS system [6, 7] induces the meanings of
unknown words by a one-pass syntactic and semantic
constraint projection followed by conceptual enrichment from
planning and world-knowledge inferences. Consider how
POLITICS proceeds when it encounters the unknown word
&amp;quot;MPLA&amp;quot; in analyzing the sentence:
&amp;quot;Russia sent massive arms shipments to the MPLA in Angola.&amp;quot;
Since &amp;quot;MPLA&amp;quot; follows the article &amp;quot;the&amp;quot; it must be a noun,
adjective or adverb. After the word &amp;quot;MPLA&amp;quot;, the preposition
&amp;quot;In&amp;quot; is encountered, thus terminating the current
prepositional phrase begun with &amp;quot;to&amp;quot;. Hence, since all
well-formed prepositional phrases require a head noun, and
the &amp;quot;to&amp;quot; phrase has no other noun, &amp;quot;MINA&amp;quot; must be the head
noun. Thus, by projecting the syntactic constraints
necessary for the sentence to be well formed, one learns
the syntactic category of an unknown word. It Is not always
possible to narrow the categorization of a word to a single
syntactic category from one example. In such cases, I
propose intersecting the sets of possible syntactic
categories from more than one sample use of the unknown
word until the intersection has a single element.
POLITICS learns the meaning of the unknown word by a
similar, but substantially more complex, application of the
same principle of projecting constraints from other parts of
the sentence and subsequently integrating these constraints
to construct a meaning representation. In the example
</bodyText>
<page confidence="0.99707">
3
</page>
<bodyText confidence="0.999875181818182">
above, POLITICS analyzes the verb &amp;quot;to send&amp;quot; as either an
ATRANS or a PTRAHS. (Schank [8] discusses the Conceptual
Dependency case frames. Briefly, a PTRANS is a physical
transfer of location, and an ATRANS is an abstract transfer
of ownership, possession or control.) The reason why
POLITICS cannot decide on the type of TRANSfer is that it
does not know whether the destination of the transfer (I.e.,
the MPLA) 13 a location or an agent. Physical objects, such
as weapons, are PTRANSed to locations but ATRANSed to
agents. The conceptual analysis of the sentence, with MPLA
as yet unresolved, is diagrammed below:
</bodyText>
<figure confidence="0.87707775">
&apos;EMIL &lt;Iss LOC val ANGOLA*
(NKR)
RUSIN d l—s RECIPOI
odU3SIA0 &lt;us or &lt;--- oNERPONs &lt;---I
</figure>
<sectionHeader confidence="0.352664" genericHeader="method">
PTNANS I--&lt; *RUSSIA*
</sectionHeader>
<subsectionHeader confidence="0.623685">
*WRNS* &lt;Isis muneen vl 6, mom)
</subsectionHeader>
<bodyText confidence="0.999721125">
What has the analyzer learned about &amp;quot;MPLA&amp;quot; as a result of
formulating the CD case frame? Clearly the MPLA can only be
an actor (i.e., a person, an institution or a political entity in
the POLITICS domain) or a location. Anything else would
violate the constraints for the recipient case In both ATRANS
and PTRANS. Furthermore, the analyzer knows that the
location of the MPLA is Inside Angola. This Item of information
is integrated with the case constraints to form a partial
definition of &amp;quot;MPLA&amp;quot;. Unfortunately both locations and actors
can be located inside countries; thus, the identity of the
MPLA 13 still not uniquely resolved. POLITICS assigns the
name RECIP01 to the partial definition of &amp;quot;MPLA&amp;quot; and
proceeds to apply its Inference rules to understand the
political implications of the event. Here I discuss only the
Inferences relevant for further specifying the meaning of
&amp;quot;MPLA&amp;quot;.
</bodyText>
<sectionHeader confidence="0.880632" genericHeader="method">
4. Uncertain inference In Learning
</sectionHeader>
<bodyText confidence="0.999471058823529">
POLITICS 13 a goal-driven Int erencer. It must explain all
actions In terms of the goals of the actors and recipients.
The emphasis on inducing the goals of actors and relating
their actions to means of achieving these goals Is integral to
the theory of subjective understanding embodied in
POLITICS. (See En for a detailed discussion.) Thus, POLITICS
tries to determine how the action of sending weapons can be
related to the goals of the Soviet Union or any other possible
actors involved in the situation. POLITICS knows that Angola
was in a state of civil war; that is, a state where political
factions were =xercising their goals of taking military and,
therefore, political control of a country. Since possessing
weapons is a precondition to military actions, POLITICS infers
that the recipient of the weapons may have been one of the
political factions. (Weapons are a means to fulfilling the goal
of a political faction, therefore POLITICS is able to explain
why the faction wants to receive weapons.) Thus, MPLA is
inferred to be a politicai faction. This inference Is integrated
with the existing partial definition and found to be
consistent. Finally, the original action is refined to be an
ATRANS, as transfer of possession of the weapons (not
merely their location) helps the political faction to achieve
its military goal.
Next, POLITICS tries to determine how sending weapons to a
military faction can further the goals of the Soviet Union.
Communist countries have the goal of spreading their
Ideology. POLITICS concludes that this goal can be fulfilled
only if the government of Angola becomes communist. Military
aid to a political faction has the standard goal of military
takeover of the government. Putting these two facts
together, POLITICS concludes that the Russian goal can be
fulfilled if the MPLA, which may become the new Angolan
government, is Communist. The definition formed for MPLA Is
as follows:
</bodyText>
<figure confidence="0.781766">
Nemorv entry&apos;
COPS OPLAo (ma . oiNCTIONst
(PANTO, . .ANGOLA.)
(IDEOLOGY . sconnumisTo
(GONLSs ((SCION (.01PLAO IS
(SCOW OBJECT (*ANGOLA.)
vai. (Ism»;
</figure>
<bodyText confidence="0.995357235294118">
The reason why memory entries are distinct from dictionary
definitions is that there Is no one-to-one mapping between
the two. For instance, &amp;quot;Russia&amp;quot; and &amp;quot;Soviet Union&amp;quot; are two
separate dictionary entries that refer to the same concept In
memory. Similarly, the concept of SCONT (social or political
control) abstracts information useful for the goal-driven
inferences, but has no corresponding entry in the lexicon, as
I found no example where such concept was explicitly
mentioned in newspaper headlines of political conflicts (1.e.,
POLITICS&apos; domain).
Some of the Inferences that POLITICS made are much more
prone to error than others. More specificallY, the syntactic
constraint projections and the CD case-frame projections
are quite certain, but the goal-driven inferences are only
reasonable guesses. For instance, the MPLA could have been
a plateau where Russia deposited its weapons for later
delivery.
</bodyText>
<sectionHeader confidence="0.949616" genericHeader="method">
5. A Strategy for Dealing with Uncertainty
</sectionHeader>
<bodyText confidence="0.999906545454545">
Given such possibilities for error, two possible strategies to
deal with the problem of uncertain Inference come to mind.
First, the system could be restricted to making only the more
certain constraint projection and Integration inferences. This
does not usually produce a complete definition, but the
process may be Iterated for other exemplars where the
unknown word is used In different semantic contexts. Each
time the new word Is encountered, the semantic constraints
are integrated with the previous partial definition until a
complete definition is formulated. The problem with this
process is that it may require a substantial number of
iterations to converge upon a meaning representation, and
when It eventually does, this representation will not be as
rich as the representation resulting from the less certain
goal-driven inferences. For instance, it would be Impossible
to conclude that the MPLA was Communist and wanted to
take over Angola only by projecting semantic constraints.
The second method is based on the system&apos;s ability to
recover from inaccurate inferences. This is the method I
implemented in POLITICS. The first step requires the
detection of contradictions between the inferred Information
and new Incoming Information. The next step is to assign
</bodyText>
<table confidence="0.492522">
OtctIonory entry.
(OPS PER (POS NOUN (TYPE PROPER)))
(TOK .MPLA.))
</table>
<page confidence="0.9776">
4
</page>
<bodyText confidence="0.998718625">
blame to the appropriate culprit, i.e., the inference rule that
asserted the incorrect conclusion. Subsequently, the system
must delete the inaccurate assertion and later inferences
that depended upon it. (See [9] for a model of truth
maintenance.) The final step is to use the new information to
correct the memory entry. The optimal system within my
paradigm would use a combination of both strategies - it
would use its maximal inference capability, recover when
inconsistencies arise, and iterate over many exemplars to
refine and confirm the meaning of the new word. The first
two criteria are present in the POLITICS implementation, but
the system stops building a new definition after proce4sIng a
single exemplar unless it detects a contradiction.
Let us briefly trace through an example where POUTICS la
told that the MPLA is indeed a plateau after it inferred the
meaning to be a political faction.
</bodyText>
<table confidence="0.931970157894737">
t POLITICS run -- 2/06/76 I
*(INTERPRET US-CONSERVATIVE)
INPUT STORY, Russia sent massive arms shipments
to the MPLA in Angola.
PARSING... (UNKNOWN WORD, MPLA)
(SYNTACTIC EXPECTATION, NOUN)
(SEMANTIC EXPECTATION, (FRAME: (ATRANS AVMS) SLOT) RECIP
REO, (LOC ACTOR))) COMPLETED.
CREATING NEW MEMORY ENTRY, *AMA*
INFERENCE, *MPLA* MAY BE A POLITICAL FACTION OF *ANGOLA*
INFERENCE, *RUSSIA* ATRANS *ARMS* TO *APIA*
INFERENCE, *MPLA* IS PROBABLY *COMMUNIST*
INFERENCE, GOAL OF *MLA* IS TO TAKE OVER *ANGOLA*
INSTANTIATING SCRIPT: SAIDAF
INFERENCE, GOAL OF *RUSSIA* IS *ANGOLA* TO BE *COMMUNIST*
I Question-answer dialog I
*What does the MPLA want the arms for,
THE MPLA WANTS TO TAKE OVER ANGOLA USING THE WEAPONS.
What might the other factions in Angola do/
THE OTHER FACTIONS MAY ASK SOME OTHER COUNTRY FOR ARMS.
C Reading further input I
INPUT STORY, *The Zungabi faction operating from the MR
plateau received the Soviet weapons.
PARSING... COMPLETED.
CREATING NEW MEMORY ENTRY: *2UNGABI*
ACTIVE CONTEXT APPLICABLE, SAIDI1F
Cl ISA CONFLICT, *FIALA* ISA (*FACTION* *PLATEAU**
(ACTIVATE (INFCHECK Cl)) REOUESTEO
C2 SCRIPT ROLE CONFLICT,
(SAID-RECIP IN SAIDAF) • *MLR* ANO *ZUNGABI*
(ACTIVATE (INFCHECK C2)) REQUESTED
(INFCHECK CI C2) INVOKED,
ATTEMPT TO MERGE MEMORY ENTRIES: (*APIA* .2UNGAtIls)...FAILURE
INFERENCE RULE CHECKED (AULEATI . SAIDAF)...OK
INFERENCE RULE CHECKED (RULES611)...CONFLICTI
DELETING RESULT OF RULEOGB
Cl RESOLVED* eeLR. ISA *PLATEAU* IN *ANGOLA*
C2 RESOLVED&apos; (BAIT-RECIP IN SAIOMF) • *ZUNGABle
</table>
<sectionHeader confidence="0.786679" genericHeader="method">
REDEFINING *MLA* AS *2UNGA81*...COMPLETED.
CREATING NEW *APIA* MEMORY ENTRY.. .COMPLETED.
</sectionHeader>
<bodyText confidence="0.99603064">
POLITICS realizes that there is an inconsistency in its
interpretation when It tries to integrate &amp;quot;the MPLA plateau&amp;quot;
with its previous definition of &amp;quot;MPLA&amp;quot;. Political factions and
plateaus are different conceptual classes. Furthermore, the
new Input states that the Zungabl received the weapons,
not the MPLA. Assuming that the Input Its correct, POLITICS
searches for an inference rule to assign blame for the
present contradiction. This is done simply by temporarily
deleting the result of each inference rule that was activated
In the original interpretation until the contradiction no longer
exists. The rule that concluded that the MPLA was a political
faction Is found to resolve both contradictions if deleted.
Since recipients of military aid must be political entitles, the
MPLA being a geographical location no longer qualifies as a
military aid recipient.
Finally, POLITICS must check whether the inference rules
that depended upon the result of the deleted rule are no
longer applicable. Rules, such as the one that concluded that
the political faction was communist, depended upon there
being a political faction receiving military aid from Russia.
The Zungabl now fulfils this role; therefore, the inferences
about the MPLA are transfered to the Zungabl, and the MPLA
is redefined to be a plateau. (Note: the word &amp;quot;Zungabl&amp;quot; was
constructed for this example. The MPLA is the present ruling
body of Angola.)
</bodyText>
<sectionHeader confidence="0.936392" genericHeader="method">
6. Extending the Project and integrate Method
</sectionHeader>
<bodyText confidence="0.99974672">
The POLITICS implementation of the project-and-Integrate
technique is by no means complete. POLITICS can only
induce the meaning of concrete or proper nouns when there
Is sufficient contextual Information in a single exemplar.
Furthermore, POLITICS assumes that each unknown word will
have only one meaning. In general It is useful to realize when
a word is used to mean something other than Its definition,
and subsequently formulate an alternative definition.
I illustrate the case where many examples are required to
narrow down the meaning of a word with the following
example: &amp;quot;Johnny told Mary that if she didn&apos;t give him the
toy, he would &lt;unknown-word) her.&amp;quot; One can induce that the
unknown word is a verb, but its meaning can only be guessed
at, In general terms, to be something unfavorable to Mary.
For instance, the unknown word could mean &amp;quot;take the object
from&amp;quot;, or &amp;quot;cause injury to&amp;quot;. One needs more than one
example of the unknown word used to mean the same thing
In different contexts. Then one has a much richer, combined
context from which the meaning can be projected with
greater precision.
Figure 1 diagrams the general project-and-Integrate
algorithm. This extended version of POLITICS&apos; word-learning
technique addresses the problems of Iterating over many
examples, multiple word definitions, and does not restrict Its
Input to certain classes of nouns.
</bodyText>
<sectionHeader confidence="0.908434" genericHeader="method">
7. Generalizing Word Definitions.
</sectionHeader>
<bodyText confidence="0.96287675">
Words can have many senses, some more ri-neral than
others. Let us look at the problem of gen lizing the
semantic definition of a word. Consider the case where
&amp;quot;barrier&amp;quot; is defined to be a physical object that disenabies a
transfer of location. (e.g. &amp;quot;The barrier on the road is blocking
my way.&amp;quot;) Now, let us interpret the sentence, &amp;quot;Import quotas
form a barrier to international trade.&amp;quot; Clearly, an Import quota
Is not a physical object. Thus, one can minimally generalize
&amp;quot;barrier&amp;quot; to mean &amp;quot;anything that disenables a physical
transfer of location.&amp;quot;
Let us substitute &amp;quot;tariff&amp;quot; for &amp;quot;quota&amp;quot; In our example. This
suggests that our meaning for &amp;quot;barrier&amp;quot; Is insufficiently
general. A tariff cannot diaenable physical transfer% tariffs
disenable willingness to buy or sell goods. Thus, one can
further generalize the meaning of barrier to be: &amp;quot;anything
that dleenables any type of transfer&amp;quot;. Yet, some trace of the
</bodyText>
<page confidence="0.988235">
5
</page>
<bodyText confidence="0.932649916666667">
Figure le The project-and-Integrate seethed
for induciny new word and concept definitions
generalization process must be remembered because the
original meaning is often preferred, or metaphorically
referenced. Consider: &amp;quot;The trade bafflers were lifted.&amp;quot; and
&amp;quot;The new legislation bulldozed existing trade barriers.&amp;quot;
rheas sentences can only be understood metaphorically.
rhat is, one needs to refer to the original meaning of
&apos;barrier&amp;quot; as a physical object, in order for &amp;quot;lifting&amp;quot; or
&apos;bulldozing&amp;quot; to make sense. After understanding the literal
leaning of a &amp;quot;bulldozed barrier&amp;quot;, the next step Is to infer
he consequence of such an action, namely, the barrier no
ingot exists. Finally, one can refer to the generalized
leaning of &amp;quot;barrier&amp;quot; to Interpret the proposition that &amp;quot;The
ew legislation caused the trade barriers to be no longer in
xiatence.&amp;quot;
propose the following rules to generalize word definitions
id understand metaphorical references to their original,
irnel definition:
1) If the definition of a word violates the semantic
constraints projected from an interpretation of the
rest of the sentence, create a new word-sense
definition that copies the old definition minimally
relaxing (i.e., generalizing) the violated constraint.
</bodyText>
<listItem confidence="0.982104333333333">
2) in interpreting new sentences always prefer
the most specific definition if applicable.
3) if the generalized definition Is encountered
again in Interpreting text, make It part of the
permanent dictionary.
4) if a word definition requires further
</listItem>
<bodyText confidence="0.999086318181818">
generalization, choose the existing most general
definition and minimally relax Its violated semantic
constraints until a new, yet more general definition
is formed.
5) if the case frame formulated In Interpreting a
sentence projects more specific semantic
constraints onto the word meaning than those
consistent with the entire sentence, interpret the
word using the most specific definition consistent
with the case frame. If the resultant meaning of
the case frame Is inconsistent with the
interpretation of the whole sentence, Infer the
most likely consequence of the partially-build
Conceptual Dependency case frame, and use this
consequence in interpreting the rest of the
sentence.
The process described by rule 5 enables one to Interpret the
metaphoriaal uses of words like &amp;quot;lifted&amp;quot; and &amp;quot;bulldozed&amp;quot; In
our earlier examples. The literal meaning of each word Is
applied to the object case, (i.e., &amp;quot;barrier&amp;quot;), and the inferred
consequence (i.e., destruction of the baffler) is used to
Interpret the full sentence.
</bodyText>
<sectionHeader confidence="0.756035" genericHeader="method">
8. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999079363636364">
There are a multitude of ways to Incrementally improve the
language understanding capabilities of a system. In this
paper I discussed in some detail the process of learning new
words. In lesser detail I presented some Ideas on how to
generalize word meanings and Interpret metaphorical uses of
individual words. There are many more aspects to learning
language and understanding metaphors that I have not
touched upon. For Instance, many metaphors transcend
individual words and phrases. Their interpretation may
require detailed cultural knowledge [101
In order to place some perspective on project-and-Integrate
learning method, consider three general learning mechanisms
capable of Implementing different aspects of incremental
language learning.
Learning by example. This Is perhaps the most
general learning strategy. From several exemplars,
one can Intersect the common concept by, If
necessary, minimally generalizing the meaning of
the known part of each example until a common
subpart is found by Intersection. This common
subpart Is likely to be the meaning of the unknown
section of each exemplar.
Learning by near-miss analysis. Winston [2]
takes full advantage of this technique. It may be
usefully applied to a natural language system that
can interactively generate utterances using the
words it learned, and later be told whether it used
those words correctly, whether It erred seriously,
or whether It came close but failed to understand
a subtle nuance in meaning.
Learning by contextual expectation. Essentially
FOULUP and POLITICS use the method of
projecting contextual expectations to the
</bodyText>
<figure confidence="0.991788710526316">
J.
(
Mow Sentence
conlalnlnc lhe
untnown word
INTEGRATE
all the constraints
to formulate a word
definition
(Contertual,
Olscourse model,
snit goal-driven
Inferences
ERROR
Search for error
amen, Inferences
weeny • least.
certain-first
4r torten
Osiele cuipni
Inference and
unsupported
conclusion,
INC ERROR FOUND
4--
Postulate • new
word sense and
build alternate
definition
PROJECT
the syntactic and
tic constraints
from an analysis of
the other components
In the sentence
INTEGRATE
global congest to
enrich definition
</figure>
<page confidence="0.98488">
6
</page>
<bodyText confidence="0.999683625">
linguistic element whose meaning Is to be Induced.
Much more mileage can be gotten from this
method, especially if one uses strong syntactic
constraints and expectations from other
knowledge sources, such as a discourse model, a
narrative model, knowledge about who is providing
the information, and why the information is being
provided.
</bodyText>
<sectionHeader confidence="0.996926" genericHeader="method">
9. References
</sectionHeader>
<reference confidence="0.99961116">
1. Lenat, D. AM: Discovery In Mathematics as
Heuristic Search. Ph.D. Th., Stanford University,
1977.
2. Winston, P. Learning Structural Descriptions from
Examples. Ph.D. Th., MIT, 1970.
3. Granger, R. FOUL-UP: A Program that Figures Out
Meanings of Words from Context. IJCAI-77, 1977.
4. Schenk, R. C. and Abelson, R. P. Scripts, Goals,
Plans and Understanding. Hillside, NJ: Lawrence
Erlbaum, 1977.
5. Cullingford, R. Script Application: Computer
Understanding of Newspaper Stories. Ph.D. Th.,
Yale University, 1977.
G. Carbonell, J. G. POLITICS: Automated Ideological
Reasoning. Cognitive Science 2, 1 (1978), 27-51.
7. Carbonell, J. G. Subjective Understanding:
Computer Models of Belief Systems.. Ph.D. Th., Yale
University, 1979.
a. Schenk, R. C. Conceptual Information Processing.
Amsterdam: North-Holland, 1975.
9. Doyle, J. Truth Maintenance Systems for Problem
Solving. Master Th., M.I.T., 1978.
TO. lakoff, G. and Johnson, M. Towards an
Experimentalist Philosopher: The Case From Literal
Metaphor. In preparation for publication, 1979.
</reference>
<page confidence="0.999518">
7
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987356">
<title confidence="0.999146">TOWARDS A SELF-EXTENDING PARSER</title>
<author confidence="0.999988">Jaime G Carbonell</author>
<affiliation confidence="0.9999095">Department Of Computer Science Carnegie-Mellon University</affiliation>
<address confidence="0.999911">Pittsburgh, PA 15213</address>
<abstract confidence="0.998949454545454">This paper discusses an approach to incremental learning in natural language processing. The technique of projecting and Integrating semantic constraints to learn word definitions is analyzed as implemented in the POLITICS system. Extensions and improvements of this technique are developed. The problem of generalizing existing word meanings and understanding metaphorical uses of words Is addressed in terms of semantic constraint integration.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>D Lenat</author>
</authors>
<title>AM: Discovery In Mathematics as Heuristic Search.</title>
<tech>Ph.D. Th.,</tech>
<institution>Stanford University,</institution>
<contexts>
<context position="1046" citStr="[1]" startWordPosition="139" endWordPosition="139">of generalizing existing word meanings and understanding metaphorical uses of words Is addressed in terms of semantic constraint integration. 1. Introduction Natural language analysis, like most other subfields of Artificial Intelligence and Computational Linguistics, suffers from the fact that computer systems are unable to automatically better themselves. Automated learning la considered a very difficult problem, especially when applied to natural language understanding. Consequently, little effort has been focused on this problem. Some pioneering work in Artificial Intelligence, such as AM [1] and Winston&apos;s learning system [2] strove to learn or discover concept descriptions in well-defined domains. Although their efforts produced interesting ideas and techniques, these techniques do not fully extend to a domain as complex as natural language analysis. Rather than attempting the formidable task of creating a language learning system, I will discuss techniques for Incrementally increasing the abilities of a flexible language analyzer. There are many tasks that can be considered &amp;quot;Incremental language learning&amp;quot;. Initially the learning domain is restricted to learning the meaning of ne</context>
</contexts>
<marker>1.</marker>
<rawString>Lenat, D. AM: Discovery In Mathematics as Heuristic Search. Ph.D. Th., Stanford University,</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Winston</author>
</authors>
<title>Learning Structural Descriptions from Examples.</title>
<date>1970</date>
<location>Ph.D. Th., MIT,</location>
<contexts>
<context position="1080" citStr="[2]" startWordPosition="144" endWordPosition="144">ings and understanding metaphorical uses of words Is addressed in terms of semantic constraint integration. 1. Introduction Natural language analysis, like most other subfields of Artificial Intelligence and Computational Linguistics, suffers from the fact that computer systems are unable to automatically better themselves. Automated learning la considered a very difficult problem, especially when applied to natural language understanding. Consequently, little effort has been focused on this problem. Some pioneering work in Artificial Intelligence, such as AM [1] and Winston&apos;s learning system [2] strove to learn or discover concept descriptions in well-defined domains. Although their efforts produced interesting ideas and techniques, these techniques do not fully extend to a domain as complex as natural language analysis. Rather than attempting the formidable task of creating a language learning system, I will discuss techniques for Incrementally increasing the abilities of a flexible language analyzer. There are many tasks that can be considered &amp;quot;Incremental language learning&amp;quot;. Initially the learning domain is restricted to learning the meaning of new words and generalizing existing </context>
<context position="21339" citStr="[2]" startWordPosition="3281" endWordPosition="3281">[101 In order to place some perspective on project-and-Integrate learning method, consider three general learning mechanisms capable of Implementing different aspects of incremental language learning. Learning by example. This Is perhaps the most general learning strategy. From several exemplars, one can Intersect the common concept by, If necessary, minimally generalizing the meaning of the known part of each example until a common subpart is found by Intersection. This common subpart Is likely to be the meaning of the unknown section of each exemplar. Learning by near-miss analysis. Winston [2] takes full advantage of this technique. It may be usefully applied to a natural language system that can interactively generate utterances using the words it learned, and later be told whether it used those words correctly, whether It erred seriously, or whether It came close but failed to understand a subtle nuance in meaning. Learning by contextual expectation. Essentially FOULUP and POLITICS use the method of projecting contextual expectations to the J. ( Mow Sentence conlalnlnc lhe untnown word INTEGRATE all the constraints to formulate a word definition (Contertual, Olscourse model, snit</context>
</contexts>
<marker>2.</marker>
<rawString>Winston, P. Learning Structural Descriptions from Examples. Ph.D. Th., MIT, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>FOUL-UP: A Program that</title>
<date>1977</date>
<booktitle>Figures Out Meanings of Words from Context. IJCAI-77,</booktitle>
<contexts>
<context position="2655" citStr="[3]" startWordPosition="373" endWordPosition="373">yzed In some detail. Finally, I discuss the application of some general learning techniques to the problem of generalizing word definitions and understanding metaphors. 2. Learning From Script Expectations Learning word definitions in semantically-rich contexts is perhaps one of the simpler tasks of incremental learning. Initially I confine my discussion to situations where the meaning of a word can be learned from the Immediately surrounding context. Later I relax this criterion to see how global context and multiple examples can help to learn the meaning of unknown words. The FOULUP program [3] learned the meaning of some unknown words In the context of applying a script to understand a story. Scripts [4, 5] are frame-like knowledge representations abstracting the Important features and causal structure of mundane events. Scripts have general expectations of the actions and objects that will be encountered in processing a story. For instance, the restaurant script expects to see menus, waitresses, and customers ordering and eating food (at different pre-specified times in the story). FOULUP took advantage of these script expectations to conclude that Items referenced in the story, w</context>
</contexts>
<marker>3.</marker>
<rawString>Granger, R. FOUL-UP: A Program that Figures Out Meanings of Words from Context. IJCAI-77, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schenk</author>
<author>R P Scripts Abelson</author>
</authors>
<title>Goals, Plans and Understanding.</title>
<date>1977</date>
<location>Hillside, NJ: Lawrence Erlbaum,</location>
<contexts>
<context position="2771" citStr="[4, 5]" startWordPosition="393" endWordPosition="394">eralizing word definitions and understanding metaphors. 2. Learning From Script Expectations Learning word definitions in semantically-rich contexts is perhaps one of the simpler tasks of incremental learning. Initially I confine my discussion to situations where the meaning of a word can be learned from the Immediately surrounding context. Later I relax this criterion to see how global context and multiple examples can help to learn the meaning of unknown words. The FOULUP program [3] learned the meaning of some unknown words In the context of applying a script to understand a story. Scripts [4, 5] are frame-like knowledge representations abstracting the Important features and causal structure of mundane events. Scripts have general expectations of the actions and objects that will be encountered in processing a story. For instance, the restaurant script expects to see menus, waitresses, and customers ordering and eating food (at different pre-specified times in the story). FOULUP took advantage of these script expectations to conclude that Items referenced in the story, which were part of expected actions, were indeed names of objects that the script expected to see. These expectations</context>
</contexts>
<marker>4.</marker>
<rawString>Schenk, R. C. and Abelson, R. P. Scripts, Goals, Plans and Understanding. Hillside, NJ: Lawrence Erlbaum, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cullingford</author>
</authors>
<title>Script Application: Computer Understanding of Newspaper Stories.</title>
<date>1977</date>
<journal>G. Carbonell, J. G. POLITICS: Automated Ideological Reasoning. Cognitive Science</journal>
<tech>Ph.D. Th.,</tech>
<volume>2</volume>
<pages>27--51</pages>
<institution>Yale University,</institution>
<contexts>
<context position="2771" citStr="[4, 5]" startWordPosition="393" endWordPosition="394">eralizing word definitions and understanding metaphors. 2. Learning From Script Expectations Learning word definitions in semantically-rich contexts is perhaps one of the simpler tasks of incremental learning. Initially I confine my discussion to situations where the meaning of a word can be learned from the Immediately surrounding context. Later I relax this criterion to see how global context and multiple examples can help to learn the meaning of unknown words. The FOULUP program [3] learned the meaning of some unknown words In the context of applying a script to understand a story. Scripts [4, 5] are frame-like knowledge representations abstracting the Important features and causal structure of mundane events. Scripts have general expectations of the actions and objects that will be encountered in processing a story. For instance, the restaurant script expects to see menus, waitresses, and customers ordering and eating food (at different pre-specified times in the story). FOULUP took advantage of these script expectations to conclude that Items referenced in the story, which were part of expected actions, were indeed names of objects that the script expected to see. These expectations</context>
</contexts>
<marker>5.</marker>
<rawString>Cullingford, R. Script Application: Computer Understanding of Newspaper Stories. Ph.D. Th., Yale University, 1977. G. Carbonell, J. G. POLITICS: Automated Ideological Reasoning. Cognitive Science 2, 1 (1978), 27-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
</authors>
<title>Subjective Understanding: Computer Models of Belief Systems..</title>
<date>1979</date>
<tech>Ph.D. Th.,</tech>
<publisher>North-Holland,</publisher>
<institution>Yale University,</institution>
<location>Amsterdam:</location>
<contexts>
<context position="3874" citStr="[6, 7]" startWordPosition="562" endWordPosition="563">t of expected actions, were indeed names of objects that the script expected to see. These expectations were used to form definitions of new words. For instance, FOULUP induced the meaning of &amp;quot;Rabbit&amp;quot; in, &amp;quot;A Rabbit veered off the road and struck a tree,&amp;quot; to be a self-propelled vehicle. The system used information about the automobile accident script to match the unknown word with the script-role &amp;quot;VEHICLE&amp;quot;, because the script knows that the only objects that veer off roads to smash into road-side obstructions are self propelled vehicles. 3. Constraint Projection in POLITICS The POLITICS system [6, 7] induces the meanings of unknown words by a one-pass syntactic and semantic constraint projection followed by conceptual enrichment from planning and world-knowledge inferences. Consider how POLITICS proceeds when it encounters the unknown word &amp;quot;MPLA&amp;quot; in analyzing the sentence: &amp;quot;Russia sent massive arms shipments to the MPLA in Angola.&amp;quot; Since &amp;quot;MPLA&amp;quot; follows the article &amp;quot;the&amp;quot; it must be a noun, adjective or adverb. After the word &amp;quot;MPLA&amp;quot;, the preposition &amp;quot;In&amp;quot; is encountered, thus terminating the current prepositional phrase begun with &amp;quot;to&amp;quot;. Hence, since all well-formed prepositional phrases requ</context>
</contexts>
<marker>7.</marker>
<rawString>Carbonell, J. G. Subjective Understanding: Computer Models of Belief Systems.. Ph.D. Th., Yale University, 1979. a. Schenk, R. C. Conceptual Information Processing. Amsterdam: North-Holland, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Doyle</author>
</authors>
<title>Truth Maintenance Systems for Problem Solving. Master Th.,</title>
<date>1978</date>
<location>M.I.T.,</location>
<contexts>
<context position="11564" citStr="[9]" startWordPosition="1781" endWordPosition="1781">ly by projecting semantic constraints. The second method is based on the system&apos;s ability to recover from inaccurate inferences. This is the method I implemented in POLITICS. The first step requires the detection of contradictions between the inferred Information and new Incoming Information. The next step is to assign OtctIonory entry. (OPS PER (POS NOUN (TYPE PROPER))) (TOK .MPLA.)) 4 blame to the appropriate culprit, i.e., the inference rule that asserted the incorrect conclusion. Subsequently, the system must delete the inaccurate assertion and later inferences that depended upon it. (See [9] for a model of truth maintenance.) The final step is to use the new information to correct the memory entry. The optimal system within my paradigm would use a combination of both strategies - it would use its maximal inference capability, recover when inconsistencies arise, and iterate over many exemplars to refine and confirm the meaning of the new word. The first two criteria are present in the POLITICS implementation, but the system stops building a new definition after proce4sIng a single exemplar unless it detects a contradiction. Let us briefly trace through an example where POUTICS la </context>
</contexts>
<marker>9.</marker>
<rawString>Doyle, J. Truth Maintenance Systems for Problem Solving. Master Th., M.I.T., 1978. TO. lakoff, G. and Johnson, M. Towards an Experimentalist Philosopher: The Case From Literal Metaphor. In preparation for publication, 1979.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>