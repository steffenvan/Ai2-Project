<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.076646">
<title confidence="0.9884845">
Olympus: an open-source framework
for conversational spoken language interface research
</title>
<author confidence="0.995784">
Dan Bohus, Antoine Raux, Thomas K. Harris,
Maxine Eskenazi, Alexander I. Rudnicky
</author>
<affiliation confidence="0.9907205">
School of Computer Science
Carnegie Mellon University
</affiliation>
<email confidence="0.878205">
{dbohus, antoine, tkharris, max, air}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998573" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999650307692308">
We introduce Olympus, a freely available
framework for research in conversational
interfaces. Olympus’ open, transparent,
flexible, modular and scalable nature fa-
cilitates the development of large-scale,
real-world systems, and enables research
leading to technological and scientific ad-
vances in conversational spoken language
interfaces. In this paper, we describe the
overall architecture, several systems
spanning different domains, and a number
of current research efforts supported by
Olympus.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997479375">
Spoken language interfaces developed in industrial
and academic settings differ in terms of goals, the
types of tasks and research questions addressed,
and the kinds of resources available.
In order to be economically viable, most indus-
try groups need to develop real-world applications
that serve large and varied customer populations.
As a result, they gain insight into the research
questions that are truly significant for current-
generation technologies. When needed, they are
able to focus large resources (typically unavailable
in academia) on addressing these questions. To
protect their investments, companies do not gener-
ally disseminate new technologies and results.
In contrast, academia pursues long-term scien-
tific research goals, which are not tied to immedi-
</bodyText>
<page confidence="0.99642">
32
</page>
<bodyText confidence="0.999912342857143">
ate economic returns or customer populations. As a
result, academic groups are free to explore a larger
variety of research questions, even with a high risk
of failure or a lack of immediate payoff. Academic
groups also engage in a more open exchange of
ideas and results. However, building spoken lan-
guage interfaces requires significant investments
that are sometimes beyond the reach of academic
researchers. As a consequence, research in acade-
mia is oftentimes conducted with toy systems and
skewed user populations. In turn, this raises ques-
tions about the validity of the results and hinders
the research impact.
In an effort to address this problem and facilitate
research on relevant, real-world questions, we have
developed Olympus, a freely available framework
for building and studying conversational spoken
language interfaces. The Olympus architecture,
described in Section 3, has its roots in the CMU
Communicator project (Rudnicky et al., 1999).
Based on that experience and subsequent projects,
we have engineered Olympus into an open, trans-
parent, flexible, modular, and scalable architecture.
To date, Olympus has been used to develop and
deploy a number of spoken language interfaces
spanning different domains and interaction types;
these systems are presented in Section 4. They are
currently supporting research on diverse aspects of
spoken language interaction. Section 5 discusses
three such efforts: error handling, multi-participant
conversation, and turn-taking.
We believe that Olympus and other similar tool-
kits, discussed in Section 6, are essential in order
to bridge the gap between industry and academia.
Such frameworks lower the cost of entry for re-
</bodyText>
<note confidence="0.6772255">
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 32–39,
NAACL-HLT, Rochester, NY, April 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99298525">
search on practical conversational interfaces. They
also promote technology transfer through the reuse
of components, and support direct comparisons
between systems and technologies.
</bodyText>
<sectionHeader confidence="0.989629" genericHeader="method">
2 Desired characteristics
</sectionHeader>
<bodyText confidence="0.993607545454546">
While developing Olympus, we identified a num-
ber of characteristics that in our opinion are neces-
sary to effectively support and foster research. The
framework should be open, transparent, flexible,
modular, and scalable.
Open. Complete source code should be avail-
able for all the components so that researchers and
engineers can inspect and modify it towards their
ends. Ideally, source code should be free for both
research and commercial purposes and grow
through contributions from the user community.
Transparent / Analytic. Open source code
promotes transparency, but beyond that researchers
must be able to analyze the system’s behavior. To
this end, every component should provide detailed
accounts of their internal state. Furthermore, tools
for data visualization and analysis should be an
integral part of the framework.
Flexible. The framework should be able to ac-
commodate a wide range of applications and re-
search interests, and allow easy integration of new
technologies.
</bodyText>
<subsectionHeader confidence="0.628405">
Modular / Reusable. Specific functions (e.g.
</subsectionHeader>
<bodyText confidence="0.9999322">
speech recognition, parsing) should be encapsu-
lated in components with rich and well-defined
interfaces, and an application-independent design.
This will promote reusability, and will lessen the
system development effort.
Scalable. While frameworks that rely on sim-
ple, well established approaches (e.g. finite-state
dialogs in VoiceXML) allow the development of
large-scale systems, this is usually not the case for
frameworks that provide the flexibility and trans-
parency needed for research. However, some re-
search questions are not apparent until one moves
from toy systems into large-scale applications. The
framework should strive to not compromise scal-
ability for the sake of flexibility or transparency.
</bodyText>
<sectionHeader confidence="0.990771" genericHeader="method">
3 Architecture
</sectionHeader>
<bodyText confidence="0.997796537037037">
At the high level, a typical Olympus application
consists of a series of components connected in a
classical, pipeline architecture, as illustrated by the
bold components in Figure 1. The audio signal for
the user utterance is captured and passed through a
speech recognition module that produces a recog-
nition hypothesis (e.g., two p.m.). The recognition
hypothesis is then forwarded to a language under-
standing component that extracts the relevant con-
cepts (e.g., [time=2p.m.]), and then through a
confidence annotation module that assigns a confi-
dence score. Next, a dialog manager integrates this
semantic input into the current context, and pro-
duces the next action to be taken by the system in
the form of the semantic output (e.g., {request
end_time}). A language generation module pro-
duces the corresponding surface form, which is
subsequently passed to a speech synthesis module
and rendered as audio.
Galaxy communication infrastructure. While
the pipeline shown in bold in Figure 1 captures the
logical flow of information in the system, in prac-
tice the system components communicate via a
centralized message-passing infrastructure – Gal-
axy (Seneff et al., 1998). Each component is im-
plemented as a separate process that connects to a
traffic router – the Galaxy hub. The messages are
sent through the hub, which forwards them to the
appropriate destination. The routing logic is de-
scribed via a configuration script.
Speech recognition. Olympus uses the Sphinx
decoding engine (Huang et al., 1992). A recogni-
tion server captures the audio stream, forwards it to
a set of parallel recognition engines, and collects
the corresponding recognition results. The set of
best hypotheses (one from each engine) is then
forwarded to the language understanding compo-
nent. The recognition engines can also generate n-
best lists, but that process significantly slows down
the systems and has not been used live. Interfaces
to connect Sphinx-II and Sphinx-III engines, as
well as a DTMF (touch-tone) decoder to the recog-
nition server are currently available. The individual
recognition engines can use either n-gram- or
grammar-based language models. Dialog-state
specific as well as class-based language models are
supported, and tools for constructing language and
acoustic models from data are readily available.
Most of the Olympus systems described in the next
section use two gender-specific Sphinx-II recog-
nizers in parallel. Other parallel decoder configura-
tions can also be created and used.
Language understanding is performed by
Phoenix, a robust semantic parser (Ward and Issar,
</bodyText>
<page confidence="0.99649">
33
</page>
<figure confidence="0.999677233333333">
Text I/O
TTYSERVER
Recognition
Server
Synthesis
KALLIOPE
SPHINX
SPHIN
Until what time
would you like
the room?
Parsing
PHOENIX
two p.m.
HUB
Lang. Gen
ROSETTA
Confidence
HELIOS
{request end_time}
[time=2pm]
Dialog. Mgr.
RAVENCLAW
[time=2pm]/0.65
Application
Back-end
Date-Time
resolution
Process
Monitor
</figure>
<figureCaption confidence="0.999987">
Figure 1. The Olympus dialog system reference architecture (a typical system)
</figureCaption>
<bodyText confidence="0.999979586206897">
1994). Phoenix uses a semantic grammar to parse
the incoming set of recognition hypotheses. This
grammar is assembled by concatenating a set of
reusable grammar rules that capture domain-
independent constructs like [Yes], [No], [Help],
[Repeat], and [Number], with a set of domain-
specific grammar rules authored by the system de-
veloper. For each recognition hypothesis the output
of the parser consists of a sequence of slots con-
taining the concepts extracted from the utterance.
Confidence annotation. From Phoenix, the set
of parsed hypotheses is passed to Helios, the con-
fidence annotation component. Helios uses features
from different knowledge sources in the system
(e.g., recognition, understanding, dialog) to com-
pute a confidence score for each hypothesis. This
score reflects the probability of correct understand-
ing, i.e. how much the system trusts that the cur-
rent semantic interpretation corresponds to the
user’s intention. The hypothesis with the highest
score is forwarded to the dialog manager.
Dialog management. Olympus uses the Raven-
Claw dialog management framework (Bohus and
Rudnicky, 2003). In a RavenClaw-based dialog
manager, the domain-specific dialog task is repre-
sented as a tree whose internal nodes capture the
hierarchical structure of the dialog, and whose
leaves encapsulate atomic dialog actions (e.g., ask-
ing a question, providing an answer, accessing a
database). A domain-independent dialog engine
executes this dialog task, interprets the input in the
current dialog context and decides which action to
engage next. In the process, the dialog manager
may exchange information with other domain-
specific agents (e.g., application back-end, data-
base access, temporal reference resolution agent).
Language generation. The semantic output of
the dialog manager is sent to the Rosetta template-
based language generation component, which pro-
duces the corresponding surface form. Like the
Phoenix grammar, the language generation tem-
plates are assembled by concatenating a set of pre-
defined, domain-independent templates, with
manually authored task-specific templates.
Speech synthesis. The prompts are synthesized
by the Kalliope speech synthesis module. Kalliope
can be currently configured to use Festival (Black
and Lenzo, 2000), which is an open-source speech
synthesis system, or Cepstral Swift (Cepstral
2005), a commercial engine. Finally, Kalliope also
supports the SSML markup language.
Other components. The various components
briefly described above form the core of the Olym-
pus dialog system framework. Additional compo-
nents have been created throughout the
development of various systems, and, given the
modularity of the architecture, can be easily re-
used. These include a telephony component, a text
</bodyText>
<page confidence="0.995034">
34
</page>
<bodyText confidence="0.999822076923077">
input-and-output interface, and a temporal refer-
ence resolution agent that translates complex date-
time expressions (including relative references,
holidays, etc.) into a canonical form. Recently, a
Jabber interface was implemented to support inter-
actions via the popular GoogleTalk internet mes-
saging system. A Skype speech client component
is also available.
Data Analysis. Last but not least, a variety of
tools for logging, data processing and data ana-
lytics are also available as part of the framework.
These tools have been used for a wide variety of
tasks ranging from system monitoring, to trends
analysis, to training of internal models.
A key characteristic shared by all the Olympus
components is the clear separation between do-
main-independent programs and domain-specific
resources. This decoupling promotes reuse and
lessens the system development effort. To build a
new system, one can focus simply on developing
resources (e.g., language model, grammar, dialog
task specification, generation templates) without
having to do any programming. On the other hand,
since all components are open-source, any part of
the system can be modified, for example to test
new algorithms or compare approaches.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="method">
4 Systems
</sectionHeader>
<bodyText confidence="0.99994375">
To date, the Olympus framework has been used to
successfully build and deploy several spoken dia-
log systems spanning different domains and inter-
action types (see Table 1). Given the limited space,
we discuss only three of these systems in a bit
more detail: Let’s Go!, LARRI, and TeamTalk.
More information about the other systems can be
found in (RavenClaw-Olympus, 2007).
</bodyText>
<subsectionHeader confidence="0.990216">
4.1 Let’s Go!
</subsectionHeader>
<bodyText confidence="0.999806866666667">
The Let’s Go! Bus Information System (Raux et al
2005; 2006) is a telephone-based spoken dialog
system that provides access to bus schedules. In-
teraction with the system starts with an open
prompt, followed by a system-directed phase
where the user is asked the missing information.
Each of the three or four pieces of information
provided (origin, destination, time of travel, and
optional bus route) is explicitly confirmed. The
system knows 12 bus routes, and about 1800 place
names.
Originally developed as an in-lab research sys-
tem, Let’s Go! has been open to the general public
since March, 2005. Outside of business hours, calls
to the bus company are transferred to Let’s Go!,
providing a constant flow of genuine dialogs
(about 40 calls per weeknight and 70 per weekend
night). As of March, 2007, a corpus of about
30,000 calls to the system has been collected and
partially transcribed and annotated. In itself, this
publicly available corpus constitutes a unique re-
source for the community. In addition, the system
itself has been modified for research experiments
(e.g., Raux et al., 2005, Bohus et al., 2006). Be-
tween-system studies have been conducted by run-
ning several versions of the system in parallel and
picking one at random for every call. We have re-
cently opened this system to researchers from other
groups who wish to conduct their own experi-
ments.
</bodyText>
<subsectionHeader confidence="0.864414">
4.2 LARRI
</subsectionHeader>
<bodyText confidence="0.999946965517241">
LARRI (Bohus and Rudnicky, 2002a) is a multi-
modal system for support of maintenance and re-
pair activities for F/A-18 aircraft mechanics. The
system implements an Interactive Electronic Tech-
nical Manual.
LARRI integrates a graphical user interface for
easy visualization of dense technical information
(e.g., instructions, schematics, video-streams) with
a spoken dialog system that facilitates information
access and offers assistance throughout the execu-
tion of procedural tasks. The GUI is accessible via
a translucent head-worn display connected to a
wearable client computer. A rotary mouse (dial)
provides direct access to the GUI elements.
After an initial log-in phase, LARRI guides the
user through the selected task, which consists of a
sequence of steps containing instructions, option-
ally followed by verification questions. Basic steps
can include animations or short video sequences
that can be accessed by the user through the GUI
or through spoken commands. The user can also
take the initiative and access the documentation,
either via the GUI or by simple commands such as
“go to step 15” or “show me the figure”.
The Olympus architecture was easily adapted
for this mobile and multi-modal setting. The wear-
able computer hosts audio input and output clients,
as well as the graphical user interface. The Galaxy
hub architecture allows us to easily connect these
</bodyText>
<page confidence="0.981601">
35
</page>
<table confidence="0.999731681818182">
System name Domain / Description Genre
RoomLine telephone-based system that provides support for conference information access (mixed
(Bohus and Rudnicky 2005) room reservation and scheduling within the School of Com- initiative)
puter Science at CMU.
Let’s Go! Public telephone-based system that provides access to bus schedule information access
(Raux et al 2005) information in the greater Pittsburgh area. (system initiative)
LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance
(Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing
Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance
Assistant guidance and support to the astronauts on the International and procedure browsing
(Aist et al 2002) Space Station during the execution of procedural tasks and
checklists.
TeamTalk multi-participant spoken language command-and-control inter- multi-participant command-
(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control
VERA telephone-based taskable agent that can be instructed to de- voice mail / message deliv-
liver messages to a third party and make wake-up calls. ery
Madeleine text-based dialog system for medical diagnosis. diagnosis
ConQuest telephone-based spoken dialog system that provides confer- information access
(Bohus et al 2007) ence schedule information. (mixed-initiative)
RavenCalendar multimodal dialog system for managing personal calendar information access and
(Stenchikova et al 2007). information, such as meetings, classes, appointments and scheduling
reminders (uses Google Calendar as a back-end)
</table>
<tableCaption confidence="0.999968">
Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems)
</tableCaption>
<bodyText confidence="0.999965">
components to the rest of the system, which runs
on a separate server computer. The rotary-mouse
events from the GUI are rendered as semantic in-
puts and are sent to Helios which in turn forwards
the corresponding messages to the dialog manager.
</bodyText>
<subsectionHeader confidence="0.996696">
4.3 TeamTalk
</subsectionHeader>
<bodyText confidence="0.999955264705882">
TeamTalk (Harris et al., 2005) is a multi-modal
interface that facilitates communication between a
human operator and a team of heterogeneous ro-
bots, and is designed for a multi-robot-assisted
treasure-hunt domain. The human operator uses
spoken language in concert with pen-gestures on a
shared live map to elicit support from teams of ro-
bots. This support comes in the forms of mapping
unexplored areas, searching explored areas for ob-
jects of interest, and leading the human to said ob-
jects. TeamTalk has been built as a fully functional
interface to real robots, including the Pioneer
P2DX and the Segway RMP. In addition, it can
interface with virtual robots within the high-
fidelity USARSim (Balakirsky et al., 2006) simula-
tion environment. TeamTalk constitutes an excel-
lent platform for multi-agent dialog research.
To build TeamTalk, we had to address two chal-
lenges to current architecture. The multi-
participant nature of the interaction required multi-
ple dialog managers; the live map with pen-
gestured references required a multi-modal integra-
tion. Again, the flexibility and transparency of the
Olympus framework allowed for relatively simple
solutions to both of these challenges. To accom-
modate multi-participant dialog, each robot in the
domain is associated with its own RavenClaw-
based dialog manager, but all robots share the
other Olympus components: speech recognition,
language understanding, language generation and
speech synthesis. To accommodate the live map
GUI, a Galaxy server was built in Java that could
send the user’s inputs to Helios and receive outputs
from RavenClaw.
</bodyText>
<sectionHeader confidence="0.999246" genericHeader="method">
5 Research
</sectionHeader>
<bodyText confidence="0.999878285714286">
The Olympus framework, along with the systems
developed using it, provides a robust basis for re-
search in spoken language interfaces. In this sec-
tion, we briefly outline three current research
efforts supported by this architecture. Information
about other supported research can be found in
(RavenClaw-Olympus, 2007).
</bodyText>
<subsectionHeader confidence="0.953545">
5.1 Error handling
</subsectionHeader>
<bodyText confidence="0.999989333333333">
A persistent and important problem in today’s spo-
ken language interfaces is their lack of robustness
when faced with understanding errors. This prob-
lem stems from current limitations in speech rec-
ognition, and appears across most domains and
interaction types. In the last three years, we con-
ducted research aimed at improving robustness in
spoken language interfaces by: (1) endowing them
with the ability to accurately detect errors, (2) de-
</bodyText>
<page confidence="0.996081">
36
</page>
<bodyText confidence="0.9999516">
veloping a rich repertoire of error recovery strate-
gies and (3) developing scalable, data-driven ap-
proaches for building error recovery policies1. Two
of the dialog systems from Table 1 (Let’s Go! and
RoomLine) have provided a realistic experimental
platform for investigating these issues and evaluat-
ing the proposed solutions.
With respect to error detection, we have devel-
oped tools for learning confidence annotation
models by integrating information from multiple
knowledge sources in the system (Bohus and Rud-
nicky, 2002b). Additionally, Bohus and Rudnicky
(2006) proposed a data-driven approach for con-
structing more accurate beliefs in spoken language
interfaces by integrating information across multi-
ple turns in the conversation. Experiments with the
RoomLine system showed that the proposed belief
updating models led to significant improvements
(equivalent with a 13.5% absolute reduction in
WER) in both the effectiveness and the efficiency
of the interaction.
With respect to error recovery strategies, we
have developed and evaluated a large set of strate-
gies for handling misunderstandings and non-
understandings (Bohus and Rudnicky, 2005). The
strategies are implemented in a task-decoupled
manner in the RavenClaw dialog management
framework.
Finally, in (Bohus et al., 2006) we have pro-
posed a novel online-learning based approach for
building error recovery policies over a large set
of non-understanding recovery strategies. An em-
pirical evaluation conducted in the context of the
Let’s Go! system showed that the proposed ap-
proach led to a 12.5% increase in the non-
understanding recovery rate; this improvement was
attained in a relatively short (10-day) time period.
The models, tools and strategies developed
throughout this research can and have been easily
reused in other Olympus-based systems.
</bodyText>
<subsectionHeader confidence="0.998795">
5.2 Multi-participant conversation
</subsectionHeader>
<bodyText confidence="0.999802166666667">
Conversational interfaces are generally built for
one-on-one conversation. This has been a workable
assumption for telephone-based systems, and a
useful one for many single-purpose applications.
However this assumption will soon become
strained as a growing collection of always-
</bodyText>
<subsectionHeader confidence="0.467784">
1 A policy specifies how the system should choose between
different recovery strategies at runtime.
</subsectionHeader>
<bodyText confidence="0.999913068181818">
available agents (e.g., personal trainers, pedestrian
guides, or calendar systems) and embodied agents
(e.g., appliances and robots) feature spoken lan-
guage interfaces. When there are multiple active
agents that wish to engage in spoken dialog, new
issues arise. On the input side, the agents need to
be able to identify the addressee of any given user
utterance. On the output side, the agents need to
address the problem of channel contention, i.e.,
multiple participants speaking over each other.
Two architectural solutions can be envisioned:
(1) the agents share a single interface that under-
stands multi-agent requirements, or (2) each agent
uses its own interface and handles multi-participant
behavior. Agents that provide different services
have different dialog requirements, and we believe
this makes a centralized interface problematic. Fur-
thermore, the second solution better fits human
communication behavior and therefore is likely to
be more natural and habitable.
TeamTalk is a conversational system that fol-
lows the second approach: each robot has its own
dialog manager. Processed user inputs are sent to
all dialog managers in the system; each dialog
manager decides based on a simple algorithm
(Harris et al., 2004) whether or not the current in-
put is addressed to it. If so, an action is taken. Oth-
erwise the input is ignored; it will be processed and
responded to by another robot. On the output side,
to address the channel contention problem, each
RavenClaw output message is augmented with in-
formation about the identity of the robot that gen-
erated it. The shared synthesis component queues
the messages and plays them back sequentially
with the corresponding voice.
We are currently looking into two additional
challenges related to multi-participant dialog. We
are interested in how to address groups and sub-
groups in addition to individuals of a group, and
we are also interested in how to cope with multiple
humans in addition to multiple agents. Some ex-
periments investigating solutions to both of these
issues have been conducted. Analysis of the results
and refinements of these methods are ongoing.
</bodyText>
<subsectionHeader confidence="0.999661">
5.3 Timing and turn-taking
</subsectionHeader>
<bodyText confidence="0.99982025">
While a lot of research has focused on higher lev-
els of conversation such as natural language under-
standing and dialog planning, low-level inter-
actional phenomena such as turn-taking have not
</bodyText>
<page confidence="0.998605">
37
</page>
<bodyText confidence="0.999986263157895">
received as much attention. As a result, current
systems either constrain the interaction to a rigid
one-speaker-at-a-time style or expose themselves
to interactional problems such as inappropriate
delays, spurious interruptions, or turn over-taking
(Raux et al., 2006). To a large extent, these issues
stem from the fact that in common dialog architec-
tures, including Olympus, the dialog manager
works asynchronously from the real world (i.e.,
utterances and actions that are planned are as-
sumed to be executed instantaneously). This means
that user barge-ins and backchannels are often in-
terpreted in an incorrect context, which leads to
confusion, unexpected user behavior and potential
dialog breakdowns. Additionally, dialog systems’
low-level interactional behavior is generally the
result of ad-hoc rules encoded in different compo-
nents that are not precisely coordinated.
In order to investigate and resolve these is-
sues, we are currently developing version 2 of the
Olympus framework. In addition to all the compo-
nents described in this paper, Olympus 2 features
an Interaction Manager which handles the precise
timing of events perceived from the real world
(e.g., user utterances) and of system actions (e.g.,
prompts). By providing an interface between the
actual conversation and the asynchronous dialog
manager, Olympus 2 allows a more reactive behav-
ior without sacrificing the powerful dialog man-
agement features offered by RavenClaw. Olympus
2 is designed so that current Olympus-based sys-
tems can be upgraded with minimal effort.
This novel architecture, initially deployed in
the Let’s Go system, will enable research on turn-
taking and other low-level conversational phenom-
ena. Investigations within the context of other ex-
isting systems, such as LARRI and TeamTalk, will
uncover novel challenges and research directions.
</bodyText>
<sectionHeader confidence="0.996664" genericHeader="discussions">
6 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999553">
The primary goal of the Olympus framework is to
enable research that leads to technological and sci-
entific advances in spoken language interfaces.
Olympus is however by no means a singular ef-
fort. Several other toolkits for research and devel-
opment are available to the community. They
differ on a number of dimensions, such as objec-
tives, scientific underpinnings, as well as techno-
logical and implementation aspects. Several
toolkits, both commercial, e.g., TellMe, BeVocal,
and academic, e.g., Ariadne (2007), SpeechBuilder
(Glass et al., 2004), and the CSLU toolkit (Cole,
1999), are used for rapid development. Some, e.g.,
CSLU and SpeechBuilder, have also been used for
educational purposes. And yet others, such as
Olympus, GALATEEA (Kawamoto et al., 2002)
and DIPPER (Bos et al., 2003) are primarily used
for research. Different toolkits rely on different
theories and dialog representations: finite-state,
slot-filling, plan-based, information state-update.
Each toolkit balances tradeoffs between complex-
ity, ease-of-use, control, robustness, flexibility, etc.
We believe the strengths of the Olympus
framework lie not only in its current components,
but also in its open, transparent, and flexible na-
ture. As we have seen in the previous sections,
these characteristics have allowed us to develop
and deploy practical, real-world systems operating
in a broad spectrum of domains. Through these
systems, Olympus provides an excellent basis for
research on a wide variety of spoken dialog issues.
The modular construction promotes the transfer
and reuse of research contributions across systems.
While desirable, an in-depth understanding of
the differences between all these toolkits remains
an open question. We believe that an open ex-
change of experiences and resources across toolkits
will create a better understanding of the current
state-of-the-art, generate new ideas, and lead to
better systems for everyone. Towards this end, we
are making the Olympus framework, as well as a
number of systems and dialog corpora, freely
available to the community.
</bodyText>
<sectionHeader confidence="0.99649" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999252">
We would like to thank all those who have brought
contributions to the components underlying the
Olympus dialog system framework. Neither Olym-
pus nor the dialog systems discussed in this paper
would have been possible without their help. We
particularly wish to thank Alan W Black for his
continued support and advice. Work on Olympus
components and systems was supported in part by
DARPA, under contract NBCH-D-03-0010, Boe-
ing, under contract CMU-BA-GTA-1, and the US
National Science Foundation under grant number
0208835. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the funding agencies.
</bodyText>
<page confidence="0.998585">
38
</page>
<sectionHeader confidence="0.996342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999901642857143">
Aist, G., Dowding, J., Hockey, B.A., Rayner, M.,
Hieronymus, J., Bohus, D., Boven, B., Blaylock, N.,
Campana, E., Early, S., Gorrell, G., and Phan, S.,
2003. Talking through procedures: An intelligent
Space Station procedure assistant, in Proc. of EACL-
2003, Budapest, Hungary
Ariadne, 2007, The Ariadne web-site, as of January
2007, http://www.opendialog.org/.
Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M.
2006. UsarSim: providing a framework for multi-
robot performance evaluation, in Proc. of PerMIS.
Black, A. and Lenzo, K., 2000. Building Voices in the
Festival Speech System, http://festvox.org/bsv/, 2000.
Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri,
V., Krishna, G., Kumar, K., Raux, A., Tomko, S.,
2007. Conquest – an Open-Source Dialog System for
Conferences, in Proc. of HLT 2007, Rochester, USA.
Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi,
M., Rudnicky, A. 2006. Online Supervised Learning
of Non-understanding Recovery Policies, in Proc. of
SLT-2006, Aruba.
Bohus, D., and Rudnicky, A. 2006. A K-hypotheses +
Other Belief Updating Model, in Proc. of the AAAI
Workshop on Statistical and Empirical Methods in
Spoken Dialogue Systems, 2006.
Bohus, D., and Rudnicky, A., 2005. Sorry I didn’t
Catch That: An Investigation of Non-understanding
Errors and Recovery Strategies, in Proc. of SIGdial-
2005, Lisbon, Portugal.
Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog
Management Using Hierarchical Task Decomposi-
tion and an Expectation Agenda, in Proc. of Eu-
rospeech 2003, Geneva, Switzerland.
Bohus, D., and Rudnicky, A., 2002a. LARRI: A Lan-
guage-based Maintenance and Repair Assistant, in
Proc. of IDS-2002, Kloster Irsee, Germany.
Bohus, D., and Rudnicky, A., 2002b. Integrating Multi-
ple Knowledge Sources in the CMU Communicator
Dialog System, Technical Report CMU-CS-02-190.
Bos, J., Klein, E., Lemon, O., and Oka, T., 2003.
DIPPER: Description and Formalisation of an In-
formation-State Update Dialogue System Architec-
ture, in Proc. of SIGdial-2003, Sapporo, Japan
Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-to-
Speech Synthesizer, http://www.cepstral.com.
Cole, R., 1999. Tools for Research and Education in
Speech Science, in Proc. of the International Confer-
ence of Phonetic Sciences, San Francisco, USA.
Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004.
A Framework for Developing Conversational Inter-
faces, in Proc. of CADUI, Funchal, Portugal.
Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J.
Bodine, K., and Black, A. 2004. A Research Platform
for Multi-Agent Dialogue Dynamics, in Proc. of The
IEEE International Workshop on Robotics and Hu-
man Interactive Communications, Kurashiki, Japan.
Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Hetero-
genous Multi-Robot Dialogues for Search Tasks, in
AAAI Spring Symposium: Dialogical Robots, Palo
Alto, California.
Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee,
K.-F. and Rosenfeld, R., 1992. The SPHINX-II
Speech Recognition System: an overview, in Com-
puter Speech and Language, 7(2), pp 137-148, 1992.
Kawamoto, S., Shimodaira, H., Nitta, T., Nishimoto,
T., Nakamura, S., Itou, K., Morishima, S., Yotsukura,
T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T.,
Tokuda, K., Hirose, K., Minematsu, N., Yamada, A.,
Den, Y., Utsuro, T., and Sagayama, S., 2002. Open-
source software for developing anthropomorphic
spoken dialog agent, in Proc. of PRICAI-02, Interna-
tional Workshop on Lifelike Animated Agents.
Raux, A., Langner, B., Bohus, D., Black, A., and Eske-
nazi, M. 2005, Let&apos;s Go Public! Taking a Spoken
Dialog System to the Real World, in Proc. of Inter-
speech 2005, Lisbon, Portugal.
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. 2006 Doing Research on a Deployed Spoken
Dialogue System: One Year of Let&apos;s Go! Experience,
in Proc. of Interspeech 2006, Pittsburgh, USA.
RavenClaw-Olympus web page, as of January 2007:
http://www.ravenclaw-olympus.org/.
Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C.,
Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Cre-
ating natural dialogs in the Carnegie Mellon Com-
municator system, in Proc. of Eurospeech 1999.
Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and
Zue V. 1998 Galaxy-II: A reference architecture for
conversational system development, in Proc. of
ICSLP98, Sydney, Australia.
Stenchikova, S., Mucha, B., Hoffman, S., Stent, A.,
2007. RavenCalendar: A Multimodal Dialog System
for Managing A Personal Calendar, in Proc. of HLT
2007, Rochester, USA.
Ward, W., and Issar, S., 1994. Recent improvements in
the CMU spoken language understanding system, in
Proc. of the ARPA Human Language Technology
Workshop, pages 213–216, Plainsboro, NJ.
</reference>
<page confidence="0.999532">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453131">
<title confidence="0.9947975">Olympus: an open-source for conversational spoken language interface research</title>
<author confidence="0.9849765">Dan Bohus</author>
<author confidence="0.9849765">Antoine Raux</author>
<author confidence="0.9849765">Thomas K Maxine Eskenazi</author>
<author confidence="0.9849765">I Alexander</author>
<affiliation confidence="0.999765">School of Computer Science Carnegie Mellon University</affiliation>
<email confidence="0.990074">dbohus@cs.cmu.edu</email>
<email confidence="0.990074">antoine@cs.cmu.edu</email>
<email confidence="0.990074">tkharris@cs.cmu.edu</email>
<email confidence="0.990074">max@cs.cmu.edu</email>
<email confidence="0.990074">air@cs.cmu.edu</email>
<abstract confidence="0.995626153846154">We introduce Olympus, a freely available framework for research in conversational interfaces. Olympus’ open, transparent, flexible, modular and scalable nature facilitates the development of large-scale, real-world systems, and enables research leading to technological and scientific advances in conversational spoken language interfaces. In this paper, we describe the overall architecture, several systems spanning different domains, and a number of current research efforts supported by</abstract>
<intro confidence="0.486008">Olympus.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Aist</author>
<author>J Dowding</author>
<author>B A Hockey</author>
<author>M Rayner</author>
<author>J Hieronymus</author>
<author>D Bohus</author>
<author>B Boven</author>
<author>N Blaylock</author>
<author>E Campana</author>
<author>S Early</author>
<author>G Gorrell</author>
<author>S Phan</author>
</authors>
<title>Talking through procedures: An intelligent Space Station procedure assistant, in</title>
<date>2003</date>
<booktitle>Proc. of EACL2003,</booktitle>
<location>Budapest, Hungary</location>
<marker>Aist, Dowding, Hockey, Rayner, Hieronymus, Bohus, Boven, Blaylock, Campana, Early, Gorrell, Phan, 2003</marker>
<rawString>Aist, G., Dowding, J., Hockey, B.A., Rayner, M., Hieronymus, J., Bohus, D., Boven, B., Blaylock, N., Campana, E., Early, S., Gorrell, G., and Phan, S., 2003. Talking through procedures: An intelligent Space Station procedure assistant, in Proc. of EACL2003, Budapest, Hungary</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadne</author>
</authors>
<title>The Ariadne web-site, as of</title>
<date>2007</date>
<pages>http://www.opendialog.org/.</pages>
<contexts>
<context position="26912" citStr="Ariadne (2007)" startWordPosition="4050" endWordPosition="4051">LARRI and TeamTalk, will uncover novel challenges and research directions. 6 Discussion and conclusion The primary goal of the Olympus framework is to enable research that leads to technological and scientific advances in spoken language interfaces. Olympus is however by no means a singular effort. Several other toolkits for research and development are available to the community. They differ on a number of dimensions, such as objectives, scientific underpinnings, as well as technological and implementation aspects. Several toolkits, both commercial, e.g., TellMe, BeVocal, and academic, e.g., Ariadne (2007), SpeechBuilder (Glass et al., 2004), and the CSLU toolkit (Cole, 1999), are used for rapid development. Some, e.g., CSLU and SpeechBuilder, have also been used for educational purposes. And yet others, such as Olympus, GALATEEA (Kawamoto et al., 2002) and DIPPER (Bos et al., 2003) are primarily used for research. Different toolkits rely on different theories and dialog representations: finite-state, slot-filling, plan-based, information state-update. Each toolkit balances tradeoffs between complexity, ease-of-use, control, robustness, flexibility, etc. We believe the strengths of the Olympus </context>
</contexts>
<marker>Ariadne, 2007</marker>
<rawString>Ariadne, 2007, The Ariadne web-site, as of January 2007, http://www.opendialog.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Balakirsky</author>
<author>C Scrapper</author>
<author>S Carpin</author>
<author>M Lewis</author>
</authors>
<title>UsarSim: providing a framework for multirobot performance evaluation, in</title>
<date>2006</date>
<booktitle>Proc. of PerMIS.</booktitle>
<contexts>
<context position="18270" citStr="Balakirsky et al., 2006" startWordPosition="2735" endWordPosition="2738">a human operator and a team of heterogeneous robots, and is designed for a multi-robot-assisted treasure-hunt domain. The human operator uses spoken language in concert with pen-gestures on a shared live map to elicit support from teams of robots. This support comes in the forms of mapping unexplored areas, searching explored areas for objects of interest, and leading the human to said objects. TeamTalk has been built as a fully functional interface to real robots, including the Pioneer P2DX and the Segway RMP. In addition, it can interface with virtual robots within the highfidelity USARSim (Balakirsky et al., 2006) simulation environment. TeamTalk constitutes an excellent platform for multi-agent dialog research. To build TeamTalk, we had to address two challenges to current architecture. The multiparticipant nature of the interaction required multiple dialog managers; the live map with pengestured references required a multi-modal integration. Again, the flexibility and transparency of the Olympus framework allowed for relatively simple solutions to both of these challenges. To accommodate multi-participant dialog, each robot in the domain is associated with its own RavenClawbased dialog manager, but a</context>
</contexts>
<marker>Balakirsky, Scrapper, Carpin, Lewis, 2006</marker>
<rawString>Balakirsky, S., Scrapper, C., Carpin, S., and Lewis, M. 2006. UsarSim: providing a framework for multirobot performance evaluation, in Proc. of PerMIS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
<author>K Lenzo</author>
</authors>
<title>Building Voices in the Festival Speech System,</title>
<date>2000</date>
<location>http://festvox.org/bsv/,</location>
<contexts>
<context position="10655" citStr="Black and Lenzo, 2000" startWordPosition="1568" endWordPosition="1571">agents (e.g., application back-end, database access, temporal reference resolution agent). Language generation. The semantic output of the dialog manager is sent to the Rosetta templatebased language generation component, which produces the corresponding surface form. Like the Phoenix grammar, the language generation templates are assembled by concatenating a set of predefined, domain-independent templates, with manually authored task-specific templates. Speech synthesis. The prompts are synthesized by the Kalliope speech synthesis module. Kalliope can be currently configured to use Festival (Black and Lenzo, 2000), which is an open-source speech synthesis system, or Cepstral Swift (Cepstral 2005), a commercial engine. Finally, Kalliope also supports the SSML markup language. Other components. The various components briefly described above form the core of the Olympus dialog system framework. Additional components have been created throughout the development of various systems, and, given the modularity of the architecture, can be easily reused. These include a telephony component, a text 34 input-and-output interface, and a temporal reference resolution agent that translates complex datetime expression</context>
</contexts>
<marker>Black, Lenzo, 2000</marker>
<rawString>Black, A. and Lenzo, K., 2000. Building Voices in the Festival Speech System, http://festvox.org/bsv/, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>Grau Puerto</author>
<author>S Huggins-Daines</author>
<author>D Keri</author>
<author>V Krishna</author>
<author>G Kumar</author>
<author>K Raux</author>
<author>A Tomko</author>
<author>S</author>
</authors>
<date>2007</date>
<booktitle>Conquest – an Open-Source Dialog System for Conferences, in Proc. of HLT 2007,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="16924" citStr="Bohus et al 2007" startWordPosition="2529" endWordPosition="2532">d procedure browsing (Aist et al 2002) Space Station during the execution of procedural tasks and checklists. TeamTalk multi-participant spoken language command-and-control inter- multi-participant command(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control VERA telephone-based taskable agent that can be instructed to de- voice mail / message delivliver messages to a third party and make wake-up calls. ery Madeleine text-based dialog system for medical diagnosis. diagnosis ConQuest telephone-based spoken dialog system that provides confer- information access (Bohus et al 2007) ence schedule information. (mixed-initiative) RavenCalendar multimodal dialog system for managing personal calendar information access and (Stenchikova et al 2007). information, such as meetings, classes, appointments and scheduling reminders (uses Google Calendar as a back-end) Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) components to the rest of the system, which runs on a separate server computer. The rotary-mouse events from the GUI are rendered as semantic inputs and are sent to Helios which in turn forwards the corresponding messages to the dial</context>
</contexts>
<marker>Bohus, Puerto, Huggins-Daines, Keri, Krishna, Kumar, Raux, Tomko, S, 2007</marker>
<rawString>Bohus, D., Grau Puerto, S., Huggins-Daines, D., Keri, V., Krishna, G., Kumar, K., Raux, A., Tomko, S., 2007. Conquest – an Open-Source Dialog System for Conferences, in Proc. of HLT 2007, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>B Langner</author>
<author>A Raux</author>
<author>A Black</author>
<author>M Eskenazi</author>
<author>A Rudnicky</author>
</authors>
<title>Online Supervised Learning of Non-understanding Recovery Policies, in</title>
<date>2006</date>
<booktitle>Proc. of SLT-2006,</booktitle>
<location>Aruba.</location>
<contexts>
<context position="13872" citStr="Bohus et al., 2006" startWordPosition="2074" endWordPosition="2077">loped as an in-lab research system, Let’s Go! has been open to the general public since March, 2005. Outside of business hours, calls to the bus company are transferred to Let’s Go!, providing a constant flow of genuine dialogs (about 40 calls per weeknight and 70 per weekend night). As of March, 2007, a corpus of about 30,000 calls to the system has been collected and partially transcribed and annotated. In itself, this publicly available corpus constitutes a unique resource for the community. In addition, the system itself has been modified for research experiments (e.g., Raux et al., 2005, Bohus et al., 2006). Between-system studies have been conducted by running several versions of the system in parallel and picking one at random for every call. We have recently opened this system to researchers from other groups who wish to conduct their own experiments. 4.2 LARRI LARRI (Bohus and Rudnicky, 2002a) is a multimodal system for support of maintenance and repair activities for F/A-18 aircraft mechanics. The system implements an Interactive Electronic Technical Manual. LARRI integrates a graphical user interface for easy visualization of dense technical information (e.g., instructions, schematics, vid</context>
<context position="21236" citStr="Bohus et al., 2006" startWordPosition="3178" endWordPosition="3181">integrating information across multiple turns in the conversation. Experiments with the RoomLine system showed that the proposed belief updating models led to significant improvements (equivalent with a 13.5% absolute reduction in WER) in both the effectiveness and the efficiency of the interaction. With respect to error recovery strategies, we have developed and evaluated a large set of strategies for handling misunderstandings and nonunderstandings (Bohus and Rudnicky, 2005). The strategies are implemented in a task-decoupled manner in the RavenClaw dialog management framework. Finally, in (Bohus et al., 2006) we have proposed a novel online-learning based approach for building error recovery policies over a large set of non-understanding recovery strategies. An empirical evaluation conducted in the context of the Let’s Go! system showed that the proposed approach led to a 12.5% increase in the nonunderstanding recovery rate; this improvement was attained in a relatively short (10-day) time period. The models, tools and strategies developed throughout this research can and have been easily reused in other Olympus-based systems. 5.2 Multi-participant conversation Conversational interfaces are genera</context>
</contexts>
<marker>Bohus, Langner, Raux, Black, Eskenazi, Rudnicky, 2006</marker>
<rawString>Bohus, D., Langner, B., Raux, A., Black, A., Eskenazi, M., Rudnicky, A. 2006. Online Supervised Learning of Non-understanding Recovery Policies, in Proc. of SLT-2006, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>A K-hypotheses + Other Belief Updating Model,</title>
<date>2006</date>
<booktitle>in Proc. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems,</booktitle>
<contexts>
<context position="20512" citStr="Bohus and Rudnicky (2006)" startWordPosition="3073" endWordPosition="3076"> with the ability to accurately detect errors, (2) de36 veloping a rich repertoire of error recovery strategies and (3) developing scalable, data-driven approaches for building error recovery policies1. Two of the dialog systems from Table 1 (Let’s Go! and RoomLine) have provided a realistic experimental platform for investigating these issues and evaluating the proposed solutions. With respect to error detection, we have developed tools for learning confidence annotation models by integrating information from multiple knowledge sources in the system (Bohus and Rudnicky, 2002b). Additionally, Bohus and Rudnicky (2006) proposed a data-driven approach for constructing more accurate beliefs in spoken language interfaces by integrating information across multiple turns in the conversation. Experiments with the RoomLine system showed that the proposed belief updating models led to significant improvements (equivalent with a 13.5% absolute reduction in WER) in both the effectiveness and the efficiency of the interaction. With respect to error recovery strategies, we have developed and evaluated a large set of strategies for handling misunderstandings and nonunderstandings (Bohus and Rudnicky, 2005). The strategi</context>
</contexts>
<marker>Bohus, Rudnicky, 2006</marker>
<rawString>Bohus, D., and Rudnicky, A. 2006. A K-hypotheses + Other Belief Updating Model, in Proc. of the AAAI Workshop on Statistical and Empirical Methods in Spoken Dialogue Systems, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>Sorry I didn’t Catch That: An Investigation of Non-understanding Errors and Recovery Strategies,</title>
<date>2005</date>
<booktitle>in Proc. of SIGdial2005,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="15676" citStr="Bohus and Rudnicky 2005" startWordPosition="2355" endWordPosition="2358">by the user through the GUI or through spoken commands. The user can also take the initiative and access the documentation, either via the GUI or by simple commands such as “go to step 15” or “show me the figure”. The Olympus architecture was easily adapted for this mobile and multi-modal setting. The wearable computer hosts audio input and output clients, as well as the graphical user interface. The Galaxy hub architecture allows us to easily connect these 35 System name Domain / Description Genre RoomLine telephone-based system that provides support for conference information access (mixed (Bohus and Rudnicky 2005) room reservation and scheduling within the School of Com- initiative) puter Science at CMU. Let’s Go! Public telephone-based system that provides access to bus schedule information access (Raux et al 2005) information in the greater Pittsburgh area. (system initiative) LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance (Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance Assistant guidance and support to the ast</context>
<context position="21098" citStr="Bohus and Rudnicky, 2005" startWordPosition="3158" endWordPosition="3161">Additionally, Bohus and Rudnicky (2006) proposed a data-driven approach for constructing more accurate beliefs in spoken language interfaces by integrating information across multiple turns in the conversation. Experiments with the RoomLine system showed that the proposed belief updating models led to significant improvements (equivalent with a 13.5% absolute reduction in WER) in both the effectiveness and the efficiency of the interaction. With respect to error recovery strategies, we have developed and evaluated a large set of strategies for handling misunderstandings and nonunderstandings (Bohus and Rudnicky, 2005). The strategies are implemented in a task-decoupled manner in the RavenClaw dialog management framework. Finally, in (Bohus et al., 2006) we have proposed a novel online-learning based approach for building error recovery policies over a large set of non-understanding recovery strategies. An empirical evaluation conducted in the context of the Let’s Go! system showed that the proposed approach led to a 12.5% increase in the nonunderstanding recovery rate; this improvement was attained in a relatively short (10-day) time period. The models, tools and strategies developed throughout this resear</context>
</contexts>
<marker>Bohus, Rudnicky, 2005</marker>
<rawString>Bohus, D., and Rudnicky, A., 2005. Sorry I didn’t Catch That: An Investigation of Non-understanding Errors and Recovery Strategies, in Proc. of SIGdial2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>RavenClaw: Dialog Management Using Hierarchical Task Decomposition and an Expectation Agenda,</title>
<date>2003</date>
<booktitle>in Proc. of Eurospeech</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="9505" citStr="Bohus and Rudnicky, 2003" startWordPosition="1403" endWordPosition="1406">nce annotation. From Phoenix, the set of parsed hypotheses is passed to Helios, the confidence annotation component. Helios uses features from different knowledge sources in the system (e.g., recognition, understanding, dialog) to compute a confidence score for each hypothesis. This score reflects the probability of correct understanding, i.e. how much the system trusts that the current semantic interpretation corresponds to the user’s intention. The hypothesis with the highest score is forwarded to the dialog manager. Dialog management. Olympus uses the RavenClaw dialog management framework (Bohus and Rudnicky, 2003). In a RavenClaw-based dialog manager, the domain-specific dialog task is represented as a tree whose internal nodes capture the hierarchical structure of the dialog, and whose leaves encapsulate atomic dialog actions (e.g., asking a question, providing an answer, accessing a database). A domain-independent dialog engine executes this dialog task, interprets the input in the current dialog context and decides which action to engage next. In the process, the dialog manager may exchange information with other domainspecific agents (e.g., application back-end, database access, temporal reference </context>
</contexts>
<marker>Bohus, Rudnicky, 2003</marker>
<rawString>Bohus, D., and Rudnicky, A., 2003. RavenClaw: Dialog Management Using Hierarchical Task Decomposition and an Expectation Agenda, in Proc. of Eurospeech 2003, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>LARRI: A Language-based Maintenance and Repair Assistant, in</title>
<date>2002</date>
<booktitle>Proc. of IDS-2002,</booktitle>
<location>Kloster Irsee, Germany.</location>
<contexts>
<context position="14166" citStr="Bohus and Rudnicky, 2002" startWordPosition="2125" endWordPosition="2128">f March, 2007, a corpus of about 30,000 calls to the system has been collected and partially transcribed and annotated. In itself, this publicly available corpus constitutes a unique resource for the community. In addition, the system itself has been modified for research experiments (e.g., Raux et al., 2005, Bohus et al., 2006). Between-system studies have been conducted by running several versions of the system in parallel and picking one at random for every call. We have recently opened this system to researchers from other groups who wish to conduct their own experiments. 4.2 LARRI LARRI (Bohus and Rudnicky, 2002a) is a multimodal system for support of maintenance and repair activities for F/A-18 aircraft mechanics. The system implements an Interactive Electronic Technical Manual. LARRI integrates a graphical user interface for easy visualization of dense technical information (e.g., instructions, schematics, video-streams) with a spoken dialog system that facilitates information access and offers assistance throughout the execution of procedural tasks. The GUI is accessible via a translucent head-worn display connected to a wearable client computer. A rotary mouse (dial) provides direct access to the</context>
<context position="16067" citStr="Bohus and Rudnicky 2002" startWordPosition="2410" endWordPosition="2413">terface. The Galaxy hub architecture allows us to easily connect these 35 System name Domain / Description Genre RoomLine telephone-based system that provides support for conference information access (mixed (Bohus and Rudnicky 2005) room reservation and scheduling within the School of Com- initiative) puter Science at CMU. Let’s Go! Public telephone-based system that provides access to bus schedule information access (Raux et al 2005) information in the greater Pittsburgh area. (system initiative) LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance (Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance Assistant guidance and support to the astronauts on the International and procedure browsing (Aist et al 2002) Space Station during the execution of procedural tasks and checklists. TeamTalk multi-participant spoken language command-and-control inter- multi-participant command(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control VERA telephone-based taskable agent that can be instructed to de- vo</context>
<context position="20469" citStr="Bohus and Rudnicky, 2002" startWordPosition="3067" endWordPosition="3071"> language interfaces by: (1) endowing them with the ability to accurately detect errors, (2) de36 veloping a rich repertoire of error recovery strategies and (3) developing scalable, data-driven approaches for building error recovery policies1. Two of the dialog systems from Table 1 (Let’s Go! and RoomLine) have provided a realistic experimental platform for investigating these issues and evaluating the proposed solutions. With respect to error detection, we have developed tools for learning confidence annotation models by integrating information from multiple knowledge sources in the system (Bohus and Rudnicky, 2002b). Additionally, Bohus and Rudnicky (2006) proposed a data-driven approach for constructing more accurate beliefs in spoken language interfaces by integrating information across multiple turns in the conversation. Experiments with the RoomLine system showed that the proposed belief updating models led to significant improvements (equivalent with a 13.5% absolute reduction in WER) in both the effectiveness and the efficiency of the interaction. With respect to error recovery strategies, we have developed and evaluated a large set of strategies for handling misunderstandings and nonunderstandin</context>
</contexts>
<marker>Bohus, Rudnicky, 2002</marker>
<rawString>Bohus, D., and Rudnicky, A., 2002a. LARRI: A Language-based Maintenance and Repair Assistant, in Proc. of IDS-2002, Kloster Irsee, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>Integrating Multiple Knowledge Sources in the CMU Communicator Dialog System,</title>
<date>2002</date>
<tech>Technical Report CMU-CS-02-190.</tech>
<contexts>
<context position="14166" citStr="Bohus and Rudnicky, 2002" startWordPosition="2125" endWordPosition="2128">f March, 2007, a corpus of about 30,000 calls to the system has been collected and partially transcribed and annotated. In itself, this publicly available corpus constitutes a unique resource for the community. In addition, the system itself has been modified for research experiments (e.g., Raux et al., 2005, Bohus et al., 2006). Between-system studies have been conducted by running several versions of the system in parallel and picking one at random for every call. We have recently opened this system to researchers from other groups who wish to conduct their own experiments. 4.2 LARRI LARRI (Bohus and Rudnicky, 2002a) is a multimodal system for support of maintenance and repair activities for F/A-18 aircraft mechanics. The system implements an Interactive Electronic Technical Manual. LARRI integrates a graphical user interface for easy visualization of dense technical information (e.g., instructions, schematics, video-streams) with a spoken dialog system that facilitates information access and offers assistance throughout the execution of procedural tasks. The GUI is accessible via a translucent head-worn display connected to a wearable client computer. A rotary mouse (dial) provides direct access to the</context>
<context position="16067" citStr="Bohus and Rudnicky 2002" startWordPosition="2410" endWordPosition="2413">terface. The Galaxy hub architecture allows us to easily connect these 35 System name Domain / Description Genre RoomLine telephone-based system that provides support for conference information access (mixed (Bohus and Rudnicky 2005) room reservation and scheduling within the School of Com- initiative) puter Science at CMU. Let’s Go! Public telephone-based system that provides access to bus schedule information access (Raux et al 2005) information in the greater Pittsburgh area. (system initiative) LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance (Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance Assistant guidance and support to the astronauts on the International and procedure browsing (Aist et al 2002) Space Station during the execution of procedural tasks and checklists. TeamTalk multi-participant spoken language command-and-control inter- multi-participant command(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control VERA telephone-based taskable agent that can be instructed to de- vo</context>
<context position="20469" citStr="Bohus and Rudnicky, 2002" startWordPosition="3067" endWordPosition="3071"> language interfaces by: (1) endowing them with the ability to accurately detect errors, (2) de36 veloping a rich repertoire of error recovery strategies and (3) developing scalable, data-driven approaches for building error recovery policies1. Two of the dialog systems from Table 1 (Let’s Go! and RoomLine) have provided a realistic experimental platform for investigating these issues and evaluating the proposed solutions. With respect to error detection, we have developed tools for learning confidence annotation models by integrating information from multiple knowledge sources in the system (Bohus and Rudnicky, 2002b). Additionally, Bohus and Rudnicky (2006) proposed a data-driven approach for constructing more accurate beliefs in spoken language interfaces by integrating information across multiple turns in the conversation. Experiments with the RoomLine system showed that the proposed belief updating models led to significant improvements (equivalent with a 13.5% absolute reduction in WER) in both the effectiveness and the efficiency of the interaction. With respect to error recovery strategies, we have developed and evaluated a large set of strategies for handling misunderstandings and nonunderstandin</context>
</contexts>
<marker>Bohus, Rudnicky, 2002</marker>
<rawString>Bohus, D., and Rudnicky, A., 2002b. Integrating Multiple Knowledge Sources in the CMU Communicator Dialog System, Technical Report CMU-CS-02-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>E Klein</author>
<author>O Lemon</author>
<author>T Oka</author>
</authors>
<title>DIPPER: Description and Formalisation of an Information-State Update Dialogue System Architecture, in</title>
<date>2003</date>
<booktitle>Proc. of SIGdial-2003,</booktitle>
<location>Sapporo, Japan</location>
<contexts>
<context position="27194" citStr="Bos et al., 2003" startWordPosition="4093" endWordPosition="4096">s a singular effort. Several other toolkits for research and development are available to the community. They differ on a number of dimensions, such as objectives, scientific underpinnings, as well as technological and implementation aspects. Several toolkits, both commercial, e.g., TellMe, BeVocal, and academic, e.g., Ariadne (2007), SpeechBuilder (Glass et al., 2004), and the CSLU toolkit (Cole, 1999), are used for rapid development. Some, e.g., CSLU and SpeechBuilder, have also been used for educational purposes. And yet others, such as Olympus, GALATEEA (Kawamoto et al., 2002) and DIPPER (Bos et al., 2003) are primarily used for research. Different toolkits rely on different theories and dialog representations: finite-state, slot-filling, plan-based, information state-update. Each toolkit balances tradeoffs between complexity, ease-of-use, control, robustness, flexibility, etc. We believe the strengths of the Olympus framework lie not only in its current components, but also in its open, transparent, and flexible nature. As we have seen in the previous sections, these characteristics have allowed us to develop and deploy practical, real-world systems operating in a broad spectrum of domains. Th</context>
</contexts>
<marker>Bos, Klein, Lemon, Oka, 2003</marker>
<rawString>Bos, J., Klein, E., Lemon, O., and Oka, T., 2003. DIPPER: Description and Formalisation of an Information-State Update Dialogue System Architecture, in Proc. of SIGdial-2003, Sapporo, Japan</rawString>
</citation>
<citation valid="true">
<authors>
<author>LLC Cepstral</author>
</authors>
<title>SwiftTM: Small Footprint Text-toSpeech Synthesizer,</title>
<date>2005</date>
<location>http://www.cepstral.com.</location>
<contexts>
<context position="10739" citStr="Cepstral 2005" startWordPosition="1582" endWordPosition="1583">anguage generation. The semantic output of the dialog manager is sent to the Rosetta templatebased language generation component, which produces the corresponding surface form. Like the Phoenix grammar, the language generation templates are assembled by concatenating a set of predefined, domain-independent templates, with manually authored task-specific templates. Speech synthesis. The prompts are synthesized by the Kalliope speech synthesis module. Kalliope can be currently configured to use Festival (Black and Lenzo, 2000), which is an open-source speech synthesis system, or Cepstral Swift (Cepstral 2005), a commercial engine. Finally, Kalliope also supports the SSML markup language. Other components. The various components briefly described above form the core of the Olympus dialog system framework. Additional components have been created throughout the development of various systems, and, given the modularity of the architecture, can be easily reused. These include a telephony component, a text 34 input-and-output interface, and a temporal reference resolution agent that translates complex datetime expressions (including relative references, holidays, etc.) into a canonical form. Recently, a</context>
</contexts>
<marker>Cepstral, 2005</marker>
<rawString>Cepstral, LLC, 2005. SwiftTM: Small Footprint Text-toSpeech Synthesizer, http://www.cepstral.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cole</author>
</authors>
<title>Tools for Research and Education in Speech Science,</title>
<date>1999</date>
<booktitle>in Proc. of the International Conference of Phonetic Sciences,</booktitle>
<location>San Francisco, USA.</location>
<contexts>
<context position="26983" citStr="Cole, 1999" startWordPosition="4061" endWordPosition="4062"> 6 Discussion and conclusion The primary goal of the Olympus framework is to enable research that leads to technological and scientific advances in spoken language interfaces. Olympus is however by no means a singular effort. Several other toolkits for research and development are available to the community. They differ on a number of dimensions, such as objectives, scientific underpinnings, as well as technological and implementation aspects. Several toolkits, both commercial, e.g., TellMe, BeVocal, and academic, e.g., Ariadne (2007), SpeechBuilder (Glass et al., 2004), and the CSLU toolkit (Cole, 1999), are used for rapid development. Some, e.g., CSLU and SpeechBuilder, have also been used for educational purposes. And yet others, such as Olympus, GALATEEA (Kawamoto et al., 2002) and DIPPER (Bos et al., 2003) are primarily used for research. Different toolkits rely on different theories and dialog representations: finite-state, slot-filling, plan-based, information state-update. Each toolkit balances tradeoffs between complexity, ease-of-use, control, robustness, flexibility, etc. We believe the strengths of the Olympus framework lie not only in its current components, but also in its open,</context>
</contexts>
<marker>Cole, 1999</marker>
<rawString>Cole, R., 1999. Tools for Research and Education in Speech Science, in Proc. of the International Conference of Phonetic Sciences, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Glass</author>
<author>E Weinstein</author>
<author>S Cyphers</author>
<author>J Polifroni</author>
</authors>
<title>A Framework for Developing Conversational Interfaces,</title>
<date>2004</date>
<booktitle>in Proc. of CADUI,</booktitle>
<location>Funchal, Portugal.</location>
<contexts>
<context position="26948" citStr="Glass et al., 2004" startWordPosition="4053" endWordPosition="4056">r novel challenges and research directions. 6 Discussion and conclusion The primary goal of the Olympus framework is to enable research that leads to technological and scientific advances in spoken language interfaces. Olympus is however by no means a singular effort. Several other toolkits for research and development are available to the community. They differ on a number of dimensions, such as objectives, scientific underpinnings, as well as technological and implementation aspects. Several toolkits, both commercial, e.g., TellMe, BeVocal, and academic, e.g., Ariadne (2007), SpeechBuilder (Glass et al., 2004), and the CSLU toolkit (Cole, 1999), are used for rapid development. Some, e.g., CSLU and SpeechBuilder, have also been used for educational purposes. And yet others, such as Olympus, GALATEEA (Kawamoto et al., 2002) and DIPPER (Bos et al., 2003) are primarily used for research. Different toolkits rely on different theories and dialog representations: finite-state, slot-filling, plan-based, information state-update. Each toolkit balances tradeoffs between complexity, ease-of-use, control, robustness, flexibility, etc. We believe the strengths of the Olympus framework lie not only in its curren</context>
</contexts>
<marker>Glass, Weinstein, Cyphers, Polifroni, 2004</marker>
<rawString>Glass, J., Weinstein, E., Cyphers, S., Polifroni, J., 2004. A Framework for Developing Conversational Interfaces, in Proc. of CADUI, Funchal, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Harris</author>
<author>S Banerjee</author>
<author>A Rudnicky</author>
<author>J Bodine Sison</author>
<author>K</author>
<author>A Black</author>
</authors>
<title>A Research Platform for Multi-Agent Dialogue Dynamics, in</title>
<date>2004</date>
<booktitle>Proc. of The IEEE International Workshop on Robotics and Human Interactive Communications,</booktitle>
<location>Kurashiki, Japan.</location>
<contexts>
<context position="23414" citStr="Harris et al., 2004" startWordPosition="3505" endWordPosition="3508">requirements, or (2) each agent uses its own interface and handles multi-participant behavior. Agents that provide different services have different dialog requirements, and we believe this makes a centralized interface problematic. Furthermore, the second solution better fits human communication behavior and therefore is likely to be more natural and habitable. TeamTalk is a conversational system that follows the second approach: each robot has its own dialog manager. Processed user inputs are sent to all dialog managers in the system; each dialog manager decides based on a simple algorithm (Harris et al., 2004) whether or not the current input is addressed to it. If so, an action is taken. Otherwise the input is ignored; it will be processed and responded to by another robot. On the output side, to address the channel contention problem, each RavenClaw output message is augmented with information about the identity of the robot that generated it. The shared synthesis component queues the messages and plays them back sequentially with the corresponding voice. We are currently looking into two additional challenges related to multi-participant dialog. We are interested in how to address groups and sub</context>
</contexts>
<marker>Harris, Banerjee, Rudnicky, Sison, K, Black, 2004</marker>
<rawString>Harris, T. K., Banerjee, S., Rudnicky, A., Sison, J. Bodine, K., and Black, A. 2004. A Research Platform for Multi-Agent Dialogue Dynamics, in Proc. of The IEEE International Workshop on Robotics and Human Interactive Communications, Kurashiki, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Harris</author>
<author>S Banerjee</author>
<author>A Rudnicky</author>
</authors>
<title>Heterogenous Multi-Robot Dialogues for Search Tasks, in AAAI Spring Symposium: Dialogical Robots,</title>
<date>2005</date>
<location>Palo Alto, California.</location>
<contexts>
<context position="16531" citStr="Harris et al 2005" startWordPosition="2469" endWordPosition="2473">tsburgh area. (system initiative) LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance (Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance Assistant guidance and support to the astronauts on the International and procedure browsing (Aist et al 2002) Space Station during the execution of procedural tasks and checklists. TeamTalk multi-participant spoken language command-and-control inter- multi-participant command(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control VERA telephone-based taskable agent that can be instructed to de- voice mail / message delivliver messages to a third party and make wake-up calls. ery Madeleine text-based dialog system for medical diagnosis. diagnosis ConQuest telephone-based spoken dialog system that provides confer- information access (Bohus et al 2007) ence schedule information. (mixed-initiative) RavenCalendar multimodal dialog system for managing personal calendar information access and (Stenchikova et al 2007). information, such as meetings, classes, a</context>
</contexts>
<marker>Harris, Banerjee, Rudnicky, 2005</marker>
<rawString>Harris, T. K., Banerjee, S., Rudnicky, A. 2005. Heterogenous Multi-Robot Dialogues for Search Tasks, in AAAI Spring Symposium: Dialogical Robots, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>F Alleva</author>
<author>H-W Hon</author>
<author>M-Y Hwang</author>
<author>K-F Lee</author>
<author>R Rosenfeld</author>
</authors>
<title>The SPHINX-II Speech Recognition System: an overview,</title>
<date>1992</date>
<journal>in Computer Speech and Language,</journal>
<volume>7</volume>
<issue>2</issue>
<pages>137--148</pages>
<contexts>
<context position="6930" citStr="Huang et al., 1992" startWordPosition="1020" endWordPosition="1023"> rendered as audio. Galaxy communication infrastructure. While the pipeline shown in bold in Figure 1 captures the logical flow of information in the system, in practice the system components communicate via a centralized message-passing infrastructure – Galaxy (Seneff et al., 1998). Each component is implemented as a separate process that connects to a traffic router – the Galaxy hub. The messages are sent through the hub, which forwards them to the appropriate destination. The routing logic is described via a configuration script. Speech recognition. Olympus uses the Sphinx decoding engine (Huang et al., 1992). A recognition server captures the audio stream, forwards it to a set of parallel recognition engines, and collects the corresponding recognition results. The set of best hypotheses (one from each engine) is then forwarded to the language understanding component. The recognition engines can also generate nbest lists, but that process significantly slows down the systems and has not been used live. Interfaces to connect Sphinx-II and Sphinx-III engines, as well as a DTMF (touch-tone) decoder to the recognition server are currently available. The individual recognition engines can use either n-</context>
</contexts>
<marker>Huang, Alleva, Hon, Hwang, Lee, Rosenfeld, 1992</marker>
<rawString>Huang, X., Alleva, F., Hon, H.-W., Hwang, M.-Y., Lee, K.-F. and Rosenfeld, R., 1992. The SPHINX-II Speech Recognition System: an overview, in Computer Speech and Language, 7(2), pp 137-148, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kawamoto</author>
<author>H Shimodaira</author>
<author>T Nitta</author>
<author>T Nishimoto</author>
<author>S Nakamura</author>
<author>K Itou</author>
<author>S Morishima</author>
<author>T Yotsukura</author>
<author>A Kai</author>
<author>A Lee</author>
<author>Y Yamashita</author>
<author>T Kobayashi</author>
<author>K Tokuda</author>
<author>K Hirose</author>
<author>N Minematsu</author>
<author>A Yamada</author>
<author>Y Den</author>
<author>T Utsuro</author>
<author>S Sagayama</author>
</authors>
<title>Opensource software for developing anthropomorphic spoken dialog agent, in</title>
<date>2002</date>
<booktitle>Proc. of PRICAI-02, International Workshop on Lifelike Animated Agents.</booktitle>
<contexts>
<context position="27164" citStr="Kawamoto et al., 2002" startWordPosition="4087" endWordPosition="4090">aces. Olympus is however by no means a singular effort. Several other toolkits for research and development are available to the community. They differ on a number of dimensions, such as objectives, scientific underpinnings, as well as technological and implementation aspects. Several toolkits, both commercial, e.g., TellMe, BeVocal, and academic, e.g., Ariadne (2007), SpeechBuilder (Glass et al., 2004), and the CSLU toolkit (Cole, 1999), are used for rapid development. Some, e.g., CSLU and SpeechBuilder, have also been used for educational purposes. And yet others, such as Olympus, GALATEEA (Kawamoto et al., 2002) and DIPPER (Bos et al., 2003) are primarily used for research. Different toolkits rely on different theories and dialog representations: finite-state, slot-filling, plan-based, information state-update. Each toolkit balances tradeoffs between complexity, ease-of-use, control, robustness, flexibility, etc. We believe the strengths of the Olympus framework lie not only in its current components, but also in its open, transparent, and flexible nature. As we have seen in the previous sections, these characteristics have allowed us to develop and deploy practical, real-world systems operating in a</context>
</contexts>
<marker>Kawamoto, Shimodaira, Nitta, Nishimoto, Nakamura, Itou, Morishima, Yotsukura, Kai, Lee, Yamashita, Kobayashi, Tokuda, Hirose, Minematsu, Yamada, Den, Utsuro, Sagayama, 2002</marker>
<rawString>Kawamoto, S., Shimodaira, H., Nitta, T., Nishimoto, T., Nakamura, S., Itou, K., Morishima, S., Yotsukura, T., Kai, A., Lee, A., Yamashita, Y., Kobayashi, T., Tokuda, K., Hirose, K., Minematsu, N., Yamada, A., Den, Y., Utsuro, T., and Sagayama, S., 2002. Opensource software for developing anthropomorphic spoken dialog agent, in Proc. of PRICAI-02, International Workshop on Lifelike Animated Agents.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>B Langner</author>
<author>D Bohus</author>
<author>A Black</author>
<author>M Eskenazi</author>
</authors>
<title>Let&apos;s Go Public! Taking a Spoken Dialog System to the Real World, in</title>
<date>2005</date>
<booktitle>Proc. of Interspeech</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="12804" citStr="Raux et al 2005" startWordPosition="1899" endWordPosition="1902">ramming. On the other hand, since all components are open-source, any part of the system can be modified, for example to test new algorithms or compare approaches. 4 Systems To date, the Olympus framework has been used to successfully build and deploy several spoken dialog systems spanning different domains and interaction types (see Table 1). Given the limited space, we discuss only three of these systems in a bit more detail: Let’s Go!, LARRI, and TeamTalk. More information about the other systems can be found in (RavenClaw-Olympus, 2007). 4.1 Let’s Go! The Let’s Go! Bus Information System (Raux et al 2005; 2006) is a telephone-based spoken dialog system that provides access to bus schedules. Interaction with the system starts with an open prompt, followed by a system-directed phase where the user is asked the missing information. Each of the three or four pieces of information provided (origin, destination, time of travel, and optional bus route) is explicitly confirmed. The system knows 12 bus routes, and about 1800 place names. Originally developed as an in-lab research system, Let’s Go! has been open to the general public since March, 2005. Outside of business hours, calls to the bus compan</context>
<context position="15882" citStr="Raux et al 2005" startWordPosition="2386" endWordPosition="2389">The Olympus architecture was easily adapted for this mobile and multi-modal setting. The wearable computer hosts audio input and output clients, as well as the graphical user interface. The Galaxy hub architecture allows us to easily connect these 35 System name Domain / Description Genre RoomLine telephone-based system that provides support for conference information access (mixed (Bohus and Rudnicky 2005) room reservation and scheduling within the School of Com- initiative) puter Science at CMU. Let’s Go! Public telephone-based system that provides access to bus schedule information access (Raux et al 2005) information in the greater Pittsburgh area. (system initiative) LARRI multi-modal system that provides assistance to F/A-18 aircraft multi-modal task guidance (Bohus and Rudnicky 2002) personnel during maintenance tasks. and procedure browsing Intelligent Procedure early prototype for a multi-modal system aimed at providing multi-modal task guidance Assistant guidance and support to the astronauts on the International and procedure browsing (Aist et al 2002) Space Station during the execution of procedural tasks and checklists. TeamTalk multi-participant spoken language command-and-control in</context>
</contexts>
<marker>Raux, Langner, Bohus, Black, Eskenazi, 2005</marker>
<rawString>Raux, A., Langner, B., Bohus, D., Black, A., and Eskenazi, M. 2005, Let&apos;s Go Public! Taking a Spoken Dialog System to the Real World, in Proc. of Interspeech 2005, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>D Bohus</author>
<author>B Langner</author>
<author>A Black</author>
<author>M Eskenazi</author>
</authors>
<title>Doing Research on a Deployed Spoken Dialogue System: One Year of Let&apos;s Go! Experience, in</title>
<date>2006</date>
<booktitle>Proc. of Interspeech</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="24802" citStr="Raux et al., 2006" startWordPosition="3727" endWordPosition="3730">g solutions to both of these issues have been conducted. Analysis of the results and refinements of these methods are ongoing. 5.3 Timing and turn-taking While a lot of research has focused on higher levels of conversation such as natural language understanding and dialog planning, low-level interactional phenomena such as turn-taking have not 37 received as much attention. As a result, current systems either constrain the interaction to a rigid one-speaker-at-a-time style or expose themselves to interactional problems such as inappropriate delays, spurious interruptions, or turn over-taking (Raux et al., 2006). To a large extent, these issues stem from the fact that in common dialog architectures, including Olympus, the dialog manager works asynchronously from the real world (i.e., utterances and actions that are planned are assumed to be executed instantaneously). This means that user barge-ins and backchannels are often interpreted in an incorrect context, which leads to confusion, unexpected user behavior and potential dialog breakdowns. Additionally, dialog systems’ low-level interactional behavior is generally the result of ad-hoc rules encoded in different components that are not precisely co</context>
</contexts>
<marker>Raux, Bohus, Langner, Black, Eskenazi, 2006</marker>
<rawString>Raux, A., Bohus, D., Langner, B., Black, A., and Eskenazi, M. 2006 Doing Research on a Deployed Spoken Dialogue System: One Year of Let&apos;s Go! Experience, in Proc. of Interspeech 2006, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RavenClaw-Olympus web page</author>
</authors>
<date>2007</date>
<location>as of</location>
<note>http://www.ravenclaw-olympus.org/.</note>
<marker>page, 2007</marker>
<rawString>RavenClaw-Olympus web page, as of January 2007: http://www.ravenclaw-olympus.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>E Thayer</author>
<author>P Constantinides</author>
<author>C Tchou</author>
<author>R Shern</author>
<author>K Lenzo</author>
<author>W Xu</author>
<author>A Oh</author>
</authors>
<title>Creating natural dialogs in the Carnegie Mellon Communicator system, in</title>
<date>1999</date>
<booktitle>Proc. of Eurospeech</booktitle>
<contexts>
<context position="2531" citStr="Rudnicky et al., 1999" startWordPosition="362" endWordPosition="365">t investments that are sometimes beyond the reach of academic researchers. As a consequence, research in academia is oftentimes conducted with toy systems and skewed user populations. In turn, this raises questions about the validity of the results and hinders the research impact. In an effort to address this problem and facilitate research on relevant, real-world questions, we have developed Olympus, a freely available framework for building and studying conversational spoken language interfaces. The Olympus architecture, described in Section 3, has its roots in the CMU Communicator project (Rudnicky et al., 1999). Based on that experience and subsequent projects, we have engineered Olympus into an open, transparent, flexible, modular, and scalable architecture. To date, Olympus has been used to develop and deploy a number of spoken language interfaces spanning different domains and interaction types; these systems are presented in Section 4. They are currently supporting research on diverse aspects of spoken language interaction. Section 5 discusses three such efforts: error handling, multi-participant conversation, and turn-taking. We believe that Olympus and other similar toolkits, discussed in Sect</context>
</contexts>
<marker>Rudnicky, Thayer, Constantinides, Tchou, Shern, Lenzo, Xu, Oh, 1999</marker>
<rawString>Rudnicky, A., Thayer, E., Constantinides, P., Tchou, C., Shern, R., Lenzo, K., Xu W., and Oh, A., 1999. Creating natural dialogs in the Carnegie Mellon Communicator system, in Proc. of Eurospeech 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>E Hurley</author>
<author>R Lau</author>
<author>C Pao</author>
<author>P Schmid</author>
<author>V Zue</author>
</authors>
<title>Galaxy-II: A reference architecture for conversational system development, in</title>
<date>1998</date>
<booktitle>Proc. of ICSLP98,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="6594" citStr="Seneff et al., 1998" startWordPosition="965" endWordPosition="968"> score. Next, a dialog manager integrates this semantic input into the current context, and produces the next action to be taken by the system in the form of the semantic output (e.g., {request end_time}). A language generation module produces the corresponding surface form, which is subsequently passed to a speech synthesis module and rendered as audio. Galaxy communication infrastructure. While the pipeline shown in bold in Figure 1 captures the logical flow of information in the system, in practice the system components communicate via a centralized message-passing infrastructure – Galaxy (Seneff et al., 1998). Each component is implemented as a separate process that connects to a traffic router – the Galaxy hub. The messages are sent through the hub, which forwards them to the appropriate destination. The routing logic is described via a configuration script. Speech recognition. Olympus uses the Sphinx decoding engine (Huang et al., 1992). A recognition server captures the audio stream, forwards it to a set of parallel recognition engines, and collects the corresponding recognition results. The set of best hypotheses (one from each engine) is then forwarded to the language understanding component.</context>
</contexts>
<marker>Seneff, Hurley, Lau, Pao, Schmid, Zue, 1998</marker>
<rawString>Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., and Zue V. 1998 Galaxy-II: A reference architecture for conversational system development, in Proc. of ICSLP98, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stenchikova</author>
<author>B Mucha</author>
<author>S Hoffman</author>
<author>A Stent</author>
</authors>
<title>RavenCalendar: A Multimodal Dialog System for Managing A Personal Calendar,</title>
<date>2007</date>
<booktitle>in Proc. of HLT 2007,</booktitle>
<location>Rochester, USA.</location>
<contexts>
<context position="17088" citStr="Stenchikova et al 2007" startWordPosition="2548" endWordPosition="2551">nd-and-control inter- multi-participant command(Harris et al 2005) face for a team of robots in the treasure-hunt domain. and-control VERA telephone-based taskable agent that can be instructed to de- voice mail / message delivliver messages to a third party and make wake-up calls. ery Madeleine text-based dialog system for medical diagnosis. diagnosis ConQuest telephone-based spoken dialog system that provides confer- information access (Bohus et al 2007) ence schedule information. (mixed-initiative) RavenCalendar multimodal dialog system for managing personal calendar information access and (Stenchikova et al 2007). information, such as meetings, classes, appointments and scheduling reminders (uses Google Calendar as a back-end) Table 1. Olympus-based spoken dialog systems (shaded cells indicate deployed systems) components to the rest of the system, which runs on a separate server computer. The rotary-mouse events from the GUI are rendered as semantic inputs and are sent to Helios which in turn forwards the corresponding messages to the dialog manager. 4.3 TeamTalk TeamTalk (Harris et al., 2005) is a multi-modal interface that facilitates communication between a human operator and a team of heterogeneo</context>
</contexts>
<marker>Stenchikova, Mucha, Hoffman, Stent, 2007</marker>
<rawString>Stenchikova, S., Mucha, B., Hoffman, S., Stent, A., 2007. RavenCalendar: A Multimodal Dialog System for Managing A Personal Calendar, in Proc. of HLT 2007, Rochester, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ward</author>
<author>S Issar</author>
</authors>
<title>Recent improvements in the CMU spoken language understanding system,</title>
<date>1994</date>
<booktitle>in Proc. of the ARPA Human Language Technology Workshop,</booktitle>
<pages>213--216</pages>
<location>Plainsboro, NJ.</location>
<marker>Ward, Issar, 1994</marker>
<rawString>Ward, W., and Issar, S., 1994. Recent improvements in the CMU spoken language understanding system, in Proc. of the ARPA Human Language Technology Workshop, pages 213–216, Plainsboro, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>