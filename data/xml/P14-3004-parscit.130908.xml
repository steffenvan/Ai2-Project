<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000449">
<title confidence="0.998578">
Analyzing Positions and Topics in Political Discussions
of the German Bundestag
</title>
<author confidence="0.989265">
C¨acilia Zirn
</author>
<affiliation confidence="0.913885">
Data and Web Science Group
University of Mannheim
Germany
</affiliation>
<email confidence="0.99028">
caecilia@informatik.uni-mannheim.de
</email>
<sectionHeader confidence="0.993653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955933333333">
We present ongoing doctoral work on au-
tomatically understanding the positions of
politicians with respect to those of the
party they belong to. To this end, we use
textual data, namely transcriptions of po-
litical speeches from meetings of the Ger-
man Bundestag, and party manifestos, in
order to automatically acquire the posi-
tions of political actors and parties, respec-
tively. We discuss a variety of possible su-
pervised and unsupervised approaches to
determine the topics of interest and com-
pare positions, and propose to explore an
approach based on topic modeling tech-
niques for these tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97487373015873">
The Bundestag is the legislative institution of Ger-
many. In its plenary sessions, the members discuss
the introduction and formulation of bills. Sub-
jects under discussion include a wide spectrum of
issues, ranging from funding of public transport
through fighting right-wing extremism, or the de-
ployment of German troops in Afghanistan. For
each issue, a few selected members give a speech
stating their opinion towards the topic, while the
audience is allowed to interact: by questions,
heckles, applause or even laughter. Transcrip-
tions of the Bundestag’s sessions provide us with a
gold-mine of political speech data, encoding het-
erogeneous political phenomena such as, for in-
stance, the prominence or engagement of the dif-
ferent politicians with respect to the current polit-
ical situation, or their interest for specific topics.
In our work, we propose to leverage these data
to enable the analysis of the speakers’ positions
with respect to the party they belong to, on the ba-
sis of the content of their speech. Questions we in-
vestigate include: which party’s views do different
politicians support? How much are their political
views aligned with those of their party? Although
we know apriori which party a speaker belongs
to, we view their positions on different topics with
respect to their party’s official lines as degrees of
alignment, and measure them based on the con-
tent of their speeches. There are several circum-
stances under which a speaker might deviate from
his or her party’s opinion. For instance, he might
stem from an election district where membership
of a particular party increases his chances of be-
ing elected. Moreover, it might just happen that a
politician who generally supports his party’s lines
personally has a different view on one particular
topic. If we are able to measure positions from
text, we allow for methods of analyzing adherence
to party lines, which is an important issue in po-
litical science (cf. (Clinton et al., 2004), (Ceron,
2013) and (Ansolabehere et al., 2001)).
At its heart, our work aims at modeling politi-
cians’ positions towards a specific topic, as in-
ferred from their speech. To estimate a position,
in turn, we need a statement of the party’s opinion
towards the topic of interest, which can be then
used for comparison against the speech. Various
work in political science suggests to take this from
party manifestos like (Keman, 2007) and (Slapin
and Proksch, 2008). Research in political sci-
ence has previously focused on analyzing politi-
cal positions within text, for instance (Laver and
Garry, 2000), (Laver et al., 2003), (Keman, 2007)
or (Sim et al., 2013). However, most of previous
work focused on the general position of a party
or a person, like (Slapin and Proksch, 2008), as
opposed to fine-grained positions towards specific
topics. In our research, we address the two follow-
ing tasks:
1. Determine the speeches’ topics – namely de-
velop methods to determine the topic(s) covered
by a political speech, such as those given in the
Bundestag.
</bodyText>
<page confidence="0.963904">
26
</page>
<note confidence="0.852651">
Proceedings of the ACL 2014 Student Research Workshop, pages 26–33,
</note>
<affiliation confidence="0.288418">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
</affiliation>
<bodyText confidence="0.99802105882353">
2. Quantify adherence to party lines – namely es-
timate the speaker’s position relatively to his
party’s opinion towards the respective topic(s).
In the following thesis proposal we present a
variety of approaches that we plan to investigate
in order to address these tasks, as well as discuss
their limitations and challenges.
The first task, determining the topics, could
be in principle addressed using well-studied su-
pervised approaches like state-of-the-art machine
learning algorithms. However, we cannot rely on
the fact that all topics are covered in the train-
ing data. Consequently, we propose to explore an
unsupervised approach that integrates information
from an external resource. We suggest to use a
variant of topic models which allows us to influ-
ence the creation of the topics.
The second task, determining the positions, is
a bigger challenge, given the current state of the
art. Some previous research looked at the related
field of opinion mining, also on political discus-
sion, as in (Abu-Jbara et al., 2012), (Anand et
al., 2011) or (Somasundaran and Wiebe, 2009).
These methods, however, are hardly applicable to
the complex data of plenary meetings. In our sce-
nario, we have to deal with a very specific kind of
text, since the discussions do not consist of spon-
taneous dialogues, but rather formal statements.
Consequently, we are forced to deal with a type of
language which lies in-between dialogue and text.
More concretely, within these speeches speakers
roughly assume what positions the parties have
and also have expectations about their opponents’
opinions. Besides, as opposed to full-fledged di-
alogues, our data shows a very limited amount
of interaction between the speaker and the audi-
ence, solely consisting of a few questions, heck-
les, laughter or applause. Further, as it is the goal
of the discussions to constructively develop laws
and agree on formulations, the speakers do not
just state reasons pro or contra some issue. They
rather illustrate different aspects of the discussed
items. Furthermore, they try to convince others by
emphasizing what their party has achieved in the
past or criticize decisions taken in the past. To ad-
dress these complex problems, we propose to start
by using manually annotated party manifestos in
order to provide us with an upper bound. Next,
we propose to investigate the applicability of topic
models to provide us, again, with a flexible unsu-
pervised approach.
</bodyText>
<sectionHeader confidence="0.979835" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999985470588235">
The German Bundestag meets about 60 times a
year, and discusses various items in each plenary
session. There are various types of items on the
agenda: they can be discussions about bills, but
also question times or government’s statements.
We are interested in the first type only. Each bill
has a unique identifier which is also mentioned by
the session chair. By looking it up in a database
provided by the Bundestag, it is possible to filter
the bill discussions from other forms of items.
For each discussed item, a few selected mem-
bers are permitted to give a speech. Most of the
members belong to a party and their affiliation is
publicly known.
The Bundestag releases the transcripts of its
sessions as plain text documents. OffenesParla-
ment1 is a project run by volunteers that processes
these documents and publishes them in a struc-
tured form on the web as HTML documents. The
data distinguishes between parts of a given speech,
utterances by the chairman and heckles, each an-
notated with its speaker. OffenesParlament makes
the attempt to divide each session’s transcript into
parts containing a single item of the agenda only.
This is not trivial, as it is the chairman who leads
over using a non-standardized formulations, and
thus contains many mistakes.
We collected a number of regular expressions
and hope to improve the segmentation of the items.
We will evaluate the performance of this heuristic
by checking a sample with human judges.
Our extracted dataset covers the time period be-
tween March 2010 and December 2012 and con-
sists of 182 meetings.
</bodyText>
<sectionHeader confidence="0.826147" genericHeader="method">
3 Determining topics in speeches
</sectionHeader>
<bodyText confidence="0.999979583333333">
We aim at comparing the positions stated within
the speeches to the general positions of the par-
ties represented in the Bundestag. The parties’ po-
sitions can be found in their manifestos, and are
commonly used as a source by scholars, as in (Ke-
man, 2007) or (Slapin and Proksch, 2008). In or-
der to being able to compare speakers’ and parties
positions, we need to address two different tasks,
namely: i) identifying the topic of a speech, and
ii) locating that very same topic within the party
manifesto or some further resource. The latter task
depends on how the comparison is done. In this
</bodyText>
<footnote confidence="0.990212">
1http://offenesParlament.de
</footnote>
<page confidence="0.998419">
27
</page>
<bodyText confidence="0.999980714285714">
section, we will focus on the first task: determin-
ing the topic of the speech.
There are two general approaches to classify
the topics of text: either the topics are known in
advance and constitute a static set of categories,
for example (Hillard et al., 2008), or they are un-
known in advance and dynamically created de-
pending on the data, as in (Quinn et al., 2010) (see
also (Grimmer and Stewart, 2013) and (Sebastiani,
2002) for an overview). In our scenario, we as-
sume a common set of topics over several data
sources, namely the party manifestos and tran-
scripts of speeches in our case. Therefore, we opt
for a fixed set of topic categories.
</bodyText>
<subsectionHeader confidence="0.999564">
3.1 Definition of topical categories
</subsectionHeader>
<bodyText confidence="0.999925735294117">
In political science, there are various schemes to
categorize political topics. A well-known and
important project is the Comparative Manifesto
Project (Budge et al., 2001), in which party man-
ifestos are hand-coded on sentence level with a
scheme of 560 categories. A similar project is the
Comparative Agendas Project2, which uses 21 top
level categories further divided into fine-grained
subcategories.
An alternative approach is to use the ministries
as definition of the available categories, which in-
spired the category scheme used in (Seher and
Pappi, 2011). In our work, we develop a category
scheme for our particular task on the basis of the
responsibilities of committees of the Bundestag,
as suggested by internal discussions with scholars
of political science. Similar to the ministries in
government, the responsibilities for political areas
are divided among various committees (see Table
1 for a list of committees). Each item discussed in
the Bundestag is assigned to all committees who
investigate the issues in more detail. For instance,
in our data we find that a discussion about contin-
uing the German participation in the International
Security Assistance Force in Afghanistan has been
assigned to the following committees: Foreign Af-
fairs, Internal Affairs, Legal Affairs, Defense, Hu-
man Rights and Humanitarian Aid, Economic Co-
operation and Development. For each issue, one
of the committees is appointed as the leading one
(German: federf¨uhrende Ausschuss), the Commit-
tee of Foreign Affairs in this case.
Note that, crucially for our work, this assign-
ment process provides us with human-annotated
</bodyText>
<footnote confidence="0.63622">
2http://www.comparativeagendas.info
</footnote>
<figure confidence="0.935423545454546">
Affairs of the European Union
Labour and social Affairs
Food, Agriculture and Consumer Protection
Family Affairs, Senior Citizens, Women and Youth
Health
Cultural and Media Affairs
Committee on Human Rights and Humanitarian Aid
Tourism
Environment, Nature Conservation and Nuclear Safety
Transport, Building and Urban Development
Scrutiny of Elections, Immunity and the Rules of Procedure
Economics and Technology
Economic Cooperation and Development
Foreign Affairs
Finance
Budget
Internal Affairs
Petitions
Legal Affairs
Sports
Defense
Education, Research and Technology Assessment
</figure>
<tableCaption confidence="0.745935">
Table 1: Committees of the 17th German Bun-
destag.
</tableCaption>
<bodyText confidence="0.997376444444444">
topic labels: in fact, not only can we use the com-
mittees as category definitions, but we can also use
these very same assignments as a gold standard.
Consequently, we use the definitions describing
the responsibilities of the committees as our cat-
egory scheme for political topics. We exclude
three committees from the experiments namely: a)
the Committee on Scrutiny of Elections, Immunity
and the Rules of Procedure, b) the Committee on
Petitions, and c) the Committee of Legal Affairs.
This is because these committees are not directly
responsible for a particular political domain, but
perform meta functions.
Descriptions of the particular committees in-
cluding their responsibilities and tasks as well as
concrete examples of their work, accomplished by
lists of current members, can be found in flyers re-
leased by the Bundestag3.
Given this definition of political categories on
the basis of the committees, we can create a gold
standard for our topic classification scenario: to
label a speech, we take the item it is given about,
and use the committees the item has been assigned
to as labels. The committee responsible, in turn,
can be seen as the most important (i.e., primary)
topic label4. Topic assignments are automatically
harvested from a freely available source of infor-
</bodyText>
<footnote confidence="0.990858">
3https://www.btg-bestellservice.de/
index.php?navi=1&amp;subnavi=52
4Henceforth, we refer to the committees as labels for our
topic classification task as “category” or “class”
</footnote>
<page confidence="0.997764">
28
</page>
<bodyText confidence="0.9999744">
mation, namely a public database offered by the
German Bundestag5. Each item discussed in the
Bundestag is associated with a printed document
(Drucksache) tagged with a unique identifier, by
which it can be tracked in the database and where
the list of assigned committees can be queried.
Given these topic assignments, we aim at ac-
quiring a model to classify the speeches with their
assigned categories. To this end, we could focus
on predicting the main label only (i.e. the commit-
tee responsible), or rather perform a multi-class la-
beling task predicting all labels (all committees the
item is assigned to). We now overview a super-
vised and unsupervised approach to address these
classification problems.
</bodyText>
<subsectionHeader confidence="0.999225">
3.2 Supervised approach
</subsectionHeader>
<bodyText confidence="0.999903545454546">
Given that we have labeled data, a first solution
is to opt for a supervised approach to text clas-
sification, which has been successfully used for
many tasks like topic detection ((Diermeier et al.,
2012), (Husby and Barbosa, 2012), or sentiment
analysis (Bakliwal et al., 2013), to name a few.
Consequently, in our case we could represent the
speeches as a word vector and train state-of-the-
art machine learning algorithms like Support Vec-
tor Machines, using the assigned committees as la-
bels.
</bodyText>
<subsectionHeader confidence="0.998795">
3.3 Unsupervised approach
</subsectionHeader>
<bodyText confidence="0.999959315789474">
In order to develop a generally applicable ap-
proach that can easily be applied to other resources
such as speeches given in a context different from
that of the Bundestag, we are interested to explore
an unsupervised approach and compare it to the
supervised one.
External definition of categories. The particu-
lar issues that fall into the responsibility of a com-
mittee are broad and might not be completely cov-
ered when using the speeches themselves as train-
ing data. As mentioned in Section 3.1, we have
a clear definition of the tasks of each committee
provided within the flyers. We will use them as a
basis for the category definitions, and extend them
with political issues discussed in party manifestos.
We will explain this further in Section 3.3.
Known set of categories. Techniques such as
LDA (Blei et al., 2003) create the topics dynam-
ically during the classification process. Recently
</bodyText>
<page confidence="0.333274">
5dipbt.bundestag.de/dip21.web/bt
</page>
<figureCaption confidence="0.999909">
Figure 1: Approach overview
</figureCaption>
<bodyText confidence="0.999719285714286">
they became quite popular in political science, c.f.
(Grimmer, 2010), (Quinn et al., 2010) or (Gerrish
and Blei, 2011). As discussed in Section 3, we
prefer to have a fixed set of categories. This allows
for comparison between applications of the clas-
sification on different sources and domains sep-
arately. But while topic models do not fit this
requirement, they have one property that corre-
sponds quite well to our task: rather than assign-
ing the text one single label, they return a dis-
tribution over topics contained by it. The items
discussed in the speeches touch a range of polit-
ical topics, and are assigned to various commit-
tees. There are variations of topic models that al-
low for influencing the creation of the topics, such
as the systems of (Ramage et al., 2009) (Labeled
LDA), (Andrzejewski and Zhu, 2009) or (Jagar-
lamudi et al., 2012). Labeled LDA is trained on
a corpus of documents. In contrast to standard
topic model approaches, it needs as input the in-
formation which labels (topics) are contained by
the document, though not their proportions, thus
uses a fixed set of categories.
We illustrate our methodology in Figure 1. Our
proposed approach starts by extracting seed words
for the categories from the flyers about the com-
mittees. These seed words are then used to label
training data for labeled LDA. As training data,
we take an external resource: the manifestos6 of
all parties. Finally, we apply the trained model to
the speeches to infer the labels. The output can be
evaluated by comparing the predicted categories
to the committees the issue is actually assigned to.
In the following, we will explain each step in more
detail.
</bodyText>
<footnote confidence="0.7079785">
6We combine the general party programs and the current
election programs of each party
</footnote>
<page confidence="0.998551">
29
</page>
<bodyText confidence="0.984055651162791">
1) Extraction of seed words. We first download
the flyers provided by the Bundestag. Then, we
filter for nouns and calculate their TF-IDF val-
ues for the committee, by which we rank them.
In a final step, we ask a scholar of political sci-
ence to clean them, i.e. to delete nouns that are
not necessarily important for the particular com-
mittee or are too ambiguous, and to cut the tail of
low-ranked nouns. To give an example, we finally
receive the following keywords for the committee
of Labour and Social Affairs: age-related poverty,
labour-market policy, employee, social affairs, so-
cial security, labour, work, pension, basic social
security, regulated rates, partial retirement, social
standard, subcontracted labour.
2) Automatically generating training data.
We take the manifestos of all parties in the Bun-
destag to train our labeled LDA model. While
topic models expect a whole collection of docu-
ments as input, we only provide a handful of them:
accordingly, we generate a pseudo document col-
lection by cutting the documents into snippets, fol-
lowing our previous work in (Zirn and Stucken-
schmidt, 2013), and treating each of them as sin-
gle documents. If a keyword for a committee is
found within a snippet, we add the corresponding
category to the documents labels. We finally run
labeled LDA using standard configurations on the
so labeled data.
3) Applying labeled LDA. Finally, we can ap-
ply the trained model on our transcribed speech
data: we do this by inferring, for each speech,
the distribution of topics, i.e. of categories. To
evaluate the model, we check that the committee
responsible corresponds to the highest probable
topic inferred for the speech, and the other n as-
signed committees to the n most probable topics.
Currently, in our work, we are in the final stages
of creating the gold standard, and evaluating our
method. However, we have already implemented
the proposed system as prototype, and accordingly
show a part of the created topic model in Table 2
to give the reader an impression.
</bodyText>
<sectionHeader confidence="0.982197" genericHeader="method">
4 Detecting positions
</sectionHeader>
<bodyText confidence="0.9969355">
The overall goal of our work is to analyze the
positions expressed by the speakers towards the
debated item. As we aim at performing a fine-
grained analysis, approaches merely classifying
</bodyText>
<table confidence="0.998501727272727">
ENCNS LSA TBUD
consumer (male) labour mobility
consumer (female) employee male research
environment employees female infrastructure
protection salary railway
products pension traffic
farming labour market investments
nature old-age provision development
variety unemployment future
raw materials employment rails
transparency percentage streets
</table>
<tableCaption confidence="0.990478">
Table 2: Top 10 terms for the committees on Envi-
</tableCaption>
<bodyText confidence="0.98438">
ronment, Nature Conservation and Nuclear Safety
(ENCNS), on Labour and social affairs (LSA) and
on Transport, Building and Urban Development
(TBUD).
pro or contra (like those of (Walker et al., 2012)
or (Somasundaran and Wiebe, 2009) are not ap-
plicable in our case. The same applies to the task
of subgroup detection (as done by (Abu-Jbara et
al., 2012), (Anand et al., 2011) or (Thomas et al.,
2006)).
In order to produce a finer-grained model of po-
sitions, we want to develop a model that places
positions stated in text along a one-dimensional
scale, as done by (Slapin and Proksch, 2008)
with their system called Wordfish, (Gabel and Hu-
ber, 2000),(Laver and Garry, 2000), (Laver et al.,
2003) or (Sim et al., 2013). Wordfish places party
manifestos on a left-right-scale, what visualizes
very well which parties are close to each other and
which ones are distant. This is similar in spirit
to the purpose of our work, since we are inter-
ested primarily in estimating closeness and dis-
tances between the speakers’ and the parties’ po-
sitions. However, in contrast to their work, we are
interested in positions towards specific topics, as
opposed to general parties’ positions.
We define our task as follows: we want to an-
alyze the distance between the position towards a
topic expressed in a speech and the position to-
wards the same topic stated in a party manifesto.
In the previous section, we described an approach
to determine the topic of the speech. We now
move on and present how we can retrieve the seg-
ments of the manifestos that correspond to the
topic(s) addressed within the speeches, as well as
how to compare these positions.
</bodyText>
<subsectionHeader confidence="0.981228">
4.1 Approach A: Hand-coding of manifestos
</subsectionHeader>
<bodyText confidence="0.8464685">
Extract positions As part of a larger collabora-
tion project with scholars of political science we
</bodyText>
<page confidence="0.996164">
30
</page>
<bodyText confidence="0.999950321428572">
decided to start with hand-coding a set of man-
ifestos on sentence-level in order to have a gold
standard for further work. To facilitate the manual
work, we use a computer-assisted method based
using the seed words created in Section 3.3. In
more detail, we first use occurrences of the seed
words to assign them the corresponding category
label. Then, a human annotator validates these as-
signments, optionally adding missing labels.
If the sentence-wise labeled data proofs suc-
cessful and necessary for the further analysis of
political positions, we will investigate approaches
to automate this process, for example with super-
vised learning or bootstrapping techniques starting
with our seed words. For each topic, we can then
accumulate the sentences assigned to its corre-
sponding category and use this data as the party’s
opinion towards this topic.
Compare positions The comparison between
the speech and the parties’ opinions can then be
performed as follows: for each party, we extract
the sentences from the manifesto that are tagged
with the topic covered in the speech. We then rep-
resent the extracted sentences and the speeches as
word vectors, and compare them with a distance
metric, e.g., a standard measure like cosine simi-
larity, which gives us the closeness of the speech
to each party’s position.
</bodyText>
<subsectionHeader confidence="0.997253">
4.2 Approach B: Topic Models
</subsectionHeader>
<bodyText confidence="0.999970481481481">
Extract positions Instead of selecting sentences
from the manifesto that cover a topic, the posi-
tion could be extracted from the manifesto using
topic models, as shown in (Thomas et al., 2006)
and (Gerrish and Blei, 2011). To extract the topics
from the manifestos, we run labeled LDA sepa-
rately on each manifesto, following the technique
described in Section 3, yet with an important dif-
ference. In Section 3, we trained one common
topic model on all manifestos, in order to have a
broad coverage over all topics. Here, we are in-
terested in the positions carried by the particular
words chosen by the party to describe a topic. Ac-
cordingly, we train a separate topic model on each
manifesto. The result is a distribution over terms
for each committee, hence for each topic.
Compare positions As a result of the process
to determine the topic of a speech (Section 3),
the speeches also have a representation of the dis-
cussed topics as a distribution over terms. This
way we can directly compare the distributions
for the most probable topics in the speech with
the corresponding topic in the party manifestos.
This can be done using measures to estimate
the distance between probability distributions like,
for instance, Kullback-Leibler distance or Jensen-
Shannon divergence.
</bodyText>
<sectionHeader confidence="0.989101" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999971837837838">
In this paper, we presented an overview of our the-
sis proposal on comparing positions found within
political speeches against those expressed in party
manifestos. To the best of our knowledge, this is
the first work of this kind to aim at providing a
fine-grained analysis of speakers’ positions on po-
litical data. Arguably, the most exiting aspect of
this work is that it grounds a variety of Natural
Language Processing topics – e.g., polarity detec-
tion, topic modeling, among others – within a con-
crete, multi-faceted application scenario.
Being this a proposal, the first step in the fu-
ture will be to complete the implementation of all
above described methods and evaluate them. In
our dataset, we are provided with additional in-
formation apart from the speech text: we know
about heckles, laughter and applause and even
know their origin. This knowledge can be used
to estimate a network of support or opposition.
This knowledge is also used in (Strapparava et
al., 2010) to predict persuasiveness of sentences,
which could constitute another source of informa-
tion for our model. Another idea would be to make
use of the speaker’s given party affiliations and
bootstrap an approach to analyze their positions:
if we assume that a majority of the speakers actu-
ally does follow their parties’ lines, we can train a
classifier for each party for each topic, and apply
it to the same data to detect outliers. Besides, a
big research question would be to see how much
we can complement our topic models with addi-
tional supervision in the form of symbolic knowl-
edge sources like wide-coverage ontologies, e.g.,
DBpedia. Finally, while we do focus in this work
on German data, we are interested in extending our
model to other languages, including resource-rich
ones like English as well as resource-poor ones.
</bodyText>
<sectionHeader confidence="0.996552" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9826625">
We thank Google for travel and conference sup-
port for this paper.
</bodyText>
<page confidence="0.999822">
31
</page>
<sectionHeader confidence="0.989755" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999576444444445">
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and
Dragomir Radev. 2012. Subgroup detection in ide-
ological discussions. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers - Volume 1, ACL ’12,
pages 399–409, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean
E. Fox Tree, Robeson Bowmani, and Michael Mi-
nor. 2011. Cats rule and dogs drool!: Classifying
stance in online debate. In Proceedings of the 2Nd
Workshop on Computational Approaches to Subjec-
tivity and Sentiment Analysis, WASSA ’11, pages 1–
9, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
David Andrzejewski and Xiaojin Zhu. 2009. La-
tent dirichlet allocation with topic-in-set knowledge.
In Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing, pages 43–48. Association for Computa-
tional Linguistics.
Stephen Ansolabehere, James M Snyder, and Charles
Stewart III. 2001. The effects of party and prefer-
ences on congressional roll-call voting. Legislative
Studies Quarterly, 26(4):533–572.
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O’Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
Workshop on Language Analysis in Social Media,
pages 49–58, Atlanta, Georgia, June. Association
for Computational Linguistics.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet allocation. Journal of Machine Learning
Research (JMLR), 3:993–1022.
Ian Budge, Hans”=Dieter Klingemann, Andrea
Volkens, Judith Bara, and Eric Tanenbaum. 2001.
Mapping Policy Preferences. Estimates for Parties,
Electors, and Governments 1945-1998. Oxford
University Press, Oxford u. a.
Andrea Ceron. 2013. Brave rebels stay home: Assess-
ing the effect of intra-party ideological heterogene-
ity and party whip on roll-call votes. Party Politics,
page 1354068812472581.
Joshua Clinton, Simon Jackman, and Douglas Rivers.
2004. The statistical analysis of roll call data. Amer-
ican Political Science Review, 98(02):355–370.
Daniel Diermeier, Jean-Franois Godbout, Bei Yu, and
Stefan Kaufmann. 2012. Language and ideology
in congress. British Journal of Political Science,
42:31–55, 1.
Matthew J. Gabel and John D. Huber. 2000. Putting
parties in their place: Inferring party left-right ideo-
logical positions from party manifestos data. Amer-
ican Journal of Political Science, 44(1):pp. 94–103.
Sean Gerrish and David M Blei. 2011. Predicting leg-
islative roll calls from text. In Proceedings of the
28th International Conference on Machine Learning
(ICML-11), pages 489–496.
Justin Grimmer and Brandon M Stewart. 2013. Text as
data: The promise and pitfalls of automatic content
analysis methods for political texts. Political Analy-
sis.
Justin Grimmer. 2010. A bayesian hierarchical topic
model for political texts: Measuring expressed agen-
das in senate press releases. Political Analysis,
18(1):1–35.
Dustin Hillard, Stephen Purpura, and John Wilkerson.
2008. Computer assisted topic classification for
mixed methods social science research. Journal of
Information Technology and Politics.
Stephanie Husby and Denilson Barbosa. 2012. Topic
classification of blog posts using distant supervision.
In Proceedings of the Workshop on Semantic Analy-
sis in Social Media, pages 28–36, Avignon, France,
April. Association for Computational Linguistics.
Jagadeesh Jagarlamudi, Hal Daum´e, III, and
Raghavendra Udupa. 2012. Incorporating lex-
ical priors into topic models. In Proceedings of
the 13th Conference of the European Chapter of
the Association for Computational Linguistics,
EACL ’12, pages 204–213, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Hans Keman. 2007. Experts and manifestos: Differ-
ent sources - same results for comparative research.
Electoral Studies, 26:76–89.
Michael Laver and John Garry. 2000. Estimating pol-
icy positions from political texts. American Journal
of Political Science, pages 619–634.
Michael Laver, Kenneth Benoit, and John Garry. 2003.
Extracting policy positions from political texts using
words as data. American Political Science Review,
97(02):311–331.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2010.
How to analyze political attention with minimal as-
sumptions and costs. American Journal of Political
Science, 54(1):209–228, January.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D Manning. 2009. Labeled lda: A su-
pervised topic model for credit attribution in multi-
labeled corpora. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 248–256.
Association for Computational Linguistics.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM computing surveys
(CSUR), 34(1):1–47.
</reference>
<page confidence="0.984254">
32
</page>
<reference confidence="0.999481466666666">
Nicole Michaela Seher and Franz Urban Pappi.
2011. Politikfeldspezifische positionen der lan-
desverb¨ande der deutschen parteien. Working Paper
139, Mannheimer Zentrum f¨ur Europ¨aische Sozial-
forschung (MZES).
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross,
and Noah A. Smith. 2013. Measuring ideological
proportions in political speeches. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 91–101, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Jonathan B. Slapin and Sven-Oliver Proksch. 2008. A
Scaling Model for Estimating Time-Series Party Po-
sitions from Texts. American Journal of Political
Science, 52(3):705–722, July.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP: Volume 1 - Volume 1, ACL ’09, pages 226–
234, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Carlo Strapparava, Marco Guerini, and Oliviero Stock.
2010. Predicting persuasiveness in political dis-
courses. In LREC. European Language Resources
Association.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’06,
pages 327–335, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marilyn A Walker, Pranav Anand, Robert Abbott, and
Ricky Grant. 2012. Stance classification using dia-
logic properties of persuasion. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 592–596. Asso-
ciation for Computational Linguistics.
C¨acilia Zirn and Heiner Stuckenschmidt. 2013. Multi-
dimensional topic analysis in political texts. Data &amp;
Knowledge Engineering.
</reference>
<page confidence="0.999377">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573839">
<title confidence="0.872941666666667">Analyzing Positions and Topics in Political of the German Bundestag C¨acilia</title>
<author confidence="0.791064">Data</author>
<author confidence="0.791064">Web Science</author>
<affiliation confidence="0.997103">University of</affiliation>
<email confidence="0.961771">caecilia@informatik.uni-mannheim.de</email>
<abstract confidence="0.9986268125">We present ongoing doctoral work on automatically understanding the positions of politicians with respect to those of the party they belong to. To this end, we use textual data, namely transcriptions of political speeches from meetings of the German Bundestag, and party manifestos, in order to automatically acquire the positions of political actors and parties, respectively. We discuss a variety of possible supervised and unsupervised approaches to determine the topics of interest and compare positions, and propose to explore an approach based on topic modeling techniques for these tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Mona Diab</author>
<author>Pradeep Dasigi</author>
<author>Dragomir Radev</author>
</authors>
<title>Subgroup detection in ideological discussions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>399--409</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5015" citStr="Abu-Jbara et al., 2012" startWordPosition="801" endWordPosition="804"> well-studied supervised approaches like state-of-the-art machine learning algorithms. However, we cannot rely on the fact that all topics are covered in the training data. Consequently, we propose to explore an unsupervised approach that integrates information from an external resource. We suggest to use a variant of topic models which allows us to influence the creation of the topics. The second task, determining the positions, is a bigger challenge, given the current state of the art. Some previous research looked at the related field of opinion mining, also on political discussion, as in (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Somasundaran and Wiebe, 2009). These methods, however, are hardly applicable to the complex data of plenary meetings. In our scenario, we have to deal with a very specific kind of text, since the discussions do not consist of spontaneous dialogues, but rather formal statements. Consequently, we are forced to deal with a type of language which lies in-between dialogue and text. More concretely, within these speeches speakers roughly assume what positions the parties have and also have expectations about their opponents’ opinions. Besides, as opposed to full-fledged di</context>
<context position="20036" citStr="Abu-Jbara et al., 2012" startWordPosition="3231" endWordPosition="3234">tructure protection salary railway products pension traffic farming labour market investments nature old-age provision development variety unemployment future raw materials employment rails transparency percentage streets Table 2: Top 10 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primari</context>
</contexts>
<marker>Abu-Jbara, Diab, Dasigi, Radev, 2012</marker>
<rawString>Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi, and Dragomir Radev. 2012. Subgroup detection in ideological discussions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 399–409, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pranav Anand</author>
<author>Marilyn Walker</author>
<author>Rob Abbott</author>
<author>Jean E Fox Tree</author>
<author>Robeson Bowmani</author>
<author>Michael Minor</author>
</authors>
<title>Cats rule and dogs drool!: Classifying stance in online debate.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2Nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’11,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5037" citStr="Anand et al., 2011" startWordPosition="805" endWordPosition="808">pproaches like state-of-the-art machine learning algorithms. However, we cannot rely on the fact that all topics are covered in the training data. Consequently, we propose to explore an unsupervised approach that integrates information from an external resource. We suggest to use a variant of topic models which allows us to influence the creation of the topics. The second task, determining the positions, is a bigger challenge, given the current state of the art. Some previous research looked at the related field of opinion mining, also on political discussion, as in (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Somasundaran and Wiebe, 2009). These methods, however, are hardly applicable to the complex data of plenary meetings. In our scenario, we have to deal with a very specific kind of text, since the discussions do not consist of spontaneous dialogues, but rather formal statements. Consequently, we are forced to deal with a type of language which lies in-between dialogue and text. More concretely, within these speeches speakers roughly assume what positions the parties have and also have expectations about their opponents’ opinions. Besides, as opposed to full-fledged dialogues, our data show</context>
<context position="20058" citStr="Anand et al., 2011" startWordPosition="3235" endWordPosition="3238"> railway products pension traffic farming labour market investments nature old-age provision development variety unemployment future raw materials employment rails transparency percentage streets Table 2: Top 10 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating close</context>
</contexts>
<marker>Anand, Walker, Abbott, Tree, Bowmani, Minor, 2011</marker>
<rawString>Pranav Anand, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, and Michael Minor. 2011. Cats rule and dogs drool!: Classifying stance in online debate. In Proceedings of the 2Nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’11, pages 1– 9, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Latent dirichlet allocation with topic-in-set knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing,</booktitle>
<pages>43--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16130" citStr="Andrzejewski and Zhu, 2009" startWordPosition="2594" endWordPosition="2597">This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single label, they return a distribution over topics contained by it. The items discussed in the speeches touch a range of political topics, and are assigned to various committees. There are variations of topic models that allow for influencing the creation of the topics, such as the systems of (Ramage et al., 2009) (Labeled LDA), (Andrzejewski and Zhu, 2009) or (Jagarlamudi et al., 2012). Labeled LDA is trained on a corpus of documents. In contrast to standard topic model approaches, it needs as input the information which labels (topics) are contained by the document, though not their proportions, thus uses a fixed set of categories. We illustrate our methodology in Figure 1. Our proposed approach starts by extracting seed words for the categories from the flyers about the committees. These seed words are then used to label training data for labeled LDA. As training data, we take an external resource: the manifestos6 of all parties. Finally, we </context>
</contexts>
<marker>Andrzejewski, Zhu, 2009</marker>
<rawString>David Andrzejewski and Xiaojin Zhu. 2009. Latent dirichlet allocation with topic-in-set knowledge. In Proceedings of the NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pages 43–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Ansolabehere</author>
<author>James M Snyder</author>
<author>Charles Stewart</author>
</authors>
<title>The effects of party and preferences on congressional roll-call voting.</title>
<date>2001</date>
<journal>Legislative Studies Quarterly,</journal>
<volume>26</volume>
<issue>4</issue>
<contexts>
<context position="2824" citStr="Ansolabehere et al., 2001" startWordPosition="448" endWordPosition="451">heir speeches. There are several circumstances under which a speaker might deviate from his or her party’s opinion. For instance, he might stem from an election district where membership of a particular party increases his chances of being elected. Moreover, it might just happen that a politician who generally supports his party’s lines personally has a different view on one particular topic. If we are able to measure positions from text, we allow for methods of analyzing adherence to party lines, which is an important issue in political science (cf. (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et</context>
</contexts>
<marker>Ansolabehere, Snyder, Stewart, 2001</marker>
<rawString>Stephen Ansolabehere, James M Snyder, and Charles Stewart III. 2001. The effects of party and preferences on congressional roll-call voting. Legislative Studies Quarterly, 26(4):533–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Jennifer Foster</author>
<author>Jennifer van der Puil</author>
<author>Ron O’Brien</author>
<author>Lamia Tounsi</author>
<author>Mark Hughes</author>
</authors>
<title>Sentiment analysis of political tweets: Towards an accurate classifier.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Language Analysis in Social Media,</booktitle>
<pages>49--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Bakliwal, Foster, van der Puil, O’Brien, Tounsi, Hughes, 2013</marker>
<rawString>Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil, Ron O’Brien, Lamia Tounsi, and Mark Hughes. 2013. Sentiment analysis of political tweets: Towards an accurate classifier. In Proceedings of the Workshop on Language Analysis in Social Media, pages 49–58, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>3--993</pages>
<contexts>
<context position="15175" citStr="Blei et al., 2003" startWordPosition="2438" endWordPosition="2441">nsupervised approach and compare it to the supervised one. External definition of categories. The particular issues that fall into the responsibility of a committee are broad and might not be completely covered when using the speeches themselves as training data. As mentioned in Section 3.1, we have a clear definition of the tasks of each committee provided within the flyers. We will use them as a basis for the category definitions, and extend them with political issues discussed in party manifestos. We will explain this further in Section 3.3. Known set of categories. Techniques such as LDA (Blei et al., 2003) create the topics dynamically during the classification process. Recently 5dipbt.bundestag.de/dip21.web/bt Figure 1: Approach overview they became quite popular in political science, c.f. (Grimmer, 2010), (Quinn et al., 2010) or (Gerrish and Blei, 2011). As discussed in Section 3, we prefer to have a fixed set of categories. This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research (JMLR), 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Budge</author>
<author>Hans”Dieter Klingemann</author>
<author>Andrea Volkens</author>
<author>Judith Bara</author>
<author>Eric Tanenbaum</author>
</authors>
<title>Mapping Policy Preferences. Estimates for Parties, Electors, and Governments 1945-1998.</title>
<date>2001</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford</location>
<note>u. a.</note>
<contexts>
<context position="9515" citStr="Budge et al., 2001" startWordPosition="1554" endWordPosition="1557"> (Hillard et al., 2008), or they are unknown in advance and dynamically created depending on the data, as in (Quinn et al., 2010) (see also (Grimmer and Stewart, 2013) and (Sebastiani, 2002) for an overview). In our scenario, we assume a common set of topics over several data sources, namely the party manifestos and transcripts of speeches in our case. Therefore, we opt for a fixed set of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in which party manifestos are hand-coded on sentence level with a scheme of 560 categories. A similar project is the Comparative Agendas Project2, which uses 21 top level categories further divided into fine-grained subcategories. An alternative approach is to use the ministries as definition of the available categories, which inspired the category scheme used in (Seher and Pappi, 2011). In our work, we develop a category scheme for our particular task on the basis of the responsibilities of committees of the Bundestag, as suggested by internal discussions with scholars of political science.</context>
</contexts>
<marker>Budge, Klingemann, Volkens, Bara, Tanenbaum, 2001</marker>
<rawString>Ian Budge, Hans”=Dieter Klingemann, Andrea Volkens, Judith Bara, and Eric Tanenbaum. 2001. Mapping Policy Preferences. Estimates for Parties, Electors, and Governments 1945-1998. Oxford University Press, Oxford u. a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Ceron</author>
</authors>
<title>Brave rebels stay home: Assessing the effect of intra-party ideological heterogeneity and party whip on roll-call votes. Party Politics,</title>
<date>2013</date>
<pages>1354068812472581</pages>
<contexts>
<context position="2792" citStr="Ceron, 2013" startWordPosition="445" endWordPosition="446">n the content of their speeches. There are several circumstances under which a speaker might deviate from his or her party’s opinion. For instance, he might stem from an election district where membership of a particular party increases his chances of being elected. Moreover, it might just happen that a politician who generally supports his party’s lines personally has a different view on one particular topic. If we are able to measure positions from text, we allow for methods of analyzing adherence to party lines, which is an important issue in political science (cf. (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al.,</context>
</contexts>
<marker>Ceron, 2013</marker>
<rawString>Andrea Ceron. 2013. Brave rebels stay home: Assessing the effect of intra-party ideological heterogeneity and party whip on roll-call votes. Party Politics, page 1354068812472581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Clinton</author>
<author>Simon Jackman</author>
<author>Douglas Rivers</author>
</authors>
<title>The statistical analysis of roll call data.</title>
<date>2004</date>
<journal>American Political Science Review,</journal>
<volume>98</volume>
<issue>02</issue>
<contexts>
<context position="2777" citStr="Clinton et al., 2004" startWordPosition="441" endWordPosition="444">and measure them based on the content of their speeches. There are several circumstances under which a speaker might deviate from his or her party’s opinion. For instance, he might stem from an election district where membership of a particular party increases his chances of being elected. Moreover, it might just happen that a politician who generally supports his party’s lines personally has a different view on one particular topic. If we are able to measure positions from text, we allow for methods of analyzing adherence to party lines, which is an important issue in political science (cf. (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000),</context>
</contexts>
<marker>Clinton, Jackman, Rivers, 2004</marker>
<rawString>Joshua Clinton, Simon Jackman, and Douglas Rivers. 2004. The statistical analysis of roll call data. American Political Science Review, 98(02):355–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Diermeier</author>
<author>Jean-Franois Godbout</author>
<author>Bei Yu</author>
<author>Stefan Kaufmann</author>
</authors>
<title>Language and ideology in congress.</title>
<date>2012</date>
<journal>British Journal of Political Science,</journal>
<volume>42</volume>
<contexts>
<context position="14035" citStr="Diermeier et al., 2012" startWordPosition="2247" endWordPosition="2250">ments, we aim at acquiring a model to classify the speeches with their assigned categories. To this end, we could focus on predicting the main label only (i.e. the committee responsible), or rather perform a multi-class labeling task predicting all labels (all committees the item is assigned to). We now overview a supervised and unsupervised approach to address these classification problems. 3.2 Supervised approach Given that we have labeled data, a first solution is to opt for a supervised approach to text classification, which has been successfully used for many tasks like topic detection ((Diermeier et al., 2012), (Husby and Barbosa, 2012), or sentiment analysis (Bakliwal et al., 2013), to name a few. Consequently, in our case we could represent the speeches as a word vector and train state-of-theart machine learning algorithms like Support Vector Machines, using the assigned committees as labels. 3.3 Unsupervised approach In order to develop a generally applicable approach that can easily be applied to other resources such as speeches given in a context different from that of the Bundestag, we are interested to explore an unsupervised approach and compare it to the supervised one. External definition</context>
</contexts>
<marker>Diermeier, Godbout, Yu, Kaufmann, 2012</marker>
<rawString>Daniel Diermeier, Jean-Franois Godbout, Bei Yu, and Stefan Kaufmann. 2012. Language and ideology in congress. British Journal of Political Science, 42:31–55, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew J Gabel</author>
<author>John D Huber</author>
</authors>
<title>Putting parties in their place: Inferring party left-right ideological positions from party manifestos data.</title>
<date>2000</date>
<journal>American Journal of Political Science,</journal>
<volume>44</volume>
<issue>1</issue>
<pages>94--103</pages>
<contexts>
<context position="20333" citStr="Gabel and Huber, 2000" startWordPosition="3282" endWordPosition="3286"> and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances between the speakers’ and the parties’ positions. However, in contrast to their work, we are interested in positions towards specific topics, as opposed to general parties’ positions. We define our task as follows: we want to analyze the distance between t</context>
</contexts>
<marker>Gabel, Huber, 2000</marker>
<rawString>Matthew J. Gabel and John D. Huber. 2000. Putting parties in their place: Inferring party left-right ideological positions from party manifestos data. American Journal of Political Science, 44(1):pp. 94–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Gerrish</author>
<author>David M Blei</author>
</authors>
<title>Predicting legislative roll calls from text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>489--496</pages>
<contexts>
<context position="15429" citStr="Gerrish and Blei, 2011" startWordPosition="2472" endWordPosition="2475">training data. As mentioned in Section 3.1, we have a clear definition of the tasks of each committee provided within the flyers. We will use them as a basis for the category definitions, and extend them with political issues discussed in party manifestos. We will explain this further in Section 3.3. Known set of categories. Techniques such as LDA (Blei et al., 2003) create the topics dynamically during the classification process. Recently 5dipbt.bundestag.de/dip21.web/bt Figure 1: Approach overview they became quite popular in political science, c.f. (Grimmer, 2010), (Quinn et al., 2010) or (Gerrish and Blei, 2011). As discussed in Section 3, we prefer to have a fixed set of categories. This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single label, they return a distribution over topics contained by it. The items discussed in the speeches touch a range of political topics, and are assigned to various committees. There are variations of topic models that allow for influencing the creation of</context>
<context position="23032" citStr="Gerrish and Blei, 2011" startWordPosition="3731" endWordPosition="3734">e performed as follows: for each party, we extract the sentences from the manifesto that are tagged with the topic covered in the speech. We then represent the extracted sentences and the speeches as word vectors, and compare them with a distance metric, e.g., a standard measure like cosine similarity, which gives us the closeness of the speech to each party’s position. 4.2 Approach B: Topic Models Extract positions Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al., 2006) and (Gerrish and Blei, 2011). To extract the topics from the manifestos, we run labeled LDA separately on each manifesto, following the technique described in Section 3, yet with an important difference. In Section 3, we trained one common topic model on all manifestos, in order to have a broad coverage over all topics. Here, we are interested in the positions carried by the particular words chosen by the party to describe a topic. Accordingly, we train a separate topic model on each manifesto. The result is a distribution over terms for each committee, hence for each topic. Compare positions As a result of the process t</context>
</contexts>
<marker>Gerrish, Blei, 2011</marker>
<rawString>Sean Gerrish and David M Blei. 2011. Predicting legislative roll calls from text. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 489–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Grimmer</author>
<author>Brandon M Stewart</author>
</authors>
<title>Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis.</title>
<date>2013</date>
<contexts>
<context position="9063" citStr="Grimmer and Stewart, 2013" startWordPosition="1481" endWordPosition="1484">) identifying the topic of a speech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for example (Hillard et al., 2008), or they are unknown in advance and dynamically created depending on the data, as in (Quinn et al., 2010) (see also (Grimmer and Stewart, 2013) and (Sebastiani, 2002) for an overview). In our scenario, we assume a common set of topics over several data sources, namely the party manifestos and transcripts of speeches in our case. Therefore, we opt for a fixed set of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in which party manifestos are hand-coded on sentence level with a scheme of 560 categories. A similar project is the Comparative Agendas Project2,</context>
</contexts>
<marker>Grimmer, Stewart, 2013</marker>
<rawString>Justin Grimmer and Brandon M Stewart. 2013. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Grimmer</author>
</authors>
<title>A bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases. Political Analysis,</title>
<date>2010</date>
<contexts>
<context position="15379" citStr="Grimmer, 2010" startWordPosition="2465" endWordPosition="2466">ed when using the speeches themselves as training data. As mentioned in Section 3.1, we have a clear definition of the tasks of each committee provided within the flyers. We will use them as a basis for the category definitions, and extend them with political issues discussed in party manifestos. We will explain this further in Section 3.3. Known set of categories. Techniques such as LDA (Blei et al., 2003) create the topics dynamically during the classification process. Recently 5dipbt.bundestag.de/dip21.web/bt Figure 1: Approach overview they became quite popular in political science, c.f. (Grimmer, 2010), (Quinn et al., 2010) or (Gerrish and Blei, 2011). As discussed in Section 3, we prefer to have a fixed set of categories. This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single label, they return a distribution over topics contained by it. The items discussed in the speeches touch a range of political topics, and are assigned to various committees. There are variations of topic</context>
</contexts>
<marker>Grimmer, 2010</marker>
<rawString>Justin Grimmer. 2010. A bayesian hierarchical topic model for political texts: Measuring expressed agendas in senate press releases. Political Analysis, 18(1):1–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Stephen Purpura</author>
<author>John Wilkerson</author>
</authors>
<title>Computer assisted topic classification for mixed methods social science research.</title>
<date>2008</date>
<journal>Journal of Information Technology and Politics.</journal>
<contexts>
<context position="8919" citStr="Hillard et al., 2008" startWordPosition="1454" endWordPosition="1457">in and Proksch, 2008). In order to being able to compare speakers’ and parties positions, we need to address two different tasks, namely: i) identifying the topic of a speech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for example (Hillard et al., 2008), or they are unknown in advance and dynamically created depending on the data, as in (Quinn et al., 2010) (see also (Grimmer and Stewart, 2013) and (Sebastiani, 2002) for an overview). In our scenario, we assume a common set of topics over several data sources, namely the party manifestos and transcripts of speeches in our case. Therefore, we opt for a fixed set of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in</context>
</contexts>
<marker>Hillard, Purpura, Wilkerson, 2008</marker>
<rawString>Dustin Hillard, Stephen Purpura, and John Wilkerson. 2008. Computer assisted topic classification for mixed methods social science research. Journal of Information Technology and Politics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Husby</author>
<author>Denilson Barbosa</author>
</authors>
<title>Topic classification of blog posts using distant supervision.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Semantic Analysis in Social Media,</booktitle>
<pages>28--36</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="14062" citStr="Husby and Barbosa, 2012" startWordPosition="2251" endWordPosition="2254"> a model to classify the speeches with their assigned categories. To this end, we could focus on predicting the main label only (i.e. the committee responsible), or rather perform a multi-class labeling task predicting all labels (all committees the item is assigned to). We now overview a supervised and unsupervised approach to address these classification problems. 3.2 Supervised approach Given that we have labeled data, a first solution is to opt for a supervised approach to text classification, which has been successfully used for many tasks like topic detection ((Diermeier et al., 2012), (Husby and Barbosa, 2012), or sentiment analysis (Bakliwal et al., 2013), to name a few. Consequently, in our case we could represent the speeches as a word vector and train state-of-theart machine learning algorithms like Support Vector Machines, using the assigned committees as labels. 3.3 Unsupervised approach In order to develop a generally applicable approach that can easily be applied to other resources such as speeches given in a context different from that of the Bundestag, we are interested to explore an unsupervised approach and compare it to the supervised one. External definition of categories. The particu</context>
</contexts>
<marker>Husby, Barbosa, 2012</marker>
<rawString>Stephanie Husby and Denilson Barbosa. 2012. Topic classification of blog posts using distant supervision. In Proceedings of the Workshop on Semantic Analysis in Social Media, pages 28–36, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Raghavendra Udupa</author>
</authors>
<title>Incorporating lexical priors into topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>204--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Jagarlamudi, Daum´e, Udupa, 2012</marker>
<rawString>Jagadeesh Jagarlamudi, Hal Daum´e, III, and Raghavendra Udupa. 2012. Incorporating lexical priors into topic models. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 204–213, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Keman</author>
</authors>
<title>Experts and manifestos: Different sources - same results for comparative research. Electoral Studies,</title>
<date>2007</date>
<pages>26--76</pages>
<contexts>
<context position="3208" citStr="Keman, 2007" startWordPosition="515" endWordPosition="516"> we are able to measure positions from text, we allow for methods of analyzing adherence to party lines, which is an important issue in political science (cf. (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et al., 2013). However, most of previous work focused on the general position of a party or a person, like (Slapin and Proksch, 2008), as opposed to fine-grained positions towards specific topics. In our research, we address the two following tasks: 1. Determine the speeches’ topics – namely develop methods to determine the topic(s) covered by a political speech, such as those given </context>
<context position="8289" citStr="Keman, 2007" startWordPosition="1348" endWordPosition="1350">s contains many mistakes. We collected a number of regular expressions and hope to improve the segmentation of the items. We will evaluate the performance of this heuristic by checking a sample with human judges. Our extracted dataset covers the time period between March 2010 and December 2012 and consists of 182 meetings. 3 Determining topics in speeches We aim at comparing the positions stated within the speeches to the general positions of the parties represented in the Bundestag. The parties’ positions can be found in their manifestos, and are commonly used as a source by scholars, as in (Keman, 2007) or (Slapin and Proksch, 2008). In order to being able to compare speakers’ and parties positions, we need to address two different tasks, namely: i) identifying the topic of a speech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for </context>
</contexts>
<marker>Keman, 2007</marker>
<rawString>Hans Keman. 2007. Experts and manifestos: Different sources - same results for comparative research. Electoral Studies, 26:76–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>John Garry</author>
</authors>
<title>Estimating policy positions from political texts.</title>
<date>2000</date>
<journal>American Journal of Political Science,</journal>
<pages>619--634</pages>
<contexts>
<context position="3376" citStr="Laver and Garry, 2000" startWordPosition="539" endWordPosition="542"> (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et al., 2013). However, most of previous work focused on the general position of a party or a person, like (Slapin and Proksch, 2008), as opposed to fine-grained positions towards specific topics. In our research, we address the two following tasks: 1. Determine the speeches’ topics – namely develop methods to determine the topic(s) covered by a political speech, such as those given in the Bundestag. 26 Proceedings of the ACL 2014 Student Research Workshop, pages 26–33, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational </context>
<context position="20357" citStr="Laver and Garry, 2000" startWordPosition="3286" endWordPosition="3289">NS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances between the speakers’ and the parties’ positions. However, in contrast to their work, we are interested in positions towards specific topics, as opposed to general parties’ positions. We define our task as follows: we want to analyze the distance between the position towards a to</context>
</contexts>
<marker>Laver, Garry, 2000</marker>
<rawString>Michael Laver and John Garry. 2000. Estimating policy positions from political texts. American Journal of Political Science, pages 619–634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Laver</author>
<author>Kenneth Benoit</author>
<author>John Garry</author>
</authors>
<title>Extracting policy positions from political texts using words as data.</title>
<date>2003</date>
<journal>American Political Science Review,</journal>
<volume>97</volume>
<issue>02</issue>
<contexts>
<context position="3398" citStr="Laver et al., 2003" startWordPosition="543" endWordPosition="546">(Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et al., 2013). However, most of previous work focused on the general position of a party or a person, like (Slapin and Proksch, 2008), as opposed to fine-grained positions towards specific topics. In our research, we address the two following tasks: 1. Determine the speeches’ topics – namely develop methods to determine the topic(s) covered by a political speech, such as those given in the Bundestag. 26 Proceedings of the ACL 2014 Student Research Workshop, pages 26–33, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics 2. Quantif</context>
<context position="20379" citStr="Laver et al., 2003" startWordPosition="3290" endWordPosition="3293"> affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances between the speakers’ and the parties’ positions. However, in contrast to their work, we are interested in positions towards specific topics, as opposed to general parties’ positions. We define our task as follows: we want to analyze the distance between the position towards a topic expressed in a spe</context>
</contexts>
<marker>Laver, Benoit, Garry, 2003</marker>
<rawString>Michael Laver, Kenneth Benoit, and John Garry. 2003. Extracting policy positions from political texts using words as data. American Political Science Review, 97(02):311–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin M Quinn</author>
<author>Burt L Monroe</author>
<author>Michael Colaresi</author>
<author>Michael H Crespin</author>
<author>Dragomir R Radev</author>
</authors>
<title>How to analyze political attention with minimal assumptions and costs.</title>
<date>2010</date>
<journal>American Journal of Political Science,</journal>
<volume>54</volume>
<issue>1</issue>
<contexts>
<context position="9025" citStr="Quinn et al., 2010" startWordPosition="1475" endWordPosition="1478"> two different tasks, namely: i) identifying the topic of a speech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for example (Hillard et al., 2008), or they are unknown in advance and dynamically created depending on the data, as in (Quinn et al., 2010) (see also (Grimmer and Stewart, 2013) and (Sebastiani, 2002) for an overview). In our scenario, we assume a common set of topics over several data sources, namely the party manifestos and transcripts of speeches in our case. Therefore, we opt for a fixed set of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in which party manifestos are hand-coded on sentence level with a scheme of 560 categories. A similar projec</context>
<context position="15401" citStr="Quinn et al., 2010" startWordPosition="2467" endWordPosition="2470"> speeches themselves as training data. As mentioned in Section 3.1, we have a clear definition of the tasks of each committee provided within the flyers. We will use them as a basis for the category definitions, and extend them with political issues discussed in party manifestos. We will explain this further in Section 3.3. Known set of categories. Techniques such as LDA (Blei et al., 2003) create the topics dynamically during the classification process. Recently 5dipbt.bundestag.de/dip21.web/bt Figure 1: Approach overview they became quite popular in political science, c.f. (Grimmer, 2010), (Quinn et al., 2010) or (Gerrish and Blei, 2011). As discussed in Section 3, we prefer to have a fixed set of categories. This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single label, they return a distribution over topics contained by it. The items discussed in the speeches touch a range of political topics, and are assigned to various committees. There are variations of topic models that allow for</context>
</contexts>
<marker>Quinn, Monroe, Colaresi, Crespin, Radev, 2010</marker>
<rawString>Kevin M. Quinn, Burt L. Monroe, Michael Colaresi, Michael H. Crespin, and Dragomir R. Radev. 2010. How to analyze political attention with minimal assumptions and costs. American Journal of Political Science, 54(1):209–228, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled lda: A supervised topic model for credit attribution in multilabeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>248--256</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16086" citStr="Ramage et al., 2009" startWordPosition="2588" endWordPosition="2591">r to have a fixed set of categories. This allows for comparison between applications of the classification on different sources and domains separately. But while topic models do not fit this requirement, they have one property that corresponds quite well to our task: rather than assigning the text one single label, they return a distribution over topics contained by it. The items discussed in the speeches touch a range of political topics, and are assigned to various committees. There are variations of topic models that allow for influencing the creation of the topics, such as the systems of (Ramage et al., 2009) (Labeled LDA), (Andrzejewski and Zhu, 2009) or (Jagarlamudi et al., 2012). Labeled LDA is trained on a corpus of documents. In contrast to standard topic model approaches, it needs as input the information which labels (topics) are contained by the document, though not their proportions, thus uses a fixed set of categories. We illustrate our methodology in Figure 1. Our proposed approach starts by extracting seed words for the categories from the flyers about the committees. These seed words are then used to label training data for labeled LDA. As training data, we take an external resource: </context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Hall, Ramesh Nallapati, and Christopher D Manning. 2009. Labeled lda: A supervised topic model for credit attribution in multilabeled corpora. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 248–256. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys (CSUR),</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="9086" citStr="Sebastiani, 2002" startWordPosition="1486" endWordPosition="1487">ech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for example (Hillard et al., 2008), or they are unknown in advance and dynamically created depending on the data, as in (Quinn et al., 2010) (see also (Grimmer and Stewart, 2013) and (Sebastiani, 2002) for an overview). In our scenario, we assume a common set of topics over several data sources, namely the party manifestos and transcripts of speeches in our case. Therefore, we opt for a fixed set of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in which party manifestos are hand-coded on sentence level with a scheme of 560 categories. A similar project is the Comparative Agendas Project2, which uses 21 top leve</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicole Michaela Seher</author>
<author>Franz Urban Pappi</author>
</authors>
<title>Politikfeldspezifische positionen der landesverb¨ande der deutschen parteien. Working Paper 139, Mannheimer Zentrum f¨ur Europ¨aische Sozialforschung (MZES).</title>
<date>2011</date>
<contexts>
<context position="9906" citStr="Seher and Pappi, 2011" startWordPosition="1615" endWordPosition="1618"> of topic categories. 3.1 Definition of topical categories In political science, there are various schemes to categorize political topics. A well-known and important project is the Comparative Manifesto Project (Budge et al., 2001), in which party manifestos are hand-coded on sentence level with a scheme of 560 categories. A similar project is the Comparative Agendas Project2, which uses 21 top level categories further divided into fine-grained subcategories. An alternative approach is to use the ministries as definition of the available categories, which inspired the category scheme used in (Seher and Pappi, 2011). In our work, we develop a category scheme for our particular task on the basis of the responsibilities of committees of the Bundestag, as suggested by internal discussions with scholars of political science. Similar to the ministries in government, the responsibilities for political areas are divided among various committees (see Table 1 for a list of committees). Each item discussed in the Bundestag is assigned to all committees who investigate the issues in more detail. For instance, in our data we find that a discussion about continuing the German participation in the International Securi</context>
</contexts>
<marker>Seher, Pappi, 2011</marker>
<rawString>Nicole Michaela Seher and Franz Urban Pappi. 2011. Politikfeldspezifische positionen der landesverb¨ande der deutschen parteien. Working Paper 139, Mannheimer Zentrum f¨ur Europ¨aische Sozialforschung (MZES).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanchuan Sim</author>
<author>Brice D L Acree</author>
<author>Justin H Gross</author>
<author>Noah A Smith</author>
</authors>
<title>Measuring ideological proportions in political speeches.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>91--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="3435" citStr="Sim et al., 2013" startWordPosition="550" endWordPosition="553"> 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et al., 2013). However, most of previous work focused on the general position of a party or a person, like (Slapin and Proksch, 2008), as opposed to fine-grained positions towards specific topics. In our research, we address the two following tasks: 1. Determine the speeches’ topics – namely develop methods to determine the topic(s) covered by a political speech, such as those given in the Bundestag. 26 Proceedings of the ACL 2014 Student Research Workshop, pages 26–33, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics 2. Quantify adherence to party lines – namely e</context>
<context position="20401" citStr="Sim et al., 2013" startWordPosition="3295" endWordPosition="3298">ansport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances between the speakers’ and the parties’ positions. However, in contrast to their work, we are interested in positions towards specific topics, as opposed to general parties’ positions. We define our task as follows: we want to analyze the distance between the position towards a topic expressed in a speech and the position t</context>
</contexts>
<marker>Sim, Acree, Gross, Smith, 2013</marker>
<rawString>Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and Noah A. Smith. 2013. Measuring ideological proportions in political speeches. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 91–101, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan B Slapin</author>
<author>Sven-Oliver Proksch</author>
</authors>
<title>A Scaling Model for Estimating Time-Series Party Positions from Texts.</title>
<date>2008</date>
<journal>American Journal of Political Science,</journal>
<volume>52</volume>
<issue>3</issue>
<contexts>
<context position="3239" citStr="Slapin and Proksch, 2008" startWordPosition="518" endWordPosition="521">asure positions from text, we allow for methods of analyzing adherence to party lines, which is an important issue in political science (cf. (Clinton et al., 2004), (Ceron, 2013) and (Ansolabehere et al., 2001)). At its heart, our work aims at modeling politicians’ positions towards a specific topic, as inferred from their speech. To estimate a position, in turn, we need a statement of the party’s opinion towards the topic of interest, which can be then used for comparison against the speech. Various work in political science suggests to take this from party manifestos like (Keman, 2007) and (Slapin and Proksch, 2008). Research in political science has previously focused on analyzing political positions within text, for instance (Laver and Garry, 2000), (Laver et al., 2003), (Keman, 2007) or (Sim et al., 2013). However, most of previous work focused on the general position of a party or a person, like (Slapin and Proksch, 2008), as opposed to fine-grained positions towards specific topics. In our research, we address the two following tasks: 1. Determine the speeches’ topics – namely develop methods to determine the topic(s) covered by a political speech, such as those given in the Bundestag. 26 Proceeding</context>
<context position="8319" citStr="Slapin and Proksch, 2008" startWordPosition="1352" endWordPosition="1355">istakes. We collected a number of regular expressions and hope to improve the segmentation of the items. We will evaluate the performance of this heuristic by checking a sample with human judges. Our extracted dataset covers the time period between March 2010 and December 2012 and consists of 182 meetings. 3 Determining topics in speeches We aim at comparing the positions stated within the speeches to the general positions of the parties represented in the Bundestag. The parties’ positions can be found in their manifestos, and are commonly used as a source by scholars, as in (Keman, 2007) or (Slapin and Proksch, 2008). In order to being able to compare speakers’ and parties positions, we need to address two different tasks, namely: i) identifying the topic of a speech, and ii) locating that very same topic within the party manifesto or some further resource. The latter task depends on how the comparison is done. In this 1http://offenesParlament.de 27 section, we will focus on the first task: determining the topic of the speech. There are two general approaches to classify the topics of text: either the topics are known in advance and constitute a static set of categories, for example (Hillard et al., 2008)</context>
<context position="20274" citStr="Slapin and Proksch, 2008" startWordPosition="3273" endWordPosition="3276">0 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances between the speakers’ and the parties’ positions. However, in contrast to their work, we are interested in positions towards specific topics, as opposed to general parties’ positions. We define our</context>
</contexts>
<marker>Slapin, Proksch, 2008</marker>
<rawString>Jonathan B. Slapin and Sven-Oliver Proksch. 2008. A Scaling Model for Estimating Time-Series Party Positions from Texts. American Journal of Political Science, 52(3):705–722, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in online debates.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09,</booktitle>
<pages>226--234</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5071" citStr="Somasundaran and Wiebe, 2009" startWordPosition="810" endWordPosition="813">the-art machine learning algorithms. However, we cannot rely on the fact that all topics are covered in the training data. Consequently, we propose to explore an unsupervised approach that integrates information from an external resource. We suggest to use a variant of topic models which allows us to influence the creation of the topics. The second task, determining the positions, is a bigger challenge, given the current state of the art. Some previous research looked at the related field of opinion mining, also on political discussion, as in (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Somasundaran and Wiebe, 2009). These methods, however, are hardly applicable to the complex data of plenary meetings. In our scenario, we have to deal with a very specific kind of text, since the discussions do not consist of spontaneous dialogues, but rather formal statements. Consequently, we are forced to deal with a type of language which lies in-between dialogue and text. More concretely, within these speeches speakers roughly assume what positions the parties have and also have expectations about their opponents’ opinions. Besides, as opposed to full-fledged dialogues, our data shows a very limited amount of interac</context>
<context position="19916" citStr="Somasundaran and Wiebe, 2009" startWordPosition="3208" endWordPosition="3211">ng ENCNS LSA TBUD consumer (male) labour mobility consumer (female) employee male research environment employees female infrastructure protection salary railway products pension traffic farming labour market investments nature old-age provision development variety unemployment future raw materials employment rails transparency percentage streets Table 2: Top 10 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each</context>
</contexts>
<marker>Somasundaran, Wiebe, 2009</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2009. Recognizing stances in online debates. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1, ACL ’09, pages 226– 234, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava</author>
<author>Marco Guerini</author>
<author>Oliviero Stock</author>
</authors>
<title>Predicting persuasiveness in political discourses. In</title>
<date>2010</date>
<booktitle>LREC. European Language Resources Association.</booktitle>
<contexts>
<context position="25094" citStr="Strapparava et al., 2010" startWordPosition="4076" endWordPosition="4079">this work is that it grounds a variety of Natural Language Processing topics – e.g., polarity detection, topic modeling, among others – within a concrete, multi-faceted application scenario. Being this a proposal, the first step in the future will be to complete the implementation of all above described methods and evaluate them. In our dataset, we are provided with additional information apart from the speech text: we know about heckles, laughter and applause and even know their origin. This knowledge can be used to estimate a network of support or opposition. This knowledge is also used in (Strapparava et al., 2010) to predict persuasiveness of sentences, which could constitute another source of information for our model. Another idea would be to make use of the speaker’s given party affiliations and bootstrap an approach to analyze their positions: if we assume that a majority of the speakers actually does follow their parties’ lines, we can train a classifier for each party for each topic, and apply it to the same data to detect outliers. Besides, a big research question would be to see how much we can complement our topic models with additional supervision in the form of symbolic knowledge sources lik</context>
</contexts>
<marker>Strapparava, Guerini, Stock, 2010</marker>
<rawString>Carlo Strapparava, Marco Guerini, and Oliviero Stock. 2010. Predicting persuasiveness in political discourses. In LREC. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>327--335</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="20083" citStr="Thomas et al., 2006" startWordPosition="3240" endWordPosition="3243">n traffic farming labour market investments nature old-age provision development variety unemployment future raw materials employment rails transparency percentage streets Table 2: Top 10 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very well which parties are close to each other and which ones are distant. This is similar in spirit to the purpose of our work, since we are interested primarily in estimating closeness and distances betwee</context>
<context position="23003" citStr="Thomas et al., 2006" startWordPosition="3726" endWordPosition="3729">rties’ opinions can then be performed as follows: for each party, we extract the sentences from the manifesto that are tagged with the topic covered in the speech. We then represent the extracted sentences and the speeches as word vectors, and compare them with a distance metric, e.g., a standard measure like cosine similarity, which gives us the closeness of the speech to each party’s position. 4.2 Approach B: Topic Models Extract positions Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al., 2006) and (Gerrish and Blei, 2011). To extract the topics from the manifestos, we run labeled LDA separately on each manifesto, following the technique described in Section 3, yet with an important difference. In Section 3, we trained one common topic model on all manifestos, in order to have a broad coverage over all topics. Here, we are interested in the positions carried by the particular words chosen by the party to describe a topic. Accordingly, we train a separate topic model on each manifesto. The result is a distribution over terms for each committee, hence for each topic. Compare positions</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from congressional floor-debate transcripts. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 327–335, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Pranav Anand</author>
<author>Robert Abbott</author>
<author>Ricky Grant</author>
</authors>
<title>Stance classification using dialogic properties of persuasion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>592--596</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19882" citStr="Walker et al., 2012" startWordPosition="3203" endWordPosition="3206">proaches merely classifying ENCNS LSA TBUD consumer (male) labour mobility consumer (female) employee male research environment employees female infrastructure protection salary railway products pension traffic farming labour market investments nature old-age provision development variety unemployment future raw materials employment rails transparency percentage streets Table 2: Top 10 terms for the committees on Environment, Nature Conservation and Nuclear Safety (ENCNS), on Labour and social affairs (LSA) and on Transport, Building and Urban Development (TBUD). pro or contra (like those of (Walker et al., 2012) or (Somasundaran and Wiebe, 2009) are not applicable in our case. The same applies to the task of subgroup detection (as done by (Abu-Jbara et al., 2012), (Anand et al., 2011) or (Thomas et al., 2006)). In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000),(Laver and Garry, 2000), (Laver et al., 2003) or (Sim et al., 2013). Wordfish places party manifestos on a left-right-scale, what visualizes very we</context>
</contexts>
<marker>Walker, Anand, Abbott, Grant, 2012</marker>
<rawString>Marilyn A Walker, Pranav Anand, Robert Abbott, and Ricky Grant. 2012. Stance classification using dialogic properties of persuasion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 592–596. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C¨acilia Zirn</author>
<author>Heiner Stuckenschmidt</author>
</authors>
<title>Multidimensional topic analysis in political texts.</title>
<date>2013</date>
<journal>Data &amp; Knowledge Engineering.</journal>
<contexts>
<context position="18172" citStr="Zirn and Stuckenschmidt, 2013" startWordPosition="2932" endWordPosition="2936">ee of Labour and Social Affairs: age-related poverty, labour-market policy, employee, social affairs, social security, labour, work, pension, basic social security, regulated rates, partial retirement, social standard, subcontracted labour. 2) Automatically generating training data. We take the manifestos of all parties in the Bundestag to train our labeled LDA model. While topic models expect a whole collection of documents as input, we only provide a handful of them: accordingly, we generate a pseudo document collection by cutting the documents into snippets, following our previous work in (Zirn and Stuckenschmidt, 2013), and treating each of them as single documents. If a keyword for a committee is found within a snippet, we add the corresponding category to the documents labels. We finally run labeled LDA using standard configurations on the so labeled data. 3) Applying labeled LDA. Finally, we can apply the trained model on our transcribed speech data: we do this by inferring, for each speech, the distribution of topics, i.e. of categories. To evaluate the model, we check that the committee responsible corresponds to the highest probable topic inferred for the speech, and the other n assigned committees to</context>
</contexts>
<marker>Zirn, Stuckenschmidt, 2013</marker>
<rawString>C¨acilia Zirn and Heiner Stuckenschmidt. 2013. Multidimensional topic analysis in political texts. Data &amp; Knowledge Engineering.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>