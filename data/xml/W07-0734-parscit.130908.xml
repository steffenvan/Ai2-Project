<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003283">
<title confidence="0.992732">
METEOR: An Automatic Metric for MT Evaluation with High
Levels of Correlation with Human Judgments
</title>
<author confidence="0.98957">
Alon Lavie and Abhaya Agarwal
</author>
<affiliation confidence="0.9839565">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.570386">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.999648">
{alavie,abhayaa}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892">
METEOR is an automatic metric for Ma-
chine Translation evaluation which has been
demonstrated to have high levels of corre-
lation with human judgments of translation
quality, significantly outperforming the more
commonly used BLEU metric. It is one of
several automatic metrics used in this year’s
shared task within the ACL WMT-07 work-
shop. This paper recaps the technical de-
tails underlying the metric and describes re-
cent improvements in the metric. The latest
release includes improved metric parameters
and extends the metric to support evalua-
tion of MT output in Spanish, French and
German, in addition to English.
</bodyText>
<sectionHeader confidence="0.99849" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987445">
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. Automatic metrics are useful for comparing
the performance of different systems on a common
translation task, and can be applied on a frequent
and ongoing basis during MT system development.
The most commonly used MT evaluation metric in
recent years has been IBM’s BLEU metric (Papineni
et al., 2002). BLEU is fast and easy to run, and it
can be used as a target function in parameter op-
timization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, BLEU
does not produce very reliable sentence-level scores.
METEOR , as well as several other proposed metrics
such as GTM (Melamed et al., 2003), TER (Snover
et al., 2006) and CDER (Leusch et al., 2006) aim to
address some of these weaknesses.
METEOR , initially proposed and released in 2004
(Lavie et al., 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
METEOR (Lavie et al., 2004; Banerjee and Lavie,
2005) have described the details underlying the met-
ric and have extensively compared its performance
with BLEU and several other MT evaluation met-
rics. This paper recaps the technical details underly-
ing METEOR and describes recent improvements in
the metric. The latest release extends METEOR to
support evaluation of MT output in Spanish, French
and German, in addition to English. Furthermore,
several parameters within the metric have been opti-
mized on language-specific training data. We present
experimental results that demonstrate the improve-
ments in correlations with human judgments that re-
sult from these parameter tunings.
</bodyText>
<sectionHeader confidence="0.97483" genericHeader="method">
2 The METEOR Metric
</sectionHeader>
<bodyText confidence="0.99984475">
METEOR evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, METEOR cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The “exact” module maps two words if they are ex-
actly the same. The “porter stem” module maps two
words if they are the same after they are stemmed us-
ing the Porter stemmer. The “WN synonymy” mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
“synset&amp;quot; in WordNet.
The word-mapping modules initially identify all
</bodyText>
<page confidence="0.980124">
228
</page>
<bodyText confidence="0.964931916666667">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231,
Prague, June 2007. c�2007 Association for Computational Linguistics
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, METEOR selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of “crossing” unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the “exact” mapping module, followed by
“porter stemming” and then “WN synonymy”.
Once a final alignment has been produced between
the system translation and the reference translation,
the METEOR score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parameterized har-
monic mean of P and R (van Rijsbergen, 1979):
</bodyText>
<equation confidence="0.994961333333333">
P · R
Fmean =
a · P + (1 − a) · R
</equation>
<bodyText confidence="0.999271307692308">
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, METEOR computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of “chunks”
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
</bodyText>
<equation confidence="0.95957">
Pen = y · fragβ
</equation>
<bodyText confidence="0.974313230769231">
The value of y determines the maximum penalty
(0 &lt; y &lt; 1). The value of 0 determines the
functional relation between fragmentation and the
penalty. Finally, the METEOR score for the align-
ment between the two strings is calculated as:
score = (1 − Pen) · Fmean
In all previous versions of METEOR , the values of
the three parameters mentioned above were set to be:
a = 0.9, 0 = 3.0 and y = 0.5, based on experimen-
tation performed in early 2004. In the latest release,
we tuned these parameters to optimize correlation
with human judgments based on more extensive ex-
perimentation, as reported in section 4.
</bodyText>
<sectionHeader confidence="0.8035225" genericHeader="method">
3 METEOR Implementations for
Spanish, French and German
</sectionHeader>
<bodyText confidence="0.999891068965517">
We have recently expanded the implementation of
METEOR to support evaluation of translations in
Spanish, French and German, in addition to English.
Two main language-specific issues required adapta-
tion within the metric: (1) language-specific word-
matching modules; and (2) language-specific param-
eter tuning. The word-matching component within
the English version of METEOR uses stemming and
synonymy modules in constructing a word-to-word
alignment between translation and reference. The re-
sources used for stemming and synonymy detection
for English are the Porter Stemmer (Porter, 2001)
and English WordNet (Miller and Fellbaum, 2007).
In order to construct instances of METEOR for Span-
ish, French and German, we created new language-
specific “stemming” modules. We use the freely
available Perl implementation packages for Porter
stemmers for the three languages (Humphrey, 2007).
Unfortunately, we have so far been unable to obtain
freely available WordNet resources for these three
languages. METEOR versions for Spanish, French
and German therefore currently include only “exact”
and “stemming” matching modules. We are investi-
gating the possibility of developing new synonymy
modules for the various languages based on alterna-
tive methods, which could then be used in place of
WordNet. The second main language-specific issue
which required adaptation is the tuning of the three
parameters within METEOR , described in section 4.
</bodyText>
<sectionHeader confidence="0.993309" genericHeader="method">
4 Optimizing Metric Parameters
</sectionHeader>
<bodyText confidence="0.99998065">
The original version of METEOR (Banerjee and
Lavie, 2005) has instantiated values for three pa-
rameters in the metric: one for controlling the rela-
tive weight of precision and recall in computing the
Fmean score (a); one governing the shape of the
penalty as a function of fragmentation (0) and one
for the relative weight assigned to the fragmenta-
tion penalty (y). In all versions of METEOR to date,
these parameters were instantiated with the values
a = 0.9, 0 = 3.0 and y = 0.5, based on early data ex-
perimentation. We recently conducted a more thor-
ough investigation aimed at tuning these parameters
based on several available data sets, with the goal of
finding parameter settings that maximize correlation
with human judgments. Human judgments come in
the form of “adequacy” and “fluency&amp;quot; quantitative
scores. In our experiments, we looked at optimizing
parameters for each of these human judgment types
separately, as well as optimizing parameters for the
sum of adequacy and fluency. Parameter adapta-
</bodyText>
<page confidence="0.988485">
229
</page>
<table confidence="0.9968461">
Corpus Judgments Systems
NIST 2003 Ara-to-Eng 3978 6
NIST 2004 Ara-to-Eng 347 5
WMT-06 Eng-to-Fre 729 4
WMT-06 Eng-to-Ger 756 5
WMT-06 Eng-to-Spa 1201 7
Adequacy Fluency Sum
α 0.82 0.78 0.81
0 1.0 0.75 0.83
y 0.21 0.38 0.28
</table>
<tableCaption confidence="0.914279">
Table 2: Optimal Values of Tuned Parameters for
Different Criteria for English
Table 1: Corpus Statistics for Various Languages
</tableCaption>
<bodyText confidence="0.9995518">
tion is also an issue in the newly created METEOR
instances for other languages. We suspected that
parameters that were optimized to maximize corre-
lation with human judgments for English would not
necessarily be optimal for other languages.
</bodyText>
<subsectionHeader confidence="0.937014">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999949333333333">
For English, we used the NIST 2003 Arabic-to-
English MT evaluation data for training and the
NIST 2004 Arabic-to-English evaluation data for
testing. For Spanish, German and French we used
the evaluation data provided by the shared task at
last year’s WMT workshop. Sizes of various corpora
are shown in Table 1. Some, but not all, of these data
sets have multiple human judgments per translation
hypothesis. To partially address human bias issues,
we normalize the human judgments, which trans-
forms the raw judgment scores so that they have sim-
ilar distributions. We use the normalization method
described in (Blatz et al., 2003). Multiple judgments
are combined into a single number by taking their
average.
</bodyText>
<subsectionHeader confidence="0.915652">
4.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999996466666667">
We performed a “hill climbing” search to find the
parameters that achieve maximum correlation with
human judgments on the training set. We use Pear-
son’s correlation coefficient as our measure of corre-
lation. We followed a “leave one out” training proce-
dure in order to avoid over-fitting. When n systems
were available for a particular language, we train the
parameters n times, leaving one system out in each
training, and pooling the segments from all other
systems. The final parameter values are calculated
as the mean of the n sets of trained parameters that
were obtained. When evaluating a set of parameters
on test data, we compute segment-level correlation
with human judgments for each of the systems in the
test set and then report the mean over all systems.
</bodyText>
<footnote confidence="0.3234935">
4.3 Results
4.3.1 Optimizing for Adequacy and Fluency
We trained parameters to obtain maximum cor-
relation with normalized adequacy and fluency judg-
</footnote>
<table confidence="0.998358">
Adequacy Fluency Sum
Original 0.6123 0.4355 0.5704
Adequacy 0.6171 0.4354 0.5729
Fluency 0.6191 0.4502 0.5818
Sum 0.6191 0.4425 0.5778
</table>
<tableCaption confidence="0.729095714285714">
Table 3: Pearson Correlation with Human Judg-
ments on Test Data for English
ments separately and also trained for maximal corre-
lation with the sum of the two. The resulting optimal
parameter values on the training corpus are shown in
Table 2. Pearson correlations with human judgments
on the test set are shown in Table 3.
</tableCaption>
<bodyText confidence="0.999819428571429">
The optimal parameter values found are somewhat
different than our previous metric parameters (lower
values for all three parameters). The new parame-
ters result in moderate but noticeable improvements
in correlation with human judgments on both train-
ing and testing data. Tests for statistical significance
using bootstrap sampling indicate that the differ-
ences in correlation levels are all significant at the
95% level. Another interesting observation is that
precision receives slightly more “weight” when op-
timizing correlation with fluency judgments (versus
when optimizing correlation with adequacy). Recall,
however, is still given more weight than precision.
Another interesting observation is that the value of
y is higher for fluency optimization. Since the frag-
mentation penalty reflects word-ordering, which is
closely related to fluency, these results are consistent
with our expectations. When optimizing correlation
with the sum of adequacy and fluency, optimal val-
ues fall in between the values found for adequacy and
fluency.
</bodyText>
<subsubsectionHeader confidence="0.661378">
4.3.2 Parameters for Other Languages
</subsubsectionHeader>
<bodyText confidence="0.997988">
Similar to English, we trained parameters for
Spanish, French and German on the available WMT-
06 training data. We optimized for maximum corre-
lation with human judgments of adequacy, fluency
and for the sum of the two. Resulting parameters
are shown in Table 4.3.2. For all three languages, the
parameters that were found to be optimal were quite
different than those that were found for English, and
using the language-specific optimal parameters re-
</bodyText>
<page confidence="0.988345">
230
</page>
<table confidence="0.9996198">
Adequacy Fluency Sum
French:α 0.86 0.74 0.76
0 0.5 0.5 0.5
y 1.0 1.0 1.0
German:α 0.95 0.95 0.95
0 0.5 0.5 0.5
y 0.6 0.8 0.75
Spanish:α 0.95 0.62 0.95
0 1.0 1.0 1.0
y 0.9 1.0 0.98
</table>
<tableCaption confidence="0.999445">
Table 4: Tuned Parameters for Different Languages
</tableCaption>
<bodyText confidence="0.999224888888889">
sults in significant gains in Pearson correlation levels
with human judgments on the training data (com-
pared with those obtained using the English opti-
mal parameters)&apos;. Note that the training sets used
for these optimizations are comparatively very small,
and that we currently do not have unseen test data
to evaluate the parameters for these three languages.
Further validation will need to be performed once ad-
ditional data becomes available.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999348571428572">
In this paper we described newly developed
language-specific instances of the METEOR metric
and the process of optimizing metric parameters for
different human measures of translation quality and
for different languages. Our evaluations demonstrate
that parameter tuning improves correlation with hu-
man judgments. The stability of the optimized pa-
rameters on different data sets remains to be inves-
tigated for languages other than English. We are
currently exploring broadening the set of features
used in METEOR to include syntax-based features
and alternative notions of synonymy. The latest re-
lease of METEOR is freely available on our website
at: http://www.cs.cmu.edu/~alavie/METEOR/
</bodyText>
<sectionHeader confidence="0.993358" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.966888">
The work reported in this paper was supported by
NSF Grant IIS-0534932.
</bodyText>
<sectionHeader confidence="0.997372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999355655172414">
Satanjeev Banerjee and Alon Lavie. 2005. ME-
TEOR: An Automatic Metric for MT Evalua-
tion with Improved Correlation with Human Judg-
ments. In Proceedings of the ACL Workshop
on Intrinsic and Extrinsic Evaluation Measures
&apos;Detailed tables are not included for lack of space.
for Machine Translation and/or Summarization,
pages 65–72, Ann Arbor, Michigan, June.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence Es-
timation for Machine Translation. Technical Re-
port Natural Language Engineering Workshop Fi-
nal Report, Johns Hopkins University.
Marvin Humphrey. 2007. Perl In-
terface to Snowball Stemmers.
http://search.cpan.org/ creamyg/Lingua-Stem-
Snowball-0.941/lib/Lingua/Stem/Snowball.pm.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The Significance of Recall in Auto-
matic Metrics for MT Evaluation. In Proceedings
of the 6th Conference of the Association for Ma-
chine Translation in the Americas (AMTA-2004),
pages 134–143, Washington, DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using
Block Movements. In Proceedings of the Thir-
teenth Conference of the European Chapter of the
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the HLT-NAACL 2003
Conference: Short Papers, pages 61–63, Edmon-
ton, Alberta.
George Miller and Christiane Fellbaum. 2007. Word-
Net. http://wordnet.princeton.edu/.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Pro-
ceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 311–
318, Philadelphia, PA, July.
Martin Porter. 2001. The Porter Stem-
ming Algorithm. http://www.tartarus.org/ mar-
tin/PorterStemmer/index.html.
Matthew Snover, Bonnie Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Confer-
ence of the Association for Machine Translation in
the Americas (AMTA-2006), pages 223–231, Cam-
bridge, MA, August.
C. van Rijsbergen, 1979. Information Retrieval.
Butterworths, London, UK, 2nd edition.
</reference>
<page confidence="0.997936">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211093">
<title confidence="0.936114">An Automatic Metric for MT Evaluation with Levels of Correlation with Human Judgments</title>
<author confidence="0.416486">Lavie</author>
<affiliation confidence="0.972046">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.99966">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.992089866666667">an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and</abstract>
<note confidence="0.60245">German, in addition to English.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2227" citStr="Banerjee and Lavie, 2005" startWordPosition="347" endWordPosition="350">art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. This paper recaps the technical details underlying METEOR and describes recent improvements in the metric. The latest release extends METEOR to support evaluation of MT output in Spanish, French and German, in addition to English. Furthermore, several parameters within the metric have been optimized on language-specific training data. We present experimental results that demonstrate the improvements in correlations with human judgments that result f</context>
<context position="7930" citStr="Banerjee and Lavie, 2005" startWordPosition="1286" endWordPosition="1289">e have so far been unable to obtain freely available WordNet resources for these three languages. METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules. We are investigating the possibility of developing new synonymy modules for the various languages based on alternative methods, which could then be used in place of WordNet. The second main language-specific issue which required adaptation is the tuning of the three parameters within METEOR , described in section 4. 4 Optimizing Metric Parameters The original version of METEOR (Banerjee and Lavie, 2005) has instantiated values for three parameters in the metric: one for controlling the relative weight of precision and recall in computing the Fmean score (a); one governing the shape of the penalty as a function of fragmentation (0) and one for the relative weight assigned to the fragmentation penalty (y). In all versions of METEOR to date, these parameters were instantiated with the values a = 0.9, 0 = 3.0 and y = 0.5, based on early data experimentation. We recently conducted a more thorough investigation aimed at tuning these parameters based on several available data sets, with the goal of</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures &apos;Detailed tables are not included for lack of space. for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation.</title>
<date>2003</date>
<tech>Technical Report Natural Language Engineering Workshop Final Report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="10115" citStr="Blatz et al., 2003" startWordPosition="1646" endWordPosition="1649">h, we used the NIST 2003 Arabic-toEnglish MT evaluation data for training and the NIST 2004 Arabic-to-English evaluation data for testing. For Spanish, German and French we used the evaluation data provided by the shared task at last year’s WMT workshop. Sizes of various corpora are shown in Table 1. Some, but not all, of these data sets have multiple human judgments per translation hypothesis. To partially address human bias issues, we normalize the human judgments, which transforms the raw judgment scores so that they have similar distributions. We use the normalization method described in (Blatz et al., 2003). Multiple judgments are combined into a single number by taking their average. 4.2 Methodology We performed a “hill climbing” search to find the parameters that achieve maximum correlation with human judgments on the training set. We use Pearson’s correlation coefficient as our measure of correlation. We followed a “leave one out” training procedure in order to avoid over-fitting. When n systems were available for a particular language, we train the parameters n times, leaving one system out in each training, and pooling the segments from all other systems. The final parameter values are calc</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence Estimation for Machine Translation. Technical Report Natural Language Engineering Workshop Final Report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Humphrey</author>
</authors>
<title>Perl Interface to Snowball Stemmers.</title>
<date>2007</date>
<note>http://search.cpan.org/ creamyg/Lingua-StemSnowball-0.941/lib/Lingua/Stem/Snowball.pm.</note>
<contexts>
<context position="7287" citStr="Humphrey, 2007" startWordPosition="1191" endWordPosition="1192">d (2) language-specific parameter tuning. The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference. The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007). In order to construct instances of METEOR for Spanish, French and German, we created new languagespecific “stemming” modules. We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007). Unfortunately, we have so far been unable to obtain freely available WordNet resources for these three languages. METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules. We are investigating the possibility of developing new synonymy modules for the various languages based on alternative methods, which could then be used in place of WordNet. The second main language-specific issue which required adaptation is the tuning of the three parameters within METEOR , described in section 4. 4 Optimizing Metric Parameters The original v</context>
</contexts>
<marker>Humphrey, 2007</marker>
<rawString>Marvin Humphrey. 2007. Perl Interface to Snowball Stemmers. http://search.cpan.org/ creamyg/Lingua-StemSnowball-0.941/lib/Lingua/Stem/Snowball.pm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Kenji Sagae</author>
<author>Shyamsundar Jayaraman</author>
</authors>
<title>The Significance of Recall in Automatic Metrics for MT Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004),</booktitle>
<pages>134--143</pages>
<location>Washington, DC,</location>
<contexts>
<context position="2044" citStr="Lavie et al., 2004" startWordPosition="317" endWordPosition="320">pineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. This paper recaps the technical details underlying METEOR and describes recent improvements in the metric. The latest release extends METEOR to support evaluation of MT output in Spanish, French and German, in addition to English. Furthermore, several parameters within </context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. 2004. The Significance of Recall in Automatic Metrics for MT Evaluation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004), pages 134–143, Washington, DC, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements.</title>
<date>2006</date>
<booktitle>In Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1933" citStr="Leusch et al., 2006" startWordPosition="298" endWordPosition="301">T system development. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. This paper recaps the technical details underlying METEOR and describes recent improvements in the metric. The latest release extends METEOR to support evaluat</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Conference: Short Papers,</booktitle>
<pages>61--63</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1875" citStr="Melamed et al., 2003" startWordPosition="287" endWordPosition="290">and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. This paper recaps the technical details underlying METEOR and describes recent improvements in the me</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the HLT-NAACL 2003 Conference: Short Papers, pages 61–63, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<date>2007</date>
<note>WordNet. http://wordnet.princeton.edu/.</note>
<contexts>
<context position="7042" citStr="Miller and Fellbaum, 2007" startWordPosition="1152" endWordPosition="1155">ently expanded the implementation of METEOR to support evaluation of translations in Spanish, French and German, in addition to English. Two main language-specific issues required adaptation within the metric: (1) language-specific wordmatching modules; and (2) language-specific parameter tuning. The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference. The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007). In order to construct instances of METEOR for Spanish, French and German, we created new languagespecific “stemming” modules. We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007). Unfortunately, we have so far been unable to obtain freely available WordNet resources for these three languages. METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules. We are investigating the possibility of developing new synonymy modules for the various languages based on alternative</context>
</contexts>
<marker>Miller, Fellbaum, 2007</marker>
<rawString>George Miller and Christiane Fellbaum. 2007. WordNet. http://wordnet.princeton.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1640" citStr="Och, 2003" startWordPosition="252" endWordPosition="253">ics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have describ</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1445" citStr="Papineni et al., 2002" startWordPosition="217" endWordPosition="220">ish, French and German, in addition to English. 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311– 318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>The Porter Stemming Algorithm.</title>
<date>2001</date>
<note>http://www.tartarus.org/ martin/PorterStemmer/index.html.</note>
<contexts>
<context position="6994" citStr="Porter, 2001" startWordPosition="1147" endWordPosition="1148">nish, French and German We have recently expanded the implementation of METEOR to support evaluation of translations in Spanish, French and German, in addition to English. Two main language-specific issues required adaptation within the metric: (1) language-specific wordmatching modules; and (2) language-specific parameter tuning. The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference. The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007). In order to construct instances of METEOR for Spanish, French and German, we created new languagespecific “stemming” modules. We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007). Unfortunately, we have so far been unable to obtain freely available WordNet resources for these three languages. METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules. We are investigating the possibility of developing new synonymy module</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin Porter. 2001. The Porter Stemming Algorithm. http://www.tartarus.org/ martin/PorterStemmer/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1902" citStr="Snover et al., 2006" startWordPosition="292" endWordPosition="295">uent and ongoing basis during MT system development. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. This paper recaps the technical details underlying METEOR and describes recent improvements in the metric. The latest release ex</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006), pages 223–231, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London, UK,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. van Rijsbergen, 1979. Information Retrieval. Butterworths, London, UK, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>