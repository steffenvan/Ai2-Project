<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.983793">
An Efficient A* Search Algorithm for Statistical Machine Translation
</title>
<author confidence="0.996712">
Franz Josef Och, Nicola Ueffing, Hermann Ney
</author>
<affiliation confidence="0.969991">
Lehrstuhl f¨ur Informatik VI, Computer Science Department
RWTH Aachen - University of Technology
</affiliation>
<address confidence="0.652638">
D-52056 Aachen, Germany
</address>
<email confidence="0.997464">
{och,ueffing,ney}@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999588">
In this paper, we describe an efficient
A* search algorithm for statistical ma-
chine translation. In contrary to beam-
search or greedy approaches it is possi-
ble to guarantee the avoidance of search
errors with A*. We develop various so-
phisticated admissible and almost ad-
missible heuristic functions. Especially
our newly developped method to per-
form a multi-pass A* search with an
iteratively improved heuristic function
allows us to translate even long sen-
tences. We compare the A* search al-
gorithm with a beam-search approach
on the Hansards task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997886464285714">
The goal of machine translation is the transla-
tion of a text given in some source language into
a target language. We are given a source string
fJ1 = f1...fj...fJ, which is to be translated into a
target string eI1 = e1...ei...eI. Among all possible
target strings, we will choose the string with the
highest probability:
The argmax operation denotes the search prob-
lem, i.e. the generation of the output sentence
in the target language. Pr(eI1) is the language
model of the target language, whereas Pr(fJ1 |eI1)
denotes the translation model.
Many statistical translation models (Brown et
al., 1993; Vogel et al., 1996; Och and Ney, 2000b)
try to model word-to-word correspondences be-
tween source and target words. These correspon-
dences are called an alignment. The model is
often further restricted in a way such that each
source word is assigned exactly one target word.
The alignment mapping is j —* i = aj from
source position j to target position i = aj. The
alignment aJ1 may contain alignments aj = 0
with the ‘empty’ word e0 to account for source
words that are not aligned to any target word. In
(statistical) alignment models Pr(fJ1 , aJ1 |eI1), the
alignment aJ1 is introduced as a hidden variable.
Typically, the search is performed using the so-
called maximum approximation:
</bodyText>
<equation confidence="0.997644857142857">
ˆeI1 = arg max
eI
1
�
= arg max Pr(eI1) • maxPr(fJ1 , aJ1 |eI1)
eI aJ
1 1
</equation>
<bodyText confidence="0.978517">
The search space consists of the set of all possible
target language strings eI1 and all possible align-
ments aJ1 .
</bodyText>
<sectionHeader confidence="0.980883" genericHeader="method">
2 IBM Model 4
</sectionHeader>
<bodyText confidence="0.995596857142857">
Various statistical alignment models of the form
Pr(fJ1 , aJ1 |eI1) have been introduced in (Brown
et al., 1993; Vogel et al., 1996; Och and Ney,
2000a). In this paper we use the so-called Model
4 from (Brown et al., 1993).
In Model 4 the statistical alignment model is
decomposed into five sub-models:
</bodyText>
<listItem confidence="0.9892884">
• the lexicon model p(f|e) for the probability
that the source word f is a translation of the
target word e,
• the distortion model p=1(j−j&apos;|C(fj), E) for
the probability that the translations of two
</listItem>
<figure confidence="0.525180166666667">
{ }
ˆeI 1 = arg max Pr(eJ 1 |fI 1 )
eI
1
= arg max
eI
1
{ }
Pr(eI 1) � Pr(fJ 1 |eI 1)
I �Pr(eI1) � Pr(fJ1 , aJ1 |eI1) I
aJ
1
</figure>
<bodyText confidence="0.92760125">
consecutive target words have the position
difference j − j&apos; where C(fj) is the word
class of fj and E is the word class of the
first of the two consecutive target words,
</bodyText>
<listItem confidence="0.9869687">
• the distortion model p&gt;1(j − j&apos;|C(fj)) for
the probability that the words aligned to one
target words have the position difference j −
j&apos;,
• the fertility model p(φ|e) for the probability
that a target language word e is aligned to φ
source language words,
• the empty word fertility model p(φ0|e0) for
the probability that exactly φ0 words remain
unaligned to.
</listItem>
<bodyText confidence="0.999912764705883">
The final probability p(fJ1 , aJ1 |eI1) for Model 4 is
obtained by multiplying the probabilities of the
sub-models for all words. For a detailed descrip-
tion for Model 4 the reader is referred to (Brown
et al., 1993).
We use Model 4 in this paper for two reasons.
First, it has been shown that Model 4 produces
a very good alignment quality in comparison to
various other alignment models (Och and Ney,
2000b). Second, the dependences in the distortion
model along the target language words make it
quite easy to integrate standard n-gram language
models in the search process. This would be more
difficult in the HMM alignment model (Vogel et
al., 1996). Yet, many of the results presented in
the following are also applicable to other align-
ment models.
</bodyText>
<sectionHeader confidence="0.848083" genericHeader="method">
3 Search problem
</sectionHeader>
<bodyText confidence="0.9724365">
The following tasks have to be performed both us-
ing A* and beam search (BS):
</bodyText>
<listItem confidence="0.94860525">
• The search space has to be structured into
a search graph. This search graph typically
includes an initial node, intermediary nodes
(partial hypotheses), and goal nodes (com-
pleted hypotheses). A node contains the fol-
lowing information:
– the predecessor words u, v in the target
language,
– the score of the hypothesis,
– a backpointer to the preceding partial
hypothesis,
– the model specific information de-
scribed at the end of this subsection.
• A scoring function Q(n) + h(n) has to be
defined which assigns a score to every node
n. For beam search, this is the score Q(n) of
</listItem>
<bodyText confidence="0.983319333333334">
a best path to this node. In the A* algorithm,
an estimation h(n) of the score of a best path
from node n to a goal node is added.
(Berger et al., 1996) presented a method to struc-
ture the search space. Our search algorithm for
Model 4 uses a similar structuring of the search
space. We will shortly review the basic concepts
of this search space structure: Every partial hy-
pothesis consists of a prefix of the target sentence
and a corresponding alignment. A partial hypoth-
esis is extended by accounting for exactly one ad-
ditional word of the source sentence. Every exten-
sion yields an extension score which is computed
by taking into account the lexicon, distortion, and
fertility probabilities involved with this extension.
A partial hypothesis is called open if more source
words are to be aligned to the current target word
in the following extensions. A hypothesis that is
not open is said to be closed. Every extension of
an open hypothesis will extend the fertility of the
previously produced target word and an extension
of a closed hypothesis will produce a new word.
Therefore, the language model score is added as
well if a closed hypothesis is extended.
It is prohibitive to consider all possible transla-
tions of all words. Instead, we restrict the search
to the most promising candidates by calculating
“inverse translations” (Al-Onaizan et al., 1999).
The inverse translation probability p(e  |f) of a
source word f is calculated as
</bodyText>
<equation confidence="0.995317">
p(e  |f) = p (f  |e) p (e)
E p (f  |e&apos;) p (e&apos;) ,
e/
</equation>
<bodyText confidence="0.999907615384615">
where we use a unigram model p (e) to esti-
mate the prior probability of a target word be-
ing used. Like (Al-Onaizan et al., 1999), we use
only the top 12 translations of a given source lan-
guage word. In addition, we remove from this list
all words whose inverse translation probability is
lower than 0.01 times the best inverse translation
probability. This observation pruning is the only
pruning involved in our A* search algorithm. Ex-
periments showed this does not impair translation
quality, but the search becomes much more effi-
cient.
In order to keep the search space as small as
possible it is crucial to perform a recombina-
tion of search hypotheses. Every two hypothe-
ses which can be distinguished by neither the lan-
guage model state nor the translation model state
can be recombined, only the hypothesis with a
better score of the two needs to be considered in
the subsequent search process. We use a standard
trigram language model, so the relevant language
model state of node n consists of the current word
w(n) and the previous word v(n) (later on we will
describe an improvement to this). The translation
model state depends on the specific model depen-
dencies of Model 4:
</bodyText>
<listItem confidence="0.9925124">
• a coverage set C(n) containing the already
translated source language positions,
• the position j(n) of the previously translated
source word,
• a flag indicating whether the hypothesis is
open or closed,
• the number of source language words which
are aligned to the empty word,
• a flag showing whether the hypothesis is a
complete hypothesis or not.
</listItem>
<subsectionHeader confidence="0.48123">
Efficient language model recombination
</subsectionHeader>
<bodyText confidence="0.999941666666667">
The recombination procedure which is described
above can be improved by taking into account the
backing-off structure of the language model. The
trigram language model we use has the property
that if the count of the bigram N(u, v) = 0, then
the probability P(w|u, v) depends only on v. In
this case the recombination can be significantly
improved by recombining all nodes whose lan-
guage model state has the property N(u, v) = 0
only with respect to v. Obviously, this could be
generalized to other types of language models as
well.
Experiments have shown that by using this ef-
ficient recombination, the number of needed hy-
potheses can be reduced by about a factor of 4.
</bodyText>
<subsectionHeader confidence="0.797535">
Search algorithms
</subsectionHeader>
<bodyText confidence="0.999393">
We evaluate the following two search algorithms:
</bodyText>
<listItem confidence="0.989856">
• beam search algorithm (BS): (Tillmann,
2001; Tillmann and Ney, 2000)
</listItem>
<bodyText confidence="0.999672">
In this algorithm the search space is explored
in a breadth-first manner. The search algo-
rithm is based on a dynamic programming
approach and applies various pruning tech-
niques in order to restrict the number of con-
sidered hypotheses. For more details see
(Tillmann, 2001).
</bodyText>
<listItem confidence="0.964028076923077">
• A* search algorithm:
In A*, all search hypotheses are managed in
a priority queue. The basic A* search (Nils-
son, 1971) can be described as follows:
1. initialize priority queue with an empty
hypothesis
2. remove the hypothesis with the highest
score from the priority queue
3. if this hypothesis is a goal hypothesis:
output this hypothesis and terminate
4. produce all extensions of this hypothe-
sis and put the extensions to the queue
5. goto 2
</listItem>
<bodyText confidence="0.999953625">
The so-called heuristic function estimates
the probability of a completion of a partial
hypothesis. This function is called admissi-
ble if it never underestimates this probabil-
ity. Thus, admissible heuristic functions are
always optimistic. The A* search algorithm
corresponds to the Dijkstra algorithm if the
heuristic function is equal to zero.
</bodyText>
<sectionHeader confidence="0.935631" genericHeader="method">
4 Admissible heuristic function
</sectionHeader>
<bodyText confidence="0.9998555">
In order to perform an efficient search with the
A* search algorithm it is crucial to use a good
heuristic function. We only know of the work by
(Wang and Waibel, 1997) dealing with heuristic
functions for search in statistical machine trans-
lation. They developed a simple heuristic func-
tion for Model 2 from (Brown et al., 1993) which
was non admissible. In the following we de-
velop a guaranteed admissible heuristic function
for Model 4 taking into account distortion proba-
bilities and the coupling of lexicon, fertility, and
language model probabilities.
The basic idea for developing a heuristic func-
tion for the alignment models is the fact that all
source sentence positions which have not been
covered so far still have to be translated in order
to complete the sentence. Therefore, the value of
the heuristic function HX(n) for a node n can
be deduced if we have an estimation hX(j) of the
optimal score of translating position j (here X de-
notes different possibilities to choose the heuristic
function):
</bodyText>
<equation confidence="0.994244">
11 HX(n) = hX(j) ,
j6∈C(n)
</equation>
<bodyText confidence="0.988021">
where C(n) is the coverage set.
The simplest realization of a heuristic func-
tion, denoted as hT (j), takes into account only
the translation probability p(f|e):
</bodyText>
<equation confidence="0.9960555">
hT (j) = max
e p(fj|e)
</equation>
<bodyText confidence="0.999933">
This heuristic function can be refined by intro-
ducing also the fertility probabilities (symbol F)
of a target word e:
</bodyText>
<equation confidence="0.99557175">
hTF(j) = l
= maxmax p(fj |e) 6(φ|e), p(f |e0)
}
t6=e0,φ JJ
</equation>
<bodyText confidence="0.999959454545454">
Thereby, a coupling between the translation and
fertility probabilities is achieved. We have to
take the φ-th root in order to avoid that the fer-
tility probability of a target word whose fertility
is higher than one is taken into account for every
source word aligned to it. For words which are
translated by the empty word e0, no fertility prob-
ability is used.
The language model can be incorporated by
considering that for every target word there exists
an optimal language model probability:
</bodyText>
<equation confidence="0.992492">
pL(e) = max
u,v p(e|u, v)
</equation>
<bodyText confidence="0.99083225">
Here, we assume a trigram language model.
Thus, a heuristic function including a coupling
between translation, fertility, and language model
probabilities (TFL) is given by:
</bodyText>
<equation confidence="0.973647">
hTFL(j) =(
= max { maxp(fj |e) 6p(φ|e)p L(e), p(f |e0)
l e,φ J
</equation>
<bodyText confidence="0.9784196">
This value can be precomputed efficiently before
the search process itself starts.
The heuristic function for the distortion proba-
bilities depends on the used model. For Model 4,
we obtain:
</bodyText>
<equation confidence="0.972815">
hD(j) = maExp(j − j0|E, C(fj))
</equation>
<bodyText confidence="0.990124727272727">
jl,Here, E refers to the class of the previously
aligned target word.
The heuristic functions hD(j) involve maxi-
mizations over the source positions j0. The do-
main of this variable shrinks during search as
more and more words get translated. Therefore, it
is possible to improve this heuristic function dur-
ing search to perform a maximization only over
the free source language positions j0. For Model 4
we compute the following heuristic function with
two arguments:
</bodyText>
<equation confidence="0.9991925">
hD(j0,j) = maxp(j − j0|E,C(fj))
E
</equation>
<bodyText confidence="0.8805765">
Thus, we obtain as an estimation of the distortion
probability
</bodyText>
<equation confidence="0.999315">
hD(j) = max hD(j0,j) .
j�6∈C(n)
</equation>
<bodyText confidence="0.999878333333333">
This yields the following heuristic functions tak-
ing into account translation, fertility, language,
and distortion model probabilities:
</bodyText>
<equation confidence="0.996352">
11 HTFLD(n) = hTFL(j) - hD(j) (1)
j6∈C(n)
</equation>
<bodyText confidence="0.998142263157895">
Using these heuristic functions we have the over-
head of performing this rest cost estimation for
every coverage set in search. The experiments
will show that these additional costs are over-
compensated by the gain in reducing the search
space that has to be expanded during the A*
search.
To assess the predictive power of the vari-
ous components in the heuristic, we compare the
value of the heuristic function of the empty hy-
pothesis with the score of the optimal transla-
tion. A heuristic function is better if the dif-
ference between these two values is small. Ta-
ble 1 contains a comparison of various heuristic
functions. We compare the average costs (nega-
tive logarithm of the probabilities) of the optimal
translation and the average of the estimated costs
of the empty hypothesis. Typically, the estimated
costs of TFLD and the real costs differ by factor
</bodyText>
<page confidence="0.519354">
3.
</page>
<tableCaption confidence="0.999687">
Table 1: Predictive power of admissible and almost admissible heuristic functions.
</tableCaption>
<table confidence="0.995513857142857">
sentence HF for initial node empirical goal node
length score score
T TF TFL TFLD
6 5.1 7.2 12.7 13.0 25.9 35.5
8 5.7 8.2 16.0 16.3 29.8 43.7
10 8.1 11.6 19.4 19.7 36.5 55.8
12 9.5 13.7 20.7 21.1 43.9 63.4
</table>
<bodyText confidence="0.998912333333334">
We will see later in Section 6 that the guar-
anteed admissible heuristic functions described
above result in dramatically more efficient search.
</bodyText>
<sectionHeader confidence="0.980286" genericHeader="method">
5 Empirical heuristic functions
</sectionHeader>
<bodyText confidence="0.999929428571429">
In this section we describe a new method to ob-
tain an almost admissible heuristic function by a
multi pass search. This yields a significantly more
efficient search than using the admissible heuris-
tic functions. Thus, we lose the strict guarantee to
avoid search errors, but obtain a significant time
gain.
The idea of an empirical heuristic function
is to perform a multi-pass search. In the first
pass a good admissible heuristic function (here:
HTFLD) is used. If this search does not need too
much memory the search process is finished. If
the search failed, it is restarted using an improved
heuristic function which had been obtained during
the initial search process. This heuristic function
is computed such that it has the property that it
is admissible with respect to the explored search
space. That means, the heuristic function is op-
timistic with respect to every node in the search
space explored in the first pass.
Specifically, during the first pass, we maintain
a two-dimensional matrix hE(j, j0) with (J + 2) ·
(J + 2) entries which are all initialized with oc.
The entry hE(j, j0) is the best score that was com-
puted for translating the source language word in
position j0 if the previously covered source sen-
tence position is j. The matrix entry is updated
for every extension of a node n → n0:
</bodyText>
<equation confidence="0.966591">
hE(j(n),j(n0)) :=
{ }
= max hE(j(n),j(n0)), p(n,n0)
</equation>
<bodyText confidence="0.996618">
Here, p(n, n0) is the probability of the extension
n → n0. hE(0, j) is the empirical score of start-
ing a sentence by covering the j-th source sen-
tence position first. Likewise, hE(j, J + 1) is the
empirical score of finishing a sentence with j as
the last source sentence position that was covered.
This yields
</bodyText>
<equation confidence="0.9949885">
hE(j) = max
j06∈C(n)∨j0=J+1
</equation>
<bodyText confidence="0.999993">
In this calculation of hE(j), we maximize over
the columns of a matrix. The translation of the
source sentence can be viewed as a Traveling
Salesman Problem where the source sentence po-
sitions are the cities that have to be visited. Thus,
the maximization over the columns is equivalent
to assuring that the position j will be left after
the visit. We design an improved heuristic func-
tion using the following principle (Aigner, 1993):
Each city has to be both reached and left. There-
fore, in order to take an upper bound of reaching
a city into account, we divide each column of the
matrix by its maximum and maximize over the
rows of the matrix (Aigner, 1993):
</bodyText>
<equation confidence="0.9996055">
hE+(j) = max
j06∈C(n)∨j0=j(n) hE(j0,j)/hE(j0) .
</equation>
<bodyText confidence="0.993393">
We obtain the following empirical heuristic func-
tions:
</bodyText>
<equation confidence="0.998894">
11 HE(n) = hE(j)
j6∈C(n)∨j=j(n)
HE+(n) =
11 = hE(j) · 11 hE+(j0)
j6∈C(n)∨j=j(n) j06∈C(n)∨j0=J+1
</equation>
<bodyText confidence="0.971809">
If the search fails in the first pass due to the re-
striction of the number of hypotheses – which was
1 million in all experiments – the search can be
started again using HE+(n) as a heuristic. To
avoid an overestimation of the actual costs, we
multiply the empirical costs by a factor lower than
</bodyText>
<equation confidence="0.69324">
hE(j,j0) .
</equation>
<tableCaption confidence="0.9771065">
Table 3: Training corpus statistics (* without
punctuation marks).
</tableCaption>
<table confidence="0.999768333333333">
French English
sentences 49000 49000
words 743903 816964
words* 664058 730880
average sentence length 16.9 14.6
vocabulary size 19831 24892
</table>
<tableCaption confidence="0.978862">
Table 4: Test corpora statistics.
</tableCaption>
<table confidence="0.998286714285714">
Corpus # Sentences # Words
F E
T6 50 300 329
T8 50 400 403
T10 50 500 509
T12 50 600 601
T14 50 700 644
</table>
<bodyText confidence="0.908405545454546">
1. We found in our experiments that a factor of
0.7 is sufficient. The search was restarted up to 4
times if it failed. Using this method, it is possi-
ble to translate sentences that are longer than 10
words with a restriction to 1 million hypotheses.
Table 1 shows the value of the empirical heuris-
tic function of the empty node compared to the
score of the optimal goal node. The estimated
costs and the real costs now differ only by a fac-
tor of 1.5 instead of a factor of 3 for the TFLD
heuristic function before.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.95121525">
We present results on the HANSARDS task which
consists of proceedings of the Canadian parlia-
ment that are kept both in French and in English.
Table 3 shows the details of our training corpus.
We used different the test corpora with sentences
of length 6-14 words (Table 4).
In all experiments, we use the following two
error criteria:
</bodyText>
<listItem confidence="0.918914">
• WER (word error rate):
</listItem>
<bodyText confidence="0.994900672727273">
The WER is computed as the minimum
number of substitution, insertion and dele-
tion operations that have to be performed to
convert the generated string into the target
string.
• PER (position independent word error rate):
The word order of a French/English sentence
pair can be quite different. As a result, the
word order of the automatically generated
target sentence can be different from that of
the given target sentence, but nevertheless
acceptable so that the WER measure alone
could be misleading. In order to overcome
this problem, we introduce the position inde-
pendent word error rate (PER) as additional
measure. This measure compares the words
in the two sentences without taking the word
order into account.
In the following experiments we restricted the
maximum number of active search hypotheses in
A* search to 1 million. Every hypothesis has an
effective memory requirement of about 100 Byte.
Therefore, we obtain a dynamic memory require-
ment of about 100 MByte.
In order to speed up the search, we restricted
the reordering of words in IBM-style (Berger et
al., 1996; Tillmann, 2001). According to this re-
striction, up to 3 source sentence positions may be
skipped and translated later, i. e. during the search
process there may be up to 3 uncovered positions
left of the rightmost covered position in the source
sentence. The word error rate does not increase
compared to a non-restricted reordering, but the
search becomes much more efficient.
Table 5 shows how many sentences with differ-
ent sentence lengths can be translated using beam
search and A* with various heuristic functions.
Obviously, the BS approach is able to translate
any sentence length, therefore the search success
rate is 100%. Without any heuristic function A*
is only able to translate all 8-word sentences (with
the restriction of a maximum number of 1 million
hypotheses). Using more sophisticated heuristic
functions we are also able to translate all 10-word
sentences with A*.
Table 6 compares the search errors of A* and
BS. During the BS search, translation pruning
is carried out. The different hypotheses are dis-
tinguished according to the set of covered posi-
tions of the source sentence. For every set, the
best score of all hypotheses is computed. Only
those hypotheses are kept whose score is greater
than this best score multiplied with a threshold.
We chose the threshold to be 2.5, 5.0, 7.5 and 10.0
(see Table 6).
</bodyText>
<tableCaption confidence="0.994707">
Table 2: Effect of observation pruning on the translation quality (average over all test sets).
</tableCaption>
<table confidence="0.99990575">
# inverse 10 12 14 16 18 20
translations
WER 73.81 73.33 75.50 76.23 76.19 76.59
PER 68.02 66.93 70.07 71.16 71.24 71.16
</table>
<tableCaption confidence="0.9626995">
Table 5: Search Success Rate (1 million hypothe-
ses) [%].
</tableCaption>
<table confidence="0.999863">
sentence length 6 8 10 12
BS 100 100 100 100
A*: no 100 100 86 12
T 100 100 88 20
TF 100 100 88 22
TFL 100 100 92 36
TFLD 100 100 92 36
E 100 100 100 74
E+ 100 100 100 84
</table>
<tableCaption confidence="0.749108">
Table 6: Search errors [%].
</tableCaption>
<table confidence="0.873444833333333">
sentence length 6 8 10 12 14
BS 2.5 26 28 38 50 38
5.0 2 0 2 6 4
7.5 0 0 0 4 2
10.0 0 0 0 4 2
A* 0 0 0 0 0
</table>
<bodyText confidence="0.999022222222222">
For A* we never observe any search errors. In
the case of the admissible heuristic functions, this
is guaranteed by the approach. As can be seen
from Table 6, the BS algorithm with a large beam
rarely produces search errors.
Table 7 compares the translation efficiency of
the various search algorithms. We see that beam
search even with a very large beam producing
only very few search errors is much more efficient
than the used A* search algorithm.
Table 8 contains an assessment of translation
quality comparison of A* and BS using the T6,
T8, T10, T12-test corpus. For A*, we use the E+
rest cost estimation as this gives optimal results.
From the 200 sentences of these test corpora we
can translate 192 sentences using the 1 million hy-
potheses constraint. For the remaining sentences
we performed a search with 4 million hypotheses
</bodyText>
<tableCaption confidence="0.999268">
Table 7: Average search time [s] per sentence.
</tableCaption>
<table confidence="0.998642285714286">
sentence 6 8 10 12
length
BS: 2.5 0.06 0.18 0.60 1.16
5.0 0.24 0.84 2.90 6.48
7.5 0.50 2.14 7.06 16.26
10.0 0.78 3.30 11.86 26.42
A*: E+ 1.58 13.04 100 394
</table>
<bodyText confidence="0.896026">
(cf. below) which lead to a success for all the 12-
word sentences.
The number of hypotheses in A* search
We restricted the maximal number of hypotheses
to 1 million. This was sufficient for translating
10-word sentences, as the search algorithm suc-
cess rate in Table 5 shows. For longer sentences
it is necessary to allow for a larger number of hy-
potheses. For the sentences of lengths 12 and 14,
we performed an A* search (E+) with 2, 4 and
8 million possible hypotheses. The search algo-
rithm success rate for those searches is contained
in Table 9. We see a significant effect on the num-
ber of successful searches.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.978576">
We have developed sophisticated admissible and
almost admissible heuristic functions for statis-
tical machine translation. We have focussed on
Model 4, but most of the computations could
be easily extended to other statistical alignment
models (like HMM or Model 5). We especially
have observed the following effects:
• The heuristic function has a strong effect on
the efficiency of the A* search. Without any
heuristic function only 75 % of the test cor-
pus sentences can be translated (using the
1 million hypotheses constraint). Using the
</bodyText>
<tableCaption confidence="0.998493">
Table 8: Translation quality.
</tableCaption>
<table confidence="0.999053666666667">
BS (2.5) BS (5.0) BS (7.5) BS (10.0) A* (E+)
WER 69.65 68.78 68.68 68.68 68.68
PER 62.65 61.62 61.51 61.51 61.45
</table>
<tableCaption confidence="0.990542">
Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%].
</tableCaption>
<table confidence="0.898848666666667">
# hypotheses 1 million 2 million 4 million 8 million
12 42 80 100 100
14 2 20 70 100
</table>
<bodyText confidence="0.87514395">
best admissible heuristic function TFLD
we can translate 82 %.
• Using the empirical heuristic function we
can translate 96 % of the sentences with
A* search. This heuristic function does not
guarantee to avoid search errors, but this
case never occurred in our experiments.
From these results we conclude that it is often
possible to faster compute acceptable results us-
ing a beam search approach. Therefore, this is
the method of choice in practice. From a theo-
retical viewpoint it is interesting that using A* it
is possible to translate guaranteed without search
errors. In addition, without having a chance to
perform search without search errors it is almost
impossible to assess if errors in translation should
be assigned to the model/training or to the search
heuristics. Therefore, the A* algorithm is espe-
cially useful during the development of a statisti-
cal machine translation system.
</bodyText>
<sectionHeader confidence="0.972455" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999415777777778">
This paper is based on work supported partly
by the VERBMOBIL project (contract number
01 IV 701 T4) by the German Federal Min-
istry of Education, Science, Research and Tech-
nology. In addition, this work was supported
by the National Science Foundation under Grant
No. IIS-9820687 through the 1999 Workshop on
Language Engineering, Center for Language and
Speech Processing, Johns Hopkins University.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999872162790698">
M. Aigner. 1993. Diskrete Mathematik. Verlag Vieweg,
Braunschweig/Wiesbaden, Germany.
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf-
ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.
Smith, and D. Yarowsky. 1999. Statistical ma-
chine translation, final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/projects/
mt/final report/mt-final-report.ps.
A. L. Berger, S. A. Della Pietra P. F. Brown, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language translation apparatus and method of us-
ing context-based translation models. In United States
Patent, number 5510981. April.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263–311.
N. Nilsson. 1971. Problem-Solving Methods in Artificial
Intelligence. McGraw-Hill, McGraw-Hill, New York.
F. J. Och and H. Ney. 2000a. A comparison of alignment
models for statistical machine translation. In COLING
’00: The 18th Int. Conf. on Computational Linguistics,
pages 1086–1090, Saarbr¨ucken, Germany, August.
F. J. Och and H. Ney. 2000b. Improved statistical alignment
models. In Proc. of the 38th Annual Meeting of the As-
sociation for Computational Linguistics, pages 440–447,
Hongkong, China, October.
C. Tillmann and H. Ney. 2000. Word re-ordering and DP-
based search in statistical machine translation. In COL-
ING ’00: The 18th Int. Conf. on Computational Linguis-
tics, pages 850–856, Saarbr¨ucken, Germany, August.
C. Tillmann. 2001. Word Re-Ordering and Dynamic Pro-
gramming based Search Algorithms for Statistical Ma-
chine Translation. Ph.D. thesis, RWTH Aachen, Ger-
many, May.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING ’96: The
16th Int. Conf. on Computational Linguistics, pages 836–
841, Copenhagen, August.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in
statistical translation. In Proc. 35th Annual Conf. of the
Association for Computational Linguistics, pages 366–
372, Madrid, Spain, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880404">
<title confidence="0.999968">An Efficient A* Search Algorithm for Statistical Machine Translation</title>
<author confidence="0.999935">Franz Josef Och</author>
<author confidence="0.999935">Nicola Ueffing</author>
<author confidence="0.999935">Hermann Ney</author>
<affiliation confidence="0.959865">Lehrstuhl f¨ur Informatik VI, Computer Science RWTH Aachen - University of</affiliation>
<address confidence="0.930547">D-52056 Aachen,</address>
<abstract confidence="0.9994811875">In this paper, we describe an efficient A* search algorithm for statistical machine translation. In contrary to beamsearch or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various sophisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search algorithm with a beam-search approach on the Hansards task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Aigner</author>
</authors>
<title>Diskrete Mathematik.</title>
<date>1993</date>
<publisher>Verlag</publisher>
<contexts>
<context position="16542" citStr="Aigner, 1993" startWordPosition="2840" endWordPosition="2841">first. Likewise, hE(j, J + 1) is the empirical score of finishing a sentence with j as the last source sentence position that was covered. This yields hE(j) = max j06∈C(n)∨j0=J+1 In this calculation of hE(j), we maximize over the columns of a matrix. The translation of the source sentence can be viewed as a Traveling Salesman Problem where the source sentence positions are the cities that have to be visited. Thus, the maximization over the columns is equivalent to assuring that the position j will be left after the visit. We design an improved heuristic function using the following principle (Aigner, 1993): Each city has to be both reached and left. Therefore, in order to take an upper bound of reaching a city into account, we divide each column of the matrix by its maximum and maximize over the rows of the matrix (Aigner, 1993): hE+(j) = max j06∈C(n)∨j0=j(n) hE(j0,j)/hE(j0) . We obtain the following empirical heuristic functions: 11 HE(n) = hE(j) j6∈C(n)∨j=j(n) HE+(n) = 11 = hE(j) · 11 hE+(j0) j6∈C(n)∨j=j(n) j06∈C(n)∨j0=J+1 If the search fails in the first pass due to the restriction of the number of hypotheses – which was 1 million in all experiments – the search can be started again using HE</context>
</contexts>
<marker>Aigner, 1993</marker>
<rawString>M. Aigner. 1993. Diskrete Mathematik. Verlag Vieweg, Braunschweig/Wiesbaden, Germany. Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty, I. D. Melamed, F. J. Och, D. Purdy, N. A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smith</author>
<author>D Yarowsky</author>
</authors>
<date>1999</date>
<note>Statistical machine translation, final report, JHU workshop. http://www.clsp.jhu.edu/ws99/projects/ mt/final report/mt-final-report.ps.</note>
<marker>Smith, Yarowsky, 1999</marker>
<rawString>Smith, and D. Yarowsky. 1999. Statistical machine translation, final report, JHU workshop. http://www.clsp.jhu.edu/ws99/projects/ mt/final report/mt-final-report.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra P F Brown</author>
<author>V J Della Pietra</author>
<author>J R Gillett</author>
<author>A S Kehler</author>
<author>R L Mercer</author>
</authors>
<title>Language translation apparatus and method of using context-based translation models.</title>
<date>1996</date>
<booktitle>In United States Patent,</booktitle>
<pages>5510981</pages>
<contexts>
<context position="5088" citStr="Berger et al., 1996" startWordPosition="891" endWordPosition="894"> intermediary nodes (partial hypotheses), and goal nodes (completed hypotheses). A node contains the following information: – the predecessor words u, v in the target language, – the score of the hypothesis, – a backpointer to the preceding partial hypothesis, – the model specific information described at the end of this subsection. • A scoring function Q(n) + h(n) has to be defined which assigns a score to every node n. For beam search, this is the score Q(n) of a best path to this node. In the A* algorithm, an estimation h(n) of the score of a best path from node n to a goal node is added. (Berger et al., 1996) presented a method to structure the search space. Our search algorithm for Model 4 uses a similar structuring of the search space. We will shortly review the basic concepts of this search space structure: Every partial hypothesis consists of a prefix of the target sentence and a corresponding alignment. A partial hypothesis is extended by accounting for exactly one additional word of the source sentence. Every extension yields an extension score which is computed by taking into account the lexicon, distortion, and fertility probabilities involved with this extension. A partial hypothesis is c</context>
<context position="19591" citStr="Berger et al., 1996" startWordPosition="3372" endWordPosition="3375">e WER measure alone could be misleading. In order to overcome this problem, we introduce the position independent word error rate (PER) as additional measure. This measure compares the words in the two sentences without taking the word order into account. In the following experiments we restricted the maximum number of active search hypotheses in A* search to 1 million. Every hypothesis has an effective memory requirement of about 100 Byte. Therefore, we obtain a dynamic memory requirement of about 100 MByte. In order to speed up the search, we restricted the reordering of words in IBM-style (Berger et al., 1996; Tillmann, 2001). According to this restriction, up to 3 source sentence positions may be skipped and translated later, i. e. during the search process there may be up to 3 uncovered positions left of the rightmost covered position in the source sentence. The word error rate does not increase compared to a non-restricted reordering, but the search becomes much more efficient. Table 5 shows how many sentences with different sentence lengths can be translated using beam search and A* with various heuristic functions. Obviously, the BS approach is able to translate any sentence length, therefore</context>
</contexts>
<marker>Berger, Brown, Pietra, Gillett, Kehler, Mercer, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra P. F. Brown, V. J. Della Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer. 1996. Language translation apparatus and method of using context-based translation models. In United States Patent, number 5510981. April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1451" citStr="Brown et al., 1993" startWordPosition="220" endWordPosition="223">sk. 1 Introduction The goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ1 = f1...fj...fJ, which is to be translated into a target string eI1 = e1...ei...eI. Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) try to model word-to-word correspondences between source and target words. These correspondences are called an alignment. The model is often further restricted in a way such that each source word is assigned exactly one target word. The alignment mapping is j —* i = aj from source position j to target position i = aj. The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source words that are not aligned to any target word. In (statistical) alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidde</context>
<context position="3721" citStr="Brown et al., 1993" startWordPosition="646" endWordPosition="649">st of the two consecutive target words, • the distortion model p&gt;1(j − j&apos;|C(fj)) for the probability that the words aligned to one target words have the position difference j − j&apos;, • the fertility model p(φ|e) for the probability that a target language word e is aligned to φ source language words, • the empty word fertility model p(φ0|e0) for the probability that exactly φ0 words remain unaligned to. The final probability p(fJ1 , aJ1 |eI1) for Model 4 is obtained by multiplying the probabilities of the sub-models for all words. For a detailed description for Model 4 the reader is referred to (Brown et al., 1993). We use Model 4 in this paper for two reasons. First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b). Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process. This would be more difficult in the HMM alignment model (Vogel et al., 1996). Yet, many of the results presented in the following are also applicable to other alignment models. 3 Search problem The following tasks have to be performed bot</context>
<context position="10249" citStr="Brown et al., 1993" startWordPosition="1766" endWordPosition="1769">hypothesis. This function is called admissible if it never underestimates this probability. Thus, admissible heuristic functions are always optimistic. The A* search algorithm corresponds to the Dijkstra algorithm if the heuristic function is equal to zero. 4 Admissible heuristic function In order to perform an efficient search with the A* search algorithm it is crucial to use a good heuristic function. We only know of the work by (Wang and Waibel, 1997) dealing with heuristic functions for search in statistical machine translation. They developed a simple heuristic function for Model 2 from (Brown et al., 1993) which was non admissible. In the following we develop a guaranteed admissible heuristic function for Model 4 taking into account distortion probabilities and the coupling of lexicon, fertility, and language model probabilities. The basic idea for developing a heuristic function for the alignment models is the fact that all source sentence positions which have not been covered so far still have to be translated in order to complete the sentence. Therefore, the value of the heuristic function HX(n) for a node n can be deduced if we have an estimation hX(j) of the optimal score of translating po</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Nilsson</author>
</authors>
<date>1971</date>
<booktitle>Problem-Solving Methods in Artificial Intelligence.</booktitle>
<publisher>McGraw-Hill, McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="9214" citStr="Nilsson, 1971" startWordPosition="1598" endWordPosition="1600">mbination, the number of needed hypotheses can be reduced by about a factor of 4. Search algorithms We evaluate the following two search algorithms: • beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner. The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses. For more details see (Tillmann, 2001). • A* search algorithm: In A*, all search hypotheses are managed in a priority queue. The basic A* search (Nilsson, 1971) can be described as follows: 1. initialize priority queue with an empty hypothesis 2. remove the hypothesis with the highest score from the priority queue 3. if this hypothesis is a goal hypothesis: output this hypothesis and terminate 4. produce all extensions of this hypothesis and put the extensions to the queue 5. goto 2 The so-called heuristic function estimates the probability of a completion of a partial hypothesis. This function is called admissible if it never underestimates this probability. Thus, admissible heuristic functions are always optimistic. The A* search algorithm correspo</context>
</contexts>
<marker>Nilsson, 1971</marker>
<rawString>N. Nilsson. 1971. Problem-Solving Methods in Artificial Intelligence. McGraw-Hill, McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING ’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="1490" citStr="Och and Ney, 2000" startWordPosition="228" endWordPosition="231">ranslation is the translation of a text given in some source language into a target language. We are given a source string fJ1 = f1...fj...fJ, which is to be translated into a target string eI1 = e1...ei...eI. Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) try to model word-to-word correspondences between source and target words. These correspondences are called an alignment. The model is often further restricted in a way such that each source word is assigned exactly one target word. The alignment mapping is j —* i = aj from source position j to target position i = aj. The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source words that are not aligned to any target word. In (statistical) alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable. Typically, the search is pe</context>
<context position="3912" citStr="Och and Ney, 2000" startWordPosition="681" endWordPosition="684">ity model p(φ|e) for the probability that a target language word e is aligned to φ source language words, • the empty word fertility model p(φ0|e0) for the probability that exactly φ0 words remain unaligned to. The final probability p(fJ1 , aJ1 |eI1) for Model 4 is obtained by multiplying the probabilities of the sub-models for all words. For a detailed description for Model 4 the reader is referred to (Brown et al., 1993). We use Model 4 in this paper for two reasons. First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b). Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process. This would be more difficult in the HMM alignment model (Vogel et al., 1996). Yet, many of the results presented in the following are also applicable to other alignment models. 3 Search problem The following tasks have to be performed both using A* and beam search (BS): • The search space has to be structured into a search graph. This search graph typically includes an initial node, intermediary nodes (partial hypotheses), an</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000a. A comparison of alignment models for statistical machine translation. In COLING ’00: The 18th Int. Conf. on Computational Linguistics, pages 1086–1090, Saarbr¨ucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="1490" citStr="Och and Ney, 2000" startWordPosition="228" endWordPosition="231">ranslation is the translation of a text given in some source language into a target language. We are given a source string fJ1 = f1...fj...fJ, which is to be translated into a target string eI1 = e1...ei...eI. Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) try to model word-to-word correspondences between source and target words. These correspondences are called an alignment. The model is often further restricted in a way such that each source word is assigned exactly one target word. The alignment mapping is j —* i = aj from source position j to target position i = aj. The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source words that are not aligned to any target word. In (statistical) alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable. Typically, the search is pe</context>
<context position="3912" citStr="Och and Ney, 2000" startWordPosition="681" endWordPosition="684">ity model p(φ|e) for the probability that a target language word e is aligned to φ source language words, • the empty word fertility model p(φ0|e0) for the probability that exactly φ0 words remain unaligned to. The final probability p(fJ1 , aJ1 |eI1) for Model 4 is obtained by multiplying the probabilities of the sub-models for all words. For a detailed description for Model 4 the reader is referred to (Brown et al., 1993). We use Model 4 in this paper for two reasons. First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b). Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process. This would be more difficult in the HMM alignment model (Vogel et al., 1996). Yet, many of the results presented in the following are also applicable to other alignment models. 3 Search problem The following tasks have to be performed both using A* and beam search (BS): • The search space has to be structured into a search graph. This search graph typically includes an initial node, intermediary nodes (partial hypotheses), an</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000b. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Word re-ordering and DPbased search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING ’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>850--856</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="8819" citStr="Tillmann and Ney, 2000" startWordPosition="1530" endWordPosition="1533">operty that if the count of the bigram N(u, v) = 0, then the probability P(w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose language model state has the property N(u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well. Experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about a factor of 4. Search algorithms We evaluate the following two search algorithms: • beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner. The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses. For more details see (Tillmann, 2001). • A* search algorithm: In A*, all search hypotheses are managed in a priority queue. The basic A* search (Nilsson, 1971) can be described as follows: 1. initialize priority queue with an empty hypothesis 2. remove the hypothesis with the highest score from the priority queue 3. if this hypothesis is a goal hypothesis: outpu</context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>C. Tillmann and H. Ney. 2000. Word re-ordering and DPbased search in statistical machine translation. In COLING ’00: The 18th Int. Conf. on Computational Linguistics, pages 850–856, Saarbr¨ucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
</authors>
<title>Word Re-Ordering and Dynamic Programming based Search Algorithms for Statistical Machine Translation.</title>
<date>2001</date>
<booktitle>Ph.D. thesis, RWTH</booktitle>
<location>Aachen, Germany,</location>
<contexts>
<context position="8794" citStr="Tillmann, 2001" startWordPosition="1528" endWordPosition="1529">e use has the property that if the count of the bigram N(u, v) = 0, then the probability P(w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose language model state has the property N(u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well. Experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about a factor of 4. Search algorithms We evaluate the following two search algorithms: • beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner. The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses. For more details see (Tillmann, 2001). • A* search algorithm: In A*, all search hypotheses are managed in a priority queue. The basic A* search (Nilsson, 1971) can be described as follows: 1. initialize priority queue with an empty hypothesis 2. remove the hypothesis with the highest score from the priority queue 3. if this hypothesis is</context>
<context position="19608" citStr="Tillmann, 2001" startWordPosition="3376" endWordPosition="3377">ould be misleading. In order to overcome this problem, we introduce the position independent word error rate (PER) as additional measure. This measure compares the words in the two sentences without taking the word order into account. In the following experiments we restricted the maximum number of active search hypotheses in A* search to 1 million. Every hypothesis has an effective memory requirement of about 100 Byte. Therefore, we obtain a dynamic memory requirement of about 100 MByte. In order to speed up the search, we restricted the reordering of words in IBM-style (Berger et al., 1996; Tillmann, 2001). According to this restriction, up to 3 source sentence positions may be skipped and translated later, i. e. during the search process there may be up to 3 uncovered positions left of the rightmost covered position in the source sentence. The word error rate does not increase compared to a non-restricted reordering, but the search becomes much more efficient. Table 5 shows how many sentences with different sentence lengths can be translated using beam search and A* with various heuristic functions. Obviously, the BS approach is able to translate any sentence length, therefore the search succe</context>
</contexts>
<marker>Tillmann, 2001</marker>
<rawString>C. Tillmann. 2001. Word Re-Ordering and Dynamic Programming based Search Algorithms for Statistical Machine Translation. Ph.D. thesis, RWTH Aachen, Germany, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen,</location>
<contexts>
<context position="1471" citStr="Vogel et al., 1996" startWordPosition="224" endWordPosition="227">he goal of machine translation is the translation of a text given in some source language into a target language. We are given a source string fJ1 = f1...fj...fJ, which is to be translated into a target string eI1 = e1...ei...eI. Among all possible target strings, we will choose the string with the highest probability: The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language. Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000b) try to model word-to-word correspondences between source and target words. These correspondences are called an alignment. The model is often further restricted in a way such that each source word is assigned exactly one target word. The alignment mapping is j —* i = aj from source position j to target position i = aj. The alignment aJ1 may contain alignments aj = 0 with the ‘empty’ word e0 to account for source words that are not aligned to any target word. In (statistical) alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable. Typicall</context>
<context position="4159" citStr="Vogel et al., 1996" startWordPosition="721" endWordPosition="724">|eI1) for Model 4 is obtained by multiplying the probabilities of the sub-models for all words. For a detailed description for Model 4 the reader is referred to (Brown et al., 1993). We use Model 4 in this paper for two reasons. First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b). Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process. This would be more difficult in the HMM alignment model (Vogel et al., 1996). Yet, many of the results presented in the following are also applicable to other alignment models. 3 Search problem The following tasks have to be performed both using A* and beam search (BS): • The search space has to be structured into a search graph. This search graph typically includes an initial node, intermediary nodes (partial hypotheses), and goal nodes (completed hypotheses). A node contains the following information: – the predecessor words u, v in the target language, – the score of the hypothesis, – a backpointer to the preceding partial hypothesis, – the model specific informati</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836– 841, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding algorithm in statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. 35th Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>366--372</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="10088" citStr="Wang and Waibel, 1997" startWordPosition="1740" endWordPosition="1743">extensions of this hypothesis and put the extensions to the queue 5. goto 2 The so-called heuristic function estimates the probability of a completion of a partial hypothesis. This function is called admissible if it never underestimates this probability. Thus, admissible heuristic functions are always optimistic. The A* search algorithm corresponds to the Dijkstra algorithm if the heuristic function is equal to zero. 4 Admissible heuristic function In order to perform an efficient search with the A* search algorithm it is crucial to use a good heuristic function. We only know of the work by (Wang and Waibel, 1997) dealing with heuristic functions for search in statistical machine translation. They developed a simple heuristic function for Model 2 from (Brown et al., 1993) which was non admissible. In the following we develop a guaranteed admissible heuristic function for Model 4 taking into account distortion probabilities and the coupling of lexicon, fertility, and language model probabilities. The basic idea for developing a heuristic function for the alignment models is the fact that all source sentence positions which have not been covered so far still have to be translated in order to complete the</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical translation. In Proc. 35th Annual Conf. of the Association for Computational Linguistics, pages 366– 372, Madrid, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>