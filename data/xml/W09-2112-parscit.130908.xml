<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002162">
<title confidence="0.996205">
GenERRate: Generating Errors for Use in Grammatical Error Detection
</title>
<author confidence="0.995494">
Jennifer Foster
</author>
<affiliation confidence="0.995615666666667">
National Centre for Language Technology
School of Computing
Dublin City University, Ireland
</affiliation>
<email confidence="0.972382">
jfoster@computing.dcu.ie
</email>
<author confidence="0.920389">
Øistein E. Andersen
</author>
<affiliation confidence="0.9765425">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.652339">
United Kingdom
</address>
<email confidence="0.994205">
oa223@cam.ac.uk
</email>
<sectionHeader confidence="0.995577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9987949">
This paper explores the issue of automatically
generated ungrammatical data and its use in
error detection, with a focus on the task of
classifying a sentence as grammatical or un-
grammatical. We present an error generation
tool called GenERRate and show how Gen-
ERRate can be used to improve the perfor-
mance of a classifier on learner data. We de-
scribe initial attempts to replicate Cambridge
Learner Corpus errors using GenERRate.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945380952381">
In recent years automatically generated ungrammat-
ical data has been used in the training and evalu-
ation of error detection systems, in evaluating the
robustness of NLP tools and as negative evidence
in unsupervised learning. The main advantage of
using such artificial data is that it is cheap to pro-
duce. However, it is of little use if it is not a real-
istic model of the naturally occurring and expensive
data that it is designed to replace. In this paper we
explore the issues involved in generating synthetic
data and present a tool called GenERRate which can
be used to produce many different kinds of syntacti-
cally noisy data. We use the tool in two experiments
in which we attempt to train classifiers to distinguish
between grammatical and ungrammatical sentences.
In the first experiment, we show how GenERRate
can be used to improve the performance of an ex-
isting classifier on sentences from a learner corpus
of transcribed spoken utterances. In the second ex-
periment we try to produce a synthetic error corpus
that is inspired by the Cambridge Learner Corpus
</bodyText>
<page confidence="0.987931">
82
</page>
<bodyText confidence="0.9998898125">
(CLC)1, and we evaluate the difference between a
classifier’s performance when trained on this data
and its performance when trained on original CLC
material. The results of both experiments provide
pointers on how to improve GenERRate, as well as
highlighting some of the challenges associated with
automatically generating negative evidence.
The paper is organised as follows: In Section 2,
we discuss the reasons why artificial ungrammatical
data has been used in NLP and we survey its use
in the field, focussing mainly on grammatical error
detection. Section 3 contains a description of the
GenERRate tool. The two classification experiments
which use GenERRate are described in Section 4.
Problematic issues are discussed in Section 5 and
avenues for future work in Section 6.
</bodyText>
<sectionHeader confidence="0.995" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999295">
2.1 Why artifical error data is useful
</subsectionHeader>
<bodyText confidence="0.999854833333333">
Before pointing out the benefit of using artificial
negative evidence in grammatical error detection, it
is worth reminding ourselves of the benefits of em-
ploying negative evidence, be it artificial or natu-
rally occurring. By grammatical error detection, we
mean either the task of distinguishing the grammat-
ical from the ungrammatical at the sentence level
or more local targeted error detection, involving the
identification, and possibly also correction, of par-
ticular types of errors. Distinguishing grammati-
cal utterances from ungrammatical ones involves the
use of a binary classifier or a grammaticality scor-
</bodyText>
<footnote confidence="0.9982385">
1http://www.cambridge.org/elt/corpus/
learner_corpus2.htm
</footnote>
<note confidence="0.994665">
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999432573333334">
ing model. Examples are Andersen (2006; 2007),
Okanohara and Tsujii (2007), Sun et al. (2007) and
Wagner et al. (2007). In targeted error detection,
the focus is on identifying the common errors made
either by language learners or native speakers (de-
pending on the application). For ESL applications,
this includes the detection of errors involving ar-
ticles (Han et al., 2006; De Felice and Pulman,
2008; Gamon et al., 2008), prepositions (De Felice
and Pulman, 2008; Gamon et al., 2008; Tetreault
and Chodorow, 2008), verb forms (Lee and Seneff,
2008b), mass/count noun confusions (Brockett et al.,
2006) and word order (Metcalf and Meurers, 2006).
The presence of a pattern in a corpus of well-
formed language is positive evidence that the pat-
tern is well-formed. The presence of a pattern in an
corpus of ill-formed language is negative evidence
that the pattern is erroneous. Discriminative tech-
niques usually lead to more accurate systems than
those based on one class alone. The use of the two
types of evidence can be seen at work in the system
described by Lee and Seneff (2008b): Verb phrases
are parsed and their parse trees are examined. If
the parse trees resemble the “disturbed” trees that
statistical parsers typically produce when an incor-
rect verb form is used, the verb phrase is consid-
ered a likely candidate for correction. However, to
avoid overcorrection, positive evidence in the form
of Google n-gram statistics is also employed: a cor-
rection is only applied if its n-gram frequency is
higher than that of the original uncorrected n-gram.
The ideal situation for a grammatical error de-
tection system is one where a large amount of la-
belled positive and negative evidence is available.
Depending on the aims of the system, this labelling
can range from simply marking a sentence as un-
grammatical to a detailed description of the error
along with a correction. If an error detection sys-
tem employs machine learning, the performance of
the system will improve as the training set size in-
creases (up to a certain point). For systems which
employ learning algorithms with large feature sets
(e.g. maximum entropy, support vector machines),
the size of the training set is particularly important
so that overfitting is avoided. The collection of a
large corpus of ungrammatical data requires a good
deal of manual effort. Even if the annotation only
involves marking the sentence as correct/incorrect,
it still requires that the sentence be read and a gram-
maticality judgement applied to it. If more detailed
annotation is applied, the process takes even longer.
Some substantially-sized annotated error corpora do
exist, e.g. the Cambridge Learner Corpus, but these
are not freely available.
One way around this problem of lack of availabil-
ity of suitably large error-annotated corpora is to in-
troduce errors into sentences automatically. In order
for the resulting error corpus to be useful in an error
detection system, the errors that are introduced need
to resemble those that the system aims to detect.
Thus, the process is not without some manual effort:
knowing what kind of errors to introduce requires
the inspection of real error data, a process similar
to error annotation. Once the error types have been
specified though, the process is fully automatic and
allows large error corpora to be compiled. If the set
of well-formed sentences into which the errors are
introduced is large and varied enough, it is possi-
ble that this will result in ungrammatical sentence
structures which learners produce but which have
not yet been recorded in the smaller naturally occur-
ring learner corpora. To put it another way, the same
type of error will appear in lexically and syntacti-
cally varied contexts, which is potentially advanta-
geous when training a classifier.
</bodyText>
<subsectionHeader confidence="0.999585">
2.2 Where artificial error data has been used
</subsectionHeader>
<bodyText confidence="0.999956105263158">
Artificial errors have been employed previously in
targeted error detection. Sj¨obergh and Knutsson
(2005) introduce split compound errors and word or-
der errors into Swedish texts and use the resulting
artificial data to train their error detection system.
These two particular error types are chosen because
they are frequent errors amongst non-native Swedish
speakers whose first language does not contain com-
pounds or has a fixed word order. They compare the
resulting system to three Swedish grammar check-
ers, and find that their system has higher recall at the
expense of lower precision. Brockett et al. (2006)
introduce errors involving mass/count noun confu-
sions into English newswire text and then use the re-
sulting parallel corpus to train a phrasal SMT system
to perform error correction. Lee and Seneff (2008b)
automatically introduce verb form errors (subject–
verb agreement errors, complementation errors and
errors in a main verb after an auxiliary) into well-
</bodyText>
<page confidence="0.997988">
83
</page>
<bodyText confidence="0.999981386666666">
formed text, parse the resulting text and examine the
parse trees produced.
Both Okanohara and Tsujii (2007) and Wagner et
al. (2007) attempt to learn a model which discrimi-
nates between grammatical and ungrammatical sen-
tences, and both use synthetic negative data which
is obtained by distorting sentences from the British
National Corpus (BNC) (Burnard, 2000). The meth-
ods used to distort the BNC sentences are, however,
quite different. Okanohara and Tsujii (2007) gener-
ate ill-formed sentences by sampling a probabilistic
language model and end up with “pseudo-negative”
examples which resemble machine translation out-
put more than they do learner texts. Indeed, ma-
chine translation is one of the applications of their
resulting discriminative language model. Wagner
et al. (2007) introduce grammatical errors of the
following four types into BNC sentences: context-
sensitive spelling errors, agreement errors, errors in-
volving a missing word and errors involving an extra
word. All four types are considered equally likely
and the resulting synthetic corpus contains errors
that look like the kind of slips that would be made
by native speakers (e.g. repeated adjacent words) as
well as errors that resemble learner errors (e.g. miss-
ing articles). Wagner et al. (2009) report a drop in
accuracy for their classification methods when ap-
plied to real learner texts as opposed to held-out syn-
thetic test data, reinforcing the earlier point that ar-
tificial errors need to be tailored for the task at hand
(we return to this in Section 4.1).
Artificial error data has also proven useful in
the automatic evaluation of error detection systems.
Bigert (2004) describes how a tool called Missplel
is used to generate context-sensitive spelling errors
which are then used to evaluate a context-sensitive
spelling error detection system. The performance
of general-purpose NLP tools such as part-of-speech
taggers and parsers in the face of noisy ungrammat-
ical data has been automatically evaluated using ar-
tificial error data. Since the features of machine-
learned error detectors are often part-of-speech n-
grams or word–word dependencies extracted from
parser output (De Felice and Pulman, 2008, for ex-
ample), it is important to understand how part-of-
speech taggers and parsers react to particular gram-
matical errors. Bigert et al. (2005) introduce artifi-
cial context-sensitive spelling errors into error-free
Swedish text and then evaluate parsers and a part-
of-speech tagger on this text using their performance
on the error-free text as a reference. Similarly, Fos-
ter (2007) investigates the effect of common English
grammatical errors on two widely-used statistical
parsers using distorted treebank trees as references.
The procedure used by Wagner et al. (2007; 2009) is
used to introduce errors into the treebank sentences.
Finally, negative evidence in the form of automat-
ically distorted sentences has been used in unsuper-
vised learning. Smith and Eisner (2005a; 2005b)
generate negative evidence for their contrastive es-
timation method by moving or removing a word in a
sentence. Since the aim of this work is not to detect
grammatical errors, there is no requirement to gener-
ate the kind of negative evidence that might actually
be produced by either native or non-native speakers
of a language. The negative examples are used to
guide the unsupervised learning of a part-of-speech
tagger and a dependency grammar.
We can conclude from this survey that synthetic
error data is useful in a variety of NLP applications,
including error detection and evaluation of error de-
tectors. In Section 3, we describe an automatic error
generation tool, which has a modular design and is
flexible enough to accommodate the generation of
the various types of synthetic data described above.
</bodyText>
<sectionHeader confidence="0.998381" genericHeader="method">
3 Error Generation Tool
</sectionHeader>
<bodyText confidence="0.999985">
GenERRate is an error generation tool which ac-
cepts as input a corpus and an error analysis file
consisting of a list of errors and produces an error-
tagged corpus of syntactically ill-formed sentences.
The sentences in the input corpus are assumed to be
grammatically well-formed. GenERRate is imple-
mented in Java and will be made available to down-
load for use by other researchers.2
</bodyText>
<subsectionHeader confidence="0.999673">
3.1 Supported Error Types
</subsectionHeader>
<bodyText confidence="0.999929833333333">
Error types are defined in terms of their corrections,
that is, in terms of the operations (insert, delete, sub-
stitute and move) that are applied to a well-formed
sentence to make it ill-formed. As well as being
a popular classification scheme in the field of er-
ror analysis (James, 1998), it has the advantage of
</bodyText>
<footnote confidence="0.959888">
2http://www.computing.dcu.ie/˜jfoster/
resources/generrate.html
</footnote>
<page confidence="0.998366">
84
</page>
<bodyText confidence="0.99648675">
being theory-neutral. This is important in this con-
text since it is hoped that GenERRate will be used
to create negative evidence of various types, be it
L2-like grammatical errors, native speaker slips or
more random syntactic noise. It is hoped that Gen-
ERRate will be easy to use for anyone working in
linguistics, applied linguistics, language teaching or
computational linguistics.
The inheritance hierarchy in Fig. 1 shows the er-
ror types that are supported by GenERRate. We
briefly describe each error type.
Errors generated by removing a word
</bodyText>
<listItem confidence="0.997438">
• DeletionError: Generated by selecting a word
at random from the sentence and removing it.
• DeletionPOSError: Extends DeletionError by
allowing a specific POS to be specified.
• DeletionPOSWhereError: Extends Deletion-
POSError by allowing left and/or right context
(POS tag or start/end) to be specified.
</listItem>
<bodyText confidence="0.807612">
Errors generated by inserting a word
</bodyText>
<listItem confidence="0.9979520625">
• InsertionError: Insert a random word at a ran-
dom position. The word is chosen either from
the sentence itself or from a word list, and this
choice is also random.
• InsertionFromFileOrSentenceError: This
differs from the InsertionError in that the de-
cision of whether to use the sentence itself or a
word list is not made at random but supplied in
the error type specification.
• InsertionPOSError: Extends InsertionFrom-
FileOrSentenceError by allowing the POS of
the new word to be specified.
• InsertionPOSWhereError: Analogous to the
DeletionPOSWhereError, this extends Inser-
tionPOSError by allowing left and/or right con-
text to be specified.
</listItem>
<bodyText confidence="0.777803">
Errors generated by moving a word
</bodyText>
<listItem confidence="0.997845125">
• MoveError: Generated by randomly selecting
a word in the sentence and moving it to another
position, randomly chosen, in the sentence.
• MovePOSError: A word tagged with the
specified POS is randomly chosen and moved
to a randomly chosen position in the sentence.
• MovePOSWhereError: Extends Move-
POSError by allowing the change in position
</listItem>
<figure confidence="0.5793594">
subst,word,an,a,0.2
subst,NNS,NN,0.4
subst,VBG,TO,0.2
delete,DT,0.1
move,RB,left,1,0.1
</figure>
<figureCaption confidence="0.997082">
Figure 2: GenERRate Toy Error Analysis File
</figureCaption>
<bodyText confidence="0.952457333333333">
to be specified in terms of direction and
number of words.
Errors generated by substituting a word
</bodyText>
<listItem confidence="0.873509888888889">
• SubstError: Replace a random word by a
word chosen at random from a word list.
• SubstWordConfusionError: Extends Sub-
stError by allowing the POS to be specified
(same POS for both words).
• SubstWordConfusionNewPOSError: Simi-
lar to SubstWordConfusionError, but allows
different POSs to be specified.
• SubstSpecificWordConfusionError:
Replace a specific word with another
(e.g. be/have).
• SubstWrongFormError: Replace a word
with a different form of the same word. The
following changes are currently supported:
noun number (e.g. word/words), verb number
(write/writes), verb form (writing/written), ad-
jective form (big/bigger) and adjective/adverb
(quick/quickly). Note that this is the only er-
</listItem>
<bodyText confidence="0.9330795">
ror type which is language-specific. At the mo-
ment, only English is supported.
</bodyText>
<subsectionHeader confidence="0.992588">
3.2 Input Corpus
</subsectionHeader>
<bodyText confidence="0.999263285714286">
The corpus that is supplied as input to GenERRate
must be split into sentences. It does not have to be
part-of-speech tagged, but it will not be possible to
generate many of the errors if it is not. GenERRate
has been tested using two part-of-speech tagsets,
the Penn Treebank tagset (Santorini, 1991) and the
CLAWS tagset (Garside et al., 1987).
</bodyText>
<subsectionHeader confidence="0.97014">
3.3 Error Analysis File
</subsectionHeader>
<bodyText confidence="0.9997608">
The error analysis file specifies the errors that Gen-
ERRate should attempt to insert into the sentences
in the input corpus. A toy example with the Penn
tagset is shown in Fig. 2. The first line is an instance
of a SubstSpecificWordConfusion error. The second
</bodyText>
<page confidence="0.991845">
85
</page>
<figure confidence="0.991576285714286">
Error
Move
MovePOS
MovePOSWhere
Subst
SubstWordConfusion SubstWrongForm
Deletion
DeletionPOS
DeletionPOSWhere
Insertion
InsertionFromFileOrSentence
InsertionPOS
InsertionPOSWhere
SubstWordConfusionNewPOS SubstSpecificWordConfusion
</figure>
<figureCaption confidence="0.999961">
Figure 1: GenERRate Error Types
</figureCaption>
<bodyText confidence="0.999623777777778">
and third are instances of the SubstWrongFormEr-
ror type. The fourth is a DeletionPOSError, and the
fifth is a MovePOSWhereError. The number in the
final column specifies the desired proportion of the
particular error type in the output corpus and is op-
tional. However, if it is present for one error type, it
must be present for all. The overall size of the out-
put corpus is supplied as a parameter when running
GenERRate.
</bodyText>
<subsectionHeader confidence="0.79188">
3.4 Error Generation
</subsectionHeader>
<bodyText confidence="0.9998792">
When frequency information is not supplied in the
error analysis file, GenERRate iterates through each
error in the error analysis file and each sentence in
the input corpus, tries to insert an error of this type
into the sentence and writes the resulting sentence
to the output file together with a description of the
error. GenERRate includes an option to write the
sentences into which an error could not be inserted
and the reason for the failure to a log file. When the
error analysis file does include frequency informa-
tion, a slightly different algorithm is used: for each
error, GenERRate selects sentences from the input
file and attempts to generate an instance of that error
until the desired number of errors has been produced
or all sentences have been tried.
</bodyText>
<sectionHeader confidence="0.995804" genericHeader="method">
4 Classification Experiments
</sectionHeader>
<bodyText confidence="0.999989">
We describe two experiments which involve the
use of GenERRate in a binary classification task
in which the classifiers attempt to distinguish be-
tween grammatically well-formed and ill-formed
sentences or, more precisely, to distinguish between
sentences in learner corpora which have been anno-
tated as erroneous and their corrected counterparts.
In the first experiment we use GenERRate to cre-
ate ungrammatical training data using information
about error types gleaned from a subset of a corpus
of transcribed spoken utterances produced by ESL
learners in a classroom environment. The classifier
is one of those described in Wagner et al. (2007).
In the second experiment we try to generate a CLC-
inspired error corpus and we use one of the simplest
classifiers described in Andersen (2006). Our aim
is not to improve classification performance, but to
test the GenERRate tool, to demonstrate how it can
be used and to investigate differences between syn-
thetic and naturally occurring datasets.
</bodyText>
<subsectionHeader confidence="0.9995915">
4.1 Experiments with a Spoken Language
Learner Corpus
</subsectionHeader>
<bodyText confidence="0.9999103125">
Wagner et al. (2009) train various classifiers to
distinguish between BNC sentences and artificially
produced ungrammatical versions of BNC sentences
(see §2). They report a significant drop in accuracy
when they apply these classifiers to real learner data,
including the sentences in a corpus of transcribed
spoken utterances. The aim of this experiment is to
investigate to what extent this loss in accuracy can
be reduced by using GenERRate to produce a more
realistic set of ungrammatical training examples.
The spoken language learner corpus contains over
4,000 transcribed spoken sentences which were pro-
duced by learners of English of all levels and with
a variety of L1s. The sentences were produced in
a classroom setting and transcribed by the teacher.
The transcriptions were verified by the students. All
</bodyText>
<page confidence="0.992579">
86
</page>
<bodyText confidence="0.933969">
of the utterances have been marked as erroneous.
</bodyText>
<subsectionHeader confidence="0.596471">
4.1.1 Setup
</subsectionHeader>
<bodyText confidence="0.999975214285714">
A 200-sentence held-out section of the corpus is
analysed by hand and a GenERRate error analysis
file containing 89 errors is compiled. The most fre-
quent errors are those involving a change in noun or
verb number or an article deletion. GenERRate then
applies this error analysis file to 440,930 BNC sen-
tences resulting in the same size set of synthetic ex-
amples (“new-ungram-BNC”). Another set of syn-
thetic sentences (“old-ungram-BNC”) is produced
from the same input using the error generation pro-
cedure used by Wagner et al. (2007; 2009). Table 1
shows examples from both sets.
Two classifiers are then trained, one on the orig-
inal BNC sentences and the old-ungram-BNC sen-
tences, and the other on the original BNC sentences
and the new-ungram-BNC sentences. Both classi-
fiers are tested on 4,095 sentences from the spo-
ken language corpus (excluding the held-out sec-
tion). 310 of these sentences are corrected, resulting
in a small set of grammatical test data. The classi-
fier used is the POS n-gram frequency classifier de-
scribed in Wagner et al. (2007).3 The features are
the frequencies of the least frequent n-grams (2–7)
in the input sentence. The BNC (excluding those
sentences that are used as training data) is used as
reference data to compute the frequencies. Learning
is carried out using the Weka implementation of the
J48 decision tree algorithm.4
</bodyText>
<sectionHeader confidence="0.787134" genericHeader="method">
4.1.2 Results
</sectionHeader>
<bodyText confidence="0.999869555555555">
The results of the experiment are displayed in Ta-
ble 2. The evaluation measures used are precision,
recall, total accuracy and accuracy on the grammat-
ical side of the test data. Recall is the same as accu-
racy on the ungrammatical side of the test data.
The results are encouraging. There is a signifi-
cant increase in accuracy when we train on the new-
ungram-BNC set instead of the old-ungram-BNC
set. This increase is on the ungrammatical side of
</bodyText>
<footnote confidence="0.980075666666667">
3Wagner et al. (2009) report accuracy figures in the range
55–70% for their various classifiers (when tested on synthetic
test data), but the best performance is obtained by combining
parser-output and n-gram POS frequency features using deci-
sion trees in a voting scheme.
4http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<bodyText confidence="0.999991647058824">
the test data, i.e. an increase in recall, demonstrat-
ing that by analysing a small set of data from our
test domain, we can automatically create more effec-
tive training data. This is useful in a scenario where
a small-to-medium-sized learner corpus is available
but which is not large enough to be split into a train-
ing/development/test set. These results seem to indi-
cate that reasonably useful training data can be cre-
ated with minimum effort. Of course, the accuracy is
still rather low but we suspect that some of this dif-
ference can be explained by domain effects — the
sentences in the training data are BNC written sen-
tences (or distorted versions of them) whereas the
sentences in the learner corpus are transcribed spo-
ken utterances. Re-running the experiments using
the spoken language section of the BNC as training
data might yield better results.
</bodyText>
<subsectionHeader confidence="0.999097">
4.2 A CLC-Inspired Corpus
</subsectionHeader>
<bodyText confidence="0.999979714285714">
We investigate to what extent it is possible to cre-
ate a large error corpus inspired by the CLC using
the current version of GenERRate. The CLC is a
30-million-word corpus of learner English collected
from University of Cambridge ESOL exam papers
at different levels. Approximately 50% of the CLC
has been annotated for errors and corrected.
</bodyText>
<subsectionHeader confidence="0.762745">
4.2.1 Setup
</subsectionHeader>
<bodyText confidence="0.9999682">
We attempt to use GenERRate to insert errors
into corrected CLC sentences. In order to do
this, we need to create a CLC-specific error anal-
ysis file. In contrast to the previous experiment,
we do this automatically by extracting erroneous
POS trigrams from the error-annotated CLC sen-
tences and encoding them as GenERRate errors.
This results in approximately 13,000 errors of the
following types: DeletionPOSWhereError, Inser-
tionPOSWhereError, MovePOSWhereError, Sub-
stWordConfusionError, SubstWordConfusionNew-
POSError, SubstSpecificWordConfusionError and
SubstWrongFormError. Frequencies are extracted,
and errors occurring only once are excluded.
Three classifiers are trained. The first is trained
on corrected CLC sentences (the grammatical sec-
tion of the training set) and original CLC sentences
(the ungrammatical section). The second classifier
is trained on corrected CLC sentences and the sen-
tences that are generated from the corrected CLC
</bodyText>
<page confidence="0.996429">
87
</page>
<table confidence="0.9337522">
Old-Ungram-BNC New-Ungram-BNC
Biogas production production is growing rapidly Biogas productions is growing rapidly
Emil as courteous and helpful Emil courteous and was helpful
I knows what makes you tick I know what make you tick
He did n’t bother to lift his eyes from the task hand He did n’t bother lift his eyes from the task at hand
</table>
<tableCaption confidence="0.997266">
Table 1: Examples from two synthetic BNC sets
</tableCaption>
<table confidence="0.998958333333333">
Training Data Precision Recall Accuracy Accuracy on Grammatical
BNC/old-ungram-BNC 95.5 37.0 39.8 76.8
BNC/new-ungram-BNC 94.9 51.6 52.4 63.2
</table>
<tableCaption confidence="0.999347">
Table 2: Spoken Language Learner Corpus Classification Experiment
</tableCaption>
<bodyText confidence="0.999642235294118">
sentences using GenERRate (we call these “faux-
CLC”). The third is trained on corrected CLC sen-
tences and a 50/50 combination of CLC and faux-
CLC sentences. In all experiments, the grammat-
ical section of the training data contains 438,150
sentences and the ungrammatical section 454,337.
The classifiers are tested on a held-out section of
the CLC containing 43,639 corrected CLC sen-
tences and 45,373 original CLC sentences. To train
the classifiers, the Mallet implementation of Naive
Bayes is used.5 The features are word unigrams
and bigrams, as well as part-of-speech unigrams, bi-
grams and trigrams. Andersen (2006) experimented
with various learning algorithms and, taking into ac-
count training time and performance, found Naive
Bayes to be optimal. The POS-tagging is carried out
by the RASP system (Briscoe and Carroll, 2002).
</bodyText>
<sectionHeader confidence="0.492028" genericHeader="method">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.999868733333333">
The results of the CLC classification experiment
are presented in Table 3. There is a 6.2% drop in
accuracy when we move from training on original
CLC sentences to artificially generated sentences.
This is somewhat disappointing since it means that
we have not completely succeeded in replicating the
CLC errors using GenERRate. Most of the accu-
racy drop is on the ungrammatical side, i.e. the cor-
rect/faux model classifies more incorrect CLC sen-
tences as correct than the correct/incorrect model.
This drop in accuracy occurs because some fre-
quently occurring error types are not included in the
error analysis file. One reason for the gap in cover-
age is the failure of the part-of-speech tagset to make
some important distinctions. The corrected CLC
</bodyText>
<footnote confidence="0.879233">
5http://mallet.cs.umass.edu/
</footnote>
<bodyText confidence="0.999921076923077">
sentences which were used to generate the faux-
CLC set were tagged with the CLAWS tagset, and
although more fine-grained than the Penn tagset, it
does not, for example, make a distinction between
mass and count nouns, a common source of error.
Another important reason for the drop in accuracy
are the recurrent spelling errors which occur in the
incorrect CLC test set but not in the faux-CLC test
set. It is promising, however, that much of the per-
formance degradation is recovered when a mixture
of the two types of ungrammatical training data is
used, suggesting that artificial data could be used to
augment naturally occurring training sets
</bodyText>
<sectionHeader confidence="0.659136" genericHeader="method">
5 Limitations of GenERRate
</sectionHeader>
<bodyText confidence="0.999835">
We present three issues that make the task of gener-
ating synthetic error data non-trivial.
</bodyText>
<subsectionHeader confidence="0.999471">
5.1 Sophistication of Input Format
</subsectionHeader>
<bodyText confidence="0.9999863125">
The experiments in §4 highlight coverage issues
with GenERRate, some of which are due to the sim-
plicity of the supported error types. When linguis-
tic context is supplied for deletion or insertion er-
rors, it takes the form of the POS of the words im-
mediately to the left and/or right of the target word.
Lee and Seneff (2008a) analysed preposition errors
made by Japanese learners of English and found that
a greater proportion of errors in argument preposi-
tional phrases (look at him) involved a deletion than
those in adjunct PPs (came at night). The only way
for such a distinction to be encoded in a GenERRate
error analysis file is to allow parsed input to be ac-
cepted. This brings with it the problem, however,
that parsers are less accurate than POS-taggers. An-
other possible improvement would be to make use
</bodyText>
<page confidence="0.996945">
88
</page>
<table confidence="0.9994556">
Training Data Precision Recall Accuracy Accuracy on Grammatical
Held-Out Test Data
Correct/Incorrect CLC 69.7 42.6 61.3 80.8
Correct/Faux CLC 62.0 30.7 55.1 80.5
Correct/Incorrect+Faux CLC 69.7 38.2 60.0 82.7
</table>
<tableCaption confidence="0.998156">
Table 3: CLC Classification Experiment
</tableCaption>
<bodyText confidence="0.891019">
of WordNet synsets in order to choose the new word
in substitution errors.
</bodyText>
<subsectionHeader confidence="0.999015">
5.2 Covert Errors
</subsectionHeader>
<bodyText confidence="0.999960527777778">
A covert error is an error that results in a syntac-
tically well-formed sentence with an interpretation
different from the intended one. Covert errors are a
natural phenomenon, occurring in real corpora. Lee
and Seneff (2008b) give the example I am preparing
for the exam which has been annotated as erroneous
because, given its context, it is clear that the per-
son meant to write I am prepared for the exam. The
problems lie in deciding what covert errors should
be handled by an error detection system and how to
create synthetic data which gets the balance right.
When to avoid: Covert errors can be produced
by GenERRate as a result of the sparse linguistic
context provided for an error in the error analysis
file. An inspection of the new-ungram-BNC set
shows that some error types are more likely to re-
sult in covert errors. An example is the SubstWrong-
FormError when it is used to change a noun from
singular to plural. This results in the sentence But
there was no sign of Benny’s father being changed
to the well-formed but more implausible But there
was no sign of Benny’s fathers. The next version of
GenERRate should include the option to change the
form of a word in a certain context.
When not to avoid: In the design of GenERRate,
particularly in the design of the SubstWrongFormEr-
ror type, the decision was made to exclude tense er-
rors because they are likely to result in covert er-
rors, e.g. She walked home → She walks home. But
in doing so we also avoid generating examples like
this one from the spoken language learner corpus:
When I was a high school student, Igo to bed at one
o’clock. These tense errors are common in L2 data
and their omission from the faux-CLC training set
is one of the reasons why the performance of this
model is inferior to the real-CLC model.
</bodyText>
<subsectionHeader confidence="0.991455">
5.3 More complex errors
</subsectionHeader>
<bodyText confidence="0.991178428571429">
The learner corpora contain some errors that are
corrected by applying more than one transforma-
tion. Some are handled by the SubstWrongFormEr-
ror type (I spend a long time to fish →I spend a long
time fishing) but some are not (She is one of reason
I became interested in English → She is one of the
reasons I became interested in English).
</bodyText>
<sectionHeader confidence="0.999177" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999998692307692">
We have presented GenERRate, a tool for automati-
cally introducing syntactic errors into sentences and
shown how it can be useful for creating synthetic
training data to be used in grammatical error detec-
tion research. Although we have focussed on the
binary classification task, we also intend to test Gen-
ERRate in targeted error detection. Another avenue
for future work is to explore whether GenERRate
could be of use in the automatic generation of lan-
guage test items (Chen et al., 2006, for example).
Our immediate aim is to produce a new version of
GenERRate which tackles some of the coverage is-
sues highlighted by our experiments.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999913">
This paper reports on research supported by the Uni-
versity of Cambridge ESOL Examinations. We are
very grateful to Cambridge University Press for giv-
ing us access to the Cambridge Learner Corpus and
to James Hunter from Gonzaga College for sup-
plying us with the spoken language learner corpus.
We thank Ted Briscoe, Josef van Genabith, Joachim
Wagner and the reviewers for their very helpful sug-
gestions.
</bodyText>
<page confidence="0.99968">
89
</page>
<sectionHeader confidence="0.990364" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774144329897">
Øistein E. Andersen. 2006. Grammatical error detection.
Master’s thesis, Cambridge University.
Øistein E. Andersen. 2007. Grammatical error detection
using corpora and supervised learning. In Ville Nurmi
and Dmitry Sustretov, editors, Proceedings of the 12th
Student Session of the European Summer School for
Logic, Language and Information, Dublin.
Johnny Bigert, Jonas Sj¨obergh, Ola Knutsson, and Mag-
nus Sahlgren. 2005. Unsupervised evaluation of
parser robustness. In Proceedings of the 6th CICling,
Mexico City.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of the 4th
LREC, Lisbon.
Ted Briscoe and John Carroll. 2002. Robust accurate
statistical annotation of general text. In Proceedings
of the 3rd LREC, Las Palmas.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL errors using phrasal SMT tech-
niques. In Proceedings of the 21st COLING and the
44th ACL, Sydney.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Chia-Yin Chen, Liou Hsien-Chin, and Jason S. Chang.
2006. Fast — an automatic generation system for
grammar tests. In Proceedings of the COLING/ACL
2006 Interactive Presentation Sessions, Sydney.
Rachele De Felice and Stephen G. Pulman. 2008.
A classifier-based approach to preposition and deter-
miner error correction in L2 English. In Proceedings
of the 22nd COLING, Manchester.
Jennifer Foster. 2007. Treebanks gone bad: Parser evalu-
ation and retraining using a treebank of ungrammatical
sentences. International Journal on Document Analy-
sis and Recognition, 10(3-4):129–145.
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitriy Belenko,
and Lucy Vanderwende. 2008. Using contextual
speller techniques and language modelling for ESL er-
ror correction. In Proceedings of the 3rd IJCNLP, Hy-
derabad.
Roger Garside, Geoffrey Leech, and Geoffrey Sampson,
editors. 1987. The Computational Analysis of En-
glish: a Corpus-Based Approach. Longman, London.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineering,
12(2):115–129.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
John Lee and Stephanie Seneff. 2008a. An analysis of
grammatical errors in non-native speech in English. In
Proceedings of the 2008 Spoken Language Technology
Workshop, Goa.
John Lee and Stephanie Seneff. 2008b. Correcting mis-
use of verb forms. In Proceedings of the 46th ACL,
Columbus.
Vanessa Metcalf and Detmar Meurers. 2006. Towards
a treatment of word order errors: When to use deep
processing – and when not to. Presentation at the NLP
in CALL Workshop, CALICO 2006.
Daisuke Okanohara and Jun’ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proceedings of the 45th ACL, Prague.
Beatrice Santorini. 1991. Part-of-speech tagging guide-
lines for the Penn Treebank project. Technical report,
University of Pennsylvania, Philadelphia, PA.
Jonas Sj¨obergh and Ola Knutsson. 2005. Faking errors to
avoid making errors. In Proceedings of RANLP 2005,
Borovets.
Noah A. Smith and Jason Eisner. 2005a. Contrastive
Estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd ACL, Ann Arbor.
Noah A. Smith and Jason Eisner. 2005b. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In Proceedings of the IJCAI Workshop on Gram-
matical Inference Applications, Edinburgh.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proceedings of the
45rd ACL, Prague.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proceedings of the 22nd COLING, Manchester.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2007. A comparative evaluation of deep and
shallow approaches to the automatic detection of com-
mon grammatical errors. In Proceedings of the joint
EMNLP/CoNLL, Prague.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal. Special Issue
on the 2008 Automatic Analysis of Learner Language
CALICO Workshop. To Appear.
</reference>
<page confidence="0.99864">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.362157">
<title confidence="0.999726">GenERRate: Generating Errors for Use in Grammatical Error Detection</title>
<author confidence="0.988062">Jennifer</author>
<affiliation confidence="0.991743333333333">National Centre for Language School of Dublin City University,</affiliation>
<email confidence="0.756589">jfoster@computing.dcu.ie</email>
<affiliation confidence="0.87773675">Øistein E. Computer University of United</affiliation>
<email confidence="0.828392">oa223@cam.ac.uk</email>
<abstract confidence="0.981858909090909">This paper explores the issue of automatically generated ungrammatical data and its use in error detection, with a focus on the task of classifying a sentence as grammatical or ungrammatical. We present an error generation tool called GenERRate and show how Gen- ERRate can be used to improve the performance of a classifier on learner data. We describe initial attempts to replicate Cambridge Learner Corpus errors using GenERRate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Øistein E Andersen</author>
</authors>
<title>Grammatical error detection. Master’s thesis,</title>
<date>2006</date>
<institution>Cambridge University.</institution>
<contexts>
<context position="3565" citStr="Andersen (2006" startWordPosition="547" endWordPosition="548">shing the grammatical from the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors. Distinguishing grammatical utterances from ungrammatical ones involves the use of a binary classifier or a grammaticality scor1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meur</context>
<context position="18806" citStr="Andersen (2006)" startWordPosition="2962" endWordPosition="2963">d and ill-formed sentences or, more precisely, to distinguish between sentences in learner corpora which have been annotated as erroneous and their corrected counterparts. In the first experiment we use GenERRate to create ungrammatical training data using information about error types gleaned from a subset of a corpus of transcribed spoken utterances produced by ESL learners in a classroom environment. The classifier is one of those described in Wagner et al. (2007). In the second experiment we try to generate a CLCinspired error corpus and we use one of the simplest classifiers described in Andersen (2006). Our aim is not to improve classification performance, but to test the GenERRate tool, to demonstrate how it can be used and to investigate differences between synthetic and naturally occurring datasets. 4.1 Experiments with a Spoken Language Learner Corpus Wagner et al. (2009) train various classifiers to distinguish between BNC sentences and artificially produced ungrammatical versions of BNC sentences (see §2). They report a significant drop in accuracy when they apply these classifiers to real learner data, including the sentences in a corpus of transcribed spoken utterances. The aim of t</context>
<context position="25483" citStr="Andersen (2006)" startWordPosition="4025" endWordPosition="4026">ntences using GenERRate (we call these “fauxCLC”). The third is trained on corrected CLC sentences and a 50/50 combination of CLC and fauxCLC sentences. In all experiments, the grammatical section of the training data contains 438,150 sentences and the ungrammatical section 454,337. The classifiers are tested on a held-out section of the CLC containing 43,639 corrected CLC sentences and 45,373 original CLC sentences. To train the classifiers, the Mallet implementation of Naive Bayes is used.5 The features are word unigrams and bigrams, as well as part-of-speech unigrams, bigrams and trigrams. Andersen (2006) experimented with various learning algorithms and, taking into account training time and performance, found Naive Bayes to be optimal. The POS-tagging is carried out by the RASP system (Briscoe and Carroll, 2002). 4.2.2 Results The results of the CLC classification experiment are presented in Table 3. There is a 6.2% drop in accuracy when we move from training on original CLC sentences to artificially generated sentences. This is somewhat disappointing since it means that we have not completely succeeded in replicating the CLC errors using GenERRate. Most of the accuracy drop is on the ungram</context>
</contexts>
<marker>Andersen, 2006</marker>
<rawString>Øistein E. Andersen. 2006. Grammatical error detection. Master’s thesis, Cambridge University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Øistein E Andersen</author>
</authors>
<title>Grammatical error detection using corpora and supervised learning.</title>
<date>2007</date>
<booktitle>In Ville Nurmi and Dmitry Sustretov, editors, Proceedings of the 12th Student Session of the European Summer School for Logic, Language and Information,</booktitle>
<location>Dublin.</location>
<marker>Andersen, 2007</marker>
<rawString>Øistein E. Andersen. 2007. Grammatical error detection using corpora and supervised learning. In Ville Nurmi and Dmitry Sustretov, editors, Proceedings of the 12th Student Session of the European Summer School for Logic, Language and Information, Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johnny Bigert</author>
<author>Jonas Sj¨obergh</author>
<author>Ola Knutsson</author>
<author>Magnus Sahlgren</author>
</authors>
<title>Unsupervised evaluation of parser robustness.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th CICling,</booktitle>
<location>Mexico City.</location>
<marker>Bigert, Sj¨obergh, Knutsson, Sahlgren, 2005</marker>
<rawString>Johnny Bigert, Jonas Sj¨obergh, Ola Knutsson, and Magnus Sahlgren. 2005. Unsupervised evaluation of parser robustness. In Proceedings of the 6th CICling, Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johnny Bigert</author>
</authors>
<title>Probabilistic detection of contextsensitive spelling errors.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th LREC,</booktitle>
<location>Lisbon.</location>
<contexts>
<context position="9991" citStr="Bigert (2004)" startWordPosition="1586" endWordPosition="1587">ng synthetic corpus contains errors that look like the kind of slips that would be made by native speakers (e.g. repeated adjacent words) as well as errors that resemble learner errors (e.g. missing articles). Wagner et al. (2009) report a drop in accuracy for their classification methods when applied to real learner texts as opposed to held-out synthetic test data, reinforcing the earlier point that artificial errors need to be tailored for the task at hand (we return to this in Section 4.1). Artificial error data has also proven useful in the automatic evaluation of error detection systems. Bigert (2004) describes how a tool called Missplel is used to generate context-sensitive spelling errors which are then used to evaluate a context-sensitive spelling error detection system. The performance of general-purpose NLP tools such as part-of-speech taggers and parsers in the face of noisy ungrammatical data has been automatically evaluated using artificial error data. Since the features of machinelearned error detectors are often part-of-speech ngrams or word–word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-ofspeech ta</context>
</contexts>
<marker>Bigert, 2004</marker>
<rawString>Johnny Bigert. 2004. Probabilistic detection of contextsensitive spelling errors. In Proceedings of the 4th LREC, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd LREC, Las</booktitle>
<location>Palmas.</location>
<contexts>
<context position="25696" citStr="Briscoe and Carroll, 2002" startWordPosition="4056" endWordPosition="4059">e training data contains 438,150 sentences and the ungrammatical section 454,337. The classifiers are tested on a held-out section of the CLC containing 43,639 corrected CLC sentences and 45,373 original CLC sentences. To train the classifiers, the Mallet implementation of Naive Bayes is used.5 The features are word unigrams and bigrams, as well as part-of-speech unigrams, bigrams and trigrams. Andersen (2006) experimented with various learning algorithms and, taking into account training time and performance, found Naive Bayes to be optimal. The POS-tagging is carried out by the RASP system (Briscoe and Carroll, 2002). 4.2.2 Results The results of the CLC classification experiment are presented in Table 3. There is a 6.2% drop in accuracy when we move from training on original CLC sentences to artificially generated sentences. This is somewhat disappointing since it means that we have not completely succeeded in replicating the CLC errors using GenERRate. Most of the accuracy drop is on the ungrammatical side, i.e. the correct/faux model classifies more incorrect CLC sentences as correct than the correct/incorrect model. This drop in accuracy occurs because some frequently occurring error types are not inc</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Ted Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd LREC, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st COLING and the 44th ACL,</booktitle>
<location>Sydney.</location>
<contexts>
<context position="4132" citStr="Brockett et al., 2006" startWordPosition="636" endWordPosition="639">onal Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If the parse trees resemble the “disturbed” trees that st</context>
<context position="7974" citStr="Brockett et al. (2006)" startWordPosition="1266" endWordPosition="1269"> used Artificial errors have been employed previously in targeted error detection. Sj¨obergh and Knutsson (2005) introduce split compound errors and word order errors into Swedish texts and use the resulting artificial data to train their error detection system. These two particular error types are chosen because they are frequent errors amongst non-native Swedish speakers whose first language does not contain compounds or has a fixed word order. They compare the resulting system to three Swedish grammar checkers, and find that their system has higher recall at the expense of lower precision. Brockett et al. (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction. Lee and Seneff (2008b) automatically introduce verb form errors (subject– verb agreement errors, complementation errors and errors in a main verb after an auxiliary) into well83 formed text, parse the resulting text and examine the parse trees produced. Both Okanohara and Tsujii (2007) and Wagner et al. (2007) attempt to learn a model which discriminates between grammatical and ungrammatical sentences, and both u</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Chris Brockett, William B. Dolan, and Michael Gamon. 2006. Correcting ESL errors using phrasal SMT techniques. In Proceedings of the 21st COLING and the 44th ACL, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User reference guide for the British National Corpus.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="8697" citStr="Burnard, 2000" startWordPosition="1380" endWordPosition="1381">parallel corpus to train a phrasal SMT system to perform error correction. Lee and Seneff (2008b) automatically introduce verb form errors (subject– verb agreement errors, complementation errors and errors in a main verb after an auxiliary) into well83 formed text, parse the resulting text and examine the parse trees produced. Both Okanohara and Tsujii (2007) and Wagner et al. (2007) attempt to learn a model which discriminates between grammatical and ungrammatical sentences, and both use synthetic negative data which is obtained by distorting sentences from the British National Corpus (BNC) (Burnard, 2000). The methods used to distort the BNC sentences are, however, quite different. Okanohara and Tsujii (2007) generate ill-formed sentences by sampling a probabilistic language model and end up with “pseudo-negative” examples which resemble machine translation output more than they do learner texts. Indeed, machine translation is one of the applications of their resulting discriminative language model. Wagner et al. (2007) introduce grammatical errors of the following four types into BNC sentences: contextsensitive spelling errors, agreement errors, errors involving a missing word and errors invo</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. User reference guide for the British National Corpus. Technical report, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chia-Yin Chen</author>
<author>Liou Hsien-Chin</author>
<author>Jason S Chang</author>
</authors>
<title>Fast — an automatic generation system for grammar tests.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<location>Sydney.</location>
<marker>Chen, Hsien-Chin, Chang, 2006</marker>
<rawString>Chia-Yin Chen, Liou Hsien-Chin, and Jason S. Chang. 2006. Fast — an automatic generation system for grammar tests. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
<author>Stephen G Pulman</author>
</authors>
<title>A classifier-based approach to preposition and determiner error correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd COLING,</booktitle>
<location>Manchester.</location>
<marker>De Felice, Pulman, 2008</marker>
<rawString>Rachele De Felice and Stephen G. Pulman. 2008. A classifier-based approach to preposition and determiner error correction in L2 English. In Proceedings of the 22nd COLING, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences.</title>
<date>2007</date>
<journal>International Journal on Document Analysis and Recognition,</journal>
<pages>10--3</pages>
<contexts>
<context position="10907" citStr="Foster (2007)" startWordPosition="1724" endWordPosition="1726">a has been automatically evaluated using artificial error data. Since the features of machinelearned error detectors are often part-of-speech ngrams or word–word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-ofspeech taggers and parsers react to particular grammatical errors. Bigert et al. (2005) introduce artificial context-sensitive spelling errors into error-free Swedish text and then evaluate parsers and a partof-speech tagger on this text using their performance on the error-free text as a reference. Similarly, Foster (2007) investigates the effect of common English grammatical errors on two widely-used statistical parsers using distorted treebank trees as references. The procedure used by Wagner et al. (2007; 2009) is used to introduce errors into the treebank sentences. Finally, negative evidence in the form of automatically distorted sentences has been used in unsupervised learning. Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. Since the aim of this work is not to detect grammatical errors, there is no requirement </context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation and retraining using a treebank of ungrammatical sentences. International Journal on Document Analysis and Recognition, 10(3-4):129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Jianfeng Gao</author>
<author>Chris Brockett</author>
<author>Alexandre Klementiev</author>
<author>William B Dolan</author>
<author>Dmitriy Belenko</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modelling for ESL error correction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd IJCNLP,</booktitle>
<location>Hyderabad.</location>
<contexts>
<context position="3951" citStr="Gamon et al., 2008" startWordPosition="609" endWordPosition="612"> Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be </context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Michael Gamon, Jianfeng Gao, Chris Brockett, Alexandre Klementiev, William B. Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using contextual speller techniques and language modelling for ESL error correction. In Proceedings of the 3rd IJCNLP, Hyderabad.</rawString>
</citation>
<citation valid="true">
<title>The Computational Analysis of English: a Corpus-Based Approach.</title>
<date>1987</date>
<editor>Roger Garside, Geoffrey Leech, and Geoffrey Sampson, editors.</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<marker>1987</marker>
<rawString>Roger Garside, Geoffrey Leech, and Geoffrey Sampson, editors. 1987. The Computational Analysis of English: a Corpus-Based Approach. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="3902" citStr="Han et al., 2006" startWordPosition="600" endWordPosition="603">.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class al</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl James</author>
</authors>
<title>Errors in Language Learning and Use: Exploring Error Analysis.</title>
<date>1998</date>
<publisher>Addison Wesley Longman.</publisher>
<contexts>
<context position="12839" citStr="James, 1998" startWordPosition="2041" endWordPosition="2042">lysis file consisting of a list of errors and produces an errortagged corpus of syntactically ill-formed sentences. The sentences in the input corpus are assumed to be grammatically well-formed. GenERRate is implemented in Java and will be made available to download for use by other researchers.2 3.1 Supported Error Types Error types are defined in terms of their corrections, that is, in terms of the operations (insert, delete, substitute and move) that are applied to a well-formed sentence to make it ill-formed. As well as being a popular classification scheme in the field of error analysis (James, 1998), it has the advantage of 2http://www.computing.dcu.ie/˜jfoster/ resources/generrate.html 84 being theory-neutral. This is important in this context since it is hoped that GenERRate will be used to create negative evidence of various types, be it L2-like grammatical errors, native speaker slips or more random syntactic noise. It is hoped that GenERRate will be easy to use for anyone working in linguistics, applied linguistics, language teaching or computational linguistics. The inheritance hierarchy in Fig. 1 shows the error types that are supported by GenERRate. We briefly describe each error</context>
</contexts>
<marker>James, 1998</marker>
<rawString>Carl James. 1998. Errors in Language Learning and Use: Exploring Error Analysis. Addison Wesley Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>An analysis of grammatical errors in non-native speech in English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Spoken Language Technology Workshop,</booktitle>
<location>Goa.</location>
<contexts>
<context position="4078" citStr="Lee and Seneff, 2008" startWordPosition="629" endWordPosition="632">olorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If </context>
<context position="8178" citStr="Lee and Seneff (2008" startWordPosition="1299" endWordPosition="1302">g artificial data to train their error detection system. These two particular error types are chosen because they are frequent errors amongst non-native Swedish speakers whose first language does not contain compounds or has a fixed word order. They compare the resulting system to three Swedish grammar checkers, and find that their system has higher recall at the expense of lower precision. Brockett et al. (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction. Lee and Seneff (2008b) automatically introduce verb form errors (subject– verb agreement errors, complementation errors and errors in a main verb after an auxiliary) into well83 formed text, parse the resulting text and examine the parse trees produced. Both Okanohara and Tsujii (2007) and Wagner et al. (2007) attempt to learn a model which discriminates between grammatical and ungrammatical sentences, and both use synthetic negative data which is obtained by distorting sentences from the British National Corpus (BNC) (Burnard, 2000). The methods used to distort the BNC sentences are, however, quite different. Ok</context>
<context position="27615" citStr="Lee and Seneff (2008" startWordPosition="4375" endWordPosition="4378">he two types of ungrammatical training data is used, suggesting that artificial data could be used to augment naturally occurring training sets 5 Limitations of GenERRate We present three issues that make the task of generating synthetic error data non-trivial. 5.1 Sophistication of Input Format The experiments in §4 highlight coverage issues with GenERRate, some of which are due to the simplicity of the supported error types. When linguistic context is supplied for deletion or insertion errors, it takes the form of the POS of the words immediately to the left and/or right of the target word. Lee and Seneff (2008a) analysed preposition errors made by Japanese learners of English and found that a greater proportion of errors in argument prepositional phrases (look at him) involved a deletion than those in adjunct PPs (came at night). The only way for such a distinction to be encoded in a GenERRate error analysis file is to allow parsed input to be accepted. This brings with it the problem, however, that parsers are less accurate than POS-taggers. Another possible improvement would be to make use 88 Training Data Precision Recall Accuracy Accuracy on Grammatical Held-Out Test Data Correct/Incorrect CLC </context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008a. An analysis of grammatical errors in non-native speech in English. In Proceedings of the 2008 Spoken Language Technology Workshop, Goa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Stephanie Seneff</author>
</authors>
<title>Correcting misuse of verb forms.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th ACL,</booktitle>
<location>Columbus.</location>
<contexts>
<context position="4078" citStr="Lee and Seneff, 2008" startWordPosition="629" endWordPosition="632">olorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If </context>
<context position="8178" citStr="Lee and Seneff (2008" startWordPosition="1299" endWordPosition="1302">g artificial data to train their error detection system. These two particular error types are chosen because they are frequent errors amongst non-native Swedish speakers whose first language does not contain compounds or has a fixed word order. They compare the resulting system to three Swedish grammar checkers, and find that their system has higher recall at the expense of lower precision. Brockett et al. (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction. Lee and Seneff (2008b) automatically introduce verb form errors (subject– verb agreement errors, complementation errors and errors in a main verb after an auxiliary) into well83 formed text, parse the resulting text and examine the parse trees produced. Both Okanohara and Tsujii (2007) and Wagner et al. (2007) attempt to learn a model which discriminates between grammatical and ungrammatical sentences, and both use synthetic negative data which is obtained by distorting sentences from the British National Corpus (BNC) (Burnard, 2000). The methods used to distort the BNC sentences are, however, quite different. Ok</context>
<context position="27615" citStr="Lee and Seneff (2008" startWordPosition="4375" endWordPosition="4378">he two types of ungrammatical training data is used, suggesting that artificial data could be used to augment naturally occurring training sets 5 Limitations of GenERRate We present three issues that make the task of generating synthetic error data non-trivial. 5.1 Sophistication of Input Format The experiments in §4 highlight coverage issues with GenERRate, some of which are due to the simplicity of the supported error types. When linguistic context is supplied for deletion or insertion errors, it takes the form of the POS of the words immediately to the left and/or right of the target word. Lee and Seneff (2008a) analysed preposition errors made by Japanese learners of English and found that a greater proportion of errors in argument prepositional phrases (look at him) involved a deletion than those in adjunct PPs (came at night). The only way for such a distinction to be encoded in a GenERRate error analysis file is to allow parsed input to be accepted. This brings with it the problem, however, that parsers are less accurate than POS-taggers. Another possible improvement would be to make use 88 Training Data Precision Recall Accuracy Accuracy on Grammatical Held-Out Test Data Correct/Incorrect CLC </context>
</contexts>
<marker>Lee, Seneff, 2008</marker>
<rawString>John Lee and Stephanie Seneff. 2008b. Correcting misuse of verb forms. In Proceedings of the 46th ACL, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Metcalf</author>
<author>Detmar Meurers</author>
</authors>
<title>Towards a treatment of word order errors: When to use deep processing – and when not to. Presentation at the NLP in CALL Workshop,</title>
<date>2006</date>
<location>CALICO</location>
<contexts>
<context position="4175" citStr="Metcalf and Meurers, 2006" startWordPosition="643" endWordPosition="646">e Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and their parse trees are examined. If the parse trees resemble the “disturbed” trees that statistical parsers typically produce when an</context>
</contexts>
<marker>Metcalf, Meurers, 2006</marker>
<rawString>Vanessa Metcalf and Detmar Meurers. 2006. Towards a treatment of word order errors: When to use deep processing – and when not to. Presentation at the NLP in CALL Workshop, CALICO 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudo-negative samples.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="3601" citStr="Okanohara and Tsujii (2007)" startWordPosition="550" endWordPosition="553">rom the ungrammatical at the sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors. Distinguishing grammatical utterances from ungrammatical ones involves the use of a binary classifier or a grammaticality scor1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a patter</context>
<context position="8444" citStr="Okanohara and Tsujii (2007)" startWordPosition="1339" endWordPosition="1342">he resulting system to three Swedish grammar checkers, and find that their system has higher recall at the expense of lower precision. Brockett et al. (2006) introduce errors involving mass/count noun confusions into English newswire text and then use the resulting parallel corpus to train a phrasal SMT system to perform error correction. Lee and Seneff (2008b) automatically introduce verb form errors (subject– verb agreement errors, complementation errors and errors in a main verb after an auxiliary) into well83 formed text, parse the resulting text and examine the parse trees produced. Both Okanohara and Tsujii (2007) and Wagner et al. (2007) attempt to learn a model which discriminates between grammatical and ungrammatical sentences, and both use synthetic negative data which is obtained by distorting sentences from the British National Corpus (BNC) (Burnard, 2000). The methods used to distort the BNC sentences are, however, quite different. Okanohara and Tsujii (2007) generate ill-formed sentences by sampling a probabilistic language model and end up with “pseudo-negative” examples which resemble machine translation output more than they do learner texts. Indeed, machine translation is one of the applica</context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudo-negative samples. In Proceedings of the 45th ACL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project.</title>
<date>1991</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16174" citStr="Santorini, 1991" startWordPosition="2548" endWordPosition="2549">ng changes are currently supported: noun number (e.g. word/words), verb number (write/writes), verb form (writing/written), adjective form (big/bigger) and adjective/adverb (quick/quickly). Note that this is the only error type which is language-specific. At the moment, only English is supported. 3.2 Input Corpus The corpus that is supplied as input to GenERRate must be split into sentences. It does not have to be part-of-speech tagged, but it will not be possible to generate many of the errors if it is not. GenERRate has been tested using two part-of-speech tagsets, the Penn Treebank tagset (Santorini, 1991) and the CLAWS tagset (Garside et al., 1987). 3.3 Error Analysis File The error analysis file specifies the errors that GenERRate should attempt to insert into the sentences in the input corpus. A toy example with the Penn tagset is shown in Fig. 2. The first line is an instance of a SubstSpecificWordConfusion error. The second 85 Error Move MovePOS MovePOSWhere Subst SubstWordConfusion SubstWrongForm Deletion DeletionPOS DeletionPOSWhere Insertion InsertionFromFileOrSentence InsertionPOS InsertionPOSWhere SubstWordConfusionNewPOS SubstSpecificWordConfusion Figure 1: GenERRate Error Types and </context>
</contexts>
<marker>Santorini, 1991</marker>
<rawString>Beatrice Santorini. 1991. Part-of-speech tagging guidelines for the Penn Treebank project. Technical report, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Sj¨obergh</author>
<author>Ola Knutsson</author>
</authors>
<title>Faking errors to avoid making errors.</title>
<date>2005</date>
<booktitle>In Proceedings of RANLP 2005,</booktitle>
<location>Borovets.</location>
<marker>Sj¨obergh, Knutsson, 2005</marker>
<rawString>Jonas Sj¨obergh and Ola Knutsson. 2005. Faking errors to avoid making errors. In Proceedings of RANLP 2005, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive Estimation: Training log-linear models on unlabeled data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context position="11298" citStr="Smith and Eisner (2005" startWordPosition="1782" endWordPosition="1785">uce artificial context-sensitive spelling errors into error-free Swedish text and then evaluate parsers and a partof-speech tagger on this text using their performance on the error-free text as a reference. Similarly, Foster (2007) investigates the effect of common English grammatical errors on two widely-used statistical parsers using distorted treebank trees as references. The procedure used by Wagner et al. (2007; 2009) is used to introduce errors into the treebank sentences. Finally, negative evidence in the form of automatically distorted sentences has been used in unsupervised learning. Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. Since the aim of this work is not to detect grammatical errors, there is no requirement to generate the kind of negative evidence that might actually be produced by either native or non-native speakers of a language. The negative examples are used to guide the unsupervised learning of a part-of-speech tagger and a dependency grammar. We can conclude from this survey that synthetic error data is useful in a variety of NLP applications, including error detection and evaluation</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005a. Contrastive Estimation: Training log-linear models on unlabeled data. In Proceedings of the 43rd ACL, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Guiding unsupervised grammar induction using contrastive estimation.</title>
<date>2005</date>
<booktitle>In Proceedings of the IJCAI Workshop on Grammatical Inference Applications,</booktitle>
<location>Edinburgh.</location>
<contexts>
<context position="11298" citStr="Smith and Eisner (2005" startWordPosition="1782" endWordPosition="1785">uce artificial context-sensitive spelling errors into error-free Swedish text and then evaluate parsers and a partof-speech tagger on this text using their performance on the error-free text as a reference. Similarly, Foster (2007) investigates the effect of common English grammatical errors on two widely-used statistical parsers using distorted treebank trees as references. The procedure used by Wagner et al. (2007; 2009) is used to introduce errors into the treebank sentences. Finally, negative evidence in the form of automatically distorted sentences has been used in unsupervised learning. Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. Since the aim of this work is not to detect grammatical errors, there is no requirement to generate the kind of negative evidence that might actually be produced by either native or non-native speakers of a language. The negative examples are used to guide the unsupervised learning of a part-of-speech tagger and a dependency grammar. We can conclude from this survey that synthetic error data is useful in a variety of NLP applications, including error detection and evaluation</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005b. Guiding unsupervised grammar induction using contrastive estimation. In Proceedings of the IJCAI Workshop on Grammatical Inference Applications, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
<author>Xiaohua Liu</author>
<author>Gao Cong</author>
<author>Ming Zhou</author>
<author>Zhongyang Xiong</author>
<author>John Lee</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Detecting erroneous sentences using automatically mined sequential patterns.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45rd ACL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="3620" citStr="Sun et al. (2007)" startWordPosition="554" endWordPosition="557">sentence level or more local targeted error detection, involving the identification, and possibly also correction, of particular types of errors. Distinguishing grammatical utterances from ungrammatical ones involves the use of a binary classifier or a grammaticality scor1http://www.cambridge.org/elt/corpus/ learner_corpus2.htm Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of we</context>
</contexts>
<marker>Sun, Liu, Cong, Zhou, Xiong, Lee, Lin, 2007</marker>
<rawString>Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou, Zhongyang Xiong, John Lee, and Chin-Yew Lin. 2007. Detecting erroneous sentences using automatically mined sequential patterns. In Proceedings of the 45rd ACL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd COLING,</booktitle>
<location>Manchester.</location>
<contexts>
<context position="4044" citStr="Tetreault and Chodorow, 2008" startWordPosition="623" endWordPosition="626">ional Applications, pages 82–90, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ing model. Examples are Andersen (2006; 2007), Okanohara and Tsujii (2007), Sun et al. (2007) and Wagner et al. (2007). In targeted error detection, the focus is on identifying the common errors made either by language learners or native speakers (depending on the application). For ESL applications, this includes the detection of errors involving articles (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008), prepositions (De Felice and Pulman, 2008; Gamon et al., 2008; Tetreault and Chodorow, 2008), verb forms (Lee and Seneff, 2008b), mass/count noun confusions (Brockett et al., 2006) and word order (Metcalf and Meurers, 2006). The presence of a pattern in a corpus of wellformed language is positive evidence that the pattern is well-formed. The presence of a pattern in an corpus of ill-formed language is negative evidence that the pattern is erroneous. Discriminative techniques usually lead to more accurate systems than those based on one class alone. The use of the two types of evidence can be seen at work in the system described by Lee and Seneff (2008b): Verb phrases are parsed and t</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Joel R. Tetreault and Martin Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd COLING, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors.</title>
<date>2007</date>
<booktitle>In Proceedings of the joint EMNLP/CoNLL,</booktitle>
<location>Prague.</location>
<marker>Wagner, Foster, van Genabith, 2007</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2007. A comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors. In Proceedings of the joint EMNLP/CoNLL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Jennifer Foster</author>
<author>Josef van Genabith</author>
</authors>
<title>Judging grammaticality: Experiments in sentence classification.</title>
<date>2009</date>
<journal>CALICO Journal. Special Issue on the</journal>
<note>To Appear.</note>
<marker>Wagner, Foster, van Genabith, 2009</marker>
<rawString>Joachim Wagner, Jennifer Foster, and Josef van Genabith. 2009. Judging grammaticality: Experiments in sentence classification. CALICO Journal. Special Issue on the 2008 Automatic Analysis of Learner Language CALICO Workshop. To Appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>