<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.271814">
<title confidence="0.97018">
NewsViz:
Emotional Visualization of News Stories
</title>
<author confidence="0.999598">
Eva Hanser, Paul Mc Kevitt, Tom Lunney and Joan Condell
</author>
<affiliation confidence="0.997998333333333">
School of Computing &amp; Intelligent Systems
Faculty of Computing &amp; Engineering
University of Ulster, Magee
</affiliation>
<address confidence="0.931799">
Derry/Londonderry, BT48 7JL
Northern Ireland
</address>
<email confidence="0.9916485">
hanser-e@email.ulster.ac.uk,
{p.mckevitt, tf.lunney, j.condell}@ulster.ac.uk
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807555555555">
The NewsViz system aims to enhance news
reading experiences by integrating 30 seconds
long Flash-animations into news article web
pages depicting their content and emotional
aspects. NewsViz interprets football match
news texts automatically and creates abstract
2D visualizations. The user interface en-
ables animators to further refine the anima-
tions. Here, we focus on the emotion extrac-
tion component of NewsViz which facilitates
subtle background visualization. NewsViz de-
tects moods from news reports. The origi-
nal text is part-of-speech tagged and adjec-
tives and/or nouns, the word types convey-
ing most emotional meaning, are filtered out
and labeled with an emotion and intensity
value. Subsequently reoccurring emotions are
joined into longer lasting moods and matched
with appropriate animation presets. Differ-
ent linguistic analysis methods were tested on
NewsViz: word-by-word, sentence-based and
minimum threshold summarization, to find a
minimum number of occurrences of an emo-
tion in forming a valid mood. NewsViz proved
to be viable for the fixed domain of football
news, grasping the overall moods and some
more detailed emotions precisely. NewsViz
offers an efficient technique to cater for the
production of a large number of daily updated
news stories. NewsViz bypasses the lack of
information for background or environment
depiction encountered in similar applications.
Further development may refine the detection
of emotion shifts through summarization with
the full implementation of football and com-
mon linguistic knowledge.
</bodyText>
<page confidence="0.991879">
125
</page>
<sectionHeader confidence="0.999318" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99243861728395">
News reports are regarded as objective facts, com-
monly delivered in an objective, unbiased manner
and represented in a neutral and formal format: typ-
ically a static headline, a summarizing paragraph
with one image and eventually the body text with
one to three more images. Even though reporters
find the content of news stories worth mentioning for
emotional reasons and the content often affects read-
ers emotionally, story brevity, scarce background in-
formation and poor combination of visual and verbal
information hinders learning and feeling by view-
ers. In order to reach the audience emotionally, to
educate and to entertain, emphasis on visual ele-
ments is important as they tend to be more memo-
rable than verbal ones. The emphasis of NewsViz
lies on expression, impacting on the reader’s under-
standing of the article and making it more memo-
rable. The software prototype, NewsViz, automat-
ically creates animations from news articles. Ab-
stract design elements show emotions conveyed in
the stories. The main objective of NewViz remains
information provision and thus our focus is emotion
extraction which is universally applicable and with-
out opinion bias. NewsViz is an efficient software
tool for designers to be able to build daily updated
animations. Input for NewsViz is natural language
text. Multimodal systems automatically mapping
text to visuals face challenges in interpreting human
language which is variable, ambiguous, imprecise
and relies on the communicative partners possess-
ing common knowledge. Enabling a machine to un-
derstand a natural language text involves feeding the
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 125–130,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
machine with grammatical structures, e.g. part-of- tracts information from soccer match reports, an-
speech, semantic relations, e.g. emotion value and notates relevant expressions (e.g. players, teams,
intensity, and visual descriptions, e.g. colors and goals.) and generates knowledge base entities. The
motion direction, to match suitable graphics. collected football knowledge can set preconditions
2 Background and Related Research and context to consequently evaluate current events
Text-to-visual mapping relates to the areas of natu- and assign appropriate emotions. The MoodNews
ral language processing (NLP) and multimodal sto- website (Mitchell, 2005) demonstrates a very sim-
rytelling which attempt to enable computers to in- ple linguistic method to distinguish positive, neg-
terpret and generate natural human language and ative and neutral content in BBC news headlines.
mental images. Text-to-visual mapping starts with It effectively ranks them on a color scale between
linguistic analysis of the text. Despite variability, good to bad. The three kinds of emotions are ap-
ambiguity and imprecision, syntactic analysis tools pointed through keyword scoring based on a small
achieve mostly reliable results, such as trainable vocabulary of 160 words and phrases. The Emo-
part-of-speech tagger software tools which identify tion Sensitive News Agent (ESNA) (Shaikh et al.,
parts of speech with 97% accuracy. For example, 2007) chategorizes news stories from different RSS
Qtag (Mason, 2003) attaches a tag to each word la- sources into eight emotion categories according to
beling it as noun, verb, adjective or other. their emotional content, determined through a cog-
Semantic interpretation and actual understanding nitive evaluation and user preferences.
of the meaning of a text is more difficult, because it Automated story visualization systems deliver
depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in
monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static
structured, related through logical rules and entered 3D images from written descriptions. Additionally,
into databases before computational text interpreta- automated camera and character animation, inter-
tion is possible. WordNet (Miller, 1995) determines action and speech synthesis is realized in CONFU-
semantic relations between words and is an extended CIUS (Ma, 2006). ScriptViz (Liu and Leung, 2006)
dictionary specifying word relations such as simi- renders 3D scenes from NL screenplays immedi-
larity, part-of relations, hierarchy or manner. Story ately during the writing process, extracting verbs
segmentation is performed by e.g. SeLeCT (Stokes, and adverbs to interpret events and states in sen-
2003), an example application based on semantic tences. The Unseen Video (Scheibel and Wein-
analysis to find story or subtopic changes within a rother, 2005), is a good example of abstract mood
text. Groups of semantically related words called visualization. Local weather data is automatically
cohesive ‘lexical chains’ are extracted from a text. retrieved from news websites and influences the look
They are determined through WordNet’s seman- and feel of the Flash animation through shapes, col-
tic relations and additionally through statistically ors and images. The Story Picturing Engine (Joshi
acquired co-occurrences (e.g. Diego Maradonna, et al., 2004) visualizes texts selecting and matching
Hand of God). Their starting and end points indi- pictures and their annotations from image databases.
cate topical unit boundaries. The work discussed here demonstrates that suffi-
Sensing emotions from multimodal input has cient subsets of the English language can be mapped
mainly been investigated with the objective of de- to computer understandable language for the visual-
veloping human-like agents. The football commen- ization of stories.
tary system, Byrne (Binsted and Luke, 1999), in- 3 The NewsViz System
cludes a commentator with emotions influenced by NewsViz takes online news articles as input and out-
his personality and intentions. SOCCER (Retz- puts animations reflecting the content of these news
Schmidt, 1988) analyses football scenes visually in stories. NewViz consists of three main components:
order to simultaneously add linguistic descriptions the linguistic analysis, the animation composer and
of the events. SOBA (Buitelaar et al., 2006) ex- an interface for editing text and animations (Figure
126
</bodyText>
<figureCaption confidence="0.998864">
Figure 1: NewsViz System Architecture.
</figureCaption>
<bodyText confidence="0.999819909090909">
1). The linguistic component constructs three ele-
ments of the animation in different processes. The
emotion extraction tool creates atmospheric back-
ground visuals, the action visualizer depicts people,
objects and their actions and the audio creator se-
lects music and sound effects. The composer syn-
chronizes the different outputs. Here, we focus on
the emotion extraction component (Figure 2) devel-
oped in Flash MX and Photoshop. Emotional as-
pects within the news story are identified and linked
to appropriate presets of background animations.
</bodyText>
<subsectionHeader confidence="0.999546">
3.1 Emotion Extraction
</subsectionHeader>
<bodyText confidence="0.9998978">
The first step in processing the text is to tag parts
of speech for all words. The part-of-speech tagger,
Qtag (Mason, 2003), attaches tags to nouns, verbs,
adjectives and other parts of speech. The tagged text
is sent on to the adjective and noun detector. These
</bodyText>
<figureCaption confidence="0.99561">
Figure 2: Emotion Extraction Component.
</figureCaption>
<bodyText confidence="0.999816066666667">
two types of words are selected for further process-
ing because they are most central to conveying emo-
tional meaning and sufficient for the visualisation of
the emotional content. Nouns and adjectives are the
parts of speech which represent the highest num-
ber of affective words as found in WordNet-Affect
(Strapparava and Valitutti, 2004). Verbs and adverbs
will be addressed in future work to increase sensi-
tivity and precision, but their impact on the resulting
animations may not be as significant. Next, the emo-
tion word selector checks the adjectives and nouns
in the emotion dictionary and attaches emotion tags
indicating their kind of emotion and intensity. The
dictionary holds manually created emotion-indices
and default intensity values of all affective words.
</bodyText>
<figureCaption confidence="0.978415">
Figure 3: Animations for Sadness (blue), Boredom (green), Tension (red) and Happiness (yellow).
</figureCaption>
<page confidence="0.801137">
127
</page>
<bodyText confidence="0.99993247826087">
Four emotions have been found relevant in relation
to football matches - happiness, sadness, tension and
boredom. Words with a neutral emotion index do
not describe football relevant emotions. To achieve
a coherent course of emotion and animation, neutral
phrases are replaced by the previous mood with de-
creasing intensity. The list of emotion tagged words
is handed to the emotion summarizer. During the
summarization process subsequent emotions of the
same type are combined to form one longer-lasting
mood. Each mood is labeled with its type, average
intensity and display duration. With the ‘word-by-
word’ summarization method mood boundaries ap-
pear as soon as the emotion type of the next word
differs. In order to reduce error and excessive mood
swings, the minimum threshold method sets a mini-
mum number of words required to represent a mood.
Alternatively, the sentence-based method assumes
that one sentence conveys one idea and consequently
one emotion. Hence, it calculates an average emo-
tion for each sentence, before combining identical
emotions. A chronological list of mood chunks is
created.
</bodyText>
<subsectionHeader confidence="0.999616">
3.2 Animation Construction
</subsectionHeader>
<bodyText confidence="0.999997785714286">
The animation selection component loads the in-
dividual animation elements from the graphics
database and combines them in a 30 seconds long
animation. The graphics database contains prefab-
ricated graphics sorted by an emotion index which
are combined and adjusted according to mood in-
tensities. Based on the weighted mood list, the emo-
tion sequence order, the type of graphic element, its
display duration, and the background color are de-
termined. The intensity value specifies the element
size and the number of objects loaded. An emo-
tion change causes the current animation elements
to fade out and to load different elements. Anima-
tion examples are shown in Figure 3.
</bodyText>
<subsectionHeader confidence="0.99711">
3.3 User Interface
</subsectionHeader>
<bodyText confidence="0.999967">
NewsViz provides users with options to load or type
news stories into the text editor. The options menu
offers different emotion extraction and mood sum-
marization methods. By pressing the ‘run’ button
the visualization can be watched in the preview win-
dow. The text processing runs ‘on the fly’ in the
background. If the user is satisfied they can save
the animation. If the user prefers to alter the anima-
tion manually, they have the option to edit the orig-
inal text or the animation elements frame by frame.
Figure 4 shows the user interface with animation
player. The final animations are integrated at the top
of the news article’s internet page (Figure 5).
</bodyText>
<figureCaption confidence="0.999981">
Figure 4: NewsViz User Interface.
Figure 5: Animation Integrated into Website.
</figureCaption>
<sectionHeader confidence="0.972451" genericHeader="introduction">
4 Evaluation and Testing
</sectionHeader>
<bodyText confidence="0.999388333333333">
NewsViz was tested on a set of four news articles
related to the same news domain - football match
reports. The articles were taken from BBC and
FIFA online describing the same two World Cup
2006 matches. The three different emotion extrac-
tion methods, word-by-word, sentence-based and
</bodyText>
<page confidence="0.996844">
128
</page>
<figureCaption confidence="0.999573">
Figure 6: Results Analysis of all Test Texts.
</figureCaption>
<bodyText confidence="0.999951326923077">
threshold were run on these news stories with vary-
ing word types or word type combinations. The out-
put of NewsViz is evaluated against two forms of
human interpretation of the articles. A short man-
ual description outlines the general course of emo-
tion of a match as reported in each article naming
three to five emotions. A second more fine grained
interpretation assigns one (or two) emotions to each
sentence. In correspondence to Beeferman’s proba-
bilistic error metric (Beeferman et al., 1999) three
types of emotion extraction error are distinguished.
Falsely detected emotions are rated with zero points.
Missing emotions were assessed depending on their
significance in the text. If the overall feeling of the
match was represented, two to three points would
be given, but if the main emotions were missing, no
points were assigned. Very close, but not exact emo-
tions, got a value of four. A correct representation
of the course of emotion received five points. The
grain counts the number of the extracted emotions
per text. The results for correctness of emotional
findings and amount of emotions detected (grain)
of each method run on each part-of-speech or word
type combination are presented in Figure 6.
The results analysis shows that the effectiveness
of adjectives or nouns varies from text to text, but
generally the best results are achieved with the ex-
traction of both kinds of words. On average the
word-by-word method produces emotion sequences
with the closest correctness, but unfortunately its
output is too fine grained for visualization. Thirty
second long animations are best visualized with two
to ten mood swings. This means that some form
of summarization is needed. Combining emotions
of logically structured chunks of text, namely sen-
tences, in the sentence-based summarization method
achieved better results than the minimum subse-
quent occurrence of two or three emotions with
the threshold method. The sentence-based sum-
marizaion as well as the threshold method with a
minimum value of 3 produce the most appropriate
grain/number of emotions. Some misinterpretation
is due to false part-of-speech tagging by Qtag which
has particular trouble with proper nouns. More accu-
racy can be achieved through training Qtag on foot-
ball reports. Overall the results for NewsViz are sat-
isfactory and it demonstrates that it is possible to ex-
tract emotions from news texts. The generally differ-
ent sensations of the two described football matches
are distinguishable. Three of the four test texts show
good results, but for one article the extracted emo-
tions do not seem to match the human sensation.
</bodyText>
<sectionHeader confidence="0.965125" genericHeader="method">
5 Relation to Other Work
</sectionHeader>
<bodyText confidence="0.999951181818182">
NewsViz uses natural human language as input to
create animated output. NewsViz aims to solely re-
flect emotions as they are mentioned in the news ar-
ticle to keep the objective and formal character of
news reporting. Therefore, NewsViz applies a re-
duced, universal and ’personality-free’ version of
existing concepts for emotion and mood construc-
tion. Instead of facial expressions and gestures
NewsViz combines and illustrates emotions with de-
sign principles. NewsViz offers manual reediting of
the automatically created animations.
</bodyText>
<sectionHeader confidence="0.977468" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.807872">
NewsViz extracts emotion-bearing words from on-
line football news reports based on an extended dic-
tionary with emotion-indices assigned to each en-
</bodyText>
<page confidence="0.997234">
129
</page>
<bodyText confidence="0.999982285714286">
try. The extracted emotions are processed and illus-
trated in abstract background animations. Results
from initial testing demonstrate that this automated
process has satisfactory performance. Technolog-
ically, NewsViz is viable for the fixed domain of
football reports and offers a sound basis for more
affective text-to-visual mapping. Future work will
aim to improve the linguistic and semantic process-
ing of emotions. This involves the extension of the
parts of speech selection to include verbs and ad-
verbs, assuming that more input data will lead to bet-
ter results. Rules for common and linguistic knowl-
edge will be integrated. Linguistic knowledge iden-
tifies emotions in context applying language rules
to emotion interpretation, i.e. it solves negation by
inverting emotions. With the integration of a de-
pendency parser, which relates words according to
their sentence structure, emotions of related words
can be found and their average emotion determined.
Domain-specific knowledge (e.g. football) provides
background information including match statistics,
players’ and teams’ names, team colors and league
tables. It also accommodates game rules or match
situations with their emotional consequences. The
mood list is refined through moods discovered with
commonsense knowledge and football facts which
set pre-conditions and context representing long-
term moods influencing current event-based emo-
tions. Future work will reveal whether NewsViz is
feasible when extended to different domains. The
emotion database could be extended through the
WordNet-Affect dictionary (Strapparava and Vali-
tutti, 2004). NewsViz enriches standard news web-
sites with attractive and informative animations and
can track emotional aspects of people’s views on
world events. NewsViz brings news reported on
the internet closer to readers, making it more eas-
ily understood and memorized which is much appre-
ciated by online users overloaded with information.
NewsViz assists animation designers in the produc-
tion of daily updated visualizations creating initial
scenes.
</bodyText>
<sectionHeader confidence="0.998357" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999730303571428">
D. Beeferman, A. Berger and J. Lafferty. 1999. Statisti-
cal models for text segmentation. Machine Learning,
34:177–210. Springer Netherlands.
K. Binsted and S. Luke. 1999. Character Design for Soc-
cer Commentary. Lecture Notes in Computer Science.
RoboCup-98: Robot Soccer World Cup II, 1604:22–
33. Springer-Verlag, London, UK.
P. Buitelaar, T. Eigner, G. Gulrajani, A. Schutz, M.
Siegel, N. Weber, P. Cimiano, G. Ladwig, M. Mantel,
H. Zhu. 2006. Generating and Visualizing a Soccer
Knowledge Base. Proceedings of the EACL06 Demo
Session, 4/2006:123–126.
B. Coyne and R. Sproat. 2001. WordsEye: an auto-
matic text-to-scene conversion system. Proceedings
of the 28th Annual Conference on Computer Graph-
ics and Interactive Techniques, 487–496. ACM Press,
Los Angeles, USA.
D. Joshi, J. Z. Wang and J. Li. 2004. The Story Pictur-
ing Engine: Finding Elite Images to Illustrate a Story
Using Mutual Reinforcement. Proceedings of the 6th
ACM SIGMM International Workshop on Multimedia
Information Retrieval, 119–126. ACM Press, New
York, USA.
Z. Liu and K. Leung. 2006. Script visualiza-
tion (ScriptViz): a smart system that makes writ-
ing fun. Soft Computing, 10(1), 34–40. Springer
Berlin/Heidelberg, Germany.
Minhua Ma. 2006. Automatic Conversion of Natural
Language to 3D Animation. Ph.D. Thesis. School
of Computing and Intelligent Systems, University of
Ulster, UK.
O. Mason. 2003. Qtag. http://phrasys.net/
uob/om/software.
G. A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Davy Mitchell. 2005. MoodNews. http://www.
latedecember.com/sites/moodnews.
G. Retz-Schmidt. 1988. A REPLAI of SOCCER Recog-
nizing intensions in the domain of soccer games. Proc.
European Conf. AI (ECAI-88), 8:455-457.
Daniel Scheibel and Ferdinand Weinrother.
2005. The Unseen Video. http://www.
theunseenvideo.com.
Mostafa Al Masum Shaikh, Helmut Prendinger and Mit-
suru Ishizuka. 2007. Emotion Sensitive News Agent:
An Approarch Towards User Centric Emotion Sensing
from the News. Proceedings 2007 IEEE/WIC/ACM
International Conference on Web Intelligence (WI07),
614–620. Silicon Valley, USA.
N. Stokes. 2003. Spoken and Written News Story Seg-
mentation Using Lexical Chains. Proceedings of HTL-
NAACL 2003, 49–54. Edmonton, Canada.
C. Strapparava and A. Valitutti. 2004. WordNet-Affect:
an Affective Extension of WordNet. Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation (LREC 2004), 1083–1086.
</reference>
<page confidence="0.997605">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.581215">
<title confidence="0.9956695">NewsViz: Emotional Visualization of News Stories</title>
<author confidence="0.999824">Eva Hanser</author>
<author confidence="0.999824">Paul Mc Kevitt</author>
<author confidence="0.999824">Tom Lunney</author>
<author confidence="0.999824">Joan</author>
<affiliation confidence="0.999651333333333">School of Computing &amp; Intelligent Faculty of Computing &amp; University of Ulster,</affiliation>
<address confidence="0.867472">Derry/Londonderry, BT48 Northern</address>
<email confidence="0.986146">tf.lunney,</email>
<abstract confidence="0.999871054054054">The NewsViz system aims to enhance news reading experiences by integrating 30 seconds long Flash-animations into news article web pages depicting their content and emotional aspects. NewsViz interprets football match news texts automatically and creates abstract 2D visualizations. The user interface enables animators to further refine the animations. Here, we focus on the emotion extraction component of NewsViz which facilitates subtle background visualization. NewsViz detects moods from news reports. The original text is part-of-speech tagged and adjectives and/or nouns, the word types conveying most emotional meaning, are filtered out and labeled with an emotion and intensity value. Subsequently reoccurring emotions are joined into longer lasting moods and matched with appropriate animation presets. Different linguistic analysis methods were tested on NewsViz: word-by-word, sentence-based and minimum threshold summarization, to find a minimum number of occurrences of an emotion in forming a valid mood. NewsViz proved to be viable for the fixed domain of football news, grasping the overall moods and some more detailed emotions precisely. NewsViz offers an efficient technique to cater for the production of a large number of daily updated news stories. NewsViz bypasses the lack of information for background or environment depiction encountered in similar applications. Further development may refine the detection of emotion shifts through summarization with the full implementation of football and common linguistic knowledge.</abstract>
<intro confidence="0.807315">125</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--177</pages>
<publisher>Springer</publisher>
<contexts>
<context position="13430" citStr="Beeferman et al., 1999" startWordPosition="2037" endWordPosition="2040">three different emotion extraction methods, word-by-word, sentence-based and 128 Figure 6: Results Analysis of all Test Texts. threshold were run on these news stories with varying word types or word type combinations. The output of NewsViz is evaluated against two forms of human interpretation of the articles. A short manual description outlines the general course of emotion of a match as reported in each article naming three to five emotions. A second more fine grained interpretation assigns one (or two) emotions to each sentence. In correspondence to Beeferman’s probabilistic error metric (Beeferman et al., 1999) three types of emotion extraction error are distinguished. Falsely detected emotions are rated with zero points. Missing emotions were assessed depending on their significance in the text. If the overall feeling of the match was represented, two to three points would be given, but if the main emotions were missing, no points were assigned. Very close, but not exact emotions, got a value of four. A correct representation of the course of emotion received five points. The grain counts the number of the extracted emotions per text. The results for correctness of emotional findings and amount of </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>D. Beeferman, A. Berger and J. Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34:177–210. Springer Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Binsted</author>
<author>S Luke</author>
</authors>
<title>Character Design for Soccer Commentary.</title>
<date>1999</date>
<booktitle>Lecture Notes in Computer Science. RoboCup-98: Robot Soccer World Cup II, 1604:22– 33.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>London, UK.</location>
<contexts>
<context position="7683" citStr="Binsted and Luke, 1999" startWordPosition="1123" endWordPosition="1126">The Story Picturing Engine (Joshi acquired co-occurrences (e.g. Diego Maradonna, et al., 2004) visualizes texts selecting and matching Hand of God). Their starting and end points indi- pictures and their annotations from image databases. cate topical unit boundaries. The work discussed here demonstrates that suffiSensing emotions from multimodal input has cient subsets of the English language can be mapped mainly been investigated with the objective of de- to computer understandable language for the visualveloping human-like agents. The football commen- ization of stories. tary system, Byrne (Binsted and Luke, 1999), in- 3 The NewsViz System cludes a commentator with emotions influenced by NewsViz takes online news articles as input and outhis personality and intentions. SOCCER (Retz- puts animations reflecting the content of these news Schmidt, 1988) analyses football scenes visually in stories. NewViz consists of three main components: order to simultaneously add linguistic descriptions the linguistic analysis, the animation composer and of the events. SOBA (Buitelaar et al., 2006) ex- an interface for editing text and animations (Figure 126 Figure 1: NewsViz System Architecture. 1). The linguistic com</context>
</contexts>
<marker>Binsted, Luke, 1999</marker>
<rawString>K. Binsted and S. Luke. 1999. Character Design for Soccer Commentary. Lecture Notes in Computer Science. RoboCup-98: Robot Soccer World Cup II, 1604:22– 33. Springer-Verlag, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Buitelaar</author>
<author>T Eigner</author>
<author>G Gulrajani</author>
<author>A Schutz</author>
<author>M Siegel</author>
<author>N Weber</author>
<author>P Cimiano</author>
<author>G Ladwig</author>
<author>M Mantel</author>
<author>H Zhu</author>
</authors>
<title>Generating and Visualizing a Soccer Knowledge Base.</title>
<date>2006</date>
<booktitle>Proceedings of the EACL06 Demo Session,</booktitle>
<pages>4--2006</pages>
<contexts>
<context position="8160" citStr="Buitelaar et al., 2006" startWordPosition="1193" endWordPosition="1196">rstandable language for the visualveloping human-like agents. The football commen- ization of stories. tary system, Byrne (Binsted and Luke, 1999), in- 3 The NewsViz System cludes a commentator with emotions influenced by NewsViz takes online news articles as input and outhis personality and intentions. SOCCER (Retz- puts animations reflecting the content of these news Schmidt, 1988) analyses football scenes visually in stories. NewViz consists of three main components: order to simultaneously add linguistic descriptions the linguistic analysis, the animation composer and of the events. SOBA (Buitelaar et al., 2006) ex- an interface for editing text and animations (Figure 126 Figure 1: NewsViz System Architecture. 1). The linguistic component constructs three elements of the animation in different processes. The emotion extraction tool creates atmospheric background visuals, the action visualizer depicts people, objects and their actions and the audio creator selects music and sound effects. The composer synchronizes the different outputs. Here, we focus on the emotion extraction component (Figure 2) developed in Flash MX and Photoshop. Emotional aspects within the news story are identified and linked to</context>
</contexts>
<marker>Buitelaar, Eigner, Gulrajani, Schutz, Siegel, Weber, Cimiano, Ladwig, Mantel, Zhu, 2006</marker>
<rawString>P. Buitelaar, T. Eigner, G. Gulrajani, A. Schutz, M. Siegel, N. Weber, P. Cimiano, G. Ladwig, M. Mantel, H. Zhu. 2006. Generating and Visualizing a Soccer Knowledge Base. Proceedings of the EACL06 Demo Session, 4/2006:123–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coyne</author>
<author>R Sproat</author>
</authors>
<title>WordsEye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, 487–496.</booktitle>
<publisher>ACM Press,</publisher>
<location>Los Angeles, USA.</location>
<contexts>
<context position="5772" citStr="Coyne and Sproat, 2001" startWordPosition="845" endWordPosition="848">e, 2007) chategorizes news stories from different RSS Qtag (Mason, 2003) attaches a tag to each word la- sources into eight emotion categories according to beling it as noun, verb, adjective or other. their emotional content, determined through a cogSemantic interpretation and actual understanding nitive evaluation and user preferences. of the meaning of a text is more difficult, because it Automated story visualization systems deliver depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static structured, related through logical rules and entered 3D images from written descriptions. Additionally, into databases before computational text interpreta- automated camera and character animation, intertion is possible. WordNet (Miller, 1995) determines action and speech synthesis is realized in CONFUsemantic relations between words and is an extended CIUS (Ma, 2006). ScriptViz (Liu and Leung, 2006) dictionary specifying word relations such as simi- renders 3D scenes from NL screenplays immedilarity, part-of relations, hierarchy or manner. Story ately during the writing pr</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>B. Coyne and R. Sproat. 2001. WordsEye: an automatic text-to-scene conversion system. Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, 487–496. ACM Press, Los Angeles, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Joshi</author>
<author>J Z Wang</author>
<author>J Li</author>
</authors>
<title>The Story Picturing Engine: Finding Elite Images to Illustrate a Story Using Mutual Reinforcement.</title>
<date>2004</date>
<booktitle>Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval,</booktitle>
<pages>119--126</pages>
<publisher>ACM Press,</publisher>
<location>New York, USA.</location>
<marker>Joshi, Wang, Li, 2004</marker>
<rawString>D. Joshi, J. Z. Wang and J. Li. 2004. The Story Picturing Engine: Finding Elite Images to Illustrate a Story Using Mutual Reinforcement. Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval, 119–126. ACM Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Liu</author>
<author>K Leung</author>
</authors>
<title>Script visualization (ScriptViz): a smart system that makes writing fun.</title>
<date>2006</date>
<journal>Soft Computing,</journal>
<volume>10</volume>
<issue>1</issue>
<pages>34--40</pages>
<publisher>Springer</publisher>
<location>Berlin/Heidelberg, Germany.</location>
<contexts>
<context position="6195" citStr="Liu and Leung, 2006" startWordPosition="903" endWordPosition="906">stems deliver depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static structured, related through logical rules and entered 3D images from written descriptions. Additionally, into databases before computational text interpreta- automated camera and character animation, intertion is possible. WordNet (Miller, 1995) determines action and speech synthesis is realized in CONFUsemantic relations between words and is an extended CIUS (Ma, 2006). ScriptViz (Liu and Leung, 2006) dictionary specifying word relations such as simi- renders 3D scenes from NL screenplays immedilarity, part-of relations, hierarchy or manner. Story ately during the writing process, extracting verbs segmentation is performed by e.g. SeLeCT (Stokes, and adverbs to interpret events and states in sen2003), an example application based on semantic tences. The Unseen Video (Scheibel and Weinanalysis to find story or subtopic changes within a rother, 2005), is a good example of abstract mood text. Groups of semantically related words called visualization. Local weather data is automatically cohesi</context>
</contexts>
<marker>Liu, Leung, 2006</marker>
<rawString>Z. Liu and K. Leung. 2006. Script visualization (ScriptViz): a smart system that makes writing fun. Soft Computing, 10(1), 34–40. Springer Berlin/Heidelberg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minhua Ma</author>
</authors>
<title>Automatic Conversion of Natural Language to 3D Animation.</title>
<date>2006</date>
<tech>Ph.D. Thesis.</tech>
<institution>School of Computing and Intelligent Systems, University of Ulster, UK.</institution>
<contexts>
<context position="6162" citStr="Ma, 2006" startWordPosition="900" endWordPosition="901">story visualization systems deliver depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static structured, related through logical rules and entered 3D images from written descriptions. Additionally, into databases before computational text interpreta- automated camera and character animation, intertion is possible. WordNet (Miller, 1995) determines action and speech synthesis is realized in CONFUsemantic relations between words and is an extended CIUS (Ma, 2006). ScriptViz (Liu and Leung, 2006) dictionary specifying word relations such as simi- renders 3D scenes from NL screenplays immedilarity, part-of relations, hierarchy or manner. Story ately during the writing process, extracting verbs segmentation is performed by e.g. SeLeCT (Stokes, and adverbs to interpret events and states in sen2003), an example application based on semantic tences. The Unseen Video (Scheibel and Weinanalysis to find story or subtopic changes within a rother, 2005), is a good example of abstract mood text. Groups of semantically related words called visualization. Local wea</context>
</contexts>
<marker>Ma, 2006</marker>
<rawString>Minhua Ma. 2006. Automatic Conversion of Natural Language to 3D Animation. Ph.D. Thesis. School of Computing and Intelligent Systems, University of Ulster, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Mason</author>
</authors>
<date>2003</date>
<note>Qtag. http://phrasys.net/ uob/om/software.</note>
<contexts>
<context position="5221" citStr="Mason, 2003" startWordPosition="764" endWordPosition="765">al images. Text-to-visual mapping starts with It effectively ranks them on a color scale between linguistic analysis of the text. Despite variability, good to bad. The three kinds of emotions are apambiguity and imprecision, syntactic analysis tools pointed through keyword scoring based on a small achieve mostly reliable results, such as trainable vocabulary of 160 words and phrases. The Emopart-of-speech tagger software tools which identify tion Sensitive News Agent (ESNA) (Shaikh et al., parts of speech with 97% accuracy. For example, 2007) chategorizes news stories from different RSS Qtag (Mason, 2003) attaches a tag to each word la- sources into eight emotion categories according to beling it as noun, verb, adjective or other. their emotional content, determined through a cogSemantic interpretation and actual understanding nitive evaluation and user preferences. of the meaning of a text is more difficult, because it Automated story visualization systems deliver depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static structured, related through log</context>
<context position="8954" citStr="Mason, 2003" startWordPosition="1318" endWordPosition="1319">ent processes. The emotion extraction tool creates atmospheric background visuals, the action visualizer depicts people, objects and their actions and the audio creator selects music and sound effects. The composer synchronizes the different outputs. Here, we focus on the emotion extraction component (Figure 2) developed in Flash MX and Photoshop. Emotional aspects within the news story are identified and linked to appropriate presets of background animations. 3.1 Emotion Extraction The first step in processing the text is to tag parts of speech for all words. The part-of-speech tagger, Qtag (Mason, 2003), attaches tags to nouns, verbs, adjectives and other parts of speech. The tagged text is sent on to the adjective and noun detector. These Figure 2: Emotion Extraction Component. two types of words are selected for further processing because they are most central to conveying emotional meaning and sufficient for the visualisation of the emotional content. Nouns and adjectives are the parts of speech which represent the highest number of affective words as found in WordNet-Affect (Strapparava and Valitutti, 2004). Verbs and adverbs will be addressed in future work to increase sensitivity and p</context>
</contexts>
<marker>Mason, 2003</marker>
<rawString>O. Mason. 2003. Qtag. http://phrasys.net/ uob/om/software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="6035" citStr="Miller, 1995" startWordPosition="880" endWordPosition="881"> and actual understanding nitive evaluation and user preferences. of the meaning of a text is more difficult, because it Automated story visualization systems deliver depends largely on commonsense knowledge. Com- initial results for object and action depiction, as in monsense knowledge and mental images need to be WordsEye (Coyne and Sproat, 2001), creating static structured, related through logical rules and entered 3D images from written descriptions. Additionally, into databases before computational text interpreta- automated camera and character animation, intertion is possible. WordNet (Miller, 1995) determines action and speech synthesis is realized in CONFUsemantic relations between words and is an extended CIUS (Ma, 2006). ScriptViz (Liu and Leung, 2006) dictionary specifying word relations such as simi- renders 3D scenes from NL screenplays immedilarity, part-of relations, hierarchy or manner. Story ately during the writing process, extracting verbs segmentation is performed by e.g. SeLeCT (Stokes, and adverbs to interpret events and states in sen2003), an example application based on semantic tences. The Unseen Video (Scheibel and Weinanalysis to find story or subtopic changes within</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davy Mitchell</author>
</authors>
<date>2005</date>
<note>MoodNews. http://www. latedecember.com/sites/moodnews.</note>
<contexts>
<context position="4383" citStr="Mitchell, 2005" startWordPosition="637" endWordPosition="638">t-of- tracts information from soccer match reports, anspeech, semantic relations, e.g. emotion value and notates relevant expressions (e.g. players, teams, intensity, and visual descriptions, e.g. colors and goals.) and generates knowledge base entities. The motion direction, to match suitable graphics. collected football knowledge can set preconditions 2 Background and Related Research and context to consequently evaluate current events Text-to-visual mapping relates to the areas of natu- and assign appropriate emotions. The MoodNews ral language processing (NLP) and multimodal sto- website (Mitchell, 2005) demonstrates a very simrytelling which attempt to enable computers to in- ple linguistic method to distinguish positive, negterpret and generate natural human language and ative and neutral content in BBC news headlines. mental images. Text-to-visual mapping starts with It effectively ranks them on a color scale between linguistic analysis of the text. Despite variability, good to bad. The three kinds of emotions are apambiguity and imprecision, syntactic analysis tools pointed through keyword scoring based on a small achieve mostly reliable results, such as trainable vocabulary of 160 words </context>
</contexts>
<marker>Mitchell, 2005</marker>
<rawString>Davy Mitchell. 2005. MoodNews. http://www. latedecember.com/sites/moodnews.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Retz-Schmidt</author>
</authors>
<title>A REPLAI of SOCCER Recognizing intensions in the domain of soccer games.</title>
<date>1988</date>
<booktitle>Proc. European Conf. AI (ECAI-88),</booktitle>
<pages>8--455</pages>
<marker>Retz-Schmidt, 1988</marker>
<rawString>G. Retz-Schmidt. 1988. A REPLAI of SOCCER Recognizing intensions in the domain of soccer games. Proc. European Conf. AI (ECAI-88), 8:455-457.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Scheibel</author>
<author>Ferdinand Weinrother</author>
</authors>
<title>The Unseen Video.</title>
<date>2005</date>
<note>http://www. theunseenvideo.com.</note>
<marker>Scheibel, Weinrother, 2005</marker>
<rawString>Daniel Scheibel and Ferdinand Weinrother. 2005. The Unseen Video. http://www. theunseenvideo.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mostafa Al Masum Shaikh</author>
</authors>
<title>Helmut Prendinger and Mitsuru Ishizuka.</title>
<date>2007</date>
<booktitle>Proceedings 2007 IEEE/WIC/ACM International Conference on Web Intelligence (WI07), 614–620. Silicon</booktitle>
<location>Valley, USA.</location>
<marker>Shaikh, 2007</marker>
<rawString>Mostafa Al Masum Shaikh, Helmut Prendinger and Mitsuru Ishizuka. 2007. Emotion Sensitive News Agent: An Approarch Towards User Centric Emotion Sensing from the News. Proceedings 2007 IEEE/WIC/ACM International Conference on Web Intelligence (WI07), 614–620. Silicon Valley, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stokes</author>
</authors>
<title>Spoken and Written News Story Segmentation Using Lexical Chains.</title>
<date>2003</date>
<booktitle>Proceedings of HTLNAACL 2003,</booktitle>
<pages>49--54</pages>
<location>Edmonton, Canada.</location>
<marker>Stokes, 2003</marker>
<rawString>N. Stokes. 2003. Spoken and Written News Story Segmentation Using Lexical Chains. Proceedings of HTLNAACL 2003, 49–54. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNet-Affect: an Affective Extension of WordNet.</title>
<date>2004</date>
<booktitle>Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<pages>1083--1086</pages>
<contexts>
<context position="9472" citStr="Strapparava and Valitutti, 2004" startWordPosition="1399" endWordPosition="1402">step in processing the text is to tag parts of speech for all words. The part-of-speech tagger, Qtag (Mason, 2003), attaches tags to nouns, verbs, adjectives and other parts of speech. The tagged text is sent on to the adjective and noun detector. These Figure 2: Emotion Extraction Component. two types of words are selected for further processing because they are most central to conveying emotional meaning and sufficient for the visualisation of the emotional content. Nouns and adjectives are the parts of speech which represent the highest number of affective words as found in WordNet-Affect (Strapparava and Valitutti, 2004). Verbs and adverbs will be addressed in future work to increase sensitivity and precision, but their impact on the resulting animations may not be as significant. Next, the emotion word selector checks the adjectives and nouns in the emotion dictionary and attaches emotion tags indicating their kind of emotion and intensity. The dictionary holds manually created emotion-indices and default intensity values of all affective words. Figure 3: Animations for Sadness (blue), Boredom (green), Tension (red) and Happiness (yellow). 127 Four emotions have been found relevant in relation to football ma</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti. 2004. WordNet-Affect: an Affective Extension of WordNet. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC 2004), 1083–1086.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>