<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.799502">
On the Complexity of Non-Projective Data-Driven Dependency Parsing
</title>
<author confidence="0.857615">
Ryan McDonald
</author>
<affiliation confidence="0.786967">
Google Inc.
</affiliation>
<address confidence="0.9207335">
76 Ninth Avenue
New York, NY 10028
</address>
<email confidence="0.999024">
ryanmcd@google.com
</email>
<author confidence="0.990411">
Giorgio Satta
</author>
<affiliation confidence="0.997377">
University of Padua
</affiliation>
<address confidence="0.921938">
via Gradenigo 6/A
I-35131 Padova, Italy
</address>
<email confidence="0.998805">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.996667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999255076923077">
In this paper we investigate several non-
projective parsing algorithms for depen-
dency parsing, providing novel polynomial
time solutions under the assumption that
each dependency decision is independent of
all the others, called here the edge-factored
model. We also investigate algorithms for
non-projective parsing that account for non-
local information, and present several hard-
ness results. This suggests that it is unlikely
that exact non-projective dependency pars-
ing is tractable for any model richer than the
edge-factored model.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999806352941177">
Dependency representations of natural language are
a simple yet flexible mechanism for encoding words
and their syntactic dependencies through directed
graphs. These representations have been thoroughly
studied in descriptive linguistics (Tesni`ere, 1959;
Hudson, 1984; Sgall et al., 1986; Me´lˇcuk, 1988) and
have been applied in numerous language process-
ing tasks. Figure 1 gives an example dependency
graph for the sentence Mr. Tomash will remain as a
director emeritus, which has been extracted from the
Penn Treebank (Marcus et al., 1993). Each edge in
this graph represents a single syntactic dependency
directed from a word to its modifier. In this rep-
resentation all edges are labeled with the specific
syntactic function of the dependency, e.g., SBJ for
subject and NMOD for modifier of a noun. To sim-
plify computation and some important definitions,
an artificial token is inserted into the sentence as the
left most word and will always represent the root of
the dependency graph. We assume all dependency
graphs are directed trees originating out of a single
node, which is a common constraint (Nivre, 2005).
The dependency graph in Figure 1 is an exam-
ple of a nested or projective graph. Under the as-
sumption that the root of the graph is the left most
word of the sentence, a projective graph is one where
the edges can be drawn in the plane above the sen-
tence with no two edges crossing. Conversely, a
non-projective dependency graph does not satisfy
this property. Figure 2 gives an example of a non-
projective graph for a sentence that has also been
extracted from the Penn Treebank. Non-projectivity
arises due to long distance dependencies or in lan-
guages with flexible word order. For many lan-
guages, a significant portion of sentences require
a non-projective dependency analysis (Buchholz et
al., 2006). Thus, the ability to learn and infer non-
projective dependency graphs is an important prob-
lem in multilingual language processing.
Syntactic dependency parsing has seen a num-
ber of new learning and inference algorithms which
have raised state-of-the-art parsing accuracies for
many languages. In this work we focus on data-
driven models of dependency parsing. These models
are not driven by any underlying grammar, but in-
stead learn to predict dependency graphs based on
a set of parameters learned solely from a labeled
corpus. The advantage of these models is that they
negate the need for the development of grammars
when adapting the model to new languages.
One interesting class of data-driven models are
</bodyText>
<page confidence="0.972414">
121
</page>
<note confidence="0.9586755">
Proceedings of the 10th Conference on Parsing Technologies, pages 121–132,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9999935">
Figure 1: A projective dependency graph.
Figure 2: Non-projective dependency graph.
</figureCaption>
<bodyText confidence="0.999990607843137">
those that assume each dependency decision is in-
dependent modulo the global structural constraint
that dependency graphs must be trees. Such mod-
els are commonly referred to as edge-factored since
their parameters factor relative to individual edges
of the graph (Paskin, 2001; McDonald et al.,
2005a). Edge-factored models have many computa-
tional benefits, most notably that inference for non-
projective dependency graphs can be achieved in
polynomial time (McDonald et al., 2005b). The pri-
mary problem in treating each dependency as in-
dependent is that it is not a realistic assumption.
Non-local information, such as arity (or valency)
and neighbouring dependencies, can be crucial to
obtaining high parsing accuracies (Klein and Man-
ning, 2002; McDonald and Pereira, 2006). How-
ever, in the data-driven parsing setting this can be
partially adverted by incorporating rich feature rep-
resentations over the input (McDonald et al., 2005a).
The goal of this work is to further our current
understanding of the computational nature of non-
projective parsing algorithms for both learning and
inference within the data-driven setting. We start by
investigating and extending the edge-factored model
of McDonald et al. (2005b). In particular, we ap-
peal to the Matrix Tree Theorem for multi-digraphs
to design polynomial-time algorithms for calculat-
ing both the partition function and edge expecta-
tions over all possible dependency graphs for a given
sentence. To motivate these algorithms, we show
that they can be used in many important learning
and inference problems including min-risk decod-
ing, training globally normalized log-linear mod-
els, syntactic language modeling, and unsupervised
learning via the EM algorithm – none of which have
previously been known to have exact non-projective
implementations.
We then switch focus to models that account for
non-local information, in particular arity and neigh-
bouring parse decisions. For systems that model ar-
ity constraints we give a reduction from the Hamilto-
nian graph problem suggesting that the parsing prob-
lem is intractable in this case. For neighbouring
parse decisions, we extend the work of McDonald
and Pereira (2006) and show that modeling vertical
neighbourhoods makes parsing intractable in addi-
tion to modeling horizontal neighbourhoods. A con-
sequence of these results is that it is unlikely that
exact non-projective dependency parsing is tractable
for any model assumptions weaker than those made
by the edge-factored models.
</bodyText>
<sectionHeader confidence="0.836844" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999971689655172">
There has been extensive work on data-driven de-
pendency parsing for both projective parsing (Eis-
ner, 1996; Paskin, 2001; Yamada and Matsumoto,
2003; Nivre and Scholz, 2004; McDonald et al.,
2005a) and non-projective parsing systems (Nivre
and Nilsson, 2005; Hall and N´ov´ak, 2005; McDon-
ald et al., 2005b). These approaches can often be
classified into two broad categories. In the first cat-
egory are those methods that employ approximate
inference, typically through the use of linear time
shift-reduce parsing algorithms (Yamada and Mat-
sumoto, 2003; Nivre and Scholz, 2004; Nivre and
Nilsson, 2005). In the second category are those
that employ exhaustive inference algorithms, usu-
ally by making strong independence assumptions, as
is the case for edge-factored models (Paskin, 2001;
McDonald et al., 2005a; McDonald et al., 2005b).
Recently there have also been proposals for exhaus-
tive methods that weaken the edge-factored assump-
tion, including both approximate methods (McDon-
ald and Pereira, 2006) and exact methods through in-
teger linear programming (Riedel and Clarke, 2006)
or branch-and-bound algorithms (Hirakawa, 2006).
For grammar based models there has been limited
work on empirical systems for non-projective pars-
ing systems, notable exceptions include the work
of Wang and Harper (2004). Theoretical studies of
note include the work of Neuhaus and B¨oker (1997)
showing that the recognition problem for a mini-
</bodyText>
<page confidence="0.996456">
122
</page>
<bodyText confidence="0.999987428571429">
mal dependency grammar is hard. In addition, the
work of Kahane et al. (1998) provides a polynomial
parsing algorithm for a constrained class of non-
projective structures. Non-projective dependency
parsing can be related to certain parsing problems
defined for phrase structure representations, as for
instance immediate dominance CFG parsing (Barton
et al., 1987) and shake-and-bake translation (Brew,
1992).
Independently of this work, Koo et al. (2007) and
Smith and Smith (2007) showed that the Matrix-
Tree Theorem can be used to train edge-factored
log-linear models of dependency parsing. Both stud-
ies constructed implementations that compare favor-
ably with the state-of-the-art. The work of Meil˘a
and Jaakkola (2000) is also of note. In that study
they use the Matrix Tree Theorem to develop a
tractable bayesian learning algorithms for tree belief
networks, which in many ways are closely related
to probabilistic dependency parsing formalisms and
the problems we address here.
</bodyText>
<sectionHeader confidence="0.984031" genericHeader="method">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9741918">
Let L = {l1, ... ,l|L|} be a set of permissible syn-
tactic edge labels and x = x0x1 · · · x,,, be a sen-
tence such that x0=root. From this sentence we con-
struct a complete labeled directed graph (digraph)
Gx = (Vx, Ex) such that,
</bodyText>
<listItem confidence="0.999905">
• Vx = {0, 1,... , n}
• Ex = {(i,j)k  |∀ i,j ∈ Vx and 1 ≤ k ≤ |L|}
</listItem>
<bodyText confidence="0.987798307692308">
Gx is a graph where each word in the sentence is a
node, and there is a directed edge between every pair
of nodes for every possible label. By its definition,
Gx is a multi-digraph, which is a digraph that may
have more than one edge between any two nodes.
Let (i, j)k represent the kth edge from i to j. Gx en-
codes all possible labeled dependencies between the
words of x. Thus every possible dependency graph
of x must be a subgraph of Gx.
Let i →+ j be a relation that is true if and only
if there is a non-empty directed path from node i to
node j in some graph under consideration. A di-
rected spanning tree1 of a graph G, that originates
</bodyText>
<footnote confidence="0.564474">
1A directed spanning tree is commonly referred to as a ar-
borescence in the graph theory literature.
</footnote>
<listItem confidence="0.954899">
out of node 0, is any subgraph T = (VT, ET) such
that,
• VT Vx and ET ⊆ Ex
• ∀j ∈ VT, 0 →+ j if and only if j =6 0
• If (i, j)k ∈ ET, then (i&apos;, j)k&apos; ∈/ ET, ∀i&apos; =6 i
and/or k&apos; =6 k.
</listItem>
<bodyText confidence="0.9994384">
Define T(G) as the set of all directed spanning trees
for a graph G. As McDonald et al. (2005b) noted,
there is a one-to-one correspondence between span-
ning trees of Gx and labeled dependency graphs
of x, i.e., T(Gx) is exactly the set of all possible
projective and non-projective dependency graphs for
sentence x. Throughout the rest of this paper, we
will refer to any T ∈ T (Gx) as a valid dependency
graph for a sentence x. Thus, by definition, every
valid dependency graph must be a tree.
</bodyText>
<sectionHeader confidence="0.999481" genericHeader="method">
3 Edge-factored Models
</sectionHeader>
<bodyText confidence="0.9999648">
In this section we examine the class of models that
assume each dependency decision is independent.
Within this setting, every edge in an induced graph
Gx for a sentence x will have an associated weight
wk ij ≥ 0 that maps the kth directed edge from node
i to node j to a real valued numerical weight. These
weights represents the likelihood of a dependency
occurring from word wi to word wj with label lk.
Define the weight of a spanning tree T = (VT, ET)
as the product of the edge weights
</bodyText>
<equation confidence="0.9988925">
w(T) = � wk ij
(ij)kEET
</equation>
<bodyText confidence="0.999936642857143">
It is easily shown that this formulation includes
the projective model of Paskin (2001) and the non-
projective model of McDonald et al. (2005b).
The definition of wk ij depends on the context in
which it is being used. For example, in the work of
McDonald et al. (2005b) it is simply a linear classi-
fier that is a function of the words in the dependency,
the label of the dependency, and any contextual fea-
tures of the words in the sentence. In a generative
probabilistic model (such as Paskin (2001)) it could
represent the conditional probability of a word wj
being generated with a label lk given that the word
being modified is wi (possibly with some other in-
formation such as the orientation of the dependency
</bodyText>
<page confidence="0.998192">
123
</page>
<bodyText confidence="0.999505428571428">
or the number of words between wi and wj). We will
attempt to make any assumptions about the form wk ij
clear when necessary.
For the remainder of this section we discuss three
crucial problems for learning and inference while
showing that each can be computed tractably for the
non-projective case.
</bodyText>
<subsectionHeader confidence="0.999781">
3.1 Finding the Argmax
</subsectionHeader>
<bodyText confidence="0.998302">
The first problem of interest is finding the highest
weighted tree for a given input sentence x
</bodyText>
<equation confidence="0.99727875">
Y
T = argmax ij
w
TET(G.) (i,j)kEET
</equation>
<bodyText confidence="0.998909866666667">
McDonald et al. (2005b) showed that this can be
solved in O(n2) for unlabeled parsing using the
Chu-Liu-Edmonds algorithm for standard digraphs
(Chu and Liu, 1965; Edmonds, 1967). Unlike most
exact projective parsing algorithms, which use effi-
cient bottom-up chart parsing algorithms, the Chu-
Liu-Edmonds algorithm is greedy in nature. It be-
gins by selecting the single best incoming depen-
dency edge for each node j. It then post-processes
the resulting graph to eliminate cycles and then con-
tinues recursively until a spanning tree (or valid
dependency graph) results (see McDonald et al.
(2005b) for details).
The algorithm is trivially extended to the multi-
digraph case for use in labeled dependency parsing.
First we note that if the maximum directed spanning
tree of a multi-digraph Gx contains any edge (i, j)k,
then we must have k = k* = argmaxk wkij. Oth-
erwise we could simply substitute (i, j)k* in place
of (i, j)k and obtain a higher weighted tree. There-
fore, without effecting the solution to the argmax
problem, we can delete all edges in Gx that do not
satisfy this property. The resulting digraph is no
longer a multi-digraph and the Chu-Liu-Edmonds
algorithm can be applied directly. The new runtime
is O(|L|n2).
As a side note, the k-best argmax problem for di-
graphs can be solved in O(kn2) (Camerini et al.,
1980). This can also be easily extended to the multi-
digraph case for labeled parsing.
</bodyText>
<subsectionHeader confidence="0.999359">
3.2 Partition Function
</subsectionHeader>
<bodyText confidence="0.999992166666667">
A common step in many learning algorithms is to
compute the sum over the weight of all the possi-
ble outputs for a given input x. This value is often
referred to as the partition function due to its sim-
ilarity with a value by the same name in statistical
mechanics. We denote this value as Zx,
</bodyText>
<equation confidence="0.9646685">
XZx =
TET(G.)
</equation>
<bodyText confidence="0.977341714285714">
To compute this sum it is possible to use the Matrix
Tree Theorem for multi-digraphs,
Matrix Tree Theorem (Tutte, 1984): Let G be a
multi-digraph with nodes V = 10, 1, ... , n} and
edges E. Define (Laplacian) matrix Q as a (n +
1)x(n + 1) matrix indexed from 0 to n. For all i and
j, define:
</bodyText>
<equation confidence="0.99786">
Qjj = X Xwk ij &amp; Qij = −wkij
i�j,(i,j)kEE i=&apos;4j,(i,j)kEE
</equation>
<bodyText confidence="0.999134090909091">
If the ith row and column are removed from Q to
produce the matrix Qi, then the sum of the weights of
all directed spanning trees rooted at node i is equal
to |Qi |(the determinant of Qi).
Thus, if we construct Q for a graph Gx, then the de-
terminant of the matrix Qc is equivalent to Zx. The
determinant of an nxn matrix can be calculated in
numerous ways, most of which take O(n3) (Cormen
et al., 1990). The most efficient algorithms for cal-
culating the determinant of a matrix use the fact that
the problem is no harder than matrix multiplication
(Cormen et al., 1990). Matrix multiplication cur-
rently has known O(n2.38) implementations and it
has been widely conjectured that it can be solved in
O(n2) (Robinson, 2005). However, most algorithms
with sub-O(n3) running times require constants that
are large enough to negate any asymptotic advantage
for the case of dependency parsing. As a result, in
this work we use O(n3) as the runtime for comput-
ing Zx.
Since it takes O(|L|n2) to construct the matrix Q,
the entire runtime to compute Zx is O(n3 + |L|n2).
</bodyText>
<subsectionHeader confidence="0.998619">
3.3 Edge Expectations
</subsectionHeader>
<bodyText confidence="0.99995">
Another important problem for various learning
paradigms is to calculate the expected value of each
edge for an input sentence x,
</bodyText>
<equation confidence="0.9972398">
((i,j)k)x = X w(T) x I((i, j)k,T)
TET(G.)
X Y k
w(T) = (i,j)kEET wi,j
TET(G.)
</equation>
<page confidence="0.955472">
124
</page>
<table confidence="0.947402083333333">
Input: x = x0x1 · · · xn
Construct Q O(|L|n2)
for j : 1 .. n O(n)
Q0jj = Qjj and Q0ij = Qij, 0 &lt; `di &lt; n O(n)
Qjj = 1 and Qij = 0, 0 &lt; `di &lt; n O(n)
for i : 0 .. n &amp; i =� j O(n)
Qij = −1 O(1)
Z. = |Q0 |O(n3)
((i, j)k). = wkijZ., `d1 &lt; k &lt; |L |O(|L|)
end for
Qjj = Q0jj and Qij = Q0ij, 0 &lt; `di &lt; n O(n)
end for
</table>
<figureCaption confidence="0.987402">
Figure 3: Algorithm to calculate ((i, j)k)x in
</figureCaption>
<equation confidence="0.719145">
O(n5 + |L|n2).
</equation>
<bodyText confidence="0.988311914285714">
where I((i, j)k, T) is an indicator function that is
one when the edge (i, j)k is in the tree T.
To calculate the expectation for the edge (i, j)k,
we can simply eliminate all edges (i&apos;, j)k0 =� (i, j)k
from Gx and calculate Zx. Zx will now be equal
to the sum of the weights of all trees that con-
tain (i, j)k. A naive implementation to compute
the expectation of all |L|n2 edges takes O(|L|n5 +
|L|2n4), since calculating Zx takes O(n3 + |L|n2)
for a single edge. However, we can reduce this con-
siderably by constructing Q a single time and only
making modifications to it when necessary. An al-
gorithm is given in Figure 3.3 that has a runtime of
O(n5 + |L|n2). This algorithm works by first con-
structing Q. It then considers edges from the node i
to the node j. Now, assume that there is only a single
edge from i to j and that that edge has a weight of 1.
Furthermore assume that this edge is the only edge
directed into the node j. In this case Q should be
modified so that Qjj = 1, Qij = −1, and Qi0j = 0,
Vi&apos; =� i, j (by the Matrix Tree Theorem). The value
of Zx under this new Q will be equivalent to the
weight of all trees containing the single edge from i
to j with a weight of 1. For a specific edge (i, j)k its
expectation is simply wk ijZx, since we can factor out
the weight 1 edge from i to j in all the trees that con-
tribute to Zx and multiply through the actual weight
for the edge. The algorithm then reconstructs Q and
continues.
Following the work of Koo et al. (2007) and Smith
and Smith (2007), it is possible to compute all ex-
pectations in O(n3 + |L|n2) through matrix inver-
sion. To make this paper self contained, we report
here their algorithm adapted to our notation. First,
consider the equivalence,
</bodyText>
<equation confidence="0.962828083333333">
∂ log Zx ∂Zx
∂wk ij
w(T ) x I((i,j)k,T)
wk
ij
As a result, we can re-write the edge expectations as,
k k ∂ log Zx k ∂ log  |Q0 |
((i,j) ) = Zxwij ∂wk= Zxwij ∂wk
ij ij
Using the chain rule, we get,
∂ log |Q0|
∂(Q0)i0j0 ∂wk ij
</equation>
<bodyText confidence="0.9855365">
We assume the rows and columns of Q0 are in-
dexed from 1 so that the indexes of Q and Q0 co-
incide. To calculate ((i, j)k) when i, j &gt; 0, we can
use the fact that ∂ log |X|/Xij = (X−1)ji and that
</bodyText>
<equation confidence="0.9552585">
∂(Q0)i0j0/∂wk ij is non zero only when i&apos; = i and
j&apos; = j or i&apos; = j&apos; = j to get,
((i,j)k) = Zxwkij[((Q0)−1)jj − ((Q0)−1)ji]
When i = 0 and j &gt; 0 the only non zero term of
this sum is when i&apos; = j&apos; = j and so
((0,j)k) = Zxwk0j((Q0)−1)
</equation>
<bodyText confidence="0.999708285714286">
Zx and (Q0)−1 can both be calculated a single time,
each taking O(n3). Using these values, each expec-
tation is computed in O(1). Coupled with with the
fact that we need to construct Q and compute the
expectation for all |L|n2 possible edges, in total it
takes O(n3 + |L|n2) time to compute all edge ex-
pectations.
</bodyText>
<subsectionHeader confidence="0.998089">
3.4 Comparison with Projective Parsing
</subsectionHeader>
<bodyText confidence="0.9993067">
Projective dependency parsing algorithms are well
understood due to their close connection to phrase-
based chart parsing algorithms. The work of Eis-
ner (1996) showed that the argmax problem for di-
graphs could be solved in O(n3) using a bottom-
up dynamic programming algorithm similar to CKY.
Paskin (2001) presented an O(n3) inside-outside al-
gorithm for projective dependency parsing using the
Eisner algorithm as its backbone. Using this al-
gorithm it is trivial to calculate both Zx and each
</bodyText>
<equation confidence="0.952926375">
∂ log Zx
∂Zx ∂wk ij
1
Zx TET(G�)
∂ log |Q0 |�=
∂wk ij i0,j0�1
∂(Q0)i0j0
jj
</equation>
<page confidence="0.940076">
125
</page>
<table confidence="0.9991695">
Projective Non-Projective
O(n3 + |L|n2) O(|L|n2)
O(n3 + |L|n2) O(n3 + |L|n2)
O(n3 + |L|n2) O(n3 + |L|n2)
</table>
<tableCaption confidence="0.8408495">
Table 1: Comparison of runtime for non-projective
and projective algorithms.
</tableCaption>
<bodyText confidence="0.999689636363636">
edge expectation. Crucially, the nested property of
projective structures allows edge expectations to be
computed in O(n3) from the inside-outside values.
It is straight-forward to extend the algorithms of Eis-
ner (1996) and Paskin (2001) to the labeled case
adding only a factor of O(|L|n2).
Table 1 gives an overview of the computational
complexity for the three problems considered here
for both the projective and non-projective case. We
see that the non-projective case compares favorably
for all three problems.
</bodyText>
<sectionHeader confidence="0.997773" genericHeader="method">
4 Applications
</sectionHeader>
<bodyText confidence="0.999881666666667">
To motivate the algorithms from Section 3, we
present some important situations where each cal-
culation is required.
</bodyText>
<subsectionHeader confidence="0.941768">
4.1 Inference Based Learning
</subsectionHeader>
<bodyText confidence="0.99977525">
Many learning paradigms can be defined as
inference-based learning. These include the per-
ceptron (Collins, 2002) and its large-margin vari-
ants (Crammer and Singer, 2003; McDonald et al.,
2005a). In these settings, a models parameters are
iteratively updated based on the argmax calculation
for a single or set of training instances under the
current parameter settings. The work of McDon-
ald et al. (2005b) showed that it is possible to learn
a highly accurate non-projective dependency parser
for multiple languages using the Chu-Liu-Edmonds
algorithm for unlabeled parsing.
</bodyText>
<subsectionHeader confidence="0.664391">
4.2 Non-Projective Min-Risk Decoding
</subsectionHeader>
<bodyText confidence="0.999983666666667">
In min-risk decoding the goal is to find the depen-
dency graph for an input sentence x, that on average
has the lowest expected risk,
</bodyText>
<equation confidence="0.987486666666667">
1:
T = argmin w(T&apos;)R(T, T&apos;)
TET(G-) TET(G.)
</equation>
<bodyText confidence="0.999935928571429">
where R is a risk function measuring the error be-
tween two graphs. Min-risk decoding has been
studied for both phrase-structure parsing and depen-
dency parsing (Titov and Henderson, 2006). In that
work, as is common with many min-risk decoding
schemes, T(Gx) is not the entire space of parse
structures. Instead, this set is usually restricted to
a small number of possible trees that have been pre-
selected by some baseline system. In this subsection
we show that when the risk function is of a specific
form, this restriction can be dropped. The result is
an exact min-risk decoding procedure.
Let R(T, T&apos;) be the Hamming distance between
two dependency graphs for an input sentence x =
</bodyText>
<equation confidence="0.987381333333333">
x0x1 ··· xn,
R(T, T&apos;) = n − 1: j((2,j)k,T&apos;)
(i j)kEET
</equation>
<bodyText confidence="0.999362">
This is a common definition of risk between two
graphs as it corresponds directly to labeled depen-
dency parsing accuracy (McDonald et al., 2005a;
</bodyText>
<equation confidence="0.988798894736842">
Buchholz et al., 2006). Some algebra reveals,
X
T = argmin w(T0)R(T, T0)
T ∈T (Gx) T 0∈T (Gx)
X X
w(T0)[n − I((i, j)k, T0)]
T ∈T (Gx) T 0∈T (Gx) (i,j)k∈ET
w(T0) X I((i, j)k, T0)
(i,j)k∈ET
w(T0)I((i, j)k, T0)
X X
= argmax w(T0)I((i, j)k, T0)
T ∈T (Gx) (i,j)k∈ET T 0∈T (Gx)
Y PT 0∈T (Gx) w(T 0)I((i,j)k,T 0)
= argmax e
T ∈T (Gx) (i,j)k∈ET
Y h(i,j)kix
= argmax e
T ∈T (Gx) (i,j)k∈ET
</equation>
<bodyText confidence="0.904357">
By setting the edge weights to wk = e((i j)k). we
ij
can directly solve this problem using the edge ex-
pectation algorithm described in Section 3.3 and the
argmax algorithm described in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.987072">
4.3 Non-Projective Log-Linear Models
</subsectionHeader>
<bodyText confidence="0.998608166666667">
Conditional Random Fields (CRFs) (Lafferty et al.,
2001) are global discriminative learning algorithms
for problems with structured output spaces, such as
dependency parsing. For dependency parsing, CRFs
would define the conditional probability of a depen-
dency graph T for a sentence x as a globally nor-
</bodyText>
<equation confidence="0.819587826086957">
argmax
Zx
((2, j)k)x
= argmin
= argmin X−
T ∈T (Gx) T0∈T (Gx)
= argmin X−
T ∈T (Gx) (i,j)k∈ET
X
T 0∈T (Gx)
126
malized log-linear model,
Q (i,j)k∈ET ew·f(i,j,k)
p(T|x) =
PT&apos;∈T(G.) Q(i,j)k∈ET&apos; ew f(i,j,k)
-7�
k
H(i j)k∈ET wij
k
PT&apos;∈T(G.) Qj(i)k∈ET&apos; wij
w(T)
=
Z.
</equation>
<bodyText confidence="0.996792090909091">
Here, the weights wk ij are potential functions over
each edge defined as an exponentiated linear classi-
fier with weight vector w E RN and feature vector
f(i, j, k) E RN, where fu(i, j, k) E R represents a
single dimension of the vector f. The denominator,
which is exactly the sum over all graph weights, is a
normalization constant forcing the conditional prob-
ability distribution to sum to one.
CRFs set the parameters w to maximize the log-
likelihood of the conditional probability over a train-
ing set of examples T = {(xα, Tα)}|T|
</bodyText>
<equation confidence="0.973459">
α�1,
Xw = argmax
w α log p(Tα|xα)
</equation>
<bodyText confidence="0.999849666666667">
This optimization can be solved through a vari-
ety of iterative gradient based techniques. Many
of these require the calculation of feature expecta-
tions over the training set under model parameters
for the previous iteration. First, we note that the
feature functions factor over edges, i.e., fu(T) =
</bodyText>
<equation confidence="0.499357">
P
</equation>
<bodyText confidence="0.9998895">
(i,j)k∈ET fu(i,j, k). Because of this, we can use
edge expectations to compute the expectation of ev-
ery feature fu. Let (fu).α represent the expectation
of feature fu for the training instance xα,
</bodyText>
<equation confidence="0.994315363636364">
hfuixα = X p(T|xα)fu(T)
T ∈T (Gxα )
X= p(T|xα) X fu(i, j, k)
T ∈T (Gxα ) (i,j)k∈ET
X=
T ∈T (Gxα ) w(T) X
Zx (i,j)k∈ET fu(i, j, k)
1 X =
Zx(i,j)k∈Exα
1 X =
Zx(i,j)k∈Exα
</equation>
<bodyText confidence="0.9986638">
Thus, we can calculate the feature expectation per
training instance using the algorithms for comput-
ing Z,, and edge expectations. Using this, we can
calculate feature expectations over the entire train-
ing set,
</bodyText>
<equation confidence="0.9220765">
X(fu)T = p(xα)(fu).α
α
</equation>
<bodyText confidence="0.992207">
where p(xα) is typically set to 1/|T |.
</bodyText>
<subsectionHeader confidence="0.994696">
4.4 Non-projective Generative Parsing Models
</subsectionHeader>
<bodyText confidence="0.961684166666667">
A generative probabilistic dependency model over
some alphabet E consists of parameters pkx,y asso-
ciated with each dependency from word x E E to
word y E E with label lk E L. In addition, we im-
pose 0 &lt; pkx,y &lt; 1 and the normalization conditions
k = 1 for each x E E. We define a en-
</bodyText>
<subsectionHeader confidence="0.308911">
Py,k px,y g
</subsectionHeader>
<bodyText confidence="0.991251">
erative probability model p over trees T E T (G.)
and a sentence x = x0x1 · · · xn conditioned on the
sentence length, which is always known,
</bodyText>
<equation confidence="0.998181">
p(x,T|n) = p(x|T,n)p(T|n)
Y= pkxi,xj p(T |n)
(i,j)k∈ET
</equation>
<bodyText confidence="0.999966884615385">
We assume that p(T |n) = β is uniform. This model
is studied specifically by Paskin (2001). In this
model, one can view the sentence as being generated
recursively in a top-down process. First, a tree is
generated from the distribution p(T |n). Then start-
ing at the root of the tree, every word generates all of
its modifiers independently in a recursive breadth-
first manner. Thus, pkx,y represents the probability
of the word x generating its modifier y with label
lk. This distribution is usually smoothed and is of-
ten conditioned on more information including the
orientation of x relative to y (i.e., to the left/right)
and distance between the two words. In the super-
vised setting this model can be trained with maxi-
mum likelihood estimation, which amounts to sim-
ple counts over the data. Learning in the unsuper-
vised setting requires EM and is discussed in Sec-
tion 4.4.2.
Another generative dependency model of interest
is that given by Klein and Manning (2004). In this
model the sentence and tree are generated jointly,
which allows one to drop the assumption that p(T |n)
is uniform. This requires the addition to the model
of parameters px,STOP for each x E E, with the nor-
malization condition px,STOP + Py,k pkx,y = 1. It is
possible to extend the model of Klein and Manning
</bodyText>
<equation confidence="0.998125">
X w(T)I((i, j)k, T)fu(i, j, k)
T ∈T (Gx)
h(i, j)kixαfu(i, j, k)
</equation>
<page confidence="0.98846">
127
</page>
<bodyText confidence="0.999878833333333">
(2004) to the non-projective case. However, the re-
sulting distribution will be over multisets of words
from the alphabet instead of strings. The discus-
sion in this section is stated for the model in Paskin
(2001); a similar treatment can be developed for the
model in Klein and Manning (2004).
</bodyText>
<subsectionHeader confidence="0.601114">
4.4.1 Language Modeling
</subsectionHeader>
<bodyText confidence="0.9993225">
A generative model of dependency structure
might be used to determine the probability of a sen-
tence x by marginalizing out all possible depen-
dency trees,
</bodyText>
<equation confidence="0.999126">
p(x|n) = X p(x,T|n)
T∈T (Gx)
X= p(x|T, n)p(T |n)
T∈T (Gx)
= β X Y pkxi,xj = βZx
T∈T (Gx) (i,j)k∈ET
</equation>
<bodyText confidence="0.9998815">
This probability can be used directly as a non-
projective syntactic language model (Chelba et al.,
1997) or possibly interpolated with a separate n-
gram model.
</bodyText>
<subsectionHeader confidence="0.761659">
4.4.2 Unsupervised Learning
</subsectionHeader>
<bodyText confidence="0.8938312">
In unsupervised learning we train our model on
a sample of unannotated sentences X = {xα�|X |
α=1.
Let |xα |= nα and p(T |nα) = βα. We choose the
parameters that maximize the log-likelihood
</bodyText>
<equation confidence="0.9963196">
|X|
X log(p(xα|nα)) =
α=1
= X |X |log( X p(xα|T, nα)) + X |X |log(βα),
α=1 T∈T(Gx«) α=1
</equation>
<bodyText confidence="0.993891">
viewed as a function of the parameters and subject
to the normalization conditions, i.e., Py,k pkx,y = 1
and pk x,y &gt; 0.
Let xαi be the ith word of xα. By solving the
above constrained optimization problem with the
usual Lagrange multipliers method one gets
</bodyText>
<equation confidence="0.9948253">
k
px,y =
P|X |P
1
α=1 i : x«i = x,
«
j : x«j = y
= P|X1XI 1
1 Zx« Py�,k� P i : x«i = x,
j, : x«j, = y, ((i, j0)k�)x« ,
</equation>
<bodyText confidence="0.99999564516129">
where for each xα the expectation ((i, j)k)x« is de-
fined as in Section 3, but with the weight w(T) re-
placed by the probability distribution p(xα|T, nα).
The above |L|- |E|2 relations represent a non-
linear system of equations. There is no closed form
solution in the general case, and one adopts the ex-
pectation maximization (EM) method, which is a
specialization of the standard fixed-point iteration
method for the solution of non-linear systems. We
start with some initial assignment of the parameters
and at each iteration we use the induced distribu-
tion p(xα|T, nα) to compute a refined value for the
parameters themselves. We are always guaranteed
that the Kullback-Liebler divergence between two
approximated distributions computed at successive
iterations does not increase, which implies the con-
vergence of the method to some local maxima (with
the exception of saddle points).
Observe that at each iteration we can compute
quantities ((i, j)k)x« and Zx« in polynomial time
using the algorithms from Section 3 with pkx«i,x«j
in place of wki,j. Furthermore, under some standard
conditions the fixed-point iteration method guaran-
tees a constant number of bits of precision gain for
the parameters at each iteration, resulting in overall
polynomial time computation in the size of the input
and in the required number of bits for the precision.
As far as we know, this is the first EM learning algo-
rithm for the model in Paskin (2001) working in the
non-projective case. The projective case has been
investigated in Paskin (2001).
</bodyText>
<sectionHeader confidence="0.84061" genericHeader="method">
5 Beyond Edge-factored Models
</sectionHeader>
<bodyText confidence="0.999889785714286">
We have shown that several computational problems
related to parsing can be solved in polynomial time
for the class of non-projective dependency models
with the assumption that dependency relations are
mutually independent. These independence assump-
tions are unwarranted, as it has already been estab-
lished that modeling non-local information such as
arity and nearby parsing decisions improves the ac-
curacy of dependency models (Klein and Manning,
2002; McDonald and Pereira, 2006).
In the spirit of our effort to understand the nature
of exact non-projective algorithms, we examine de-
pendency models that introduce arity constraints as
well as permit edge decisions to be dependent on a
</bodyText>
<equation confidence="0.397965">
((i, j)k)x«
</equation>
<page confidence="0.983648">
128
</page>
<bodyText confidence="0.9992052">
limited neighbourhood of other edges in the graph.
Both kinds of models can no longer be considered
edge-factored, since the likelihood of a dependency
occurring in a particular analysis is now dependent
on properties beyond the edge itself.
</bodyText>
<subsectionHeader confidence="0.994382">
5.1 Arity
</subsectionHeader>
<bodyText confidence="0.99522075">
One feature of the edge-factored models is that no
restriction is imposed on the arity of the nodes in the
dependency trees. As a consequence, these models
can generate dependency trees of unbounded arity.
We show below that this is a crucial feature in the
development of the complexity results we have ob-
tained in the previous sections.
Let us assume a graph G(�)
</bodyText>
<equation confidence="0.601308">
x = (Vx, Ex) defined
as before, but with the additional condition that each
node i E Vx is associated with an integer value
0(i) &gt; 0. T (G(�)
</equation>
<bodyText confidence="0.994144">
x ) is now defined as the set of all
directed spanning trees for G(�)
x rooted in node 0,
such that every node i E Vx has arity smaller than or
equal to 0(i). We now introduce a construction that
will be used to establish several hardness results for
the computational problems discussed in this paper.
Recall that a Hamiltonian path in a directed graph
G is a directed path that visits all of the nodes of G
exactly once.
</bodyText>
<equation confidence="0.7244356">
Let G be some directed graph with set of nodes
V = {1, 2, ... , n}. We construct a target graph
G(�)
x = (Vx, Ex) with Vx = V U {0} (0 the root
node) and |L |= 1. For each i, j E Vx with i =� j,
</equation>
<bodyText confidence="0.969397566666667">
we add an edge (i, j)1 to Ex. We set w1��� = 1 if
there is an edge from i to j in G, or else if i or j
is the root node 0, and w1��� = 0 otherwise. Fur-
thermore, we set 0(i) = 1 for each i E Vx. This
construction can be clearly carried out in log-space.
Note that each T E T (G(0)
x ) must be a monadic
tree with weight equal to either 0 or 1. It is not dif-
ficult to see that if w(T) = 1, then when we remove
the root node 0 from T we obtain a Hamiltonian path
in G. Conversely, each Hamiltonian path in G can
be extended to a spanning tree T E T (G(�)
x ) with
w(T) = 1, by adding the root node 0.
Using the above observations, it can be shown that
the solution of the argmax problem for G(0) xpro-
vides some Hamiltonian directed path in G. The lat-
ter search problem is FNP-hard, and is unlikely to
be solved in polynomial time. Furthermore, quan-
tity Zx provides the count of the Hamiltonian di-
rected paths in G, and for each i E V , the expecta-
tion ((0, i)1)x provides the count of the Hamiltonian
directed paths in G starting from node i. Both these
counting problems are #P-hard, and very unlikely to
have polynomial time solutions.
This result helps to relate the hardness of data-
driven models to the commonly known hardness
results in the grammar-driven literature given by
Neuhaus and B¨oker (1997). In that work, an arity
constraint is included in their minimal grammar.
</bodyText>
<subsectionHeader confidence="0.999184">
5.2 Vertical and Horizontal Markovization
</subsectionHeader>
<bodyText confidence="0.999978857142857">
In general, we would like to say that every depen-
dency decision is dependent on every other edge in
a graph. However, modeling dependency parsing in
such a manner would be a computational nightmare.
Instead, we would like to make a Markov assump-
tion over the edges of the tree, in a similar way that
a Markov assumption can be made for sequential
classification problems in order to ensure tractable
learning and inference.
Klein and Manning (2003) distinguish between
two kinds of Markovization for unlexicalized CFG
parsing. The first is vertical Markovization, which
makes the generation of a non-terminal dependent
on other non-terminals that have been generated at
different levels in the phrase-structure tree. The
second is horizontal Markovization, which makes
the generation of a non-terminal dependent on other
non-terminals that have been generated at the same
level in the tree.
For dependency parsing there are analogous no-
tions of vertical and horizontal Markovization for a
given edge (i, j)k. First, let us define the vertical and
horizontal neighbourhoods of (i, j)k. The vertical
neighbourhood includes all edges in any path from
the root to a leaf that passes through (i, j)k. The hor-
izontal neighbourhood contains all edges (i, j&apos;)k�.
Figure 4 graphically displays the vertical and hor-
izontal neighbourhoods for an edge in the depen-
dency graph from Figure 1.
Vertical and horizontal Markovization essentially
allow the score of the graph to factor over a larger
scope of edges, provided those edges are in the same
vertical or horizontal neighbourhood. A dth order
factorization is one in which the score factors only
over the d nearest edges in the neighbourhoods. In
</bodyText>
<page confidence="0.997152">
129
</page>
<figureCaption confidence="0.9804685">
Figure 4: Vertical and Horizontal neighbourhood for
the edge from will to remain.
</figureCaption>
<bodyText confidence="0.998084">
McDonald and Pereira (2006), it was shown that
non-projective dependency parsing with horizontal
Markovization is FNP-hard. In this study we com-
plete the picture and show that vertical Markoviza-
tion is also FNP-hard.
Consider a first-order vertical Markovization in
which the score for a dependency graph factors over
pairs of vertically adjacent edges2,
</bodyText>
<equation confidence="0.909241">
�
w(T) =
(h,i)k,(i,j)k0∈ET
</equation>
<bodyText confidence="0.998818083333333">
where k hiwk0
ij is the weight of including both edges
(h, i)k and (i, j)k0 in the dependency graph. Note
that this formulation does not include any contribu-
tions from dependencies that have no vertically adja-
cent neighbours, i.e., any edge (0, i)k such that there
is no edge (i, j)k0 in the graph. We can easily rec-
tify this by inserting a second root node, say 00, and
including the weights k000wk0
0i. To ensure that only
valid dependency graphs get a weight greater than
zero, we can set k hiwk0
</bodyText>
<equation confidence="0.786561333333333">
ij = 0 if i = 00 and k 00iwk0
ij = 0
if i =�0.
</equation>
<bodyText confidence="0.915139">
Now, consider the NP-complete 3D-matching
problem (3DM). As input we are given three sets
of size m, call them A, B and C, and a set S C_
A x B x C. The 3DM problem asks if there is a set
S0 C_ S such that |S0 |= m and for any two tuples
(a, b, c), (a0, b0, c0) E S0 it is the case that a =� a0,
b =� b0, and c =�c0.
2McDonald and Pereira (2006) define this as a second-order
Markov assumption. This is simply a difference in terminology
and does not represent any meaningful distinction.
We can reduce the 3D-matching problem to the
first-order vertical Markov parsing problem by con-
structing a graph G = (V, E), such that L =
AUBUC,V = {00,0} U A U B U C and E =
{(i, j)k  |i, j E V, k E L}. The set E contains mul-
tiple edges between ever pair of nodes, each edge
taking on a label representing a single element of
the set A U B U C. Now, define k 000wk0
</bodyText>
<construct confidence="0.55483025">
0a = 1, for all
a E A and k, k0 E A U B U C, and b0awcab = 1, for
all a E A and b E B and c E C, and cabwcbc = 1, for
all (a, b, c) E S. All other weights are set to zero.
</construct>
<bodyText confidence="0.999044810810811">
We show below that there exists a bijection be-
tween the set of valid 3DMs for S and the set of non-
zero weighted dependency graphs in T(G). First, it
is easy to show that for any 3DM S0, there is a rep-
resentative dependency graph that has a weight of
1. This graph simply consists of the edges (0, a)b,
(a, b)c, and (b, c)c, for all (a, b, c) E S0, plus an ar-
bitrarily labeled edge from 00 to 0.
To prove the reverse, consider a graph with weight
1. This graph must have a weight 1 edge into the
node a of the form (0, a)b since the graph must be
spanning. By the definition of the weight function,
in any non-zero weighted tree, a must have a sin-
gle outgoing edge, and that edge must be directed
into the node b. Let’s say that this edge is (a, b)c.
Then again by the weighting function, in any non-
zero weighted graph, b must have a single outgoing
edge that is directed into c, in particular the edge
(b, c)c. Thus, for any node a, there is a single path
directed out of it to a single leaf c E C. We can
then state that the only non-zero weighted depen-
dency graph is one where each a E A, b E B and
c E C occurs in exactly one of m disjoint paths from
the root of the form 0 —* a —* b —* c. This is be-
cause the label of the single edge going into node a
will determine exactly the node b that the one outgo-
ing edge from a must go into. The label of that edge
determines exactly the single outgoing edge from b
into some node c. Now, since the weighting func-
tion ensures that the only non-zero weighted paths
into any leaf node c correspond directly to elements
of S, each of the m disjoint paths represent a single
tuple in a 3DM. Thus, if there is a non-zero weighted
graph in T(G), then it must directly correspond to a
valid 3DM, which concludes the proof.
Note that any dth order Markovization can be em-
bedded into a d + 1th Markovization. Thus, this re-
</bodyText>
<equation confidence="0.699414">
k k0
hiwij
</equation>
<page confidence="0.971596">
130
</page>
<bodyText confidence="0.833249">
sult also holds for any arbitrary Markovization.
</bodyText>
<sectionHeader confidence="0.998725" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999917084745763">
In this paper we have shown that many important
learning and inference problems can be solved effi-
ciently for non-projective edge-factored dependency
models by appealing to the Matrix Tree Theorem
for multi-digraphs. These results extend the work
of McDonald et al. (2005b) and help to further our
understanding of when exact non-projective algo-
rithms can be employed. When this analysis is cou-
pled with the projective parsing algorithms of Eisner
(1996) and Paskin (2001) we begin to get a clear pic-
ture of the complexity for data-driven dependency
parsing within an edge-factored framework. To fur-
ther justify the algorithms presented here, we out-
lined a few novel learning and inference settings in
which they are required.
However, for the non-projective case, moving
beyond edge-factored models will almost certainly
lead to intractable parsing problems. We have pro-
vided further evidence for this by proving the hard-
ness of incorporating arity constraints and hori-
zontal/vertical edge Markovization, both of which
incorporate information unavailable to an edge-
factored model. The hardness results provided
here are also of interest since both arity constraints
and Markovization can be incorporated efficiently
in the projective case through the straight-forward
augmentation of the underlying chart parsing algo-
rithms used in the projective edge-factored models.
This highlights a fundamental difference between
the nature of projective parsing algorithms and non-
projective parsing algorithms. On the projective
side, all algorithms use a bottom-up chart parsing
framework to search the space of nested construc-
tions. On the non-projective side, algorithms are
either greedy-recursive in nature (i.e., the Chu-Liu-
Edmonds algorithm) or based on the calculation of
the determinant of a matrix (i.e., the partition func-
tion and edge expectations).
Thus, the existence of bottom-up chart parsing
algorithms for projective dependency parsing pro-
vides many advantages. As mentioned above, it
permits simple augmentation techniques to incorpo-
rate non-local information such as arity constraints
and Markovization. It also ensures the compatibility
of projective parsing algorithms with many impor-
tant natural language processing methods that work
within a bottom-up chart parsing framework, includ-
ing information extraction (Miller et al., 2000) and
syntax-based machine translation (Wu, 1996).
The complexity results given here suggest that
polynomial chart-parsing algorithms do not exist
for the non-projective case. Otherwise we should
be able to augment them and move beyond edge-
factored models without encountering intractability
– just like the projective case. An interesting line
of research is to investigate classes of non-projective
structures that can be parsed with chart-parsing algo-
rithms and how these classes relate to the languages
parsable by other syntactic formalisms.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99595075">
Thanks to Ben Taskar for pointing out the work of
Meil˘a and Jaakkola (2000). Thanks to David Smith,
Noah Smith and Michael Collins for making drafts
of their EMNLP papers available.
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936416666667">
G. E. Barton, R. C. Berwick, and E. S. Ristad. 1987.
Computational Complexity and Natural Language.
MIT Press, Cambridge, MA.
C. Brew. 1992. Letting the cat out of the bag: Generation
for Shake-and-Bake MT. In Proc. COLING.
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. In Proc. CoNLL.
P. M. Camerini, L. Fratta, and F. Maffioli. 1980. The k
best spanning arborescences of a network. Networks,
10(2):91–110.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-
pur, L. Mangu, H. Printz, E.S. Ristad, R. Rosenfeld,
A. Stolcke, and D. Wu. 1997. Structure and per-
formance of a dependency language model. In Eu-
rospeech.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
</reference>
<page confidence="0.977951">
131
</page>
<reference confidence="0.999765901098902">
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233–
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
K. Hall and V. N´ov´ak. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. IWPT.
H. Hirakawa. 2006. Graph branch algorithm: An opti-
mum tree search method for scored dependency graph
with arc co-occurrence constraints. In Proc. ACL.
R. Hudson. 1984. Word Grammar. Blackwell.
S. Kahane, A. Nasr, and O Rambow. 1998. Pseudo-
projectivity: A polynomially parsable non-projective
dependency grammar. In Proc. ACL.
D. Klein and C.D. Manning. 2002. Fast exact natu-
ral language parsing with a factored model. In Proc.
NIPS.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In Proc. ACL.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In Proc. ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. EMNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT/EMNLP.
M. Meil˘a and T. Jaakkola. 2000. Tractable Bayesian
learning of tree belief networks. In Proc. UAI.
I.A. Me´lˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc NAACL, pages 226–233.
P. Neuhaus and N. B¨oker. 1997. The complexity
of recognition of linguistically adequate dependency
grammars. In Proc. ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, V¨axj¨o
University: School of Mathematics and Systems Engi-
neering.
M.A. Paskin. 2001. Cubic-time parsing and learning al-
gorithms for grammatical bigram models. Technical
Report UCB/CSD-01-1148, Computer Science Divi-
sion, University of California Berkeley.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. EMNLP.
S. Robinson. 2005. Toward an optimal algorithm for
matrix multiplication. News Journal of the Society for
Industrial and Applied Mathematics, 38(9).
P. Sgall, E. Hajiˇcov´a, and J. Panevov´a. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
D.A. Smith and N.A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In Proc. EMNLP.
L. Tesni`ere. 1959. ´El´ements de syntaxe structurale. Edi-
tions Klincksieck.
I. Titov and J. Henderson. 2006. Bayes risk minimiza-
tion in natural language parsing. University of Geneva
technical report.
W.T. Tutte. 1984. Graph Theory. Cambridge University
Press.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
D. Wu. 1996. A polynomial-time algorithm for statisti-
cal machine translation. In Proc. ACL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
</reference>
<page confidence="0.997711">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666023">
<title confidence="0.99999">On the Complexity of Non-Projective Data-Driven Dependency Parsing</title>
<author confidence="0.983797">Ryan</author>
<affiliation confidence="0.851674">Google</affiliation>
<address confidence="0.9546245">76 Ninth New York, NY</address>
<email confidence="0.999498">ryanmcd@google.com</email>
<author confidence="0.993081">Giorgio</author>
<affiliation confidence="0.94816">University of via Gradenigo</affiliation>
<address confidence="0.959966">I-35131 Padova,</address>
<email confidence="0.999268">satta@dei.unipd.it</email>
<abstract confidence="0.997489428571429">In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model. We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results. This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G E Barton</author>
<author>R C Berwick</author>
<author>E S Ristad</author>
</authors>
<date>1987</date>
<booktitle>Computational Complexity and Natural Language.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7877" citStr="Barton et al., 1987" startWordPosition="1206" endWordPosition="1209">imited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the prob</context>
</contexts>
<marker>Barton, Berwick, Ristad, 1987</marker>
<rawString>G. E. Barton, R. C. Berwick, and E. S. Ristad. 1987. Computational Complexity and Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brew</author>
</authors>
<title>Letting the cat out of the bag: Generation for Shake-and-Bake MT. In</title>
<date>1992</date>
<booktitle>Proc. COLING.</booktitle>
<contexts>
<context position="7921" citStr="Brew, 1992" startWordPosition="1213" endWordPosition="1214">arsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L </context>
</contexts>
<marker>Brew, 1992</marker>
<rawString>C. Brew. 1992. Letting the cat out of the bag: Generation for Shake-and-Bake MT. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
<author>A Dubey</author>
<author>Y Krymolowski</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. CoNLL.</booktitle>
<contexts>
<context position="2623" citStr="Buchholz et al., 2006" startWordPosition="407" endWordPosition="410">Under the assumption that the root of the graph is the left most word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing. Conversely, a non-projective dependency graph does not satisfy this property. Figure 2 gives an example of a nonprojective graph for a sentence that has also been extracted from the Penn Treebank. Non-projectivity arises due to long distance dependencies or in languages with flexible word order. For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006). Thus, the ability to learn and infer nonprojective dependency graphs is an important problem in multilingual language processing. Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages. In this work we focus on datadriven models of dependency parsing. These models are not driven by any underlying grammar, but instead learn to predict dependency graphs based on a set of parameters learned solely from a labeled corpus. The advantage of these models is that they negate the need for the deve</context>
<context position="21553" citStr="Buchholz et al., 2006" startWordPosition="3739" endWordPosition="3742">rse structures. Instead, this set is usually restricted to a small number of possible trees that have been preselected by some baseline system. In this subsection we show that when the risk function is of a specific form, this restriction can be dropped. The result is an exact min-risk decoding procedure. Let R(T, T&apos;) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 ··· xn, R(T, T&apos;) = n − 1: j((2,j)k,T&apos;) (i j)kEET This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al., 2005a; Buchholz et al., 2006). Some algebra reveals, X T = argmin w(T0)R(T, T0) T ∈T (Gx) T 0∈T (Gx) X X w(T0)[n − I((i, j)k, T0)] T ∈T (Gx) T 0∈T (Gx) (i,j)k∈ET w(T0) X I((i, j)k, T0) (i,j)k∈ET w(T0)I((i, j)k, T0) X X = argmax w(T0)I((i, j)k, T0) T ∈T (Gx) (i,j)k∈ET T 0∈T (Gx) Y PT 0∈T (Gx) w(T 0)I((i,j)k,T 0) = argmax e T ∈T (Gx) (i,j)k∈ET Y h(i,j)kix = argmax e T ∈T (Gx) (i,j)k∈ET By setting the edge weights to wk = e((i j)k). we ij can directly solve this problem using the edge expectation algorithm described in Section 3.3 and the argmax algorithm described in Section 3.1. 4.3 Non-Projective Log-Linear Models Conditi</context>
</contexts>
<marker>Buchholz, Marsi, Dubey, Krymolowski, 2006</marker>
<rawString>S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Camerini</author>
<author>L Fratta</author>
<author>F Maffioli</author>
</authors>
<title>The k best spanning arborescences of a network.</title>
<date>1980</date>
<journal>Networks,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="13270" citStr="Camerini et al., 1980" startWordPosition="2194" endWordPosition="2197">g. First we note that if the maximum directed spanning tree of a multi-digraph Gx contains any edge (i, j)k, then we must have k = k* = argmaxk wkij. Otherwise we could simply substitute (i, j)k* in place of (i, j)k and obtain a higher weighted tree. Therefore, without effecting the solution to the argmax problem, we can delete all edges in Gx that do not satisfy this property. The resulting digraph is no longer a multi-digraph and the Chu-Liu-Edmonds algorithm can be applied directly. The new runtime is O(|L|n2). As a side note, the k-best argmax problem for digraphs can be solved in O(kn2) (Camerini et al., 1980). This can also be easily extended to the multidigraph case for labeled parsing. 3.2 Partition Function A common step in many learning algorithms is to compute the sum over the weight of all the possible outputs for a given input x. This value is often referred to as the partition function due to its similarity with a value by the same name in statistical mechanics. We denote this value as Zx, XZx = TET(G.) To compute this sum it is possible to use the Matrix Tree Theorem for multi-digraphs, Matrix Tree Theorem (Tutte, 1984): Let G be a multi-digraph with nodes V = 10, 1, ... , n} and edges E.</context>
</contexts>
<marker>Camerini, Fratta, Maffioli, 1980</marker>
<rawString>P. M. Camerini, L. Fratta, and F. Maffioli. 1980. The k best spanning arborescences of a network. Networks, 10(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>D Engle</author>
<author>F Jelinek</author>
<author>V Jimenez</author>
<author>S Khudanpur</author>
<author>L Mangu</author>
<author>H Printz</author>
<author>E S Ristad</author>
<author>R Rosenfeld</author>
<author>A Stolcke</author>
<author>D Wu</author>
</authors>
<title>Structure and performance of a dependency language model.</title>
<date>1997</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="26784" citStr="Chelba et al., 1997" startWordPosition="4680" endWordPosition="4683">ing distribution will be over multisets of words from the alphabet instead of strings. The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Manning (2004). 4.4.1 Language Modeling A generative model of dependency structure might be used to determine the probability of a sentence x by marginalizing out all possible dependency trees, p(x|n) = X p(x,T|n) T∈T (Gx) X= p(x|T, n)p(T |n) T∈T (Gx) = β X Y pkxi,xj = βZx T∈T (Gx) (i,j)k∈ET This probability can be used directly as a nonprojective syntactic language model (Chelba et al., 1997) or possibly interpolated with a separate ngram model. 4.4.2 Unsupervised Learning In unsupervised learning we train our model on a sample of unannotated sentences X = {xα�|X | α=1. Let |xα |= nα and p(T |nα) = βα. We choose the parameters that maximize the log-likelihood |X| X log(p(xα|nα)) = α=1 = X |X |log( X p(xα|T, nα)) + X |X |log(βα), α=1 T∈T(Gx«) α=1 viewed as a function of the parameters and subject to the normalization conditions, i.e., Py,k pkx,y = 1 and pk x,y &gt; 0. Let xαi be the ith word of xα. By solving the above constrained optimization problem with the usual Lagrange multiplie</context>
</contexts>
<marker>Chelba, Engle, Jelinek, Jimenez, Khudanpur, Mangu, Printz, Ristad, Rosenfeld, Stolcke, Wu, 1997</marker>
<rawString>C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudanpur, L. Mangu, H. Printz, E.S. Ristad, R. Rosenfeld, A. Stolcke, and D. Wu. 1997. Structure and performance of a dependency language model. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="12102" citStr="Chu and Liu, 1965" startWordPosition="1996" endWordPosition="1999">or the number of words between wi and wj). We will attempt to make any assumptions about the form wk ij clear when necessary. For the remainder of this section we discuss three crucial problems for learning and inference while showing that each can be computed tractably for the non-projective case. 3.1 Finding the Argmax The first problem of interest is finding the highest weighted tree for a given input sentence x Y T = argmax ij w TET(G.) (i,j)kEET McDonald et al. (2005b) showed that this can be solved in O(n2) for unlabeled parsing using the Chu-Liu-Edmonds algorithm for standard digraphs (Chu and Liu, 1965; Edmonds, 1967). Unlike most exact projective parsing algorithms, which use efficient bottom-up chart parsing algorithms, the ChuLiu-Edmonds algorithm is greedy in nature. It begins by selecting the single best incoming dependency edge for each node j. It then post-processes the resulting graph to eliminate cycles and then continues recursively until a spanning tree (or valid dependency graph) results (see McDonald et al. (2005b) for details). The algorithm is trivially extended to the multidigraph case for use in labeled dependency parsing. First we note that if the maximum directed spanning</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="19967" citStr="Collins, 2002" startWordPosition="3476" endWordPosition="3477">rd to extend the algorithms of Eisner (1996) and Paskin (2001) to the labeled case adding only a factor of O(|L|n2). Table 1 gives an overview of the computational complexity for the three problems considered here for both the projective and non-projective case. We see that the non-projective case compares favorably for all three problems. 4 Applications To motivate the algorithms from Section 3, we present some important situations where each calculation is required. 4.1 Inference Based Learning Many learning paradigms can be defined as inference-based learning. These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al., 2005a). In these settings, a models parameters are iteratively updated based on the argmax calculation for a single or set of training instances under the current parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 Non-Projective Min-Risk Decoding In min-risk decoding the goal is to find the dependency graph for an input sentence x, that on avera</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T H Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT Press/McGraw-Hill.</publisher>
<contexts>
<context position="14433" citStr="Cormen et al., 1990" startWordPosition="2422" endWordPosition="2425">multi-digraph with nodes V = 10, 1, ... , n} and edges E. Define (Laplacian) matrix Q as a (n + 1)x(n + 1) matrix indexed from 0 to n. For all i and j, define: Qjj = X Xwk ij &amp; Qij = −wkij i�j,(i,j)kEE i=&apos;4j,(i,j)kEE If the ith row and column are removed from Q to produce the matrix Qi, then the sum of the weights of all directed spanning trees rooted at node i is equal to |Qi |(the determinant of Qi). Thus, if we construct Q for a graph Gx, then the determinant of the matrix Qc is equivalent to Zx. The determinant of an nxn matrix can be calculated in numerous ways, most of which take O(n3) (Cormen et al., 1990). The most efficient algorithms for calculating the determinant of a matrix use the fact that the problem is no harder than matrix multiplication (Cormen et al., 1990). Matrix multiplication currently has known O(n2.38) implementations and it has been widely conjectured that it can be solved in O(n2) (Robinson, 2005). However, most algorithms with sub-O(n3) running times require constants that are large enough to negate any asymptotic advantage for the case of dependency parsing. As a result, in this work we use O(n3) as the runtime for computing Zx. Since it takes O(|L|n2) to construct the ma</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. Introduction to Algorithms. MIT Press/McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>Y Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<publisher>JMLR.</publisher>
<contexts>
<context position="20023" citStr="Crammer and Singer, 2003" startWordPosition="3483" endWordPosition="3486">nd Paskin (2001) to the labeled case adding only a factor of O(|L|n2). Table 1 gives an overview of the computational complexity for the three problems considered here for both the projective and non-projective case. We see that the non-projective case compares favorably for all three problems. 4 Applications To motivate the algorithms from Section 3, we present some important situations where each calculation is required. 4.1 Inference Based Learning Many learning paradigms can be defined as inference-based learning. These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al., 2005a). In these settings, a models parameters are iteratively updated based on the argmax calculation for a single or set of training instances under the current parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 Non-Projective Min-Risk Decoding In min-risk decoding the goal is to find the dependency graph for an input sentence x, that on average has the lowest expected risk, 1: T = argmin w(T&apos;)R(T,</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>K. Crammer and Y. Singer. 2003. Ultraconservative online algorithms for multiclass problems. JMLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<volume>71</volume>
<pages>240</pages>
<contexts>
<context position="12118" citStr="Edmonds, 1967" startWordPosition="2000" endWordPosition="2001">rds between wi and wj). We will attempt to make any assumptions about the form wk ij clear when necessary. For the remainder of this section we discuss three crucial problems for learning and inference while showing that each can be computed tractably for the non-projective case. 3.1 Finding the Argmax The first problem of interest is finding the highest weighted tree for a given input sentence x Y T = argmax ij w TET(G.) (i,j)kEET McDonald et al. (2005b) showed that this can be solved in O(n2) for unlabeled parsing using the Chu-Liu-Edmonds algorithm for standard digraphs (Chu and Liu, 1965; Edmonds, 1967). Unlike most exact projective parsing algorithms, which use efficient bottom-up chart parsing algorithms, the ChuLiu-Edmonds algorithm is greedy in nature. It begins by selecting the single best incoming dependency edge for each node j. It then post-processes the resulting graph to eliminate cycles and then continues recursively until a spanning tree (or valid dependency graph) results (see McDonald et al. (2005b) for details). The algorithm is trivially extended to the multidigraph case for use in labeled dependency parsing. First we note that if the maximum directed spanning tree of a multi</context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="6189" citStr="Eisner, 1996" startWordPosition="953" endWordPosition="955"> Hamiltonian graph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong </context>
<context position="18583" citStr="Eisner (1996)" startWordPosition="3254" endWordPosition="3256">i = 0 and j &gt; 0 the only non zero term of this sum is when i&apos; = j&apos; = j and so ((0,j)k) = Zxwk0j((Q0)−1) Zx and (Q0)−1 can both be calculated a single time, each taking O(n3). Using these values, each expectation is computed in O(1). Coupled with with the fact that we need to construct Q and compute the expectation for all |L|n2 possible edges, in total it takes O(n3 + |L|n2) time to compute all edge expectations. 3.4 Comparison with Projective Parsing Projective dependency parsing algorithms are well understood due to their close connection to phrasebased chart parsing algorithms. The work of Eisner (1996) showed that the argmax problem for digraphs could be solved in O(n3) using a bottomup dynamic programming algorithm similar to CKY. Paskin (2001) presented an O(n3) inside-outside algorithm for projective dependency parsing using the Eisner algorithm as its backbone. Using this algorithm it is trivial to calculate both Zx and each ∂ log Zx ∂Zx ∂wk ij 1 Zx TET(G�) ∂ log |Q0 |�= ∂wk ij i0,j0�1 ∂(Q0)i0j0 jj 125 Projective Non-Projective O(n3 + |L|n2) O(|L|n2) O(n3 + |L|n2) O(n3 + |L|n2) O(n3 + |L|n2) O(n3 + |L|n2) Table 1: Comparison of runtime for non-projective and projective algorithms. edge </context>
<context position="38712" citStr="Eisner (1996)" startWordPosition="6891" endWordPosition="6892">th order Markovization can be embedded into a d + 1th Markovization. Thus, this rek k0 hiwij 130 sult also holds for any arbitrary Markovization. 6 Discussion In this paper we have shown that many important learning and inference problems can be solved efficiently for non-projective edge-factored dependency models by appealing to the Matrix Tree Theorem for multi-digraphs. These results extend the work of McDonald et al. (2005b) and help to further our understanding of when exact non-projective algorithms can be employed. When this analysis is coupled with the projective parsing algorithms of Eisner (1996) and Paskin (2001) we begin to get a clear picture of the complexity for data-driven dependency parsing within an edge-factored framework. To further justify the algorithms presented here, we outlined a few novel learning and inference settings in which they are required. However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems. We have provided further evidence for this by proving the hardness of incorporating arity constraints and horizontal/vertical edge Markovization, both of which incorporate information unavailabl</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hall</author>
<author>V N´ov´ak</author>
</authors>
<title>Corrective modeling for non-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. IWPT.</booktitle>
<marker>Hall, N´ov´ak, 2005</marker>
<rawString>K. Hall and V. N´ov´ak. 2005. Corrective modeling for non-projective dependency parsing. In Proc. IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hirakawa</author>
</authors>
<title>Graph branch algorithm: An optimum tree search method for scored dependency graph with arc co-occurrence constraints.</title>
<date>2006</date>
<booktitle>In Proc. ACL.</booktitle>
<publisher>Blackwell.</publisher>
<contexts>
<context position="7214" citStr="Hirakawa, 2006" startWordPosition="1107" endWordPosition="1108">orithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for</context>
</contexts>
<marker>Hirakawa, 2006</marker>
<rawString>H. Hirakawa. 2006. Graph branch algorithm: An optimum tree search method for scored dependency graph with arc co-occurrence constraints. In Proc. ACL. R. Hudson. 1984. Word Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudoprojectivity: A polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="7591" citStr="Kahane et al. (1998)" startWordPosition="1167" endWordPosition="1170">ve methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-th</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O Rambow. 1998. Pseudoprojectivity: A polynomially parsable non-projective dependency grammar. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact natural language parsing with a factored model.</title>
<date>2002</date>
<booktitle>In Proc. NIPS.</booktitle>
<contexts>
<context position="4324" citStr="Klein and Manning, 2002" startWordPosition="666" endWordPosition="670">must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a). The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting. We start by investigating and extending the edge-factored model of McDonald et al. (2005b). In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating b</context>
<context position="29541" citStr="Klein and Manning, 2002" startWordPosition="5151" endWordPosition="5154">ng algorithm for the model in Paskin (2001) working in the non-projective case. The projective case has been investigated in Paskin (2001). 5 Beyond Edge-factored Models We have shown that several computational problems related to parsing can be solved in polynomial time for the class of non-projective dependency models with the assumption that dependency relations are mutually independent. These independence assumptions are unwarranted, as it has already been established that modeling non-local information such as arity and nearby parsing decisions improves the accuracy of dependency models (Klein and Manning, 2002; McDonald and Pereira, 2006). In the spirit of our effort to understand the nature of exact non-projective algorithms, we examine dependency models that introduce arity constraints as well as permit edge decisions to be dependent on a ((i, j)k)x« 128 limited neighbourhood of other edges in the graph. Both kinds of models can no longer be considered edge-factored, since the likelihood of a dependency occurring in a particular analysis is now dependent on properties beyond the edge itself. 5.1 Arity One feature of the edge-factored models is that no restriction is imposed on the arity of the no</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C.D. Manning. 2002. Fast exact natural language parsing with a factored model. In Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="33038" citStr="Klein and Manning (2003)" startWordPosition="5815" endWordPosition="5818">grammar-driven literature given by Neuhaus and B¨oker (1997). In that work, an arity constraint is included in their minimal grammar. 5.2 Vertical and Horizontal Markovization In general, we would like to say that every dependency decision is dependent on every other edge in a graph. However, modeling dependency parsing in such a manner would be a computational nightmare. Instead, we would like to make a Markov assumption over the edges of the tree, in a similar way that a Markov assumption can be made for sequential classification problems in order to ensure tractable learning and inference. Klein and Manning (2003) distinguish between two kinds of Markovization for unlexicalized CFG parsing. The first is vertical Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at different levels in the phrase-structure tree. The second is horizontal Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at the same level in the tree. For dependency parsing there are analogous notions of vertical and horizontal Markovization for a given edge (i, j)k. First, let us define the vertical and h</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="25723" citStr="Klein and Manning (2004)" startWordPosition="4489" endWordPosition="4492">a recursive breadthfirst manner. Thus, pkx,y represents the probability of the word x generating its modifier y with label lk. This distribution is usually smoothed and is often conditioned on more information including the orientation of x relative to y (i.e., to the left/right) and distance between the two words. In the supervised setting this model can be trained with maximum likelihood estimation, which amounts to simple counts over the data. Learning in the unsupervised setting requires EM and is discussed in Section 4.4.2. Another generative dependency model of interest is that given by Klein and Manning (2004). In this model the sentence and tree are generated jointly, which allows one to drop the assumption that p(T |n) is uniform. This requires the addition to the model of parameters px,STOP for each x E E, with the normalization condition px,STOP + Py,k pkx,y = 1. It is possible to extend the model of Klein and Manning X w(T)I((i, j)k, T)fu(i, j, k) T ∈T (Gx) h(i, j)kixαfu(i, j, k) 127 (2004) to the non-projective case. However, the resulting distribution will be over multisets of words from the alphabet instead of strings. The discussion in this section is stated for the model in Paskin (2001);</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>D. Klein and C. Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="7968" citStr="Koo et al. (2007)" startWordPosition="1219" endWordPosition="1222">e the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1, ... ,l|L|} be a set of permissible synta</context>
<context position="17185" citStr="Koo et al. (2007)" startWordPosition="2974" endWordPosition="2977">assume that this edge is the only edge directed into the node j. In this case Q should be modified so that Qjj = 1, Qij = −1, and Qi0j = 0, Vi&apos; =� i, j (by the Matrix Tree Theorem). The value of Zx under this new Q will be equivalent to the weight of all trees containing the single edge from i to j with a weight of 1. For a specific edge (i, j)k its expectation is simply wk ijZx, since we can factor out the weight 1 edge from i to j in all the trees that contribute to Zx and multiply through the actual weight for the edge. The algorithm then reconstructs Q and continues. Following the work of Koo et al. (2007) and Smith and Smith (2007), it is possible to compute all expectations in O(n3 + |L|n2) through matrix inversion. To make this paper self contained, we report here their algorithm adapted to our notation. First, consider the equivalence, ∂ log Zx ∂Zx ∂wk ij w(T ) x I((i,j)k,T) wk ij As a result, we can re-write the edge expectations as, k k ∂ log Zx k ∂ log |Q0 | ((i,j) ) = Zxwij ∂wk= Zxwij ∂wk ij ij Using the chain rule, we get, ∂ log |Q0| ∂(Q0)i0j0 ∂wk ij We assume the rows and columns of Q0 are indexed from 1 so that the indexes of Q and Q0 coincide. To calculate ((i, j)k) when i, j &gt; 0, w</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="22202" citStr="Lafferty et al., 2001" startWordPosition="3861" endWordPosition="3864"> = argmin w(T0)R(T, T0) T ∈T (Gx) T 0∈T (Gx) X X w(T0)[n − I((i, j)k, T0)] T ∈T (Gx) T 0∈T (Gx) (i,j)k∈ET w(T0) X I((i, j)k, T0) (i,j)k∈ET w(T0)I((i, j)k, T0) X X = argmax w(T0)I((i, j)k, T0) T ∈T (Gx) (i,j)k∈ET T 0∈T (Gx) Y PT 0∈T (Gx) w(T 0)I((i,j)k,T 0) = argmax e T ∈T (Gx) (i,j)k∈ET Y h(i,j)kix = argmax e T ∈T (Gx) (i,j)k∈ET By setting the edge weights to wk = e((i j)k). we ij can directly solve this problem using the edge expectation algorithm described in Section 3.3 and the argmax algorithm described in Section 3.1. 4.3 Non-Projective Log-Linear Models Conditional Random Fields (CRFs) (Lafferty et al., 2001) are global discriminative learning algorithms for problems with structured output spaces, such as dependency parsing. For dependency parsing, CRFs would define the conditional probability of a dependency graph T for a sentence x as a globally norargmax Zx ((2, j)k)x = argmin = argmin X− T ∈T (Gx) T0∈T (Gx) = argmin X− T ∈T (Gx) (i,j)k∈ET X T 0∈T (Gx) 126 malized log-linear model, Q (i,j)k∈ET ew·f(i,j,k) p(T|x) = PT&apos;∈T(G.) Q(i,j)k∈ET&apos; ew f(i,j,k) -7� k H(i j)k∈ET wij k PT&apos;∈T(G.) Qj(i)k∈ET&apos; wij w(T) = Z. Here, the weights wk ij are potential functions over each edge defined as an exponentiated </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1343" citStr="Marcus et al., 1993" startWordPosition="189" endWordPosition="192"> tractable for any model richer than the edge-factored model. 1 Introduction Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic dependencies through directed graphs. These representations have been thoroughly studied in descriptive linguistics (Tesni`ere, 1959; Hudson, 1984; Sgall et al., 1986; Me´lˇcuk, 1988) and have been applied in numerous language processing tasks. Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (Marcus et al., 1993). Each edge in this graph represents a single syntactic dependency directed from a word to its modifier. In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun. To simplify computation and some important definitions, an artificial token is inserted into the sentence as the left most word and will always represent the root of the dependency graph. We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005). The dependency graph i</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc EACL.</booktitle>
<contexts>
<context position="4353" citStr="McDonald and Pereira, 2006" startWordPosition="671" endWordPosition="674">s are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a). The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting. We start by investigating and extending the edge-factored model of McDonald et al. (2005b). In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function an</context>
<context position="5752" citStr="McDonald and Pereira (2006)" startWordPosition="886" endWordPosition="889">erence problems including min-risk decoding, training globally normalized log-linear models, syntactic language modeling, and unsupervised learning via the EM algorithm – none of which have previously been known to have exact non-projective implementations. We then switch focus to models that account for non-local information, in particular arity and neighbouring parse decisions. For systems that model arity constraints we give a reduction from the Hamiltonian graph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N</context>
<context position="7087" citStr="McDonald and Pereira, 2006" startWordPosition="1087" endWordPosition="1091">n the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. No</context>
<context position="29570" citStr="McDonald and Pereira, 2006" startWordPosition="5155" endWordPosition="5158">l in Paskin (2001) working in the non-projective case. The projective case has been investigated in Paskin (2001). 5 Beyond Edge-factored Models We have shown that several computational problems related to parsing can be solved in polynomial time for the class of non-projective dependency models with the assumption that dependency relations are mutually independent. These independence assumptions are unwarranted, as it has already been established that modeling non-local information such as arity and nearby parsing decisions improves the accuracy of dependency models (Klein and Manning, 2002; McDonald and Pereira, 2006). In the spirit of our effort to understand the nature of exact non-projective algorithms, we examine dependency models that introduce arity constraints as well as permit edge decisions to be dependent on a ((i, j)k)x« 128 limited neighbourhood of other edges in the graph. Both kinds of models can no longer be considered edge-factored, since the likelihood of a dependency occurring in a particular analysis is now dependent on properties beyond the edge itself. 5.1 Arity One feature of the edge-factored models is that no restriction is imposed on the arity of the nodes in the dependency trees. </context>
<context position="34392" citStr="McDonald and Pereira (2006)" startWordPosition="6029" endWordPosition="6032">ses through (i, j)k. The horizontal neighbourhood contains all edges (i, j&apos;)k�. Figure 4 graphically displays the vertical and horizontal neighbourhoods for an edge in the dependency graph from Figure 1. Vertical and horizontal Markovization essentially allow the score of the graph to factor over a larger scope of edges, provided those edges are in the same vertical or horizontal neighbourhood. A dth order factorization is one in which the score factors only over the d nearest edges in the neighbourhoods. In 129 Figure 4: Vertical and Horizontal neighbourhood for the edge from will to remain. McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. In this study we complete the picture and show that vertical Markovization is also FNP-hard. Consider a first-order vertical Markovization in which the score for a dependency graph factors over pairs of vertically adjacent edges2, � w(T) = (h,i)k,(i,j)k0∈ET where k hiwk0 ij is the weight of including both edges (h, i)k and (i, j)k0 in the dependency graph. Note that this formulation does not include any contributions from dependencies that have no vertically adjacent neighbours, i.e., any edge (0, </context>
<context position="35639" citStr="McDonald and Pereira (2006)" startWordPosition="6272" endWordPosition="6275">is no edge (i, j)k0 in the graph. We can easily rectify this by inserting a second root node, say 00, and including the weights k000wk0 0i. To ensure that only valid dependency graphs get a weight greater than zero, we can set k hiwk0 ij = 0 if i = 00 and k 00iwk0 ij = 0 if i =�0. Now, consider the NP-complete 3D-matching problem (3DM). As input we are given three sets of size m, call them A, B and C, and a set S C_ A x B x C. The 3DM problem asks if there is a set S0 C_ S such that |S0 |= m and for any two tuples (a, b, c), (a0, b0, c0) E S0 it is the case that a =� a0, b =� b0, and c =�c0. 2McDonald and Pereira (2006) define this as a second-order Markov assumption. This is simply a difference in terminology and does not represent any meaningful distinction. We can reduce the 3D-matching problem to the first-order vertical Markov parsing problem by constructing a graph G = (V, E), such that L = AUBUC,V = {00,0} U A U B U C and E = {(i, j)k |i, j E V, k E L}. The set E contains multiple edges between ever pair of nodes, each edge taking on a label representing a single element of the set A U B U C. Now, define k 000wk0 0a = 1, for all a E A and k, k0 E A U B U C, and b0awcab = 1, for all a E A and b E B and</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. McDonald and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3878" citStr="McDonald et al., 2005" startWordPosition="599" endWordPosition="602">the model to new languages. One interesting class of data-driven models are 121 Proceedings of the 10th Conference on Parsing Technologies, pages 121–132, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Figure 1: A projective dependency graph. Figure 2: Non-projective dependency graph. those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations ov</context>
<context position="6278" citStr="McDonald et al., 2005" startWordPosition="966" endWordPosition="969"> this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald</context>
<context position="9826" citStr="McDonald et al. (2005" startWordPosition="1589" endWordPosition="1592">pendency graph of x must be a subgraph of Gx. Let i →+ j be a relation that is true if and only if there is a non-empty directed path from node i to node j in some graph under consideration. A directed spanning tree1 of a graph G, that originates 1A directed spanning tree is commonly referred to as a arborescence in the graph theory literature. out of node 0, is any subgraph T = (VT, ET) such that, • VT Vx and ET ⊆ Ex • ∀j ∈ VT, 0 →+ j if and only if j =6 0 • If (i, j)k ∈ ET, then (i&apos;, j)k&apos; ∈/ ET, ∀i&apos; =6 i and/or k&apos; =6 k. Define T(G) as the set of all directed spanning trees for a graph G. As McDonald et al. (2005b) noted, there is a one-to-one correspondence between spanning trees of Gx and labeled dependency graphs of x, i.e., T(Gx) is exactly the set of all possible projective and non-projective dependency graphs for sentence x. Throughout the rest of this paper, we will refer to any T ∈ T (Gx) as a valid dependency graph for a sentence x. Thus, by definition, every valid dependency graph must be a tree. 3 Edge-factored Models In this section we examine the class of models that assume each dependency decision is independent. Within this setting, every edge in an induced graph Gx for a sentence x wil</context>
<context position="11961" citStr="McDonald et al. (2005" startWordPosition="1974" endWordPosition="1977">ted with a label lk given that the word being modified is wi (possibly with some other information such as the orientation of the dependency 123 or the number of words between wi and wj). We will attempt to make any assumptions about the form wk ij clear when necessary. For the remainder of this section we discuss three crucial problems for learning and inference while showing that each can be computed tractably for the non-projective case. 3.1 Finding the Argmax The first problem of interest is finding the highest weighted tree for a given input sentence x Y T = argmax ij w TET(G.) (i,j)kEET McDonald et al. (2005b) showed that this can be solved in O(n2) for unlabeled parsing using the Chu-Liu-Edmonds algorithm for standard digraphs (Chu and Liu, 1965; Edmonds, 1967). Unlike most exact projective parsing algorithms, which use efficient bottom-up chart parsing algorithms, the ChuLiu-Edmonds algorithm is greedy in nature. It begins by selecting the single best incoming dependency edge for each node j. It then post-processes the resulting graph to eliminate cycles and then continues recursively until a spanning tree (or valid dependency graph) results (see McDonald et al. (2005b) for details). The algori</context>
<context position="20046" citStr="McDonald et al., 2005" startWordPosition="3487" endWordPosition="3490">beled case adding only a factor of O(|L|n2). Table 1 gives an overview of the computational complexity for the three problems considered here for both the projective and non-projective case. We see that the non-projective case compares favorably for all three problems. 4 Applications To motivate the algorithms from Section 3, we present some important situations where each calculation is required. 4.1 Inference Based Learning Many learning paradigms can be defined as inference-based learning. These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al., 2005a). In these settings, a models parameters are iteratively updated based on the argmax calculation for a single or set of training instances under the current parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 Non-Projective Min-Risk Decoding In min-risk decoding the goal is to find the dependency graph for an input sentence x, that on average has the lowest expected risk, 1: T = argmin w(T&apos;)R(T, T&apos;) TET(G-) TET(G.) wh</context>
<context position="21528" citStr="McDonald et al., 2005" startWordPosition="3735" endWordPosition="3738">t the entire space of parse structures. Instead, this set is usually restricted to a small number of possible trees that have been preselected by some baseline system. In this subsection we show that when the risk function is of a specific form, this restriction can be dropped. The result is an exact min-risk decoding procedure. Let R(T, T&apos;) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 ··· xn, R(T, T&apos;) = n − 1: j((2,j)k,T&apos;) (i j)kEET This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al., 2005a; Buchholz et al., 2006). Some algebra reveals, X T = argmin w(T0)R(T, T0) T ∈T (Gx) T 0∈T (Gx) X X w(T0)[n − I((i, j)k, T0)] T ∈T (Gx) T 0∈T (Gx) (i,j)k∈ET w(T0) X I((i, j)k, T0) (i,j)k∈ET w(T0)I((i, j)k, T0) X X = argmax w(T0)I((i, j)k, T0) T ∈T (Gx) (i,j)k∈ET T 0∈T (Gx) Y PT 0∈T (Gx) w(T 0)I((i,j)k,T 0) = argmax e T ∈T (Gx) (i,j)k∈ET Y h(i,j)kix = argmax e T ∈T (Gx) (i,j)k∈ET By setting the edge weights to wk = e((i j)k). we ij can directly solve this problem using the edge expectation algorithm described in Section 3.3 and the argmax algorithm described in Section 3.1. 4.3 Non-Projective </context>
<context position="38529" citStr="McDonald et al. (2005" startWordPosition="6860" endWordPosition="6863">joint paths represent a single tuple in a 3DM. Thus, if there is a non-zero weighted graph in T(G), then it must directly correspond to a valid 3DM, which concludes the proof. Note that any dth order Markovization can be embedded into a d + 1th Markovization. Thus, this rek k0 hiwij 130 sult also holds for any arbitrary Markovization. 6 Discussion In this paper we have shown that many important learning and inference problems can be solved efficiently for non-projective edge-factored dependency models by appealing to the Matrix Tree Theorem for multi-digraphs. These results extend the work of McDonald et al. (2005b) and help to further our understanding of when exact non-projective algorithms can be employed. When this analysis is coupled with the projective parsing algorithms of Eisner (1996) and Paskin (2001) we begin to get a clear picture of the complexity for data-driven dependency parsing within an edge-factored framework. To further justify the algorithms presented here, we outlined a few novel learning and inference settings in which they are required. However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems. We have pro</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proc. HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meil˘a</author>
<author>T Jaakkola</author>
</authors>
<title>Tractable Bayesian learning of tree belief networks.</title>
<date>2000</date>
<booktitle>In Proc. UAI. I.A. Me´lˇcuk.</booktitle>
<publisher>Press.</publisher>
<institution>State University of New York</institution>
<marker>Meil˘a, Jaakkola, 2000</marker>
<rawString>M. Meil˘a and T. Jaakkola. 2000. Tractable Bayesian learning of tree belief networks. In Proc. UAI. I.A. Me´lˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>H Fox</author>
<author>L A Ramshaw</author>
<author>R M Weischedel</author>
</authors>
<title>A novel use of statistical parsing to extract information from text.</title>
<date>2000</date>
<booktitle>In Proc NAACL,</booktitle>
<pages>226--233</pages>
<contexts>
<context position="40603" citStr="Miller et al., 2000" startWordPosition="7163" endWordPosition="7166">ds algorithm) or based on the calculation of the determinant of a matrix (i.e., the partition function and edge expectations). Thus, the existence of bottom-up chart parsing algorithms for projective dependency parsing provides many advantages. As mentioned above, it permits simple augmentation techniques to incorporate non-local information such as arity constraints and Markovization. It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework, including information extraction (Miller et al., 2000) and syntax-based machine translation (Wu, 1996). The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case. Otherwise we should be able to augment them and move beyond edgefactored models without encountering intractability – just like the projective case. An interesting line of research is to investigate classes of non-projective structures that can be parsed with chart-parsing algorithms and how these classes relate to the languages parsable by other syntactic formalisms. Acknowledgments Thanks to Ben Taskar for pointing out </context>
</contexts>
<marker>Miller, Fox, Ramshaw, Weischedel, 2000</marker>
<rawString>S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel. 2000. A novel use of statistical parsing to extract information from text. In Proc NAACL, pages 226–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>N B¨oker</author>
</authors>
<title>The complexity of recognition of linguistically adequate dependency grammars.</title>
<date>1997</date>
<booktitle>In Proc. ACL.</booktitle>
<marker>Neuhaus, B¨oker, 1997</marker>
<rawString>P. Neuhaus and N. B¨oker. 1997. The complexity of recognition of linguistically adequate dependency grammars. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Pseudo-projective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="6340" citStr="Nivre and Nilsson, 2005" startWordPosition="974" endWordPosition="977">work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have a</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="6255" citStr="Nivre and Scholz, 2004" startWordPosition="962" endWordPosition="965">roblem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models </context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of english text. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Dependency grammar and dependency parsing.</title>
<date>2005</date>
<tech>Technical Report MSI report 05133,</tech>
<institution>V¨axj¨o University: School of Mathematics and Systems Engineering.</institution>
<contexts>
<context position="1919" citStr="Nivre, 2005" startWordPosition="287" endWordPosition="288"> Penn Treebank (Marcus et al., 1993). Each edge in this graph represents a single syntactic dependency directed from a word to its modifier. In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun. To simplify computation and some important definitions, an artificial token is inserted into the sentence as the left most word and will always represent the root of the dependency graph. We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005). The dependency graph in Figure 1 is an example of a nested or projective graph. Under the assumption that the root of the graph is the left most word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing. Conversely, a non-projective dependency graph does not satisfy this property. Figure 2 gives an example of a nonprojective graph for a sentence that has also been extracted from the Penn Treebank. Non-projectivity arises due to long distance dependencies or in languages with flexible word order. For many languages,</context>
</contexts>
<marker>Nivre, 2005</marker>
<rawString>J. Nivre. 2005. Dependency grammar and dependency parsing. Technical Report MSI report 05133, V¨axj¨o University: School of Mathematics and Systems Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Paskin</author>
</authors>
<title>Cubic-time parsing and learning algorithms for grammatical bigram models.</title>
<date>2001</date>
<tech>Technical Report UCB/CSD-01-1148,</tech>
<institution>Computer Science Division, University of California Berkeley.</institution>
<contexts>
<context position="3855" citStr="Paskin, 2001" startWordPosition="597" endWordPosition="598">when adapting the model to new languages. One interesting class of data-driven models are 121 Proceedings of the 10th Conference on Parsing Technologies, pages 121–132, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics Figure 1: A projective dependency graph. Figure 2: Non-projective dependency graph. those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich fea</context>
<context position="6203" citStr="Paskin, 2001" startWordPosition="956" endWordPosition="957">raph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence a</context>
<context position="10854" citStr="Paskin (2001)" startWordPosition="1777" endWordPosition="1778">ed Models In this section we examine the class of models that assume each dependency decision is independent. Within this setting, every edge in an induced graph Gx for a sentence x will have an associated weight wk ij ≥ 0 that maps the kth directed edge from node i to node j to a real valued numerical weight. These weights represents the likelihood of a dependency occurring from word wi to word wj with label lk. Define the weight of a spanning tree T = (VT, ET) as the product of the edge weights w(T) = � wk ij (ij)kEET It is easily shown that this formulation includes the projective model of Paskin (2001) and the nonprojective model of McDonald et al. (2005b). The definition of wk ij depends on the context in which it is being used. For example, in the work of McDonald et al. (2005b) it is simply a linear classifier that is a function of the words in the dependency, the label of the dependency, and any contextual features of the words in the sentence. In a generative probabilistic model (such as Paskin (2001)) it could represent the conditional probability of a word wj being generated with a label lk given that the word being modified is wi (possibly with some other information such as the ori</context>
<context position="18729" citStr="Paskin (2001)" startWordPosition="3280" endWordPosition="3281">gle time, each taking O(n3). Using these values, each expectation is computed in O(1). Coupled with with the fact that we need to construct Q and compute the expectation for all |L|n2 possible edges, in total it takes O(n3 + |L|n2) time to compute all edge expectations. 3.4 Comparison with Projective Parsing Projective dependency parsing algorithms are well understood due to their close connection to phrasebased chart parsing algorithms. The work of Eisner (1996) showed that the argmax problem for digraphs could be solved in O(n3) using a bottomup dynamic programming algorithm similar to CKY. Paskin (2001) presented an O(n3) inside-outside algorithm for projective dependency parsing using the Eisner algorithm as its backbone. Using this algorithm it is trivial to calculate both Zx and each ∂ log Zx ∂Zx ∂wk ij 1 Zx TET(G�) ∂ log |Q0 |�= ∂wk ij i0,j0�1 ∂(Q0)i0j0 jj 125 Projective Non-Projective O(n3 + |L|n2) O(|L|n2) O(n3 + |L|n2) O(n3 + |L|n2) O(n3 + |L|n2) O(n3 + |L|n2) Table 1: Comparison of runtime for non-projective and projective algorithms. edge expectation. Crucially, the nested property of projective structures allows edge expectations to be computed in O(n3) from the inside-outside valu</context>
<context position="24846" citStr="Paskin (2001)" startWordPosition="4342" endWordPosition="4343">ective Generative Parsing Models A generative probabilistic dependency model over some alphabet E consists of parameters pkx,y associated with each dependency from word x E E to word y E E with label lk E L. In addition, we impose 0 &lt; pkx,y &lt; 1 and the normalization conditions k = 1 for each x E E. We define a enPy,k px,y g erative probability model p over trees T E T (G.) and a sentence x = x0x1 · · · xn conditioned on the sentence length, which is always known, p(x,T|n) = p(x|T,n)p(T|n) Y= pkxi,xj p(T |n) (i,j)k∈ET We assume that p(T |n) = β is uniform. This model is studied specifically by Paskin (2001). In this model, one can view the sentence as being generated recursively in a top-down process. First, a tree is generated from the distribution p(T |n). Then starting at the root of the tree, every word generates all of its modifiers independently in a recursive breadthfirst manner. Thus, pkx,y represents the probability of the word x generating its modifier y with label lk. This distribution is usually smoothed and is often conditioned on more information including the orientation of x relative to y (i.e., to the left/right) and distance between the two words. In the supervised setting this</context>
<context position="26322" citStr="Paskin (2001)" startWordPosition="4600" endWordPosition="4601">Manning (2004). In this model the sentence and tree are generated jointly, which allows one to drop the assumption that p(T |n) is uniform. This requires the addition to the model of parameters px,STOP for each x E E, with the normalization condition px,STOP + Py,k pkx,y = 1. It is possible to extend the model of Klein and Manning X w(T)I((i, j)k, T)fu(i, j, k) T ∈T (Gx) h(i, j)kixαfu(i, j, k) 127 (2004) to the non-projective case. However, the resulting distribution will be over multisets of words from the alphabet instead of strings. The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Manning (2004). 4.4.1 Language Modeling A generative model of dependency structure might be used to determine the probability of a sentence x by marginalizing out all possible dependency trees, p(x|n) = X p(x,T|n) T∈T (Gx) X= p(x|T, n)p(T |n) T∈T (Gx) = β X Y pkxi,xj = βZx T∈T (Gx) (i,j)k∈ET This probability can be used directly as a nonprojective syntactic language model (Chelba et al., 1997) or possibly interpolated with a separate ngram model. 4.4.2 Unsupervised Learning In unsupervised learning we train our model on a sample</context>
<context position="28961" citStr="Paskin (2001)" startWordPosition="5068" endWordPosition="5069">to some local maxima (with the exception of saddle points). Observe that at each iteration we can compute quantities ((i, j)k)x« and Zx« in polynomial time using the algorithms from Section 3 with pkx«i,x«j in place of wki,j. Furthermore, under some standard conditions the fixed-point iteration method guarantees a constant number of bits of precision gain for the parameters at each iteration, resulting in overall polynomial time computation in the size of the input and in the required number of bits for the precision. As far as we know, this is the first EM learning algorithm for the model in Paskin (2001) working in the non-projective case. The projective case has been investigated in Paskin (2001). 5 Beyond Edge-factored Models We have shown that several computational problems related to parsing can be solved in polynomial time for the class of non-projective dependency models with the assumption that dependency relations are mutually independent. These independence assumptions are unwarranted, as it has already been established that modeling non-local information such as arity and nearby parsing decisions improves the accuracy of dependency models (Klein and Manning, 2002; McDonald and Perei</context>
<context position="38730" citStr="Paskin (2001)" startWordPosition="6894" endWordPosition="6895">tion can be embedded into a d + 1th Markovization. Thus, this rek k0 hiwij 130 sult also holds for any arbitrary Markovization. 6 Discussion In this paper we have shown that many important learning and inference problems can be solved efficiently for non-projective edge-factored dependency models by appealing to the Matrix Tree Theorem for multi-digraphs. These results extend the work of McDonald et al. (2005b) and help to further our understanding of when exact non-projective algorithms can be employed. When this analysis is coupled with the projective parsing algorithms of Eisner (1996) and Paskin (2001) we begin to get a clear picture of the complexity for data-driven dependency parsing within an edge-factored framework. To further justify the algorithms presented here, we outlined a few novel learning and inference settings in which they are required. However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems. We have provided further evidence for this by proving the hardness of incorporating arity constraints and horizontal/vertical edge Markovization, both of which incorporate information unavailable to an edgefactor</context>
</contexts>
<marker>Paskin, 2001</marker>
<rawString>M.A. Paskin. 2001. Cubic-time parsing and learning algorithms for grammatical bigram models. Technical Report UCB/CSD-01-1148, Computer Science Division, University of California Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riedel</author>
<author>J Clarke</author>
</authors>
<title>Incremental integer linear programming for non-projective dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="7166" citStr="Riedel and Clarke, 2006" startWordPosition="1100" endWordPosition="1103">y through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defi</context>
</contexts>
<marker>Riedel, Clarke, 2006</marker>
<rawString>S. Riedel and J. Clarke. 2006. Incremental integer linear programming for non-projective dependency parsing. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robinson</author>
</authors>
<title>Toward an optimal algorithm for matrix multiplication.</title>
<date>2005</date>
<journal>News Journal of the Society for Industrial and Applied Mathematics,</journal>
<volume>38</volume>
<issue>9</issue>
<contexts>
<context position="14751" citStr="Robinson, 2005" startWordPosition="2475" endWordPosition="2476">directed spanning trees rooted at node i is equal to |Qi |(the determinant of Qi). Thus, if we construct Q for a graph Gx, then the determinant of the matrix Qc is equivalent to Zx. The determinant of an nxn matrix can be calculated in numerous ways, most of which take O(n3) (Cormen et al., 1990). The most efficient algorithms for calculating the determinant of a matrix use the fact that the problem is no harder than matrix multiplication (Cormen et al., 1990). Matrix multiplication currently has known O(n2.38) implementations and it has been widely conjectured that it can be solved in O(n2) (Robinson, 2005). However, most algorithms with sub-O(n3) running times require constants that are large enough to negate any asymptotic advantage for the case of dependency parsing. As a result, in this work we use O(n3) as the runtime for computing Zx. Since it takes O(|L|n2) to construct the matrix Q, the entire runtime to compute Zx is O(n3 + |L|n2). 3.3 Edge Expectations Another important problem for various learning paradigms is to calculate the expected value of each edge for an input sentence x, ((i,j)k)x = X w(T) x I((i, j)k,T) TET(G.) X Y k w(T) = (i,j)kEET wi,j TET(G.) 124 Input: x = x0x1 · · · xn </context>
</contexts>
<marker>Robinson, 2005</marker>
<rawString>S. Robinson. 2005. Toward an optimal algorithm for matrix multiplication. News Journal of the Society for Industrial and Applied Mathematics, 38(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Hajiˇcov´a</author>
<author>J Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Pragmatic Aspects.</title>
<date>1986</date>
<publisher>Reidel.</publisher>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>P. Sgall, E. Hajiˇcov´a, and J. Panevov´a. 1986. The Meaning of the Sentence in Its Pragmatic Aspects. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="7995" citStr="Smith and Smith (2007)" startWordPosition="1224" endWordPosition="1227"> Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1, ... ,l|L|} be a set of permissible syntactic edge labels and x = x0</context>
<context position="17212" citStr="Smith and Smith (2007)" startWordPosition="2979" endWordPosition="2982">is the only edge directed into the node j. In this case Q should be modified so that Qjj = 1, Qij = −1, and Qi0j = 0, Vi&apos; =� i, j (by the Matrix Tree Theorem). The value of Zx under this new Q will be equivalent to the weight of all trees containing the single edge from i to j with a weight of 1. For a specific edge (i, j)k its expectation is simply wk ijZx, since we can factor out the weight 1 edge from i to j in all the trees that contribute to Zx and multiply through the actual weight for the edge. The algorithm then reconstructs Q and continues. Following the work of Koo et al. (2007) and Smith and Smith (2007), it is possible to compute all expectations in O(n3 + |L|n2) through matrix inversion. To make this paper self contained, we report here their algorithm adapted to our notation. First, consider the equivalence, ∂ log Zx ∂Zx ∂wk ij w(T ) x I((i,j)k,T) wk ij As a result, we can re-write the edge expectations as, k k ∂ log Zx k ∂ log |Q0 | ((i,j) ) = Zxwij ∂wk= Zxwij ∂wk ij ij Using the chain rule, we get, ∂ log |Q0| ∂(Q0)i0j0 ∂wk ij We assume the rows and columns of Q0 are indexed from 1 so that the indexes of Q and Q0 coincide. To calculate ((i, j)k) when i, j &gt; 0, we can use the fact that ∂ l</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D.A. Smith and N.A. Smith. 2007. Probabilistic models of nonprojective dependency trees. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesni`ere</author>
</authors>
<title>El´ements de syntaxe structurale.</title>
<date>1959</date>
<journal>Editions Klincksieck.</journal>
<marker>Tesni`ere, 1959</marker>
<rawString>L. Tesni`ere. 1959. ´El´ements de syntaxe structurale. Editions Klincksieck.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Titov</author>
<author>J Henderson</author>
</authors>
<title>Bayes risk minimization in natural language parsing.</title>
<date>2006</date>
<institution>University of Geneva</institution>
<note>technical report.</note>
<contexts>
<context position="20830" citStr="Titov and Henderson, 2006" startWordPosition="3612" endWordPosition="3615"> parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 Non-Projective Min-Risk Decoding In min-risk decoding the goal is to find the dependency graph for an input sentence x, that on average has the lowest expected risk, 1: T = argmin w(T&apos;)R(T, T&apos;) TET(G-) TET(G.) where R is a risk function measuring the error between two graphs. Min-risk decoding has been studied for both phrase-structure parsing and dependency parsing (Titov and Henderson, 2006). In that work, as is common with many min-risk decoding schemes, T(Gx) is not the entire space of parse structures. Instead, this set is usually restricted to a small number of possible trees that have been preselected by some baseline system. In this subsection we show that when the risk function is of a specific form, this restriction can be dropped. The result is an exact min-risk decoding procedure. Let R(T, T&apos;) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 ··· xn, R(T, T&apos;) = n − 1: j((2,j)k,T&apos;) (i j)kEET This is a common definition of risk between t</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>I. Titov and J. Henderson. 2006. Bayes risk minimization in natural language parsing. University of Geneva technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W T Tutte</author>
</authors>
<title>Graph Theory.</title>
<date>1984</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13800" citStr="Tutte, 1984" startWordPosition="2293" endWordPosition="2294">k-best argmax problem for digraphs can be solved in O(kn2) (Camerini et al., 1980). This can also be easily extended to the multidigraph case for labeled parsing. 3.2 Partition Function A common step in many learning algorithms is to compute the sum over the weight of all the possible outputs for a given input x. This value is often referred to as the partition function due to its similarity with a value by the same name in statistical mechanics. We denote this value as Zx, XZx = TET(G.) To compute this sum it is possible to use the Matrix Tree Theorem for multi-digraphs, Matrix Tree Theorem (Tutte, 1984): Let G be a multi-digraph with nodes V = 10, 1, ... , n} and edges E. Define (Laplacian) matrix Q as a (n + 1)x(n + 1) matrix indexed from 0 to n. For all i and j, define: Qjj = X Xwk ij &amp; Qij = −wkij i�j,(i,j)kEE i=&apos;4j,(i,j)kEE If the ith row and column are removed from Q to produce the matrix Qi, then the sum of the weights of all directed spanning trees rooted at node i is equal to |Qi |(the determinant of Qi). Thus, if we construct Q for a graph Gx, then the determinant of the matrix Qc is equivalent to Zx. The determinant of an nxn matrix can be calculated in numerous ways, most of which</context>
</contexts>
<marker>Tutte, 1984</marker>
<rawString>W.T. Tutte. 1984. Graph Theory. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>M P Harper</author>
</authors>
<title>A statistical constraint dependency grammar (CDG) parser.</title>
<date>2004</date>
<booktitle>In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL).</booktitle>
<contexts>
<context position="7387" citStr="Wang and Harper (2004)" startWordPosition="1132" endWordPosition="1135">sually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a mini122 mal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smit</context>
</contexts>
<marker>Wang, Harper, 2004</marker>
<rawString>W. Wang and M. P. Harper. 2004. A statistical constraint dependency grammar (CDG) parser. In Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>A polynomial-time algorithm for statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="40651" citStr="Wu, 1996" startWordPosition="7171" endWordPosition="7172">t of a matrix (i.e., the partition function and edge expectations). Thus, the existence of bottom-up chart parsing algorithms for projective dependency parsing provides many advantages. As mentioned above, it permits simple augmentation techniques to incorporate non-local information such as arity constraints and Markovization. It also ensures the compatibility of projective parsing algorithms with many important natural language processing methods that work within a bottom-up chart parsing framework, including information extraction (Miller et al., 2000) and syntax-based machine translation (Wu, 1996). The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case. Otherwise we should be able to augment them and move beyond edgefactored models without encountering intractability – just like the projective case. An interesting line of research is to investigate classes of non-projective structures that can be parsed with chart-parsing algorithms and how these classes relate to the languages parsable by other syntactic formalisms. Acknowledgments Thanks to Ben Taskar for pointing out the work of Meil˘a and Jaakkola (2000). Thanks t</context>
</contexts>
<marker>Wu, 1996</marker>
<rawString>D. Wu. 1996. A polynomial-time algorithm for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. IWPT.</booktitle>
<contexts>
<context position="6231" citStr="Yamada and Matsumoto, 2003" startWordPosition="958" endWordPosition="961">uggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case f</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>