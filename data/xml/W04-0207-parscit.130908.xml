<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.947039">
Text Type Structure and Logical Document Structure
</title>
<author confidence="0.941866">
Hagen Langer
</author>
<affiliation confidence="0.73152">
Justus-Liebig-Universit¨at/
</affiliation>
<address confidence="0.416991">
Universit¨at Osnabr¨uck
</address>
<email confidence="0.844277">
hagen.langer@web.de
</email>
<author confidence="0.667859">
Harald L¨ungen
</author>
<affiliation confidence="0.533523">
Justus-Liebig-Universit¨at
Gießen, Germany
</affiliation>
<note confidence="0.415851">
luengen@uni-giessen.de
</note>
<author confidence="0.483652">
Petra Saskia Bayerl
</author>
<affiliation confidence="0.35425">
Justus-Liebig-Universit¨at
Gießen, Germany
</affiliation>
<email confidence="0.225532">
bayerl@uni-giessen.de
</email>
<sectionHeader confidence="0.985131" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934695652174">
Most research on automated categorization of doc-
uments has concentrated on the assignment of one
or many categories to a whole text. However, new
applications, e.g. in the area of the Semantic Web,
require a richer and more fine-grained annotation
of documents, such as detailed thematic informa-
tion about the parts of a document. Hence we in-
vestigate the automatic categorization of text seg-
ments of scientific articles with XML markup into
16 topic types from a text type structure schema. A
corpus of 47 linguistic articles was provided with
XML markup on different annotation layers repre-
senting text type structure, logical document struc-
ture, and grammatical categories. Six different fea-
ture extraction strategies were applied to this corpus
and combined in various parametrizations in differ-
ent classifiers. The aim was to explore the contribu-
tion of each type of information, in particular the
logical structure features, to the classification ac-
curacy. The results suggest that some of the topic
types of our hierarchy are successfully learnable,
while the features from the logical structure layer
had no particular impact on the results.
</bodyText>
<sectionHeader confidence="0.999466" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999232833333333">
Our project Semantics of Generic Document Struc-
tures is concerned with the text type structure of sci-
entific articles and its relations to document gram-
mars and markup. One of its goals is to explore
the feasibility of an automatic categorization of
text segments of scientific articles which are an-
notated with logical structure tags (e.g. section,
paragraph, appendix) into topic types such as
background, researchTopic, method, and results de-
fined in a hierarchical text type schema. The schema
representing the text type structure (or, thematic
structure) of scientific articles is shown in Figure
1 (explained more fully in section 2). It is as-
sumed that authors to some degree adhere to such
a schema when creating a logical structure for their
documents for instance by XML markup, and that
therefore such markup bears clues as to the thematic
structure of scientific articles.
</bodyText>
<figureCaption confidence="0.997549">
Figure 1: Text type schema
</figureCaption>
<bodyText confidence="0.986957175">
Automatic document classification is an ad-
vanced field within computational linguistics with
many practical applications and has yielded a
wealth of standard methods and tools over the past
10 years. Many systems have been developed on
the Reuters-21578 corpus containing 21578 English
newpaper articles manually categorized into 135
topic categories, e.g. (Zu et al., 2003). Besides
newspaper articles, some approaches have treated
the automatic categorization of (HTML) hypertexts
available on the W3 into universal text topic sets
such as the ones used in the LookSmart and Yahoo
web directories (Dumais and Chen, 2000). An ap-
proach that focuses solely on research papers is
CiteSeer (Giles et al., 1998), the online digital li-
brary of scientific articles that can be navigated via
citation indices. Though the focus of CiteSeer is
on the citation indices, it also provides a classifica-
tion of articles from computer science into the hi-
erarchically ordered topic set of its computer sci-
ence directory. The bag-of-words model of docu-
ment representation still prevails in automatic text
categorization, but the advent of XML calls for ex-
tending this model to include logical structure fea-
tures in the vector space. Yi and Sundaresan (2000)
have developed a so-called semi-structured classi-
background
problem textual evidence answers resource
framework method_evd
content
fier for semi-structured documents (e.g. XML doc-
uments) based on a document representation that
combines terms with path expressions. This clas-
sifier was shown to reduce classification error rates
considerably in comparison with a purely term-
based classifier when run on US patent documents
provided with XML markup and r´esum´e documents
in HTML. Our aim is to explore similar methods to
classify thematic segments of scientific articles in
XML.
</bodyText>
<sectionHeader confidence="0.69516" genericHeader="method">
2 Text type ontology for scientific articles
</sectionHeader>
<bodyText confidence="0.9999809">
Previous approaches to the text type structure of sci-
entific articles have been developed in the context of
automatic text summarization. In Teufel and Moens
(2002), a categorization scheme of seven ”rhetori-
cal zones” including background, other, own, aim,
textual, contrast, basis is used for the classification
of the sentences of a scientific article according to
their rhetorical status and subsequently finding the
most suitable ones for a summary. The schema we
employ (see below) is more fine-grained, consist-
ing of 16 categories that are hierarchically ordered
(although in the present experiments we did not
make use of this hierarchical order). These 16 cat-
egories refer to the dimension that is discussed un-
der problem structure in (Teufel and Moens, 2002),
rather than to exclusively rhetorical zones and are
viewed as types of topics. A topic “is the semantic-
pragmatic function that selects which concept of the
contextual information will be extended with new
information” (van Dijk, 1980, p.97). Thus while
the concept The daily newspaper Neue Z¨urcher
Zeitung is the topic of several sentences in the ar-
ticle B¨uhlmann (2002), it is an instance of the topic
type ’data’ which in turn is part of the text type
structure of many scientific articles. This text type
structure is captured in our text type schema with
16 bottom-level topic types (Figure 1) that were ob-
tained by evaluating articles from the disciplines of
linguistics and psychology. Kando (1997) presented
a similar hierarchical schema with 51 bottom-level
categories, which were employed for manually an-
notating sentences in Japanese scientific articles.
Her ”text constituents” resemble our topic types, but
we have aimed at sorting out functional categories
such as ’Reason for...’, including only purely the-
matic categories and keeping the number of cate-
gories lower for the experiments described in this
paper.
In Figure 1, the arcs represent the part-of rela-
tion such that a type lower in the hierarchy is a part
of the immediately dominating, more general type
in terms of text type structure. The schema is sup-
posed to represent the typical thematic structure of
research papers. The order of the categories repre-
sents a canonical, expected order of topic types in a
scientific article. The text type schema was initially
encoded as an XML Schema grammar where topic
types are represented by elements that are nested
such that the XML structure reflects the structure
of the text type structure tree (Figure 2).
</bodyText>
<figure confidence="0.9985036875">
&lt;xs:element name=&amp;quot;problem&amp;quot;&gt;
&lt;xs:complexType&gt;
&lt;xs:sequence&gt;
&lt;xs:element name=&amp;quot;background&amp;quot; minOccurs=&amp;quot;0&amp;quot;&gt;
&lt;xs:complexType&gt;
&lt;xs:sequence&gt;
&lt;xs:element name=&amp;quot;othersWork&amp;quot;
type=&amp;quot;xs:string&amp;quot;
minOccurs=&amp;quot;0&amp;quot;/&gt;
&lt;xs:element name=&amp;quot;background_R&amp;quot;
type=&amp;quot;xs:string&amp;quot;
minOccurs=&amp;quot;0&amp;quot;/&gt;
&lt;/xs:sequence&gt;
&lt;/xs:complexType&gt;
&lt;/xs:element&gt;
...
</figure>
<figureCaption confidence="0.974579">
Figure 2: XML Schema grammar (extract) for the
text type schema
</figureCaption>
<sectionHeader confidence="0.951597" genericHeader="method">
3 Data and annotation levels
</sectionHeader>
<bodyText confidence="0.99888388">
We carried out the experiments on a corpus of 47
linguistic research articles, taken from the German
online journal ’Linguistik Online’,1 from the vol-
umes 2000-2003. The selected articles came with
HTML markup and have an average length of 8639
word forms, dealing with subjects as diverse as
the syntax of adverbs, chat analysis, and language
learning.
Taking a text-technological approach, this cor-
pus was prepared such that all required types of in-
formation, including the target classification cate-
gories and the classification features to be extracted,
are realized as XML annotations of the raw text.
Thus, XML markup was provided for the thematic
level, a logical structure level, and a grammatical
level. As described in Bayerl et al. (2003), anno-
tation levels are distinguished from annotation lay-
ers. An annotation level is an abstract level of infor-
mation (such as the morphology and syntax levels
in linguistics), originally independent of any anno-
tation scheme. The term annotation layer, in con-
trast, refers to the realization of an annotation level
as e.g. XML markup. There need not be a 1:1-
correspondence between annotation levels and lay-
ers. As for the three annotation levels in our setting,
</bodyText>
<footnote confidence="0.941841">
1http://www.linguistik-online.de/
</footnote>
<bodyText confidence="0.999606833333333">
one (the structural level) was realized as an inde-
pendent layer, and two (thematic and grammatical)
were realized in one single annotation layer. Each
annotation layer of an article is stored in a separate
file, while it is ensured that the PCDATA of each
layer are identical.
</bodyText>
<subsectionHeader confidence="0.999971">
3.1 Annotation of text type structure
</subsectionHeader>
<bodyText confidence="0.97043044117647">
The order of topic types in a specific scientific ar-
ticle may deviate from the canonical order repre-
sented in the XML schema grammar of the text type
structure shown in Figure 2. Thus a flat version of
the hierarchical XML schema was derived by means
of an XSLT style sheet, exploiting the fact that XML
schema grammars, unlike DTDs, are XML docu-
ments themselves. In the derived flat XML schema,
topic types are represented as attribute values of el-
ements called &lt;group&gt; and &lt;segment&gt;, instead
of names of nested elements. Empty &lt;group&gt; el-
ements represent topic types that corresponded to
the nodes in the original tree of topic types, while
&lt;segment&gt; elements correspond to leaves (termi-
nal categories). The original hierarchical structure
is still represented via the ID/IDREF attributes id
and parent, similar to O’Donnell’s (2000) repre-
sentation of rhetorical structure trees.
For the annotation, the raw text of each arti-
cle was automatically partitioned into text segments
corresponding to sentences, but the annotators were
allowed to modify (join or split) segments to yield
proper thematic units. The problem of finding the-
matic boundaries other than sentence boundaries
automatically (e.g. Utiyama and Isahara (2001)) is
thus not addressed in this work. The annotator then
provided the values of the attribute topic using
the XML spy editor, choosing exactly one of the
16 terminal topic types for each segment, or alter-
natively the category void meta for metadata such
as acknowledgements. If more than one topic type
could in principle be assigned, the annotators were
instructed to choose the one that was most central to
the argumentation. An extract from a THM annota-
tion layer is shown in Figure 3.2
The two annotators were experienced in that they
had received intense training as well as annotated
a corpus of psychological articles according to an
extended version of the schema in Figure 1 earlier
(Bayerl et al., 2003). We assessed inter-rater reli-
ability on three articles from the present linguistics
corpus, which were annotated by both annotators in-
dependently according to the topic type set shown
in Figure 1. (Prior to the analysis the articles were
2The extract, which is also shown in Figure 4, is taken from
B¨uhlmann (2002).
&lt;segment id=&amp;quot;s75a&amp;quot; parent=&amp;quot;g19&amp;quot;
topic=&amp;quot;dataAnalysis&amp;quot;&gt;
Die obige Reihenfolge ver¨andert sich etwas,
wenn nicht die gesamte Anzahl der
Personenbezeichnungen ausschlaggebend ist,
sondern die Anzahl unterschiedlicher
Personenbezeichnungen (das heisst, eine
Personenbezeichnung wie z.B. Jugendliche,
die acht Mal verwendet wurde, wird trotzdem
nur einmal gez¨ahlt):
&lt;/segment&gt;
&lt;segment id=&amp;quot;s76&amp;quot; parent=&amp;quot;g4&amp;quot; topic=&amp;quot;results&amp;quot;&gt;
Im ganzen kommen in den untersuchten Artikel
261 verschiedene Personenbezeichnungen vor.
Davon sind ¨uber 46,7% generische Maskulina,
und nur 31% sind Institutions- und
Kollektivbezeichnungen. Es folgen die
geschlechtsneutralen und -abstrakten
Bezeichnungen mit 18,4%, und nach wie vor
stehen die Doppelformen mit 3,8% Bezeichnungen
am Schluss.
&lt;/segment&gt;
</bodyText>
<figureCaption confidence="0.998758">
Figure 3: THM annotation (extract)
</figureCaption>
<bodyText confidence="0.999966157894737">
resegmented manually so that segment boundaries
were completely identical.) An average agreement
of Kappa = 0.73 was reached (min: .63, max: .78),
which can be interpreted as ’substantial’ agreement
(Landis and Koch, 1977). In order to test for anno-
tation biases we also performed a Stuart-Maxwell
Test (Stuart, 1955; Maxwell, 1970), leading to the
conclusion that marginal homogeneity must be re-
jected on the 1% level (x2 = 61.24; df = 14). The
McNemar Tests (McNemar, 1947) revealed that the
topic types textual, results, interpretation, others-
Work, and conclusions were the problematic cate-
gories. Subsequent log-linear analyses revealed that
annotator1 systematically had assigned background
where annotator2 had assigned framework. Also
interpretation was regularly confused with conclu-
sions, and concepts with either background or oth-
ersWork (model-fit: x2 = 173.14, df = 155,p =
.15).
</bodyText>
<subsectionHeader confidence="0.999977">
3.2 Annotation of syntax and morphology
</subsectionHeader>
<bodyText confidence="0.999950864864865">
For an annotation of grammatical categories to word
form tokens in our corpus, the commercial tagger
Machinese Syntax by Connexor Oy was employed.
This tagger is a rule-based, robust syntactic parser
available for several languages and based on Con-
straint Grammar and Functional Dependency Gram-
mar (Tapanainen and J¨arvinen, 1997). It provides
morphological, surface syntactic, and functional
tags for each word form and a dependency struc-
ture for sentences, and besides is able to process
and output ”simple” XML (that is, XML without
attributes). No conflicts in terms of element over-
laps can arise between our THM annotation layer
and the grammatical tagging, because all tags pro-
vided by Machinese Syntax pertain to word forms.
The grammatical annotations could therefore be in-
tegrated with the THM annotations, forming the
XML annotation layer that we call THMCNX. An
XSLT stylesheet is applied to convert the THM
annotations into attribute-free XML by integrating
the information from attribute-value specifications
into the names of their respective elements. Af-
ter the grammatical tagging, a second stylesheet
re-converts the resulting attribute-free XML repre-
sentations into the original complex XML enriched
by the grammatical tags. Besides, we re-formatted
the original Machinese Syntax tags by omitting,
merging, and renaming some of them, again using
XSLT. The &lt;cmp-head-lemma&gt; tag (containing
the lemma of the head of the present word form), for
example, was derived from the original &lt;lemma&gt;
tag, the value of which contains compound segmen-
tation information. On the THMCNX layer, a sub-
set of 15 grammatical tags may appear at each word
form, including &lt;pos&gt; (part of speech), &lt;aux&gt;
(auxiliary verb), and &lt;num&gt; (number feature for
nominal categories).
</bodyText>
<subsectionHeader confidence="0.994342">
3.3 Logical document structure annotation
</subsectionHeader>
<bodyText confidence="0.9999908">
Since HTML is a hybrid markup language including
a mixture of structural and layout information, we
chose to convert the original HTML of the corpus
into XML based on the DocBook standard (Walsh
and Muellner, 1999). DocBook was originally de-
signed for technical documentation and represents a
purely logical document structure, relying on style
sheets to interpret the logical elements to produce a
desired layout. We did not employ the whole, very
large official DocBook DTD, but designed a new
XML schema that defines a subset with 45 Doc-
Book elements plus 13 additional logical elements
such as tablefootnote and numexample, which
appear in the annotations after the namespace pre-
fix log.3 The annotations were obtained using a
perl script that provided raw DocBook annotations
from the HTML markup, and the XML spy editor
for validation and manually filling in elements that
have no correspondences in HTML. Figure 4 shows
the DocBook annotation of the extract that was also
given in Figure 3.
Moreover, structural position attributes were
added to each element by means of an XSLT style
sheet. These ’POSINFO’ attributes make explicit the
position of the element in the XML DOM tree of
</bodyText>
<footnote confidence="0.910888333333333">
3This XML schema was designed in collaboration
with the HyTex project at the University of Dortmund,
http://www.hytex.info
</footnote>
<table confidence="0.769299869565217">
&lt;sect2&gt;
...
&lt;log:figure&gt;
&lt;log:mediaobject&gt;
&lt;xhtml:img src=&amp;quot;buehlmann20.gif&amp;quot;/&gt;
&lt;/log:mediaobject&gt;
&lt;/log:figure&gt;
&lt;para&gt;Die obige Reihenfolge ver¨andert sich
etwas, wenn nicht die gesamte Anzahl der
Personenbezeichnungen ausschlaggebend ist,
sondern die Anzahl unterschiedlicher
Personenbezeichnungen (das heisst, eine
Personenbezeichnung wie z.B. Jugendliche, die
acht Mal verwendet wurde, wird trotzdem nur
einmal gez¨ahlt): Im ganzen kommen in den
untersuchten Artikel 261 verschiedene
Personenbezeichnungen vor. Davon sind ¨uber
46,7% generische Maskulina, und nur 31% sind
Institutions- und Kollektivbezeichnungen. Es
folgen die geschlechtsneutralen und
-abstrakten Bezeichnungen mit 18,4%, und
nach wie vor stehen die Doppelformen mit
3,8% Bezeichnungen am Schluss.
</table>
<figure confidence="0.96097875">
...
&lt;/para&gt;
...
&lt;/sect2&gt;
</figure>
<figureCaption confidence="0.9828475">
Figure 4: Annotation according to DocBook (ex-
tract)
</figureCaption>
<bodyText confidence="0.5546335">
the document instance in an XPATH-expression as
shown in Figure 5.
</bodyText>
<figure confidence="0.948216">
&lt;para POSINFO1=
&amp;quot;/article[1]/sect1[4]/sect2[1]/para[10]&amp;quot;&gt;
Die obige Reihenfolge ver¨andert sich etwas,
...
&lt;/para&gt;
</figure>
<figureCaption confidence="0.999798">
Figure 5: Structural position path on the doc layer
</figureCaption>
<bodyText confidence="0.999948117647059">
As pointed out above, XML document structure
has been exploited formerly in the automatic classi-
fication of complete documents, e.g. in (Yi and Sun-
daresan, 2000; Denoyer and Gallinari, 2003). How-
ever, we want to use XML document structure in the
classification of thematic segments of documents,
where the thematic segments are XML elements in
the THM annotation layer. The THM and DOC
layers cannot necessarily be combined in a single
layer, as we had refrained from imposing the con-
straint that they always should be compatible, i.e.
not contain overlaps. Still we had to relate element
instances on the DOC layer to element instances on
the THM layer.
For this purpose, we resorted to the Prolog query
tool seit.pl developed at the University of Biele-
feld in the project Sekimo4 for the inference of re-
</bodyText>
<footnote confidence="0.932077">
4see (Goecke et al., 2003; Bayerl et al., 2003) and
http://www.text-technology.de/
</footnote>
<bodyText confidence="0.998382953488372">
lations between two annotation layers of the same
text. seit.pl infers 13 mutually exclusive rela-
tions between instances of element types on sep-
arate annotation layers on account of their shared
PCDATA. In view of the application we envisaged,
we defined four general relations, one of which was
Identity and three of which were defined by the
union of several more specific seit.pl relations:
Identity: The original identity relation from
seit.pl.
Included: Holds if a thematic segment is prop-
erly included in a DocBook element in terms of
the ranges of the respective PCDATA, i.e. is de-
fined as the union of the original seit.pl-relations
included A in B, starting point B and end point B.
This relation was considered to be significant be-
cause we would for example expect THM segments
annotated with the topic type interpretation to ap-
pear within /article[1]/sect1[5] rather than
/article[1]/sect1[1] elements (i.e. the fifth
rather than the first sect1 element).
Includes: Holds if a thematic segment prop-
erly includes a DocBook element in terms of the
ranges of the respective PCDATA, i.e. is defined
as the union of the original seit.pl relations in-
cluded B in A, starting point A, end point A. This
relation was considered to be significant because we
would for example expect logical elements such as
numexample to be included preferably in segments
labelled with the topic type data.
Overlap: Holds if a thematic segment prop-
erly overlaps with a DocBook element in terms of
the ranges of the respective PCDATA. This relation
was considered less significant because the overlap-
ping portion of PCDATA might be very small and
seit.pl so far does not allow for querying how
large the overlapping portion actually is.
The Prolog code of seit.pl was modified such
that it outputs XML files that contain the THM an-
notation layer including structural positions from
the DOC layer within each segment as values of el-
ements that indicate the relation found, cf. Figure
6.
</bodyText>
<sectionHeader confidence="0.998502" genericHeader="method">
4 Automatic text segment classification
experiments
</sectionHeader>
<bodyText confidence="0.999957857142857">
We applied different classification models, namely a
KNN classifier (cf. section 4.1) and, for purposes of
comparison, a simplified Rocchio classifier to text
segments, in order to evaluate the feasibility of an
automatic annotation of scientific articles according
to our THM annotation layer. One important moti-
vation for these experiments was to find out which
kind of data representation yields the best classifi-
cation accuracy, and particularly, if the combination
of complementary information sources, such as bag-
of-words representations of text, on the one hand,
and the structural information provided by the Doc-
Book path annotations, on the other hand, produces
additional synergetic effects.
</bodyText>
<subsectionHeader confidence="0.982073">
4.1 KNN classification
</subsectionHeader>
<bodyText confidence="0.995292428571429">
The basic idea of the K nearest neighbor (KNN)
classification algorithm is to use already categorized
examples from a training set in order to assign a cat-
egory to a new object. The first step is to choose the
K nearest neighbors (i.e. the K most similar objects
according to some similarity metric, such as cosine)
from the trainings set. In a second step the cate-
gorial information of the nearest neighbors is com-
bined, in the simplest case, by determining the ma-
jority class.
The version of KNN classification, adopted
here, uses the Jensen-Shannon divergence (also
known as information radius or iRad) as a
(dis-)similarity metric:
</bodyText>
<equation confidence="0.998387">
iRad(q, r) = 12[D(qllq+r
2 ) + D(r11q+r
2 )]
D(x y) is the Kullback-Leibler divergence (KL
divergence) of probability distributions x and y:
n
D(xlly) = i=1 x(i)(log(x(i)) − log(y(i)))
</equation>
<bodyText confidence="0.995854125">
iRad ranges from 0 (identity) to 2log2 (no simi-
larity) and requires that the compared objects are
probability distributions.
Let NO,C = {n1, ... , nm} (0 &lt; m &lt; K) be
the set of those objects among the K nearest
neighbors of some new object O that belong to a
particular category C. Then the score assigned to
the classification O E C is
</bodyText>
<equation confidence="0.866026">
m
score(O, C) = E iRad(O, nj)E.
</equation>
<bodyText confidence="0.9987766">
Depending on the choice of E, one yields either
a simple majority decision (if E = 0), a linear
weighting of the iRad similarity (if E = 1), or a
stronger emphasis on closer training examples (if
E &gt; 1). Actually, it turned out that very high values
of E improved the classification accuracy. Finally,
the KNN scores for each segment were normalized
to probability distributions, in order to get compa-
rable results for different K and E, when the KNN
classifications get combined with the bigram model.
</bodyText>
<figure confidence="0.997488681818182">
&lt;segment id=&amp;quot;s75a&amp;quot; topic=&amp;quot;dataCollection&amp;quot;&gt;
&lt;included&gt;/article[1]&lt;/included&gt;
&lt;included&gt;/article[1]/sect1[4]&lt;/included&gt;
&lt;included&gt;/article[1]/sect1[4]/sect2[1]&lt;/included&gt;
&lt;includes&gt;/article[1]/sect1[4]/sect2[1]/log:figure[5]&lt;/includes&gt;
&lt;includes&gt;/article[1]/sect1[4]/sect2[1]/log:figure[5]/log:mediaobject[1]&lt;/includes&gt;
&lt;includes&gt;/article[1]/sect1[4]/sect2[1]/log:figure[5]/log:mediaobject[1]/xhtml:img[1]&lt;/includes&gt;
&lt;included&gt;/article[1]/sect1[4]/sect2[1]/para[10]&lt;/included&gt;
&lt;text&gt;Die obige Reihenfolge ver¨andert sich etwas, wenn nicht die gesamte Anzahl
der Personenbezeichnungen
...
&lt;/text&gt;
&lt;/segment&gt;
&lt;segment id=&amp;quot;s76&amp;quot; topic=&amp;quot;results&amp;quot;&gt;
&lt;included&gt;/article[1]&lt;/included&gt;
&lt;included&gt;/article[1]/sect1[4]&lt;/included&gt;
&lt;included&gt;/article[1]/sect1[4]/sect2[1]&lt;/included&gt;
&lt;included&gt;/article[1]/sect1[4]/sect2[1]/para[10]&lt;/included&gt;
&lt;text&gt;Im ganzen kommen in den untersuchten Artikel 261 verschiedene Personenbezeichnungen vor.
Davon sind ...
&lt;/text&gt;
&lt;/segment&gt;
</figure>
<figureCaption confidence="0.99987">
Figure 6: Generated THMDOC layer
</figureCaption>
<subsectionHeader confidence="0.990872">
4.2 Bigram model
</subsectionHeader>
<bodyText confidence="0.9999805">
The bigram model gives the conditional probability
of a topic type T,,,+1, given its predecessor T,,,.
For a sequence of segments s1 ... sm the total
score τ(T, sz) for the assignment of a topic type T to
sz is the product of bigram probability, given the pu-
tative predecessor topic type (i.e. the topic type T&apos;
with the highest τ(T&apos;, sz−1) computed in the pre-
vious step), and the normalized score of the KNN
classifier. The total score of the topic type sequence
is the product of its τ scores.
</bodyText>
<subsectionHeader confidence="0.995511">
4.3 Information sources
</subsectionHeader>
<bodyText confidence="0.9988765">
In our classification experiments we used six differ-
ent representations which can be viewed as different
feature extraction strategies or different levels of ab-
straction:
</bodyText>
<listItem confidence="0.99654004">
• word forms (wf): a bag-of-words represen-
tation of the segment without morphologi-
cal analysis; special characters (punctuation,
braces, etc.) are treated as words.
• compound heads (ch): stems; in case of com-
pounds, the head is used instead of the whole
compound. These features were extracted from
the THMCNX layer (cf. section 3.2).
• size (sz): number of words per segment (calcu-
lation based on the THM annotation layer, cf.
section 3.1).
• DocBook paths (dbp): the segment is repre-
sented as the set of the DocBook paths which
include it (the segment stand in the the In-
cluded relation to it as explained in section
3.3).
• selected DocBook features (df): a set of 6 Doc-
Book features which indicate occurrences of
block quotes, itemized lists, numbered exam-
ples, ordered lists, tables, and references to
footnotes standing in any of the four relations
listed in section 3.2.
• POS tags (pos): the distribution of part-of-
speech tags of the segment taken from the
THMCNX layer (cf. section 3.2).
</listItem>
<subsectionHeader confidence="0.988428">
4.4 Training and Evaluation
</subsectionHeader>
<bodyText confidence="0.999943666666666">
For each test document the bigram model and the
classifier were trained with all other documents.
The overall size of the data collection was 47 docu-
ments. Thus, each classifier and each bigram model
has been trained on the basis of 46 documents, re-
spectively. The total number of segments was 7330.
</bodyText>
<subsectionHeader confidence="0.681361">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.9997174">
We performed several hundred classification tests
with different combinations of data representation,
classification algorithm, and classifier parameter
setting. Table 1 summarizes some results of these
experiments. The baseline (a ’classifier’ guessing
always the most frequent topic type) had an accu-
racy of 22%.
The best combination of data representation and
classifier setting achieved about 47% accuracy. In
this configuration we used a mixture of the com-
pound head representation (40%), the POS tag dis-
tribution (40%), the segment size (10%), and the
selected DocBook features (10%). However, the
combination of compound heads (50%) and part-
of-speech tags (50%) and a similar combination in-
</bodyText>
<table confidence="0.999326296296296">
classifier feature K E accuracy accuracy
weights classifier classifier
+ bigram
most - - - 22.4147 -
frequent
KNN* ch 40% 20 40 56.9785 -
pos 40%
sz 10%
df 10%
Rocchio ch - - 39.0267 -
KNN ch 30% 20 40 41.1278 41.6294
pos 30%
dbp 40%
KNN wf 20 40 38.9725 41.6429
KNN pos 20 40 40.5314 41.9005
KNN ch 25 40 40.4094 42.8765
KNN ch 50% 50 40 44.8556 45.8859
pos 50%
KNN ch 50% 13 40 44.3270 46.6179
pos 50%
KNN ch 49% 20 40 44.8150 46.9296
pos 49%
dbp 2%
KNN ch 40% 20 40 45.5063 47.0788
pos 40%
sz 10%
df 10%
</table>
<tableCaption confidence="0.999859">
Table 1: Results
</tableCaption>
<bodyText confidence="0.999956590909091">
cluding a 2% portion of DocBook path structure
features had similar results. In all experiments the
KNN algorithm performed better than the simplified
Rocchio algorithm. For illustrative purpose, we also
included a configuration, where all other segments
(i.e. including those from the same document) were
available as training segments (’KNN*’ in the sec-
ond line of table 1).
The variation of classification accuracy was very
high both across the topic types and across the doc-
uments. In the best configuration of our classifica-
tion experiments the average segment classification
accuracy per document had a range from 22% to
77%, reflecting the fact that the document collec-
tion was very heterogeneous in many respects. The
topic type resource had an average recall of 97.56%
and an average precision of 91.86%, while several
other topic types, e.g. rationale and dataAnalysis
were near to zero both w.r.t. precision and recall.
The most frequent error was the incorrect assign-
ment of topic type othersWork to segments of topic
types framework, concepts, and background.
</bodyText>
<subsectionHeader confidence="0.536211">
4.6 Discussion
</subsectionHeader>
<bodyText confidence="0.999970066666667">
The task of classifying small text segments, as op-
posed to whole documents, is a rather new ap-
plication field for general domain-independent text
categorization methods. Thus, we lack data from
previous experiments to compare our own results
with. Nevertheless, there are some conclusions to
be drawn from our experiments.
Although the results probably suffer from limita-
tions of our data collection (small sample size, re-
stricted thematic domain), our main conclusion is
that at least some of the topic types of our hierarchy
are successfully learnable. It is, however, question-
able if an overall accuracy of less than 50% is suffi-
cient for applications that require a high reliability.
Moreover, it should be emphasized that our classifi-
cation experiments were carried out on the basis of
manually segmented input.
The usage of structural information improved the
accuracy results slightly, but the impact of this infor-
mation source was clearly below our expectations.
The effect of adding this kind of information was
within the range of improvements which can also be
achieved by fine-tuning a classifier parameter, such
as K.
A somewhat surprising result was that a pure
part-of-speech tag representation achieved nearly
42% accuracy in combination with the bigram
model.
The usage of a bigram model improved the results
in almost all configurations.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999658931034483">
The best combination of data representation and
classifier configuration included ch (40%), pos
(40%), sz (10%) and df (10%), combined with a
topic type bigram model, which yielded an accu-
racy of 47%. However, almost the same accuracy
could be achieved by selecting ch and pos features
only. Other test runs showed that the dbp features
could not improve the results in any combination,
although these features are the ones that indicate
where a segment is situated in an article. An in-
spection of data representations revealed that, for a
particular test document (i.e. text segment), the ma-
jority of training documents with an identical dpb
representation are often assigned the desired topic
type, but this majority is so small that many other
test documents with identical dbp representation are
mis-classified. An accuracy improvement might
therefore be achieved by running different (local)
KNN classifiers trained on different feature sets and
combine their results afterwards.
More future work will focus on the inspection of
categories that have a very low precision and recall
(such as rationale) with a possible review of the text
type ontology. Furthermore, we aim at testing al-
ternative algorithms (e.g. support vector machines),
feature selection methods and at enlarging our train-
ing set. Besides, we will investigate the question,
inhowfar our results are generalizable to scientific
articles from other disciplines and languages.
</bodyText>
<sectionHeader confidence="0.998443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999842862068966">
Petra S. Bayerl, H. L¨ungen, D. Goecke, A. Witt, and
D. Naber. 2003. Methods for the semantic anal-
ysis of document markup. In Proceedings of the
ACMSymposium on Document Engineering (Do-
cEng 2003).
Regula B¨uhlmann. 2002. Ehefrau Vreni
haucht ihm ins Ohr... Untersuchungen
zur geschlechtergerechten Sprache und zur
Darstellung von Frauen in Deutschschweizer
Tageszeitungen. Linguistik Online, 11.
http://www.linguistik-online.de.
Ludovic Denoyer and Patrick Gallinari. 2003. Us-
ing belief networks and fisher kernels for struc-
tured document classification. In Proceedings of
the 7th European Conference on Principles and
Practices ofKnowledge Discovery in Databases,
Cavtat-Dubrovnik, Croatia.
Susan T. Dumais and Hao Chen. 2000. Hierarchi-
cal classification of web content. In Proceedings
ofSIGIR-00, 23rdACMInternational Conference
on Research and Development in Information Re-
trieval, pages 256–263, Athens, Greece. ACM
press, New York.
C. Lee Giles, Kurt Bollacker, and Steve Lawrence.
1998. CiteSeer: An automatic citation index-
ing system. In Ian Witten, Rob Akscyn, and
Frank M. Shipman III, editors, Digital Libraries
98 - The Third ACM Conference on Digital
Libraries, pages 89–98, Pittsburgh, PA. ACM
Press.
Daniela Goecke, Daniel Naber, and Andreas Witt.
2003. Query von Multiebenen-annotierten XML-
Dokumenten mit Prolog. In Uta Seewald-Heeg,
editor, Sprachtechnologie f¨ur die multilin-
guale Kommunikation. Beitr¨age der GLDV-
Fr¨uhjahrstagung, K¨othen 2003, volume 5 of
Sprachwissenschaft Computerlinguistik Neue
Medien, pages 391–405, Sankt Augustin.
gardez!-Verlag.
Noriko Kando. 1997. Text-level structure of re-
search papers: Implications for text-based infor-
mation processing systems. In Proceedings of the
British Computer Society Annual Colloquium of
Information Retrieval Research, pages 68–81.
J.R. Landis and G. G. Koch. 1977. The measure-
ment of observer agreement for categorical data.
Biometrics, 33:159–174.
A. Maxwell. 1970. Comparing the classification of
subjects by two independent judges. British Jour-
nal ofPsychiatry, 116:651–655.
Q. McNemar. 1947. Note on the sampling error of
the difference between correlated proportions or
percentages. Psychometrika, 12:153 –157.
A Stuart. 1955. A test for homogeneity of the
marginal distributions in a two-way classifica-
tion. Biometrika, 42:412–416.
Pasi Tapanainen and Timo J¨arvinen. 1997. A non-
projective dependency parser. In Proceedings of
the 5th Conference on Applied Natural Language
Processing, pages 64–71, Washington D.C. As-
sociation for Computational Linguistics.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientfic articles: Experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
Masao Utiyama and Hitoshi Isahara. 2001. A sta-
tistical model for domain-independent text seg-
mentation. In Meeting of the Association for
Computational Linguistics, pages 491–498.
Teun A. van Dijk. 1980. Macrostructures: An in-
terdisciplinary study of global structures in dis-
course, interaction, and cognition. Lawrence
Erlbaum Associates, Hillsdale, New Jersey.
Norman Walsh and Leonard Muellner. 1999. Doc-
Book: The Definitive Guide. O’Reilly.
Jeonghee Yi and Neel Sundaresan. 2000. A classi-
fier for semi-structured documents. In Proceed-
ings of the Conference on Knowledge Discovery
in Data, pages 190–197.
Guowei Zu, Wataru Ohyama, Tetsushi Wak-
abayashi, and Fumitaka Kimura. 2003. Accu-
racy improvement of automatic text classifica-
tion based on feature transformation. In Christine
Vanoirbeek, C. Roisin, and Ethan Munson, edi-
tors, Proceedings of the 2003 ACM Symposium
on Document Engineering - DocEng03, pages
118–120, Grenoble, France.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.296838">
<title confidence="0.999372">Text Type Structure and Logical Document Structure</title>
<author confidence="0.969128">Hagen</author>
<affiliation confidence="0.830878">Universit¨at Osnabr¨uck</affiliation>
<email confidence="0.930587">hagen.langer@web.de</email>
<affiliation confidence="0.552389">Harald</affiliation>
<address confidence="0.884016">Gießen, Germany</address>
<email confidence="0.998179">luengen@uni-giessen.de</email>
<author confidence="0.962093">Petra Saskia</author>
<address confidence="0.766924">Gießen, Germany</address>
<email confidence="0.999403">bayerl@uni-giessen.de</email>
<abstract confidence="0.999601958333333">Most research on automated categorization of documents has concentrated on the assignment of one or many categories to a whole text. However, new applications, e.g. in the area of the Semantic Web, require a richer and more fine-grained annotation of documents, such as detailed thematic information about the parts of a document. Hence we investigate the automatic categorization of text segments of scientific articles with XML markup into 16 topic types from a text type structure schema. A corpus of 47 linguistic articles was provided with XML markup on different annotation layers representing text type structure, logical document structure, and grammatical categories. Six different feature extraction strategies were applied to this corpus and combined in various parametrizations in different classifiers. The aim was to explore the contribution of each type of information, in particular the logical structure features, to the classification accuracy. The results suggest that some of the topic types of our hierarchy are successfully learnable, while the features from the logical structure layer had no particular impact on the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Petra S Bayerl</author>
<author>H L¨ungen</author>
<author>D Goecke</author>
<author>A Witt</author>
<author>D Naber</author>
</authors>
<title>Methods for the semantic analysis of document markup.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACMSymposium on Document Engineering (DocEng</booktitle>
<marker>Bayerl, L¨ungen, Goecke, Witt, Naber, 2003</marker>
<rawString>Petra S. Bayerl, H. L¨ungen, D. Goecke, A. Witt, and D. Naber. 2003. Methods for the semantic analysis of document markup. In Proceedings of the ACMSymposium on Document Engineering (DocEng 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regula B¨uhlmann</author>
</authors>
<title>Ehefrau Vreni haucht ihm ins Ohr...</title>
<date>2002</date>
<booktitle>Untersuchungen zur geschlechtergerechten Sprache und zur Darstellung von Frauen in Deutschschweizer Tageszeitungen. Linguistik Online,</booktitle>
<volume>11</volume>
<note>http://www.linguistik-online.de.</note>
<marker>B¨uhlmann, 2002</marker>
<rawString>Regula B¨uhlmann. 2002. Ehefrau Vreni haucht ihm ins Ohr... Untersuchungen zur geschlechtergerechten Sprache und zur Darstellung von Frauen in Deutschschweizer Tageszeitungen. Linguistik Online, 11. http://www.linguistik-online.de.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Denoyer</author>
<author>Patrick Gallinari</author>
</authors>
<title>Using belief networks and fisher kernels for structured document classification.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th European Conference on Principles and Practices ofKnowledge Discovery in Databases, Cavtat-Dubrovnik,</booktitle>
<contexts>
<context position="17113" citStr="Denoyer and Gallinari, 2003" startWordPosition="2589" endWordPosition="2592">neutralen und -abstrakten Bezeichnungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. ... &lt;/para&gt; ... &lt;/sect2&gt; Figure 4: Annotation according to DocBook (extract) the document instance in an XPATH-expression as shown in Figure 5. &lt;para POSINFO1= &amp;quot;/article[1]/sect1[4]/sect2[1]/para[10]&amp;quot;&gt; Die obige Reihenfolge ver¨andert sich etwas, ... &lt;/para&gt; Figure 5: Structural position path on the doc layer As pointed out above, XML document structure has been exploited formerly in the automatic classification of complete documents, e.g. in (Yi and Sundaresan, 2000; Denoyer and Gallinari, 2003). However, we want to use XML document structure in the classification of thematic segments of documents, where the thematic segments are XML elements in the THM annotation layer. The THM and DOC layers cannot necessarily be combined in a single layer, as we had refrained from imposing the constraint that they always should be compatible, i.e. not contain overlaps. Still we had to relate element instances on the DOC layer to element instances on the THM layer. For this purpose, we resorted to the Prolog query tool seit.pl developed at the University of Bielefeld in the project Sekimo4 for the </context>
</contexts>
<marker>Denoyer, Gallinari, 2003</marker>
<rawString>Ludovic Denoyer and Patrick Gallinari. 2003. Using belief networks and fisher kernels for structured document classification. In Proceedings of the 7th European Conference on Principles and Practices ofKnowledge Discovery in Databases, Cavtat-Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>Hao Chen</author>
</authors>
<title>Hierarchical classification of web content.</title>
<date>2000</date>
<booktitle>In Proceedings ofSIGIR-00, 23rdACMInternational Conference on Research and Development in Information Retrieval,</booktitle>
<pages>256--263</pages>
<publisher>ACM press,</publisher>
<location>Athens, Greece.</location>
<contexts>
<context position="3010" citStr="Dumais and Chen, 2000" startWordPosition="448" endWordPosition="451">Automatic document classification is an advanced field within computational linguistics with many practical applications and has yielded a wealth of standard methods and tools over the past 10 years. Many systems have been developed on the Reuters-21578 corpus containing 21578 English newpaper articles manually categorized into 135 topic categories, e.g. (Zu et al., 2003). Besides newspaper articles, some approaches have treated the automatic categorization of (HTML) hypertexts available on the W3 into universal text topic sets such as the ones used in the LookSmart and Yahoo web directories (Dumais and Chen, 2000). An approach that focuses solely on research papers is CiteSeer (Giles et al., 1998), the online digital library of scientific articles that can be navigated via citation indices. Though the focus of CiteSeer is on the citation indices, it also provides a classification of articles from computer science into the hierarchically ordered topic set of its computer science directory. The bag-of-words model of document representation still prevails in automatic text categorization, but the advent of XML calls for extending this model to include logical structure features in the vector space. Yi and</context>
</contexts>
<marker>Dumais, Chen, 2000</marker>
<rawString>Susan T. Dumais and Hao Chen. 2000. Hierarchical classification of web content. In Proceedings ofSIGIR-00, 23rdACMInternational Conference on Research and Development in Information Retrieval, pages 256–263, Athens, Greece. ACM press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lee Giles</author>
<author>Kurt Bollacker</author>
<author>Steve Lawrence</author>
</authors>
<title>CiteSeer: An automatic citation indexing system.</title>
<date>1998</date>
<booktitle>Digital Libraries 98 - The Third ACM Conference on Digital Libraries,</booktitle>
<pages>89--98</pages>
<editor>In Ian Witten, Rob Akscyn, and Frank M. Shipman III, editors,</editor>
<publisher>ACM Press.</publisher>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3095" citStr="Giles et al., 1998" startWordPosition="463" endWordPosition="466">with many practical applications and has yielded a wealth of standard methods and tools over the past 10 years. Many systems have been developed on the Reuters-21578 corpus containing 21578 English newpaper articles manually categorized into 135 topic categories, e.g. (Zu et al., 2003). Besides newspaper articles, some approaches have treated the automatic categorization of (HTML) hypertexts available on the W3 into universal text topic sets such as the ones used in the LookSmart and Yahoo web directories (Dumais and Chen, 2000). An approach that focuses solely on research papers is CiteSeer (Giles et al., 1998), the online digital library of scientific articles that can be navigated via citation indices. Though the focus of CiteSeer is on the citation indices, it also provides a classification of articles from computer science into the hierarchically ordered topic set of its computer science directory. The bag-of-words model of document representation still prevails in automatic text categorization, but the advent of XML calls for extending this model to include logical structure features in the vector space. Yi and Sundaresan (2000) have developed a so-called semi-structured classibackground proble</context>
</contexts>
<marker>Giles, Bollacker, Lawrence, 1998</marker>
<rawString>C. Lee Giles, Kurt Bollacker, and Steve Lawrence. 1998. CiteSeer: An automatic citation indexing system. In Ian Witten, Rob Akscyn, and Frank M. Shipman III, editors, Digital Libraries 98 - The Third ACM Conference on Digital Libraries, pages 89–98, Pittsburgh, PA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniela Goecke</author>
<author>Daniel Naber</author>
<author>Andreas Witt</author>
</authors>
<title>Query von Multiebenen-annotierten XMLDokumenten mit Prolog.</title>
<date>2003</date>
<booktitle>Sprachtechnologie f¨ur die multilinguale Kommunikation. Beitr¨age der GLDVFr¨uhjahrstagung, K¨othen 2003, volume 5 of Sprachwissenschaft Computerlinguistik Neue Medien,</booktitle>
<pages>391--405</pages>
<editor>In Uta Seewald-Heeg, editor,</editor>
<contexts>
<context position="17753" citStr="Goecke et al., 2003" startWordPosition="2700" endWordPosition="2703"> use XML document structure in the classification of thematic segments of documents, where the thematic segments are XML elements in the THM annotation layer. The THM and DOC layers cannot necessarily be combined in a single layer, as we had refrained from imposing the constraint that they always should be compatible, i.e. not contain overlaps. Still we had to relate element instances on the DOC layer to element instances on the THM layer. For this purpose, we resorted to the Prolog query tool seit.pl developed at the University of Bielefeld in the project Sekimo4 for the inference of re4see (Goecke et al., 2003; Bayerl et al., 2003) and http://www.text-technology.de/ lations between two annotation layers of the same text. seit.pl infers 13 mutually exclusive relations between instances of element types on separate annotation layers on account of their shared PCDATA. In view of the application we envisaged, we defined four general relations, one of which was Identity and three of which were defined by the union of several more specific seit.pl relations: Identity: The original identity relation from seit.pl. Included: Holds if a thematic segment is properly included in a DocBook element in terms of t</context>
</contexts>
<marker>Goecke, Naber, Witt, 2003</marker>
<rawString>Daniela Goecke, Daniel Naber, and Andreas Witt. 2003. Query von Multiebenen-annotierten XMLDokumenten mit Prolog. In Uta Seewald-Heeg, editor, Sprachtechnologie f¨ur die multilinguale Kommunikation. Beitr¨age der GLDVFr¨uhjahrstagung, K¨othen 2003, volume 5 of Sprachwissenschaft Computerlinguistik Neue Medien, pages 391–405, Sankt Augustin. gardez!-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Kando</author>
</authors>
<title>Text-level structure of research papers: Implications for text-based information processing systems.</title>
<date>1997</date>
<booktitle>In Proceedings of the British Computer Society Annual Colloquium of Information Retrieval Research,</booktitle>
<pages>68--81</pages>
<contexts>
<context position="5727" citStr="Kando (1997)" startWordPosition="881" endWordPosition="882"> semanticpragmatic function that selects which concept of the contextual information will be extended with new information” (van Dijk, 1980, p.97). Thus while the concept The daily newspaper Neue Z¨urcher Zeitung is the topic of several sentences in the article B¨uhlmann (2002), it is an instance of the topic type ’data’ which in turn is part of the text type structure of many scientific articles. This text type structure is captured in our text type schema with 16 bottom-level topic types (Figure 1) that were obtained by evaluating articles from the disciplines of linguistics and psychology. Kando (1997) presented a similar hierarchical schema with 51 bottom-level categories, which were employed for manually annotating sentences in Japanese scientific articles. Her ”text constituents” resemble our topic types, but we have aimed at sorting out functional categories such as ’Reason for...’, including only purely thematic categories and keeping the number of categories lower for the experiments described in this paper. In Figure 1, the arcs represent the part-of relation such that a type lower in the hierarchy is a part of the immediately dominating, more general type in terms of text type struc</context>
</contexts>
<marker>Kando, 1997</marker>
<rawString>Noriko Kando. 1997. Text-level structure of research papers: Implications for text-based information processing systems. In Proceedings of the British Computer Society Annual Colloquium of Information Retrieval Research, pages 68–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="12010" citStr="Landis and Koch, 1977" startWordPosition="1831" endWordPosition="1834">esults&amp;quot;&gt; Im ganzen kommen in den untersuchten Artikel 261 verschiedene Personenbezeichnungen vor. Davon sind ¨uber 46,7% generische Maskulina, und nur 31% sind Institutions- und Kollektivbezeichnungen. Es folgen die geschlechtsneutralen und -abstrakten Bezeichnungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. &lt;/segment&gt; Figure 3: THM annotation (extract) resegmented manually so that segment boundaries were completely identical.) An average agreement of Kappa = 0.73 was reached (min: .63, max: .78), which can be interpreted as ’substantial’ agreement (Landis and Koch, 1977). In order to test for annotation biases we also performed a Stuart-Maxwell Test (Stuart, 1955; Maxwell, 1970), leading to the conclusion that marginal homogeneity must be rejected on the 1% level (x2 = 61.24; df = 14). The McNemar Tests (McNemar, 1947) revealed that the topic types textual, results, interpretation, othersWork, and conclusions were the problematic categories. Subsequent log-linear analyses revealed that annotator1 systematically had assigned background where annotator2 had assigned framework. Also interpretation was regularly confused with conclusions, and concepts with either</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J.R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maxwell</author>
</authors>
<title>Comparing the classification of subjects by two independent judges.</title>
<date>1970</date>
<journal>British Journal ofPsychiatry,</journal>
<pages>116--651</pages>
<contexts>
<context position="12120" citStr="Maxwell, 1970" startWordPosition="1851" endWordPosition="1852">7% generische Maskulina, und nur 31% sind Institutions- und Kollektivbezeichnungen. Es folgen die geschlechtsneutralen und -abstrakten Bezeichnungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. &lt;/segment&gt; Figure 3: THM annotation (extract) resegmented manually so that segment boundaries were completely identical.) An average agreement of Kappa = 0.73 was reached (min: .63, max: .78), which can be interpreted as ’substantial’ agreement (Landis and Koch, 1977). In order to test for annotation biases we also performed a Stuart-Maxwell Test (Stuart, 1955; Maxwell, 1970), leading to the conclusion that marginal homogeneity must be rejected on the 1% level (x2 = 61.24; df = 14). The McNemar Tests (McNemar, 1947) revealed that the topic types textual, results, interpretation, othersWork, and conclusions were the problematic categories. Subsequent log-linear analyses revealed that annotator1 systematically had assigned background where annotator2 had assigned framework. Also interpretation was regularly confused with conclusions, and concepts with either background or othersWork (model-fit: x2 = 173.14, df = 155,p = .15). 3.2 Annotation of syntax and morphology </context>
</contexts>
<marker>Maxwell, 1970</marker>
<rawString>A. Maxwell. 1970. Comparing the classification of subjects by two independent judges. British Journal ofPsychiatry, 116:651–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<pages>157</pages>
<contexts>
<context position="12263" citStr="McNemar, 1947" startWordPosition="1877" endWordPosition="1878">ungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. &lt;/segment&gt; Figure 3: THM annotation (extract) resegmented manually so that segment boundaries were completely identical.) An average agreement of Kappa = 0.73 was reached (min: .63, max: .78), which can be interpreted as ’substantial’ agreement (Landis and Koch, 1977). In order to test for annotation biases we also performed a Stuart-Maxwell Test (Stuart, 1955; Maxwell, 1970), leading to the conclusion that marginal homogeneity must be rejected on the 1% level (x2 = 61.24; df = 14). The McNemar Tests (McNemar, 1947) revealed that the topic types textual, results, interpretation, othersWork, and conclusions were the problematic categories. Subsequent log-linear analyses revealed that annotator1 systematically had assigned background where annotator2 had assigned framework. Also interpretation was regularly confused with conclusions, and concepts with either background or othersWork (model-fit: x2 = 173.14, df = 155,p = .15). 3.2 Annotation of syntax and morphology For an annotation of grammatical categories to word form tokens in our corpus, the commercial tagger Machinese Syntax by Connexor Oy was employ</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Q. McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12:153 –157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stuart</author>
</authors>
<title>A test for homogeneity of the marginal distributions in a two-way classification.</title>
<date>1955</date>
<journal>Biometrika,</journal>
<pages>42--412</pages>
<contexts>
<context position="12104" citStr="Stuart, 1955" startWordPosition="1849" endWordPosition="1850">sind ¨uber 46,7% generische Maskulina, und nur 31% sind Institutions- und Kollektivbezeichnungen. Es folgen die geschlechtsneutralen und -abstrakten Bezeichnungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. &lt;/segment&gt; Figure 3: THM annotation (extract) resegmented manually so that segment boundaries were completely identical.) An average agreement of Kappa = 0.73 was reached (min: .63, max: .78), which can be interpreted as ’substantial’ agreement (Landis and Koch, 1977). In order to test for annotation biases we also performed a Stuart-Maxwell Test (Stuart, 1955; Maxwell, 1970), leading to the conclusion that marginal homogeneity must be rejected on the 1% level (x2 = 61.24; df = 14). The McNemar Tests (McNemar, 1947) revealed that the topic types textual, results, interpretation, othersWork, and conclusions were the problematic categories. Subsequent log-linear analyses revealed that annotator1 systematically had assigned background where annotator2 had assigned framework. Also interpretation was regularly confused with conclusions, and concepts with either background or othersWork (model-fit: x2 = 173.14, df = 155,p = .15). 3.2 Annotation of syntax</context>
</contexts>
<marker>Stuart, 1955</marker>
<rawString>A Stuart. 1955. A test for homogeneity of the marginal distributions in a two-way classification. Biometrika, 42:412–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<institution>D.C. Association for Computational Linguistics.</institution>
<location>Washington</location>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71, Washington D.C. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientfic articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="4416" citStr="Teufel and Moens (2002)" startWordPosition="667" endWordPosition="670">s (e.g. XML documents) based on a document representation that combines terms with path expressions. This classifier was shown to reduce classification error rates considerably in comparison with a purely termbased classifier when run on US patent documents provided with XML markup and r´esum´e documents in HTML. Our aim is to explore similar methods to classify thematic segments of scientific articles in XML. 2 Text type ontology for scientific articles Previous approaches to the text type structure of scientific articles have been developed in the context of automatic text summarization. In Teufel and Moens (2002), a categorization scheme of seven ”rhetorical zones” including background, other, own, aim, textual, contrast, basis is used for the classification of the sentences of a scientific article according to their rhetorical status and subsequently finding the most suitable ones for a summary. The schema we employ (see below) is more fine-grained, consisting of 16 categories that are hierarchically ordered (although in the present experiments we did not make use of this hierarchical order). These 16 categories refer to the dimension that is discussed under problem structure in (Teufel and Moens, 20</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientfic articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>491--498</pages>
<contexts>
<context position="9947" citStr="Utiyama and Isahara (2001)" startWordPosition="1527" endWordPosition="1530">n the original tree of topic types, while &lt;segment&gt; elements correspond to leaves (terminal categories). The original hierarchical structure is still represented via the ID/IDREF attributes id and parent, similar to O’Donnell’s (2000) representation of rhetorical structure trees. For the annotation, the raw text of each article was automatically partitioned into text segments corresponding to sentences, but the annotators were allowed to modify (join or split) segments to yield proper thematic units. The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. The annotator then provided the values of the attribute topic using the XML spy editor, choosing exactly one of the 16 terminal topic types for each segment, or alternatively the category void meta for metadata such as acknowledgements. If more than one topic type could in principle be assigned, the annotators were instructed to choose the one that was most central to the argumentation. An extract from a THM annotation layer is shown in Figure 3.2 The two annotators were experienced in that they had received intense training as well as annotated a corpus o</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2001. A statistical model for domain-independent text segmentation. In Meeting of the Association for Computational Linguistics, pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teun A van Dijk</author>
</authors>
<title>Macrostructures: An interdisciplinary study of global structures in discourse, interaction, and cognition. Lawrence Erlbaum Associates,</title>
<date>1980</date>
<location>Hillsdale, New Jersey.</location>
<marker>van Dijk, 1980</marker>
<rawString>Teun A. van Dijk. 1980. Macrostructures: An interdisciplinary study of global structures in discourse, interaction, and cognition. Lawrence Erlbaum Associates, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman Walsh</author>
<author>Leonard Muellner</author>
</authors>
<title>DocBook: The Definitive Guide.</title>
<date>1999</date>
<publisher>O’Reilly.</publisher>
<contexts>
<context position="14743" citStr="Walsh and Muellner, 1999" startWordPosition="2250" endWordPosition="2253">ntaining the lemma of the head of the present word form), for example, was derived from the original &lt;lemma&gt; tag, the value of which contains compound segmentation information. On the THMCNX layer, a subset of 15 grammatical tags may appear at each word form, including &lt;pos&gt; (part of speech), &lt;aux&gt; (auxiliary verb), and &lt;num&gt; (number feature for nominal categories). 3.3 Logical document structure annotation Since HTML is a hybrid markup language including a mixture of structural and layout information, we chose to convert the original HTML of the corpus into XML based on the DocBook standard (Walsh and Muellner, 1999). DocBook was originally designed for technical documentation and represents a purely logical document structure, relying on style sheets to interpret the logical elements to produce a desired layout. We did not employ the whole, very large official DocBook DTD, but designed a new XML schema that defines a subset with 45 DocBook elements plus 13 additional logical elements such as tablefootnote and numexample, which appear in the annotations after the namespace prefix log.3 The annotations were obtained using a perl script that provided raw DocBook annotations from the HTML markup, and the XML</context>
</contexts>
<marker>Walsh, Muellner, 1999</marker>
<rawString>Norman Walsh and Leonard Muellner. 1999. DocBook: The Definitive Guide. O’Reilly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeonghee Yi</author>
<author>Neel Sundaresan</author>
</authors>
<title>A classifier for semi-structured documents.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Knowledge Discovery in Data,</booktitle>
<pages>190--197</pages>
<contexts>
<context position="3628" citStr="Yi and Sundaresan (2000)" startWordPosition="550" endWordPosition="553"> 2000). An approach that focuses solely on research papers is CiteSeer (Giles et al., 1998), the online digital library of scientific articles that can be navigated via citation indices. Though the focus of CiteSeer is on the citation indices, it also provides a classification of articles from computer science into the hierarchically ordered topic set of its computer science directory. The bag-of-words model of document representation still prevails in automatic text categorization, but the advent of XML calls for extending this model to include logical structure features in the vector space. Yi and Sundaresan (2000) have developed a so-called semi-structured classibackground problem textual evidence answers resource framework method_evd content fier for semi-structured documents (e.g. XML documents) based on a document representation that combines terms with path expressions. This classifier was shown to reduce classification error rates considerably in comparison with a purely termbased classifier when run on US patent documents provided with XML markup and r´esum´e documents in HTML. Our aim is to explore similar methods to classify thematic segments of scientific articles in XML. 2 Text type ontology </context>
<context position="17083" citStr="Yi and Sundaresan, 2000" startWordPosition="2584" endWordPosition="2588">Es folgen die geschlechtsneutralen und -abstrakten Bezeichnungen mit 18,4%, und nach wie vor stehen die Doppelformen mit 3,8% Bezeichnungen am Schluss. ... &lt;/para&gt; ... &lt;/sect2&gt; Figure 4: Annotation according to DocBook (extract) the document instance in an XPATH-expression as shown in Figure 5. &lt;para POSINFO1= &amp;quot;/article[1]/sect1[4]/sect2[1]/para[10]&amp;quot;&gt; Die obige Reihenfolge ver¨andert sich etwas, ... &lt;/para&gt; Figure 5: Structural position path on the doc layer As pointed out above, XML document structure has been exploited formerly in the automatic classification of complete documents, e.g. in (Yi and Sundaresan, 2000; Denoyer and Gallinari, 2003). However, we want to use XML document structure in the classification of thematic segments of documents, where the thematic segments are XML elements in the THM annotation layer. The THM and DOC layers cannot necessarily be combined in a single layer, as we had refrained from imposing the constraint that they always should be compatible, i.e. not contain overlaps. Still we had to relate element instances on the DOC layer to element instances on the THM layer. For this purpose, we resorted to the Prolog query tool seit.pl developed at the University of Bielefeld i</context>
</contexts>
<marker>Yi, Sundaresan, 2000</marker>
<rawString>Jeonghee Yi and Neel Sundaresan. 2000. A classifier for semi-structured documents. In Proceedings of the Conference on Knowledge Discovery in Data, pages 190–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guowei Zu</author>
<author>Wataru Ohyama</author>
<author>Tetsushi Wakabayashi</author>
<author>Fumitaka Kimura</author>
</authors>
<title>Accuracy improvement of automatic text classification based on feature transformation.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 ACM Symposium on Document Engineering - DocEng03,</booktitle>
<pages>118--120</pages>
<editor>In Christine Vanoirbeek, C. Roisin, and Ethan Munson, editors,</editor>
<location>Grenoble, France.</location>
<contexts>
<context position="2762" citStr="Zu et al., 2003" startWordPosition="410" endWordPosition="413"> some degree adhere to such a schema when creating a logical structure for their documents for instance by XML markup, and that therefore such markup bears clues as to the thematic structure of scientific articles. Figure 1: Text type schema Automatic document classification is an advanced field within computational linguistics with many practical applications and has yielded a wealth of standard methods and tools over the past 10 years. Many systems have been developed on the Reuters-21578 corpus containing 21578 English newpaper articles manually categorized into 135 topic categories, e.g. (Zu et al., 2003). Besides newspaper articles, some approaches have treated the automatic categorization of (HTML) hypertexts available on the W3 into universal text topic sets such as the ones used in the LookSmart and Yahoo web directories (Dumais and Chen, 2000). An approach that focuses solely on research papers is CiteSeer (Giles et al., 1998), the online digital library of scientific articles that can be navigated via citation indices. Though the focus of CiteSeer is on the citation indices, it also provides a classification of articles from computer science into the hierarchically ordered topic set of i</context>
</contexts>
<marker>Zu, Ohyama, Wakabayashi, Kimura, 2003</marker>
<rawString>Guowei Zu, Wataru Ohyama, Tetsushi Wakabayashi, and Fumitaka Kimura. 2003. Accuracy improvement of automatic text classification based on feature transformation. In Christine Vanoirbeek, C. Roisin, and Ethan Munson, editors, Proceedings of the 2003 ACM Symposium on Document Engineering - DocEng03, pages 118–120, Grenoble, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>