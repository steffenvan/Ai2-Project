<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000151">
<title confidence="0.985832">
Referring Expression Generation through Attribute-Based Heuristics
</title>
<author confidence="0.998477">
Robert Dale and Jette Viethen
</author>
<affiliation confidence="0.878219">
Centre for Language Technology
Macquarie University
Sydney, Australia
</affiliation>
<email confidence="0.997004">
rdale@ics.mq.edu.au|jviethen@ics.mq.edu.au
</email>
<sectionHeader confidence="0.993857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929857142857">
In this paper, we explore a corpus of
human-produced referring expressions to
see to what extent we can learn the referen-
tial behaviour the corpus represents. De-
spite a wide variation in the way subjects
refer across a set of ten stimuli, we demon-
strate that component elements of the re-
ferring expression generation process ap-
pear to generalise across participants to a
significant degree. This leads us to pro-
pose an alternative way of thinking of re-
ferring expression generation, where each
attribute in a description is provided by a
separate heuristic.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958916666667">
The last few years have witnessed a considerable
move towards empiricism in referring expression
generation; this is evidenced both by the growing
body of work that analyses and tries to replicate
the content of corpora of human-produced refer-
ring expressions, and particularly by the signifi-
cant participation in the TUNA and GREC chal-
lenge tasks built around such activities (see, for
example, (Belz and Gatt, 2007; Belz et al., 2008;
Gatt et al., 2008)). One increasingly widespread
observation—obvious in hindsight, but surpris-
ingly absent from much earlier work on referring
expression generation—is that one person’s refer-
ential behaviour differs from that of another: given
the same referential task, different subjects will
choose different referring expressions to identify
a target referent. Faced with this apparent lack of
cross-speaker consistency in how to refer to enti-
ties, we might question the validity of any exercise
that tries to develop an algorithm on the basis of
data from multiple speakers.
In this paper we revisit the corpus of data
that was introduced and discussed in (Viethen
and Dale, 2008a; Viethen and Dale, 2008b) with
the objective of determining what referential be-
haviour, if any, might be learned automatically
from the data. We find that, despite the apparent
diversity of the data when we consider the pro-
duction of referring expressions across subjects,
a closer examination reveals that individual at-
tributes within referring expressions do appear to
be selected on the basis of contextual factors with
a high degree of consistency. This suggests that re-
ferring behaviour might be best thought of as con-
sisting of a combination of lower-level heuristics,
with each individual’s overall referring behaviour
being constructed from a potentially distinct com-
bination of these common heuristics.
In Section 2 we describe the corpus we use for
the experiments in this paper. In Section 3 we ex-
plore to what extent we can use this corpus to learn
an algorithm for referring expression generation;
in Section 4 we look more closely at the nature of
individual variation within the corpus. Section 5
briefly discusses related work on the use of ma-
chine learning in referring expression generation,
and Section 6 draws some conclusions and points
to future work.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="method">
2 The Corpus
</sectionHeader>
<subsectionHeader confidence="0.992419">
2.1 General Overview
</subsectionHeader>
<bodyText confidence="0.999987583333333">
The corpus we use was collected via a data gath-
ering experiment described in (Viethen and Dale,
2008a; Viethen and Dale, 2008b). The purpose of
the data gathering was to gain some insight into
how human subjects use relational referring ex-
pressions, a relatively unexplored aspect of refer-
ring expression generation. Participants visited a
website, where they first saw an introductory page
with a set of simple instructions and a sample stim-
ulus scene consisting of three objects. Each par-
ticipant was then assigned one of two trial sets of
ten scenes each; the two trial sets are superficially
</bodyText>
<note confidence="0.916031">
Proceedings of the 12th European Workshop on Natural Language Generation, pages 58–65,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997014">
58
</page>
<figureCaption confidence="0.966543272727273">
Figure 2: The schemata which form the basis for
the stimulus scenes.
Figure 1: The stimulus scenes. The letters indi-
cate which schema from Figure 2 each column of
scenes is based on.
different, but the elements of the sets are pairwise
identical in terms of the factors explored in the re-
search. The complete set of 20 scenes is shown in
Figure 1, where Trial Set 1 consists of Scenes 1
through 10, and Trial Set 2 consists of Scenes 11
through 20.1
</figureCaption>
<bodyText confidence="0.9999894375">
The scenes were presented successively in a
preset order, which was the same for each partic-
ipant. Below each scene, the participant had to
complete the sentence Please pick up the ... in a
text box before clicking on a button to see the next
scene. The task was to describe the target referent
in the scene (marked by a grey arrow) in a way that
would enable a friend looking at the same scene to
pick it out from the other objects.
The experiment was completed by 74 partici-
pants from a variety of different backgrounds and
ages; most were university-educated and in their
early or mid twenties. For reasons discussed in
(Viethen and Dale, 2008b), the data of 11 partici-
pants was discarded. Of the remaining 63 partici-
pants, 29 were female, while 34 were male.
</bodyText>
<subsectionHeader confidence="0.998831">
2.2 Stimulus Design
</subsectionHeader>
<bodyText confidence="0.9985705">
The design of the stimuli used in the experiment is
described in detail in (Viethen and Dale, 2008a).
</bodyText>
<footnote confidence="0.987588">
1Scene 1 is paired with Scene 11, Scene 2 with Scene
12, and so on; in each pair, the only differences are the
colour scheme used and the left–right orientation, with these
variations being introduced to make the experiment less
monotonous for subjects; (Viethen and Dale, 2008a) report
that these characteristics of the scenes appear to have no sig-
nificant effect on the forms of reference used.
</footnote>
<bodyText confidence="0.999804948717949">
We provide a summary of the key points here.
In order to explore even the most basic hypothe-
ses with respect to the use of relational expres-
sions, which was the aim of the original study,
scenes containing at least three objects were re-
quired. One of these objects is the intended ref-
erent, which is referred to here as the target. The
subject has to describe the target in such a way as
to distinguish it from the other two objects in the
scene. Although the scenes presented to the sub-
jects are such that spatial relations are never nec-
essary to distinguish the target, they are set up so
that one of the two non-target objects was clearly
closer to the target. This object is referred to as the
(potential) landmark; and we call the third object
in the scene the distractor.
To minimise the number of variables in the ex-
periments, scenes are restricted to only two kinds
of objects, cubes and balls. The objects also vary
in two dimensions: colour (either green, blue,
yellow, or red); and size (either large or small).
To further reduce the number of factors in the
scene design, the landmark and distractor are al-
ways placed clearly side by side, and the target is
located on top of or directly in front of the land-
mark.
Finally, to reduce the set of possible stimuli to a
manageable number, five schemata (see Figure 2)
were created as a basis for the final stimulus set.
The design of these schemata was informed by a
number of research questions with regard to the
use of relations; see (Viethen and Dale, 2008b). A
schema determines the type and size of each object
in the scenes that are based on it, and determines
which objects share colour. So, for example, in
scenes based on Schema C, the target is a small
ball; the landmark is a large cube with different
colour from the target; and the distractor is a large
ball sharing its colour with the target.
</bodyText>
<page confidence="0.948289">
59
</page>
<equation confidence="0.912304">
Label
Pattern
Example
(tg col, tg type)
</equation>
<bodyText confidence="0.853411705882353">
(tg col, tg type, rel, lm col, lm type)
(tg col, tg type, rel, lm size, lm col, lm type)
(tg col, tg type, rel, lm size, lm type)
(tg col, tg type, rel, lm type)
(tg size, tg col, tg type)
(tg size, tg col, tg type, rel, lm col, lm type)
(tg size, tg col, tg type, rel, lm size, lm col, lm type)
(tg size, tg col, tg type, rel, lm size, lm type)
(tg size, tg col, tg type, rel, lm type)
(tg size, tg type)
(tg size, tg type, rel, lm size, lm type)
(tg size, tg type, rel, lm type)
(tg type)
(tg type, rel, lm col, lm type)
(tg type, rel, lm size, lm col, lm type)
(tg type, rel, lm size, lm type)
(tg type, rel, lm type)
</bodyText>
<figure confidence="0.916264388888889">
the blue cube
the blue cube in front of the red ball
the blue cube in front of the large red ball
the blue cube in front of the large ball
the blue cube in front of the ball
the large blue cube
the large blue cube in front of the red ball
the large blue cube in front of the large red ball
the large blue cube in front of the large ball
the large blue cube in front of the ball
the large cube
the large cube in front of the large ball
the large cube in front of the ball
the cube
the cube in front of the red ball
the cube in front of the large red ball
the cube in front of the large ball
the cube in front of the ball
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
Q
R
</figure>
<tableCaption confidence="0.944563">
Table 1: The 18 different patterns corresponding to the different forms of description that occur in the
GRE3D3 corpus.
</tableCaption>
<bodyText confidence="0.999084714285714">
From each schema, four distinct scenes were
generated, resulting in the 20 stimulus scenes
shown in Figure 1. As noted above, there are really
only 10 distinct ‘underlying’ scene types here, so
in the remainder of this paper we will talk in terms
of Scenes 1 through 10, where the data from the
pairwise matched scenes are conflated.
</bodyText>
<subsectionHeader confidence="0.98798">
2.3 The GRE3D3 Corpus2
</subsectionHeader>
<bodyText confidence="0.953084857142857">
Before conducting any quantitative data analysis,
some syntactic and lexical normalisation was car-
ried out on the data provided by the participants.
In particular, spelling mistakes were corrected;
normalised names were used for colour values and
head nouns (for example, box was replaced by
cube); and complex syntactic structures such as
relative clauses were replaced with semantically
equivalent simpler ones such as adjectives. These
normalisation steps should be of no consequence
to the analysis presented here, since we are solely
interested in exploring the semantic content of re-
ferring expressions, not their lexical and syntactic
surface structure.
For the purposes of the machine learning exper-
iments described in this paper, we made a few fur-
ther changes to the data set in order to keep the
number of properties and their possible values low.
We removed locative expressions that made refer-
2The data set resulting from the experiment described
above is known as the GRE3D3 Corpus; the name stands for
‘Generation of Referring Expressions in 3D scenes with 3
Objects’.
ence to a part of the scene (58 instances) and ref-
erences to size as the same (4 instances); so, for
example, the blue cube on top of the green cube
in the right and the blue cube on top of the green
cube of the same size both became the blue cube
on top of the green cube. We also removed the
mention of a third object from ten descriptions in
order to keep the number of possible objects per
description to a maximum of two. These changes
resulted in seven descriptions no longer satisfying
the criterion of being fully distinguishing, so we
removed these descriptions from the corpus.
</bodyText>
<sectionHeader confidence="0.969017" genericHeader="method">
3 Learning Description Patterns
</sectionHeader>
<bodyText confidence="0.9999815">
The resulting corpus consists of 623 descriptions.
Every one of these is an instance of one of the 18
patterns shown in Table 1; for ease of reference,
we label these patterns A through R. Each pattern
indicates the sequence of attributes used in the de-
scription, where each attribute is identified by the
object it describes (tg for target, lm for landmark)
and the attribute used (col, size and type for colour,
size and type respectively).
Most work on referring expression generation
attempts to determine what attributes should be
used in a description by taking account of aspects
of the context of reference. An obvious question
is then whether we can learn the description pat-
terns in this data from the contexts in which they
were produced. To explore this, we chose to cap-
ture the relevant aspects of context by means of
the notion of characteristics of scenes. The char-
</bodyText>
<page confidence="0.940529">
60
</page>
<figure confidence="0.865467">
Label
Values
Attribute
</figure>
<bodyText confidence="0.99914785">
Target and Landmark share Type
Target and Distractor share Type
Landmark and Distractor share Type
Target and Landmark share Colour
Target and Distractor share Colour
Landmark and Distractor share Colour
Target and Landmark share Size
Target and Distractor share Size
Landmark and Distractor share Size
Relation between Target and Landmark
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
TRUE, FALSE
on top of, in front of
</bodyText>
<table confidence="0.7716627">
tg type = lm type
tg type = dr type
lm type = dr type
tg col = lm col
tg col = dr col
lm col = dr col
tg size = lm size
tg size = dr size
lm size = dr size
rel
</table>
<tableCaption confidence="0.995368">
Table 2: The 10 characteristics of scenes
</tableCaption>
<bodyText confidence="0.96049182">
acteristics of scenes which we hypothesize might
have an impact on the choice of referential form
are those summarised in Table 2; these are pre-
cisely the characteristics that were manipulated in
the design of the schemata in Figure 2.
Of course, there is no one correct answer for
how to refer to the target in any given scene.
Figure 3 shows the distribution of different pat-
terns across the different scenes; so, for exam-
ple, some scenes (Scenes 4, 5, 9 and 10) result
in only five semantically distinct referring expres-
sion forms, whereas Scene 7 results in 12 distinct
referring expression forms. All of these are distin-
guishing descriptions, so all are acceptable forms
of reference, although some contain more redun-
dancy than others. Most obvious from the chart
is that, for many scenes, there is a predominant
form of reference used; so, for example, pattern F
((tg size, tg col, tg type)) accounts for 43 (68%)
of the descriptions used in Scene 4, and pattern
A ((tg col, tg type)) is very frequently used in a
number of scenes.3
We used Weka (Witten and Eibe, 2005) with the
J48 decision tree classifer to see what correspon-
dences might be learned between the character-
isics of the scenes listed in Table 2 and the forms
of referring expression used for the target refer-
ents, as shown in Table 1. The pruned decision
tree learned by this method predicted the actual
form of reference used in only 48% of cases under
10-fold cross-validation, but given that there are
many ‘gold standard’ descriptions for each scene,
3The chart as presented here is obviously too small to en-
able detailed examination, and our use of colour coding will
be of no value in a monchrome rendering of the paper; how-
ever, the overall shape of the data is sufficient to demonstrate
the points we make here.
this low score is hardly surprising; a mechanism
which learns only one answer will inevitably be
‘wrong’ in many cases. More revealing, however,
is the rule learned from the data:
if tg type = dr type
then use F ((tg size, tg col, tg type))
else use A ((tg col, tg type))
endif
Patterns A and F are the two most prevalent pat-
terns in the data, and indeed one or other appears
at least once in the human data for each scene;
consequently, the learned rule is able to produce
a ‘correct’ answer for every scene.4
</bodyText>
<sectionHeader confidence="0.998909" genericHeader="method">
4 Individual Variation
</sectionHeader>
<bodyText confidence="0.99984525">
One of the most striking things about the data in
this corpus is the extent to which different subjects
appear to do different things when they construct
referring expressions, as demonstrated by the dis-
tribution of patterns in Figure 3. Another way of
looking at this variation is to characterise the be-
haviour of each subject in terms of the sequence of
descriptions they provide in response to the set of
10 stimuli.
Across the 63 subjects, there are 47 different se-
quences; of these, only four occur more than once
(in other words, 43 subjects did not produce the
same sequence of descriptions for the ten scenes as
anyone else). The recurrent sequences, i.e. those
used by at least two people, are shown in Table 3.
Note that the most frequently recurring sequence,
</bodyText>
<footnote confidence="0.9970185">
4The fact that the rule is conditioned on a property of the
distractor object may be an artefact of the stimulus set con-
struction; this would require a more diverse set of scenes to
determine.
</footnote>
<page confidence="0.998757">
61
</page>
<figureCaption confidence="0.9783225">
Figure 3: The profile of different description patterns (A through R) for each of the 10 scenes. The length
of the bar indicates how often each of the 18 patterns is used.
</figureCaption>
<bodyText confidence="0.999915942307692">
which matches the behaviour of nine separate sub-
jects, consists only of uses of patterns A and F.
It remains to be seen to what extent a larger data
set would demonstrate more convergence; how-
ever, the point to be made at present is that any
attempt to predict the behaviour of a given speaker
by means of a model of referring behaviour is go-
ing to have to take account of a great deal of indi-
viual variation.
Nonetheless, we re-ran the J48 classifier de-
scribed in the previous section, this time using
the participant ID as well as the scene character-
istics in Table 2 as features. This improved pattern
prediction to 57.62%. This suggests that individ-
ual differences may indeed be capturable from the
data, although we would need more data than the
mere 10 examples we have from each subject to
learn a good predictive model.
In the face of this lack of data, another approach
is to look for commonalities in the data in terms
of the constituent elements of the different ref-
erence patterns used for each scene. This way
of thinking about the data was foreshadowed by
(Viethen and Dale, 2008b), who observed that the
subjects could be separated into those who always
used relations, those who never used relations, and
those who sometimes used relations. This leads
us to consider whether there are characteristics of
scenes or speakers which are highly likely to result
in specific attributes being used in descriptions. If
this is the case, a decision tree learner should be
able to learn for each individual attribute whether
it should be included in a given situation.
An appropriate baseline for any experiments
here is the success rate of simply including or not
including each attribute (basically a 0-R majority
class classifier), irrespective of the characteristics
of the scene. Table 4 compares the results for
this ‘context-free’ approach with one model that
is trained on the characteristics of scenes, and an-
other that takes both the characteristics of scenes
and the participant ID into account.5
Interestingly, the ‘context-free’ strategies work
suprisingly well for predicting the inclusion of
some attributes in the human data. As has been
noted in other work (see for example (Viethen et
al., 2008)), colour is often included in referring ex-
pressions irrespective of its discriminatory power,
and this is borne out by the data here. Perhaps
more suprising is the large degree to which the in-
clusion of landmark size is captured by a context-
free strategy.
</bodyText>
<footnote confidence="0.995128">
5As before, the results reported are for the accuracy of a
pruned J48 decision tree, under 10-fold cross-validation.
</footnote>
<page confidence="0.999323">
62
</page>
<bodyText confidence="0.999900592592593">
Improvement on all attribues other than tar-
get colour improves when we take into account
the characteristics of the scenes, consistent with
our assumptions that context does matter. When
we add participant ID to the features used in the
learner, performance improves further still, indi-
cating that there are speaker-specific consistencies
in the data.
It is instructive to look at the rules learned on
the basis of the scene characteristics. Not surpris-
ingly, the rule derived for target colour inclusion is
simply to always include the colour (i.e., the same
context-free colour inclusion rule that proves most
effective in modelling the data without reference
to scene characteristics). The rules for including
the other attributes on the basis of scene charac-
teristics (but not participant ID) are shown in Fig-
ure 4.
The rules learned when we include participant
ID are more conplex, but can be summarised in a
way that demonstrates how this approach can re-
veal something about the variety of ways in which
speakers appear to approach the task of referring
expression generation. Focussing, as an example,
just on the question of whether or not to use the
target object’s colour in a referring expression, we
find the following:
</bodyText>
<listItem confidence="0.774698117647059">
• 48 participants always used colour, irrespec-
tive of the context (this corresponds to the
baseline rule learned above).
• The other participants always use colour if
the target and the landmark are of the same
type (which again is intuitively quite appro-
priate).
• When the landmark and the target are not
of the same type, we see more variation in
behaviour; 19 participants simply don’t use
colour, and the behaviour of seven can be
captured via a more complex analysis: four
use colour if the target and the distractor are
the same size, two use colour if the target and
distractor are of the same size and the target
is on top of the landmark, and one uses colour
if the target and distractor share colour.
</listItem>
<bodyText confidence="0.9995968">
Again, the specific details of the rules learned here
are probably not particularly significant, based as
they are on a limited data set and a set of stimuli
that may give elevated status to incidental proper-
ties. However, the general point remains that we
</bodyText>
<subsectionHeader confidence="0.886228">
Target Size:
</subsectionHeader>
<bodyText confidence="0.98491">
if tg type = dr type then include tg size
</bodyText>
<sectionHeader confidence="0.505579" genericHeader="method">
Relation:
</sectionHeader>
<bodyText confidence="0.9080015">
if rel = on top of and lm size = dr size
then include rel
</bodyText>
<subsectionHeader confidence="0.918499">
Landmark Colour:
</subsectionHeader>
<bodyText confidence="0.983729">
if we have used a relation then include lm col
</bodyText>
<subsectionHeader confidence="0.864617">
Landmark Size:
</subsectionHeader>
<bodyText confidence="0.868019">
if we have used a relation and tg col = lm col
then include lm size
</bodyText>
<figureCaption confidence="0.6294705">
Figure 4: Rules learned on the basis of scene char-
acteristics
</figureCaption>
<bodyText confidence="0.99986771875">
can use this kind of analysis to identify possible
rules for the inclusion of individual attributes in
referring expressions.
What this suggests is that we might be able to
capture the behaviour of individual speakers not
in terms of an overall strategy, but as a compos-
ite of heuristics, where each heuristic accounts for
the inclusion of a specific attribute. The rules, or
heuristics, shown in Figure 4 are just those which
are most successful in predicting the data; but
there can be many other rules that might be used
for the inclusion of particular attributes. So, for
example, I might be the kind of speaker who just
automatically includes the colour of an intended
referent without any analysis of the scene; and I
might be the kind of speaker who always uses a
relation to a nearby landmark in describing the in-
tended referent. Or I might be the kind of speaker
who surveys the scene and takes note of whether
the landmark’s colour is distinctive; and so on.
Thought of in this way, each speaker’s approach
to reference is like a set of ‘parallel gestalts’ that
contribute information to the description being
constructed. The particular rules for inclusion that
any speaker uses might vary depending on their
personal past history, and perhaps even on the ba-
sis of situation-specific factors that on a given oc-
casion might lean the speaker towards either being
‘risky’ or ‘cautious’ (Carletta, 1992).
As alluded to earlier, the specific content of the
rules shown in Figure 4 may appear idiosyncratic;
they are just what the limited data in the corpus
</bodyText>
<page confidence="0.998928">
63
</page>
<table confidence="0.7344242">
Pattern Sequence ((Scene#, DescriptionPattern)) Number of subjects
(1, A), (2, A), (3, G), (4, F), (5, A), (6, A), (7, A), (8, G), (9, F), (10, A) 2
(1, B), (2, B), (3, G), (4, H), (5, B), (6, B), (7, B), (8, G), (9, H), (10, B) 2
(1, N), (2, N), (3, K), (4, F), (5, A), (6, N), (7, N), (8, K), (9, F), (10, A) 6
(1, A), (2, A), (3, F), (4, F), (5, A), (6, A), (7, A), (8, F), (9, F), (10, A) 9
</table>
<tableCaption confidence="0.987852">
Table 3: Sequences of description patterns found more than once
</tableCaption>
<table confidence="0.999613375">
Attribute to Include Baseline (0-R) Using Scene Using Scene
Characteristics Characteristics
and Participant
Target Colour 78.33% 78.33% 89.57%
Target Size 57.46% 90.85% 90.85%
Relation 64.04% 65.00% 81.22%
Landmark Colour 74.80% 87.31% 93.74%
Landmark Size 88.92% 95.02% 95.02%
</table>
<tableCaption confidence="0.999782">
Table 4: Accuracy of Learning Attribute Inclusion; statistically significant increases (p&lt;.01) in bold.
</tableCaption>
<bodyText confidence="0.9998746">
supports, and some elements of the rules may be
due to artefacts of the specific stimuli used in the
data gathering. We would require a more diverse
set of stimuli to determine whether this is the case,
but the basic point stands: we can find correlations
between characteristics of the scenes and the pres-
ence or absence of particular attributes in referring
expressions, even if we cannot predict so well the
particular combinations of these correlations that
a given speaker will use in a given situation.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999972170731707">
There is a significant body of work on the use
of machine learning in referring expression gen-
eration, although typically focussed on aspects of
the problem that are distinct from those considered
here.
In the context of museum item descriptions,
Poesio et al. (1999) explore the decision of what
type of referring expression NP to use to refer to
a given discourse entity, using a statistical model
to choose between using a proper name, a definite
description, or a pronoun. More recently, Stoia et
al. (2006) attempt a similar task, but this time in
an interactive navigational domain; as well as de-
termining what type of referring expression to use,
they also try to learn whether a modifier should be
included. Cheng et al. (2001) try to learn rules for
the incorporation of non-referring modifiers into
noun phrases.
A number of the contributions to the 2008 GREC
and TUNA evaluation tasks (Gatt et al., 2008) have
made use of machine learning techniques. The
GREC task is primarily concerned with the choice
of form of reference (i.e. whether a proper name, a
descriptive NP or a pronoun should be used), and
so is less relevant to the focus of the present pa-
per. Much of the work on the TUNA Task is rel-
evant, however, since this also is concerned with
determining the content of referring expressions
in terms of the attributes used to build a distin-
guishing description. In particular, Fabbrizio et al.
(2008) explore the impact of individual style and
priming on attribute selection for referring expres-
sion generation, and Bohnet (2008) uses a nearest-
neighbour learning technique to acquire an indi-
vidual referring expression generation model for
each person.
Other related approaches to attribute selection
in the context of the TUNA task are explored in
(Gerv´as et al., 2008; de Lucena and Paraboni,
2008; Kelleher and Mac Namee, 2008; King,
2008).
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999853142857143">
We know that people’s referential behaviour varies
significantly. Despite this apparent variation, we
have demonstrated above that there does appear to
be a reasonable correlation between characteristics
of the scene and the incorporation of particular at-
tributes in a referring expression. One way to con-
ceptualise this is that the decision as to whether or
</bodyText>
<page confidence="0.997931">
64
</page>
<bodyText confidence="0.999975523809524">
not to incorporate a given feature such as colour
or size may vary from speaker to speaker; this is
evidenced by the data. We might think of these as
individual reference strategies; a good example of
such a strategy, widely attested across many exper-
iments, is the decision to include colour in a refer-
ring expression independent of its discriminatory
power, perhaps because it is an easily perceivable
and often-useful attribute. The overall approach to
reference that is demonstrated by a given speaker
then consists of the gathering together of a number
of strategies; the particular combinations may vary
from speaker to speaker, but as is demonstrated by
the analysis in this paper, some of the strategies
are widely used.
In current work, we are gathering a much larger
data set using more complex stimuli. This will al-
low the further development and testing of the ba-
sic ideas proposed in this paper as well as their
integration into a full referring expression genera-
tion algorithm.
</bodyText>
<sectionHeader confidence="0.99911" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999785926829268">
Anja Belz and Albert Gatt. 2007. The attribute selec-
tion for GRE challenge: Overview and evaluation
results. In Proceedings of UCNLG+MT: Language
Generation and Machine Translation, pages 75–83,
Copenhagen, Denmark.
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The GREC challenge 2008: Overview and
evaluation results. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 183–191, Salt Fork OH, USA.
Bernd Bohnet. 2008. The fingerprint of human refer-
ring expressions and their surface realization with
graph transducers. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion, pages 207–210, Salt Fork OH, USA.
Jean C. Carletta. 1992. Risk-taking and Recovery in
Task-Oriented Dialogue. Ph.D. thesis, University of
Edinburgh.
Hua Cheng, Massimo Poesio, Renate Henschel, and
Chris Mellish. 2001. Corpus-based NP modifier
generation. In Proceedings of the Second Meeting
of the North American Chapter of the Association
for Computational Linguistics, Pittsburgh PA, USA.
Diego Jesus de Lucena and Ivandr´e Paraboni. 2008.
USP-EACH: Frequency-based greedy attribute se-
lection for referring expressions generation. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 219–220, Salt
Fork OH, USA.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas
Bangalore. 2008. Referring expression generation
using speaker-based attribute selection and trainable
realization (ATTR). In Proceedings of the Fifth In-
ternational Natural Language Generation Confer-
ence, Salt Fork OH, USA.
Albert Gatt, Anja Belz, and Eric Kow. 2008. The
TUNA challenge 2008: Overview and evaluation re-
sults. In Proceedings of the Fifth International Nat-
ural Language Generation Conference, pages 198–
206, Salt Fork OH, USA.
Pablo Gerv´as, Raquel Herv´as, and Carlos Le´on. 2008.
NIL-UCM: Most-frequent-value-first attribute se-
lection and best-scoring-choice realization. In Pro-
ceedings of the Fifth International Natural Lan-
guage Generation Conference, pages 215–218, Salt
Fork OH, USA.
John D. Kelleher and Brian Mac Namee. 2008. Refer-
ring expression generation challenge 2008: DIT sys-
tem descriptions. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 221–223, Salt Fork OH, USA.
Josh King. 2008. OSU-GP: Attribute selection using
genetic programming. In Proceedings of the Fifth
International Natural Language Generation Confer-
ence, pages 225–226, Salt Fork OH, USA.
Massimo Poesio, Renate Henschel, Janet Hitzeman,
and Rodger Kibble. 1999. Statistical NP genera-
tion: A first report. In Proceedings of the ESSLLI
Workshop on NP Generation, Utrecht, The Nether-
lands.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2006. Noun phrase
generation for situated dialogs. In Proceedings of
the 4th International Conference on Natural Lan-
guage Generation, pages 81–88, Sydney, Australia.
Jette Viethen and Robert Dale. 2008a. Generating
referring expressions: What makes a difference?
In Australasian Language Technology Association
Workshop 2008, pages 160–168, Hobart, Australia.
Jette Viethen and Robert Dale. 2008b. The use of
spatial relations in referring expression generation.
In Proceedings of the 5th International Conference
on Natural Language Generation, pages 59–67, Salt
Fork OH, USA.
Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et
Theune, and Pascal Touset. 2008. Controlling re-
dundancy in referring expressions. In Proceedings
of the 6th Language Resources and Evaluation Con-
ference, Marrakech, Morocco.
Ian H. Witten and Frank Eibe. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.480957">
<title confidence="0.999731">Referring Expression Generation through Attribute-Based Heuristics</title>
<author confidence="0.994628">Robert Dale</author>
<author confidence="0.994628">Jette</author>
<affiliation confidence="0.824014">Centre for Language Macquarie</affiliation>
<address confidence="0.593866">Sydney,</address>
<email confidence="0.999238">rdale@ics.mq.edu.au|jviethen@ics.mq.edu.au</email>
<abstract confidence="0.999174666666667">In this paper, we explore a corpus of human-produced referring expressions to see to what extent we can learn the referential behaviour the corpus represents. Despite a wide variation in the way subjects refer across a set of ten stimuli, we demonstrate that component elements of the referring expression generation process appear to generalise across participants to a significant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Albert Gatt</author>
</authors>
<title>The attribute selection for GRE challenge: Overview and evaluation results.</title>
<date>2007</date>
<booktitle>In Proceedings of UCNLG+MT: Language Generation and Machine Translation,</booktitle>
<pages>75--83</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1208" citStr="Belz and Gatt, 2007" startWordPosition="178" endWordPosition="181"> significant degree. This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers.</context>
</contexts>
<marker>Belz, Gatt, 2007</marker>
<rawString>Anja Belz and Albert Gatt. 2007. The attribute selection for GRE challenge: Overview and evaluation results. In Proceedings of UCNLG+MT: Language Generation and Machine Translation, pages 75–83, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
<author>Jette Viethen</author>
<author>Albert Gatt</author>
</authors>
<title>The GREC challenge 2008: Overview and evaluation results.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>183--191</pages>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="1227" citStr="Belz et al., 2008" startWordPosition="182" endWordPosition="185">This leads us to propose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we r</context>
</contexts>
<marker>Belz, Kow, Viethen, Gatt, 2008</marker>
<rawString>Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2008. The GREC challenge 2008: Overview and evaluation results. In Proceedings of the Fifth International Natural Language Generation Conference, pages 183–191, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>The fingerprint of human referring expressions and their surface realization with graph transducers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Conference on Natural Language Generation,</booktitle>
<pages>207--210</pages>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="25645" citStr="Bohnet (2008)" startWordPosition="4477" endWordPosition="4478">chine learning techniques. The GREC task is primarily concerned with the choice of form of reference (i.e. whether a proper name, a descriptive NP or a pronoun should be used), and so is less relevant to the focus of the present paper. Much of the work on the TUNA Task is relevant, however, since this also is concerned with determining the content of referring expressions in terms of the attributes used to build a distinguishing description. In particular, Fabbrizio et al. (2008) explore the impact of individual style and priming on attribute selection for referring expression generation, and Bohnet (2008) uses a nearestneighbour learning technique to acquire an individual referring expression generation model for each person. Other related approaches to attribute selection in the context of the TUNA task are explored in (Gerv´as et al., 2008; de Lucena and Paraboni, 2008; Kelleher and Mac Namee, 2008; King, 2008). 6 Conclusions We know that people’s referential behaviour varies significantly. Despite this apparent variation, we have demonstrated above that there does appear to be a reasonable correlation between characteristics of the scene and the incorporation of particular attributes in a r</context>
</contexts>
<marker>Bohnet, 2008</marker>
<rawString>Bernd Bohnet. 2008. The fingerprint of human referring expressions and their surface realization with graph transducers. In Proceedings of the 5th International Conference on Natural Language Generation, pages 207–210, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean C Carletta</author>
</authors>
<title>Risk-taking and Recovery in Task-Oriented Dialogue.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="22576" citStr="Carletta, 1992" startWordPosition="3946" endWordPosition="3947">dmark in describing the intended referent. Or I might be the kind of speaker who surveys the scene and takes note of whether the landmark’s colour is distinctive; and so on. Thought of in this way, each speaker’s approach to reference is like a set of ‘parallel gestalts’ that contribute information to the description being constructed. The particular rules for inclusion that any speaker uses might vary depending on their personal past history, and perhaps even on the basis of situation-specific factors that on a given occasion might lean the speaker towards either being ‘risky’ or ‘cautious’ (Carletta, 1992). As alluded to earlier, the specific content of the rules shown in Figure 4 may appear idiosyncratic; they are just what the limited data in the corpus 63 Pattern Sequence ((Scene#, DescriptionPattern)) Number of subjects (1, A), (2, A), (3, G), (4, F), (5, A), (6, A), (7, A), (8, G), (9, F), (10, A) 2 (1, B), (2, B), (3, G), (4, H), (5, B), (6, B), (7, B), (8, G), (9, H), (10, B) 2 (1, N), (2, N), (3, K), (4, F), (5, A), (6, N), (7, N), (8, K), (9, F), (10, A) 6 (1, A), (2, A), (3, F), (4, F), (5, A), (6, A), (7, A), (8, F), (9, F), (10, A) 9 Table 3: Sequences of description patterns found </context>
</contexts>
<marker>Carletta, 1992</marker>
<rawString>Jean C. Carletta. 1992. Risk-taking and Recovery in Task-Oriented Dialogue. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Cheng</author>
<author>Massimo Poesio</author>
<author>Renate Henschel</author>
<author>Chris Mellish</author>
</authors>
<title>Corpus-based NP modifier generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh PA, USA.</location>
<contexts>
<context position="24832" citStr="Cheng et al. (2001)" startWordPosition="4337" endWordPosition="4340">cally focussed on aspects of the problem that are distinct from those considered here. In the context of museum item descriptions, Poesio et al. (1999) explore the decision of what type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GREC and TUNA evaluation tasks (Gatt et al., 2008) have made use of machine learning techniques. The GREC task is primarily concerned with the choice of form of reference (i.e. whether a proper name, a descriptive NP or a pronoun should be used), and so is less relevant to the focus of the present paper. Much of the work on the TUNA Task is relevant, however, since this also is concerned with determining the content of referring expressions in terms of the attribute</context>
</contexts>
<marker>Cheng, Poesio, Henschel, Mellish, 2001</marker>
<rawString>Hua Cheng, Massimo Poesio, Renate Henschel, and Chris Mellish. 2001. Corpus-based NP modifier generation. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, Pittsburgh PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Jesus de Lucena</author>
<author>Ivandr´e Paraboni</author>
</authors>
<title>USP-EACH: Frequency-based greedy attribute selection for referring expressions generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>219--220</pages>
<location>Salt Fork OH, USA.</location>
<marker>de Lucena, Paraboni, 2008</marker>
<rawString>Diego Jesus de Lucena and Ivandr´e Paraboni. 2008. USP-EACH: Frequency-based greedy attribute selection for referring expressions generation. In Proceedings of the Fifth International Natural Language Generation Conference, pages 219–220, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda J Stent</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Referring expression generation using speaker-based attribute selection and trainable realization (ATTR).</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<location>Salt Fork OH, USA.</location>
<marker>Di Fabbrizio, Stent, Bangalore, 2008</marker>
<rawString>Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas Bangalore. 2008. Referring expression generation using speaker-based attribute selection and trainable realization (ATTR). In Proceedings of the Fifth International Natural Language Generation Conference, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNA challenge 2008: Overview and evaluation results.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>198--206</pages>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="1247" citStr="Gatt et al., 2008" startWordPosition="186" endWordPosition="189">opose an alternative way of thinking of referring expression generation, where each attribute in a description is provided by a separate heuristic. 1 Introduction The last few years have witnessed a considerable move towards empiricism in referring expression generation; this is evidenced both by the growing body of work that analyses and tries to replicate the content of corpora of human-produced referring expressions, and particularly by the significant participation in the TUNA and GREC challenge tasks built around such activities (see, for example, (Belz and Gatt, 2007; Belz et al., 2008; Gatt et al., 2008)). One increasingly widespread observation—obvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we revisit the corpus of</context>
<context position="25012" citStr="Gatt et al., 2008" startWordPosition="4367" endWordPosition="4370">type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GREC and TUNA evaluation tasks (Gatt et al., 2008) have made use of machine learning techniques. The GREC task is primarily concerned with the choice of form of reference (i.e. whether a proper name, a descriptive NP or a pronoun should be used), and so is less relevant to the focus of the present paper. Much of the work on the TUNA Task is relevant, however, since this also is concerned with determining the content of referring expressions in terms of the attributes used to build a distinguishing description. In particular, Fabbrizio et al. (2008) explore the impact of individual style and priming on attribute selection for referring express</context>
</contexts>
<marker>Gatt, Belz, Kow, 2008</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2008. The TUNA challenge 2008: Overview and evaluation results. In Proceedings of the Fifth International Natural Language Generation Conference, pages 198– 206, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gerv´as</author>
<author>Raquel Herv´as</author>
<author>Carlos Le´on</author>
</authors>
<title>NIL-UCM: Most-frequent-value-first attribute selection and best-scoring-choice realization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>215--218</pages>
<location>Salt Fork OH, USA.</location>
<marker>Gerv´as, Herv´as, Le´on, 2008</marker>
<rawString>Pablo Gerv´as, Raquel Herv´as, and Carlos Le´on. 2008. NIL-UCM: Most-frequent-value-first attribute selection and best-scoring-choice realization. In Proceedings of the Fifth International Natural Language Generation Conference, pages 215–218, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Kelleher</author>
<author>Brian Mac Namee</author>
</authors>
<title>Referring expression generation challenge 2008: DIT system descriptions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>221--223</pages>
<location>Salt Fork OH, USA.</location>
<marker>Kelleher, Namee, 2008</marker>
<rawString>John D. Kelleher and Brian Mac Namee. 2008. Referring expression generation challenge 2008: DIT system descriptions. In Proceedings of the Fifth International Natural Language Generation Conference, pages 221–223, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josh King</author>
</authors>
<title>OSU-GP: Attribute selection using genetic programming.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>225--226</pages>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="25959" citStr="King, 2008" startWordPosition="4527" endWordPosition="4528">rned with determining the content of referring expressions in terms of the attributes used to build a distinguishing description. In particular, Fabbrizio et al. (2008) explore the impact of individual style and priming on attribute selection for referring expression generation, and Bohnet (2008) uses a nearestneighbour learning technique to acquire an individual referring expression generation model for each person. Other related approaches to attribute selection in the context of the TUNA task are explored in (Gerv´as et al., 2008; de Lucena and Paraboni, 2008; Kelleher and Mac Namee, 2008; King, 2008). 6 Conclusions We know that people’s referential behaviour varies significantly. Despite this apparent variation, we have demonstrated above that there does appear to be a reasonable correlation between characteristics of the scene and the incorporation of particular attributes in a referring expression. One way to conceptualise this is that the decision as to whether or 64 not to incorporate a given feature such as colour or size may vary from speaker to speaker; this is evidenced by the data. We might think of these as individual reference strategies; a good example of such a strategy, wide</context>
</contexts>
<marker>King, 2008</marker>
<rawString>Josh King. 2008. OSU-GP: Attribute selection using genetic programming. In Proceedings of the Fifth International Natural Language Generation Conference, pages 225–226, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renate Henschel</author>
<author>Janet Hitzeman</author>
<author>Rodger Kibble</author>
</authors>
<title>Statistical NP generation: A first report.</title>
<date>1999</date>
<booktitle>In Proceedings of the ESSLLI Workshop on NP Generation,</booktitle>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="24364" citStr="Poesio et al. (1999)" startWordPosition="4256" endWordPosition="4259">rmine whether this is the case, but the basic point stands: we can find correlations between characteristics of the scenes and the presence or absence of particular attributes in referring expressions, even if we cannot predict so well the particular combinations of these correlations that a given speaker will use in a given situation. 5 Related Work There is a significant body of work on the use of machine learning in referring expression generation, although typically focussed on aspects of the problem that are distinct from those considered here. In the context of museum item descriptions, Poesio et al. (1999) explore the decision of what type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GR</context>
</contexts>
<marker>Poesio, Henschel, Hitzeman, Kibble, 1999</marker>
<rawString>Massimo Poesio, Renate Henschel, Janet Hitzeman, and Rodger Kibble. 1999. Statistical NP generation: A first report. In Proceedings of the ESSLLI Workshop on NP Generation, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Stoia</author>
<author>Darla Magdalene Shockley</author>
<author>Donna K Byron</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>Noun phrase generation for situated dialogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 4th International Conference on Natural Language Generation,</booktitle>
<pages>81--88</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="24608" citStr="Stoia et al. (2006)" startWordPosition="4298" endWordPosition="4301">cular combinations of these correlations that a given speaker will use in a given situation. 5 Related Work There is a significant body of work on the use of machine learning in referring expression generation, although typically focussed on aspects of the problem that are distinct from those considered here. In the context of museum item descriptions, Poesio et al. (1999) explore the decision of what type of referring expression NP to use to refer to a given discourse entity, using a statistical model to choose between using a proper name, a definite description, or a pronoun. More recently, Stoia et al. (2006) attempt a similar task, but this time in an interactive navigational domain; as well as determining what type of referring expression to use, they also try to learn whether a modifier should be included. Cheng et al. (2001) try to learn rules for the incorporation of non-referring modifiers into noun phrases. A number of the contributions to the 2008 GREC and TUNA evaluation tasks (Gatt et al., 2008) have made use of machine learning techniques. The GREC task is primarily concerned with the choice of form of reference (i.e. whether a proper name, a descriptive NP or a pronoun should be used),</context>
</contexts>
<marker>Stoia, Shockley, Byron, Fosler-Lussier, 2006</marker>
<rawString>Laura Stoia, Darla Magdalene Shockley, Donna K. Byron, and Eric Fosler-Lussier. 2006. Noun phrase generation for situated dialogs. In Proceedings of the 4th International Conference on Natural Language Generation, pages 81–88, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>Generating referring expressions: What makes a difference?</title>
<date>2008</date>
<booktitle>In Australasian Language Technology Association Workshop</booktitle>
<pages>160--168</pages>
<location>Hobart, Australia.</location>
<contexts>
<context position="1913" citStr="Viethen and Dale, 2008" startWordPosition="288" endWordPosition="291">bvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we revisit the corpus of data that was introduced and discussed in (Viethen and Dale, 2008a; Viethen and Dale, 2008b) with the objective of determining what referential behaviour, if any, might be learned automatically from the data. We find that, despite the apparent diversity of the data when we consider the production of referring expressions across subjects, a closer examination reveals that individual attributes within referring expressions do appear to be selected on the basis of contextual factors with a high degree of consistency. This suggests that referring behaviour might be best thought of as consisting of a combination of lower-level heuristics, with each individual’s </context>
<context position="3223" citStr="Viethen and Dale, 2008" startWordPosition="502" endWordPosition="505"> these common heuristics. In Section 2 we describe the corpus we use for the experiments in this paper. In Section 3 we explore to what extent we can use this corpus to learn an algorithm for referring expression generation; in Section 4 we look more closely at the nature of individual variation within the corpus. Section 5 briefly discusses related work on the use of machine learning in referring expression generation, and Section 6 draws some conclusions and points to future work. 2 The Corpus 2.1 General Overview The corpus we use was collected via a data gathering experiment described in (Viethen and Dale, 2008a; Viethen and Dale, 2008b). The purpose of the data gathering was to gain some insight into how human subjects use relational referring expressions, a relatively unexplored aspect of referring expression generation. Participants visited a website, where they first saw an introductory page with a set of simple instructions and a sample stimulus scene consisting of three objects. Each participant was then assigned one of two trial sets of ten scenes each; the two trial sets are superficially Proceedings of the 12th European Workshop on Natural Language Generation, pages 58–65, Athens, Greece, 3</context>
<context position="4990" citStr="Viethen and Dale, 2008" startWordPosition="809" endWordPosition="812">d successively in a preset order, which was the same for each participant. Below each scene, the participant had to complete the sentence Please pick up the ... in a text box before clicking on a button to see the next scene. The task was to describe the target referent in the scene (marked by a grey arrow) in a way that would enable a friend looking at the same scene to pick it out from the other objects. The experiment was completed by 74 participants from a variety of different backgrounds and ages; most were university-educated and in their early or mid twenties. For reasons discussed in (Viethen and Dale, 2008b), the data of 11 participants was discarded. Of the remaining 63 participants, 29 were female, while 34 were male. 2.2 Stimulus Design The design of the stimuli used in the experiment is described in detail in (Viethen and Dale, 2008a). 1Scene 1 is paired with Scene 11, Scene 2 with Scene 12, and so on; in each pair, the only differences are the colour scheme used and the left–right orientation, with these variations being introduced to make the experiment less monotonous for subjects; (Viethen and Dale, 2008a) report that these characteristics of the scenes appear to have no significant eff</context>
<context position="7144" citStr="Viethen and Dale, 2008" startWordPosition="1195" endWordPosition="1198">balls. The objects also vary in two dimensions: colour (either green, blue, yellow, or red); and size (either large or small). To further reduce the number of factors in the scene design, the landmark and distractor are always placed clearly side by side, and the target is located on top of or directly in front of the landmark. Finally, to reduce the set of possible stimuli to a manageable number, five schemata (see Figure 2) were created as a basis for the final stimulus set. The design of these schemata was informed by a number of research questions with regard to the use of relations; see (Viethen and Dale, 2008b). A schema determines the type and size of each object in the scenes that are based on it, and determines which objects share colour. So, for example, in scenes based on Schema C, the target is a small ball; the landmark is a large cube with different colour from the target; and the distractor is a large ball sharing its colour with the target. 59 Label Pattern Example (tg col, tg type) (tg col, tg type, rel, lm col, lm type) (tg col, tg type, rel, lm size, lm col, lm type) (tg col, tg type, rel, lm size, lm type) (tg col, tg type, rel, lm type) (tg size, tg col, tg type) (tg size, tg col, t</context>
<context position="17130" citStr="Viethen and Dale, 2008" startWordPosition="3021" endWordPosition="3024">evious section, this time using the participant ID as well as the scene characteristics in Table 2 as features. This improved pattern prediction to 57.62%. This suggests that individual differences may indeed be capturable from the data, although we would need more data than the mere 10 examples we have from each subject to learn a good predictive model. In the face of this lack of data, another approach is to look for commonalities in the data in terms of the constituent elements of the different reference patterns used for each scene. This way of thinking about the data was foreshadowed by (Viethen and Dale, 2008b), who observed that the subjects could be separated into those who always used relations, those who never used relations, and those who sometimes used relations. This leads us to consider whether there are characteristics of scenes or speakers which are highly likely to result in specific attributes being used in descriptions. If this is the case, a decision tree learner should be able to learn for each individual attribute whether it should be included in a given situation. An appropriate baseline for any experiments here is the success rate of simply including or not including each attribu</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008a. Generating referring expressions: What makes a difference? In Australasian Language Technology Association Workshop 2008, pages 160–168, Hobart, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>The use of spatial relations in referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Conference on Natural Language Generation,</booktitle>
<pages>59--67</pages>
<location>Salt Fork OH, USA.</location>
<contexts>
<context position="1913" citStr="Viethen and Dale, 2008" startWordPosition="288" endWordPosition="291">bvious in hindsight, but surprisingly absent from much earlier work on referring expression generation—is that one person’s referential behaviour differs from that of another: given the same referential task, different subjects will choose different referring expressions to identify a target referent. Faced with this apparent lack of cross-speaker consistency in how to refer to entities, we might question the validity of any exercise that tries to develop an algorithm on the basis of data from multiple speakers. In this paper we revisit the corpus of data that was introduced and discussed in (Viethen and Dale, 2008a; Viethen and Dale, 2008b) with the objective of determining what referential behaviour, if any, might be learned automatically from the data. We find that, despite the apparent diversity of the data when we consider the production of referring expressions across subjects, a closer examination reveals that individual attributes within referring expressions do appear to be selected on the basis of contextual factors with a high degree of consistency. This suggests that referring behaviour might be best thought of as consisting of a combination of lower-level heuristics, with each individual’s </context>
<context position="3223" citStr="Viethen and Dale, 2008" startWordPosition="502" endWordPosition="505"> these common heuristics. In Section 2 we describe the corpus we use for the experiments in this paper. In Section 3 we explore to what extent we can use this corpus to learn an algorithm for referring expression generation; in Section 4 we look more closely at the nature of individual variation within the corpus. Section 5 briefly discusses related work on the use of machine learning in referring expression generation, and Section 6 draws some conclusions and points to future work. 2 The Corpus 2.1 General Overview The corpus we use was collected via a data gathering experiment described in (Viethen and Dale, 2008a; Viethen and Dale, 2008b). The purpose of the data gathering was to gain some insight into how human subjects use relational referring expressions, a relatively unexplored aspect of referring expression generation. Participants visited a website, where they first saw an introductory page with a set of simple instructions and a sample stimulus scene consisting of three objects. Each participant was then assigned one of two trial sets of ten scenes each; the two trial sets are superficially Proceedings of the 12th European Workshop on Natural Language Generation, pages 58–65, Athens, Greece, 3</context>
<context position="4990" citStr="Viethen and Dale, 2008" startWordPosition="809" endWordPosition="812">d successively in a preset order, which was the same for each participant. Below each scene, the participant had to complete the sentence Please pick up the ... in a text box before clicking on a button to see the next scene. The task was to describe the target referent in the scene (marked by a grey arrow) in a way that would enable a friend looking at the same scene to pick it out from the other objects. The experiment was completed by 74 participants from a variety of different backgrounds and ages; most were university-educated and in their early or mid twenties. For reasons discussed in (Viethen and Dale, 2008b), the data of 11 participants was discarded. Of the remaining 63 participants, 29 were female, while 34 were male. 2.2 Stimulus Design The design of the stimuli used in the experiment is described in detail in (Viethen and Dale, 2008a). 1Scene 1 is paired with Scene 11, Scene 2 with Scene 12, and so on; in each pair, the only differences are the colour scheme used and the left–right orientation, with these variations being introduced to make the experiment less monotonous for subjects; (Viethen and Dale, 2008a) report that these characteristics of the scenes appear to have no significant eff</context>
<context position="7144" citStr="Viethen and Dale, 2008" startWordPosition="1195" endWordPosition="1198">balls. The objects also vary in two dimensions: colour (either green, blue, yellow, or red); and size (either large or small). To further reduce the number of factors in the scene design, the landmark and distractor are always placed clearly side by side, and the target is located on top of or directly in front of the landmark. Finally, to reduce the set of possible stimuli to a manageable number, five schemata (see Figure 2) were created as a basis for the final stimulus set. The design of these schemata was informed by a number of research questions with regard to the use of relations; see (Viethen and Dale, 2008b). A schema determines the type and size of each object in the scenes that are based on it, and determines which objects share colour. So, for example, in scenes based on Schema C, the target is a small ball; the landmark is a large cube with different colour from the target; and the distractor is a large ball sharing its colour with the target. 59 Label Pattern Example (tg col, tg type) (tg col, tg type, rel, lm col, lm type) (tg col, tg type, rel, lm size, lm col, lm type) (tg col, tg type, rel, lm size, lm type) (tg col, tg type, rel, lm type) (tg size, tg col, tg type) (tg size, tg col, t</context>
<context position="17130" citStr="Viethen and Dale, 2008" startWordPosition="3021" endWordPosition="3024">evious section, this time using the participant ID as well as the scene characteristics in Table 2 as features. This improved pattern prediction to 57.62%. This suggests that individual differences may indeed be capturable from the data, although we would need more data than the mere 10 examples we have from each subject to learn a good predictive model. In the face of this lack of data, another approach is to look for commonalities in the data in terms of the constituent elements of the different reference patterns used for each scene. This way of thinking about the data was foreshadowed by (Viethen and Dale, 2008b), who observed that the subjects could be separated into those who always used relations, those who never used relations, and those who sometimes used relations. This leads us to consider whether there are characteristics of scenes or speakers which are highly likely to result in specific attributes being used in descriptions. If this is the case, a decision tree learner should be able to learn for each individual attribute whether it should be included in a given situation. An appropriate baseline for any experiments here is the success rate of simply including or not including each attribu</context>
</contexts>
<marker>Viethen, Dale, 2008</marker>
<rawString>Jette Viethen and Robert Dale. 2008b. The use of spatial relations in referring expression generation. In Proceedings of the 5th International Conference on Natural Language Generation, pages 59–67, Salt Fork OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
<author>Pascal Touset</author>
</authors>
<title>Controlling redundancy in referring expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Language Resources and Evaluation Conference,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="18256" citStr="Viethen et al., 2008" startWordPosition="3199" endWordPosition="3202"> for any experiments here is the success rate of simply including or not including each attribute (basically a 0-R majority class classifier), irrespective of the characteristics of the scene. Table 4 compares the results for this ‘context-free’ approach with one model that is trained on the characteristics of scenes, and another that takes both the characteristics of scenes and the participant ID into account.5 Interestingly, the ‘context-free’ strategies work suprisingly well for predicting the inclusion of some attributes in the human data. As has been noted in other work (see for example (Viethen et al., 2008)), colour is often included in referring expressions irrespective of its discriminatory power, and this is borne out by the data here. Perhaps more suprising is the large degree to which the inclusion of landmark size is captured by a contextfree strategy. 5As before, the results reported are for the accuracy of a pruned J48 decision tree, under 10-fold cross-validation. 62 Improvement on all attribues other than target colour improves when we take into account the characteristics of the scenes, consistent with our assumptions that context does matter. When we add participant ID to the feature</context>
</contexts>
<marker>Viethen, Dale, Krahmer, Theune, Touset, 2008</marker>
<rawString>Jette Viethen, Robert Dale, Emiel Krahmer, Mari¨et Theune, and Pascal Touset. 2008. Controlling redundancy in referring expressions. In Proceedings of the 6th Language Resources and Evaluation Conference, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Frank Eibe</author>
</authors>
<title>edition.</title>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="13647" citStr="Witten and Eibe, 2005" startWordPosition="2395" endWordPosition="2398">, 9 and 10) result in only five semantically distinct referring expression forms, whereas Scene 7 results in 12 distinct referring expression forms. All of these are distinguishing descriptions, so all are acceptable forms of reference, although some contain more redundancy than others. Most obvious from the chart is that, for many scenes, there is a predominant form of reference used; so, for example, pattern F ((tg size, tg col, tg type)) accounts for 43 (68%) of the descriptions used in Scene 4, and pattern A ((tg col, tg type)) is very frequently used in a number of scenes.3 We used Weka (Witten and Eibe, 2005) with the J48 decision tree classifer to see what correspondences might be learned between the characterisics of the scenes listed in Table 2 and the forms of referring expression used for the target referents, as shown in Table 1. The pruned decision tree learned by this method predicted the actual form of reference used in only 48% of cases under 10-fold cross-validation, but given that there are many ‘gold standard’ descriptions for each scene, 3The chart as presented here is obviously too small to enable detailed examination, and our use of colour coding will be of no value in a monchrome </context>
</contexts>
<marker>Witten, Eibe, 2005</marker>
<rawString>Ian H. Witten and Frank Eibe. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>