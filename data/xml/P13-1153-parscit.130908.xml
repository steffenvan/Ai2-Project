<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001653">
<title confidence="0.830566">
Named Entity Recognition using Cross-lingual Resources: Arabic as an
Example
</title>
<author confidence="0.995957">
Kareem Darwish
</author>
<affiliation confidence="0.8627025">
Qatar Computing Research Institute
Doha, Qatar
</affiliation>
<email confidence="0.996395">
kdarwish@qf.org.qa
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999883238095238">
Some languages lack large knowledge bases
and good discriminative features for Name
Entity Recognition (NER) that can general-
ize to previously unseen named entities. One
such language is Arabic, which: a) lacks a
capitalization feature; and b) has relatively
small knowledge bases, such as Wikipedia. In
this work we address both problems by in-
corporating cross-lingual features and knowl-
edge bases from English using cross-lingual
links. We show that such features have a
dramatic positive effect on recall. We show
the effectiveness of cross-lingual features and
resources on a standard dataset as well as
on two new test sets that cover both news
and microblogs. On the standard dataset, we
achieved a 4.1% relative improvement in F-
measure over the best reported result in the
literature. The features led to improvements
of 17.1% and 20.5% on the new news and mi-
croblogs test sets respectively.
</bodyText>
<sectionHeader confidence="0.999508" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998859377358491">
Named Entity Recognition (NER) is essential for a
variety of Natural Language Processing (NLP) ap-
plications such as information extraction. There has
been a fair amount of work on NER for a variety of
languages including Arabic. To train an NER sys-
tem, some of the following feature types are typi-
cally used (Benajiba and Rosso, 2008; Nadeau and
Sekine, 2009):
- Orthographic features: These features include
capitalization, punctuation, existence of digits, etc.
One of the most effective orthographic features is
capitalization in English, which helps NER to gener-
alize to new text of different genres. However, capi-
talization is not very useful in some languages such
as German, and nonexistent in other languages such
as Arabic. Further, even in English social media,
capitalization may be inconsistent.
- Contextual features: Certain words are indica-
tive of the existence of named entities. For example,
the word “said” is often preceded by a named en-
tity of type “person” or “organization”. Sequence
labeling algorithms (ex. Conditional Random Fields
(CRF)) can often identify such indicative words.
- Character-level features: These features typ-
ically include the leading and trailing letters of
words. In some languages, these letters could pre-
fixes and suffixes. Such features can be indicative or
counter-indicative of the existence of named entities.
For example, a word ending with “ing” is typically
not a named entity, while a word ending in “berg” is
often a named entity.
- Part-of-speech (POS) tags and morphological
features: POS tags indicate (or counter-indicate) the
possible presence of a named entity at word level or
at word sequence level. Morphological features can
mostly indicate the absence of named entities. For
example, Arabic allows the attachment of pronouns
to nouns and verbs. However, pronouns are rarely
ever attached to named entities.
- Gazetteers: This feature checks the presence of
a word or a sequence of words in large lists of named
entities. If gazetteers are small, then they would
have low coverage, and if they are very large then
their entries may be ambiguous. For example, “syn-
tax” may refer to sentence construction or the music
band “Syntax”.
Typically, a subset of these features are available
for different languages. For example, morpholog-
ical, contextual, and character-level features have
been shown to be effective for Arabic NER (Bena-
jiba and Rosso, 2008). However, Arabic lacks in-
dicative orthographic features that generalize to pre-
viously unseen named entities. Also, although some
</bodyText>
<page confidence="0.953343">
1558
</page>
<note confidence="0.913405">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1558–1567,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.993098965517241">
of the Arabic gazetteers that were used for NER
were small (Benajiba and Rosso, 2008), there has
been efforts to build larger Arabic gazetteers (Attia
et al., 2010). Since training and test parts of stan-
dard datasets for Arabic NER are drawn from the
same genre in relatively close temporal proximity,
a named entity recognizer that simply memorizes
named entities in the training set generally performs
well on such test sets. Thus, the results that are re-
ported in the literature are generally high (Abdul-
Hamid and Darwish, 2010; Benajiba et al., 2008).
We illustrate the limited capacity of existing recog-
nizers to generalize to previously unseen named en-
tities using two new test sets that include microblogs
as well as news texts that cover local and interna-
tional politics, economics, health, sports, entertain-
ment, and science. As we will show later, recall is
well below 50% for all named entity types on the
new test sets.
To address this problem, we introduce the use
of cross-lingual links between a disadvantaged lan-
guage, Arabic, and a language with good discrim-
inative features and large resources, English, to
improve Arabic NER. We exploit English’s ortho-
graphic features, particularly capitalization, as well
as Arabic and English Wikipedias, including exist-
ing annotations from large knowledge sources such
as DBpedia. We also show how to use transliter-
ation mining to improve NER, even when neither
language has a capitalization (or similar) feature.
The intuition is that if the translation of a word is
in fact a transliteration, then the word is likely a
named entity. Cross-lingual links are obtained using
Wikipedia cross-language links and a large Machine
Translation (MT) phrase table that is true cased,
where word casing is preserved during training. We
show the effectiveness of these new features on a
standard dataset as well as two new test sets. The
contributions of this paper are as follows:
- Using cross-lingual links to exploit orthographic
features in other languages.
- Employing transliteration mining to improve NER.
- Using cross-lingual links to exploit a large knowl-
edge base, namely English DBpedia, to benefit
NER.
- Introducing two new NER test sets for Arabic that
include recent news as well as microblogs. We plan
to release these test sets.
- Improving over the best reported results in the liter-
ature by 4.1% (Abdul-Hamid and Darwish, 2010) by
strictly adding cross-lingual features. We also show
improvements of 17.1% and 20.5% on the new test
sets.
The remainder of the paper is organized as fol-
lows: Section 2 provides related work; Section 3 de-
scribes the baseline system; Section 4 introduces the
cross-lingual features and reports on their effective-
ness; and Section 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999959">
2.1 Using cross-lingual Features
</subsectionHeader>
<bodyText confidence="0.999830147058824">
For many NLP tasks, some languages may have sig-
nificantly more training data, better knowledge re-
sources, or more discriminating features than other
languages. If cross-lingual resources are available,
such as parallel data, increased training data, better
resources, or superior features can be used to im-
prove the processing (ex. tagging) for other lan-
guages (Ganchev et al., 2009; Shi et al., 2010;
Yarowsky and Ngai, 2001). Some work has at-
tempted to use bilingual features in NER. Burkett
et al. (2010) used bilingual text to improve mono-
lingual models including NER models for German,
which lacks a good capitalization feature. They did
so by training a bilingual model and then generat-
ing more training data from unlabeled parallel data.
They showed significant improvement in German
NER effectiveness, particularly for recall. In our
work, there is no need for tagged text that has a
parallel equivalent in another language. Benajiba et
al. (2008) used an Arabic English dictionary from
MADA, an Arabic analyzer, to indicate if a word
is capitalized in English or not. They reported that
it was the second most discriminating feature that
they used. However, there seems to be room for im-
provement because: (1) MADA’s dictionary is rela-
tively small and would have low coverage; and (2)
the use of such a binary feature is problematic, be-
cause Arabic names are often common Arabic words
and hence a word may be translated as a named en-
tity and as a common word. To overcome these two
problems, we use cross-lingual features to improve
NER using large bilingual resources, and we incor-
porate confidences to avoid having a binary feature.
Richman and Schone (2008) used English linguis-
</bodyText>
<page confidence="0.99409">
1559
</page>
<bodyText confidence="0.999857">
tic tools and cross language links in Wikipedia to
automatically annotate text in different languages.
Transliteration Mining (TM) has been used to en-
rich MT phrase tables or to improve cross language
search (Udupa et al., 2009). Conversely, people have
used NER to determine if a word is to be transliter-
ated or not (Hermjakob et al., 2008). However, we
are not aware of any prior work on using TM to de-
termine if a sequence is a NE. Further, we are not
aware of prior work on using TM (or transliteration
in general) as a cross lingual feature in any annota-
tion task. In our work, we use state-of-the-art TM as
described by El-Kahki et al. (2011)
</bodyText>
<subsectionHeader confidence="0.994695">
2.2 Arabic NER
</subsectionHeader>
<bodyText confidence="0.999713689189189">
Much work has been done on NER with mul-
tiple public evaluation forums. Nadeau and
Sekine (Nadeau and Sekine, 2009) surveyed lots of
work on NER for a variety of languages. Signifi-
cant work has been conducted by Benajiba and col-
leagues on Arabic NER (Benajiba and Rosso, 2008;
Benajiba et al., 2008; Benajiba and Rosso, 2007;
Benajiba et al., 2007). Benajiba et al. (2007) used
a maximum entropy classifier trained on a feature
set that includes the use of gazetteers and a stop-
word list, appearance of a NE in the training set,
leading and trailing word bigrams, and the tag of the
previous word. They reported 80%, 37%, and 47%
F-measure for locations, organizations, and persons
respectively on the ANERCORP dataset that they
created and publicly released. Benajiba and Rosso
(2007) improved their system by incorporating POS
tags to improve NE boundary detection. They re-
ported 87%, 46%, and 52% F-measure for loca-
tions, organizations, and persons respectively. Be-
najiba and Rosso (2008) used CRF sequence label-
ing and incorporated many language specific fea-
tures, namely POS tagging, base-phrase chunking,
Arabic tokenization, and adjectives indicating na-
tionality. They reported that tokenization generally
improved recall. Using POS tagging generally im-
proved recall at the expense of precision, leading
to overall improvements in F-measure. Using all
their suggested features, they reported 90%, 66%,
and 73% F-measure for location, organization, and
persons respectively. In Benajiba et al. (2008),
they examined the same feature set on the Auto-
matic Content Extraction (ACE) datasets using CRF
sequence labeling and a Support Vector Machine
(SVM) classifier. They did not report per category
F-measure, but they reported overall 81%, 75%, and
78% macro-average F-measure for broadcast news
and newswire on the ACE 2003, 2004, and 2005
datasets respectively. Huang (2005) used an HMM-
based NE recognizer for Arabic and reported 77%
F-measure on the ACE 2003 dataset. Farber et
al. (2008) used POS tags obtained from an Ara-
bic tagger to enhance NER. They reported 70% F-
measure on the ACE 2005 dataset. Shaalan and
Raza (2007) reported on a rule-based system that
uses hand crafted grammars and regular expressions
in conjunction with gazetteers. They reported up-
wards of 93% F-measure, but they conducted their
experiments on non-standard datasets, making com-
parison difficult. Abdul-Hamid and Darwish (2010)
used a simplified feature set that relied primarily on
character level features, namely leading and trailing
letters in a word. They also experimented with a
variety of phrase level features with little success.
They reported an F-measure of 76% and 81% for
the ACE2005 and the ANERCorp datasets datasets
respectively. We used their simplified features in our
baseline system. The different experiments reported
in the literature may not have been done on the same
training/test splits. Thus, the results may not be
completely comparable. Mohit et al. (2012) per-
formed NER on a different genre from news, namely
Arabic Wikipedia articles, and reported recall values
as low as 35.6%. They used self training and recall
oriented classification to improve recall, typically at
the expense of precision. McNamee and Mayfield
(2002) and Mayfield et al. (2003) used thousands
of language independent features such as character
n-grams, capitalization, word length, and position in
a sentence, along with language dependent features
such as POS tags and BP chunking. The use of CRF
sequence labeling for NER has shown success (Mc-
Callum and Li, 2003; Nadeau and Sekine, 2009; Be-
najiba and Rosso, 2008).
</bodyText>
<sectionHeader confidence="0.993611" genericHeader="method">
3 Baseline Arabic NER System
</sectionHeader>
<bodyText confidence="0.999275333333333">
For the baseline system, we used the CRF++1 im-
plementation of CRF sequence labeling with default
parameters. We opted to reimplement the most suc-
</bodyText>
<footnote confidence="0.986996">
1http://code.google.com/p/crfpp/
</footnote>
<page confidence="0.987782">
1560
</page>
<bodyText confidence="0.999975766666667">
cessful features that were reported by Benajiba et
al. (2008) and Abdul-Hamid and Darwish (2010),
namely the leading and trailing 1, 2, 3, and 4 letters
in a word; whether a word appears in the gazetteer
that was created by Benajiba et al. (2008), which
is publicly available, but is rather small (less than
5,000 entries); and the stemmed form of words (after
removing coordinating conjunctions, prepositions,
and determiners using a rule-based stemmer akin
to (Larkey et al., 2002)). As mentioned earlier, the
leading and trailing letters in a word may indicate
or counter-indicate the presence of named entities.
Stemming is important due to the morphological
complexity of Arabic. We used the previous and
the next words in their raw and stemmed forms as
features. For training and testing, we used the AN-
ERCORP dataset (Benajiba and Rosso, 2007). The
dataset has approximately 150k tokens and we used
the 80/20 training/test splits of Abdul-Hamid and
Darwish (2010), who graciously provided us with
their splits of the collection and they achieved the
best reported results on the dataset. We will re-
fer to their results, which are provided in Table 1,
as “baseline-lit”. Table 2 (a) shows our results on
the ANERCORP dataset. Our results were slightly
lower than their results (Abdul-Hamid and Darwish,
2010). It is noteworthy that 69% of the named enti-
ties in the test part were seen during training.
We also created two new test sets. The first test
set is composed of news snippets from the RSS feed
of the Arabic (Egypt) version of news.google.com
from Oct. 6, 2012. The RSS feed contains the
headline and the first 50-100 words in the news ar-
ticles. The set has news from over a dozen differ-
ent news sources and covers international and local
news, politics, financial news, health, sports, enter-
tainment, and technology. This set contains roughly
15k tokens. The second set contains a set of 1,423
tweets that were randomly selected from tweets au-
thored between November 23, 2011 and Novem-
ber 27, 2011. We scraped tweets from Twitter us-
ing the query “lang:ar” (language=Arabic). This set
contains approximately 26k tokens. The test sets
will be henceforth be referred to as the NEWS and
TWEETS sets respectively. They were annotated by
one person, a native Arabic speaker, using the Lin-
guistics Data Consortium tagging guidelines. Ta-
ble 2 (b) and (c) report on the results for the baseline
system on both test sets. The results on the NEWS
test are substantially lower than those for ANER-
CORP. It is worth noting that only 27% of the named
entities in the NEWS test set were observed in the
training set (compared to 69% for ANERCORP). As
Table 3 shows for the ANERCORP dataset, using
only the tokens as features, where the labeler mainly
memorizes previously seen named entities, yields
higher results than the baseline results for the NEWS
dataset (Table 2 (b)). The results on the TWEETS
test are very poor, with 24% of the named entities in
the test set appearing in the training set.
</bodyText>
<table confidence="0.992608166666667">
ANERCORP Dataset
Precision Recall Fβ=1
LOC 93 83 88
ORG 84 64 73
PERS 90 75 82
Overall 89 74 81
</table>
<tableCaption confidence="0.945283">
Table 1: “Baseline-lit” Results from (Abdul-Hamid and
Darwish, 2010)
</tableCaption>
<table confidence="0.999893055555556">
(a) ANERCORP Dataset
Precision Recall Fβ=1
LOC 93.6 83.3 88.1
ORG 83.8 61.2 70.8
PERS 84.3 64.4 73.0
Overall 88.9 72.5 79.9
(b) NEWS Test Set
Precision Recall Fβ=1
LOC 84.1 53.2 65.1
ORG 73.2 23.2 35.2
PERS 74.8 47.1 57.8
Overall 78.0 41.9 54.6
(c) TWEETS Test Set
Precision Recall Fβ=1
LOC 79.9 27.1 40.4
ORG 44.4 9.1 15.1
PERS 45.7 27.8 34.5
Overall 58.0 23.1 33.1
</table>
<tableCaption confidence="0.894823">
Table 2: Baseline Results for the three test sets
</tableCaption>
<table confidence="0.999654">
ANERCORP Dataset
Precision Recall Fβ=1
LOC 95.3 62.7 75.6
ORG 86.3 44.7 58.9
PERS 85.4 36.4 51.0
Overall 91.0 50.0 64.5
</table>
<tableCaption confidence="0.959382">
Table 3: Results of using only tokens as features on AN-
ERCORP
</tableCaption>
<page confidence="0.990036">
1561
</page>
<sectionHeader confidence="0.999339" genericHeader="method">
4 Cross-lingual Features
</sectionHeader>
<bodyText confidence="0.999801571428572">
We experimented with three different cross-lingual
features that used Arabic and English Wikipedia
cross-language links and a true-cased phrase ta-
ble that was generated using Moses (Koehn et al.,
2007). True-casing preserves case information dur-
ing training. We used the Arabic Wikipedia snap-
shot from September 28, 2012. The snapshot has
348,873 titles including redirects, which are alter-
native names to articles. Of these articles, 254,145
have cross-lingual links to English Wikipedia. We
used DBpedia 3.8 which includes 6,157,591 entries
of Wikipedia titles and their “types”, such as “per-
son”, “plant”, or “device”, where a title can have
multiple types. The phrase table was trained on a set
of 3.69 million parallel sentences containing 123.4
million English tokens. The sentences were drawn
from the UN parallel data along with a variety of
parallel news data from LDC and the GALE project.
The Arabic side was stemmed (by removing just pre-
fixes) using the Stanford word segmenter (Green and
DeNero, 2012).
</bodyText>
<subsectionHeader confidence="0.921973">
4.1 Cross-lingual Capitalization
</subsectionHeader>
<bodyText confidence="0.991066285714286">
As we mentioned earlier, Arabic lacks capitalization
and Arabic names are often common Arabic words.
For example, the Arabic name “Hasan” means good.
To capture cross-lingual capitalization, we used the
aforementioned true-cased phrase table at word and
phrase levels as follows:
Input: True-cased phrase table PT, sentence S containing n words
</bodyText>
<equation confidence="0.8430622">
w0..n, max sequence length l, translations T1..k..m of wi..j
for i = 0 → n do
j = min(i + l − 1, n)
if PT contains wi..j &amp; ∃ Tk isCaps then
weight(wi..j) =
</equation>
<bodyText confidence="0.6026635">
round weight(wi..j) to first significant figure
set tag of wi = B-CAPS-weight
</bodyText>
<figure confidence="0.466277625">
set tag for words wi+1..j = I-CAPS-weight
else
if j &gt; i then
j- -
else
tag of wi = null
end if
end if
</figure>
<subsectionHeader confidence="0.542126">
end for
</subsectionHeader>
<bodyText confidence="0.998312419354839">
Where: PT was the aforementioned phrase ta-
ble; l = 4; P(Tk) equaled to the product of
p(source|target) and p(targetlsource) for a word
sequence; isCaps and notCaps were whether the
translation was capitalized or not respectively; and
the weights were binned because CRF++ only takes
nominal features. In essence we tried every subse-
quence of S of length l or less to see if the translation
was capitalized. A subsequence can be 1 word long.
We tried longer sequences first. To determine if the
corresponding phrase was capitalized (isCaps), all
non-function words on the English side needed to be
capitalized. As an example, the phrase ø� XAêË@ ¡J�jÖÏ@
(meaning ”Pacific Ocean”) was translated to a cap-
italized phrase 36.7% of the time. Thus, the word
¡J�jÖÏ@ was assigned B-CAPS-0.4 and ø� XAêË@ was
assigned I-CAPS-0.4. Using weights avoids using
capitalization as a binary feature.
Table 4 reports on the results of the baseline
system with the capitalization feature on the three
datasets. In comparing baseline results in Table 2
and cross-lingual capitalization results in Table 4,
recall consistently increased for all datasets, par-
ticularly for “persons” and “locations”. For the
different test sets, recall increased by 3.1 to 6.1
points (absolute) or by 8.4% to 13.6% (relative).
This led to an overall improvement in F-measure of
1.8 to 3.4 points (absolute) or 4.2% to 5.7% (rela-
tive). Precision dropped overall on the ANERCORP
dataset and dropped substantially for the NEWS and
TWEETS test sets.
</bodyText>
<table confidence="0.999498611111111">
(a) ANERCORP Dataset
Precision Recall Fβ=1
LOC 92.0/-1.6/-1.7 86.8/3.5/4.2 89.3/1.2/1.4
ORG 82.8/-1.1/-1.3 63.9/2.7/4.4 72.1/1.4/1.9
PERS 86.0/1.7/2.0 75.4/11.0/17.1 80.3/7.3/10.1
Overall 88.4/-0.4/-0.5 78.6/6.1/8.4 83.2/3.4/4.2
(b) NEWS Test Set
Precision Recall Fβ=1
LOC 82.1/-2.0/-2.4 59.0/5.8/11.0 68.7/3.5/5.4
ORG 68.4/-4.9/-6.6 23.2/0.0/0.0 34.6/-0.6/-1.7
PERS 70.7/-4.0/-5.4 55.6/8.4/17.9 62.2/4.4/7.6
Overall 74.5/-3.5/-4.5 47.0/5.1/12.2 57.7/3.1/5.7
(c) TWEETS Test Set
Precision Recall Fβ=1
LOC 76.9/-3.0/-3.7 27.9/0.9/3.2 41.0/0.5/1.4
ORG 44.4/0.0/0.0 10.4/1.3/14.3 16.8/1.8/11.6
PERS 40.0/-5.7/-12.5 35.0/7.3/26.2 37.3/2.8/8.1
Overall 51.8/-6.2/-10.7 26.3/3.1/13.6 34.9/1.8/5.4
</table>
<tableCaption confidence="0.9869095">
Table 4: Results with cross-lingual capitalization with
/absolute/relative differences compared to baseline
</tableCaption>
<table confidence="0.4876725">
� P(Tk)
Tk %sCaps
E P(Tk)+ E P(Tk)
Tk %sCaps Tk notCaps
</table>
<page confidence="0.966797">
1562
</page>
<subsectionHeader confidence="0.970258">
4.2 Transliteration Mining
</subsectionHeader>
<bodyText confidence="0.999988481481481">
An alternative to capitalization can be translitera-
tion mining. The intuition is that named entities are
often transliterated, particularly the names of loca-
tions and persons. This feature is helpful if cross-
lingual resources do not have capitalization infor-
mation, or if the “helper” language to be consulted
does not have a useful capitalization feature. We per-
formed transliteration mining (aka cognate match-
ing) at word level for each Arabic word against all
its possible translations in the phrase table. We
used a transliteration miner akin to that of El-Kahki
et al. (2011) that was trained using 3,452 parallel
Arabic-English transliteration pairs. We aligned the
word-pairs at character level using GIZA++ and the
phrase extractor and scorer from the Moses machine
translation package (Koehn et al., 2007). The align-
ment produced mappings between English letters se-
quences and Arabic letter sequences with associated
mapping probabilities. Given an Arabic word, we
produced all its possible segmentations along with
their associated mappings into English letters. We
retained valid target sequences that produced trans-
lations in the phrase table.
Again we used a weight similar to the one for
cross-lingual capitalization and we rounded the val-
ues of the ratio the significant figure. The weights
were computed as:
</bodyText>
<equation confidence="0.999238166666667">
E P(Tk)
Tk isTransliteration
E �
P(Tk) + P(Tk)
Tk isTransliteration Tk notTransliteration
(1)
</equation>
<bodyText confidence="0.998388875">
where P(Tk) is probability of the kth translation of
a word in the phrase table.
If a word was not found in the phrase table, the
feature value was assigned null. For example, if the
translations of the word `‚k are “Hasan”, “Has-
san”, and “good”, where the first two are transliter-
ations and the last not, then the weight of the word
would be:
</bodyText>
<equation confidence="0.999677666666667">
P(Hasan |&amp;‚k) + P(Hassan |&amp;‚k)
(2)
P(Hasan |á‚k) + P (Hassan |á‚k) + P (good |á‚k)
</equation>
<bodyText confidence="0.999972466666667">
In our experiments, the weight of `‚k was equal
to 0.5 (after rounding). Table 5 reports on the re-
sults using the baseline system with the transliter-
ation mining feature. Like the capitalization fea-
ture, transliteration mining slightly lowered preci-
sion – except for the TWEETS test set where the
drop in precision was significant – and positively
increased recall, leading to an overall improvement
in F-measure for all test sets. Overall, F-measure
improved by 1.9%, 3.7%, and 3.9% (relative) com-
pared to the baseline for the ANERCORP, NEWS,
and TWEETS test sets respectively. The similarity
of results between using transliteration mining and
word-level cross-lingual capitalization suggests that
perhaps they can serve as surrogates.
</bodyText>
<subsectionHeader confidence="0.999772">
4.3 Using DBpedia
</subsectionHeader>
<bodyText confidence="0.99996403030303">
DBpedia2 is a large collaboratively-built knowledge
base in which structured information is extracted
from Wikipedia (Bizer et al., 2009). DBpedia 3.8,
the release we used in this paper, contains 6,157,591
Wikipedia titles belonging to 296 types. Types vary
in granularity with each Wikipedia title having one
or more type. For example, NASA is assigned the
following types: Agent, Organization, and Govern-
mentAgency. In all, DBpedia includes the names of
764k persons, 573k locations, and 192k organiza-
tions. Of the Arabic Wikipedia titles, 254,145 have
Wikipedia cross-lingual links to English Wikipedia,
and of those English Wikipedia titles, 185,531 have
entries in DBpedia. Since Wikipedia titles may have
multiple DBpedia types, we opted to keep the most
popular type (by count of how many Wikipedia ti-
tles are assigned a particular type) for each title, and
we disregarded the rest. We also chose not to use
the “Agent” and “Work” types because they were
highly ambiguous. We found word sequences in
the manner described in the pseudocode for cross-
lingual capitalization. For translation, we gener-
ated two features using two translation resources,
namely the aforementioned phrase table and Arabic-
English Wikipedia cross-lingual links. When using
the phrase table, we used the most likely transla-
tion into English that matches an entry in DBpedia
provided that the product of p(source|target) and
p(target|source) of translation was above 10−5.
We chose the threshold qualitatively using offline
experiments. When using Arabic-English Wikipedia
cross-lingual links, if an entry was found in the
Arabic Wikipedia, we performed a look up in DB-
</bodyText>
<footnote confidence="0.993297">
2http://dbpedia.org
</footnote>
<page confidence="0.964457">
1563
</page>
<bodyText confidence="0.9904140625">
pedia using the English Wikipedia title that corre-
sponds to the Arabic Wikipedia title. We used Ara-
bic Wikipedia page-redirects to improve coverage.
For both features (using the two translation meth-
ods), for an Arabic word sequence corresponding to
the DBpedia entry, the first word in the sequence
was assigned the feature “B-” plus the DBpedia
type and subsequent words were assigned the fea-
ture “I-” plus the DBpedia type. For example, for
�aUI u p (meaning “Hezbollah”), the words u p
�
and aUI were assigned “B-Organization” and “I-
Organization” respectively. For all other words, the
feature was assigned “null”. Using the phrase ta-
ble for translation likely yielded improved coverage
over using Wikipedia cross-lingual links. However,
Wikipedia cross-lingual links likely led to higher
quality translations, because they were manually cu-
rated. Table 6 reports on the results of using the
baseline system with the two DBpedia features. Us-
ing DBpedia consistently improved precision and re-
call for named entity types on all test sets, except
for a small drop in precision for locations on the
ANERCORP dataset and for locations and persons
on the TWEETS test set. For the different test sets,
improvements in recall ranged between 4.4 and 7.5
points (absolute) or 6.5% and 19.1% (relative). Pre-
cision improved by 0.9 and 5.5 points (absolute) or
1.0% and 7.1% (relative) for the ANERCORP and
NEWS test sets respectively. Overall improvement
in F-measure ranged between 3.2 and 7.6 points (ab-
solute) or 4.1% and 13.9% (relative).
</bodyText>
<subsectionHeader confidence="0.999234">
4.4 Putting it All Together
</subsectionHeader>
<bodyText confidence="0.9980685">
Table 7 reports on the results of using all aforemen-
tioned cross-lingual features together. Figures 1, 2,
and 3 compare the results of the different setups. As
the results show, the impact of cross-lingual features
on recall were much more pronounced on the NEWS
and TWEETS test sets – compared to the ANER-
CORP dataset. Further, the recall values for the AN-
ERCORP dataset in the baseline experiments were
much higher than those for the two other test sets.
This confirms our suspicion that the reported values
in the literature on the standard datasets are unrealis-
tically high due to the similarity between the training
and test sets. Hence, these high effectiveness results
may not generalize to other test sets. Of all the cross-
</bodyText>
<table confidence="0.999070222222222">
(a) ANERCORP Dataset
Precision Recall F,9=1
LOC 92.9/-0.7/-0.7 83.5/0.2/0.3 88.0/-0.2/-0.2
ORG 82.9/-0.9/-1.0 61.8/0.6/1.0 70.9/0.1/0.1
PERS 84.5/0.3/0.3 71.9/7.5/11.7 77.7/4.7/6.5
Overall 88.3/-0.5/-0.6 75.5/2.9/4.1 81.4/1.5/1.9
(b) NEWS Test Set
Precision Recall F,9=1
LOC 84.9/0.7/0.9 53.6/0.5/0.9 65.7/0.6/0.9
ORG 67.2/-6.1/-8.3 22.9/-0.3/-1.1 34.2/-1.0/-2.9
PERS 72.8/-1.9/-2.6 55.0/7.8/16.7 62.7/4.8/8.4
Overall 75.9/-2.1/-2.6 45.0/3.1/7.4 56.6/2.0/3.7
(c) TWEETS Test Set
Precision Recall F,9=1
LOC 79.1/-0.8/-1.0 27.1/0.0/0.0 40.3/-0.1/-0.3
ORG 41.8/-2.7/-6.0 9.1/0.0/0.0 14.9/-0.2/-1.1
PERS 40.0/-5.7/-12.5 35.5/7.7/27.8 37.6/3.1/8.8
Overall 51.7/-6.3/-10.9 25.8/2.6/11.3 34.4/1.3/3.9
</table>
<tableCaption confidence="0.9866835">
Table 5: Results with transliteration mining with /abso-
lute/relative differences compared to baseline
</tableCaption>
<table confidence="0.999807888888889">
(a) ANERCORP Dataset
Precision Recall F,9=1
LOC 92.7/-0.9/-0.9 87.1/3.9/4.6 89.9/1.7/1.9
ORG 84.6/0.8/0.9 66.6/5.3/8.7 74.5/3.7/5.3
PERS 87.8/3.6/4.2 69.9/5.5/8.6 77.8/4.8/6.6
Overall 89.8/0.9/1.0 77.2/4.7/6.5 83.0/3.2/4.0
(b) NEWS Test Set
Precision Recall F,9=1
LOC 87.8/3.6/4.3 61.8/8.6/16.2 72.5/7.4/11.3
ORG 76.1/2.9/3.9 30.2/7.0/30.1 43.2/8.0/22.7
PERS 83.2/8.5/11.3 54.2/7.1/15.0 65.7/7.8/13.6
Overall 83.5/5.5/7.1 49.5/7.5/18.0 62.2/7.6/13.9
(c) TWEETS Test Set
Precision Recall F,9=1
LOC 77.4/-2.5/-3.1 30.5/3.5/12.9 43.8/3.4/8.4
ORG 57.0/12.5/28.2 15.9/6.8/75.1 24.8/9.8/64.9
PERS 40.8/-4.9/-10.6 31.7/4.0/14.3 35.7/1.2/3.4
Overall 55.3/-2.6/-4.5 27.5/4.4/19.1 36.8/3.7/11.2
</table>
<tableCaption confidence="0.993758">
Table 6: Results using DBpedia with /absolute/relative
differences compared to baseline
</tableCaption>
<bodyText confidence="0.999622416666667">
lingual features that we experimented with, the use
of DBpedia led to improvements in both precision
and recall (except for precision on the TWEETS test
set). Other cross-lingual features yielded overall im-
provements in F-measure, mostly due to gains in re-
call, typically at the expense of precision. Overall,
F-measure improved by 5.5%, 17.1%, and 20.5%
(relative) compared to the baseline for the ANER-
CORP, NEWS, and TWEETS test sets respectively.
For the ANERCORP test set, our results improved
over the baseline-lit results (Abdul-Hamid and Dar-
wish, 2010) by 4.1% (relative).
</bodyText>
<page confidence="0.996513">
1564
</page>
<figureCaption confidence="0.999994">
Figure 1: ANERCORP Dataset Results
Figure 2: NEWS Test Set Results
</figureCaption>
<bodyText confidence="0.999734896551724">
When using all the features together, one notable
result is that precision dropped significantly for the
TWEETS test sets. We examined the output for the
TWEETS test set and here are some of the factors
that affected precision:
- the presence of words that would typically be
named entities in news but would generally be reg-
ular words in tweets. For example, the Arabic word
“Mubarak” is most likely the name of the former
Egyptian president in the context of news, but it
would most likely mean “blessed”, which is com-
mon in expressions of congratulations, in tweets.
- the use of dialectic words that may have transliter-
ations or a named entity as the most likely transla-
tion into English. For example, the word ú
æ: is typ-
ically the dialectic version of the Arabic word Zú
æ;,
meaning something. However, since the MT sys-
tem that we used was trained on modern standard
Arabic, the dialectic word would not appear in train-
ing and would typically be translated/transliterated
to the name “Che” (as in Che Guevara).
- Since tweets are restricted in length, authors fre-
quently use shortened versions of named entities.
For example, tweets would mostly have “Morsi”
instead of “Mohamed Morsi” and without trigger
words such as “Dr.” or “president”. The full ver-
sion of a name and trigger words are more com-
</bodyText>
<figureCaption confidence="0.993098">
Figure 3: TWEETS Test Set Results
</figureCaption>
<table confidence="0.998159222222223">
(a) ANERCORP Dataset
Precision Recall Fβ=1
LOC 92.3/-1.3/-1.4 87.8/4.6/5.5 90.0/1.9/2.1
ORG 81.4/-2.4/-2.9 66.0/4.7/7.7 72.9/2.1/3.0
PERS 87.0/2.8/3.3 77.7/13.3/20.7 82.1/9.1/12.5
Overall 88.7/-0.2/-0.2 80.3/7.8/10.7 84.3/4.4/5.5
(b) NEWS Test Set
Precision Recall Fβ=1
LOC 85.1/1.0/1.2 64.1/11.0/20.6 73.1/8.0/12.3
ORG 73.8/0.5/0.7 29.4/6.2/26.9 42.1/6.8/19.4
PERS 76.8/2.0/2.7 63.4/16.3/34.5 69.5/11.7/20.2
Overall 79.2/1.2/1.6 53.6/11.6/27.7 63.9/9.4/17.1
(c) TWEETS Test Set
Precision Recall Fβ=1
LOC 81.4/1.5/1.8 33.5/6.5/23.9 47.5/7.1/17.4
ORG 52.1/7.6/17.2 16.2/7.1/78.6 24.7/9.6/64.1
PERS 40.5/-5.2/-11.4 39.2/11.5/41.3 39.8/5.3/15.4
Overall 54.4/-3.6/-6.2 31.4/8.3/35.9 39.9/6.8/20.5
</table>
<tableCaption confidence="0.9909585">
Table 7: Results using all the cross-lingual features with
/absolute/relative differences compared to baseline
</tableCaption>
<bodyText confidence="0.999954666666667">
mon in news. This same problem was present in the
NEWS test set, because it was constructed from an
RSS feed, and headlines, which are typically com-
pact, had a higher representation in the test collec-
tion. We observed the same phenomenon for orga-
nization names. For example, “the Real” refers to
“Real Madrid”. Nicknames are also prevalent. For
example, “the Land of the two Sanctuaries” refers to
“Saudi Arabia”.
We believe that this problem can be overcome by
introducing new training data that include tweets (or
other social text) and performing domain adaptation.
New training data would help: identify words and
expressions that are common in conversations, ac-
count for common dialectic words, and learn a bet-
ter word transition model. Further, gazetteers that
cover shortened versions of names could be helpful
as well.
</bodyText>
<page confidence="0.98963">
1565
</page>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982588235294">
In this paper, we presented different cross-lingual
features that can make use of linguistic properties
and knowledge bases of other languages for NER.
For translation, we used an MT phrase table and
Wikipedia cross-lingual links. We used English
as the “helper” language and we exploited the En-
glish capitalization feature and an English knowl-
edge base, DBpedia. If the helper language did
not have capitalization, then transliteration mining
could provide some of the benefit of capitalization.
Transliteration mining requires limited amounts of
training examples. We believe that the proposed
cross-lingual features can be used to help NER for
other languages, particularly languages that lack
good features that generalize well. For Arabic NER,
the new features yielded an improvement of 5.5%
over a strong baseline system on a standard dataset,
with 10.7% gain in recall and negligible change in
precision. We tested on a new news test set, NEWS,
which has recent news articles (the same genre as
the standard dataset), and indeed NER effective-
ness was much lower. For the new NEWS test set,
cross-lingual features led to a small increase in pre-
cision (1.6%) and a very large improvement in re-
call (27.7%). This led to a 17.1% improvement
in overall F-measure. We also tested NER on the
TWEETS test set, where we observed substantial
improvements in recall (35.9%). However, precision
dropped by 6.2% for the reasons we mentioned ear-
lier. For future work, it would be interesting to apply
cross-lingual features to other language pairs and to
make use of joint cross-lingual models. Further, we
also plan to investigate Arabic NER on social media,
particularly microblogs.
</bodyText>
<sectionHeader confidence="0.998646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999214968253968">
A. Abdul-Hamid and K. Darwish. 2010. Simplified Fea-
ture Set for Arabic Named Entity Recognition. Pro-
ceedings of the 2010 Named Entities Workshop, ACL
2010, pages 110115.
Mohammed Attia, Antonio Toral, Lamia Tounsi, Mon-
ica Monachini, and Josef van Genabith. 2010. An au-
tomatically built named entity lexicon for Arabic. In:
LREC 2010 - 7th conference on International Lan-
guage Resources and Evaluation, 17-23 May 2010,
Valletta, Malta.
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic Named
Entity Recognition using Optimized Feature Sets. Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 284293,
Honolulu, October 2008.
Y. Benajiba and P. Rosso. 2008. Arabic Named En-
tity Recognition using Conditional Random Fields. In
Proc. of Workshop on HLT &amp; NLP within the Arabic
World, LREC08.
Y. Benajiba, P. Rosso and J. M. Benedi. 2007. ANER-
sys: An Arabic Named Entity Recognition system
based on Maximum Entropy. In Proc. of CICLing-
2007, Springer-Verlag, LNCS(4394), pp.143-153
Y. Benajiba and P. Rosso. 2007. ANERsys 2.0: Con-
quering the NER task for the Arabic language by
combining the Maximum Entropy with POS-tag infor-
mation. In Proc. of Workshop on Natural Language-
Independent Engineering, IICAI-2007.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sren
Auer, Christian Becker, Richard Cyganiak, Sebastian
Hellmann. 2009. DBpedia A Crystallization Point for
the Web of Data. Journal of Web Semantics: Science,
Services and Agents on the World Wide Web, Issue 7,
Pages 154165, 2009.
D. Burkett, S. Petrov, J. Blitzer, D. Klein. 2010. Learning
Better Monolingual Models with Unannotated Bilin-
gual Text. Proceedings of the Fourteenth Conference
on Computational Natural Language Learning, pages
46–54.
A. El Kahki, K. Darwish, A. Saad El Din, M. Abd El-
Wahab and A. Hefny. 2011. Improved Transliteration
Mining Using Graph Reinforcement. EMNLP-2011.
B. Farber, D. Freitag, N. Habash, and O. Rambow. 2008.
Improving NER in Arabic Using a Morphological Tag-
ger. In Proc. of LREC08.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. Depen-
dency grammar induction via bitext projection con-
straints. In ACL-2009.
Spence Green and John DeNero. 2012. A Class-Based
Agreement Model for Generating Accurately Inflected
Translations. In ACL-2012.
Ulf Hermjakob, Kevin Knight, and Hal Daum III. 2008.
Name translation in statistical machine translation:
Learning when to transliterate. ACL-08: HLT, Pages
389-397.
F. Huang. 2005. Multilingual Named Entity Extraction
and Translation from Text and Speech. Ph.D. Thesis.
Pittsburgh: Carnegie Mellon University.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst, Moses: Open Source Toolkit
</reference>
<page confidence="0.815707">
1566
</page>
<reference confidence="0.999846709090909">
for Statistical Machine Translation, Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, Prague, Czech Republic, June
2007.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data, In Proc. of ICML,
pp.282-289, 2001.
Leah S. Larkey, Lisa Ballesteros, and Margaret E. Con-
nell. 2002. Improving stemming for Arabic informa-
tion retrieval: light stemming and co-occurrence anal-
ysis. SIGIR-2002.
J. Mayfield, P. McNamee, and C. Piatko. 2003.Named
Entity Recognition using Hundreds of Thousands of
Features. HLT-NAACL 2003-Volume 4, 2003.
A. McCallum and W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields,
Features Induction and Web-Enhanced Lexicons. In
Proc. Conference on Computational Natural Language
Learning.
P. McNamee and J. Mayfield. 2002. Entity extraction
without language-specific. Proceedings of CoNLL,
.2002
Behrang Mohit, Nathan Schneider, Rishav Bhowmick,
Kemal Oflazer, Noah A. Smith. 2012. Recall-oriented
learning of named entities in Arabic Wikipedia. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL 2012), pp. 162-173. 2012.
D. Nadeau and S. Sekine. 2009. A Survey of Named En-
tity Recognition and Classification. Named Entities:
Recognition, Classification and Use, ed. S. Sekine and
E. Ranchhod, John Benjamins Publishing Company.
Alexander E. Richman and Patrick Schone. 2008. Mining
wiki resources for multilingual named entity recogni-
tion. Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies. 2008.
K. Shaalan and H. Raza. 2007. Person Name Entity
Recognition for Arabic. Proceedings of the 5th Work-
shop on Important Unresolved Matters, pages 1724,
Prague, Czech Republic, June 2007.
L. Shi, R. Mihalcea, M. Tian. 2010. Cross Language
Text Classification by Model Translation and Semi-
supervised Learning. Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2010.
Raghavendra Udupa, Anton Bakalov, and Abhijit Bhole.
2009. They Are Out There, If You Know Where to
Look: Mining Transliterations of OOV Query Terms
for Cross-Language Information Retrieval. Advances
in Information Retrieval. Pages: 437-448.
D. Yarowsky and G. Ngai. 2001. Inducing Multilingual
POS Taggers and NP Bracketers via Robust Projection
across Aligned Corpora. In NAACL-2001.
</reference>
<page confidence="0.993688">
1567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659108">
<title confidence="0.99326">Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</title>
<author confidence="0.988091">Kareem</author>
<affiliation confidence="0.999707">Qatar Computing Research</affiliation>
<address confidence="0.695359">Doha,</address>
<email confidence="0.995672">kdarwish@qf.org.qa</email>
<abstract confidence="0.9986345">Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and microblogs test sets respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abdul-Hamid</author>
<author>K Darwish</author>
</authors>
<title>Simplified Feature Set for Arabic Named Entity Recognition.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Named Entities Workshop, ACL</booktitle>
<pages>110115</pages>
<contexts>
<context position="6215" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="980" endWordPosition="983">uring training. We show the effectiveness of these new features on a standard dataset as well as two new test sets. The contributions of this paper are as follows: - Using cross-lingual links to exploit orthographic features in other languages. - Employing transliteration mining to improve NER. - Using cross-lingual links to exploit a large knowledge base, namely English DBpedia, to benefit NER. - Introducing two new NER test sets for Arabic that include recent news as well as microblogs. We plan to release these test sets. - Improving over the best reported results in the literature by 4.1% (Abdul-Hamid and Darwish, 2010) by strictly adding cross-lingual features. We also show improvements of 17.1% and 20.5% on the new test sets. The remainder of the paper is organized as follows: Section 2 provides related work; Section 3 describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 Related Work 2.1 Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are avail</context>
<context position="11392" citStr="Abdul-Hamid and Darwish (2010)" startWordPosition="1829" endWordPosition="1832">broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMMbased NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic tagger to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. Abdul-Hamid and Darwish (2010) used a simplified feature set that relied primarily on character level features, namely leading and trailing letters in a word. They also experimented with a variety of phrase level features with little success. They reported an F-measure of 76% and 81% for the ACE2005 and the ANERCorp datasets datasets respectively. We used their simplified features in our baseline system. The different experiments reported in the literature may not have been done on the same training/test splits. Thus, the results may not be completely comparable. Mohit et al. (2012) performed NER on a different genre from </context>
<context position="12896" citStr="Abdul-Hamid and Darwish (2010)" startWordPosition="2065" endWordPosition="2068">language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (after removing coordinating conjunctions, prepositions, and determiners using a rule-based stemmer akin to (Larkey et al., 2002)). As mentioned earlier, the leading and trailing letters in a word may indicate or counter-indicate the presence of named entities. Stemming is important due to the morphological complexity of Arabic. We used the</context>
<context position="15965" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="2589" endWordPosition="2592">27% of the named entities in the NEWS test set were observed in the training set (compared to 69% for ANERCORP). As Table 3 shows for the ANERCORP dataset, using only the tokens as features, where the labeler mainly memorizes previously seen named entities, yields higher results than the baseline results for the NEWS dataset (Table 2 (b)). The results on the TWEETS test are very poor, with 24% of the named entities in the test set appearing in the training set. ANERCORP Dataset Precision Recall Fβ=1 LOC 93 83 88 ORG 84 64 73 PERS 90 75 82 Overall 89 74 81 Table 1: “Baseline-lit” Results from (Abdul-Hamid and Darwish, 2010) (a) ANERCORP Dataset Precision Recall Fβ=1 LOC 93.6 83.3 88.1 ORG 83.8 61.2 70.8 PERS 84.3 64.4 73.0 Overall 88.9 72.5 79.9 (b) NEWS Test Set Precision Recall Fβ=1 LOC 84.1 53.2 65.1 ORG 73.2 23.2 35.2 PERS 74.8 47.1 57.8 Overall 78.0 41.9 54.6 (c) TWEETS Test Set Precision Recall Fβ=1 LOC 79.9 27.1 40.4 ORG 44.4 9.1 15.1 PERS 45.7 27.8 34.5 Overall 58.0 23.1 33.1 Table 2: Baseline Results for the three test sets ANERCORP Dataset Precision Recall Fβ=1 LOC 95.3 62.7 75.6 ORG 86.3 44.7 58.9 PERS 85.4 36.4 51.0 Overall 91.0 50.0 64.5 Table 3: Results of using only tokens as features on ANERCORP </context>
<context position="29422" citStr="Abdul-Hamid and Darwish, 2010" startWordPosition="4628" endWordPosition="4632">using DBpedia with /absolute/relative differences compared to baseline lingual features that we experimented with, the use of DBpedia led to improvements in both precision and recall (except for precision on the TWEETS test set). Other cross-lingual features yielded overall improvements in F-measure, mostly due to gains in recall, typically at the expense of precision. Overall, F-measure improved by 5.5%, 17.1%, and 20.5% (relative) compared to the baseline for the ANERCORP, NEWS, and TWEETS test sets respectively. For the ANERCORP test set, our results improved over the baseline-lit results (Abdul-Hamid and Darwish, 2010) by 4.1% (relative). 1564 Figure 1: ANERCORP Dataset Results Figure 2: NEWS Test Set Results When using all the features together, one notable result is that precision dropped significantly for the TWEETS test sets. We examined the output for the TWEETS test set and here are some of the factors that affected precision: - the presence of words that would typically be named entities in news but would generally be regular words in tweets. For example, the Arabic word “Mubarak” is most likely the name of the former Egyptian president in the context of news, but it would most likely mean “blessed”,</context>
</contexts>
<marker>Abdul-Hamid, Darwish, 2010</marker>
<rawString>A. Abdul-Hamid and K. Darwish. 2010. Simplified Feature Set for Arabic Named Entity Recognition. Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Attia</author>
<author>Antonio Toral</author>
<author>Lamia Tounsi</author>
<author>Monica Monachini</author>
<author>Josef van Genabith</author>
</authors>
<title>An automatically built named entity lexicon for Arabic. In:</title>
<date>2010</date>
<booktitle>LREC 2010 - 7th conference on International Language Resources and Evaluation,</booktitle>
<pages>17--23</pages>
<location>Valletta,</location>
<marker>Attia, Toral, Tounsi, Monachini, van Genabith, 2010</marker>
<rawString>Mohammed Attia, Antonio Toral, Lamia Tounsi, Monica Monachini, and Josef van Genabith. 2010. An automatically built named entity lexicon for Arabic. In: LREC 2010 - 7th conference on International Language Resources and Evaluation, 17-23 May 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>M Diab</author>
<author>P Rosso</author>
</authors>
<title>Arabic Named Entity Recognition using Optimized Feature Sets.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284293</pages>
<location>Honolulu,</location>
<contexts>
<context position="4376" citStr="Benajiba et al., 2008" startWordPosition="682" endWordPosition="685">, August 4-9 2013. c�2013 Association for Computational Linguistics of the Arabic gazetteers that were used for NER were small (Benajiba and Rosso, 2008), there has been efforts to build larger Arabic gazetteers (Attia et al., 2010). Since training and test parts of standard datasets for Arabic NER are drawn from the same genre in relatively close temporal proximity, a named entity recognizer that simply memorizes named entities in the training set generally performs well on such test sets. Thus, the results that are reported in the literature are generally high (AbdulHamid and Darwish, 2010; Benajiba et al., 2008). We illustrate the limited capacity of existing recognizers to generalize to previously unseen named entities using two new test sets that include microblogs as well as news texts that cover local and international politics, economics, health, sports, entertainment, and science. As we will show later, recall is well below 50% for all named entity types on the new test sets. To address this problem, we introduce the use of cross-lingual links between a disadvantaged language, Arabic, and a language with good discriminative features and large resources, English, to improve Arabic NER. We exploi</context>
<context position="7574" citStr="Benajiba et al. (2008)" startWordPosition="1198" endWordPosition="1201">agging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to indicate if a word is capitalized in English or not. They reported that it was the second most discriminating feature that they used. However, there seems to be room for improvement because: (1) MADA’s dictionary is relatively small and would have low coverage; and (2) the use of such a binary feature is problematic, because Arabic names are often common Arabic words and hence a word may be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using lar</context>
<context position="9275" citStr="Benajiba et al., 2008" startWordPosition="1500" endWordPosition="1503">). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 2007). Benajiba et al. (2007) used a maximum entropy classifier trained on a feature set that includes the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively on the ANERCORP dataset that they created and publicly released. Benajiba and Rosso (2007) improved their system by incorporating POS tags to improve NE boundary detection. They reported 87%, 46%, and 52</context>
<context position="12861" citStr="Benajiba et al. (2008)" startWordPosition="2060" endWordPosition="2063">. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (after removing coordinating conjunctions, prepositions, and determiners using a rule-based stemmer akin to (Larkey et al., 2002)). As mentioned earlier, the leading and trailing letters in a word may indicate or counter-indicate the presence of named entities. Stemming is important due to the morphologica</context>
</contexts>
<marker>Benajiba, Diab, Rosso, 2008</marker>
<rawString>Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic Named Entity Recognition using Optimized Feature Sets. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 284293, Honolulu, October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
</authors>
<title>Arabic Named Entity Recognition using Conditional Random Fields.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on HLT &amp; NLP within the Arabic World, LREC08.</booktitle>
<contexts>
<context position="1414" citStr="Benajiba and Rosso, 2008" startWordPosition="220" endWordPosition="223"> sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and microblogs test sets respectively. 1 Introduction Named Entity Recognition (NER) is essential for a variety of Natural Language Processing (NLP) applications such as information extraction. There has been a fair amount of work on NER for a variety of languages including Arabic. To train an NER system, some of the following feature types are typically used (Benajiba and Rosso, 2008; Nadeau and Sekine, 2009): - Orthographic features: These features include capitalization, punctuation, existence of digits, etc. One of the most effective orthographic features is capitalization in English, which helps NER to generalize to new text of different genres. However, capitalization is not very useful in some languages such as German, and nonexistent in other languages such as Arabic. Further, even in English social media, capitalization may be inconsistent. - Contextual features: Certain words are indicative of the existence of named entities. For example, the word “said” is often</context>
<context position="3498" citStr="Benajiba and Rosso, 2008" startWordPosition="545" endWordPosition="549"> to nouns and verbs. However, pronouns are rarely ever attached to named entities. - Gazetteers: This feature checks the presence of a word or a sequence of words in large lists of named entities. If gazetteers are small, then they would have low coverage, and if they are very large then their entries may be ambiguous. For example, “syntax” may refer to sentence construction or the music band “Syntax”. Typically, a subset of these features are available for different languages. For example, morphological, contextual, and character-level features have been shown to be effective for Arabic NER (Benajiba and Rosso, 2008). However, Arabic lacks indicative orthographic features that generalize to previously unseen named entities. Also, although some 1558 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1558–1567, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics of the Arabic gazetteers that were used for NER were small (Benajiba and Rosso, 2008), there has been efforts to build larger Arabic gazetteers (Attia et al., 2010). Since training and test parts of standard datasets for Arabic NER are drawn from the same genre in relatively </context>
<context position="9252" citStr="Benajiba and Rosso, 2008" startWordPosition="1496" endWordPosition="1499">ot (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 2007). Benajiba et al. (2007) used a maximum entropy classifier trained on a feature set that includes the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively on the ANERCORP dataset that they created and publicly released. Benajiba and Rosso (2007) improved their system by incorporating POS tags to improve NE boundary detection. They re</context>
<context position="12586" citStr="Benajiba and Rosso, 2008" startWordPosition="2018" endWordPosition="2022"> NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (after removing coordinating con</context>
</contexts>
<marker>Benajiba, Rosso, 2008</marker>
<rawString>Y. Benajiba and P. Rosso. 2008. Arabic Named Entity Recognition using Conditional Random Fields. In Proc. of Workshop on HLT &amp; NLP within the Arabic World, LREC08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
<author>J M Benedi</author>
</authors>
<title>ANERsys: An Arabic Named Entity Recognition system based on Maximum Entropy.</title>
<date>2007</date>
<booktitle>In Proc. of CICLing2007,</booktitle>
<volume>4394</volume>
<pages>143--153</pages>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="9325" citStr="Benajiba et al., 2007" startWordPosition="1508" endWordPosition="1511"> using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 2007). Benajiba et al. (2007) used a maximum entropy classifier trained on a feature set that includes the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively on the ANERCORP dataset that they created and publicly released. Benajiba and Rosso (2007) improved their system by incorporating POS tags to improve NE boundary detection. They reported 87%, 46%, and 52% F-measure for locations, organizations, and pers</context>
</contexts>
<marker>Benajiba, Rosso, Benedi, 2007</marker>
<rawString>Y. Benajiba, P. Rosso and J. M. Benedi. 2007. ANERsys: An Arabic Named Entity Recognition system based on Maximum Entropy. In Proc. of CICLing2007, Springer-Verlag, LNCS(4394), pp.143-153</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
</authors>
<title>ANERsys 2.0: Conquering the NER task for the Arabic language by combining the Maximum Entropy with POS-tag information.</title>
<date>2007</date>
<booktitle>In Proc. of Workshop on Natural LanguageIndependent Engineering, IICAI-2007.</booktitle>
<contexts>
<context position="9301" citStr="Benajiba and Rosso, 2007" startWordPosition="1504" endWordPosition="1507">aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 2007). Benajiba et al. (2007) used a maximum entropy classifier trained on a feature set that includes the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively on the ANERCORP dataset that they created and publicly released. Benajiba and Rosso (2007) improved their system by incorporating POS tags to improve NE boundary detection. They reported 87%, 46%, and 52% F-measure for locations,</context>
<context position="13650" citStr="Benajiba and Rosso, 2007" startWordPosition="2189" endWordPosition="2192">enajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (after removing coordinating conjunctions, prepositions, and determiners using a rule-based stemmer akin to (Larkey et al., 2002)). As mentioned earlier, the leading and trailing letters in a word may indicate or counter-indicate the presence of named entities. Stemming is important due to the morphological complexity of Arabic. We used the previous and the next words in their raw and stemmed forms as features. For training and testing, we used the ANERCORP dataset (Benajiba and Rosso, 2007). The dataset has approximately 150k tokens and we used the 80/20 training/test splits of Abdul-Hamid and Darwish (2010), who graciously provided us with their splits of the collection and they achieved the best reported results on the dataset. We will refer to their results, which are provided in Table 1, as “baseline-lit”. Table 2 (a) shows our results on the ANERCORP dataset. Our results were slightly lower than their results (Abdul-Hamid and Darwish, 2010). It is noteworthy that 69% of the named entities in the test part were seen during training. We also created two new test sets. The fir</context>
</contexts>
<marker>Benajiba, Rosso, 2007</marker>
<rawString>Y. Benajiba and P. Rosso. 2007. ANERsys 2.0: Conquering the NER task for the Arabic language by combining the Maximum Entropy with POS-tag information. In Proc. of Workshop on Natural LanguageIndependent Engineering, IICAI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>Sren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>DBpedia A Crystallization Point for the Web of Data.</title>
<date>2009</date>
<journal>Journal of Web Semantics: Science, Services and Agents on the World Wide Web, Issue</journal>
<volume>7</volume>
<pages>154165</pages>
<contexts>
<context position="23466" citStr="Bizer et al., 2009" startWordPosition="3767" endWordPosition="3770">ETS test set where the drop in precision was significant – and positively increased recall, leading to an overall improvement in F-measure for all test sets. Overall, F-measure improved by 1.9%, 3.7%, and 3.9% (relative) compared to the baseline for the ANERCORP, NEWS, and TWEETS test sets respectively. The similarity of results between using transliteration mining and word-level cross-lingual capitalization suggests that perhaps they can serve as surrogates. 4.3 Using DBpedia DBpedia2 is a large collaboratively-built knowledge base in which structured information is extracted from Wikipedia (Bizer et al., 2009). DBpedia 3.8, the release we used in this paper, contains 6,157,591 Wikipedia titles belonging to 296 types. Types vary in granularity with each Wikipedia title having one or more type. For example, NASA is assigned the following types: Agent, Organization, and GovernmentAgency. In all, DBpedia includes the names of 764k persons, 573k locations, and 192k organizations. Of the Arabic Wikipedia titles, 254,145 have Wikipedia cross-lingual links to English Wikipedia, and of those English Wikipedia titles, 185,531 have entries in DBpedia. Since Wikipedia titles may have multiple DBpedia types, we</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, Sren Auer, Christian Becker, Richard Cyganiak, Sebastian Hellmann. 2009. DBpedia A Crystallization Point for the Web of Data. Journal of Web Semantics: Science, Services and Agents on the World Wide Web, Issue 7, Pages 154165, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Burkett</author>
<author>S Petrov</author>
<author>J Blitzer</author>
<author>D Klein</author>
</authors>
<title>Learning Better Monolingual Models with Unannotated Bilingual Text.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="7126" citStr="Burkett et al. (2010)" startWordPosition="1127" endWordPosition="1130">eports on their effectiveness; and Section 5 concludes the paper. 2 Related Work 2.1 Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to indicate if a word is capitalized in English or not. They reported that it was the </context>
</contexts>
<marker>Burkett, Petrov, Blitzer, Klein, 2010</marker>
<rawString>D. Burkett, S. Petrov, J. Blitzer, D. Klein. 2010. Learning Better Monolingual Models with Unannotated Bilingual Text. Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A El Kahki</author>
<author>K Darwish</author>
<author>A Saad El Din</author>
<author>M Abd ElWahab</author>
<author>A Hefny</author>
</authors>
<title>Improved Transliteration Mining Using Graph Reinforcement.</title>
<date>2011</date>
<tech>EMNLP-2011.</tech>
<marker>El Kahki, Darwish, El Din, ElWahab, Hefny, 2011</marker>
<rawString>A. El Kahki, K. Darwish, A. Saad El Din, M. Abd ElWahab and A. Hefny. 2011. Improved Transliteration Mining Using Graph Reinforcement. EMNLP-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Farber</author>
<author>D Freitag</author>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Improving NER in Arabic Using a Morphological Tagger.</title>
<date>2008</date>
<booktitle>In Proc. of LREC08.</booktitle>
<contexts>
<context position="10972" citStr="Farber et al. (2008)" startWordPosition="1763" endWordPosition="1766">features, they reported 90%, 66%, and 73% F-measure for location, organization, and persons respectively. In Benajiba et al. (2008), they examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and a Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMMbased NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic tagger to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. Abdul-Hamid and Darwish (2010) used a simplified feature set that relied primarily on character level features, namely leading and trailing letters in a word. They also experimented with a variety of phrase lev</context>
</contexts>
<marker>Farber, Freitag, Habash, Rambow, 2008</marker>
<rawString>B. Farber, D. Freitag, N. Habash, and O. Rambow. 2008. Improving NER in Arabic Using a Morphological Tagger. In Proc. of LREC08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In ACL-2009.</booktitle>
<contexts>
<context position="7001" citStr="Ganchev et al., 2009" startWordPosition="1104" endWordPosition="1107">tion 2 provides related work; Section 3 describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 Related Work 2.1 Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dic</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>K. Ganchev, J. Gillenwater, and B. Taskar. 2009. Dependency grammar induction via bitext projection constraints. In ACL-2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>John DeNero</author>
</authors>
<title>A Class-Based Agreement Model for Generating Accurately Inflected Translations.</title>
<date>2012</date>
<booktitle>In ACL-2012.</booktitle>
<contexts>
<context position="17609" citStr="Green and DeNero, 2012" startWordPosition="2863" endWordPosition="2866">ative names to articles. Of these articles, 254,145 have cross-lingual links to English Wikipedia. We used DBpedia 3.8 which includes 6,157,591 entries of Wikipedia titles and their “types”, such as “person”, “plant”, or “device”, where a title can have multiple types. The phrase table was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn from the UN parallel data along with a variety of parallel news data from LDC and the GALE project. The Arabic side was stemmed (by removing just prefixes) using the Stanford word segmenter (Green and DeNero, 2012). 4.1 Cross-lingual Capitalization As we mentioned earlier, Arabic lacks capitalization and Arabic names are often common Arabic words. For example, the Arabic name “Hasan” means good. To capture cross-lingual capitalization, we used the aforementioned true-cased phrase table at word and phrase levels as follows: Input: True-cased phrase table PT, sentence S containing n words w0..n, max sequence length l, translations T1..k..m of wi..j for i = 0 → n do j = min(i + l − 1, n) if PT contains wi..j &amp; ∃ Tk isCaps then weight(wi..j) = round weight(wi..j) to first significant figure set tag of wi = </context>
</contexts>
<marker>Green, DeNero, 2012</marker>
<rawString>Spence Green and John DeNero. 2012. A Class-Based Agreement Model for Generating Accurately Inflected Translations. In ACL-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Hal Daum</author>
</authors>
<title>Name translation in statistical machine translation: Learning when to transliterate. ACL-08: HLT,</title>
<date>2008</date>
<pages>389--397</pages>
<contexts>
<context position="8655" citStr="Hermjakob et al., 2008" startWordPosition="1384" endWordPosition="1387">y be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using large bilingual resources, and we incorporate confidences to avoid having a binary feature. Richman and Schone (2008) used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages. Transliteration Mining (TM) has been used to enrich MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; B</context>
</contexts>
<marker>Hermjakob, Knight, Daum, 2008</marker>
<rawString>Ulf Hermjakob, Kevin Knight, and Hal Daum III. 2008. Name translation in statistical machine translation: Learning when to transliterate. ACL-08: HLT, Pages 389-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
</authors>
<title>Multilingual Named Entity Extraction and Translation from Text and Speech.</title>
<date>2005</date>
<tech>Ph.D. Thesis.</tech>
<institution>Pittsburgh: Carnegie Mellon University.</institution>
<contexts>
<context position="10857" citStr="Huang (2005)" startWordPosition="1744" endWordPosition="1745">ecall at the expense of precision, leading to overall improvements in F-measure. Using all their suggested features, they reported 90%, 66%, and 73% F-measure for location, organization, and persons respectively. In Benajiba et al. (2008), they examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and a Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMMbased NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic tagger to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. Abdul-Hamid and Darwish (2010) used a simplified feature set that relied primarily on character</context>
</contexts>
<marker>Huang, 2005</marker>
<rawString>F. Huang. 2005. Multilingual Named Entity Extraction and Translation from Text and Speech. Ph.D. Thesis. Pittsburgh: Carnegie Mellon University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<date>2007</date>
<booktitle>Open Source Toolkit for Statistical Machine Translation, Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, Moses:</location>
<contexts>
<context position="16796" citStr="Koehn et al., 2007" startWordPosition="2733" endWordPosition="2736"> 47.1 57.8 Overall 78.0 41.9 54.6 (c) TWEETS Test Set Precision Recall Fβ=1 LOC 79.9 27.1 40.4 ORG 44.4 9.1 15.1 PERS 45.7 27.8 34.5 Overall 58.0 23.1 33.1 Table 2: Baseline Results for the three test sets ANERCORP Dataset Precision Recall Fβ=1 LOC 95.3 62.7 75.6 ORG 86.3 44.7 58.9 PERS 85.4 36.4 51.0 Overall 91.0 50.0 64.5 Table 3: Results of using only tokens as features on ANERCORP 1561 4 Cross-lingual Features We experimented with three different cross-lingual features that used Arabic and English Wikipedia cross-language links and a true-cased phrase table that was generated using Moses (Koehn et al., 2007). True-casing preserves case information during training. We used the Arabic Wikipedia snapshot from September 28, 2012. The snapshot has 348,873 titles including redirects, which are alternative names to articles. Of these articles, 254,145 have cross-lingual links to English Wikipedia. We used DBpedia 3.8 which includes 6,157,591 entries of Wikipedia titles and their “types”, such as “person”, “plant”, or “device”, where a title can have multiple types. The phrase table was trained on a set of 3.69 million parallel sentences containing 123.4 million English tokens. The sentences were drawn f</context>
<context position="21542" citStr="Koehn et al., 2007" startWordPosition="3458" endWordPosition="3461">pful if crosslingual resources do not have capitalization information, or if the “helper” language to be consulted does not have a useful capitalization feature. We performed transliteration mining (aka cognate matching) at word level for each Arabic word against all its possible translations in the phrase table. We used a transliteration miner akin to that of El-Kahki et al. (2011) that was trained using 3,452 parallel Arabic-English transliteration pairs. We aligned the word-pairs at character level using GIZA++ and the phrase extractor and scorer from the Moses machine translation package (Koehn et al., 2007). The alignment produced mappings between English letters sequences and Arabic letter sequences with associated mapping probabilities. Given an Arabic word, we produced all its possible segmentations along with their associated mappings into English letters. We retained valid target sequences that produced translations in the phrase table. Again we used a weight similar to the one for cross-lingual capitalization and we rounded the values of the ratio the significant figure. The weights were computed as: E P(Tk) Tk isTransliteration E � P(Tk) + P(Tk) Tk isTransliteration Tk notTransliteration </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, Moses: Open Source Toolkit for Statistical Machine Translation, Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data,</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, In Proc. of ICML, pp.282-289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leah S Larkey</author>
<author>Lisa Ballesteros</author>
<author>Margaret E Connell</author>
</authors>
<title>Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis.</title>
<date>2002</date>
<pages>2002</pages>
<contexts>
<context position="13283" citStr="Larkey et al., 2002" startWordPosition="2129" endWordPosition="2132">plementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (after removing coordinating conjunctions, prepositions, and determiners using a rule-based stemmer akin to (Larkey et al., 2002)). As mentioned earlier, the leading and trailing letters in a word may indicate or counter-indicate the presence of named entities. Stemming is important due to the morphological complexity of Arabic. We used the previous and the next words in their raw and stemmed forms as features. For training and testing, we used the ANERCORP dataset (Benajiba and Rosso, 2007). The dataset has approximately 150k tokens and we used the 80/20 training/test splits of Abdul-Hamid and Darwish (2010), who graciously provided us with their splits of the collection and they achieved the best reported results on t</context>
</contexts>
<marker>Larkey, Ballesteros, Connell, 2002</marker>
<rawString>Leah S. Larkey, Lisa Ballesteros, and Margaret E. Connell. 2002. Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis. SIGIR-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mayfield</author>
<author>P McNamee</author>
<author>C Piatko</author>
</authors>
<title>2003.Named Entity Recognition using Hundreds of Thousands of Features.</title>
<date>2003</date>
<journal>HLT-NAACL</journal>
<volume>2003</volume>
<contexts>
<context position="12247" citStr="Mayfield et al. (2003)" startWordPosition="1964" endWordPosition="1967">easure of 76% and 81% for the ACE2005 and the ANERCorp datasets datasets respectively. We used their simplified features in our baseline system. The different experiments reported in the literature may not have been done on the same training/test splits. Thus, the results may not be completely comparable. Mohit et al. (2012) performed NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba</context>
</contexts>
<marker>Mayfield, McNamee, Piatko, 2003</marker>
<rawString>J. Mayfield, P. McNamee, and C. Piatko. 2003.Named Entity Recognition using Hundreds of Thousands of Features. HLT-NAACL 2003-Volume 4, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random Fields, Features Induction and Web-Enhanced Lexicons.</title>
<date>2003</date>
<booktitle>In Proc. Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="12534" citStr="McCallum and Li, 2003" startWordPosition="2009" endWordPosition="2013">letely comparable. Mohit et al. (2012) performed NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the st</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>A. McCallum and W. Li. 2003. Early Results for Named Entity Recognition with Conditional Random Fields, Features Induction and Web-Enhanced Lexicons. In Proc. Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>J Mayfield</author>
</authors>
<title>Entity extraction without language-specific.</title>
<date>2002</date>
<booktitle>Proceedings of CoNLL,</booktitle>
<contexts>
<context position="12220" citStr="McNamee and Mayfield (2002)" startWordPosition="1959" endWordPosition="1962">le success. They reported an F-measure of 76% and 81% for the ACE2005 and the ANERCorp datasets datasets respectively. We used their simplified features in our baseline system. The different experiments reported in the literature may not have been done on the same training/test splits. Thus, the results may not be completely comparable. Mohit et al. (2012) performed NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features tha</context>
</contexts>
<marker>McNamee, Mayfield, 2002</marker>
<rawString>P. McNamee and J. Mayfield. 2002. Entity extraction without language-specific. Proceedings of CoNLL, .2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Nathan Schneider</author>
<author>Rishav Bhowmick</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Recall-oriented learning of named entities in Arabic Wikipedia.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012),</booktitle>
<pages>162--173</pages>
<contexts>
<context position="11951" citStr="Mohit et al. (2012)" startWordPosition="1917" endWordPosition="1920"> making comparison difficult. Abdul-Hamid and Darwish (2010) used a simplified feature set that relied primarily on character level features, namely leading and trailing letters in a word. They also experimented with a variety of phrase level features with little success. They reported an F-measure of 76% and 81% for the ACE2005 and the ANERCorp datasets datasets respectively. We used their simplified features in our baseline system. The different experiments reported in the literature may not have been done on the same training/test splits. Thus, the results may not be completely comparable. Mohit et al. (2012) performed NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Seki</context>
</contexts>
<marker>Mohit, Schneider, Bhowmick, Oflazer, Smith, 2012</marker>
<rawString>Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Kemal Oflazer, Noah A. Smith. 2012. Recall-oriented learning of named entities in Arabic Wikipedia. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2012), pp. 162-173. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A Survey of Named Entity Recognition and Classification. Named Entities: Recognition, Classification and Use,</title>
<date>2009</date>
<editor>ed. S. Sekine and E. Ranchhod, John</editor>
<publisher>Benjamins Publishing Company.</publisher>
<contexts>
<context position="1440" citStr="Nadeau and Sekine, 2009" startWordPosition="224" endWordPosition="227"> and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and microblogs test sets respectively. 1 Introduction Named Entity Recognition (NER) is essential for a variety of Natural Language Processing (NLP) applications such as information extraction. There has been a fair amount of work on NER for a variety of languages including Arabic. To train an NER system, some of the following feature types are typically used (Benajiba and Rosso, 2008; Nadeau and Sekine, 2009): - Orthographic features: These features include capitalization, punctuation, existence of digits, etc. One of the most effective orthographic features is capitalization in English, which helps NER to generalize to new text of different genres. However, capitalization is not very useful in some languages such as German, and nonexistent in other languages such as Arabic. Further, even in English social media, capitalization may be inconsistent. - Contextual features: Certain words are indicative of the existence of named entities. For example, the word “said” is often preceded by a named entit</context>
<context position="9092" citStr="Nadeau and Sekine, 2009" startWordPosition="1467" endWordPosition="1470"> MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of languages. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 2007). Benajiba et al. (2007) used a maximum entropy classifier trained on a feature set that includes the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively on the ANERCORP data</context>
<context position="12559" citStr="Nadeau and Sekine, 2009" startWordPosition="2014" endWordPosition="2017">t et al. (2012) performed NER on a different genre from news, namely Arabic Wikipedia articles, and reported recall values as low as 35.6%. They used self training and recall oriented classification to improve recall, typically at the expense of precision. McNamee and Mayfield (2002) and Mayfield et al. (2003) used thousands of language independent features such as character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. The use of CRF sequence labeling for NER has shown success (McCallum and Li, 2003; Nadeau and Sekine, 2009; Benajiba and Rosso, 2008). 3 Baseline Arabic NER System For the baseline system, we used the CRF++1 implementation of CRF sequence labeling with default parameters. We opted to reimplement the most suc1http://code.google.com/p/crfpp/ 1560 cessful features that were reported by Benajiba et al. (2008) and Abdul-Hamid and Darwish (2010), namely the leading and trailing 1, 2, 3, and 4 letters in a word; whether a word appears in the gazetteer that was created by Benajiba et al. (2008), which is publicly available, but is rather small (less than 5,000 entries); and the stemmed form of words (afte</context>
</contexts>
<marker>Nadeau, Sekine, 2009</marker>
<rawString>D. Nadeau and S. Sekine. 2009. A Survey of Named Entity Recognition and Classification. Named Entities: Recognition, Classification and Use, ed. S. Sekine and E. Ranchhod, John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander E Richman</author>
<author>Patrick Schone</author>
</authors>
<title>Mining wiki resources for multilingual named entity recognition.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association</booktitle>
<contexts>
<context position="8288" citStr="Richman and Schone (2008)" startWordPosition="1322" endWordPosition="1325">s capitalized in English or not. They reported that it was the second most discriminating feature that they used. However, there seems to be room for improvement because: (1) MADA’s dictionary is relatively small and would have low coverage; and (2) the use of such a binary feature is problematic, because Arabic names are often common Arabic words and hence a word may be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using large bilingual resources, and we incorporate confidences to avoid having a binary feature. Richman and Schone (2008) used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages. Transliteration Mining (TM) has been used to enrich MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In o</context>
</contexts>
<marker>Richman, Schone, 2008</marker>
<rawString>Alexander E. Richman and Patrick Schone. 2008. Mining wiki resources for multilingual named entity recognition. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shaalan</author>
<author>H Raza</author>
</authors>
<title>Person Name Entity Recognition for Arabic.</title>
<date>2007</date>
<booktitle>Proceedings of the 5th Workshop on Important Unresolved Matters,</booktitle>
<pages>1724</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11109" citStr="Shaalan and Raza (2007)" startWordPosition="1789" endWordPosition="1792">hey examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and a Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMMbased NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic tagger to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. Abdul-Hamid and Darwish (2010) used a simplified feature set that relied primarily on character level features, namely leading and trailing letters in a word. They also experimented with a variety of phrase level features with little success. They reported an F-measure of 76% and 81% for the ACE2005 and the ANERCorp datasets datasets respectivel</context>
</contexts>
<marker>Shaalan, Raza, 2007</marker>
<rawString>K. Shaalan and H. Raza. 2007. Person Name Entity Recognition for Arabic. Proceedings of the 5th Workshop on Important Unresolved Matters, pages 1724, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shi</author>
<author>R Mihalcea</author>
<author>M Tian</author>
</authors>
<title>Cross Language Text Classification by Model Translation and Semisupervised Learning.</title>
<date>2010</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<contexts>
<context position="7019" citStr="Shi et al., 2010" startWordPosition="1108" endWordPosition="1111">d work; Section 3 describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 Related Work 2.1 Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA,</context>
</contexts>
<marker>Shi, Mihalcea, Tian, 2010</marker>
<rawString>L. Shi, R. Mihalcea, M. Tian. 2010. Cross Language Text Classification by Model Translation and Semisupervised Learning. Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raghavendra Udupa</author>
<author>Anton Bakalov</author>
<author>Abhijit Bhole</author>
</authors>
<title>They Are Out There, If You Know Where to Look: Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval.</title>
<date>2009</date>
<booktitle>Advances in Information Retrieval. Pages:</booktitle>
<pages>437--448</pages>
<contexts>
<context position="8542" citStr="Udupa et al., 2009" startWordPosition="1363" endWordPosition="1366"> such a binary feature is problematic, because Arabic names are often common Arabic words and hence a word may be translated as a named entity and as a common word. To overcome these two problems, we use cross-lingual features to improve NER using large bilingual resources, and we incorporate confidences to avoid having a binary feature. Richman and Schone (2008) used English linguis1559 tic tools and cross language links in Wikipedia to automatically annotate text in different languages. Transliteration Mining (TM) has been used to enrich MT phrase tables or to improve cross language search (Udupa et al., 2009). Conversely, people have used NER to determine if a word is to be transliterated or not (Hermjakob et al., 2008). However, we are not aware of any prior work on using TM to determine if a sequence is a NE. Further, we are not aware of prior work on using TM (or transliteration in general) as a cross lingual feature in any annotation task. In our work, we use state-of-the-art TM as described by El-Kahki et al. (2011) 2.2 Arabic NER Much work has been done on NER with multiple public evaluation forums. Nadeau and Sekine (Nadeau and Sekine, 2009) surveyed lots of work on NER for a variety of lan</context>
</contexts>
<marker>Udupa, Bakalov, Bhole, 2009</marker>
<rawString>Raghavendra Udupa, Anton Bakalov, and Abhijit Bhole. 2009. They Are Out There, If You Know Where to Look: Mining Transliterations of OOV Query Terms for Cross-Language Information Retrieval. Advances in Information Retrieval. Pages: 437-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>G Ngai</author>
</authors>
<title>Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora. In</title>
<date>2001</date>
<contexts>
<context position="7045" citStr="Yarowsky and Ngai, 2001" startWordPosition="1112" endWordPosition="1115">describes the baseline system; Section 4 introduces the cross-lingual features and reports on their effectiveness; and Section 5 concludes the paper. 2 Related Work 2.1 Using cross-lingual Features For many NLP tasks, some languages may have significantly more training data, better knowledge resources, or more discriminating features than other languages. If cross-lingual resources are available, such as parallel data, increased training data, better resources, or superior features can be used to improve the processing (ex. tagging) for other languages (Ganchev et al., 2009; Shi et al., 2010; Yarowsky and Ngai, 2001). Some work has attempted to use bilingual features in NER. Burkett et al. (2010) used bilingual text to improve monolingual models including NER models for German, which lacks a good capitalization feature. They did so by training a bilingual model and then generating more training data from unlabeled parallel data. They showed significant improvement in German NER effectiveness, particularly for recall. In our work, there is no need for tagged text that has a parallel equivalent in another language. Benajiba et al. (2008) used an Arabic English dictionary from MADA, an Arabic analyzer, to in</context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>D. Yarowsky and G. Ngai. 2001. Inducing Multilingual POS Taggers and NP Bracketers via Robust Projection across Aligned Corpora. In NAACL-2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>