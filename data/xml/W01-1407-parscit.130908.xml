<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996546">
Toward hierarchical models for statistical machine translation of
inflected languages
</title>
<author confidence="0.976925">
Sonja Nießen and Hermann Ney
</author>
<affiliation confidence="0.831373">
Lehrstuhl f¨ur Informatik VI,
Computer Science Department
RWTH Aachen - University of Technology
D-52056 Aachen, Germany
</affiliation>
<email confidence="0.971861">
niessen,ney@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.981902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989019047619">
In statistical machine translation, cor-
respondences between the words in
the source and the target language are
learned from bilingual corpora on the
basis of so called alignment models.
Existing statistical systems for MT of-
ten treat different derivatives of the
same lemma as if they were indepen-
dent of each other. In this paper we
argue that a better exploitation of the
bilingual training data can be achieved
by explicitly taking into account the in-
terdependencies of the different deriva-
tives. We do this along two direc-
tions: Usage of hierarchical lexicon
models and the introduction of equiv-
alence classes in order to ignore in-
formation not relevant for the trans-
lation task. The improvement of the
translation results is demonstrated on a
German-English corpus.
</bodyText>
<sectionHeader confidence="0.995167" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979678571429">
The statistical approach to machine translation
has become widely accepted in the last few years.
It has been successfully applied to realistic tasks
in various national and international research pro-
grams. However in many applications only small
amounts of bilingual training data are available
for the desired domain and language pair, and it
is highly desirable to avoid at least parts of the
costly data collection process.
Some recent publications have dealt with the
problem of translation with scarce resources.
(Brown et al., 1994) describe the use of dictio-
naries. (Al-Onaizan et al., 2000) report on an ex-
periment of Tetun-to-English translation by dif-
ferent groups, including one using statistical ma-
chine translation. They assume the absence of
linguistic knowledge sources such as morphologi-
cal analyzers and dictionaries. Nevertheless, they
found that human mind is very well capable of
deriving dependencies such as morphology, cog-
nates, proper names, spelling variations etc., and
that this capability was finally at the basis of the
better results produced by humans compared to
corpus based machine translation. The additional
information results from complex reasoning and it
is not directly accessible from the full word form
representation of the data.
In this paper, we take a different point of
view: Even if full bilingual training data is scarce,
monolingual knowledge sources like morpholog-
ical analyzers and data for training the target lan-
guage model as well as conventional dictionar-
ies (one word and its translation per entry) may
be available and of substantial usefulness for im-
proving the performance of statistical translation
systems. This is especially the case for highly in-
flected languages like German.
We address the question of how to achieve a
better exploitation of the resources for training the
parameters for statistical machine translation by
taking into account explicit knowledge about the
languages under consideration. In our approach
we introduce equivalence classes in order to ig-
nore information not relevant to the translation
process. We furthermore suggest the use of hi-
erarchical lexicon models.
The paper is organized as follows. After re-
viewing the statistical approach to machine trans-
lation, we first explain our motivation for exam-
ining the morphological characteristics of an in-
flected language like German. We then describe
the chosen output representation after the analysis
and present our approach for exploiting the infor-
mation from morpho-syntactic analysis. Experi-
mental results on the German-English Verbmobil
task are reported.
</bodyText>
<sectionHeader confidence="0.943645" genericHeader="method">
2 Statistical Machine Translation
</sectionHeader>
<bodyText confidence="0.9999068">
The goal of the translation process in statisti-
cal machine translation can be formulated as fol-
lows: A source language string
is to be translated into a target language string
. In the experiments reported in this
paper, the source language is German and the tar-
get language is English. Every English string is
considered as a possible translation for the input.
If we assign a probability to each pair
of strings , then according to Bayes’ de-
cision rule, we have to choose the English string
that maximizes the product of the English lan-
guage model and the string translation
model .
Many existing systems for statistical machine
translation (Wang and Waibel, 1997; Nießen et
al., 1998; Och and Weber, 1998) make use of a
special way of structuring the string translation
model like proposed by (Brown et al., 1993): The
correspondence between the words in the source
and the target string is described by alignments
which assign one target word position to each
source word position. The lexicon probability
of a certain English word is assumed
to depend basically only on the source word
aligned to it.
The overall architecture of the statistical trans-
lation approach is depicted in Figure 1. In this
figure we already anticipate the fact that we can
transform the source strings in a certain manner.
</bodyText>
<sectionHeader confidence="0.982971" genericHeader="method">
3 Basic Considerations
</sectionHeader>
<bodyText confidence="0.9972235">
The parameters of the statistical knowledge
sources mentioned above are trained on bilingual
</bodyText>
<subsectionHeader confidence="0.817517">
Source Language Text
morpho-syntactic
Analysis
</subsectionHeader>
<table confidence="0.970551625">
Pr(fj  |e1I )
Transformation
J
f1
Lexicon Model
Global Search:
maximize Pr( e1
I) Pr(fj  |e1I)
over eI
1
I )
Pr( e1
Alignment Model
Language Model
Transformation
Target Language Text
</table>
<figureCaption confidence="0.8362035">
Figure 1: Architecture of the translation approach
based on Bayes’ decision rule.
</figureCaption>
<bodyText confidence="0.999957945945946">
corpora. In general, the resulting probabilistic
lexica contain all word forms occurring in this
training corpora as separate entries, not taking
into account whether or not they are derivatives of
the same lemma. Bearing in mind that 40% of the
word forms have only been seen once in training
(see Table 2), it is obvious that learning the cor-
rect translations is difficult for many words. Be-
sides, new input sentences are expected to con-
tain unknown word forms, for which no transla-
tion can be retrieved from the lexica. As Table
2 shows, this problem is especially relevant for
highly inflected languages like German: Texts in
German contain many more different word forms
than their English translations. The table also re-
veals that these words are often derived from a
much smaller set of base forms (“lemmata”), and
when we look at the number of different lemmata
and the respective number of lemmata, for which
there is only one occurrence in the training data,
German and English texts are more resembling.
Another aspect is the fact that conventional dic-
tionaries are often available in an electronic form
for the considered language pair. Their usabil-
ity for statistical machine translation is restricted
because they are substantially different from full
bilingual parallel corpora inasmuch the entries are
often pairs of base forms that are translations of
each other, whereas the corpora contain full sen-
tences with inflected forms. To make the informa-
tion taken from external dictionaries more useful
for the translation of inflected language is an in-
teresting objective.
As a consequence of these considerations, we
aim at taking into account the interdependencies
between the different derivatives of the same base
form.
</bodyText>
<sectionHeader confidence="0.934275" genericHeader="method">
4 Output Representation after
</sectionHeader>
<subsectionHeader confidence="0.627287">
Morpho-syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.973524875">
We use GERCG, a constraint grammar parser for
German for lexical analysis and morphological
and syntactic disambiguation. For a description
of the Constraint Grammar approach we refer the
reader to (Karlsson, 1990). Figure 2 gives an ex-
ample of the information provided by this tool.
Input: Wir wollen nach dem Essen
nach Essen aufbrechen
</bodyText>
<equation confidence="0.942721909090909">
&amp;quot;&lt;*wir&gt;&amp;quot;
&amp;quot;wir&amp;quot; * PRON PERS PL1 NOM
&amp;quot;&lt;wollen&gt;&amp;quot;
&amp;quot;wollen&amp;quot; V IND PR¨AS PL1
&amp;quot;&lt;nach&gt;&amp;quot;
&amp;quot;nach&amp;quot; pre PR¨AP
&amp;quot;&lt;dem&gt;&amp;quot;
&amp;quot;das&amp;quot; ART DEF SG DAT NEUTR
&amp;quot;&lt;*essen&gt;&amp;quot;
&amp;quot;*essen&amp;quot; S NEUTR SG DAT
&amp;quot;&lt;nach&gt;&amp;quot;
</equation>
<bodyText confidence="0.9102181875">
is a verb in the indicative present first
person plural form. Without any context taken
into account,
has other readings. It can
even be interpreted as derived not from a verb,
but from an adjective with the meaning
of
In this sense, the information inherent to
the original word forms is augmented by the dis-
than one reading, it is often possible to apply sim-
ple heuristics based on domain specific preference
rules or to use a more general, non-ambiguous
analysis.
The new representation of the corpus where
full word forms are replaced by lemma plus mor-
phological and syntactic tags makes it possible
to gradually reduce the information: For exam-
ple we can consider certain instances of words as
equivalent. We have used this fact to better exploit
the bilingual training data along two directions:
Omitting unimportant information and using hi-
erarchical tran
“wollen”
“wollen”
“made
wool”.
slation models.
ambiguating analyzer. This can be useful for de-
riving the correct translation of ambiguous words.
In the rare cases where the tools returned more
5 Equivalence classes of words with
similar Translations
</bodyText>
<equation confidence="0.6505587">
Dat
&amp;quot;nach&amp;quot; pre PR¨AP Dat
&amp;quot;&lt;*essen&gt;&amp;quot;
&amp;quot;*essen&amp;quot; S EIGEN NEUTR SG DAT
&amp;quot;*esse&amp;quot; S FEM PL DAT
&amp;quot;*essen&amp;quot; S NEUTR PL DAT
&amp;quot;*essen&amp;quot; S NEUTR SG DAT
&amp;quot;&lt;aufbrechen&gt;&amp;quot;
&amp;quot;aufbrechen&amp;quot; V INF
sentence
</equation>
<bodyText confidence="0.795560666666667">
ysis: From the interpretation
i.e. the lemma plus part of speech
plus the other tags the word form
can
be restored. From Figure 2 we see that the tool
can
</bodyText>
<equation confidence="0.553376">
“gehen-V-IND-
PR¨AS-SG1”,
“gehe”
</equation>
<bodyText confidence="0.6195395">
quite reliably disambiguate between differ-
ent readings: It infers for instance that the word
</bodyText>
<figureCaption confidence="0.936114">
Figure 2: Sample analysis of a German
</figureCaption>
<bodyText confidence="0.999307571428572">
A full word form is represented by the infor-
mation provided by the morpho-syntactic anal-
Inflected forms of words in the input language
contain information that is not relevant for trans-
lation. This is especially true for the task of
translating from a highly inflected language like
German into English for instance: In bilingual
German-English corpora, the German part con-
tains many more different word forms than the
English part (see Table 2). It is useful for the
process of statistical machine translation to define
equivalence classes of word forms which tend to
be translated by the same target language word,
because then, the resulting statistical translation
lexica become smoother and the coverage is sig-
nificantly improved. We construct these equiva-
lence classes by omitting those informations from
morpho-syntactic analysis, which are not relevant
for the translation task.
The representation of the corpus like it is pro-
vided by the analyzing tools helps to identify -
and access -the unimportant information. The
definition of relevant and unimportant informa-
tion, respectively, depends on many factors like
the involved languages, the tran
slation direction
and the choice of the models.
Linguistic knowledge can provide information
about which characteristics of an input sentence
are crucial to the translation task and which can
be ignored, but it is desirable to find a method
for automating this decision process. We found
that the impact on the end result due to different
choices of features to be ignored was not large
enough to serve as reliable criterion. Instead, we
could think of defining a likelihood criterion on
a held-out corpus for this purpose. Another pos-
sibility is to assess the impact on the alignment
quality after training, which can be evaluated au-
tomatically (Langlais et al., 1998; Och and Ney,
2000), but as we found that the alignment quality
on the Verbmobil data is consistently very high,
and extremely robust against manipulation of the
training data, we abandoned this approach.
We resorted to detecting candidates from the
probabilistic lexica trained for translation from
German to English. For this, we focussed on
those derivatives of the same base form, which
resulted in the same translation. For each set
of tags, we counted how often an additional tag
could be replaced by a certain other tag without
effect on the translation. Table 1 gives some of
the most frequently identified candidates to be ig-
nored while translating: The gender of nouns is
irrelevant for their translation (which is straight-
forward, because the gender is unambiguous for a
certain noun) and the case, i.e. nominative, dative,
accusative. For the genitive forms, the translation
in English differs. For verbs we found the candi-
dates number and person. That is, the translation
of the first person singular form of a verb is of-
ten the same as the translation of the third person
plural form, for example.
</bodyText>
<tableCaption confidence="0.965068">
Table 1: Candidates for equivalence classes.
</tableCaption>
<table confidence="0.984743142857143">
POS candidates
noun gender: MASK,FEM,NEUTR
verb and case: NOM,DAT,AKK
adjective number: SG,PL
number and person: 1,2,3
gender, case and number
case
</table>
<bodyText confidence="0.7064545">
which were most often identified as irrelevant for
translation from German to English.
</bodyText>
<sectionHeader confidence="0.996366" genericHeader="method">
6 Hierarchical Models
</sectionHeader>
<bodyText confidence="0.952128956521739">
One way of taking into account the interdepen-
dencies of different derivatives of the same base
form is to introduce equivalence classes at vari-
ous levels of abstraction starting with the inflected
form and ending with the lemma.
Consider, for example, the German verb
form &amp;quot;ankomme&amp;quot;, which is derived from
the lemma &amp;quot;ankommen&amp;quot; and which can be
translated into English by &amp;quot;arrive&amp;quot;. The
hierarchy of equivalence classes is as follows:
&amp;quot;ankommen-V-IND-PR¨AS-SG1&amp;quot;
&amp;quot;ankommen-V-IND-PR¨AS-SG&amp;quot;
&amp;quot;ankommen-V-IND-PR¨AS&amp;quot;
&amp;quot;ankommen&amp;quot;
is the maximal number of morpho-syntactic
tags. contains the forms &amp;quot;ankomme&amp;quot;,
&amp;quot;ankommst&amp;quot; and &amp;quot;ankommt&amp;quot;; in the
number (SG or PL) is ignored and so on. The
largest equivalence class contains all derivatives
of the infinitive &amp;quot;ankommen&amp;quot;.
We can now define the lexicon probability of a
word to be translated by with respect to the
level:
</bodyText>
<equation confidence="0.967288">
(1)
</equation>
<bodyText confidence="0.949898473684211">
where is the representation of a
word where the lemma and additional tags
are taken into account. For the example above,
&amp;quot;ankommen&amp;quot;, &amp;quot;V&amp;quot;, and so on.
is the probability of fora given.
We make the assumption that this probability does
not depend on. is always assumed to be
1. In other words, the inflected form can non-
ambiguously be derived from the full interpreta-
tion.
...
As a consequence, we dropped those tags,
is the probability of the translation for
to belong to the equivalence class. The sum
over amounts to summing up over all possible
readings of .1
We combine the by means of linear interpo-
lation:
(2)
</bodyText>
<sectionHeader confidence="0.941542" genericHeader="method">
7 Translation Experiments
</sectionHeader>
<bodyText confidence="0.9999">
Experiments were carried out on Verbmobil data,
which consists of spontaneously spoken dialogs
in the appointment scheduling domain (Wahlster,
1993). German source sentences are translated
into English.
</bodyText>
<subsectionHeader confidence="0.995822">
7.1 Treatment of Ambiguity
</subsectionHeader>
<bodyText confidence="0.999995227272727">
Common bilingual corpora normally contain full
sentences which provide enough context informa-
tion for ruling out all but one reading for an in-
flected word form. To reduce the remaining un-
certainty, we have implemented preference rules.
For instance, we assume that the corpus is cor-
rectly true-case-converted beforehand and as a
consequence, we drop non-noun interpretations
of uppercase words. Besides, we prefer indica-
tive verb readings instead of subjunctive or im-
perative. For the remaining ambiguities, we resort
to the unambiguous parts of the readings, i.e. we
drop all tags causing mixed interpretations.
There are some special problems with the anal-
ysis of external lexica, which do not provide
enough context to enable efficient disambigua-
tion. We are currently implementing methods for
handling this special situation.
It can be argued that it would be more elegant to
leave the decision between different readings, for
instance, to the overall decision process in search.
We plan this integration for the future.
</bodyText>
<subsectionHeader confidence="0.989984">
7.2 Performance Measures
</subsectionHeader>
<bodyText confidence="0.999769428571429">
We use the following evaluation criteria (Nießen
et al., 2000):
SSER (subjective sentence error rate):
Each translated sentence is judged by a hu-
man examiner according to an error scale
from 0.0 (semantically and syntactically cor-
rect) to 1.0 (completely wrong).
</bodyText>
<footnote confidence="0.437388">
1The probability functions are defined to return zero for
impossible interpretations of.
</footnote>
<bodyText confidence="0.999605111111111">
ISER (information item semantic error rate):
The test sentences are segmented into infor-
mation items; for each of them, the trans-
lation candidates are assigned either “ok”
or an error class. If the intended informa-
tion is conveyed, the error count is not in-
creased, even if there are slight syntactical
errors, which do not seriously deteriorate the
intelligibility.
</bodyText>
<subsectionHeader confidence="0.997743">
7.3 Translation Results
</subsectionHeader>
<bodyText confidence="0.93600075">
The training set consists of 58 322 sentence pairs.
Table 2 summarizes the characteristics of the
training corpus used for training the parameters of
Model 4 proposed in (Brown et al., 1993). Testing
</bodyText>
<tableCaption confidence="0.729909333333333">
Table 2: Corpus statistics: Verbmobil training.
Singletons are types occurring only once in train-
ing.
</tableCaption>
<table confidence="0.775814571428571">
English German
no. of running words 550 213 519 790
no. of word forms 4 670 7 940
no. of singletons 1 696 3 452
singletons [%] 36 43
no. of lemmata 3 875 3 476
no. of singletons 1 322 1 457
</table>
<tableCaption confidence="0.5743545">
was carried out on 200 sentences not contained in
the training data. For a detailed statistics see Ta-
ble 3.
Table 3: Statistics of the Verbmobil test corpus
for German-to-English translation. Unknowns are
word forms not contained in the training corpus.
</tableCaption>
<table confidence="0.58841">
no. of sentences 200
no. of running words 2 055
no. of word forms 385
no. of unknown word forms 25
</table>
<bodyText confidence="0.9729275">
We used a translation system called “single-
word based approach” described in (Tillmann and
Ney, 2000) and compared to other approaches in
(Ney et al., 2000).
</bodyText>
<subsectionHeader confidence="0.632174">
7.3.1 Lexicon Combination
</subsectionHeader>
<bodyText confidence="0.999749310344827">
So far we have performed experiments with hi-
erarchical lexica, where two levels are combined,
i.e. in Equation (2) is set to 1. and are
set to and is modeled as a uniform
distribution over all derivations of the lemma
occurring in the training data plus the base form
itself, in case it is not contained. The process of
lemmatization is unique in the majority of cases,
and as a consequence, the sum in Equation (1) is
not needed for a two-level lexicon combination of
full word forms and lemmata.
As the results summarized in Table 4 show, the
combined lexicon outperforms the conventional
one-level lexicon. As expected, the quality gain
achieved by smoothing the lexicon is larger if
the training procedure can take advantage of an
additional conventional dictionary to learn trans-
lation pairs, because these dictionaries typically
only contain base forms of words, whereas trans-
lations of fully inflected forms are needed in the
test situation.
Examples taken from the test set are given in
Figure 3. Smoothing the lexicon entries over the
derivatives of the same lemma enables the trans-
lation of “sind” by “would” instead of “are”. The
smoothed lexicon contains the translation “conve-
nient” for any derivative of “bequem”. The com-
parative “more convenient” would be the com-
pletely correct translation.
</bodyText>
<subsectionHeader confidence="0.878164">
7.3.2 Equivalence classes
</subsectionHeader>
<bodyText confidence="0.9575364">
As already mentioned, we resorted to choos-
ing one single reading for each word by applying
some heuristics (see Section 7.1). For the nor-
mal training corpora, unlike additional external
dictionaries, this is not critical because they con-
tain predominantly full sentences which provide
enough context for an efficient disambiguation.
Currently, we are working on the problem of ana-
lyzing the entries in conventional dictionaries, but
for the time being, experiments for equivalence
classes have been carried out using only bilingual
corpora for estimating the model parameters.
Table 5 shows the effect of the introduction of
equivalence classes. The information from the
morpho-syntactic analyzer (stems plus tags like
described in Section 4) is reduced by dropping
unimportant information like described in Section
5. Both error metrics could be decreased in com-
parison to the usage of the original corpus with
inflected word forms. A reduction of 3.3% of the
information item semantic error rate shows that
more of the intended meaning could be found in
the produced translations.
Table 5: Effect of the introduction of equivalence
classes. For the baseline we used the original in-
flected word forms.
SSER [%] ISER [%]
inflected words 37.4 26.8
equivalence classes 35.9 23.5
The first two examples in Figure 4 demonstrate
the effect of the disambiguating analyzer which
identifies “Hotelzimmer” as singular on the ba-
sis of the context (the word itself can represent
the plural form as well), and “das” as article in
contrast to a pronoun. The third example shows
the advantage of grouping words in equivalence
classes: The training data does not contain the
word “billigeres”, but when generalizing over the
gender and case information, a correct translation
can be produced.
</bodyText>
<sectionHeader confidence="0.938604" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99986347826087">
We have presented methods for a better exploita-
tion of the bilingual training data for statisti-
cal machine translation by explicitly taking into
account the interdependencies of the different
derivatives of the same base form. We suggest the
usage of hierarchical models as well as an alter-
native representation of the data in combination
with the identification and omission of informa-
tion not relevant for the translation task.
First experiments prove their general applica-
bility to realistic tasks such as spontaneously spo-
ken dialogs. We expect the described methods to
yield more improvement of the translation quality
for cases where much smaller amounts of training
data are available.
As there is a large overlap between the mod-
eled events in the combined probabilistic models,
we assume that log-linear combination would re-
sult in more improvement of the translation qual-
ity than the combination by linear interpolation
does. We will investigate this in the future. We
also plan to integrate the decision regarding the
choice of readings into the search process.
</bodyText>
<tableCaption confidence="0.981991">
Table 4: Effect of two-level lexicon combination. For the baseline we used the conventional one-level
full form lexicon.
</tableCaption>
<table confidence="0.9954892">
ext. dictionary SSER [%] ISER [%]
baseline yes 35.7 23.9
combined yes 33.8 22.3
baseline no 37.4 26.8
combined no 36.9 25.8
</table>
<bodyText confidence="0.767552">
input sind Sie mit einem Doppelzimmer einverstanden?
baseline are you agree with a double room?
combined lexica would you agree with a double room?
input mit dem Zug ist es bequemer
baseline by train it is UNKNOWN-bequemer
combined lexica by train it is convenient
</bodyText>
<figureCaption confidence="0.94503">
Figure 3: Examples for the effect of the combined lexica.
</figureCaption>
<bodyText confidence="0.52267425">
Acknowledgement. This work was partly sup-
ported by the German Federal Ministry of Educa-
tion, Science, Research and Technology under the
Contract Number 01 IV 701 T4 (VERBMOBIL).
</bodyText>
<sectionHeader confidence="0.970408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995283125">
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and
Kenji Yamada. 2000. Translating with scarce re-
sources. In Proceedings of the Seventeenth Na-
tional Conference on Artificial Intelligence (AAAI),
pages 672–678, Austin, Texas, August.
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. Mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
P. F. Brown, S. A. Della Pietra, and M. J. Della Pietra,
V. J. a nd Goldsmith. 1994. But dictionaries are
data too. In Proc. ARPA Human Language Tech-
nology Workshop ’93, pages 202–205, Princeton,
NJ, March. distributed as Human Language Tech-
nology by San Mateo, CA: Morgan Kaufmann Pub-
lishers.
Fred Karlsson. 1990. Constraint grammar as a frame-
work for parsing running text. In Proceedings of the
13th International Conference on Computational
Linguistics, volume 3, pages 168–173, Helsinki,
Finland.
Philippe Langlais, Michel Simard, and Jean V´eronis.
1998. Methods and practical issues in evaluat-
ing alignment techniques. In Proceedings of 36th
Annual Meeting of the Association for Computa-
tional Linguistics and 17th International Confer-
ence on Computational Linguistic, pages 711–717,
Montr´eal, P.Q., Canada, August.
Hermann Ney, Sonja Nießen, Franz Josef Och, Has-
san Sawaf, Christoph Tillmann, and Stephan Vogel.
2000. Algorithms for statistical translation of spo-
ken language. IEEE Transactions on Speech and
Audio Processing, 8(1):24–36, January.
Sonja Nießen, Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1998. A DP based search al-
gorithm for statistical machine translation. In Pro-
ceedings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and the 17th In-
ternational Conference on Computational Linguis-
tics, pages 960–967, Montr´eal, P.Q., Canada, Au-
gust.
Sonja Nießen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for MT research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation, pages 39–
45, Athens, Greece, May.
Franz Josef Och and Hermann Ney. 2000. Im-
proved statistical alignment models. In Proc. of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440–447, Hongkong,
China, October.
Franz Josef Och and Hans Weber. 1998. Improv-
ing statistical natural language translation with cat-
egories and rules. In Proceedings of the 36th
</reference>
<bodyText confidence="0.994464705882353">
ich reserviere das Hotelzimmer
I will reserve that hotel rooms
ich-PRON-PERS-SG1-NOM reservieren-V-IND-PR¨AS-SG1
das-ART-DEF-SG-AKK-NEUTR Hotelzimmer-S-NEUTR-SG-AKK
ich-PRON-PERS-SG1 reservieren-V-IND-PR¨AS-SG das-ART-DEF-SG Hotelzimmer-S-SG
I will reserve the hotel room
fliegen wir?
we flying?
fliegen-V-IND-PR¨AS-PL1 wir-PRON-PERS-PL1-NOM?
fliegen-V-IND-PR¨AS-PL wir-PRON-PERS-PL?
do we fly?
gibt es nichts billigeres?
there is do not UNKNOWN-billigeres?
geben-V-IND-PR¨AS-SG3 es-PRON-PERS-SG3-NOM-NEUTR
nichts-DET-INDEF-NEG-SG-AKK billig-A-KOMP-SG-NOM/AKK-NEUTR?
geben-V-IND-PR¨AS-SG es-PRON-PERS-SG3 nichts-DET-INDEF-NEG-SG billig-A-KOMP?
there is nothing cheaper?
</bodyText>
<figureCaption confidence="0.496881">
Figure 4: Examples for the effect of equivalence classes resulting from dropping morpho-syntactic
tags not relevant for translation. First the translation using the original representation, then the new
representation, its reduced form and the resulting translation.
</figureCaption>
<reference confidence="0.9949780625">
Annual Meeting of the Association for Computa-
tional Linguistics and the 17th International Con-
ference on Computational Linguistics, pages 985–
989, Montr´eal, P.Q., Canada, August.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and DP-based search in statistical ma-
chine translation. In Proc. COLING 2000: The
18th Int. Conf. on Computational Linguistics, pages
850–856, Saarbr¨ucken, Germany, August.
Wolfgang Wahlster. 1993. Verbmobil: Translation of
Face-to-Face Dialogs. In Proceedings of the MT
Summit IV, pages 127–135, Kobe, Japan.
Ye-Yi Wang and Alex Waibel. 1997. Decoding al-
gorithm in statistical translation. In Proceedings of
the ACL/EACL ’97, Madrid, Spain, pages 366–372,
July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.681693">
<title confidence="0.9982595">Toward hierarchical models for statistical machine translation inflected languages</title>
<author confidence="0.988069">Nießen Ney</author>
<affiliation confidence="0.917658333333333">Lehrstuhl f¨ur Informatik Computer Science RWTH Aachen - University of</affiliation>
<address confidence="0.978091">D-52056 Aachen,</address>
<email confidence="0.997034">niessen,ney@informatik.rwth-aachen.de</email>
<abstract confidence="0.999413466666667">In statistical machine translation, correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models. Existing statistical systems for MT often treat different derivatives of the same lemma as if they were independent of each other. In this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives. We do this along two direc-</abstract>
<intro confidence="0.931059">tions: Usage of hierarchical lexicon</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Ulrich Germann</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Translating with scarce resources.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI),</booktitle>
<pages>672--678</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="1669" citStr="Al-Onaizan et al., 2000" startWordPosition="251" endWordPosition="254">h corpus. 1 Introduction The statistical approach to machine translation has become widely accepted in the last few years. It has been successfully applied to realistic tasks in various national and international research programs. However in many applications only small amounts of bilingual training data are available for the desired domain and language pair, and it is highly desirable to avoid at least parts of the costly data collection process. Some recent publications have dealt with the problem of translation with scarce resources. (Brown et al., 1994) describe the use of dictionaries. (Al-Onaizan et al., 2000) report on an experiment of Tetun-to-English translation by different groups, including one using statistical machine translation. They assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries. Nevertheless, they found that human mind is very well capable of deriving dependencies such as morphology, cognates, proper names, spelling variations etc., and that this capability was finally at the basis of the better results produced by humans compared to corpus based machine translation. The additional information results from complex reasoning and it is n</context>
</contexts>
<marker>Al-Onaizan, Germann, Hermjakob, Knight, Koehn, Marcu, Yamada, 2000</marker>
<rawString>Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Daniel Marcu, and Kenji Yamada. 2000. Translating with scarce resources. In Proceedings of the Seventeenth National Conference on Artificial Intelligence (AAAI), pages 672–678, Austin, Texas, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>Mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4521" citStr="Brown et al., 1993" startWordPosition="700" endWordPosition="703">reported in this paper, the source language is German and the target language is English. Every English string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the English string that maximizes the product of the English language model and the string translation model . Many existing systems for statistical machine translation (Wang and Waibel, 1997; Nießen et al., 1998; Och and Weber, 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position. The lexicon probability of a certain English word is assumed to depend basically only on the source word aligned to it. The overall architecture of the statistical translation approach is depicted in Figure 1. In this figure we already anticipate the fact that we can transform the source strings in a certain manner. 3 Basic Considerations The parameters of the statistical knowledge sources mentioned above are trained on biling</context>
<context position="16302" citStr="Brown et al., 1993" startWordPosition="2588" endWordPosition="2591">ro for impossible interpretations of. ISER (information item semantic error rate): The test sentences are segmented into information items; for each of them, the translation candidates are assigned either “ok” or an error class. If the intended information is conveyed, the error count is not increased, even if there are slight syntactical errors, which do not seriously deteriorate the intelligibility. 7.3 Translation Results The training set consists of 58 322 sentence pairs. Table 2 summarizes the characteristics of the training corpus used for training the parameters of Model 4 proposed in (Brown et al., 1993). Testing Table 2: Corpus statistics: Verbmobil training. Singletons are types occurring only once in training. English German no. of running words 550 213 519 790 no. of word forms 4 670 7 940 no. of singletons 1 696 3 452 singletons [%] 36 43 no. of lemmata 3 875 3 476 no. of singletons 1 322 1 457 was carried out on 200 sentences not contained in the training data. For a detailed statistics see Table 3. Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. no. of sentences 200 no. of running words 2 </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. Mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>M J Della Pietra</author>
<author>V J</author>
</authors>
<title>a nd Goldsmith.</title>
<date>1994</date>
<booktitle>In Proc. ARPA Human Language Technology Workshop ’93,</booktitle>
<pages>202--205</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Princeton, NJ,</location>
<contexts>
<context position="1609" citStr="Brown et al., 1994" startWordPosition="241" endWordPosition="244"> translation results is demonstrated on a German-English corpus. 1 Introduction The statistical approach to machine translation has become widely accepted in the last few years. It has been successfully applied to realistic tasks in various national and international research programs. However in many applications only small amounts of bilingual training data are available for the desired domain and language pair, and it is highly desirable to avoid at least parts of the costly data collection process. Some recent publications have dealt with the problem of translation with scarce resources. (Brown et al., 1994) describe the use of dictionaries. (Al-Onaizan et al., 2000) report on an experiment of Tetun-to-English translation by different groups, including one using statistical machine translation. They assume the absence of linguistic knowledge sources such as morphological analyzers and dictionaries. Nevertheless, they found that human mind is very well capable of deriving dependencies such as morphology, cognates, proper names, spelling variations etc., and that this capability was finally at the basis of the better results produced by humans compared to corpus based machine translation. The addit</context>
</contexts>
<marker>Brown, Pietra, Pietra, J, 1994</marker>
<rawString>P. F. Brown, S. A. Della Pietra, and M. J. Della Pietra, V. J. a nd Goldsmith. 1994. But dictionaries are data too. In Proc. ARPA Human Language Technology Workshop ’93, pages 202–205, Princeton, NJ, March. distributed as Human Language Technology by San Mateo, CA: Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Karlsson</author>
</authors>
<title>Constraint grammar as a framework for parsing running text.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<volume>3</volume>
<pages>168--173</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="7442" citStr="Karlsson, 1990" startWordPosition="1167" endWordPosition="1168"> corpora contain full sentences with inflected forms. To make the information taken from external dictionaries more useful for the translation of inflected language is an interesting objective. As a consequence of these considerations, we aim at taking into account the interdependencies between the different derivatives of the same base form. 4 Output Representation after Morpho-syntactic Analysis We use GERCG, a constraint grammar parser for German for lexical analysis and morphological and syntactic disambiguation. For a description of the Constraint Grammar approach we refer the reader to (Karlsson, 1990). Figure 2 gives an example of the information provided by this tool. Input: Wir wollen nach dem Essen nach Essen aufbrechen &amp;quot;&lt;*wir&gt;&amp;quot; &amp;quot;wir&amp;quot; * PRON PERS PL1 NOM &amp;quot;&lt;wollen&gt;&amp;quot; &amp;quot;wollen&amp;quot; V IND PR¨AS PL1 &amp;quot;&lt;nach&gt;&amp;quot; &amp;quot;nach&amp;quot; pre PR¨AP &amp;quot;&lt;dem&gt;&amp;quot; &amp;quot;das&amp;quot; ART DEF SG DAT NEUTR &amp;quot;&lt;*essen&gt;&amp;quot; &amp;quot;*essen&amp;quot; S NEUTR SG DAT &amp;quot;&lt;nach&gt;&amp;quot; is a verb in the indicative present first person plural form. Without any context taken into account, has other readings. It can even be interpreted as derived not from a verb, but from an adjective with the meaning of In this sense, the information inherent to the original word forms is augmented </context>
</contexts>
<marker>Karlsson, 1990</marker>
<rawString>Fred Karlsson. 1990. Constraint grammar as a framework for parsing running text. In Proceedings of the 13th International Conference on Computational Linguistics, volume 3, pages 168–173, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
<author>Michel Simard</author>
<author>Jean V´eronis</author>
</authors>
<title>Methods and practical issues in evaluating alignment techniques.</title>
<date>1998</date>
<booktitle>In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistic,</booktitle>
<pages>711--717</pages>
<location>Montr´eal, P.Q., Canada,</location>
<marker>Langlais, Simard, V´eronis, 1998</marker>
<rawString>Philippe Langlais, Michel Simard, and Jean V´eronis. 1998. Methods and practical issues in evaluating alignment techniques. In Proceedings of 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistic, pages 711–717, Montr´eal, P.Q., Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Ney</author>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Hassan Sawaf</author>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Algorithms for statistical translation of spoken language.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="17113" citStr="Ney et al., 2000" startWordPosition="2735" endWordPosition="2738">no. of singletons 1 696 3 452 singletons [%] 36 43 no. of lemmata 3 875 3 476 no. of singletons 1 322 1 457 was carried out on 200 sentences not contained in the training data. For a detailed statistics see Table 3. Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. no. of sentences 200 no. of running words 2 055 no. of word forms 385 no. of unknown word forms 25 We used a translation system called “singleword based approach” described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000). 7.3.1 Lexicon Combination So far we have performed experiments with hierarchical lexica, where two levels are combined, i.e. in Equation (2) is set to 1. and are set to and is modeled as a uniform distribution over all derivations of the lemma occurring in the training data plus the base form itself, in case it is not contained. The process of lemmatization is unique in the majority of cases, and as a consequence, the sum in Equation (1) is not needed for a two-level lexicon combination of full word forms and lemmata. As the results summarized in Table 4 show, the combined lexicon outperform</context>
</contexts>
<marker>Ney, Nießen, Och, Sawaf, Tillmann, Vogel, 2000</marker>
<rawString>Hermann Ney, Sonja Nießen, Franz Josef Och, Hassan Sawaf, Christoph Tillmann, and Stephan Vogel. 2000. Algorithms for statistical translation of spoken language. IEEE Transactions on Speech and Audio Processing, 8(1):24–36, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>A DP based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>960--967</pages>
<location>Montr´eal, P.Q., Canada,</location>
<contexts>
<context position="4391" citStr="Nießen et al., 1998" startWordPosition="677" endWordPosition="680">ion can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every English string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the English string that maximizes the product of the English language model and the string translation model . Many existing systems for statistical machine translation (Wang and Waibel, 1997; Nießen et al., 1998; Och and Weber, 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position. The lexicon probability of a certain English word is assumed to depend basically only on the source word aligned to it. The overall architecture of the statistical translation approach is depicted in Figure 1. In this figure we already anticipate the fact that we can transform the source strings in </context>
</contexts>
<marker>Nießen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>Sonja Nießen, Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1998. A DP based search algorithm for statistical machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 960–967, Montr´eal, P.Q., Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="15431" citStr="Nießen et al., 2000" startWordPosition="2451" endWordPosition="2454">ng ambiguities, we resort to the unambiguous parts of the readings, i.e. we drop all tags causing mixed interpretations. There are some special problems with the analysis of external lexica, which do not provide enough context to enable efficient disambiguation. We are currently implementing methods for handling this special situation. It can be argued that it would be more elegant to leave the decision between different readings, for instance, to the overall decision process in search. We plan this integration for the future. 7.2 Performance Measures We use the following evaluation criteria (Nießen et al., 2000): SSER (subjective sentence error rate): Each translated sentence is judged by a human examiner according to an error scale from 0.0 (semantically and syntactically correct) to 1.0 (completely wrong). 1The probability functions are defined to return zero for impossible interpretations of. ISER (information item semantic error rate): The test sentences are segmented into information items; for each of them, the translation candidates are assigned either “ok” or an error class. If the intended information is conveyed, the error count is not increased, even if there are slight syntactical errors,</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, pages 39– 45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="11195" citStr="Och and Ney, 2000" startWordPosition="1781" endWordPosition="1784">an provide information about which characteristics of an input sentence are crucial to the translation task and which can be ignored, but it is desirable to find a method for automating this decision process. We found that the impact on the end result due to different choices of features to be ignored was not large enough to serve as reliable criterion. Instead, we could think of defining a likelihood criterion on a held-out corpus for this purpose. Another possibility is to assess the impact on the alignment quality after training, which can be evaluated automatically (Langlais et al., 1998; Och and Ney, 2000), but as we found that the alignment quality on the Verbmobil data is consistently very high, and extremely robust against manipulation of the training data, we abandoned this approach. We resorted to detecting candidates from the probabilistic lexica trained for translation from German to English. For this, we focussed on those derivatives of the same base form, which resulted in the same translation. For each set of tags, we counted how often an additional tag could be replaced by a certain other tag without effect on the translation. Table 1 gives some of the most frequently identified cand</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hans Weber</author>
</authors>
<title>Improving statistical natural language translation with categories and rules.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>985--989</pages>
<location>Montr´eal, P.Q., Canada,</location>
<contexts>
<context position="4413" citStr="Och and Weber, 1998" startWordPosition="681" endWordPosition="684"> as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every English string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the English string that maximizes the product of the English language model and the string translation model . Many existing systems for statistical machine translation (Wang and Waibel, 1997; Nießen et al., 1998; Och and Weber, 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position. The lexicon probability of a certain English word is assumed to depend basically only on the source word aligned to it. The overall architecture of the statistical translation approach is depicted in Figure 1. In this figure we already anticipate the fact that we can transform the source strings in a certain manner. 3 Ba</context>
</contexts>
<marker>Och, Weber, 1998</marker>
<rawString>Franz Josef Och and Hans Weber. 1998. Improving statistical natural language translation with categories and rules. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 985– 989, Montr´eal, P.Q., Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word re-ordering and DP-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proc. COLING 2000: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>850--856</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="17058" citStr="Tillmann and Ney, 2000" startWordPosition="2725" endWordPosition="2728"> running words 550 213 519 790 no. of word forms 4 670 7 940 no. of singletons 1 696 3 452 singletons [%] 36 43 no. of lemmata 3 875 3 476 no. of singletons 1 322 1 457 was carried out on 200 sentences not contained in the training data. For a detailed statistics see Table 3. Table 3: Statistics of the Verbmobil test corpus for German-to-English translation. Unknowns are word forms not contained in the training corpus. no. of sentences 200 no. of running words 2 055 no. of word forms 385 no. of unknown word forms 25 We used a translation system called “singleword based approach” described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000). 7.3.1 Lexicon Combination So far we have performed experiments with hierarchical lexica, where two levels are combined, i.e. in Equation (2) is set to 1. and are set to and is modeled as a uniform distribution over all derivations of the lemma occurring in the training data plus the base form itself, in case it is not contained. The process of lemmatization is unique in the majority of cases, and as a consequence, the sum in Equation (1) is not needed for a two-level lexicon combination of full word forms and lemmata. As the results summ</context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and DP-based search in statistical machine translation. In Proc. COLING 2000: The 18th Int. Conf. on Computational Linguistics, pages 850–856, Saarbr¨ucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>Verbmobil: Translation of Face-to-Face Dialogs.</title>
<date>1993</date>
<booktitle>In Proceedings of the MT Summit IV,</booktitle>
<pages>127--135</pages>
<location>Kobe, Japan.</location>
<contexts>
<context position="14237" citStr="Wahlster, 1993" startWordPosition="2269" endWordPosition="2270">ra given. We make the assumption that this probability does not depend on. is always assumed to be 1. In other words, the inflected form can nonambiguously be derived from the full interpretation. ... As a consequence, we dropped those tags, is the probability of the translation for to belong to the equivalence class. The sum over amounts to summing up over all possible readings of .1 We combine the by means of linear interpolation: (2) 7 Translation Experiments Experiments were carried out on Verbmobil data, which consists of spontaneously spoken dialogs in the appointment scheduling domain (Wahlster, 1993). German source sentences are translated into English. 7.1 Treatment of Ambiguity Common bilingual corpora normally contain full sentences which provide enough context information for ruling out all but one reading for an inflected word form. To reduce the remaining uncertainty, we have implemented preference rules. For instance, we assume that the corpus is correctly true-case-converted beforehand and as a consequence, we drop non-noun interpretations of uppercase words. Besides, we prefer indicative verb readings instead of subjunctive or imperative. For the remaining ambiguities, we resort </context>
</contexts>
<marker>Wahlster, 1993</marker>
<rawString>Wolfgang Wahlster. 1993. Verbmobil: Translation of Face-to-Face Dialogs. In Proceedings of the MT Summit IV, pages 127–135, Kobe, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding algorithm in statistical translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL ’97,</booktitle>
<pages>366--372</pages>
<location>Madrid,</location>
<contexts>
<context position="4370" citStr="Wang and Waibel, 1997" startWordPosition="673" endWordPosition="676">stical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every English string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the English string that maximizes the product of the English language model and the string translation model . Many existing systems for statistical machine translation (Wang and Waibel, 1997; Nießen et al., 1998; Och and Weber, 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments which assign one target word position to each source word position. The lexicon probability of a certain English word is assumed to depend basically only on the source word aligned to it. The overall architecture of the statistical translation approach is depicted in Figure 1. In this figure we already anticipate the fact that we can transform t</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical translation. In Proceedings of the ACL/EACL ’97, Madrid, Spain, pages 366–372, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>