<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.049965">
<title confidence="0.99773">
Language Resources for a Network-based Dictionary
</title>
<author confidence="0.997237">
Veit Reuer
</author>
<affiliation confidence="0.916720666666667">
Institute of Cognitive Science
University of Osnabr¨uck
Germany
</affiliation>
<email confidence="0.989871">
vreuer@uos.de
</email>
<sectionHeader confidence="0.993637" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996675">
In order to facilitate the use of a dictionary for lan-
guage production and language learning we propose
the construction of a new network-based electronic
dictionary along the lines of Zock (2002). How-
ever, contrary to Zock who proposes just a paradig-
matic network with information about the various
ways in which words are similar we would like to
present several existing language resources (LRs)
which will be integrated in such a network result-
ing in more linguistic levels than one with paradig-
matically associated words. We argue that just as
the mental lexicon exhibits various, possibly inter-
woven layers of networks, electronic LRs contain-
ing syntagmatic, morphological and phonological
information need to be integrated into an associa-
tive electronic dictionary.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99974905882353">
Traditional dictionaries are mainly used for lan-
guage reception, even though they were also de-
veloped to be used for language production. How-
ever the form-based structure following ortho-
graphic conventions which could also be called
“one-dimensional”, makes it difficult to access the
information by meaning. Therefore the usage of
a traditional dictionary for text production is quite
limited as opposed to, for example, a thesaurus.
The main advantage of a thesaurus is the structuring
based on the semantic relation between words in an
entry. This allows for the availability of a different
type of information.
Therefore our proposal is to construct an elec-
tronic dictionary which has a network-like structure
and whose content is drawn from various existing
lexical resources. The dictionary will represent both
paradigmatic information - information about the
various ways in which words are similar - as well as
syntagmatic information - information about the re-
lationships among words that appear together. Ad-
ditionally information from other types of resources
such as morphology and phonology will be inte-
grated as they are also relevant in models of the
mental lexicon. In these models “associations” be-
tween words are based not only on meaning but also
on phonological or morphological properties of the
connected words. Following Brown and McNeill
(1966) and subsequent research people in the so-
called “tip-of-the-tongue”-state (TOT-state) are able
to clearly recall the properties of the missing word
such as the number of syllables or the meaning, and
can easily identify the target word when it is pre-
sented to them.
</bodyText>
<figureCaption confidence="0.954107333333333">
Figure 1: Example network with data from Word-
Net (–) and Deutscher Wortschatz (=)
Figure 1 exemplifies the visualization of a single
</figureCaption>
<bodyText confidence="0.862708071428571">
node with related information from two LRs1. Here
a user would be able to find the term “shortcake”
even if s/he only knows only one part, namely straw-
berries.2 A click on a neighbouring node should e.g.
re-center the structure and hence allow the user to
“explore” the network.
As mentioned above the most obvious usage
seems to be in language production where infor-
mation can be provided not only for words already
activated in the mind of the language producer but
also for alternatives, specifications or for words not
directly accessible because of a TOT-state. This
seems reasonable in light of the fact that speaker’s
passively vocabularies are known to be larger than
</bodyText>
<footnote confidence="0.9961942">
1http://www.cogsci.princeton.edu/˜wn
http://www.wortschatz.uni-leipzig.de
2LDOCE (1987) mentions “cream” and “jam” but not
“shortcake” as part of the entry for “strawberry”. The entry for
“shortcake” however lists specifically “strawberry shortcake”.
</footnote>
<figure confidence="0.974693068965517">
chocolate
birthmark
herb
berry
�
hyper
shortcake
hyper �
�� ��
yper
�����
h
cream
strawberry
_hypo
����
fruit
���
hypo
����
jam
thypo
wild strwb.
beach strwb.
garden strwb.
���������
������ ��
hypo????
Virginia Strwb.
</figure>
<bodyText confidence="0.99987459375">
their active vocabularies. The range of information
available of course depends on the material inte-
grated into the dictionary from the various resources
which are explored more closely below.
A second area of application of such a dictio-
nary is language learning. Apart from specify-
ing paradigmatic information which is usually also
part of the definition of a lemma, syntagmatic in-
formation representing collocations and cooccur-
rances is an important resource for language learn-
ers. Knowledge about collocations is a kind of lin-
guistic knowledge which is language-specific and
not systematically derivable making collocations es-
pecially difficult to learn.
Even though there are some studies that compare
the results from statistically computed association
measures with word association norms from psy-
cholinguistic experiments (Landauer et al., 1998;
Rapp, 2002) there has not been any research on
the usage of a digital, network-based dictionary re-
flecting the organisation of the mental lexicon to
our knowledge. Apart from studies using so called
Mind Maps or Concept Maps to visualize “world
knowledge”3 (Novak, 1998) nothing is known about
the psycholinguistic aspects which need to be con-
sidered for the construction of a network-based dic-
tionary.
In the following section we will summarize the
information made available by the various LRs we
plan to integrate into our system. The ideas pre-
sented here were developed in preparation of a
project at the University of Osnabr¨uck.
</bodyText>
<sectionHeader confidence="0.916732" genericHeader="method">
2 Language Resources
</sectionHeader>
<bodyText confidence="0.999919555555556">
Zock (2002) proposes the use of only one type of in-
formation structure in his network, namely a type of
semantic information. There are, however, a num-
ber of other types of information structures that may
also be relevant for a user. Psychological experi-
ments show that almost all levels of linguistic de-
scription reveal priming effects. Strong mental be-
tween words are based not only on a semantic rela-
tionship but also on morphological an phonological
relationships. These types of relationships should
also be included in a network based dictionary as
well.
A number of LRs that are suitable in this scenario
already provide some sort of network-like structure
possibly closely related to networks meaningful to
a human user. All areas are large research fields of
their own and we will therefore only touch upon a
few aspects.
</bodyText>
<footnote confidence="0.85435">
3The maps constitute a representation of the world rather
than reflecting the mental lexicon.
</footnote>
<subsectionHeader confidence="0.971225">
2.1 Manually Constructed Networks
</subsectionHeader>
<bodyText confidence="0.999990972972973">
Manually constructed networks usually consist of
paradigmatic information since words of the same
part-of-speech are related to each other. In ontolo-
gies usually only nouns are considered and are inte-
grated into these in order to structure the knowledge
to be covered.
The main advantage of such networks, since they
are hand-built, is the presumable correctness (if not
completeness) of the content. Additionally, these
semantic nets usually include typed relations be-
tween nodes, such as e.g. “hyperonymy” and “is a”
and therefore provides additional information for a
user. It is safe to rely on the structure of a network
coded by humans to a certain extend even if it has
certain disadvantages, too. For example networks
tend to be selective on the amount of data included,
i.e. sometimes only one restricted area of know-
ledge is covered. Furthermore they include basi-
cally only paradigmatic information with some ex-
ceptions. This however is only part of the greater
structure of lexical networks.
The most famous example is WordNet (Fellbaum,
1998) for English – which has been visualized al-
ready at http://www.visualthesaurus.com – and its
various sisters for other languages. It reflects a cer-
tain cognitive claim and was designed to be used
in computational tasks such as word sense disam-
biguation. Furthermore ontologies may be used as a
resource, because in ontologies usually single words
or NPs are used to label the nodes in the network.
An example is the “Universal Decimal Classifica-
tion”4 which was originally designed to classify all
printed and electronic publications in libraries with
the help of some 60,000 classes. However one can
also think of it as a knowledge representation sys-
tem as the information is coded in order to reflect
the knowledge about the topics covered.
</bodyText>
<subsectionHeader confidence="0.984356">
2.2 Automatically Generated Paradigmatic
Networks
</subsectionHeader>
<bodyText confidence="0.99926225">
A common approach to the automatic generation of
semantic networks is to use some form of the so
called vector-space-model in order to map semanti-
cally similar words closely together in vector space
if they occur in similar contexts in a corpus (Man-
ning and Sch¨utze, 1999). One example, Latent Se-
mantic Analysis (Landauer et al., 1998, LSA) has
been accepted as a model of the mental lexicon and
is even used by psycholinguists as a basis for the
categorization and evaluation of test-items. The re-
sults from this line of research seem to describe not
only relations between words but seem to provide
</bodyText>
<footnote confidence="0.939129">
4http://www.udcc.org
</footnote>
<bodyText confidence="0.999942310344828">
the basis for a network which could be integrated
into a network-based dictionary. A disadvantage of
LSA is the positioning of polysemous words at a
position between the two extremes, i.e. between the
two senses which makes the approach worthless for
polysemous words in the data.
There are several other approaches such as Ji
and Ploux (2003) and the already mentioned Rapp
(2002). Ji and Ploux also develop a statistics-based
method in order to determine so called “contex-
onyms”. This method allows one to determine dif-
ferent senses of a word as it connects to different
clusters for the various senses, which can be seen
as automatically derived SynSets as they are known
from WordNet. Furthermore her group developed a
visualization tool, that presents the results in a way
unseen before. Even though they claim to have de-
veloped an “organisation model” of the mental lex-
icon only the restricted class of paradigmatic rela-
tions shows up in their calculations.
Common to almost all the automatically derived
semantic networks is the problem of the unknown
relation between items as opposed to manually con-
structed networks. On the one hand a typed rela-
tion provides additional information for a user about
two connected nodes but on the other hand it seems
questionable if a known relation would really help
to actually infer the meaning of a connected node
(contrary to Zock (2002)).
</bodyText>
<subsectionHeader confidence="0.972146">
2.3 Automatically Generated Syntagmatic
Networks
</subsectionHeader>
<bodyText confidence="0.993050387096774">
Substantial parts of the mental lexicon probably
also consist of syntagmatic relations between words
which are even more important for the interpre-
tation of collocations.5 The automatic extraction
of collocations, i.e. syntagmatic relations between
words, from large corpora has been an area of inter-
est in recent years as it provides a basis for the au-
tomatic enrichment of electronic lexicons and also
dictionaries. Usually attempts have been made at
extracting verb-noun-, verb-PP- or adjective-noun-
combinations. Noteworthy are the works by Krenn
and Evert (2001) who have tried to compare the
different lexical association measures used for the
extraction of collocations. Even though most ap-
proaches are purely statistics-based and use little
linguistic information, there are a few cases where
a parser was applied in order to enhance the recog-
nition of collocations with the relevant words not
5We define collocations as a syntactically more or less fixed
combination of words where the meaning of one word is usu-
ally altered so that a compositional construction of the meaning
is prevented.
being next to each other (Seretan et al., 2003).
The data available from the collocation extrac-
tion research of course cannot be put together to
give a complete and comprehensive network. How-
ever certain examples such as the German project
“Deutscher Wortschatz”6 and the visualization tech-
nique used there suggest a network like structure
also in this area useful for example in the language
learning scenario as mentioned above.
</bodyText>
<subsectionHeader confidence="0.989499">
2.4 Phonological/Morphological Networks
</subsectionHeader>
<bodyText confidence="0.999812051282051">
Electronic lexica and rule systems for the phonolog-
ical representation of words can be used for spell-
checking as has been done e.g. in the Soundex ap-
proach (Mitton, 1996). In this approach a word not
contained in the lexicon is mapped onto a simpli-
fied and reduced phonological representation and
compared with the representations of words in the
lexicon. The correct words coming close to the
misspelled word on the basis of the comparison
are then chosen as possible correction candidates.
However this approach makes some drastic assump-
tions about the phonology of a language in order to
keep the system simple. With a more elaborate set
of rules describing the phonology of a language a
more complex analysis is possible which even al-
lows the determination of words that rhyme.7 Set-
ting a suitable threshold for some measure of simi-
larity a network should evolve with phonologically
similar words being connected with each other. A
related approach to spelling correction is the use of
so called “tries” for the efficient storage of lexical
data (Oflazer, 1996). The calculation of a minimal
editing distance between an unknown word and a
word in a trie determines a possible correct candi-
date.
Contrary to Zock (2002) who suggests this as an
analysis step on its own we think that the phonolo-
gical and morphological similarity can be exploited
to form yet another layer in a network-based dictio-
nary. Zock’s example of the looked-for “relegate”
may than be connected to “renegade” and “dele-
gate” via a single link and thus found easily. Here
again, probably only partial nets are created but they
may nevertheless help a user looking for a certain
word whose spelling s/he is not sure of.
Finally there are even more types of LRs contain-
ing network-like structures which may contribute
to a network-based dictionary. One example to be
mentioned here is the content of machine-readable
</bodyText>
<footnote confidence="0.9952438">
6Note however that the use of the term “Kollokation” in this
project is strictly based on statistics and has no relation to a
collocation in a linguistic sense (see figure 1).
7Dissertation project of Tobias Thelen: personal communi-
cation.
</footnote>
<bodyText confidence="0.999914">
dictionaries. The words in definitions contained in
the dictionary entries – especially for nouns – are
usually on the one hand semantically connected to
the lemma and on the other hand are mostly entries
themselves which again may provide data for a net-
work. In research in computational linguistics the
relation between the lemma and the definition has
been utilized especially for word sense disambigua-
tion tasks and for the automatic enrichment of lan-
guage processing systems (Ide and V´eronis, 1994).
</bodyText>
<sectionHeader confidence="0.984809" genericHeader="method">
3 Open Issues and Conclusion
</sectionHeader>
<bodyText confidence="0.985027547619048">
So far we have said nothing about two further im-
portant parts of such a dictionary: the representation
and the visualization of the data. There are a num-
ber of questions which still need to be answered in
order to build a comprehensive dictionary suitable
for an evaluation. With respect to the representation
two major questions seem to be the following.
• As statistical methods for the analysis of cor-
pora and for the extraction of frequent cooccur-
rance phenomena tend to use non-lemmatized
data, the question is whether it makes sense
to provide the user with the more specific data
based on inflected material.
• Secondly the question arises how to integrate
different senses of a word into the representa-
tion, if the data provides for this information
(as WordNet does).
With regard to visualization especially the dynamic
aspects of the presentation need to be considered.
There are various techniques that can be used to fo-
cus on parts of the network and suppress others in
order to make the network-based dictionary man-
ageable for a user which need to be evaluated in us-
ability studies. Among these are hyperbolic views
and so-called cone trees
As we have shown a number of LRs, espe-
cially those including syntagmatic, morphological
and phonological information, provide suitable data
to be included into a network-based dictionary. The
data in these LRs either correspond to the presumed
content of the mental lexicon or seem especially
suited for the intended usage. One major prop-
erty of the new type of dictionary proposed here
is the disintegration of the macro- and the micro-
structure of a traditional dictionary because parts
of the micro-structure (the definition of the entries)
become part of the macro-structure (primary links
to related nodes) of the new dictionary. Reflect-
ing the structure of the mental lexicon this dictio-
nary should allow new ways to access the lexical
data and support language production and language
learning.
</bodyText>
<sectionHeader confidence="0.990984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995915411764706">
R. Brown and D. McNeill. 1966. The ‘tip-of-the-
tongue’ phenomenon. Journal of Verbal Learn-
ing and Verbal Behavior, 5:325–337.
C. Fellbaum, editor. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
N. Ide and J. V´eronis. 1994. Machine readable dic-
tionaries: What have we learned, where do we go.
In Proceedings of the post-COLING94 interna-
tional workshop on directions of lexical research,
Beijing.
H. Ji and S. Ploux. 2003. A mental lexicon organi-
zation model. In Proceedings of the Joint Inter-
national Conference on Cognitive Science, pages
240–245, Sydney.
B. Krenn and S. Evert. 2001. Can we do bet-
ter than frequency? A case study on extract-
ing PP-verb collocations. In Proceedings of the
ACL-Workshop on Collocations, pages 39–46,
Toulouse.
T. K. Landauer, P. W. Foltz, and D. Laham. 1998.
An introduction to latent semantic analysis. Dis-
course Processes, 25:259–284.
LDOCE. 1987. Longman Dictionary of Contempo-
rary English. Langenscheidt, Berlin, 2. edition.
C. D. Manning and H. Sch¨utze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, MA.
R. Mitton. 1996. English Spelling and the Com-
puter. Longman, London.
J. D. Novak. 1998. Learning, Creating and Using
Knowledge: Concept Maps as Facilitative Tools
in Schools and Corporations. Erlbaum, Mahwah,
NJ.
K. Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analy-
sis and spelling correction. Computational Lin-
guistics, 22(1):73–89.
R. Rapp. 2002. The computation of word associa-
tions: Comparing syntagmatic and paradigmatic
approaches. In Proc. 19th Int. Conference on
Computational Linguistics (COLING), Taipeh.
V. Seretan, L. Nerima, and E. Wehrli. 2003. Extrac-
tion of multi-word collocations using syntactic
bigram composition. In Proceedings of the Inter-
national Conference on Recent Advances in NLP
(RANLP-2003), Borovets, Bulgaria.
M. Zock. 2002. Sorry, but what was your name
again, or, how to overcome the tip-of-the tongue
problem with the help of a computer? In Pro-
ceedings of the COLING-Workshop on Building
and Using Semantic Networks, Taipeh.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.843177">
<title confidence="0.997993">Language Resources for a Network-based Dictionary</title>
<author confidence="0.961469">Veit</author>
<affiliation confidence="0.999354">Institute of Cognitive University of</affiliation>
<email confidence="0.936229">vreuer@uos.de</email>
<abstract confidence="0.996345">In order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of Zock (2002). However, contrary to Zock who proposes just a paradigmatic network with information about the various ways in which words are similar we would like to present several existing language resources (LRs) which will be integrated in such a network resulting in more linguistic levels than one with paradigmatically associated words. We argue that just as the mental lexicon exhibits various, possibly interwoven layers of networks, electronic LRs containing syntagmatic, morphological and phonological information need to be integrated into an associative electronic dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Brown</author>
<author>D McNeill</author>
</authors>
<title>The ‘tip-of-thetongue’ phenomenon.</title>
<date>1966</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<pages>5--325</pages>
<contexts>
<context position="2303" citStr="Brown and McNeill (1966)" startWordPosition="350" endWordPosition="353">om various existing lexical resources. The dictionary will represent both paradigmatic information - information about the various ways in which words are similar - as well as syntagmatic information - information about the relationships among words that appear together. Additionally information from other types of resources such as morphology and phonology will be integrated as they are also relevant in models of the mental lexicon. In these models “associations” between words are based not only on meaning but also on phonological or morphological properties of the connected words. Following Brown and McNeill (1966) and subsequent research people in the socalled “tip-of-the-tongue”-state (TOT-state) are able to clearly recall the properties of the missing word such as the number of syllables or the meaning, and can easily identify the target word when it is presented to them. Figure 1: Example network with data from WordNet (–) and Deutscher Wortschatz (=) Figure 1 exemplifies the visualization of a single node with related information from two LRs1. Here a user would be able to find the term “shortcake” even if s/he only knows only one part, namely strawberries.2 A click on a neighbouring node should e.</context>
</contexts>
<marker>Brown, McNeill, 1966</marker>
<rawString>R. Brown and D. McNeill. 1966. The ‘tip-of-thetongue’ phenomenon. Journal of Verbal Learning and Verbal Behavior, 5:325–337.</rawString>
</citation>
<citation valid="true">
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: an electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J V´eronis</author>
</authors>
<title>Machine readable dictionaries: What have we learned, where do we go.</title>
<date>1994</date>
<booktitle>In Proceedings of the post-COLING94 international workshop on directions of lexical research,</booktitle>
<location>Beijing.</location>
<marker>Ide, V´eronis, 1994</marker>
<rawString>N. Ide and J. V´eronis. 1994. Machine readable dictionaries: What have we learned, where do we go. In Proceedings of the post-COLING94 international workshop on directions of lexical research, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>S Ploux</author>
</authors>
<title>A mental lexicon organization model.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint International Conference on Cognitive Science,</booktitle>
<pages>240--245</pages>
<location>Sydney.</location>
<contexts>
<context position="9129" citStr="Ji and Ploux (2003)" startWordPosition="1441" endWordPosition="1444">een accepted as a model of the mental lexicon and is even used by psycholinguists as a basis for the categorization and evaluation of test-items. The results from this line of research seem to describe not only relations between words but seem to provide 4http://www.udcc.org the basis for a network which could be integrated into a network-based dictionary. A disadvantage of LSA is the positioning of polysemous words at a position between the two extremes, i.e. between the two senses which makes the approach worthless for polysemous words in the data. There are several other approaches such as Ji and Ploux (2003) and the already mentioned Rapp (2002). Ji and Ploux also develop a statistics-based method in order to determine so called “contexonyms”. This method allows one to determine different senses of a word as it connects to different clusters for the various senses, which can be seen as automatically derived SynSets as they are known from WordNet. Furthermore her group developed a visualization tool, that presents the results in a way unseen before. Even though they claim to have developed an “organisation model” of the mental lexicon only the restricted class of paradigmatic relations shows up in</context>
</contexts>
<marker>Ji, Ploux, 2003</marker>
<rawString>H. Ji and S. Ploux. 2003. A mental lexicon organization model. In Proceedings of the Joint International Conference on Cognitive Science, pages 240–245, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Krenn</author>
<author>S Evert</author>
</authors>
<title>Can we do better than frequency? A case study on extracting PP-verb collocations.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL-Workshop on Collocations,</booktitle>
<pages>39--46</pages>
<location>Toulouse.</location>
<contexts>
<context position="10789" citStr="Krenn and Evert (2001)" startWordPosition="1705" endWordPosition="1708">002)). 2.3 Automatically Generated Syntagmatic Networks Substantial parts of the mental lexicon probably also consist of syntagmatic relations between words which are even more important for the interpretation of collocations.5 The automatic extraction of collocations, i.e. syntagmatic relations between words, from large corpora has been an area of interest in recent years as it provides a basis for the automatic enrichment of electronic lexicons and also dictionaries. Usually attempts have been made at extracting verb-noun-, verb-PP- or adjective-nouncombinations. Noteworthy are the works by Krenn and Evert (2001) who have tried to compare the different lexical association measures used for the extraction of collocations. Even though most approaches are purely statistics-based and use little linguistic information, there are a few cases where a parser was applied in order to enhance the recognition of collocations with the relevant words not 5We define collocations as a syntactically more or less fixed combination of words where the meaning of one word is usually altered so that a compositional construction of the meaning is prevented. being next to each other (Seretan et al., 2003). The data available</context>
</contexts>
<marker>Krenn, Evert, 2001</marker>
<rawString>B. Krenn and S. Evert. 2001. Can we do better than frequency? A case study on extracting PP-verb collocations. In Proceedings of the ACL-Workshop on Collocations, pages 39–46, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="4699" citStr="Landauer et al., 1998" startWordPosition="719" endWordPosition="722"> dictionary is language learning. Apart from specifying paradigmatic information which is usually also part of the definition of a lemma, syntagmatic information representing collocations and cooccurrances is an important resource for language learners. Knowledge about collocations is a kind of linguistic knowledge which is language-specific and not systematically derivable making collocations especially difficult to learn. Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments (Landauer et al., 1998; Rapp, 2002) there has not been any research on the usage of a digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge. Apart from studies using so called Mind Maps or Concept Maps to visualize “world knowledge”3 (Novak, 1998) nothing is known about the psycholinguistic aspects which need to be considered for the construction of a network-based dictionary. In the following section we will summarize the information made available by the various LRs we plan to integrate into our system. The ideas presented here were developed in preparation of a proj</context>
<context position="8498" citStr="Landauer et al., 1998" startWordPosition="1336" endWordPosition="1339">nd electronic publications in libraries with the help of some 60,000 classes. However one can also think of it as a knowledge representation system as the information is coded in order to reflect the knowledge about the topics covered. 2.2 Automatically Generated Paradigmatic Networks A common approach to the automatic generation of semantic networks is to use some form of the so called vector-space-model in order to map semantically similar words closely together in vector space if they occur in similar contexts in a corpus (Manning and Sch¨utze, 1999). One example, Latent Semantic Analysis (Landauer et al., 1998, LSA) has been accepted as a model of the mental lexicon and is even used by psycholinguists as a basis for the categorization and evaluation of test-items. The results from this line of research seem to describe not only relations between words but seem to provide 4http://www.udcc.org the basis for a network which could be integrated into a network-based dictionary. A disadvantage of LSA is the positioning of polysemous words at a position between the two extremes, i.e. between the two senses which makes the approach worthless for polysemous words in the data. There are several other approac</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDOCE</author>
</authors>
<title>Longman Dictionary of Contemporary English.</title>
<date>1987</date>
<volume>2</volume>
<pages>edition.</pages>
<location>Langenscheidt, Berlin,</location>
<contexts>
<context position="3465" citStr="LDOCE (1987)" startWordPosition="536" endWordPosition="537">berries.2 A click on a neighbouring node should e.g. re-center the structure and hence allow the user to “explore” the network. As mentioned above the most obvious usage seems to be in language production where information can be provided not only for words already activated in the mind of the language producer but also for alternatives, specifications or for words not directly accessible because of a TOT-state. This seems reasonable in light of the fact that speaker’s passively vocabularies are known to be larger than 1http://www.cogsci.princeton.edu/˜wn http://www.wortschatz.uni-leipzig.de 2LDOCE (1987) mentions “cream” and “jam” but not “shortcake” as part of the entry for “strawberry”. The entry for “shortcake” however lists specifically “strawberry shortcake”. chocolate birthmark herb berry � hyper shortcake hyper � �� �� yper ����� h cream strawberry _hypo ���� fruit ��� hypo ���� jam thypo wild strwb. beach strwb. garden strwb. ��������� ������ �� hypo???? Virginia Strwb. their active vocabularies. The range of information available of course depends on the material integrated into the dictionary from the various resources which are explored more closely below. A second area of applicat</context>
</contexts>
<marker>LDOCE, 1987</marker>
<rawString>LDOCE. 1987. Longman Dictionary of Contemporary English. Langenscheidt, Berlin, 2. edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>C. D. Manning and H. Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitton</author>
</authors>
<title>English Spelling and the Computer.</title>
<date>1996</date>
<location>Longman, London.</location>
<contexts>
<context position="11963" citStr="Mitton, 1996" startWordPosition="1895" endWordPosition="1896">retan et al., 2003). The data available from the collocation extraction research of course cannot be put together to give a complete and comprehensive network. However certain examples such as the German project “Deutscher Wortschatz”6 and the visualization technique used there suggest a network like structure also in this area useful for example in the language learning scenario as mentioned above. 2.4 Phonological/Morphological Networks Electronic lexica and rule systems for the phonological representation of words can be used for spellchecking as has been done e.g. in the Soundex approach (Mitton, 1996). In this approach a word not contained in the lexicon is mapped onto a simplified and reduced phonological representation and compared with the representations of words in the lexicon. The correct words coming close to the misspelled word on the basis of the comparison are then chosen as possible correction candidates. However this approach makes some drastic assumptions about the phonology of a language in order to keep the system simple. With a more elaborate set of rules describing the phonology of a language a more complex analysis is possible which even allows the determination of words </context>
</contexts>
<marker>Mitton, 1996</marker>
<rawString>R. Mitton. 1996. English Spelling and the Computer. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Novak</author>
</authors>
<title>Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations. Erlbaum,</title>
<date>1998</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="4971" citStr="Novak, 1998" startWordPosition="765" endWordPosition="766">s is a kind of linguistic knowledge which is language-specific and not systematically derivable making collocations especially difficult to learn. Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments (Landauer et al., 1998; Rapp, 2002) there has not been any research on the usage of a digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge. Apart from studies using so called Mind Maps or Concept Maps to visualize “world knowledge”3 (Novak, 1998) nothing is known about the psycholinguistic aspects which need to be considered for the construction of a network-based dictionary. In the following section we will summarize the information made available by the various LRs we plan to integrate into our system. The ideas presented here were developed in preparation of a project at the University of Osnabr¨uck. 2 Language Resources Zock (2002) proposes the use of only one type of information structure in his network, namely a type of semantic information. There are, however, a number of other types of information structures that may also be r</context>
</contexts>
<marker>Novak, 1998</marker>
<rawString>J. D. Novak. 1998. Learning, Creating and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations. Erlbaum, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="12858" citStr="Oflazer, 1996" startWordPosition="2043" endWordPosition="2044">hosen as possible correction candidates. However this approach makes some drastic assumptions about the phonology of a language in order to keep the system simple. With a more elaborate set of rules describing the phonology of a language a more complex analysis is possible which even allows the determination of words that rhyme.7 Setting a suitable threshold for some measure of similarity a network should evolve with phonologically similar words being connected with each other. A related approach to spelling correction is the use of so called “tries” for the efficient storage of lexical data (Oflazer, 1996). The calculation of a minimal editing distance between an unknown word and a word in a trie determines a possible correct candidate. Contrary to Zock (2002) who suggests this as an analysis step on its own we think that the phonological and morphological similarity can be exploited to form yet another layer in a network-based dictionary. Zock’s example of the looked-for “relegate” may than be connected to “renegade” and “delegate” via a single link and thus found easily. Here again, probably only partial nets are created but they may nevertheless help a user looking for a certain word whose s</context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>K. Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>The computation of word associations: Comparing syntagmatic and paradigmatic approaches.</title>
<date>2002</date>
<booktitle>In Proc. 19th Int. Conference on Computational Linguistics (COLING), Taipeh.</booktitle>
<contexts>
<context position="4712" citStr="Rapp, 2002" startWordPosition="723" endWordPosition="724"> learning. Apart from specifying paradigmatic information which is usually also part of the definition of a lemma, syntagmatic information representing collocations and cooccurrances is an important resource for language learners. Knowledge about collocations is a kind of linguistic knowledge which is language-specific and not systematically derivable making collocations especially difficult to learn. Even though there are some studies that compare the results from statistically computed association measures with word association norms from psycholinguistic experiments (Landauer et al., 1998; Rapp, 2002) there has not been any research on the usage of a digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge. Apart from studies using so called Mind Maps or Concept Maps to visualize “world knowledge”3 (Novak, 1998) nothing is known about the psycholinguistic aspects which need to be considered for the construction of a network-based dictionary. In the following section we will summarize the information made available by the various LRs we plan to integrate into our system. The ideas presented here were developed in preparation of a project at the Un</context>
<context position="9167" citStr="Rapp (2002)" startWordPosition="1449" endWordPosition="1450">and is even used by psycholinguists as a basis for the categorization and evaluation of test-items. The results from this line of research seem to describe not only relations between words but seem to provide 4http://www.udcc.org the basis for a network which could be integrated into a network-based dictionary. A disadvantage of LSA is the positioning of polysemous words at a position between the two extremes, i.e. between the two senses which makes the approach worthless for polysemous words in the data. There are several other approaches such as Ji and Ploux (2003) and the already mentioned Rapp (2002). Ji and Ploux also develop a statistics-based method in order to determine so called “contexonyms”. This method allows one to determine different senses of a word as it connects to different clusters for the various senses, which can be seen as automatically derived SynSets as they are known from WordNet. Furthermore her group developed a visualization tool, that presents the results in a way unseen before. Even though they claim to have developed an “organisation model” of the mental lexicon only the restricted class of paradigmatic relations shows up in their calculations. Common to almost </context>
</contexts>
<marker>Rapp, 2002</marker>
<rawString>R. Rapp. 2002. The computation of word associations: Comparing syntagmatic and paradigmatic approaches. In Proc. 19th Int. Conference on Computational Linguistics (COLING), Taipeh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Seretan</author>
<author>L Nerima</author>
<author>E Wehrli</author>
</authors>
<title>Extraction of multi-word collocations using syntactic bigram composition.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in NLP (RANLP-2003), Borovets,</booktitle>
<contexts>
<context position="11369" citStr="Seretan et al., 2003" startWordPosition="1800" endWordPosition="1803">y are the works by Krenn and Evert (2001) who have tried to compare the different lexical association measures used for the extraction of collocations. Even though most approaches are purely statistics-based and use little linguistic information, there are a few cases where a parser was applied in order to enhance the recognition of collocations with the relevant words not 5We define collocations as a syntactically more or less fixed combination of words where the meaning of one word is usually altered so that a compositional construction of the meaning is prevented. being next to each other (Seretan et al., 2003). The data available from the collocation extraction research of course cannot be put together to give a complete and comprehensive network. However certain examples such as the German project “Deutscher Wortschatz”6 and the visualization technique used there suggest a network like structure also in this area useful for example in the language learning scenario as mentioned above. 2.4 Phonological/Morphological Networks Electronic lexica and rule systems for the phonological representation of words can be used for spellchecking as has been done e.g. in the Soundex approach (Mitton, 1996). In t</context>
</contexts>
<marker>Seretan, Nerima, Wehrli, 2003</marker>
<rawString>V. Seretan, L. Nerima, and E. Wehrli. 2003. Extraction of multi-word collocations using syntactic bigram composition. In Proceedings of the International Conference on Recent Advances in NLP (RANLP-2003), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zock</author>
</authors>
<title>Sorry, but what was your name again, or, how to overcome the tip-of-the tongue problem with the help of a computer?</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING-Workshop on Building and Using Semantic Networks,</booktitle>
<location>Taipeh.</location>
<contexts>
<context position="5368" citStr="Zock (2002)" startWordPosition="830" endWordPosition="831"> of a digital, network-based dictionary reflecting the organisation of the mental lexicon to our knowledge. Apart from studies using so called Mind Maps or Concept Maps to visualize “world knowledge”3 (Novak, 1998) nothing is known about the psycholinguistic aspects which need to be considered for the construction of a network-based dictionary. In the following section we will summarize the information made available by the various LRs we plan to integrate into our system. The ideas presented here were developed in preparation of a project at the University of Osnabr¨uck. 2 Language Resources Zock (2002) proposes the use of only one type of information structure in his network, namely a type of semantic information. There are, however, a number of other types of information structures that may also be relevant for a user. Psychological experiments show that almost all levels of linguistic description reveal priming effects. Strong mental between words are based not only on a semantic relationship but also on morphological an phonological relationships. These types of relationships should also be included in a network based dictionary as well. A number of LRs that are suitable in this scenario</context>
<context position="10171" citStr="Zock (2002)" startWordPosition="1617" endWordPosition="1618">way unseen before. Even though they claim to have developed an “organisation model” of the mental lexicon only the restricted class of paradigmatic relations shows up in their calculations. Common to almost all the automatically derived semantic networks is the problem of the unknown relation between items as opposed to manually constructed networks. On the one hand a typed relation provides additional information for a user about two connected nodes but on the other hand it seems questionable if a known relation would really help to actually infer the meaning of a connected node (contrary to Zock (2002)). 2.3 Automatically Generated Syntagmatic Networks Substantial parts of the mental lexicon probably also consist of syntagmatic relations between words which are even more important for the interpretation of collocations.5 The automatic extraction of collocations, i.e. syntagmatic relations between words, from large corpora has been an area of interest in recent years as it provides a basis for the automatic enrichment of electronic lexicons and also dictionaries. Usually attempts have been made at extracting verb-noun-, verb-PP- or adjective-nouncombinations. Noteworthy are the works by Kren</context>
<context position="13015" citStr="Zock (2002)" startWordPosition="2070" endWordPosition="2071">le. With a more elaborate set of rules describing the phonology of a language a more complex analysis is possible which even allows the determination of words that rhyme.7 Setting a suitable threshold for some measure of similarity a network should evolve with phonologically similar words being connected with each other. A related approach to spelling correction is the use of so called “tries” for the efficient storage of lexical data (Oflazer, 1996). The calculation of a minimal editing distance between an unknown word and a word in a trie determines a possible correct candidate. Contrary to Zock (2002) who suggests this as an analysis step on its own we think that the phonological and morphological similarity can be exploited to form yet another layer in a network-based dictionary. Zock’s example of the looked-for “relegate” may than be connected to “renegade” and “delegate” via a single link and thus found easily. Here again, probably only partial nets are created but they may nevertheless help a user looking for a certain word whose spelling s/he is not sure of. Finally there are even more types of LRs containing network-like structures which may contribute to a network-based dictionary. </context>
</contexts>
<marker>Zock, 2002</marker>
<rawString>M. Zock. 2002. Sorry, but what was your name again, or, how to overcome the tip-of-the tongue problem with the help of a computer? In Proceedings of the COLING-Workshop on Building and Using Semantic Networks, Taipeh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>