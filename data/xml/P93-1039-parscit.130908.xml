<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018022">
<title confidence="0.617321">
RESPONDING TO USER QUERIES lN A COLLABORATIVE ENVIRONMENT*
</title>
<author confidence="0.870688">
Jennifer Chu
</author>
<affiliation confidence="0.944863">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.503292">
Newark, DE 19716, USA
</address>
<email confidence="0.978779">
Internet: jchu@cis.udel.edu
</email>
<sectionHeader confidence="0.993128" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991111111111">
We propose a plan-based approach for responding
to user queries in a collaborative environment. We
argue that in such an environment, the system should
not accept the user&apos;s query automatically, but should
consider it a proposal open for negotiation. In this pa-
per we concentrate on cases in which the system and
user disagree, and discuss how this disagreement can
be detected, negotiated, and how final modifications
should be made to the existing plan.
</bodyText>
<sectionHeader confidence="0.999345" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881641025641">
In task-oriented consultation dialogues, the user and ex-
pert jointly construct a plan for achieving the user&apos;s goal.
In such an environment, it is important that the agents
agree on the domain plan being constructed and on the
problem-solving actions being taken to develop it. This
suggests that the participants communicate their disagree-
ments when they arise lest the agents work on developing
different plans. We are extending the dialogue under-
standing system in [6] to include a system that responds
to the user&apos;s utterances in a collaborative manner.
Each utterance by a participant constitutes a proposal
intended to affect the agents&apos; shared plan. One component
of our architecture, the evaluator, examines the user&apos;s pro-
posal and decides whether to accept or reject it. Since the
user has knowledge about his/her particular circumstances
and preferences that influence the domain plan and how
it is constructed, the evaluator must be a reactive planner
that interacts with the user to obtain information used
in building the evaluation meta-plan. Depending on the
evaluation, the system can accept or reject the proposal, or
suggest what it considers to be a better alternative, leading
to an embedded negotiation subdialogue.
In addition to the evaluator, our architecture consists of
a goal selector, an intentional planner, and a discourse
realizer. The goal selector, based on the result of the
evaluation and the current dialogue model, selects an
appropriate intentional goal for the system to pursue. The
intentional planner builds a plan to achieve the intentional
goal, and the discourse realizer generates utterances to
convey information based on the intentional plan.
This paper describes the evaluator, concentrating on
cases in which the system and user disagree. We show how
the system determines that the user&apos;s proposed additions
are erroneous and, instead of directly responding to the
user&apos;s utterances, conveys the disagreement. Thus, our
work contributes to an overall dialogue system by 1)
extending the model in [6] to eliminate the assumption that
the system will automatically answer the user&apos;s questions
or follow the user&apos;s proposals, and 2) capturing the notion
</bodyText>
<subsectionHeader confidence="0.7886075">
*This material is based upon work supported by the National
Science Foundation under Grant No. ERI-9122026.
</subsectionHeader>
<bodyText confidence="0.999359">
of cooperative responses within an overall collaborative
framework that allows for negotiation.
</bodyText>
<sectionHeader confidence="0.970193" genericHeader="method">
2 The Tripartite Model
</sectionHeader>
<bodyText confidence="0.998819848484848">
Lambert and Carberry proposed a plan-based tripartite
model of expert/novice consultation dialogue which in-
cludes a domain level, a problem-solving level, and a
discourse level [6]. The domain level represents the sys-
tern&apos;s beliefs about the user&apos;s plan for achieving some
goal in the application domain. The problem-solving
level encodes the system&apos;s beliefs about how both agents
are going about constructing the domain plan. The dis-
course level represents the system&apos;s beliefs about both
agents&apos; communicative actions. Lambert developed a
plan recognition algorithm that uses contextual knowl-
edge, world knowledge, linguistic clues, and a library
of generic recipes for actions to analyze utterances and
construct a dialogue model [6].
Lambert&apos;s system automatically adds to the dialogue
model all actions inferred from an utterance. However,
we argue that in a collaborative environment, the system
should only accept the proposed additions if the system
believes that they are appropriate. Hence, we separate
the dialogue model into an existing dialogue model and a
proposed model, where the former constitutes the shared
plan agreed upon by both agents, and the latter the newly
proposed actions that have not yet been confirmed.
Suppose earlier dialogue suggests that the user has
the goal of getting a Master&apos;s degree in CS (Get-
Masters(U,CS)). Figure I illustrates the dialogue model
that would be built after the following utterances by Lam-
bert&apos;s plan recognition algorithm modified to accommo-
date the separation of the existing and proposed dialogue
models, and augmented with a relaxation algorithm to
recognize ill-formed plans [2].
U: I want to satisfy my seminar course requirement.
Who&apos;s teaching CS689?
</bodyText>
<sectionHeader confidence="0.973033" genericHeader="method">
3 The Evaluator
</sectionHeader>
<bodyText confidence="0.999984666666667">
A collaborative system should only incorporate proposed
actions into an existing plan if they are considered appro-
priate. This decision is made by the evaluator, which will
be discussed in this section. This paper only considers
cases in which the user&apos;s proposal contains an infeasible
action (one that cannot be performed) or would result in
an ill-formed plan (one whose actions do not contribute
to one another as intended)[9].
We argue that the evaluator, in order to check for
erroneous plans/goals, only needs to examine actions in
the proposed model, since actions in the existing model
would have been checked when they were proposed.
When a chain of actions is proposed, the evaluator starts
examining from the top-most action so that the most
general action that is inappropriate will be addressed.
</bodyText>
<page confidence="0.969752">
280
</page>
<figure confidence="0.9877715">
Domain L,evel
I want to satisfy my seminar course requirements. Who&apos;s teaching CS689?
</figure>
<figureCaption confidence="0.999997">
Figure 1: The Structure of the User&apos;s Utterances
</figureCaption>
<bodyText confidence="0.99998734375">
The evaluator checks whether the existing and proposed
actions together constitute a well-formed plan, one in
which the children of each action contribute to their parent
action. Therefore, for each pair of actions, the evaluator
checks against its recipe library to determine if their
parent-child relationship holds. The evaluator also checks
whether each additional action is feasible by examining
whether its applicability conditions are satisfied and its
preconditionsl can be satisfied.
We contend that well-formedness should be checked
before feasibility since the feasibility of an action that does
not contribute to its parent action is irrelevant. Similarly,
the well-formedness of a plan that attempts to achieve an
infeasible goal is also irrelevant. Therefore, we argue that
the processes of checking well-formedness and feasibility
should be interleaved in order to address the most general
action that is inappropriate. We show how this interleaved
process works by referring back to figure 1.
Suppose the system believes that CS689 is not a sem-
inar course. The evaluation process starts from Satisfy-
Seminar-Course(U,CS), the top-most action in the pro-
posed domain model. The system&apos;s knowledge indi-
cates that Satisfy-Seminar-Course(U,CS) contributes to
Get-Masters(U,CS). The system also believes that the
applicability conditions and the preconditions for the
Satisfy-Seminar-Course domain plan are satisfied, indi-
cating that the action is feasible. However, the sys-
tem&apos;s recipe library gives no reason to believe that
Take-Course(U,CS689) contributes to Satisfy-Seminar-
Course(U,CS), since CS689 is not a seminar course. The
evaluator then decides that this pair of proposed actions
would make the domain plan ill-formed.
</bodyText>
<sectionHeader confidence="0.971574" genericHeader="method">
4 When the Proposal is Erroneous
</sectionHeader>
<bodyText confidence="0.862595375">
The goal selector&apos;s task is to determine, based on the
current dialogue model, an intentional goal [8] that is
most appropriate for the system to pursue. An intentional
goal could be to directly respond to the user&apos;s utterance,
&apos;Both applicability conditions and preconditions are prereq-
uisites for executing a recipe. However, it is unreasonable to
attempt to satisfy an applicability condition whereas precondi-
tions can be planned for.
</bodyText>
<figure confidence="0.640929611111111">
Correct-Inference(..s1,_s2,_proposed)
Decomposition
believe(_s1, —contributes(_actl ,_act2))
believe(_s2, contributes(_actl,_act2))
in-plan(_actl,_proposed) V
in-plan(_act2,_proposed)
Modify-Acts(_s1,_s2,_proposed,_actl,_act2)
Insert-Correction(_s1,_s2,_proposed)
modified(_proposed)
well-formed(_proposed)
Modify-Acts(_s1,_s2,_proposed,_actl,_act2)
Specialization
believe(_sl, —contributes(_actl ,_act2))
believe(_s2, —,contributes(_actl ,_act2))
Remove-Act(_s1,32,_proposed,_actl)
Alter-Act(_s1,_s2,_proposed,_actl)
modified(_proposed)
modified(_proposed)
</figure>
<figureCaption confidence="0.999268">
Figure 2: Two Problem-Solving Recipes
</figureCaption>
<bodyText confidence="0.999544951219512">
to correct a user&apos;s misconception, to provide a better
alternative, etc. In this paper we only discuss the goal
selector&apos;s task when the user has an erroneous plan/goal.
In a collaborative environment, if the system decides
that the proposed model is infeasible/ill-formed, it should
refuse to accept the additions and suggest modifications
to the proposal by entering a negotiation subdialogue. For
this purpose, we developed recipes for two problem-
solving actions, Correct-Goal and Correct-Inference,
each a specialization of a Modify-Proposal action. We
illustrate the Correct-Inference action in more detail.
We show two problem-solving recipes, Correct-
Inference and Modify-Acts, in figure 2. The Correct-
Inference recipe is applicable when _s2 believes that
_actl contributes to achieving _act2, while ..sl believes
that such a relationship does not hold. The goal is
to make the resultant plan a well-formed plan; there-
fore, its body consists of an action Modify-Acts that
deletes the problematic components of the plan, and
Insert-Correction, that inserts new actions/variables into
the plan. One precondition in Modify-Acts is be-
lieve(_s2, —,contributes( _actl,_act2)) (note that in Correct-
Inference, _s2 believes contributes(_actl ,_act2)), and the
change in _s2&apos;s belief can be accomplished by invoking
the discourse level action Inform so that .s1 can convey
the ill-formedness to _s2. This Inform act may lead to fur-
ther negotiation about whether _actl contributes to _act2.
Only when _sl receives a positive feedback from _s2,
indicating that _s2 accepts _s 1 &apos; s belief, can ..sl assume
that the proposed actions can be modified.
Earlier discussion shows that the proposed actions in
figure 1 would make the domain plan ill-formed. There-
fore, the goal selector posts a goal to modify the proposal,
which causes the Correct-Inference recipe in figure 2 to be
selected. The variables _actl and _act2 are bound to Take-
Course(U,CS689) and Satisfy-Seminar-Course(U,CS), re-
spectively, since the system believes that the former does
not contribute to the latter.
Figure 3 shows how we envision the planner to expand
on the Correct-Inference recipe, which results in the
generation of the following two utterances:
</bodyText>
<footnote confidence="0.527192333333333">
(1)S: Taking CS689 does not contribute to satisfying
the seminar course requirement,
(2) CS689 is not a seminar course.
</footnote>
<figure confidence="0.99503853125">
IGet-Masters(U,CS) I
Proposed Domain Actions
n.tonfy Seminar-Course ,C
1
ITake-Course(U,CS689) 1.--1 .......
IBuild-Plan(U,S,Get-MastersO-LCSD I :: Planed Problem-Solviag ACti0116
........ Plan(U,S,Satidy-Sennnar-Course(U,CS)) I--
......, ...........
4
II
ItimId-Plan(U,S,Talce-Comse(U.CS689)) I
lInstantate-Single-Var(U,S,_fac,Teaches(_fac,CS689)) I
s-
Problem-Solving Level
Proposed Discourse Actions
Obtain Info Ref(U S fac,Tesches(_fac,CS689))
lAsk-Re (U,S ac,Teaches(_fu,CS689))
Make-Q-Acceptable(U,S,Teaches(_facgS689))
Cnve-Background(U,S,Teaches(_fac,CS689))
Inform(U,S,5vant(U,Satisfy-Seminar-CotuneLD,CSA I
Tell(U,Savant(U,Salisfy-Seminar-Course(0 CS))) Ref ftNmaN,sJ u two. cunn
4
Surface-Say -Prop(U,S,want(U,
Satisfy-Serninar-Coune(U,CS)))
Surface-WH-Q(U S, fac,T aches fac,C5689
Action:
Recipe-Type:
Appl Cond:
Constraints:
Body:
Effects:
Goal:
Action:
Recipe-Type:
Appl Cond:
Preconditions:
Body:
Effects:
Goal:
281
Problem-Solving Level Dialogue Model in Sgurc I
.1
Generate-Response(S U Proposed-Model)
Evaluate-Pro sal S U Pro..sed-Model
Mo -Pro sal S,U Pro osed-Model
ICorrect-Inference(S,U,Proposed-Model)
Modify-Acts(S,U,Proposed-Model,Take-Course(IJ,CS699),
Satisfy-;sminar-Course(ti CS))
Discourse Level
Infonu(S,U,-inference(Take-Course(U.CS689),
Satisfy,Seininar-Course(U,CS)))
Tell(S.U,-onferencegoke.Course(U,CS689),
Satisfy-Senunar-Course(U,CS)))
r-Surface-Say-Prop(S,U,-inference(
Take-Course(U.CS689),
Satisfv-Seminay-Course(U,CS)11
Address-Believability(S,U,-(ioference(
Take-Coune(U,CS689),
Salisfy-Seminar-Course(U,CS)))
Inform(S,1.1,-ma(CS689,senunar-course))
ITell(5,U,-isa(CS689xminar-course))
Surface-Say-Prop(S,U,-isa(CS689,seminan.course))
Taking CS689 does not contribute to mashing CS689 is not a seminar course
the seminar course requirement
</figure>
<figureCaption confidence="0.99999">
Figure 3: The Dialogue Model for the System&apos;s Response
</figureCaption>
<bodyText confidence="0.999955740740741">
The action Inform(_s1,_s2,_prop) has the goal be-
lieve( _s2,_prop); therefore, utterance (1) is generated by
executing the Inform action as an attempt to satisfy the
preconditions for the Modify-Acts recipe. Utterance (2)
results from the Address-Believability action, which is a
subaction of Inform, to support the claim in (1). The
problem-solving and discourse levels in figure 3 operate
on the entire dialogue model shown in figure 1, since
the evaluation process acts upon this model. Due to this
nature, the evaluation process can be viewed as a meta-
planning process, and when the goal of this process is
achieved, the modified dialogue model is returned to.
Now consider the case in which the user continues by
accepting utterances (1) and (2), which satisfies the pre-
condition of Modify-Acts. Modify-Acts has two special-
izations, Remove-Act, which removes the incorrect action
(and all of its children), and Alter-Act, which generalizes
the proposed action so that the plan will be well-formed.
Since Take-Course contributes to Satisfy-Seminar-Course
as long as the course is a seminar course, the system gen-
eralizes the user&apos;s proposed action by replacing CS689
with a variable. This variable may be instantiated by the
Insert-Correction subaction of Correct-Inference when
the dialogue continues. Note that our model accounts for
why the user&apos;s original question about the instructor of
CS689 is never answered —a conflict was detected that
made the question superfluous.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999951666666667">
Several researchers have studied collaboration [1, 3, 10]
and Allen proposed different plan modalities depending
on whether a plan fragment is shared, proposed and ac-
knowledged, or merely private [1]. However, they have
emphasized discourse analysis and none has provided a
plan-based framework for proposal negotiation, specified
appropriate system response during collaboration, or ac-
counted for why a question might never be answered.
Litman and Allen used discourse meta-plans to handle
a class of correction subdialogues [7]. However, their
Correct-Plan only addressed cases in which an agent adds
a repair step to a pre-existing plan that does not execute as
expected. Thus their meta-plans do not handle correction
of proposed additions to the dialogue model (since this
generally does not involve adding a step to the proposal).
Furthermore, they were only concerned with understand-
ing utterances, not with generating appropriate responses.
The work in [5, 11, 9] addressed generating cooperative
responses and responding to plan-based misconceptions,
but did not capture these within an overall collaborative
system that must negotiate proposals with the user. Hee-
man [4] used meta-plans to account for collaboration on
referring expressions. We have addressed collaboration in
constructing the user&apos;s task-related plan, captured cooper-
ative responses and negotiation of how the plan should be
constructed, and provided an accounting for why a user&apos;s
question may never be answered.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999687045454546">
We have presented a plan-based framework for generating
responses in a collaborative environment. Our framework
improves upon previous ones in that, 1) it captures co-
operative responses as a part of collaboration, 2) it is
capable of initiating negotiation subdialogues to deter-
mine what actions should be added to the shared plan,
3) the correction process, instead of merely pointing out
problematic plans/goals to the user, modifies the plan into
its most specific form accepted by both participants, and
4) the evaluation/correction process operates at a meta-
level which keeps the negotiation subdialogue separate
from the original dialogue model, while allowing the
same plan-inference mechanism to be used at both levels.
We intend to enhance our evaluator so that it also
recognizes sub-optimal solutions and can suggest bet-
ter alternatives. We will also study the goal selector&apos;s
task when the user&apos;s plan/goal is well-formed/feasible.
This includes identifying a set of intentional goals and
a strategy for the goal selector to choose amongst them.
Furthermore, we need to develop the intentional planner
which constructs a plan to achieve the posted goal, and a
discourse realizer to generate natural language text.
</bodyText>
<sectionHeader confidence="0.999351" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999747766666667">
[1] James Allen. Discourse structure in the TRAINS project.
In Darpa Speech and Natural Language Workshop, 1991.
[2] Rhonda Eller and Sandra Carberry. A meta-rule approach
to flexible plan recognition in dialogue. User Modeling
and User-Adapted Interaction, 2:27--53, 1992.
[3] Barbara Grosz and Candace Sidner. Plans for discourse. In
Cohen et al., editor, Intentions in Communication, pages
417--444. 1990.
[4] Peter Heeman. A computational model of collaboration
on referring expressions. Master&apos;s thesis, University of
Toronto, 1991.
Aravind Joshi, Bonnie Webber, and Ralph Weischedel.
Living up to expectations: Computing expert responses. In
Proc. AAAI, pages 169-175, 1984.
Lynn Lambert and Sandra Carberry. A tripartite plan-based
model of dialogue. In Proc. ACL, pages 47-54, 1991:
Diane Litman and James Allen. A plan recognition
model for subdialogues in conversation. Cognitive Sci-
ence, 11:163--200, 1987.
Johanna Moore and Cecile Paris. Planning text for advisory
dialogues. In Proc. ACL, pages 203--211, 1989.
Martha Pollack. A model of plan inference that distin-
guishes between the beliefs of actors and observers. In
Proc. ACL, pages 207--214, 1986.
[10] Candace Sidner. Using discourse to negotiate in collabo-
rative activity: An artificial language. In Workshop Notes:
AAAI-92 Cooperation Among Heterogeneous Intelligent
Systems, pages 121--128, 1992.
[11] Peter van -Beek. A model for generating better explanations.
In Proc. ACL, pages 215--220, 1987.
</reference>
<page confidence="0.997442">
282
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.977084">
<title confidence="0.995119">RESPONDING TO USER QUERIES lN A COLLABORATIVE ENVIRONMENT*</title>
<author confidence="0.999982">Jennifer Chu</author>
<affiliation confidence="0.9999105">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.998784">Newark, DE 19716, USA</address>
<email confidence="0.998686">Internet:jchu@cis.udel.edu</email>
<abstract confidence="0.998127">We propose a plan-based approach for responding to user queries in a collaborative environment. We argue that in such an environment, the system should not accept the user&apos;s query automatically, but should consider it a proposal open for negotiation. In this paper we concentrate on cases in which the system and user disagree, and discuss how this disagreement can be detected, negotiated, and how final modifications should be made to the existing plan.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Discourse structure in the TRAINS project.</title>
<date>1991</date>
<booktitle>In Darpa Speech and Natural Language Workshop,</booktitle>
<contexts>
<context position="14320" citStr="[1, 3, 10]" startWordPosition="1994" endWordPosition="1996"> the proposed action so that the plan will be well-formed. Since Take-Course contributes to Satisfy-Seminar-Course as long as the course is a seminar course, the system generalizes the user&apos;s proposed action by replacing CS689 with a variable. This variable may be instantiated by the Insert-Correction subaction of Correct-Inference when the dialogue continues. Note that our model accounts for why the user&apos;s original question about the instructor of CS689 is never answered —a conflict was detected that made the question superfluous. 5 Related Work Several researchers have studied collaboration [1, 3, 10] and Allen proposed different plan modalities depending on whether a plan fragment is shared, proposed and acknowledged, or merely private [1]. However, they have emphasized discourse analysis and none has provided a plan-based framework for proposal negotiation, specified appropriate system response during collaboration, or accounted for why a question might never be answered. Litman and Allen used discourse meta-plans to handle a class of correction subdialogues [7]. However, their Correct-Plan only addressed cases in which an agent adds a repair step to a pre-existing plan that does not exe</context>
</contexts>
<marker>[1]</marker>
<rawString>James Allen. Discourse structure in the TRAINS project. In Darpa Speech and Natural Language Workshop, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rhonda Eller</author>
<author>Sandra Carberry</author>
</authors>
<title>A meta-rule approach to flexible plan recognition in dialogue. User Modeling and User-Adapted Interaction,</title>
<date>1992</date>
<pages>2--27</pages>
<contexts>
<context position="4743" citStr="[2]" startWordPosition="725" endWordPosition="725"> existing dialogue model and a proposed model, where the former constitutes the shared plan agreed upon by both agents, and the latter the newly proposed actions that have not yet been confirmed. Suppose earlier dialogue suggests that the user has the goal of getting a Master&apos;s degree in CS (GetMasters(U,CS)). Figure I illustrates the dialogue model that would be built after the following utterances by Lambert&apos;s plan recognition algorithm modified to accommodate the separation of the existing and proposed dialogue models, and augmented with a relaxation algorithm to recognize ill-formed plans [2]. U: I want to satisfy my seminar course requirement. Who&apos;s teaching CS689? 3 The Evaluator A collaborative system should only incorporate proposed actions into an existing plan if they are considered appropriate. This decision is made by the evaluator, which will be discussed in this section. This paper only considers cases in which the user&apos;s proposal contains an infeasible action (one that cannot be performed) or would result in an ill-formed plan (one whose actions do not contribute to one another as intended)[9]. We argue that the evaluator, in order to check for erroneous plans/goals, on</context>
</contexts>
<marker>[2]</marker>
<rawString>Rhonda Eller and Sandra Carberry. A meta-rule approach to flexible plan recognition in dialogue. User Modeling and User-Adapted Interaction, 2:27--53, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Plans for discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>417--444</pages>
<editor>In Cohen et al., editor,</editor>
<contexts>
<context position="14320" citStr="[1, 3, 10]" startWordPosition="1994" endWordPosition="1996"> the proposed action so that the plan will be well-formed. Since Take-Course contributes to Satisfy-Seminar-Course as long as the course is a seminar course, the system generalizes the user&apos;s proposed action by replacing CS689 with a variable. This variable may be instantiated by the Insert-Correction subaction of Correct-Inference when the dialogue continues. Note that our model accounts for why the user&apos;s original question about the instructor of CS689 is never answered —a conflict was detected that made the question superfluous. 5 Related Work Several researchers have studied collaboration [1, 3, 10] and Allen proposed different plan modalities depending on whether a plan fragment is shared, proposed and acknowledged, or merely private [1]. However, they have emphasized discourse analysis and none has provided a plan-based framework for proposal negotiation, specified appropriate system response during collaboration, or accounted for why a question might never be answered. Litman and Allen used discourse meta-plans to handle a class of correction subdialogues [7]. However, their Correct-Plan only addressed cases in which an agent adds a repair step to a pre-existing plan that does not exe</context>
</contexts>
<marker>[3]</marker>
<rawString>Barbara Grosz and Candace Sidner. Plans for discourse. In Cohen et al., editor, Intentions in Communication, pages 417--444. 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Peter Heeman</author>
</authors>
<title>A computational model of collaboration on referring expressions. Master&apos;s thesis,</title>
<date>1991</date>
<journal>Cognitive Science,</journal>
<booktitle>In Proc. AAAI,</booktitle>
<volume>11</volume>
<pages>169--175</pages>
<publisher>Aravind</publisher>
<institution>University of Toronto,</institution>
<contexts>
<context position="15443" citStr="[4]" startWordPosition="2164" endWordPosition="2164">es in which an agent adds a repair step to a pre-existing plan that does not execute as expected. Thus their meta-plans do not handle correction of proposed additions to the dialogue model (since this generally does not involve adding a step to the proposal). Furthermore, they were only concerned with understanding utterances, not with generating appropriate responses. The work in [5, 11, 9] addressed generating cooperative responses and responding to plan-based misconceptions, but did not capture these within an overall collaborative system that must negotiate proposals with the user. Heeman [4] used meta-plans to account for collaboration on referring expressions. We have addressed collaboration in constructing the user&apos;s task-related plan, captured cooperative responses and negotiation of how the plan should be constructed, and provided an accounting for why a user&apos;s question may never be answered. 6 Conclusions and Future Work We have presented a plan-based framework for generating responses in a collaborative environment. Our framework improves upon previous ones in that, 1) it captures cooperative responses as a part of collaboration, 2) it is capable of initiating negotiation s</context>
</contexts>
<marker>[4]</marker>
<rawString>Peter Heeman. A computational model of collaboration on referring expressions. Master&apos;s thesis, University of Toronto, 1991. Aravind Joshi, Bonnie Webber, and Ralph Weischedel. Living up to expectations: Computing expert responses. In Proc. AAAI, pages 169-175, 1984. Lynn Lambert and Sandra Carberry. A tripartite plan-based model of dialogue. In Proc. ACL, pages 47-54, 1991: Diane Litman and James Allen. A plan recognition model for subdialogues in conversation. Cognitive Science, 11:163--200, 1987. Johanna Moore and Cecile Paris. Planning text for advisory dialogues. In Proc. ACL, pages 203--211, 1989. Martha Pollack. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proc. ACL, pages 207--214, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace Sidner</author>
</authors>
<title>Using discourse to negotiate in collaborative activity: An artificial language.</title>
<date>1992</date>
<booktitle>In Workshop Notes: AAAI-92 Cooperation Among Heterogeneous Intelligent Systems,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="14320" citStr="[1, 3, 10]" startWordPosition="1994" endWordPosition="1996"> the proposed action so that the plan will be well-formed. Since Take-Course contributes to Satisfy-Seminar-Course as long as the course is a seminar course, the system generalizes the user&apos;s proposed action by replacing CS689 with a variable. This variable may be instantiated by the Insert-Correction subaction of Correct-Inference when the dialogue continues. Note that our model accounts for why the user&apos;s original question about the instructor of CS689 is never answered —a conflict was detected that made the question superfluous. 5 Related Work Several researchers have studied collaboration [1, 3, 10] and Allen proposed different plan modalities depending on whether a plan fragment is shared, proposed and acknowledged, or merely private [1]. However, they have emphasized discourse analysis and none has provided a plan-based framework for proposal negotiation, specified appropriate system response during collaboration, or accounted for why a question might never be answered. Litman and Allen used discourse meta-plans to handle a class of correction subdialogues [7]. However, their Correct-Plan only addressed cases in which an agent adds a repair step to a pre-existing plan that does not exe</context>
</contexts>
<marker>[10]</marker>
<rawString>Candace Sidner. Using discourse to negotiate in collaborative activity: An artificial language. In Workshop Notes: AAAI-92 Cooperation Among Heterogeneous Intelligent Systems, pages 121--128, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter van -Beek</author>
</authors>
<title>A model for generating better explanations.</title>
<date>1987</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>215--220</pages>
<contexts>
<context position="15234" citStr="[5, 11, 9]" startWordPosition="2133" endWordPosition="2135">ring collaboration, or accounted for why a question might never be answered. Litman and Allen used discourse meta-plans to handle a class of correction subdialogues [7]. However, their Correct-Plan only addressed cases in which an agent adds a repair step to a pre-existing plan that does not execute as expected. Thus their meta-plans do not handle correction of proposed additions to the dialogue model (since this generally does not involve adding a step to the proposal). Furthermore, they were only concerned with understanding utterances, not with generating appropriate responses. The work in [5, 11, 9] addressed generating cooperative responses and responding to plan-based misconceptions, but did not capture these within an overall collaborative system that must negotiate proposals with the user. Heeman [4] used meta-plans to account for collaboration on referring expressions. We have addressed collaboration in constructing the user&apos;s task-related plan, captured cooperative responses and negotiation of how the plan should be constructed, and provided an accounting for why a user&apos;s question may never be answered. 6 Conclusions and Future Work We have presented a plan-based framework for gene</context>
</contexts>
<marker>[11]</marker>
<rawString>Peter van -Beek. A model for generating better explanations. In Proc. ACL, pages 215--220, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>