<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000403">
<title confidence="0.995714">
Robust Parsing of the Proposition Bank
</title>
<author confidence="0.983376">
Gabriele Musillo
</author>
<affiliation confidence="0.969802">
Depts of Linguistics and Computer Science
University of Geneva
</affiliation>
<address confidence="0.704170666666667">
2 Rue de Candolle
1211 Geneva 4
Switzerland
</address>
<email confidence="0.99223">
musillo4@etu.unige.ch
</email>
<author confidence="0.990144">
Paola Merlo
</author>
<affiliation confidence="0.9869585">
Department of Linguistics
University of Geneva
</affiliation>
<address confidence="0.710808">
2 Rue de Candolle
1211 Geneva 4
Switzerland
</address>
<email confidence="0.992947">
merlo@lettres.unige.ch
</email>
<sectionHeader confidence="0.995563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719818181818">
In this paper, we extend an existing statis-
tical parsing model to produce richer out-
put parse trees, annotated with PropBank
semantic role labels. Our results show
that the model can be robustly extended to
produce more complex output parse trees
without any loss in performance and sug-
gest that joint inference of syntactic and
semantic representations is a viable alter-
native to approaches based on a pipeline
of local processing steps.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999528551724138">
Recent successes in statistical syntactic parsing
based on supervised learning techniques trained
on a large corpus of syntactic trees (Collins, 1999;
Charniak, 2000; Henderson, 2003) have brought
forth the hope that the same approaches could be
applied to the more ambitious goal of recover-
ing the propositional content and the frame se-
mantics of a sentence. Moving towards a shal-
low semantic level of representation is a first ini-
tial step towards the distant goal of natural lan-
guage understanding and has immediate applica-
tions in question-answering and information ex-
traction. For example, an automatic flight reserva-
tion system processing the sentence I want to book
a flight from Geneva to Trento will need to know
that from Geneva denotes the origin of the flight
and to Trento denotes its destination. Knowing
that these two phrases are prepositional phrases,
the information provided by a syntactic parser, is
only moderately useful.
The growing interest in learning deeper infor-
mation is to a large extent supported and due to
the recent development of semantically annotated
databases such as FrameNet (Baker et al., 1998)
or the Proposition Bank (Palmer et al., 2005), that
can be used as training resources for a number of
supervised learning paradigms. We focus here on
the Proposition Bank (PropBank). PropBank en-
codes propositional information by adding a layer
of argument structure annotation to the syntactic
structures of the Penn Treebank (Marcus et al.,
1993). Verbal predicates in the Penn Treebank
(PTB) receive a label REL and their arguments
are annotated with abstract semantic role labels
A0-A5 or AA for those complements of the pred-
icative verb that are considered arguments while
those complements of the verb labelled with a se-
mantic functional label in the original PTB receive
the composite semantic role label AM-X, where
X stands for labels such as LOC, TMP or ADV,
for locative, temporal and adverbial modifiers re-
spectively. A tree structure with PropBank labels
for a sentence from the PTB (section 00) is shown
in Figure 1 below. PropBank uses two levels of
granularity in its annotation, at least conceptually.
Arguments receiving labels A0-A5 or AA do not
express consistent semantic roles and are specific
to a verb, while arguments receiving an AM-X la-
bel are supposed to be adjuncts and the respective
roles they express are consistent across all verbs.1
Recent approaches to learning semantic role la-
bels are based on two-stage architectures. The first
stage selects the elements to be labelled, while the
second determines the labels to be assigned to the
selected elements. While some of these models
are based on full parse trees (Gildea and Jurafsky,
2002; Gildea and Palmer, 2002), other methods
have been proposed that eschew the need for a full
</bodyText>
<footnote confidence="0.887788">
1There are thirteen semantic role labels for modifiers. See
(Palmer et al., 2005) for a detailed discussion of PropBank
semantic roles labels.
</footnote>
<page confidence="0.995796">
11
</page>
<note confidence="0.458745">
S
</note>
<equation confidence="0.921697333333333">
❍
✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍
NP-A1 VP
P
✏✏✏✏✏✏✏ PP ❍
P ✟✟✟✟✟✟✟✟
P P ❍
✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘
P ❍
❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳
❍
❍
</equation>
<figure confidence="0.989294064516129">
the government’s borrowing authority ❍ ❍ ❍
IN
NP
IN
NP
NN
at
QP
✏✏✏ P
PP
$ 2.80 trillion
from
midnight
PP-A3
✟✟❍❍
NP
QP
✏✏✏ P
PP
$ 2.87 trillion
PP-AM-TMP
✟✟❍❍
PP-A4
✟✟❍❍
TO
to
VBD-REL
dropped
NP-AM-TMP
NNP
Tuesday
</figure>
<figureCaption confidence="0.999979">
Figure 1: A sample syntactic structure from the PropBank with semantic role annotations.
</figureCaption>
<bodyText confidence="0.999947642857143">
parse (CoNNL, 2004; CoNLL, 2005). Because of
the way the problem has been formulated – as a
pipeline of parsing (or chunking) feeding into la-
belling – specific investigations of integrated ap-
proaches that solve both the parsing and the se-
mantic role labelling problems at the same time
have not been studied.
We present work to test the hypothesis that a
current statistical parser (Henderson, 2003) can
output richer information robustly, that is with-
out any significant degradation of the parser’s ac-
curacy on the original parsing task, by explicitly
modelling semantic role labels as the interface be-
tween syntax and semantics.
We achieve promising results both on the simple
parsing task, where the accuracy of the parser is
measured on the standard Parseval measures, and
also on the parsing task where the more complex
labels of PropBank are taken into account. We will
call the former task Penn Treebank parsing (PTB
parsing) and the latter task PropBank parsing be-
low.
These results have several consequences. On
the one hand, we show that it is possible to build a
single integrated robust system successfully. This
is a meaningful achievement, as a task combining
semantic role labelling and parsing is more com-
plex than simple syntactic parsing. While the shal-
low semantics of a constituent and its structural
position are often correlated, they sometimes di-
verge. For example, some nominal temporal mod-
ifiers occupy an object position without being ob-
jects, like Tuesday in Figure 1 below. On the other
hand, our results indicate that the proposed mod-
els are robust. To model our task accurately, ad-
ditional parameters must be estimated. However,
given the current limited availability of annotated
treebanks, this more complex task will have to be
solved with the same overall amount of data, ag-
gravating the difficulty of estimating the model’s
parameters due to sparse data. The limited avail-
ability of data is increased further by the high vari-
ability of the argumental labels A0-A5 whose se-
mantics is specific to a given verb or a given verb
sense. Solving this more complex problem suc-
cessfully, then, indicates that the models used are
robust.
Finally, we achieve robustness without simpli-
fying the parsing architecture. Specifically, ro-
bustness is achieved without resorting to the stip-
ulation of strong independence assumptions to
compensate for the limited availability and high
variability of data. Consequently, such an achieve-
ment demonstrates not only that the robustness
of the parsing model, but also its scalability and
portability.
</bodyText>
<sectionHeader confidence="0.896147" genericHeader="method">
2 The Basic Parsing Architecture
</sectionHeader>
<bodyText confidence="0.999954916666667">
To achieve the complex task of assigning seman-
tic role labels while parsing, we use a family of
statistical parsers, the Simple Synchrony Network
(SSN) parsers (Henderson, 2003), which do not
make any explicit independence assumptions, and
are therefore likely to adapt without much modi-
fication to the current problem. This architecture
has shown state-of-the-art performance.
SSN parsers comprise two components, one
which estimates the parameters of a stochastic
model for syntactic trees, and one which searches
for the most probable syntactic tree given the
</bodyText>
<page confidence="0.993065">
12
</page>
<bodyText confidence="0.999766024390244">
parameter estimates. As with many other sta-
tistical parsers (Collins, 1999; Charniak, 2000),
SSN parsers use a history-based model of parsing.
Events in such a model are derivation moves. The
set of well-formed sequences of derivation moves
in this parser is defined by a Predictive LR push-
down automaton (Nederhof, 1994), which imple-
ments a form of left-corner parsing strategy. The
derivation moves include: projecting a constituent
with a specified label, attaching one constituent
to another, and shifting a tag-word pair onto the
pushdown stack.
Unlike standard history-based models, SSN
parsers do not state any explicit independence as-
sumptions between derivation steps. They use a
neural network architecture, called Simple Syn-
chrony Network (Henderson and Lane, 1998), to
induce a finite history representation of an un-
bounded sequence of moves. The history repre-
sentation of a parse history d1, ... , di−1, which
we denote h(d1, ... , di−1), is assigned to the con-
stituent that is on the top of the stack before the ith
move.
The representation h(d1, ... , di−1) is computed
from a set f of features of the derivation move
di−1 and from a finite set D of recent history rep-
resentations h(d1, ... , dj), where j &lt; i − 1. Be-
cause the history representation computed for the
move i − 1 is included in the inputs to the com-
putation of the representation for the next move
i, virtually any information about the derivation
history could flow from history representation to
history representation and be used to estimate the
probability of a derivation move. However, the re-
cency preference exhibited by recursively defined
neural networks biases learning towards informa-
tion which flows through fewer history represen-
tations. (Henderson, 2003) exploits this bias by
directly inputting information which is considered
relevant at a given step to the history representa-
tion of the constituent on the top of the stack be-
fore that step. In addition to history representa-
tions, the inputs to h(d1, ... , di−1) include hand-
crafted features of the derivation history that are
meant to be relevant to the move to be chosen
at step i. For each of the experiments reported
here, the set D that is input to the computation of
the history representation of the derivation moves
d1, ... , di−1 includes the most recent history rep-
resentation of the following nodes: topi, the node
on top of the pushdown stack before the ith move;
the left-corner ancestor of topi (that is, the second
top-most node on the parser’s stack); the leftmost
child of topi; and the most recent child of topi, if
any. The set of features f includes the last move in
the derivation, the label or tag of topi, the tag-word
pair of the most recently shifted word, and the left-
most tag-word pair that topi dominates. Given the
hidden history representation h(d1, · · · , di−1) of a
derivation, a normalized exponential output func-
tion is computed by SSNs to estimate a probabil-
ity distribution over the possible next derivation
moves di.2
The second component of SSN parsers, which
searches for the best derivation given the pa-
rameter estimates, implements a severe pruning
strategy. Such pruning handles the high compu-
tational cost of computing probability estimates
with SSNs, and renders the search tractable. The
space of possible derivations is pruned in two dif-
ferent ways. The first pruning occurs immediately
after a tag-word pair has been pushed onto the
stack: only a fixed beam of the 100 best deriva-
tions ending in that tag-word pair are expanded.
For training, the width of such beam is set to five.
A second reduction of the search space prunes
the space of possible project or attach derivation
moves: a best-first search strategy is applied to the
five best alternative decisions only.
The next section describes our model, extended
to produce richer output parse trees annotated with
semantic role labels.
</bodyText>
<sectionHeader confidence="0.979803" genericHeader="method">
3 Learning Semantic Role Labels
</sectionHeader>
<bodyText confidence="0.999765">
Previous work on learning function labels during
parsing (Merlo and Musillo, 2005; Musillo and
Merlo, 2005) assumed that function labels repre-
sent the interface between lexical semantics and
syntax. We extend this hypothesis to the seman-
tic role labels assigned in PropBank, as they are
an exhaustive extension of function labels, which
have been reorganised in a coherent inventory of
labels and assigned exhaustively to all sentences in
the PTB. Because PropBank is built on the PTB, it
inherits in part its notion of function labels which
is directly integrated into the AM-X role labels.
A0-A5 or AA labels correspond to many of the
unlabelled elements in the PTB and also to those
elements that PTB annotators had classified as re-
</bodyText>
<tableCaption confidence="0.5801475">
2The on-line version of Backpropagation is used to train
SSN parsing models. It performs a gradient descent with
a maximum likelihood objective function and weight decay
regularization (Bishop, 1995).
</tableCaption>
<page confidence="0.982787">
13
</page>
<equation confidence="0.844960272727273">
S
❍
✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍
NP-A1 VP
P
✏✏✏✏✏✏✏ PP ❍
P ✟✟✟✟✟✟✟✟
P P ❍
✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘
P ❍
❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳
</equation>
<figure confidence="0.970318290322581">
❍
❍
the government’s borrowing authority ❍ ❍ ❍
VBD-REL
dropped
PP-AM-TMP
✟✟ ❍ ❍
IN(-AM-TMP)
at
midnight
NP-AM-TMP
NNP(-AM-TMP)
Tuesday
PP-A4
✟✟❍❍
TO
to QP
✏✏✏ P
PP
$ 2.80 trillion
PP-A3
✟✟❍❍
NP
QP
✏✏✏ P
PP
$ 2.87 trillion
NP IN
from
NP
NN
</figure>
<figureCaption confidence="0.999706">
Figure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals.
</figureCaption>
<bodyText confidence="0.9998645">
ceiving a syntactic functional label such as SBJ
(subject) or DTV (dative).
Because they are projections of the lexical se-
mantics of the elements in the sentence, semantic
role labels are projected bottom-up, they tend to
appear low in the tree and they are infrequently
found on the higher levels of the parse tree, where
projections of grammatical, as opposed to lexical,
elements usually reside. Because they are the in-
terface level with syntax, semantic labels are also
subject to distributional constraints that govern
syntactic dependencies, such as argument struc-
ture or subcategorization. We attempt to capture
such constraints by modelling the c-command re-
lation. Recall that the c-command relation relates
two nodes in a tree, even if they are not close to
each other, provided that the first node dominat-
ing one node also dominate the other. This notion
of c-command captures both linear and hierarchi-
cal constraints and defines the domain in which
semantic role labelling applies.
While PTB function labels appear to overlap to
a large extent with PropBank semantic rolel labels,
work by (Ye and Baldwin, 2005) on semantic la-
belling prepositional phrases, however, indicates
that the function labels in the Penn Treebank are
assigned more sporadically and heterogeneously
than in PropBank. Apparently only the “easy”
cases have been tagged functionally, because as-
signing these function tags was not the main goal
of the annotation. PropBank instead was anno-
tated exhaustively, taking all cases into account,
annotating multiple roles, coreferences and dis-
continuous constituents. It is therefore not void
of interest to test our hypothesis that, like function
labels, semantic role labels are the interface be-
tween syntax and semantics, and they need to be
recovered by applying constraints that model both
higher level nodes and lower level ones.
We assume that semantic roles are very often
projected by the lexical semantics of the words in
the sentence. We introduce this bottom-up lexical
information by fine-grained modelling of seman-
tic role labels. Extending a technique presented in
(Klein and Manning, 2003) and adopted in (Merlo
and Musillo, 2005; Musillo and Merlo, 2005) for
function labels, we split some part-of-speech tags
into tags marked with semantic role labels. The
semantic role labels attached to a non-terminal di-
rectly projected by a preterminal and belonging to
a few selected categories (DIR, EXT, LOC, MNR,
PNC, CAUS and TMP) were propagated down to
the pre-terminal part-of-speech tag of its head. To
affect only labels that are projections of lexical se-
mantics properties, the propagation takes into ac-
count the distance of the projection from the lex-
ical head to the label, and distances greater than
two are not included. Figure 2 illustrates the result
of this operation.
In our augmented model, inputs to each history
representation are selected according to a linguis-
tically motivated notion of structural locality over
which dependencies such as argument structure or
subcategorization could be specified.
In SSN parsing models, the set D of nodes that
are structurally local to a given node on top of the
stack defines the structural distance between this
given node and other nodes in the tree. Such a no-
tion of distance determines the number of history
representations through which information passes
</bodyText>
<page confidence="0.991534">
14
</page>
<figure confidence="0.871779">
C-COMMAND
</figure>
<figureCaption confidence="0.9916815">
Figure 3: Flow of information in original SSN parsers (dashed lines), enhanced by biases specific to
semantic role labels to capture the notion of c-command (solid lines).
</figureCaption>
<figure confidence="0.958801">
S
α Q
... 01 γ ... 6
E ... S 02 η ... θ
VP
</figure>
<bodyText confidence="0.999367769230769">
to flow from the representation of a node i to the
representation of a node j. By adding nodes to
the set D, one can shorten the structural distance
between two nodes and enlarge the locality do-
main over which dependencies can be specified.
To capture a locality domain appropriate for se-
mantic role parsing, we add the most recent child
of topi labelled with a semantic role label to the set
D. These additions yield a model that is sensitive
to regularities in structurally defined sequences
of nodes bearing semantic role labels, within and
across constituents. This modification of the bi-
ases is illustrated in Figure 3.
This figure displays two constituents, S and VP
with some of their respective child nodes. The VP
node is assumed to be on the top of the parser’s
stack, and the S one is supposed to be its left-
corner ancestor. The directed arcs represent the
information that flows from one node to another.
According to the original SSN model in (Hender-
son, 2003), only the information carried over by
the leftmost child and the most recent child of a
constituent directly flows to that constituent. In
the figure above, only the information conveyed
by the nodes α and 6 is directly input to the node
S. Similarly, the only bottom-up information di-
rectly input to the VP node is conveyed by the
child nodes E and θ. In the original SSN models,
nodes bearing a function label such as 01 and 02
are not directly input to their respective parents.
In our extended model, information conveyed by
01 and 02 directly flows to their respective par-
ents. So the distance between the nodes 01 and
02, which stand in a c-command relation, is short-
ened. For more information on this technique to
capture domains induced by the c-command rela-
tion, see (Musillo and Merlo, 2005).
We report the effects of these augmentations on
parsing results in the experiments described below.
</bodyText>
<sectionHeader confidence="0.999586" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999968">
Our extended semantic role SSN parser was
trained on sections 2-21 and validated on section
24 from the PropBank. Training, validating and
testing data sets consist of the PTB data anno-
tated with PropBank semantic roles labels, as pro-
vided in the CoNLL-2005 shared task (Carreras
and Marquez, 2005).
Our augmented model has a total 613 of non-
terminals to represents both the PTB and Prop-
Bank labels of constituents, instead of the 33 of
the original SSN parser. The 580 newly introduced
labels consist of a standard PTB label followed
by a set of one or more PropBank semantic role
such as PP-AM-TMP or NP-A0-A1. As a result
of lowering the six AM-X semantic role labels,
240 new part-of-speech tags were introduced to
partition the original tag set which consisted of 45
tags. SSN parsers do not tag their input sentences.
To provide the augmented model with tagged in-
put sentences, we trained an SVM tagger whose
features and parameters are described in detail in
(Gimenez and Marquez, 2004). Trained on section
2-21, the tagger reaches a performance of 95.45%
on the test set (section 23) using our new tag set.
As already mentioned, argumental labels A0-A5
are specific to a given verb or a given verb sense,
thus their distribution is highly variable. To re-
duce variability, we add some of the tag-verb pairs
licensing these argumental labels to the vocabu-
</bodyText>
<page confidence="0.994155">
15
</page>
<note confidence="0.8485385">
F R P
PropBank training and PropBank parsing task 82.3 82.1 82.4
PropBank training and PTB parsing task 88.8 88.6 88.9
PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9
</note>
<tableCaption confidence="0.985443">
Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks
</tableCaption>
<bodyText confidence="0.992670918918919">
and the original SSN parser.
lary of our model. We reach a total of 4970 tag-
word pairs.3 This vocabulary comprises the orig-
inal 512 pairs of the original SSN model, and our
added pairs which must occur at least 10 times in
the training data. Our vocabulary as well as the
new 240 POS tags and the new 580 non-terminal
labels are included in the set f of features input to
the history representations as described in section
2.
We perform two different evaluations on our
model trained on PropBank data. Recall that
we distinguish between two parsing tasks: the
PropBank parsing task and the PTB parsing task.
To evaluate the first parsing task, we compute
the standard Parseval measures of labelled recall
and precision of constituents, taking into account
not only the 33 original labels but also the 580
newly introduced PropBank labels. This evalua-
tion gives us an indication of how accurately and
exhaustively we can recover this richer set of non-
terminal labels. The results, computed on the test-
ing data set from the PropBank, are shown on the
first line of Table 1.
To evaluate the PTB task, we compute the la-
belled recall and precision of constituents, ignor-
ing the set of PropBank semantic role labels that
our model assigns to constituents. This evalua-
tion indicates how well we perform on the stan-
dard PTB parsing task alone, and its results on the
testing data set from the PTB are shown on the
second line of Table 1.
The third line of Table 1 gives the performance
on the simpler PTB parsing task of the original
SSN parser (Henderson, 2003), that was trained
on the PTB data sets contrary to our SSN model
trained on the PropBank data sets.
</bodyText>
<sectionHeader confidence="0.998675" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9909925">
These results clearly indicate that our model can
perform the PTB parsing task at levels of per-
</bodyText>
<subsectionHeader confidence="0.4545025">
3Such pairs consists of a tag and a word token. No attempt
at collecting word types was made.
</subsectionHeader>
<bodyText confidence="0.999883558139535">
formance comparable to state-of-the-art statistical
parsing, by extensions that take the nature of the
richer labels to be recovered into account. They
also suggest that the relationship between syntac-
tic PTB parsing and semantic PropBank parsing
is strict enough that an integrated approach to the
problem of semantic role labelling is beneficial.
In particular, recent models of semantic role la-
belling separate input indicators of the correlation
between the structural position in the tree and the
semantic label, such as path, from those indicators
that encode constraints on the sequence, such as
the previously assigned role (Kwon et al., 2004).
In this way, they can never encode directly the con-
straining power of a certain role in a given struc-
tural position onto a following node in its struc-
tural position. In our augmented model, we at-
tempt to capture these constraints by directly mod-
elling syntactic domains defined by the notion of
c-command.
Our results also confirm the findings in (Palmer
et al., 2005). They take a critical look at some
commonly used features in the semantic role la-
belling task, such as the path feature. They sug-
gest that the path feature is not very effective be-
cause it is sparse. Its sparseness is due to the oc-
currence of intermediate nodes that are not rele-
vant for the syntactic relations between an argu-
ment and its predicate. Our model of domains is
less noisy, and consequently more robust, because
it can focus only on c-commanding nodes bearing
semantic role labels, thus abstracting away from
those nodes that smear the pertinent relations.
(Yi and Palmer, 2005) share the motivation of
our work. Like the current work, they observe
that the distributions of semantic labels could po-
tentially interact with the distributions of syntactic
labels and redefine the boundaries of constituents,
thus yielding trees that reflect generalisations over
both these sources of information.
To our knowledge, no results have yet been pub-
lished on parsing the PropBank. Accordingly, it is
not possible to draw a straigthforward quantitative
</bodyText>
<page confidence="0.994899">
16
</page>
<table confidence="0.99856">
F R P
(Haghighi et al., 2005) 83.4 83.1 83.7
(Pradhan et al., 2005) 83.3 83.0 83.5
(Punyakanok et al., 2005) 83.1 82.8 83.3
(Marquez et al., 2005) 83.1 82.8 83.3
(Surdeanu and Turmo, 2005) 82.7 82.5 83.0
PropBank SSN 81.6 81.3 81.9
</table>
<tableCaption confidence="0.950589">
Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and state-
of-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank
validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).
</tableCaption>
<bodyText confidence="0.998413387096774">
comparison between our PropBank SSN parser
and other PropBank parsers. However, state-of-
the-art semantic role labelling systems (CoNLL,
2005) use parse trees output by state-of-the-art
parsers (Collins, 1999; Charniak, 2000), both for
training and testing, and return partial trees anno-
tated with semantic role labels. An indirect way
of comparing our parser with semantic role la-
bellers suggests itself. We merge the partial trees
output by a semantic role labeller with the output
of a parser it was trained on, and compute Prop-
Bank parsing performance measures on the result-
ing parse trees. The first five lines of Table 2 re-
port such measures for the five best semantic role
labelling systems (Haghighi et al., 2005; Pradhan
et al., 2005; Punyakanok et al., 2005; Marquez
et al., 2005; Surdeanu and Turmo, 2005) accord-
ing to (CoNLL, 2005). The partial trees output
by these systems were merged with the parse trees
returned by (Charniak, 2000)’s parser. These sys-
tems use (Charniak, 2000)’s parse trees both for
training and testing as well as various other infor-
mation sources including sets of n-best parse trees
(Punyakanok et al., 2005; Haghighi et al., 2005)
or chunks (Marquez et al., 2005; Pradhan et al.,
2005) and named entities (Surdeanu and Turmo,
2005). While our preliminary results indicated in
the last line of Table 2 are not state-of-the-art, they
do demonstrate the viability of SSN parsers for
joint inference of syntactic and semantic represen-
tations.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999757272727273">
In this paper, we have explored extensions to an
existing state-of-the-art parsing model. We have
achieved promising results on parsing the Propo-
sition Bank, showing that our extensions are suf-
ficiently robust to produce parse trees annotated
with shallow semantic information. Future work
will lie in extracting semantic role relations from
such richly annotated trees, for applications such
as information extraction or question answering.
In addition, further research will explore the rele-
vance of semantic role features to parse reranking.
</bodyText>
<sectionHeader confidence="0.996933" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99987425">
We thank the Swiss National Science Foundation
for supporting this research under grant number
101411-105286/1. We also thank James Hender-
son and Ivan Titov for allowing us to use and mod-
ify their SSN software, Xavier Carreras for pro-
viding the CoNLL-2005 shared task data sets and
the anonymous reviewers for their valuable com-
ments.
</bodyText>
<sectionHeader confidence="0.998583" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999409714285714">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the Thirty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics and Seven-
teenth International Conference on Computational
Linguistics (ACL-COLING’98), pages 86–90, Mon-
treal, Canada.
Christopher M. Bishop. 1995. Neural Networks for
Pattern Recognition. Oxford University Press, Ox-
ford, UK.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, Ann Arbor,
MI USA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Meeting
of North American Chapter of Association for Com-
putational Linguistics (NAACL’00), pages 132–139,
Seattle, Washington.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. the-
</reference>
<page confidence="0.987829">
17
</page>
<reference confidence="0.999820397849462">
sis, Department of Computer Science, University of
Pennsylvania.
CoNLL. 2005. Ninth Conference on Computational
Natural Language Learning (CoNLL-2005). Ann
Arbor, MI, USA.
CoNNL. 2004. Eighth Conference on Computa-
tional Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2002),
pages 239–246, Philadelphia, PA.
Jesus Gimenez and Lluis Marquez. 2004. Svmtool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of the 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’04), Lisbon, Portugal.
Aria Haghighi, Kristina Toutanova, and Christopher
Manning. 2005. A joint model for semantic role
labeling. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
James Henderson and Peter Lane. 1998. A connection-
ist architecture for learning to parse. In Proceedings
of 17th International Conference on Computational
Linguistics and the 36th Annual Meeting of the As-
sociation for Computational Linguistics (COLING-
ACL‘98), pages 531–537, University of Montreal,
Canada.
Jamie Henderson. 2003. Inducing history representa-
tions for broad-coverage statistical parsing. In Pro-
ceedings of the Joint Meeting of the North American
Chapter of the Association for Computational Lin-
guistics and the Human Language Technology Con-
ference (NAACL-HLT’03), pages 103–110, Edmon-
ton, Canada.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the ACL (ACL’03), pages
423–430, Sapporo, Japan.
Namhee Kwon, Michael Fleischman, and Eduard
Hovy. 2004. Senseval automatic labeling of se-
mantic roles using maximum entropy models. In
Senseval-3, pages 129–132, Barcelona, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313–330.
Lluis Marquez, Pere Comas, Jesus Gimenez, and Neus
Catala. 2005. Semantic role labeling as sequential
tagging. In Proceedings of CoNLL-2005, Ann Ar-
bor, MI USA.
Paola Merlo and Gabriele Musillo. 2005. Accurate
function parsing. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 620–627, Vancouver, British Columbia,
Canada, October.
Gabriele Musillo and Paola Merlo. 2005. Lexical and
structural biases for function parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 83–92, Vancouver, British
Columbia, October.
Mark Jan Nederhof. 1994. Linguistic Parsing and Pro-
gram Transformations. Ph.D. thesis, Department of
Computer Science, University of Nijmegen.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31:71–105.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of CoNLL-2005,
Ann Arbor, MI USA.
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multiple
semantic role labeling systems. In Proceedings of
CoNLL-2005, Ann Arbor, MI USA.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic role
labeling using complete syntactic analysis. In Pro-
ceedings of CoNLL-2005, Ann Arbor, MI USA.
Patrick Ye and Timothy Baldwin. 2005. Semantic role
labelling of prepositional phrases. In Proceedings of
the Second International Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages pp.
779–791, Jeju, South Korea.
Szu-ting Yi and Martha Palmer. 2005. The integration
of semantic parsing and semantic role labelling. In
Proceedings of CoNLL’05, Ann Arbor, Michigan.
</reference>
<page confidence="0.999283">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.243225">
<title confidence="0.94025">Robust Parsing of the Proposition Bank</title>
<author confidence="0.984102">Gabriele</author>
<affiliation confidence="0.934164666666667">Depts of Linguistics and Computer University of 2 Rue de</affiliation>
<address confidence="0.644305">1211 Geneva</address>
<email confidence="0.876773">musillo4@etu.unige.ch</email>
<author confidence="0.907684">Paola</author>
<affiliation confidence="0.920785333333333">Department of University of 2 Rue de</affiliation>
<address confidence="0.843306">1211 Geneva</address>
<email confidence="0.968036">merlo@lettres.unige.ch</email>
<abstract confidence="0.999680083333333">In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98),</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="1908" citStr="Baker et al., 1998" startWordPosition="295" endWordPosition="298">ate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful. The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al., 1998) or the Proposition Bank (Palmer et al., 2005), that can be used as training resources for a number of supervised learning paradigms. We focus here on the Proposition Bank (PropBank). PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993). Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0-A5 or AA for those complements of the predicative verb that are considered arguments while those complements of t</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98), pages 86–90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford, UK.</location>
<contexts>
<context position="12164" citStr="Bishop, 1995" startWordPosition="2003" endWordPosition="2004">n labels, which have been reorganised in a coherent inventory of labels and assigned exhaustively to all sentences in the PTB. Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels. A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to those elements that PTB annotators had classified as re2The on-line version of Backpropagation is used to train SSN parsing models. It performs a gradient descent with a maximum likelihood objective function and weight decay regularization (Bishop, 1995). 13 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ VBD-REL dropped PP-AM-TMP ✟✟ ❍ ❍ IN(-AM-TMP) at midnight NP-AM-TMP NNP(-AM-TMP) Tuesday PP-A4 ✟✟❍❍ TO to QP ✏✏✏ P PP $ 2.80 trillion PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion NP IN from NP NN Figure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals. ceiving a syntactic functional label such as SBJ (subject) or DTV (dative). Because they are projections of the lexical semantics of the </context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="18398" citStr="Carreras and Marquez, 2005" startWordPosition="3058" endWordPosition="3061">e distance between the nodes 01 and 02, which stand in a c-command relation, is shortened. For more information on this technique to capture domains induced by the c-command relation, see (Musillo and Merlo, 2005). We report the effects of these augmentations on parsing results in the experiments described below. 4 Experiments Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank. Training, validating and testing data sets consist of the PTB data annotated with PropBank semantic roles labels, as provided in the CoNLL-2005 shared task (Carreras and Marquez, 2005). Our augmented model has a total 613 of nonterminals to represents both the PTB and PropBank labels of constituents, instead of the 33 of the original SSN parser. The 580 newly introduced labels consist of a standard PTB label followed by a set of one or more PropBank semantic role such as PP-AM-TMP or NP-A0-A1. As a result of lowering the six AM-X semantic role labels, 240 new part-of-speech tags were introduced to partition the original tag set which consisted of 45 tags. SSN parsers do not tag their input sentences. To provide the augmented model with tagged input sentences, we trained an </context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of North American Chapter of Association for Computational Linguistics (NAACL’00),</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="939" citStr="Charniak, 2000" startWordPosition="139" endWordPosition="140">is paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Introduction Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the </context>
<context position="7402" citStr="Charniak, 2000" startWordPosition="1207" endWordPosition="1208">emantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown state-of-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the 12 parameter estimates. As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. Events in such a model are derivation moves. The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy. The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack. Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation steps. They use a ne</context>
<context position="24463" citStr="Charniak, 2000" startWordPosition="4081" endWordPosition="4082">82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. We merge the partial trees output by a semantic role labeller with the output of a parser it was trained on, and compute PropBank parsing performance measures on the resulting parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) accordin</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st Meeting of North American Chapter of Association for Computational Linguistics (NAACL’00), pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Pennsylvania.</institution>
<contexts>
<context position="923" citStr="Collins, 1999" startWordPosition="137" endWordPosition="138"> Abstract In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Introduction Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes th</context>
<context position="7385" citStr="Collins, 1999" startWordPosition="1205" endWordPosition="1206"> of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown state-of-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the 12 parameter estimates. As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. Events in such a model are derivation moves. The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy. The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack. Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation ste</context>
<context position="24446" citStr="Collins, 1999" startWordPosition="4079" endWordPosition="4080">l., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. We merge the partial trees output by a semantic role labeller with the output of a parser it was trained on, and compute PropBank parsing performance measures on the resulting parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turm</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Department of Computer Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2005</date>
<booktitle>Ninth Conference on Computational Natural Language Learning (CoNLL-2005).</booktitle>
<location>Ann Arbor, MI, USA.</location>
<contexts>
<context position="4182" citStr="CoNLL, 2005" startWordPosition="693" endWordPosition="694">ll 1There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ IN NP IN NP NN at QP ✏✏✏ P PP $ 2.80 trillion from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample syntactic structure from the PropBank with semantic role annotations. parse (CoNNL, 2004; CoNLL, 2005). Because of the way the problem has been formulated – as a pipeline of parsing (or chunking) feeding into labelling – specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser’s accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. W</context>
<context position="24380" citStr="CoNLL, 2005" startWordPosition="4070" endWordPosition="4071"> 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. We merge the partial trees output by a semantic role labeller with the output of a parser it was trained on, and compute PropBank parsing performance measures on the resulting parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005</context>
</contexts>
<marker>CoNLL, 2005</marker>
<rawString>CoNLL. 2005. Ninth Conference on Computational Natural Language Learning (CoNLL-2005). Ann Arbor, MI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CoNNL</author>
</authors>
<date>2004</date>
<booktitle>Eighth Conference on Computational Natural Language Learning (CoNLL-2004).</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="4168" citStr="CoNNL, 2004" startWordPosition="691" endWordPosition="692">need for a full 1There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ IN NP IN NP NN at QP ✏✏✏ P PP $ 2.80 trillion from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample syntactic structure from the PropBank with semantic role annotations. parse (CoNNL, 2004; CoNLL, 2005). Because of the way the problem has been formulated – as a pipeline of parsing (or chunking) feeding into labelling – specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser’s accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax an</context>
</contexts>
<marker>CoNNL, 2004</marker>
<rawString>CoNNL. 2004. Eighth Conference on Computational Natural Language Learning (CoNLL-2004). Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3480" citStr="Gildea and Jurafsky, 2002" startWordPosition="552" endWordPosition="555">ses two levels of granularity in its annotation, at least conceptually. Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ IN NP IN NP NN at QP ✏✏✏ P PP $ 2.80 trillion from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>239--246</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3506" citStr="Gildea and Palmer, 2002" startWordPosition="556" endWordPosition="559">ty in its annotation, at least conceptually. Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ IN NP IN NP NN at QP ✏✏✏ P PP $ 2.80 trillion from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample syntactic structure from t</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 239–246, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Lluis Marquez</author>
</authors>
<title>Svmtool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="19093" citStr="Gimenez and Marquez, 2004" startWordPosition="3179" endWordPosition="3182">oth the PTB and PropBank labels of constituents, instead of the 33 of the original SSN parser. The 580 newly introduced labels consist of a standard PTB label followed by a set of one or more PropBank semantic role such as PP-AM-TMP or NP-A0-A1. As a result of lowering the six AM-X semantic role labels, 240 new part-of-speech tags were introduced to partition the original tag set which consisted of 45 tags. SSN parsers do not tag their input sentences. To provide the augmented model with tagged input sentences, we trained an SVM tagger whose features and parameters are described in detail in (Gimenez and Marquez, 2004). Trained on section 2-21, the tagger reaches a performance of 95.45% on the test set (section 23) using our new tag set. As already mentioned, argumental labels A0-A5 are specific to a given verb or a given verb sense, thus their distribution is highly variable. To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu15 F R P PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9 Table 1: Percentage F-measure (F), recall (</context>
</contexts>
<marker>Gimenez, Marquez, 2004</marker>
<rawString>Jesus Gimenez and Lluis Marquez. 2004. Svmtool: A general POS tagger generator based on Support Vector Machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
</authors>
<title>A joint model for semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="23725" citStr="Haghighi et al., 2005" startWordPosition="3971" endWordPosition="3974">aring semantic role labels, thus abstracting away from those nodes that smear the pertinent relations. (Yi and Palmer, 2005) share the motivation of our work. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-</context>
<context position="24958" citStr="Haghighi et al., 2005" startWordPosition="4165" endWordPosition="4168">mantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. An indirect way of comparing our parser with semantic role labellers suggests itself. We merge the partial trees output by a semantic role labeller with the output of a parser it was trained on, and compute PropBank parsing performance measures on the resulting parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005). The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser. These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005). While our preliminary results indicated in the la</context>
</contexts>
<marker>Haghighi, Toutanova, Manning, 2005</marker>
<rawString>Aria Haghighi, Kristina Toutanova, and Christopher Manning. 2005. A joint model for semantic role labeling. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Peter Lane</author>
</authors>
<title>A connectionist architecture for learning to parse.</title>
<date>1998</date>
<booktitle>In Proceedings of 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLINGACL‘98),</booktitle>
<pages>531--537</pages>
<institution>University of Montreal, Canada.</institution>
<contexts>
<context position="8087" citStr="Henderson and Lane, 1998" startWordPosition="1309" endWordPosition="1312">n such a model are derivation moves. The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy. The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack. Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation steps. They use a neural network architecture, called Simple Synchrony Network (Henderson and Lane, 1998), to induce a finite history representation of an unbounded sequence of moves. The history representation of a parse history d1, ... , di−1, which we denote h(d1, ... , di−1), is assigned to the constituent that is on the top of the stack before the ith move. The representation h(d1, ... , di−1) is computed from a set f of features of the derivation move di−1 and from a finite set D of recent history representations h(d1, ... , dj), where j &lt; i − 1. Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next mov</context>
</contexts>
<marker>Henderson, Lane, 1998</marker>
<rawString>James Henderson and Peter Lane. 1998. A connectionist architecture for learning to parse. In Proceedings of 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLINGACL‘98), pages 531–537, University of Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jamie Henderson</author>
</authors>
<title>Inducing history representations for broad-coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Meeting of the North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conference (NAACL-HLT’03),</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="957" citStr="Henderson, 2003" startWordPosition="141" endWordPosition="142">end an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels. Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps. 1 Introduction Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Tren</context>
<context position="4549" citStr="Henderson, 2003" startWordPosition="755" endWordPosition="756">n from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample syntactic structure from the PropBank with semantic role annotations. parse (CoNNL, 2004; CoNLL, 2005). Because of the way the problem has been formulated – as a pipeline of parsing (or chunking) feeding into labelling – specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied. We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser’s accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. We achieve promising results both on the simple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labels of PropBank are taken into account. We will call the former task Penn Treebank parsing (PTB parsing) and the latter task PropBank parsing below. These results have sev</context>
<context position="6922" citStr="Henderson, 2003" startWordPosition="1136" endWordPosition="1137">obust. Finally, we achieve robustness without simplifying the parsing architecture. Specifically, robustness is achieved without resorting to the stipulation of strong independence assumptions to compensate for the limited availability and high variability of data. Consequently, such an achievement demonstrates not only that the robustness of the parsing model, but also its scalability and portability. 2 The Basic Parsing Architecture To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem. This architecture has shown state-of-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the 12 parameter estimates. As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. Events in such a model are derivation moves. The set of well-formed </context>
<context position="9059" citStr="Henderson, 2003" startWordPosition="1481" endWordPosition="1482"> di−1 and from a finite set D of recent history representations h(d1, ... , dj), where j &lt; i − 1. Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move. However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations. (Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. In addition to history representations, the inputs to h(d1, ... , di−1) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i. For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d1, ... , di−1 includes the most recent history representation of the followi</context>
<context position="17182" citStr="Henderson, 2003" startWordPosition="2851" endWordPosition="2853">th a semantic role label to the set D. These additions yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents. This modification of the biases is illustrated in Figure 3. This figure displays two constituents, S and VP with some of their respective child nodes. The VP node is assumed to be on the top of the parser’s stack, and the S one is supposed to be its leftcorner ancestor. The directed arcs represent the information that flows from one node to another. According to the original SSN model in (Henderson, 2003), only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent. In the figure above, only the information conveyed by the nodes α and 6 is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes E and θ. In the original SSN models, nodes bearing a function label such as 01 and 02 are not directly input to their respective parents. In our extended model, information conveyed by 01 and 02 directly flows to their respective parents. So the distance </context>
<context position="19634" citStr="Henderson, 2003" startWordPosition="3274" endWordPosition="3275">tures and parameters are described in detail in (Gimenez and Marquez, 2004). Trained on section 2-21, the tagger reaches a performance of 95.45% on the test set (section 23) using our new tag set. As already mentioned, argumental labels A0-A5 are specific to a given verb or a given verb sense, thus their distribution is highly variable. To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu15 F R P PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9 Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser. lary of our model. We reach a total of 4970 tagword pairs.3 This vocabulary comprises the original 512 pairs of the original SSN model, and our added pairs which must occur at least 10 times in the training data. Our vocabulary as well as the new 240 POS tags and the new 580 non-terminal labels are included in the set f of features input to the history representations as described in section 2. We perform two different evaluations on our model t</context>
<context position="21311" citStr="Henderson, 2003" startWordPosition="3571" endWordPosition="3572">r set of nonterminal labels. The results, computed on the testing data set from the PropBank, are shown on the first line of Table 1. To evaluate the PTB task, we compute the labelled recall and precision of constituents, ignoring the set of PropBank semantic role labels that our model assigns to constituents. This evaluation indicates how well we perform on the standard PTB parsing task alone, and its results on the testing data set from the PTB are shown on the second line of Table 1. The third line of Table 1 gives the performance on the simpler PTB parsing task of the original SSN parser (Henderson, 2003), that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets. 5 Discussion These results clearly indicate that our model can perform the PTB parsing task at levels of per3Such pairs consists of a tag and a word token. No attempt at collecting word types was made. formance comparable to state-of-the-art statistical parsing, by extensions that take the nature of the richer labels to be recovered into account. They also suggest that the relationship between syntactic PTB parsing and semantic PropBank parsing is strict enough that an integrated approach to th</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>Jamie Henderson. 2003. Inducing history representations for broad-coverage statistical parsing. In Proceedings of the Joint Meeting of the North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conference (NAACL-HLT’03), pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the ACL (ACL’03),</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="14757" citStr="Klein and Manning, 2003" startWordPosition="2435" endWordPosition="2438"> into account, annotating multiple roles, coreferences and discontinuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface between syntax and semantics, and they need to be recovered by applying constraints that model both higher level nodes and lower level ones. We assume that semantic roles are very often projected by the lexical semantics of the words in the sentence. We introduce this bottom-up lexical information by fine-grained modelling of semantic role labels. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005; Musillo and Merlo, 2005) for function labels, we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PNC, CAUS and TMP) were propagated down to the pre-terminal part-of-speech tag of its head. To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distance</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the ACL (ACL’03), pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Namhee Kwon</author>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Senseval automatic labeling of semantic roles using maximum entropy models.</title>
<date>2004</date>
<booktitle>In Senseval-3,</booktitle>
<pages>129--132</pages>
<location>Barcelona,</location>
<contexts>
<context position="22265" citStr="Kwon et al., 2004" startWordPosition="3725" endWordPosition="3728">he-art statistical parsing, by extensions that take the nature of the richer labels to be recovered into account. They also suggest that the relationship between syntactic PTB parsing and semantic PropBank parsing is strict enough that an integrated approach to the problem of semantic role labelling is beneficial. In particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (Kwon et al., 2004). In this way, they can never encode directly the constraining power of a certain role in a given structural position onto a following node in its structural position. In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains defined by the notion of c-command. Our results also confirm the findings in (Palmer et al., 2005). They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature. They suggest that the path feature is not very effective because it is sparse. Its sparseness is due to the </context>
</contexts>
<marker>Kwon, Fleischman, Hovy, 2004</marker>
<rawString>Namhee Kwon, Michael Fleischman, and Eduard Hovy. 2004. Senseval automatic labeling of semantic roles using maximum entropy models. In Senseval-3, pages 129–132, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2256" citStr="Marcus et al., 1993" startWordPosition="350" endWordPosition="353">rases, the information provided by a syntactic parser, is only moderately useful. The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al., 1998) or the Proposition Bank (Palmer et al., 2005), that can be used as training resources for a number of supervised learning paradigms. We focus here on the Proposition Bank (PropBank). PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993). Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0-A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively. A tree structure with PropBank labels for a sentence from the PTB (section 00) is shown in Figure 1 below. PropBank us</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitch Marcus, Beatrice Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Pere Comas</author>
<author>Jesus Gimenez</author>
<author>Neus Catala</author>
</authors>
<title>Semantic role labeling as sequential tagging.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="23842" citStr="Marquez et al., 2005" startWordPosition="3992" endWordPosition="3995"> 2005) share the motivation of our work. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, </context>
<context position="25438" citStr="Marquez et al., 2005" startWordPosition="4245" endWordPosition="4248">g parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005). The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser. These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005). While our preliminary results indicated in the last line of Table 2 are not state-of-the-art, they do demonstrate the viability of SSN parsers for joint inference of syntactic and semantic representations. 6 Conclusions In this paper, we have explored extensions to an existing state-of-the-art parsing model. We have achieved promising results on parsing the Proposition Bank, showing that our extensions are sufficiently robust to produce parse trees annotated with shallow semantic information. Future work will lie in extract</context>
</contexts>
<marker>Marquez, Comas, Gimenez, Catala, 2005</marker>
<rawString>Lluis Marquez, Pere Comas, Jesus Gimenez, and Neus Catala. 2005. Semantic role labeling as sequential tagging. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Accurate function parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>620--627</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="11311" citStr="Merlo and Musillo, 2005" startWordPosition="1863" endWordPosition="1866">ely after a tag-word pair has been pushed onto the stack: only a fixed beam of the 100 best derivations ending in that tag-word pair are expanded. For training, the width of such beam is set to five. A second reduction of the search space prunes the space of possible project or attach derivation moves: a best-first search strategy is applied to the five best alternative decisions only. The next section describes our model, extended to produce richer output parse trees annotated with semantic role labels. 3 Learning Semantic Role Labels Previous work on learning function labels during parsing (Merlo and Musillo, 2005; Musillo and Merlo, 2005) assumed that function labels represent the interface between lexical semantics and syntax. We extend this hypothesis to the semantic role labels assigned in PropBank, as they are an exhaustive extension of function labels, which have been reorganised in a coherent inventory of labels and assigned exhaustively to all sentences in the PTB. Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels. A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to t</context>
<context position="14797" citStr="Merlo and Musillo, 2005" startWordPosition="2442" endWordPosition="2445"> coreferences and discontinuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface between syntax and semantics, and they need to be recovered by applying constraints that model both higher level nodes and lower level ones. We assume that semantic roles are very often projected by the lexical semantics of the words in the sentence. We introduce this bottom-up lexical information by fine-grained modelling of semantic role labels. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005; Musillo and Merlo, 2005) for function labels, we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PNC, CAUS and TMP) were propagated down to the pre-terminal part-of-speech tag of its head. To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distances greater than two are not included. Fig</context>
</contexts>
<marker>Merlo, Musillo, 2005</marker>
<rawString>Paola Merlo and Gabriele Musillo. 2005. Accurate function parsing. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 620–627, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Musillo</author>
<author>Paola Merlo</author>
</authors>
<title>Lexical and structural biases for function parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>83--92</pages>
<location>Vancouver, British Columbia,</location>
<contexts>
<context position="11337" citStr="Musillo and Merlo, 2005" startWordPosition="1867" endWordPosition="1870"> has been pushed onto the stack: only a fixed beam of the 100 best derivations ending in that tag-word pair are expanded. For training, the width of such beam is set to five. A second reduction of the search space prunes the space of possible project or attach derivation moves: a best-first search strategy is applied to the five best alternative decisions only. The next section describes our model, extended to produce richer output parse trees annotated with semantic role labels. 3 Learning Semantic Role Labels Previous work on learning function labels during parsing (Merlo and Musillo, 2005; Musillo and Merlo, 2005) assumed that function labels represent the interface between lexical semantics and syntax. We extend this hypothesis to the semantic role labels assigned in PropBank, as they are an exhaustive extension of function labels, which have been reorganised in a coherent inventory of labels and assigned exhaustively to all sentences in the PTB. Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels. A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to those elements that PTB ann</context>
<context position="14823" citStr="Musillo and Merlo, 2005" startWordPosition="2446" endWordPosition="2449">inuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface between syntax and semantics, and they need to be recovered by applying constraints that model both higher level nodes and lower level ones. We assume that semantic roles are very often projected by the lexical semantics of the words in the sentence. We introduce this bottom-up lexical information by fine-grained modelling of semantic role labels. Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005; Musillo and Merlo, 2005) for function labels, we split some part-of-speech tags into tags marked with semantic role labels. The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PNC, CAUS and TMP) were propagated down to the pre-terminal part-of-speech tag of its head. To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distances greater than two are not included. Figure 2 illustrates the resu</context>
<context position="17984" citStr="Musillo and Merlo, 2005" startWordPosition="2992" endWordPosition="2995"> conveyed by the nodes α and 6 is directly input to the node S. Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes E and θ. In the original SSN models, nodes bearing a function label such as 01 and 02 are not directly input to their respective parents. In our extended model, information conveyed by 01 and 02 directly flows to their respective parents. So the distance between the nodes 01 and 02, which stand in a c-command relation, is shortened. For more information on this technique to capture domains induced by the c-command relation, see (Musillo and Merlo, 2005). We report the effects of these augmentations on parsing results in the experiments described below. 4 Experiments Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank. Training, validating and testing data sets consist of the PTB data annotated with PropBank semantic roles labels, as provided in the CoNLL-2005 shared task (Carreras and Marquez, 2005). Our augmented model has a total 613 of nonterminals to represents both the PTB and PropBank labels of constituents, instead of the 33 of the original SSN parser. The 580 newly introduc</context>
</contexts>
<marker>Musillo, Merlo, 2005</marker>
<rawString>Gabriele Musillo and Paola Merlo. 2005. Lexical and structural biases for function parsing. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 83–92, Vancouver, British Columbia, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Jan Nederhof</author>
</authors>
<title>Linguistic Parsing and Program Transformations.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Nijmegen.</institution>
<contexts>
<context position="7632" citStr="Nederhof, 1994" startWordPosition="1245" endWordPosition="1246">thout much modification to the current problem. This architecture has shown state-of-the-art performance. SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the 12 parameter estimates. As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. Events in such a model are derivation moves. The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy. The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack. Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation steps. They use a neural network architecture, called Simple Synchrony Network (Henderson and Lane, 1998), to induce a finite history representation of an unbounded sequence of moves. The history representation of a parse history d1, ... , di−1, whic</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>Mark Jan Nederhof. 1994. Linguistic Parsing and Program Transformations. Ph.D. thesis, Department of Computer Science, University of Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<pages>31--71</pages>
<contexts>
<context position="1954" citStr="Palmer et al., 2005" startWordPosition="303" endWordPosition="306">formation extraction. For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination. Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful. The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al., 1998) or the Proposition Bank (Palmer et al., 2005), that can be used as training resources for a number of supervised learning paradigms. We focus here on the Proposition Bank (PropBank). PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993). Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0-A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional la</context>
<context position="3654" citStr="Palmer et al., 2005" startWordPosition="581" endWordPosition="584"> while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures. The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements. While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers. See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels. 11 S ❍ ✟✟✟✟✟✟✟✟✟✟ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ ❍ NP-A1 VP P ✏✏✏✏✏✏✏ PP ❍ P ✟✟✟✟✟✟✟✟ P P ❍ ✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘✘ P ❍ ❳❳ ❳ ❳ ❳❳ ❳ ❳ ❳ ❳❳ ❳ ❳❳ ❳❳ ❍ ❍ the government’s borrowing authority ❍ ❍ ❍ IN NP IN NP NN at QP ✏✏✏ P PP $ 2.80 trillion from midnight PP-A3 ✟✟❍❍ NP QP ✏✏✏ P PP $ 2.87 trillion PP-AM-TMP ✟✟❍❍ PP-A4 ✟✟❍❍ TO to VBD-REL dropped NP-AM-TMP NNP Tuesday Figure 1: A sample syntactic structure from the PropBank with semantic role annotations. parse (CoNNL, 2004; CoNLL, 2005). Because of the way the problem has been formulated – as a pipeline of </context>
<context position="22635" citStr="Palmer et al., 2005" startWordPosition="3790" endWordPosition="3793">ing separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (Kwon et al., 2004). In this way, they can never encode directly the constraining power of a certain role in a given structural position onto a following node in its structural position. In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains defined by the notion of c-command. Our results also confirm the findings in (Palmer et al., 2005). They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature. They suggest that the path feature is not very effective because it is sparse. Its sparseness is due to the occurrence of intermediate nodes that are not relevant for the syntactic relations between an argument and its predicate. Our model of domains is less noisy, and consequently more robust, because it can focus only on c-commanding nodes bearing semantic role labels, thus abstracting away from those nodes that smear the pertinent relations. (Yi and Palmer, 2005) share t</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31:71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role chunking combining complementary syntactic views.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="23763" citStr="Pradhan et al., 2005" startWordPosition="3978" endWordPosition="3981">cting away from those nodes that smear the pertinent relations. (Yi and Palmer, 2005) share the motivation of our work. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling syst</context>
<context position="25461" citStr="Pradhan et al., 2005" startWordPosition="4249" endWordPosition="4252">st five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005). The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser. These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005). While our preliminary results indicated in the last line of Table 2 are not state-of-the-art, they do demonstrate the viability of SSN parsers for joint inference of syntactic and semantic representations. 6 Conclusions In this paper, we have explored extensions to an existing state-of-the-art parsing model. We have achieved promising results on parsing the Proposition Bank, showing that our extensions are sufficiently robust to produce parse trees annotated with shallow semantic information. Future work will lie in extracting semantic role relat</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Semantic role chunking combining complementary syntactic views. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Peter Koomen</author>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>Generalized inference with multiple semantic role labeling systems.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="23804" citStr="Punyakanok et al., 2005" startWordPosition="3985" endWordPosition="3988"> the pertinent relations. (Yi and Palmer, 2005) share the motivation of our work. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output </context>
<context position="25382" citStr="Punyakanok et al., 2005" startWordPosition="4235" endWordPosition="4238">mpute PropBank parsing performance measures on the resulting parse trees. The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005). The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser. These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005). While our preliminary results indicated in the last line of Table 2 are not state-of-the-art, they do demonstrate the viability of SSN parsers for joint inference of syntactic and semantic representations. 6 Conclusions In this paper, we have explored extensions to an existing state-of-the-art parsing model. We have achieved promising results on parsing the Proposition Bank, showing that our extensions are sufficiently robust to produce parse trees annotated with shall</context>
</contexts>
<marker>Punyakanok, Koomen, Roth, Yih, 2005</marker>
<rawString>Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen tau Yih. 2005. Generalized inference with multiple semantic role labeling systems. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>Semantic role labeling using complete syntactic analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<location>Ann Arbor, MI USA.</location>
<contexts>
<context position="23884" citStr="Surdeanu and Turmo, 2005" startWordPosition="3999" endWordPosition="4002">k. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html). comparison between our PropBank SSN parser and other PropBank parsers. However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training a</context>
<context position="25507" citStr="Surdeanu and Turmo, 2005" startWordPosition="4256" endWordPosition="4259">res for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005). The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser. These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005). While our preliminary results indicated in the last line of Table 2 are not state-of-the-art, they do demonstrate the viability of SSN parsers for joint inference of syntactic and semantic representations. 6 Conclusions In this paper, we have explored extensions to an existing state-of-the-art parsing model. We have achieved promising results on parsing the Proposition Bank, showing that our extensions are sufficiently robust to produce parse trees annotated with shallow semantic information. Future work will lie in extracting semantic role relations from such richly annotated trees, for app</context>
</contexts>
<marker>Surdeanu, Turmo, 2005</marker>
<rawString>Mihai Surdeanu and Jordi Turmo. 2005. Semantic role labeling using complete syntactic analysis. In Proceedings of CoNLL-2005, Ann Arbor, MI USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Semantic role labelling of prepositional phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<pages>779--791</pages>
<location>Jeju, South</location>
<contexts>
<context position="13749" citStr="Ye and Baldwin, 2005" startWordPosition="2281" endWordPosition="2284">onstraints that govern syntactic dependencies, such as argument structure or subcategorization. We attempt to capture such constraints by modelling the c-command relation. Recall that the c-command relation relates two nodes in a tree, even if they are not close to each other, provided that the first node dominating one node also dominate the other. This notion of c-command captures both linear and hierarchical constraints and defines the domain in which semantic role labelling applies. While PTB function labels appear to overlap to a large extent with PropBank semantic rolel labels, work by (Ye and Baldwin, 2005) on semantic labelling prepositional phrases, however, indicates that the function labels in the Penn Treebank are assigned more sporadically and heterogeneously than in PropBank. Apparently only the “easy” cases have been tagged functionally, because assigning these function tags was not the main goal of the annotation. PropBank instead was annotated exhaustively, taking all cases into account, annotating multiple roles, coreferences and discontinuous constituents. It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface be</context>
</contexts>
<marker>Ye, Baldwin, 2005</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2005. Semantic role labelling of prepositional phrases. In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05), pages pp. 779–791, Jeju, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szu-ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>The integration of semantic parsing and semantic role labelling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL’05,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="23227" citStr="Yi and Palmer, 2005" startWordPosition="3893" endWordPosition="3896">ngs in (Palmer et al., 2005). They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature. They suggest that the path feature is not very effective because it is sparse. Its sparseness is due to the occurrence of intermediate nodes that are not relevant for the syntactic relations between an argument and its predicate. Our model of domains is less noisy, and consequently more robust, because it can focus only on c-commanding nodes bearing semantic role labels, thus abstracting away from those nodes that smear the pertinent relations. (Yi and Palmer, 2005) share the motivation of our work. Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information. To our knowledge, no results have yet been published on parsing the PropBank. Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marque</context>
</contexts>
<marker>Yi, Palmer, 2005</marker>
<rawString>Szu-ting Yi and Martha Palmer. 2005. The integration of semantic parsing and semantic role labelling. In Proceedings of CoNLL’05, Ann Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>