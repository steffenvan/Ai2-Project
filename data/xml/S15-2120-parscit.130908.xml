<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015035">
<title confidence="0.756877">
DsUniPi: An SVM-based Approach for Sentiment Analysis of Figurative
Language on Twitter
</title>
<author confidence="0.990509">
Maria Karanasou
</author>
<affiliation confidence="0.919332333333333">
Dept. of Digital Systems
University of Piraeus
Greece
</affiliation>
<email confidence="0.995617">
karanasou@gmail.com
</email>
<author confidence="0.965518">
Christos Doulkeridis
</author>
<affiliation confidence="0.917732333333333">
Dept. of Digital Systems
University of Piraeus
Greece
</affiliation>
<email confidence="0.995746">
cdoulk@unipi.gr
</email>
<author confidence="0.986495">
Maria Halkidi
</author>
<affiliation confidence="0.919181333333333">
Dept. of Digital Systems
University of Piraeus
Greece
</affiliation>
<email confidence="0.997426">
mhalk@unipi.gr
</email>
<sectionHeader confidence="0.995443" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.925063055555556">
The DsUniPi team participated in the SemEval
2015 Task#11: Sentiment Analysis of Figura-
tive Language in Twitter. The proposed ap-
proach employs syntactical and morphological
features, which indicate sentiment polarity in
both figurative and non-figurative tweets. These
features were combined with others that indi-
cate presence of figurative language in order to
predict a fine-grained sentiment score. The
method is supervised and makes use of struc-
tured knowledge resources, such as Senti-
WordNet sentiment lexicon for assigning
sentiment score to words and WordNet for cal-
culating word similarity. We have experiment-
ed with different classification algorithms
(Naïve Bayes, Decision trees, and SVM), and
the best results were achieved by an SVM clas-
sifier with linear kernel.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956315789474">
Sentiment analysis on figurative speech is a chal-
lenging task that becomes even more difficult on
short social-media related text. Tweet text can be
rich in irony that is either stated with hashtags ex-
plicitly (such as #irony) or implied. Identifying the
underlying sentiment of such text is challenging
due to its restricted size and features such as use of
abbreviations and slang. Consequently, assigning
positive or negative polarity is quite a difficult
task. The actual meaning can be very different than
what is stated, since, for example, in ironic lan-
guage what is said can be the opposite of what it is
meant. To address this challenge, we propose a
system for sentiment analysis of figurative lan-
guage, which relies on feature selection and trains
a classifier to predict the label of a tweet. Given a
labelled trial set, the objective of the system is to
correctly determine how positive, negative or neu-
tral a tweet is considered to be on a scale of [-5, 5].
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999779482758621">
Tweets have unique characteristics compared to
other text corpora, such as emoticons, abbrevia-
tions, and hashtags. Use of emoticons is considered
a reasonably effective way to conveying emotion
(Derks et al. 2008, Thelwall et al.). Go et al. (2009)
show that machine learning algorithms achieve
accuracy above 80% when trained with emoticon
data. It is also indicated that the use of hashtags
and presence of intensifiers, such as capitalization
and punctuation, can affect sentiment identification
(Kouloumpis et al., 2010). According to Agarwal
et al. (2011) such features can add value to a clas-
sifier, but only marginally. Additionally, natural
language related features, such as part-of-speech
tagging and use of lexicon resources, can signifi-
cantly contribute to detecting the sentiment of a
tweet. Moreover, features that combine the prior
polarity of words and their parts-of-speech tags are
considered most useful.
The problem of sentiment analysis on figurative
language has been addressed in many ways. Re-
searchers have investigated the use of lexical and
syntactic features in order to identify figurative
language and classify the conveyed sentiment. The
complexity of such a task is high, especially given
the fact that irony and sarcasm are frequently
mixed. Sarcasm is usually used for putting down
the target of the comment and is somewhat easier
to detect. Irony works as a negation, and it can be
</bodyText>
<page confidence="0.97951">
709
</page>
<bodyText confidence="0.964347060606061">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 709–713,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
conveyed through a positive context, which makes
it difficult to understand the actual meaning of a
tweet (Reyes et al. 2012, Veale et al. 2010). Da-
vidov et al. (2010) examined hashtags that indicat-
ed sarcasm to identify if such labelled tweets can
be a reliable source of sarcasm. They concluded
that user-labelled sarcastic tweets can be noisy and
constitute the hardest form of sarcasm. Riloff et al.
(2013) identify sarcasm that arises from the con-
trast between a positive sentiment referring to a
negative situation. Reyes et al. (2012) involved in
their work features that make use of contextual
imbalance, natural language concepts, syntactical
and morphological aspects of a tweet. Many stud-
ies exploit the use of contextual imbalance detec-
tion through calculation of semantic similarity
among the words. This is achieved using lexical
resources, such as WordNet or Whisel’s dictionary,
and the goal is to identify features like emotional
content, polarity of words and pleasantness, ad-
verbs implying negation or expressing timing.
Shutova et al. (2010) have deployed an unsuper-
vised method to identify metaphor using synonymy
information from WordNet. Reyes et al. (2013)
argue that other features such as punctuation
marks, emoticons, quotes, and capitalized words,
n-grams and skip-grams are also useful to the sen-
timent analysis process. Moreover, patterns such as
“As * As *” or “about as * as *” have been shown
to be useful in detecting ironic similes (Veale et al.
2010).
</bodyText>
<sectionHeader confidence="0.994135" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999970764705882">
The proposed system consists of two main mod-
ules: (a) the preprocessing, and (b) the classifica-
tion module. Each tweet t was submitted to
preprocessing, in order to remove useless infor-
mation and extract the desired/targeted features f.
The result of the preprocessing of a given tweet t
consists of a feature dictionary (fd) that stores the
values calculated for each feature. In the classifica-
tion part, the feature dictionaries are converted to
vectors and the result matrix is converted to a
term-frequency matrix. The aforementioned pro-
cess is the same for trial and test data and the tf
matrices are used by a classifier for training and
prediction. We tested different classifiers, includ-
ing Naïve Bayes, Decision trees, and SVM, in or-
der to study their performance and select the best-
performing.
</bodyText>
<subsectionHeader confidence="0.999579">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.995646">
Each tweet is given as input to the preprocessing
module, in order to transform it to a feature-value
dictionary representation:
</bodyText>
<equation confidence="0.859209">
fdt= (f1:v1, ..., fn:vn} (1)
</equation>
<bodyText confidence="0.999995125">
The preprocessing includes cleaning, which
starts with the removal of non-ascii characters and
is followed by the detection of certain features.
Feature detection takes place before the actual
cleaning of the text in order to avoid loss of infor-
mation, such as punctuation, urls and emoticons.
This process checks if a tweet contains question
marks or exclamation marks, capitalized words,
urls, negations, laughing, retweet, emoticons and
hashtags. The last two are categorized concerning
the sentiment they may convey. We manually cate-
gorized the top20 emoticons and some minor varia-
tions (http://datagenetics.com/blog/october52012)
as positive or negative, whereas hashtags are cate-
gorized as positive, negative or neutral. Hashtag
categorization makes use of SentiWordNet score
(swnScore) and the result is a representation of all
the hashtags present in a tweet.
In the hashtag categorization process, if a hashtag
ht is spelled correctly, its swnScore is retrieved.
Otherwise, spellchecking (Kelly) is tried once and
if it fails then the hashtag is categorized as neutral.
The result depends on the number of positive,
negative, neutral hashtags in HTt as follows:
</bodyText>
<equation confidence="0.995856666666667">
HT os, c(htfts) &gt; c(htNeg) &gt; 0
HTEmt = JHTpneu, c(htf�s) = c(htNeg) = 0 (2)
HT_neg, c(htNeg) &gt;_ c(htf�s) &gt; 0
</equation>
<bodyText confidence="0.999910692307692">
where c(htPos), c(htNeg) denote the count of posi-
tive and negative hashtags in a tweet t respectively.
Motivated by the “As * as *” pattern and after
studying the data set, we further identify in the fea-
ture selection process the presence of patterns such
as “Don’t you*”, “Oh so*?” and “As * As *”.
Cleaning proceeds with punctuation, stop-words,
urls, emoticons, hashtags and references removal.
Additionally, multiple consecutive letters in a word
are reduced to two. Finally, spellchecking is per-
formed to words that have been identified as mis-
spelled in order to deduce the correct word. After
cleaning, the process continues with part of speech
</bodyText>
<page confidence="0.976424">
710
</page>
<bodyText confidence="0.9998936">
(POS) tagging. POS-tagging is performed with the
use of a custom model (Derczynski et al., 2013)
and simplified tags (NN, VB, ADJ, RB). Words
that belong to the same part of speech are used in
semantic text similarity calculation simt. For this
feature, different similarity measures (Resnik’s,
Lin’s, and Wu &amp; Palmer’s) provided by nltk are
used (Pedersen et al., 2008). The value simt is cal-
culated as the maximum similarity score of every
combination of two words and their synonyms.
</bodyText>
<equation confidence="0.9975422">
E����+E����+ E ����+ E ����
���t __ (3)
c(V)+c(N)+c(A)+c(R)
51111A __ max(sim(Ai, Ai+1)), ... � 1 (4)
max(sim(A._1, A.)) J
</equation>
<bodyText confidence="0.999886153846154">
where V, N, A, and R denote the sets that contain
the total words that have been identified as verbs,
nouns, adjectives and adverbs respectively, while
max(sim(Ai, Ai+1)) is the maximum similarity be-
tween the processed words and their n synonyms.
Finally, the SentiWordNet score for each word in a
tweet is calculated (Baccianella et al., 2010), ignor-
ing words that have fewer than two letters. If the
score of a word cannot be determined, then we cal-
culate the SentiWordNet score of the stemmed
word. Given that the word wi occurs j times in the
SentiWordNet corpus, the total score of wi is given
by
</bodyText>
<equation confidence="0.979416">
SW.SCOrewi — Ek=11+�wScore(i,k)p—wscore( i,k)n (5)
—
�
</equation>
<bodyText confidence="0.999951692307692">
where wScore(i, k)p and wScore(i, k)n is the k-th
positive (PosScore) and negative (NegScore) score
respectively of wi in SentiWordNet. The index i of
each word was used in an attempt to correlate each
word’s position with the calculated sentiment.
Moreover, the total score of a tweet t is calculated
as the average of SentiWordNet scores of the
words in t.
The result is a dictionary with feature names as
keys and values that indicate feature existence. Ta-
ble 1 depicts the set of features considered by our
system, together with the domain of values that
they take.
</bodyText>
<subsectionHeader confidence="0.997512">
3.2 Classification
</subsectionHeader>
<bodyText confidence="0.999917076923077">
For the classification process, the feature dictionar-
ies fdt of each data set were processed by a vector-
izer to produce a vector array (http://scikit-
learn.org/stable/modules/generated/sklearn.feature
extraction.DictVectorizer.html). From the vector
array, a term-frequency matrix is calculated (with
the use of a TfidfTransformer and the parameter
“use_idf” set to False: http://scikit-
learn.org/stable/modules/generated/sklearn.feature_
extraction.text.TfidfTransformer.html) and is given
as input for training to the chosen classifier. This
frequency matrix is used to make predictions about
the test set.
</bodyText>
<subsectionHeader confidence="0.592521">
Feature Value
</subsectionHeader>
<tableCaption confidence="0.991142">
Table 1: Calculated features with their value.
</tableCaption>
<sectionHeader confidence="0.99568" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.992019">
The SemEval data set consists of 9000 tweets that
are rich in figurative language and stemmed from
</bodyText>
<figure confidence="0.995226517241379">
Oh so* (*) True/ False
Don’t you*(*) True/ False
As*As*(*) True/ False
Question mark(*) True/ False
Exclamation - True/ False
mark(*)
Capitals(*) True/ False
Reference(*) True/ False
RT True/ False
Negations(*) True/ False
URL True/ False
HT_pos(*) True/ False
HT_neg(*) True/ False
HT_neu(*) True/ False
Emoticon Pos(*) True/ False
Emoticon Neg(*) True/ False
&amp;quot;NN&amp;quot;, &amp;quot;VB&amp;quot;,
&amp;quot;ADJ&amp;quot;,&amp;quot;RB&amp;quot;
POS-tags(*)
swnScorewi(*) “positive”,
“somewhat positive”,
“neutral”, “negative”
“somewhat negative”
“positive”,
“somewhat positive”,
“neutral”, “negative”
“somewhat negative”
simt (Resnik*) Decimal score
swnScoreTotal
</figure>
<page confidence="0.993771">
711
</page>
<bodyText confidence="0.999903666666667">
user-generated tags, such as “#sarcasm&amp;quot; and “#iro-
ny&amp;quot;. There is a 90-10 split for trial and test data.
We retrieved 8529 tweets in total, 7606 from the
trial set and 923 from the test set. Out of these data
sets, positive tweets in total are 8,2%, negative
tweets are 85,2% and neutral 6,6%.
</bodyText>
<subsectionHeader confidence="0.948972">
4.1 Experiments
</subsectionHeader>
<bodyText confidence="0.99986075">
We experimented by incrementally adding features,
and trying different classifiers. The results of the
features that seem to contribute most were used to
make the prediction with which the system partici-
pated in the task and are the ones marked with (*)
in Table 1. It is also worthwhile mentioning that,
after trials, discretization was applied to swnScorewi
as follows:
</bodyText>
<equation confidence="0.9656635">
positive, (&gt; 1.2)
� somewhat positive, (&gt; 0.05 &lt; 1.2)
swnScorewi = neutral, (&lt; 0.05 &gt;_ 0.95)
� somewhat negative, (&lt; 0.95 &gt;_ 0.2)
�
�
</equation>
<subsectionHeader confidence="0.994362">
4.2 Final Results
</subsectionHeader>
<bodyText confidence="0.989654133333333">
We evaluate the performance of our approach
measuring the cosine similarity between the output
of our system and the given scores for the test data
set. Other measures such as accuracy, precision
and recall are also used in our study.
The most useful features are pos-tags and Sen-
tiWordNet score. Semantic similarity (Resnik
measure) and hashtags also seem to contribute and
the rest of the selected features contribute margin-
ally. These results are coherent with sentiment
analysis literature where prior polarity along with
POS-tagging seem to add most value to a classifi-
er, and other features like emoticons add up only
marginally (Agarwal et al., 2011, Kouloumpis et
al., 2010).
Table 2 shows the evaluation results (cosine
similarity and accuracy) of our system for both
initial and final data set. We can observe that Line-
ar SVM (default parameters: http://scikit-
learn.org/stable/modules/generated/sklearn.svm.Li
nearSVC.html) achieves the best performance with
respect to tweets classification. For the final sub-
mission, the total of the test and trial sets were
used as input for the learning process of the classi-
fier and only one run was submitted. The analysis
of the results of the final submission, presented in
Table 3, suggests that predictions on ironic and
sarcastic tweets are more accurate than tweets that
contain metaphor those that do not contain figura-
tive language.
</bodyText>
<table confidence="0.999207571428572">
Classi- Decision Naïve Linear
fiers Tree Bayes SVM
trials/ t f t f t f
final
Cosine 0.68 0.45 0.70 0.55 0.78 0.60
Accu- 0.31 0.21 0.33 0.23 0.38 0.29
racy
</table>
<tableCaption confidence="0.997569333333333">
Table 2: The results of the classifiers used on the initial
test data set (t) and the final (f), with the selected fea-
tures of the final submission.
</tableCaption>
<table confidence="0.9998215">
Cosine MSE
Similarity
Overall 0.601 3.925
Sarcasm 0.87 1.499
Irony 0.839 1.656
Metaphor 0.359 7.106
Other 0.271 5.744
Rank 10 10
</table>
<tableCaption confidence="0.999895">
Table 3: The final results by category.
</tableCaption>
<sectionHeader confidence="0.987226" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999966555555555">
The proposed system combines structured
knowledge sources along with common tweet and
figurative text features. A supervised learning ap-
proach is followed, having as goal to classify
tweets containing irony and metaphors. The system
ranked 10th (out of 15) based on both the cosine
similarity measure and MSE. Among ironic, sar-
castic, metaphoric and others, the best results were
achieved in tweets containing irony and sarcasm.
The most useful features for learning are pos-tags,
Senti-WordNet score, text semantic similarity and
hashtags. Our study shows that the performance of
our system could be improved by adding features
related to metaphor and considering better use of
hashtags in the classification process. Besides, the
use of non-figurative tweets in learning can signif-
icantly contribute to classify tweets that do not
contain figurative language.
</bodyText>
<sectionHeader confidence="0.998626" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.670506666666667">
The work of C. Doulkeridis and M. Halkidi has been
co-financed by ESF and Greek national funds through
the Operational Program “Education and Lifelong
Learning” of the National Strategic Reference Frame-
work (NSRF) - Research Funding Program: Aristeia II,
Project: ROADRUNNER.
</reference>
<figure confidence="0.8387875">
(6)
negative, (&lt; 0.2)
</figure>
<page confidence="0.986955">
712
</page>
<sectionHeader confidence="0.99563" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999854472527473">
Antonio Reyes, Paolo Rosso, Davide Buscaldi
(2012). From Humor Recognition to Irony De-
tection: The Figurative Language of Social Me-
dia. Data &amp; Knowledge Engineering 74:1-12.
Yanfen Hao, Tony Veale (2010). An Ironic Fist in
a Velvet Glove: Creative Mis-Representation in
the Construction of Ironic Similes. Minds and
Machines 20(4):635–650.
Antonio Reyes, Paolo Rosso, Tony Veale (2013).
A Multidimensional Approach for Detecting
Irony in Twitter. Languages Resources and
Evaluation 47(1): 239-268.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, Ruihong Huang.
Sarcasm as Contrast between a Positive Senti-
ment and Negative Situation. In Proceedings of
the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2013).
Dmitry Davidov, Oren Tsur, and Ari Rappoport
(2010). Semi-supervised recognition of sarcastic
sentences in twitter and amazon. In Proceedings
of the Fourteenth Conference on Computational
Natural Language Learning, CoNLL 2010.
Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo
Rosso, Ekaterina Shutova, Antonio Reyes, John
Barnden (2015). SemEval-2015 Task 11: Senti-
ment Analysis of Figurative Language in Twit-
ter. In: Proc. Int. Workshop on Semantic
Evaluation (SemEval-2015), Co-located with
NAACL and *SEM, Denver, Colorado, US,
June 4-5, 2015.
Ekaterina Shutova, Lin Sun and Anna Korhonen
(2010). Metaphor Identification Using Verb and
Noun Clustering. In: Proceedings of the 23rd In-
ternational Conference on Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang (2009).
Twitter sentiment classification using distant su-
pervision. In: Proceeding LSM &apos;11 Proceedings
of the Workshop on Languages in Social Media
Pages 30-38.
Daantje Derks, Arjan E. R. Bos, and Jasper von
Grumbkow (2007). Emoticons and online mes-
sage interpretation. Social Science Computer
Review, 26(3), 379-388.
Mike Thelwall, Kevan Buckley, Georgios Pal-
toglou, and Di Cai, Arvid Kappas (2010). Sen-
timent Strength Detection in Short Informal
Text. Journal of the American Society for In-
formation Science and Technology Volume 61,
Issue 12, pages 2544–2558, December 2010
Efthymios Kouloumpis, Theresa Wilson, and Jo-
hanna Moore (2011). Twitter sentiment analysis:
The Good the Bad and the OMG! In: Lada A.
Adamic, Ricardo A. Baeza-Yates, and Scott
Counts, editors, Proceedings of the Fifth Inter-
national Conference on Weblogs and Social
Media, ICWSM’ 11, pages 538–541, Barcelona,
Spain.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen
Rambow, Rebecca Passonneau (2011). Senti-
ment Analysis of Twitter Data. In: LSM&apos;11 Pro-
ceedings of the Workshop on Languages in
Social Media Pages 30-38.
Leon Derczynski, Alan Ritter, Sam Clark, and Ka-
lina Bontcheva (2013). Twitter Part-of-Speech
Tagging for All: Overcoming Sparse and Noisy
Data. In: Proceedings of the International Con-
ference on Recent Advances in Natural Lan-
guage Processing, ACL.
Stefano Baccianella, Andrea Esuli, and Fabrizio
Sebastiani (2010). SentiWordNet 3.0: An en-
hanced lexical resource for sentiment analysis
and opinion mining. In: Proceedings of the 7th
Conference on Language Resources and Evalua-
tion (LREC 2010), Valletta, MT, 2010, pp.
2200-2204.
Christiane Fellbaum (1998, ed.) WordNet: An
Electronic Lexical Database. Cambridge, MA.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. (2004). Wordnet::similarity - meas-
uring the relatedness of concepts. In: Demon-
stration papers at HLT-NAACL, pages 38-42.
Fabian Pedregosa et al. (2011). Scikit-learn: Ma-
chine Learning in Python. In Journal of Machine
Learning Research 12, pp. 2825-2830.
Steven Bird, Ewan Klein, and Edward Loper
(2009), Natural Language Processing with Py-
thon, O&apos;Reilly Media.
Ryan Kelly, https://pythonhosted.org/pyenchant/,
v. 1.6.5.
</reference>
<page confidence="0.998976">
713
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.169467">
<title confidence="0.997766">DsUniPi: An SVM-based Approach for Sentiment Analysis of Figurative Language on Twitter</title>
<author confidence="0.978846">Maria</author>
<affiliation confidence="0.923076333333333">Dept. of Digital University of Greece</affiliation>
<email confidence="0.999375">karanasou@gmail.com</email>
<author confidence="0.839056">Christos</author>
<affiliation confidence="0.999625">Dept. of Digital University of</affiliation>
<address confidence="0.544848">Greece</address>
<email confidence="0.907581">cdoulk@unipi.gr</email>
<author confidence="0.805244">Maria</author>
<affiliation confidence="0.895758333333333">Dept. of Digital University of Greece</affiliation>
<email confidence="0.976656">mhalk@unipi.gr</email>
<abstract confidence="0.995339368421053">The DsUniPi team participated in the SemEval 2015 Task#11: Sentiment Analysis of Figurative Language in Twitter. The proposed approach employs syntactical and morphological features, which indicate sentiment polarity in both figurative and non-figurative tweets. These features were combined with others that indicate presence of figurative language in order to predict a fine-grained sentiment score. The method is supervised and makes use of structured knowledge resources, such as Senti- WordNet sentiment lexicon for assigning sentiment score to words and WordNet for calculating word similarity. We have experimented with different classification algorithms (Naïve Bayes, Decision trees, and SVM), and the best results were achieved by an SVM classifier with linear kernel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>The work of C Doulkeridis</author>
<author>M</author>
</authors>
<title>Halkidi has been co-financed by ESF and Greek national funds through the</title>
<booktitle>Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) - Research Funding Program: Aristeia II, Project: ROADRUNNER.</booktitle>
<marker>Doulkeridis, M, </marker>
<rawString>The work of C. Doulkeridis and M. Halkidi has been co-financed by ESF and Greek national funds through the Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) - Research Funding Program: Aristeia II, Project: ROADRUNNER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Davide Buscaldi</author>
</authors>
<title>From Humor Recognition to Irony Detection: The Figurative Language of Social Media.</title>
<date>2012</date>
<journal>Data &amp; Knowledge Engineering</journal>
<pages>74--1</pages>
<contexts>
<context position="3867" citStr="Reyes et al. 2012" startWordPosition="596" endWordPosition="599">urative language and classify the conveyed sentiment. The complexity of such a task is high, especially given the fact that irony and sarcasm are frequently mixed. Sarcasm is usually used for putting down the target of the comment and is somewhat easier to detect. Irony works as a negation, and it can be 709 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 709–713, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conveyed through a positive context, which makes it difficult to understand the actual meaning of a tweet (Reyes et al. 2012, Veale et al. 2010). Davidov et al. (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm. They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm. Riloff et al. (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation. Reyes et al. (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet. Many studies exploit the use of co</context>
</contexts>
<marker>Reyes, Rosso, Buscaldi, 2012</marker>
<rawString>Antonio Reyes, Paolo Rosso, Davide Buscaldi (2012). From Humor Recognition to Irony Detection: The Figurative Language of Social Media. Data &amp; Knowledge Engineering 74:1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanfen Hao</author>
<author>Tony Veale</author>
</authors>
<title>An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes.</title>
<date>2010</date>
<journal>Minds and Machines</journal>
<volume>20</volume>
<issue>4</issue>
<marker>Hao, Veale, 2010</marker>
<rawString>Yanfen Hao, Tony Veale (2010). An Ironic Fist in a Velvet Glove: Creative Mis-Representation in the Construction of Ironic Similes. Minds and Machines 20(4):635–650.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A Multidimensional Approach for Detecting Irony in Twitter.</title>
<date>2013</date>
<journal>Languages Resources and Evaluation</journal>
<volume>47</volume>
<issue>1</issue>
<pages>239--268</pages>
<contexts>
<context position="4923" citStr="Reyes et al. (2013)" startWordPosition="761" endWordPosition="764">ork features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet. Many studies exploit the use of contextual imbalance detection through calculation of semantic similarity among the words. This is achieved using lexical resources, such as WordNet or Whisel’s dictionary, and the goal is to identify features like emotional content, polarity of words and pleasantness, adverbs implying negation or expressing timing. Shutova et al. (2010) have deployed an unsupervised method to identify metaphor using synonymy information from WordNet. Reyes et al. (2013) argue that other features such as punctuation marks, emoticons, quotes, and capitalized words, n-grams and skip-grams are also useful to the sentiment analysis process. Moreover, patterns such as “As * As *” or “about as * as *” have been shown to be useful in detecting ironic similes (Veale et al. 2010). 3 Approach The proposed system consists of two main modules: (a) the preprocessing, and (b) the classification module. Each tweet t was submitted to preprocessing, in order to remove useless information and extract the desired/targeted features f. The result of the preprocessing of a given t</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Antonio Reyes, Paolo Rosso, Tony Veale (2013). A Multidimensional Approach for Detecting Irony in Twitter. Languages Resources and Evaluation 47(1): 239-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Ashequl Qadir</author>
<author>Prafulla Surve</author>
<author>Lalindra De Silva</author>
<author>Nathan Gilbert</author>
</authors>
<title>Ruihong Huang. Sarcasm as Contrast between a Positive Sentiment and Negative Situation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<marker>Riloff, Qadir, Surve, De Silva, Gilbert, 2013</marker>
<rawString>Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalindra De Silva, Nathan Gilbert, Ruihong Huang. Sarcasm as Contrast between a Positive Sentiment and Negative Situation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL</booktitle>
<contexts>
<context position="3910" citStr="Davidov et al. (2010)" startWordPosition="604" endWordPosition="608">yed sentiment. The complexity of such a task is high, especially given the fact that irony and sarcasm are frequently mixed. Sarcasm is usually used for putting down the target of the comment and is somewhat easier to detect. Irony works as a negation, and it can be 709 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 709–713, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics conveyed through a positive context, which makes it difficult to understand the actual meaning of a tweet (Reyes et al. 2012, Veale et al. 2010). Davidov et al. (2010) examined hashtags that indicated sarcasm to identify if such labelled tweets can be a reliable source of sarcasm. They concluded that user-labelled sarcastic tweets can be noisy and constitute the hardest form of sarcasm. Riloff et al. (2013) identify sarcasm that arises from the contrast between a positive sentiment referring to a negative situation. Reyes et al. (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet. Many studies exploit the use of contextual imbalance detection through calcul</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport (2010). Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniruddha Ghosh</author>
<author>Guofu Li</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
</authors>
<title>Ekaterina Shutova, Antonio Reyes,</title>
<date>2015</date>
<booktitle>Proc. Int. Workshop on Semantic Evaluation (SemEval-2015), Co-located with NAACL and *SEM,</booktitle>
<location>John Barnden</location>
<marker>Ghosh, Li, Veale, Rosso, 2015</marker>
<rawString>Aniruddha Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, John Barnden (2015). SemEval-2015 Task 11: Sentiment Analysis of Figurative Language in Twitter. In: Proc. Int. Workshop on Semantic Evaluation (SemEval-2015), Co-located with NAACL and *SEM, Denver, Colorado, US, June 4-5, 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Metaphor Identification Using Verb and Noun Clustering. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4804" citStr="Shutova et al. (2010)" startWordPosition="743" endWordPosition="746">from the contrast between a positive sentiment referring to a negative situation. Reyes et al. (2012) involved in their work features that make use of contextual imbalance, natural language concepts, syntactical and morphological aspects of a tweet. Many studies exploit the use of contextual imbalance detection through calculation of semantic similarity among the words. This is achieved using lexical resources, such as WordNet or Whisel’s dictionary, and the goal is to identify features like emotional content, polarity of words and pleasantness, adverbs implying negation or expressing timing. Shutova et al. (2010) have deployed an unsupervised method to identify metaphor using synonymy information from WordNet. Reyes et al. (2013) argue that other features such as punctuation marks, emoticons, quotes, and capitalized words, n-grams and skip-grams are also useful to the sentiment analysis process. Moreover, patterns such as “As * As *” or “about as * as *” have been shown to be useful in detecting ironic similes (Veale et al. 2010). 3 Approach The proposed system consists of two main modules: (a) the preprocessing, and (b) the classification module. Each tweet t was submitted to preprocessing, in order </context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Ekaterina Shutova, Lin Sun and Anna Korhonen (2010). Metaphor Identification Using Verb and Noun Clustering. In: Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. In:</title>
<date>2009</date>
<booktitle>Proceeding LSM &apos;11 Proceedings of the Workshop on Languages in Social Media</booktitle>
<pages>30--38</pages>
<contexts>
<context position="2388" citStr="Go et al. (2009)" startWordPosition="369" endWordPosition="372"> To address this challenge, we propose a system for sentiment analysis of figurative language, which relies on feature selection and trains a classifier to predict the label of a tweet. Given a labelled trial set, the objective of the system is to correctly determine how positive, negative or neutral a tweet is considered to be on a scale of [-5, 5]. 2 Related Work Tweets have unique characteristics compared to other text corpora, such as emoticons, abbreviations, and hashtags. Use of emoticons is considered a reasonably effective way to conveying emotion (Derks et al. 2008, Thelwall et al.). Go et al. (2009) show that machine learning algorithms achieve accuracy above 80% when trained with emoticon data. It is also indicated that the use of hashtags and presence of intensifiers, such as capitalization and punctuation, can affect sentiment identification (Kouloumpis et al., 2010). According to Agarwal et al. (2011) such features can add value to a classifier, but only marginally. Additionally, natural language related features, such as part-of-speech tagging and use of lexicon resources, can significantly contribute to detecting the sentiment of a tweet. Moreover, features that combine the prior p</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang (2009). Twitter sentiment classification using distant supervision. In: Proceeding LSM &apos;11 Proceedings of the Workshop on Languages in Social Media Pages 30-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daantje Derks</author>
<author>Arjan E R Bos</author>
<author>Jasper von Grumbkow</author>
</authors>
<title>Emoticons and online message interpretation.</title>
<date>2007</date>
<journal>Social Science Computer Review,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>379--388</pages>
<marker>Derks, Bos, von Grumbkow, 2007</marker>
<rawString>Daantje Derks, Arjan E. R. Bos, and Jasper von Grumbkow (2007). Emoticons and online message interpretation. Social Science Computer Review, 26(3), 379-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Thelwall</author>
<author>Kevan Buckley</author>
<author>Georgios Paltoglou</author>
<author>Di Cai</author>
</authors>
<title>Arvid Kappas (2010). Sentiment Strength Detection in Short Informal Text.</title>
<date>2010</date>
<journal>Journal of the American Society for Information Science and Technology</journal>
<volume>61</volume>
<pages>2544--2558</pages>
<marker>Thelwall, Buckley, Paltoglou, Di Cai, 2010</marker>
<rawString>Mike Thelwall, Kevan Buckley, Georgios Paltoglou, and Di Cai, Arvid Kappas (2010). Sentiment Strength Detection in Short Informal Text. Journal of the American Society for Information Science and Technology Volume 61, Issue 12, pages 2544–2558, December 2010</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The Good the Bad and the OMG!</title>
<date>2011</date>
<booktitle>Proceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM’ 11,</booktitle>
<pages>538--541</pages>
<editor>In: Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors,</editor>
<location>Barcelona, Spain.</location>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore (2011). Twitter sentiment analysis: The Good the Bad and the OMG! In: Lada A. Adamic, Ricardo A. Baeza-Yates, and Scott Counts, editors, Proceedings of the Fifth International Conference on Weblogs and Social Media, ICWSM’ 11, pages 538–541, Barcelona, Spain.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Apoorv Agarwal</author>
</authors>
<title>Boyi Xie, Ilia Vovsha, Owen Rambow, Rebecca Passonneau (2011). Sentiment Analysis of Twitter Data. In:</title>
<booktitle>LSM&apos;11 Proceedings of the Workshop on Languages in Social Media</booktitle>
<pages>30--38</pages>
<marker>Agarwal, </marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, Rebecca Passonneau (2011). Sentiment Analysis of Twitter Data. In: LSM&apos;11 Proceedings of the Workshop on Languages in Social Media Pages 30-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data. In:</title>
<date>2013</date>
<booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="8253" citStr="Derczynski et al., 2013" startWordPosition="1297" endWordPosition="1300">s *” pattern and after studying the data set, we further identify in the feature selection process the presence of patterns such as “Don’t you*”, “Oh so*?” and “As * As *”. Cleaning proceeds with punctuation, stop-words, urls, emoticons, hashtags and references removal. Additionally, multiple consecutive letters in a word are reduced to two. Finally, spellchecking is performed to words that have been identified as misspelled in order to deduce the correct word. After cleaning, the process continues with part of speech 710 (POS) tagging. POS-tagging is performed with the use of a custom model (Derczynski et al., 2013) and simplified tags (NN, VB, ADJ, RB). Words that belong to the same part of speech are used in semantic text similarity calculation simt. For this feature, different similarity measures (Resnik’s, Lin’s, and Wu &amp; Palmer’s) provided by nltk are used (Pedersen et al., 2008). The value simt is calculated as the maximum similarity score of every combination of two words and their synonyms. E����+E����+ E ����+ E ���� ���t __ (3) c(V)+c(N)+c(A)+c(R) 51111A __ max(sim(Ai, Ai+1)), ... � 1 (4) max(sim(A._1, A.)) J where V, N, A, and R denote the sets that contain the total words that have been ident</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva (2013). Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data. In: Proceedings of the International Conference on Recent Advances in Natural Language Processing, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 7th Conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>2200--2204</pages>
<location>Valletta, MT,</location>
<contexts>
<context position="9111" citStr="Baccianella et al., 2010" startWordPosition="1439" endWordPosition="1442"> by nltk are used (Pedersen et al., 2008). The value simt is calculated as the maximum similarity score of every combination of two words and their synonyms. E����+E����+ E ����+ E ���� ���t __ (3) c(V)+c(N)+c(A)+c(R) 51111A __ max(sim(Ai, Ai+1)), ... � 1 (4) max(sim(A._1, A.)) J where V, N, A, and R denote the sets that contain the total words that have been identified as verbs, nouns, adjectives and adverbs respectively, while max(sim(Ai, Ai+1)) is the maximum similarity between the processed words and their n synonyms. Finally, the SentiWordNet score for each word in a tweet is calculated (Baccianella et al., 2010), ignoring words that have fewer than two letters. If the score of a word cannot be determined, then we calculate the SentiWordNet score of the stemmed word. Given that the word wi occurs j times in the SentiWordNet corpus, the total score of wi is given by SW.SCOrewi — Ek=11+�wScore(i,k)p—wscore( i,k)n (5) — � where wScore(i, k)p and wScore(i, k)n is the k-th positive (PosScore) and negative (NegScore) score respectively of wi in SentiWordNet. The index i of each word was used in an attempt to correlate each word’s position with the calculated sentiment. Moreover, the total score of a tweet t</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani (2010). SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In: Proceedings of the 7th Conference on Language Resources and Evaluation (LREC 2010), Valletta, MT, 2010, pp. 2200-2204.</rawString>
</citation>
<citation valid="true">
<title>ed.) WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<institution>Christiane Fellbaum</institution>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts. In: Demonstration papers at HLT-NAACL,</title>
<date>2004</date>
<pages>38--42</pages>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. (2004). Wordnet::similarity - measuring the relatedness of concepts. In: Demonstration papers at HLT-NAACL, pages 38-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>In Journal of Machine Learning Research</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<marker>Pedregosa, 2011</marker>
<rawString>Fabian Pedregosa et al. (2011). Scikit-learn: Machine Learning in Python. In Journal of Machine Learning Research 12, pp. 2825-2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python,</booktitle>
<location>O&apos;Reilly Media.</location>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper (2009), Natural Language Processing with Python, O&apos;Reilly Media.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ryan Kelly</author>
</authors>
<volume>https://pythonhosted.org/pyenchant/,</volume>
<pages>1--6</pages>
<marker>Kelly, </marker>
<rawString>Ryan Kelly, https://pythonhosted.org/pyenchant/, v. 1.6.5.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>