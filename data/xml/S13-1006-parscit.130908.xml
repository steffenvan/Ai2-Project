<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000368">
<title confidence="0.735776">
iKernels-Core: Tree Kernel Learning for Textual Similarity
</title>
<author confidence="0.941435">
Aliaksei Severyn&apos; and Massimo Nicosia&apos; and Alessandro Moschitti&apos;,2
</author>
<affiliation confidence="0.982947">
&apos;University of Trento, DISI, 38123 Povo (TN), Italy
</affiliation>
<email confidence="0.880154">
{severyn,m.nicosia,moschitti}@disi.unitn.it
</email>
<address confidence="0.624109">
2Qatar Foundation, QCRI, Doha, Qatar
</address>
<email confidence="0.997028">
{amoschitti}@qf.org.qa
</email>
<sectionHeader confidence="0.998591" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914263157895">
This paper describes the participation of iKer-
nels system in the Semantic Textual Similar-
ity (STS) shared task at *SEM 2013. Different
from the majority of approaches, where a large
number of pairwise similarity features are
used to learn a regression model, our model
directly encodes the input texts into syntac-
tic/semantic structures. Our systems rely on
tree kernels to automatically extract a rich set
of syntactic patterns to learn a similarity score
correlated with human judgements. We ex-
periment with different structural representa-
tions derived from constituency and depen-
dency trees. While showing large improve-
ments over the top results from the previous
year task (STS-2012), our best system ranks
21st out of total 88 participated in the STS-
2013 task. Nevertheless, a slight refinement to
our model makes it rank 4th.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979458333334">
Comparing textual data to establish the degree of se-
mantic similarity is of key importance in many Nat-
ural Language Processing (NLP) tasks ranging from
document categorization to textual entailment and
summarization. The key aspect of having an accu-
rate STS framework is the design of features that can
adequately represent various aspects of the similar-
ity between texts, e.g. using lexical, syntactic and
semantic similarity metrics.
The majority of approaches to semantic textual
similarity treat the input text pairs as feature vec-
tors where each feature is a score corresponding to a
certain type of similarity. This approach is concep-
tually easy to implement and STS-2012 (Agirre et
al., 2012) has shown that the best systems were built
following this idea, i.e. a number of features encod-
ing similarity of an input text pair were combined in
a single scoring model, such as Linear Regression
or Support Vector Regression (SVR). One potential
limitation of using only similarity features to repre-
sent a text pair is that of low representation power.
The novelty of our approach is that we encode the
input text pairs directly into structural objects, e.g.
trees, and rely on the power of kernel learning to ex-
tract relevant structures. This completely different
from (Croce et al., ), where tree kernels where used
to establish syntactic similarity and then plugged as
similarity features. To link the documents in a pair
we mark the nodes in the related structures with a
special relational tag. In this way effective struc-
tural relational patterns are implicitly encoded in the
trees and can be automatically learned by the kernel-
based machine learning methods. We build our sys-
tems on top of the features used by two best systems
from STS-2012 and combine them with the tree ker-
nel models within the Support Vector Regression to
derive a single scoring model. Since the test data
used for evaluation in STS-2013 (Agirre et al., 2013)
is different from the 2012 data provided for the sys-
tem development, domain adaptation represents an
additional challenge. To address this problem we
augment our feature vector representation with fea-
tures extracted from a text pair as a whole to capture
individual properties of each dataset. Additionally,
we experiment with a corpus type classifier and in-
clude its prediction score as additional features. Fi-
nally, we use stacking to combine several structural
models into the feature vector representation.
</bodyText>
<page confidence="0.982687">
53
</page>
<subsubsectionHeader confidence="0.254052">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
</subsubsectionHeader>
<bodyText confidence="0.9063585">
and the Shared Task, pages 53–58, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
In the following sections we describe our ap-
proach to combine structural representations with
the pairwise similarity features in a single SVR
learning framework. We then report results on both
STS-2012 and 2013 tasks.
</bodyText>
<sectionHeader confidence="0.938155" genericHeader="method">
2 Structural Relational Similarity
</sectionHeader>
<bodyText confidence="0.999147">
In this section we first describe the kernel framework
to combine structural and vector models, then we
explain how to construct the tree models and briefly
describe tree kernels we use to automatically extract
the features.
</bodyText>
<subsectionHeader confidence="0.993825">
2.1 Structural Kernel Learning
</subsectionHeader>
<bodyText confidence="0.973967117647059">
In supervised learning, given the labeled data
{(xi,yi)lni�1, the goal is to estimate a decision func-
tion h(x) = y that maps input examples to the tar-
get variables. A conventional approach is to rep-
resent a pair of texts as a set of similarity features
{fil, s.t. the predictions are computed as h(x) =
w · x = Ei wifi, where w is the model weight vec-
tor. Hence, the learning problem boils down to es-
timating the individual weight of each of the sim-
ilarity feature fi. One downside of such approach
is that a great deal of similarity information carried
by a given text pair is lost when modeled by single
real-valued scores.
A more versatile approach in terms of the input
representation relies on kernels. In a typical ker-
nel machine, e.g. SVM, the prediction function for
a test input x takes on the following form h(x) =
</bodyText>
<equation confidence="0.768854">
E
i αiyiK(x,xi), where αi are the model parame-
</equation>
<bodyText confidence="0.979292777777778">
ters estimated from the training data, yi - target vari-
ables, xi are support vectors, and K(·, ·) is a kernel
function.
To encode both structural representation and sim-
ilarity feature vectors of input text pairs xi in a sin-
gle model, we treat it as the following tuple: xi =
(xai ,xbi) = ((tai ,vai ), (tbi,vbi)), where xai xbi are the
first and the second document of xi, and t and v de-
note tree and vector representations respectively.
To compute a kernel between two text pairs xi
and xj we define the following all-vs-all kernel,
where all possible combinations of documents from
each pair are considered: K(xi,xj) = K(xai ,xaj) +
K(xai ,xbj) + K(xbi,xaj) + K(xbi,xbj). Each of the
kernel computations K between two documents xa
and xb can be broken down into the following:
K(xa,xb) = KTK(ta,tb) + Kfvec(va,vb), where
KTK computes a tree kernel and Kfvec is a kernel
over feature vectors, e.g. linear, polynomial or RBF,
etc. Further in the text we refer to structural tree
kernel models as TK and explicit feature vector rep-
resentation as fvec.
Having defined a way to jointly model text pairs
using structural TK representations along with the
similarity features fvec, we next briefly review tree
kernels and our relational structures derived from
constituency and dependency trees.
</bodyText>
<subsectionHeader confidence="0.999365">
2.2 Tree Kernels
</subsectionHeader>
<bodyText confidence="0.99979375">
We use tree structures as our base representation
since they provide sufficient flexibility in represen-
tation and allow for easier feature extraction than,
for example, graph structures. We use a Partial Tree
Kernel (PTK) (Moschitti, 2006) to take care of auto-
matic feature extraction and compute KTK(·, ·).
PTK is a tree kernel function that can be ef-
fectively applied to both constituency and depen-
dency parse trees. It generalizes a subset tree ker-
nel (STK) (Collins and Duffy, 2002) that maps a
tree into the space of all possible tree fragments con-
strained by the rule that the sibling nodes from their
parents cannot be separated. Different from STK
where the nodes in the generated tree fragments are
constrained to include none or all of their direct chil-
dren, PTK fragments can contain any subset of the
features, i.e. PTK allows for breaking the production
rules. Consequently, PTK generalizes STK generat-
ing an extremely rich feature space, which results in
higher generalization ability.
</bodyText>
<subsectionHeader confidence="0.998993">
2.3 Relational Structures
</subsectionHeader>
<bodyText confidence="0.99997125">
The idea of using relational structures to jointly
model text pairs was previously proposed in (Sev-
eryn and Moschitti, 2012), where shallow syntactic
structures derived from chunks and part-of-speech
tags were used to represent question/answer pairs.
In this paper, we define novel relational structures
based on: (i) constituency and (ii) dependency trees.
Constituency tree. Each document in a given text
pair is represented by its constituency parse tree.
If a document contains multiple sentences they are
merged in a single tree with a common root. To
encode the structural relationships between docu-
</bodyText>
<page confidence="0.996964">
54
</page>
<figureCaption confidence="0.999842">
Figure 1: A dependency-based structural representation of a text pair. REL tag links related fragments.
</figureCaption>
<bodyText confidence="0.9999504">
ments in a pair a special REL tag is used to link
the related structures. We adopt a simple strategy
to establish such links: words from two documents
that have a common lemma get their parents (POS
tags) and grandparents, non-terminals, marked with
a REL tag.
Dependency tree. We propose to use dependency
relations between words to derive an alternative
structural representation. In particular, dependency
relations are used to link words in a way that words
are always at the leaf level. This reordering of the
nodes helps to avoid the situation where nodes with
words tend to form long chains. This is essential
for PTK to extract meaningful fragments. We also
plug part-of-speech tags between the word nodes
and nodes carrying their grammatical role. Again
a special REL tag is used to establish relations be-
tween tree fragments. Fig. 1 gives an example of
a dependency-based structure taken from STS-2013
headlines dataset.
</bodyText>
<sectionHeader confidence="0.963493" genericHeader="method">
3 Pairwise similarity features.
</sectionHeader>
<bodyText confidence="0.99975625">
Along with the direct representation of input text
pairs as structural objects our framework also en-
codes feature vectors (base), which we describe
below.
</bodyText>
<subsectionHeader confidence="0.994712">
3.1 Baseline features
</subsectionHeader>
<bodyText confidence="0.9999428">
We adopt similarity features from two best perform-
ing systems of STS-2012, which were publicly re-
leased: namely, the Takelab1 system (ˇSari´c et al.,
2012) and the UKP Lab’s system2 (Bar et al., 2012).
Both systems represent input texts with similar-
</bodyText>
<footnote confidence="0.938692333333333">
1http://takelab.fer.hr/sts/
2https://code.google.com/p/dkpro-similarity-
asl/wiki/SemEval2013
</footnote>
<bodyText confidence="0.9986536">
ity features which combine multiple text similarity
measures of varying complexity.
UKP provides metrics based on matching of char-
acter, word n-grams and common subsequences. It
also includes features derived from Explicit Seman-
tic Analysis vector comparisons and aggregation of
word similarity based on lexical-semantic resources,
e.g. WordNet. In total it provides 18 features.
Takelab includes n-gram matching of varying size,
weighted word matching, length difference, Word-
Net similarity and vector space similarity where
pairs of input sentences are mapped into Latent Se-
mantic Analysis (LSA) space (Turney and Pantel,
2010). The features are computed over several sen-
tence representations where stop words are removed
and/or lemmas are used in place of raw tokens.
The total number of Takelab’s features is 21. Even
though some of the UKP and Takelab features over-
lap we include all of them in a combined system with
the total of 39 features.
</bodyText>
<subsectionHeader confidence="0.998197">
3.2 iKernels features
</subsectionHeader>
<bodyText confidence="0.999986692307692">
Here we describe our additional features added to
the fvec representation. First, we note that word
frequencies used to compute weighted word match-
ings and the word-vector mappings to compute LSA
similarities required by Takelab features are pro-
vided only for the vocabulary extracted from 2012
data. Hence, we use both STS-2012 and 2013 data to
obtain the word counts and re-estimate LSA vector
representations. For the former we extract unigram
counts from Google Books Ngrams3, while for the
latter we use additional corpora as described below.
LSA similarity. To construct LSA word-vector
mappings we use the following three sources: (i)
</bodyText>
<footnote confidence="0.952916">
3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html
</footnote>
<page confidence="0.997946">
55
</page>
<bodyText confidence="0.998902534883721">
Aquaint4, which consists of more than 1 million
newswire documents, (ii) ukWaC (Baroni et al.,
2009) - a 2 billion word corpus constructed from
the Web, and (iii) and a collection of documents
extracted from Wikipedia dump5. To extract LSA
topics we use GenSim6 software. We preprocess
the data by lowercasing, removing stopwords and
words with frequency lower than 5. Finally, we ap-
ply tf-idf weighting. For all representations we fix
the number of dimensions to 250. For all corpora
we use document-level representation, except for
Wikipedia we also experimented with a sentence-
level document representation, which typically pro-
vides a more restricted context for estimating word-
document distributions.
Brown Clusters. In addition to vector represen-
tations derived from LSA, we extract word-vector
mappings using Brown word clusters7 (Turian et al.,
2010), where words are organized into a hierarchy
and each word is represented as a bit-string. We
encode each word by a feature vector where each
entry corresponds to a prefix extracted from its bit-
string. We use prefix lengths in the following range:
k = 14, 8,12,16, 20}. Finally, the document is rep-
resented as a feature vector composed by the indi-
vidual word vectors.
Term-overlap features. In addition to the word
overlap features computed by UKP and Takelab
systems we also compute a cosine similarity over
the following representations: (i) n-grams of part-
of-speech tags (up to 4-grams), (ii) SuperSense
tags (Ciaramita and Altun, 2006), (iii) named enti-
ties, and (iv) dependency triplets.
PTK similarity. We use PTK to provide a syn-
tactic similarity score between documents in a pair:
PTK(a, b) = PTK(a, b), where as input represen-
tations we use dependency and constituency trees.
Explicit Semantic Analysis (ESA) similarity.
ESA (Gabrilovich and Markovitch, 2007) represents
input documents as vectors of Wikipedia concepts.
To compute ESA features we use Lucene8 to in-
dex documents extracted from a Wikipedia dump.
Given a text pair we retrieve k top documents (i.e.
</bodyText>
<footnote confidence="0.9999318">
4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/
5http://dumps.wikimedia.org/
6http://radimrehurek.com/gensim/
7http://metaoptimize.com/projects/wordreprs/
8http://lucene.apache.org/
</footnote>
<bodyText confidence="0.9927085">
Wikipedia concepts) and compute the metric by
looking at the overlap of the concepts between the
documents: esak(a, b) = |W- � Wb|
k , where Wa is
the set of concepts retrieved for document a. We
compute esa features with k E 110, 25,50, 100}.
</bodyText>
<subsectionHeader confidence="0.99517">
3.3 Corpus type features
</subsectionHeader>
<bodyText confidence="0.999967217391304">
Here we describe two complementary approaches
(corpus) in an attempt to alleviate the problem of
domain adaptation, where the datasets used for train-
ing and testing are drawn from different sources.
Pair representation. We treat each pair of texts as a
whole and extract the following sets of corpus fea-
tures: plain bag-of-words, dependency triplets, pro-
duction rules of the syntactic parse tree and a length
feature, i.e. a log-normalized length of the combined
text. Each feature set is normalized and added to the
fvec model.
Corpus classifier. We use the above set of features
to train a multi-class classifier to predict for each in-
stance its most likely corpus type. Our categories
correspond to five dataset types of STS-2012. Pre-
diction scores for each of the dataset categories are
then plugged as features into the final fvec repre-
sentation. Our multi-class classifier is a one-vs-all
binary SVM trained on the merged data from STS-
2012. We apply 5-fold cross-validation scheme, s.t.
for each of the held-out folds we obtain independent
predictions. The accuracy (averaged over 5-folds)
on the STS-2012 data is 92.0%.
</bodyText>
<subsectionHeader confidence="0.980323">
3.4 Stacking
</subsectionHeader>
<bodyText confidence="0.999995888888889">
To integrate multiple TK models into a single model
we apply a classifier stacking approach (Fast and
Jensen, 2008). Each of the learned TK models is
used to generate predictions which are then plugged
as features into the final fvec representation, s.t.
the final model uses only explicit feature vector
representation. We apply a 5-fold cross-validation
scheme to obtain prediction scores in the same man-
ner as described above.
</bodyText>
<sectionHeader confidence="0.994249" genericHeader="method">
4 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.983671">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.998821">
To encode TK models along with the similarity fea-
ture vectors into a single regression scoring model,
</bodyText>
<page confidence="0.986114">
56
</page>
<figure confidence="0.826860333333333">
base corpus TK
U T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn
• 0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470
• 0.7589 0.6863 0.6814 0.8637 0.4950 0.7091 0.5395
• • 0.8079 0.7161 0.7134 0.8837 0.5519 0.7343 0.5607
• • • 0.8187 0.7137 0.7157 0.8833 0.5131 0.7355 0.5809
</figure>
<listItem confidence="0.980906285714286">
• • • • 0.8458 0.7047 0.6935 0.8953 0.5080 0.7101 0.5834
• • • • 0.8468 0.6954 0.6717 0.8902 0.4652 0.7089 0.6133
• • • • • 0.8539 0.7132 0.6993 0.9005 0.4772 0.7189 0.6481
• • • • • 0.8529 0.7249 0.7080 0.8984 0.5142 0.7263 0.6700
• • • • • • 0.8546 0.7156 0.6989 0.8979 0.4884 0.7181 0.6609
• • • • • • 0.8810 0.7416 0.7210 0.8971 0.5912 0.7328 0.6778
• • • • • • 0.8705 0.7339 0.7039 0.9012 0.5629 0.7376 0.6656
</listItem>
<table confidence="0.838744">
UKPbest 0.8239 0.6773 0.6830 0.8739 0.5280 0.6641 0.4937
</table>
<tableCaption confidence="0.99020225">
Table 1: System configurations and results on STS-2012. Column set base lists 3 feature sets : UKP (U), Takelab
(T) and iKernels (I); corpus type features (corpus) include plain features (B), corpus classifier (O), and manually
encoded dataset category (M); TK contains constituency (C) and dependency-based (D) models. UKPbest is the best
system of STS-2012. First column shows configuration of our three system runs submitted to STS-2013.
</tableCaption>
<bodyText confidence="0.888505785714286">
Sys,
Sys3
Sys2
we use an SVR framework implemented in SVM-
Light-TK9. We use the following parameter settings
-t 5 -F 3 -W A -C +, which specifies to use
a combination of trees and feature vectors (-C +),
PTK over trees (-F 3) computed in all-vs-all mode
(-W A) and using polynomial kernel of degree 3 for
the feature vector (active by default).
We report the following metrics employed in the
final evaluation: Pearson correlation for individual
test sets10 and Mean – an average score weighted by
the test set size.
</bodyText>
<subsectionHeader confidence="0.997518">
4.2 STS-2012
</subsectionHeader>
<bodyText confidence="0.9999230625">
For STS-2013 task the entire data from STS-2012
was provided for the system development. To com-
pare with the best systems of the previous year we
followed the same setup, where 3 datasets (MSRp,
MSRv and SMTe) are used for training and 5 for test-
ing (two “surprise” datasets were added: OnWN and
SMTn). We use the entire training data to obtain a
single model.
Table 1 summarizes the results using structural
models (TK), pairwise similarity (base) and corpus
type features (corpus). We first note, that com-
bining all three features sets (U, T and I) provides
a good match to the best system UKPbest. Next,
adding TK models results in a large improvement
beating the top results in STS-2012. Furthermore,
using corpus features results in even greater im-
</bodyText>
<footnote confidence="0.822204666666667">
9http://disi.unitn.it/moschitti/Tree-Kernel.htm
10for STS-2012 we also report the results for a concatenation
of all five test sets (ALL)
</footnote>
<bodyText confidence="0.787315">
provement with the Mean = 0.7416 and Pearson
ALL = 0.8810.
</bodyText>
<subsectionHeader confidence="0.984639">
4.3 STS-2013
</subsectionHeader>
<bodyText confidence="0.999968607142857">
Below we specify the configuration for each of the
submitted runs (also shown in Table 1) and report the
results on the STS-2013 test sets: headlines (head),
OnWN, FNWN, and SMT:
Sys,: combines base features (U, T and I), TK
models (C and D) and plain corpus type features (B).
We use STS-2012 data to train a single model.
Sys2: different from Sys, where a single model
trained on the entire data is used to make predictions,
we adopt a different training/test setup to account for
the different nature of the data used for training and
testing. After performing manual analysis of the test
data we came up with the following strategy to split
the training data into two sets to learn two differ-
ent models: STMe and OnWN (model,) and MSRp,
SMTn and STMe (model2); model, is then used to
get predictions for OnWN, FNWN, while model2 is
used for SMT and headlines.
Sys3: same as Sys, + a corpus type classifier O as
described in Sec. 3.3.
Table 2 shows the resulting performance of our
systems and the best UMBC system published in the
final ranking. Sys2 appears the most accurate among
our systems, which ranked 21st out of 88. Compar-
ing to the best system across four datasets we ob-
serve that it performs reasonably well on the head-
lines dataset (it is 5th best), while completely fails
on the OnWN and FNWN test sets. After performing
</bodyText>
<page confidence="0.994671">
57
</page>
<bodyText confidence="0.999942133333333">
error analysis, we found that TK models underper-
form on FNWN and OnWN sets, which appear un-
derrepresented in the training data from STS-2012.
We build a new system (Sys*), which is based on
Sys2, by making two adjustments in the setup: (i)
we exclude SMTe from training to obtain predictions
on SMT and head and (ii) we remove all TK features
to train a model for FNWN and OnWN. This is mo-
tivated by the observation that text pairs from STS-
2012 yield a paraphrase model, since the texts are
syntactically very similar. Yet, two datasets from
STS-2013 FNWN, and OnWN contain text pairs
where documents exhibit completely different struc-
tures. This is misleading for our syntactic similarity
model learned on the STS-2012.
</bodyText>
<tableCaption confidence="0.982963">
Table 2: Results on STS-2013.
</tableCaption>
<sectionHeader confidence="0.997856" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999987058823529">
We have described our participation in STS-2013
task. Our approach treats text pairs as structural
objects which provides much richer representation
for the learning algorithm to extract useful patterns.
We experiment with structures derived from con-
stituency and dependency trees where related frag-
ments are linked with a special tag. Such struc-
tures are then used to learn tree kernel models which
can be efficiently combined with the a feature vector
representation in a single scoring model. Our ap-
proach ranks 1st with a large margin w.r.t. to the
best systems in STS-2012 task, while it is 21st ac-
cording to the final rankings of STS-2013. Never-
theless, a small change in the system setup makes
it rank 4th. Clearly, domain adaptation represents a
big challenge in STS-2013 task. We plan to address
this issue in our future work.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.98558375">
This research has been supported by the Euro-
pean Community’s Seventh Framework Program
(FP7/2007-2013) under the #288024 LIMOSINE
project.
</bodyText>
<sectionHeader confidence="0.997861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999647019607843">
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre. 2012. Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In First Joint Conference on
Lexical and Computational Semantics (*SEM).
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012).
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
EMNLP.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over Dis-
crete Structures, and the Voted Perceptron. In ACL.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. Unitor: Combining semantic text similarity
functions through sv regression. In SemEval 2012.
Andrew S. Fast and David Jensen. 2008. Why stacked
models perform effective collective classification. In
ICDM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI.
A. Moschitti. 2006. Efficient convolution kernels for
dependency and constituent syntactic trees. In ECML.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In SIGIR.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In ACL.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: vector space models of semantics.
J. Artif. Int. Res., 37(1):141–188.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems
for measuring semantic text similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012).
</reference>
<table confidence="0.941601">
System head OnWN FNWN SMT Mean Rank
UMBC 0.7642 0.7529 0.5818 0.3804 0.6181 1
Sys2 0.7465 0.5572 0.3875 0.3409 0.5339 21
Sys1 0.7352 0.5432 0.3842 0.3180 0.5188 28
Sys3 0.7395 0.4228 0.3596 0.3294 0.4919 40
Sys� 0.7538 0.6872 0.4478 0.3391 0.5732 4*
</table>
<page confidence="0.7700575">
2
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400313">
<title confidence="0.828537">iKernels-Core: Tree Kernel Learning for Textual Similarity</title>
<note confidence="0.617506">of Trento, DISI, 38123 Povo (TN), Foundation, QCRI, Doha,</note>
<abstract confidence="0.9968024">This paper describes the participation of iKernels system in the Semantic Textual Similarity (STS) shared task at *SEM 2013. Different from the majority of approaches, where a large number of pairwise similarity features are used to learn a regression model, our model directly encodes the input texts into syntactic/semantic structures. Our systems rely on tree kernels to automatically extract a rich set of syntactic patterns to learn a similarity score correlated with human judgements. We experiment with different structural representations derived from constituency and dependency trees. While showing large improvements over the top results from the previous year task (STS-2012), our best system ranks 21st out of total 88 participated in the STS- 2013 task. Nevertheless, a slight refinement to our model makes it rank 4th.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>GonzalezAgirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<contexts>
<context position="1837" citStr="Agirre et al., 2012" startWordPosition="273" endWordPosition="276"> importance in many Natural Language Processing (NLP) tasks ranging from document categorization to textual entailment and summarization. The key aspect of having an accurate STS framework is the design of features that can adequately represent various aspects of the similarity between texts, e.g. using lexical, syntactic and semantic similarity metrics. The majority of approaches to semantic textual similarity treat the input text pairs as feature vectors where each feature is a score corresponding to a certain type of similarity. This approach is conceptually easy to implement and STS-2012 (Agirre et al., 2012) has shown that the best systems were built following this idea, i.e. a number of features encoding similarity of an input text pair were combined in a single scoring model, such as Linear Regression or Support Vector Regression (SVR). One potential limitation of using only similarity features to represent a text pair is that of low representation power. The novelty of our approach is that we encode the input text pairs directly into structural objects, e.g. trees, and rely on the power of kernel learning to extract relevant structures. This completely different from (Croce et al., ), where tr</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and GonzalezAgirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="3080" citStr="Agirre et al., 2013" startWordPosition="484" endWordPosition="487">o establish syntactic similarity and then plugged as similarity features. To link the documents in a pair we mark the nodes in the related structures with a special relational tag. In this way effective structural relational patterns are implicitly encoded in the trees and can be automatically learned by the kernelbased machine learning methods. We build our systems on top of the features used by two best systems from STS-2012 and combine them with the tree kernel models within the Support Vector Regression to derive a single scoring model. Since the test data used for evaluation in STS-2013 (Agirre et al., 2013) is different from the 2012 data provided for the system development, domain adaptation represents an additional challenge. To address this problem we augment our feature vector representation with features extracted from a text pair as a whole to capture individual properties of each dataset. Additionally, we experiment with a corpus type classifier and include its prediction score as additional features. Finally, we use stacking to combine several structural models into the feature vector representation. 53 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proc</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="9597" citStr="Bar et al., 2012" startWordPosition="1550" endWordPosition="1553"> carrying their grammatical role. Again a special REL tag is used to establish relations between tree fragments. Fig. 1 gives an example of a dependency-based structure taken from STS-2013 headlines dataset. 3 Pairwise similarity features. Along with the direct representation of input text pairs as structural objects our framework also encodes feature vectors (base), which we describe below. 3.1 Baseline features We adopt similarity features from two best performing systems of STS-2012, which were publicly released: namely, the Takelab1 system (ˇSari´c et al., 2012) and the UKP Lab’s system2 (Bar et al., 2012). Both systems represent input texts with similar1http://takelab.fer.hr/sts/ 2https://code.google.com/p/dkpro-similarityasl/wiki/SemEval2013 ity features which combine multiple text similarity measures of varying complexity. UKP provides metrics based on matching of character, word n-grams and common subsequences. It also includes features derived from Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources, e.g. WordNet. In total it provides 18 features. Takelab includes n-gram matching of varying size, weighted word matching, lengt</context>
</contexts>
<marker>Bar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel Bar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="11520" citStr="Baroni et al., 2009" startWordPosition="1830" endWordPosition="1833"> LSA similarities required by Takelab features are provided only for the vocabulary extracted from 2012 data. Hence, we use both STS-2012 and 2013 data to obtain the word counts and re-estimate LSA vector representations. For the former we extract unigram counts from Google Books Ngrams3, while for the latter we use additional corpora as described below. LSA similarity. To construct LSA word-vector mappings we use the following three sources: (i) 3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html 55 Aquaint4, which consists of more than 1 million newswire documents, (ii) ukWaC (Baroni et al., 2009) - a 2 billion word corpus constructed from the Web, and (iii) and a collection of documents extracted from Wikipedia dump5. To extract LSA topics we use GenSim6 software. We preprocess the data by lowercasing, removing stopwords and words with frequency lower than 5. Finally, we apply tf-idf weighting. For all representations we fix the number of dimensions to 250. For all corpora we use document-level representation, except for Wikipedia we also experimented with a sentencelevel document representation, which typically provides a more restricted context for estimating worddocument distributi</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="12916" citStr="Ciaramita and Altun, 2006" startWordPosition="2052" endWordPosition="2055"> are organized into a hierarchy and each word is represented as a bit-string. We encode each word by a feature vector where each entry corresponds to a prefix extracted from its bitstring. We use prefix lengths in the following range: k = 14, 8,12,16, 20}. Finally, the document is represented as a feature vector composed by the individual word vectors. Term-overlap features. In addition to the word overlap features computed by UKP and Takelab systems we also compute a cosine similarity over the following representations: (i) n-grams of partof-speech tags (up to 4-grams), (ii) SuperSense tags (Ciaramita and Altun, 2006), (iii) named entities, and (iv) dependency triplets. PTK similarity. We use PTK to provide a syntactic similarity score between documents in a pair: PTK(a, b) = PTK(a, b), where as input representations we use dependency and constituency trees. Explicit Semantic Analysis (ESA) similarity. ESA (Gabrilovich and Markovitch, 2007) represents input documents as vectors of Wikipedia concepts. To compute ESA features we use Lucene8 to index documents extracted from a Wikipedia dump. Given a text pair we retrieve k top documents (i.e. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/ 5http://dumps.w</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7005" citStr="Collins and Duffy, 2002" startWordPosition="1137" endWordPosition="1140"> fvec, we next briefly review tree kernels and our relational structures derived from constituency and dependency trees. 2.2 Tree Kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures. We use a Partial Tree Kernel (PTK) (Moschitti, 2006) to take care of automatic feature extraction and compute KTK(·, ·). PTK is a tree kernel function that can be effectively applied to both constituency and dependency parse trees. It generalizes a subset tree kernel (STK) (Collins and Duffy, 2002) that maps a tree into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. Different from STK where the nodes in the generated tree fragments are constrained to include none or all of their direct children, PTK fragments can contain any subset of the features, i.e. PTK allows for breaking the production rules. Consequently, PTK generalizes STK generating an extremely rich feature space, which results in higher generalization ability. 2.3 Relational Structures The idea of using relational structures to jointly model tex</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Paolo Annesi</author>
<author>Valerio Storch</author>
<author>Roberto Basili</author>
</authors>
<title>Unitor: Combining semantic text similarity functions through sv regression.</title>
<date>2012</date>
<booktitle>In SemEval</booktitle>
<marker>Croce, Annesi, Storch, Basili, 2012</marker>
<rawString>Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto Basili. Unitor: Combining semantic text similarity functions through sv regression. In SemEval 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Fast</author>
<author>David Jensen</author>
</authors>
<title>Why stacked models perform effective collective classification.</title>
<date>2008</date>
<booktitle>In ICDM.</booktitle>
<contexts>
<context position="15159" citStr="Fast and Jensen, 2008" startWordPosition="2396" endWordPosition="2399">h instance its most likely corpus type. Our categories correspond to five dataset types of STS-2012. Prediction scores for each of the dataset categories are then plugged as features into the final fvec representation. Our multi-class classifier is a one-vs-all binary SVM trained on the merged data from STS2012. We apply 5-fold cross-validation scheme, s.t. for each of the held-out folds we obtain independent predictions. The accuracy (averaged over 5-folds) on the STS-2012 data is 92.0%. 3.4 Stacking To integrate multiple TK models into a single model we apply a classifier stacking approach (Fast and Jensen, 2008). Each of the learned TK models is used to generate predictions which are then plugged as features into the final fvec representation, s.t. the final model uses only explicit feature vector representation. We apply a 5-fold cross-validation scheme to obtain prediction scores in the same manner as described above. 4 Experimental Evaluation 4.1 Experimental setup To encode TK models along with the similarity feature vectors into a single regression scoring model, 56 base corpus TK U T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn • 0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470 • 0.7589 0.6863</context>
</contexts>
<marker>Fast, Jensen, 2008</marker>
<rawString>Andrew S. Fast and David Jensen. 2008. Why stacked models perform effective collective classification. In ICDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="13245" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2103" endWordPosition="2106"> by the individual word vectors. Term-overlap features. In addition to the word overlap features computed by UKP and Takelab systems we also compute a cosine similarity over the following representations: (i) n-grams of partof-speech tags (up to 4-grams), (ii) SuperSense tags (Ciaramita and Altun, 2006), (iii) named entities, and (iv) dependency triplets. PTK similarity. We use PTK to provide a syntactic similarity score between documents in a pair: PTK(a, b) = PTK(a, b), where as input representations we use dependency and constituency trees. Explicit Semantic Analysis (ESA) similarity. ESA (Gabrilovich and Markovitch, 2007) represents input documents as vectors of Wikipedia concepts. To compute ESA features we use Lucene8 to index documents extracted from a Wikipedia dump. Given a text pair we retrieve k top documents (i.e. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/ 5http://dumps.wikimedia.org/ 6http://radimrehurek.com/gensim/ 7http://metaoptimize.com/projects/wordreprs/ 8http://lucene.apache.org/ Wikipedia concepts) and compute the metric by looking at the overlap of the concepts between the documents: esak(a, b) = |W- � Wb| k , where Wa is the set of concepts retrieved for document a. We compute esa fe</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML.</booktitle>
<contexts>
<context position="6758" citStr="Moschitti, 2006" startWordPosition="1095" endWordPosition="1096">Further in the text we refer to structural tree kernel models as TK and explicit feature vector representation as fvec. Having defined a way to jointly model text pairs using structural TK representations along with the similarity features fvec, we next briefly review tree kernels and our relational structures derived from constituency and dependency trees. 2.2 Tree Kernels We use tree structures as our base representation since they provide sufficient flexibility in representation and allow for easier feature extraction than, for example, graph structures. We use a Partial Tree Kernel (PTK) (Moschitti, 2006) to take care of automatic feature extraction and compute KTK(·, ·). PTK is a tree kernel function that can be effectively applied to both constituency and dependency parse trees. It generalizes a subset tree kernel (STK) (Collins and Duffy, 2002) that maps a tree into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. Different from STK where the nodes in the generated tree fragments are constrained to include none or all of their direct children, PTK fragments can contain any subset of the features, i.e. PTK allows </context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>A. Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="7669" citStr="Severyn and Moschitti, 2012" startWordPosition="1243" endWordPosition="1247">all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. Different from STK where the nodes in the generated tree fragments are constrained to include none or all of their direct children, PTK fragments can contain any subset of the features, i.e. PTK allows for breaking the production rules. Consequently, PTK generalizes STK generating an extremely rich feature space, which results in higher generalization ability. 2.3 Relational Structures The idea of using relational structures to jointly model text pairs was previously proposed in (Severyn and Moschitti, 2012), where shallow syntactic structures derived from chunks and part-of-speech tags were used to represent question/answer pairs. In this paper, we define novel relational structures based on: (i) constituency and (ii) dependency trees. Constituency tree. Each document in a given text pair is represented by its constituency parse tree. If a document contains multiple sentences they are merged in a single tree with a common root. To encode the structural relationships between docu54 Figure 1: A dependency-based structural representation of a text pair. REL tag links related fragments. ments in a p</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="12277" citStr="Turian et al., 2010" startWordPosition="1946" endWordPosition="1949">SA topics we use GenSim6 software. We preprocess the data by lowercasing, removing stopwords and words with frequency lower than 5. Finally, we apply tf-idf weighting. For all representations we fix the number of dimensions to 250. For all corpora we use document-level representation, except for Wikipedia we also experimented with a sentencelevel document representation, which typically provides a more restricted context for estimating worddocument distributions. Brown Clusters. In addition to vector representations derived from LSA, we extract word-vector mappings using Brown word clusters7 (Turian et al., 2010), where words are organized into a hierarchy and each word is represented as a bit-string. We encode each word by a feature vector where each entry corresponds to a prefix extracted from its bitstring. We use prefix lengths in the following range: k = 14, 8,12,16, 20}. Finally, the document is represented as a feature vector composed by the individual word vectors. Term-overlap features. In addition to the word overlap features computed by UKP and Takelab systems we also compute a cosine similarity over the following representations: (i) n-grams of partof-speech tags (up to 4-grams), (ii) Supe</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: vector space models of semantics.</title>
<date>2010</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="10367" citStr="Turney and Pantel, 2010" startWordPosition="1651" endWordPosition="1654">eatures which combine multiple text similarity measures of varying complexity. UKP provides metrics based on matching of character, word n-grams and common subsequences. It also includes features derived from Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources, e.g. WordNet. In total it provides 18 features. Takelab includes n-gram matching of varying size, weighted word matching, length difference, WordNet similarity and vector space similarity where pairs of input sentences are mapped into Latent Semantic Analysis (LSA) space (Turney and Pantel, 2010). The features are computed over several sentence representations where stop words are removed and/or lemmas are used in place of raw tokens. The total number of Takelab’s features is 21. Even though some of the UKP and Takelab features overlap we include all of them in a combined system with the total of 39 features. 3.2 iKernels features Here we describe our additional features added to the fvec representation. First, we note that word frequencies used to compute weighted word matchings and the word-vector mappings to compute LSA similarities required by Takelab features are provided only fo</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: vector space models of semantics. J. Artif. Int. Res., 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsi´c.</title>
<date></date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>ˇSari´c, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. Takelab: Systems for measuring semantic text similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>