<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001250">
<title confidence="0.995794">
Parsing Formal Languages using Natural Language Parsing Techniques
</title>
<author confidence="0.99963">
Jens Nilsson* Welf L¨owe* Johan Hall†* Joakim Nivre†*
</author>
<affiliation confidence="0.9966225">
*V¨axj¨o University, School of Mathematics and Systems Engineering, Sweden
†Uppsala University, Department of Linguistics and Philology, Sweden
</affiliation>
<email confidence="0.990786">
{jens.nilsson|welf.lowe|johan.hall|joakim.nivre}@vxu.se
</email>
<sectionHeader confidence="0.997274" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999698842105263">
Program analysis tools used in software
maintenance must be robust and ought to
be accurate. Many data-driven parsing ap-
proaches developed for natural languages
are robust and have quite high accuracy
when applied to parsing of software. We
show this for the programming languages
Java, C/C++, and Python. Further studies
indicate that post-processing can almost
completely remove the remaining errors.
Finally, the training data for instantiating
the generic data-driven parser can be gen-
erated automatically for formal languages,
as opposed to the manually development
of treebanks for natural languages. Hence,
our approach could improve the robust-
ness of software maintenance tools, proba-
bly without showing a significant negative
effect on their accuracy.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954474576272">
Software engineering, especially software mainte-
nance, is supported by numerous program anal-
ysis tools. Maintenance tasks include program
comprehension (understanding unknown code for
fixing bugs or further development), quality as-
sessment (judging code, e.g., in code reviews),
and reverse-engineering (reifying the design doc-
uments for given source code). To extract infor-
mation from the programs, the tools first parse the
program code and produce an abstract syntax tree
(AST) for further analysis and abstraction (Strein
et al., 2007). As long as the program conforms
to the syntax of a programming language, clas-
sical parsing techniques known from the field of
compiler construction may be applied. This, how-
ever, cannot be assumed in general, as the pro-
grams to analyze can be incomplete, erroneous, or
conform to a (yet unknown) dialect or version of
the language. Despite error stabilization, classi-
cal parsers then lose a lot of information or simply
break down. This is unsatisfactory for tools sup-
porting maintenance. Therefore, quite some effort
has gone into the development of robust parsers of
programs for these tools (cf. our related work sec-
tion 5). This effort, however, has to be repeated
for every programming language.
The development of robust parsers is of special
interest for languages like C/C++ due to their nu-
merous dialects in use (Anderson, 2008). Also,
tools for languages frequently coming in new ver-
sions, like Java, benefit from robust parsing. Fi-
nally, there are languages like HTML where exist-
ing browsers are forgiving if documents do not ad-
here to the formal standard with the consequence
that there exist many formally erroneous docu-
ments. In such cases, robust parsing is even a pre-
requisite for tool-supported maintenance.
The accuracy of parsing is a secondary goal
in the context of software maintenance. Tasks
like program comprehension, quality assessment,
and reverse-engineering are fuzzy by their nature.
There is no well-defined notion of correctness—
rather an empirical answer to the question: Did
it help the software engineers in fulfilling their
tasks? Moreover, the information provided to the
engineers abstracts anyway from the concrete pro-
gram syntax and semantics, i.e., inaccuracies in
the input may disappear in the output. Finally, pro-
gram analyses are often heuristics themselves, ap-
proximating computationally hard problems like
pattern matching and optimal clustering.
The natural language processing (NLP) com-
munity has for many years developed parsing tech-
nology that is both completely robust and highly
accurate. The present approach applies this tech-
nology to programming languages. It is robust in
the sense that, for each program, the parser always
gives a meaningful model even for slightly incor-
rect and incomplete programs. The approach is,
</bodyText>
<page confidence="0.994271">
49
</page>
<note confidence="0.8774645">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 49–60,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999961388888889">
however, not accurate to 100%, i.e., even correct
programs may lead to slightly incorrect models.
As we will show, it is quite accurate when applied
to programming languages.
The data-driven dependency parsing approach
applied here only needs correct examples of the
source and the expected analysis model. Then it
automatically trains and adapts a generic parser.
As we will show, training data for adapting to a
new programming language can even be gener-
ated automatically. Hence, the effort for creating
a parser for a new programming language is quite
small.
The basic idea – applying natural language pars-
ing to programming languages – has been pre-
sented to the program maintenance community be-
fore (Nilsson et al., 2009). This paper contributes
with experimental results on
</bodyText>
<listItem confidence="0.98772475">
1. data-driven dependency parsing of the pro-
gramming languages C/C++, Java, and
Python,
2. transformations between dependency struc-
ture and phrase structure adapted to program-
ming languages,
3. generic parser model selection and its effect
on parsing accuracy.
</listItem>
<bodyText confidence="0.999879142857143">
Section 2 gives an introduction to the parsing tech-
nology applied here. In section 3, the preparation
of the training examples necessary is described,
while section 4 presents the experimental results.
Section 5 discusses related work in information
extraction for software maintenance. We end with
conclusions and future work in section 6.
</bodyText>
<sectionHeader confidence="0.995994" genericHeader="method">
2 NLP Background
</sectionHeader>
<bodyText confidence="0.999830285714286">
Dependency structure is one way of representing
the syntax of natural languages. Dependency trees
form labeled, directed and rooted trees, as shown
in figure 1. One essential difference compared to
context-free grammar is the absence of nontermi-
nals. Another difference is that the syntactic struc-
ture is composed of lexical tokens (also called ter-
minals or words) linked by binary and directed re-
lations called dependencies. Each token in the fig-
ure is labeled with a part-of-speech, shown at the
bottom of the figure. Each dependency relation is
also labeled.
The parsing algorithm used in the experiments
of section 4, known as the Nivre’s arc-eager al-
</bodyText>
<figureCaption confidence="0.992529">
Figure 1: Sentence with a dependency tree.
</figureCaption>
<bodyText confidence="0.999851">
gorithm (Nivre, 2003), can produce such depen-
dency trees. It bears a resemblance to the shift-
reduce parser for context-free grammars, with the
most apparent difference being that terminals (not
nonterminals) are pushed onto the stack. Parser
configurations are represented by a stack, a list
of (remaining) input tokens, and the (current) set
of arcs for the dependency tree. Similar to the
shift-reduce parser, the construction of syntactic
structure is created by a sequence of transitions.
The parser starts with an empty stack and termi-
nates when the input queue is empty, parsing in-
put from left to right. It has four transitions (Left-
Arc, Right-Arc, Reduce and Shift), manipulating
these data structures. The algorithm has a linear
time complexity as it is guaranteed to terminate
after at most 2n transitions, given that the length
of the input sentence is n.
In contrast to a parser guided by a grammar
(e.g., ordinary shift-reduce parsing for context-
free grammars), this parser is guided by a clas-
sifier induced from empirical data using machine
learning (Nivre et al., 2004). Hence, the parser re-
quires training data containing dependency trees.
In other words, the parser has a training phase
where the training data is used by the training
module in order to learn the correct sequence of
transitions. The training data can contain depen-
dency trees for sentences of any language irrespec-
tively of whether the language is a natural or for-
mal one.
The training module produces the correct tran-
sition sequences using the dependency trees of
the training data. These correct parser configura-
tions and transition sequences are then provided as
training data to a classifier, which predicts the cor-
rect transitions (including a dependency label for
Left-Arc, Right-Arc) given parser configurations.
A parser configuration contains a vast amount of
information located in the data-structures. It is
therefore necessary to abstract it into a set of fea-
tures. Possible features are word forms and parts-
</bodyText>
<page confidence="0.992391">
50
</page>
<bodyText confidence="0.999250333333333">
of-speech of tokens on the stack and in the list
of input tokens, and dependency labels of depen-
dency arcs created so far.
The parser produces exactly one syntactic anal-
ysis for every input, even if the input does not con-
form to a grammar. The price we have to pay for
this robustness is that any classifier is bound to
commit errors even if the input is acceptable ac-
cording to a grammar.
</bodyText>
<sectionHeader confidence="0.987697" genericHeader="method">
3 General Approach
</sectionHeader>
<bodyText confidence="0.992213642857143">
In section 2, we presented a parsing algorithm for
producing dependency trees for natural languages.
Here we will show how it can be used to produce
syntactic structures for programming languages.
Since the framework requires training data form-
ing correct dependency trees, we need an approach
for converting source code to dependency trees.
The general approach can be divided into two
phases, training and production. In order to be
able to perform both these phases in this study, we
need to adapt natural language parsing to the needs
of information extraction from programming lan-
guage code, i.e., we need to automatically produce
training data. Therefore, we apply:
</bodyText>
<listItem confidence="0.998720727272727">
(a) Source Code ⇒ Syntax Tree: the classical
approach for generating syntax trees for cor-
rect and complete source code of a program-
ming language.
(b) Syntax Tree ⇒ Dependency Tree: an ap-
proach for encoding the syntax trees as de-
pendency trees adapted to programming lan-
guages.
(c) Dependency Tree ⇒ Syntax Tree: an ap-
proach to convert the dependency trees back
to syntax trees.
</listItem>
<bodyText confidence="0.6544445">
These approaches have been accomplished as pre-
sented below. In the training phase, we need to
train and adapt the generic parsing approach to a
specific programming language. Therefore:
</bodyText>
<listItem confidence="0.993161666666667">
(1) Generate training data automatically by
producing syntax trees and then dependency
trees for correct programs using approaches
(a) and (b).
(2) Train the generic parser with the training
data.
</listItem>
<bodyText confidence="0.9846004">
This automated training phase needs to be done
for every new programming language we adapt to.
Finally, in the production phase, we extract the in-
formation from (not necessarily correct and com-
plete) programs:
</bodyText>
<listItem confidence="0.997068">
(3) Parse the new source code into dependency
trees.
(4) Convert the dependency trees into syntax
trees using approach (c).
</listItem>
<bodyText confidence="0.999301222222222">
This automated production phase needs to be exe-
cuted for every project we analyze.
Steps (2) and (3) have already been discussed in
section 2 for parsing natural languages. They can
be generalized to parsing programming languages
as described in section 3.1. Both the training phase
and the production phase are complete, once the
steps (a)–(c) have been accomplished. We present
them in sections 3.2, 3.3, and 3.4, respectively.
</bodyText>
<subsectionHeader confidence="0.999749">
3.1 Adapting the Input
</subsectionHeader>
<bodyText confidence="0.99998625">
As mentioned, the parsing algorithm described
in section 2 has been developed for natural lan-
guages, which makes it necessary to resolve a
number of issues that arise when the parser is
adapted for source code as input. First, the parsing
algorithm takes a sequence of words as input, and
for simplicity, we map the tokens in a program-
ming language to words.
One slightly more problematic issue is how to
define a “sentence” in source code. A natural
language text syntactically decomposes into a se-
quence of sentences in a relatively natural way.
But is there also a natural way of splitting source
code into sentences? The most apparent approach
may be to define a sentence as a compilation unit,
that is, a file of source code. This can however re-
sult in practical problems since a sentence in a nat-
ural language text is usually on average between
15–25 words long, partially depending on the au-
thor and the type of text. The sequence of tokens
in a source file may on the other hand be much
longer. Time complexity is usually in practice of
less importance when the average sentence length
is as low as in natural languages, but that is hardly
the case when there can be several thousands to-
kens in a sentence to parse.
Other approaches could for instance be to let
one method be a sentence. However, then we need
to deal with other types of source code construc-
tions explicitly. We have in this study for sim-
plicity let one compilation unit be one sentence.
This is possible in practice due to the linear time
</bodyText>
<page confidence="0.991374">
51
</page>
<bodyText confidence="0.99993025">
complexity of the parsing algorithm of section 2,
a quite unusual property compared to other NLP
parsers guided by machine learning with state-of-
the-art accuracy.
</bodyText>
<subsectionHeader confidence="0.998885">
3.2 Source Code ⇒ Syntax Tree
</subsectionHeader>
<bodyText confidence="0.999374222222222">
In order to produce training data for the parser
for a programming language, an analyzer that
constructs syntax trees for correct and complete
source code of the programming language is
needed. We are in this study focusing on Java,
Python and C/C++, and consequently need one
such analyzer for each language. For example, fig-
ure 2 shows the concrete syntax tree of the follow-
ing fragments of Java:
</bodyText>
<construct confidence="0.688000375">
Example (1):
public String getName() {
return name;
}
Example (2):
while (count &gt; 0) {
stack[--count]=null;
}
</construct>
<bodyText confidence="0.999964933333333">
We also map the output of the lexical ana-
lyzer to the parts-of-speech for the words (e.g.,
Identifier for String and getName). All
source code comments and indentation informa-
tion (except for Python where the indentation con-
veys hierarchical information) have been excluded
from the syntax trees. All string and character
literals have also been mapped to “string” and
“char”, respectively. This does not entail that the
approach is lossy, since all this information can
be retained in a post-processing step, if neces-
sary. As pointed out by, for instance, Collard et
al. (2003), comments and indentation may among
other things be of interest when trying to under-
stand source code.
</bodyText>
<subsectionHeader confidence="0.998778">
3.3 Syntax Tree ⇒ Dependency Tree
</subsectionHeader>
<bodyText confidence="0.9999049">
Here we will discuss the conversion of syntax trees
into dependency trees. We use a method that has
been successfully applied for natural languages
for converting syntax trees into a convertible de-
pendency tree that makes it possible to perform
the inverse conversion, meaning that information
about the syntax tree is saved in complex arc la-
bels (Hall and Nivre, 2008). We also present re-
sults in section 4 using the dependency trees that
cannot be used for the inverse conversion, which
we call non-convertible dependency trees.
The conversion is performed in a two-step ap-
proach. First, the algorithm traverses the syntax
tree from the root and identifies the head-child and
the terminal head for all nonterminals in a recur-
sive depth-first search. To identify the head-child
for each nonterminal, the algorithm uses heuristics
called head-finding rules, inspired by, for instance,
Magerman (1995). Three head-finding strategies
have been investigated. For each nonterminal:
</bodyText>
<listItem confidence="0.996642545454545">
1. FREQ: Let the element with the most fre-
quently occurring name be the head, but ex-
clude the token ‘;’ as a potential head. If two
tokens have the same frequency, let the left-
most occurring element be the head.
2. LEFT: let the leftmost terminal in the entire
subtree of the nonterminal be the head of all
other elements.
3. RIGHT: let the rightmost terminal in the en-
tire subtree of the nonterminal be the head of
all other elements.
</listItem>
<bodyText confidence="0.995931964285714">
The dependency trees in figures 3 and 4 use LEFT
and FREQ. LEFT and RIGHT induce that all arcs
are pointing to the right and left, respectively. The
head-finding rules for FREQ are automatically cre-
ated by counting the children’s names for each
distinct non-terminal name in the syntax trees of
the training data. The priority list is then com-
piled by ordering the elements by descending fre-
quency for each distinct non-terminal name. For
instance, given that the syntax trees are grammati-
cally correct, every non-terminal While will con-
tain the tokens (, ) and while. These tokens
have thus the highest priority, and while there-
fore becomes the head in the lower dependency
tree of figure 4. This is the same as choosing the
left-most mandatory element for each left-hand
side in the grammar. An interesting observation
is that binary operators and the copy assignment
operator become the heads of their operands for
FREQ, which is the case for &lt; and = in figure 4.
Note also that the element names of terminals act
as part-of-speech tags, e.g., the part-of-speech for
Stringis Identifier.
In the second step, a dependency tree is created
according to the identified terminal heads. The
arcs in the convertible dependency tree are labeled
with complex arc labels, where each complex arc
label consists of two sublabels:
</bodyText>
<page confidence="0.996792">
52
</page>
<figureCaption confidence="0.9997115">
Figure 2: Syntax trees for examples (1) and (2).
Figure 3: Non-convertible dependency trees for example (1) using LEFT (upper) and FREQ (lower).
</figureCaption>
<bodyText confidence="0.967149631578947">
1. Encode the dependent spine, i.e., the se-
quence of nonterminal labels from the de-
pendent terminal to the highest nonterminal
where the dependent terminal is the terminal
head; “|” separates the nonterminal labels,
2. Encode the attachment point in the head
spine, a non-negative integer value a, which
means that the dependent spine is attached a
steps up in the head spine.
By encoding the arc labels with these two subla-
bels, it is possible to perform the inverse conver-
sion, (see subsection 3.4).
The non-convertible dependency labels allow us
to reduce the complexity of the arc labels, making
the learning problem simpler due to fewer distinct
arc labels. This may result in a higher accuracy
during parsing and can be used as input for fur-
ther processing directly without taking the detour
back to syntax trees. This can be motivated by
the fact that all information in the syntax trees is
usually not needed anyway in many reverse engi-
neering tasks, but labels indicating method calls
and declarations – the most important information
for most program comprehension tasks – are pre-
served. This is exemplified by the fact that both
dependency structures in figure 3 contain the la-
bel MethodsDecl.. We thus believe that all the
necessary information is also captured in this less
informative dependency tree. Each dependency la-
bel is the highest nonterminal name of the spine,
that is, the single nonterminal name that is closest
to its head. The non-convertible dependency label
also excludes the attachment point value, making
the learning problem even simpler. Figures 3 and
4 show the non-convertible dependency labels of
the syntax trees (or phrase structure trees) in the
same figures, where each label contains just a sin-
gle nonterminal name of the original syntax trees.
</bodyText>
<subsectionHeader confidence="0.8672">
3.4 Dependency Tree ⇒ Syntax Tree
</subsectionHeader>
<bodyText confidence="0.999553">
The inverse conversion is a bottom-up and top-
down process on the convertible dependency tree
</bodyText>
<page confidence="0.997775">
53
</page>
<figureCaption confidence="0.999489">
Figure 4: Non-convertible dependency trees for example (2) using LEFT (upper) and FREQ (lower).
</figureCaption>
<bodyText confidence="0.999902529411765">
(must contain complex arc labels). First, the algo-
rithm visits every terminal in the convertible de-
pendency tree and restores the spines of nontermi-
nals with labels for each terminal using the infor-
mation in the first sublabel of the incoming arc.
Thus, the bottom-up process results in a spine of
zero or more arcs from each terminal to the highest
nonterminal of which the terminal is the terminal
head. Secondly, the spines are weaved together ac-
cording to the arcs of the dependency tree. This is
achieved by traversing the dependency tree recur-
sively from the root using a pre-order depth-first
search, where the dependent spine is attached to
its head spine or to the root of the syntax tree. The
attachment point a, given by the second sublabel,
specifies the number of nonterminals between the
terminal head and the attachment nonterminal.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9991351">
We will in this section present parsing experiments
and evaluate the accuracy of the syntax trees pro-
duced by the parser. As mentioned in section 2,
the parsing algorithm is robust in the sense that it
always produces a syntactic analysis no matter the
input, but it can commit errors even for correct in-
put. This section investigates the accuracy for cor-
rect input, when varying feature set, head-finding
rules and language. We begin with the experimen-
tal setup.
</bodyText>
<subsectionHeader confidence="0.950177">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999305">
The open-source software MaltParser (malt-
parser.org) (Nivre et al., 2006) is used in the ex-
periments. It contains an implementation of the
parsing algorithm, as well as an implementation
of the conversion strategy from syntax trees to
dependency trees and back, presented in subsec-
tions 3.3 and 3.4. It comes with the machine
learner LIBSVM (Chang and Lin, 2001), pro-
ducing the most accurate results for parsing nat-
ural languages compared to other evaluated ma-
chine learners (Hall et al., 2006). LIBSVM re-
quires training data. The source files of the follow-
ing projects have been converted into dependency
trees:
</bodyText>
<listItem confidence="0.976551090909091">
• For Java: Recoder 0.83 (Gutzmann et al.,
2007), using all source files in the directory
“src” (having 400 source files with 92k LOC
and 335k tokens).
• For C/C++: Elsa 2005.08.22b (McPeak,
2005), where 1389 source files were used,
including the 978 C/C++ benchmark files in
the distribution (thus comprising 1389 source
files with 265k LOC and 691k tokens).
• For Python: Natural Language Toolkit
0.9.5 (Bird et al., 2008), where all source files
</listItem>
<bodyText confidence="0.927929">
in the directory “nltk” were used (having 160
source files with 65k LOC and 280k tokens).
To construct the syntax tree for the source code
file of Recoder, we have used Recoder. It cre-
ates an abstract syntax tree for a source file, but
we are currently interested in the concrete syntax
tree with all the original tokens. In this first con-
version step, the tokens of the syntax trees are thus
retained. For example, the syntax trees in figure 2
are generated by Recoder.
</bodyText>
<page confidence="0.991958">
54
</page>
<bodyText confidence="0.999963642857143">
The same strategy was adopted for Elsa with the
difference that CDT 4.0.3, a plug-in to the Eclipse
IDE to produce syntax trees for source code of
C/C++, was used for producing the abstract syntax
trees.1 It produces abstract syntax trees just like
Recoder, so the concrete syntax trees have also
been created by retaining the tokens.
The Python 2.5 interpreter is actually shipped
with an analyzer that produces concrete syn-
tax trees (using the Python imports from
ast import PyCF ONLY AST and import
parser), which we have utilized for the Python
project above. Hence, no additional processing is
needed in order prepare the concrete syntax trees
as training data.
For the experiments, the source files have been
divided into a training set T and a development
test set D, where the former comprises 80% of the
dependency trees and the latter 10%. The remain-
ing 10% (E) has been left untouched for later use.
The source files have been ordered alphabetically
by the file names including the path. The depen-
dency trees have then been distributed into the data
sets in a pseudo-randomized way. Every tenth de-
pendency tree starting at index 9 (i.e. dependency
trees 9, 19, 29, ...) will belong to D, and every
tenth dependency trees starting at index 0 to E.
The remaining trees constitute the training set T.
</bodyText>
<subsectionHeader confidence="0.943961">
4.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999962055555556">
The standard evaluation metric for parse trees for
natural languages based on context-free grammar
is F-score, the harmonic mean of precision and
recall. F-score compares constituents – defined
by triples (i, j, XP) spanning between terminals
i and j – derived from the test data with those
derived from the parser. A constituent in the
parser output matches a constituent in the test data
when they span over the same terminals in the
input string. Recall is the ratio of matched con-
stituents over all constituents in the test data. Pre-
cision is the ratio of matched constituents over
all constituents found by the parser. F-score
comes in two versions, one unlabeled (FU) and
one labeled (FL), where each correct constituent
in the latter also must have the correct nontermi-
nal name (i.e., XP). The metric is implemented
in Evalb (Collins and Sekine, 2008).
</bodyText>
<footnote confidence="0.9498095">
1It is worth noting that CDT failed to produce syntax trees
for 2.2% of these source files, which were consequently ex-
cluded from the experiments. This again indicates the diffi-
cult of parsing C/C++ due to its different dialects.
</footnote>
<table confidence="0.98777425">
FR FL RI FR FU RI
LE LE
UL 82.1 93.5 74.6 92.3 97.9 90.6
L 89.7 97.7 80.8 95.8 99.3 92.1
</table>
<tableCaption confidence="0.982297">
Table 1: F-score for various parser models and
</tableCaption>
<bodyText confidence="0.940141555555555">
head-finding rules for Java, where FR = FREQ, LE
= LEFT and RI = RIGHT.
The standard evaluation metric measuring accu-
racy for dependency parsing for natural language
is, on the other hand, labeled (ASL) and unlabeled
(ASU) attachment score. ASU is the ratio of to-
kens attached to its correct head. ASL is the same
as ASU with the additional requirement that the
dependency label should be correct as well.
</bodyText>
<subsectionHeader confidence="0.686616">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999637606060606">
This section presents the parsing results. The first
experiment was conducted for Java, using the in-
verse transformation back to syntax trees. Two
feature models are evaluated, one unlexicalized
feature sets (UL) containing 13 parts-of-speech
and 4 dependency label features, and one lexical-
ized feature sets (L) containing all these 17 fea-
tures and 13 additional word form features, de-
veloped by manual feature optimization. Table 1
compares these two feature sets, as well as the dif-
ferent head-finding rules discussed previously.
The figures give a clear answer to the question
whether lexical information is beneficial or not.
Every figure in the row L is higher than its cor-
responding figure in the row UL. This means that
names of variables, methods, classes, etc., actu-
ally contain valuable information for the classifier.
This is in contrast to ordinary syntactic parsing us-
ing a grammar of programming languages where
all names are mapped to the same value (e.g. Iden-
tifier), and, e.g., integer constants to IntLiteral, be-
fore the parse. One potential contributing factor
of the difference is the naming conventions that
programmers normally follow. For example, nam-
ing classes, class attributes and local variables, etc.
using typical methods names, such as equals in
Java, is usually avoided by programmers.
It is just as clear that the choice of head-finding
strategy is very important. For both FL and FU,
the best choice is with a wide margin LEFT, fol-
lowed by FREQ. RIGHT is consequently the least
accurate one. A higher amount of arcs pointing to
the right seems to be beneficial for the strategy of
</bodyText>
<page confidence="0.998092">
55
</page>
<table confidence="0.99385725">
FR ASL RI FR ASU RI
LE LE
CO 87.6 96.6 86.6 90.9 98.2 90.7
NC 91.0 99.1 89.5 92.1 99.7 90.7
</table>
<tableCaption confidence="0.994592">
Table 2: Attachment score for Java and the lexical
feature set, where CO = convertible and NC = non-
convertible dependency trees.
</tableCaption>
<table confidence="0.999631">
Python C/C++
FL FU FL FU
UL 91.5 92.1 95.6 96.4
L 99.1 99.2 96.5 96.9
</table>
<tableCaption confidence="0.64680725">
Table 3: F-score for various parser models and
head-finding rules LEFT for Python and C/C++.
parsing from left to right.
Table 1 can be compared to the accuracy on
</tableCaption>
<bodyText confidence="0.998122115384616">
the parser output before conversion from depen-
dency trees to syntax trees. This is shown in the
first row (CO) of table 2, where all information
in the complex dependency label is concatenated
and placed in the dependency label. The relation-
ships between the head-finding strategies remain
the same, but it is worth noting that the accuracies
for FREQ and RIGHT are closer to each other, en-
tailing a more difficult conversion to syntax trees
for the latter. The first row can also be compared
to the second row (NC) in the same table, show-
ing the accuracies when training and parsing with
non-convertible dependency trees. One observa-
tion is that each figure in NC is higher than its
corresponding figure in CO (even ASU for RIGHT
with more decimals), probably attributed to the
lower burden on the parser. Both ASU and ASL
are above 99% for the non-convertible dependency
trees using LEFT.
We can see that choosing an appropriate repre-
sentation of syntactic structure to be used during
parsing is just as important for programming lan-
guages as for natural languages, when using data-
driven natural language parsers (Bikel, 2004).
The parser output in table 1 can more eas-
ily be used as input to existing program com-
prehension tools, normally requiring abstract syn-
tax trees. However, the highly accurate output
for LEFT using non-convertible dependency trees
could be worth using instead, but it requires some
additional processing.
In order to investigate the language indepen-
dence of our approach, table 3 contains the cor-
responding figures as in table 1 for Python and
C/C++, restricted to LEFT, which is the best
head-finding strategy for these languages as well.
Again, each lexicalized feature set (L) outper-
forms its corresponding unlexicalized feature set
(UL). Python has higher FL and virtually the same
FU as Java, whereas C/C++ has the lowest accu-
racies for L. However, the UL figures are not far
behind the L figures for C/C++, and C/C++ has
in fact higher FL for UL compared to Java and
Python. These results can maybe be explained by
the fact that C/C++ has less verbose syntax than
both Java and Python, making the lexical features
less informative.
The FL figures for Java, Python and C/C++ us-
ing LEFT can also be compared to the correspond-
ing figures in Nilsson et al. (2009). They use the
same data sets but a slightly different head-finding
strategy. Instead of selecting the leftmost element
(terminal or non-terminal) as in LEFT, they always
select the leftmost terminal, resulting in FL=99.5
for Java, FL=98.3 for Python and FL=96.5 for
C/C++. That is, our results are slightly lower for
Java, higher for Python, and slightly higher for
C/C++. The same holds for FU as well. That
is, having only arcs pointing to the right results in
high accuracy for all languages (which is the case
for Left described in section 3), but small devia-
tions from this head-finding strategy can in fact be
beneficial for some languages.
We are not aware of any similar studies for
programming languages2 so we compare the re-
sults to natural language parsing. First, the fig-
ures in table 2 for dependency structure are better
than figures reported for natural languages. Some
natural languages are easier to parse than others,
and the parsing results of the CoNLL shared task
2007 (Nivre et al., 2007) for dependency structure
indicate that English and Catalan are relatively
easy, with ASL around 88-89% and ASU around
90-94% for the best dependency parsers.
Secondly, compared to parsing German with
phrase structure with the same approach as here,
with FU = 81.4 and FL = 78.7%, and Swedish,
with FU = 76.8 and FL = 74.0 (Hall and Nivre,
</bodyText>
<footnote confidence="0.996293">
2A comparative experiment using another data-driven
NLP parser for context-free grammar could be of theoreti-
cal interest. However, fast parsing time is important in pro-
gram comprehension tasks, and data-driven NLP parsers for
context-free grammar have worse than a linear time complex-
ity. As, e.g., the Java project has 838 tokens per source file,
linear time complexity is a prerequisite in practice.
</footnote>
<page confidence="0.991508">
56
</page>
<table confidence="0.9992699375">
Correct Label Parsing Label
66 FieldReference VariableReference
25 VariableReference FieldReference
12 MethodDeclaration LocalVariableDeclaration
9 Conditional FieldReference
5 NotEquals MethodReference
4 Plus MethodReference
4 Positive *
4 LessThan FieldReference
4 GreaterOrEquals FieldReference
4 Divide FieldReference
4 Modulo FieldReference
4 LessOrEquals FieldReference
3 Equals NotEquals
3 LessOrEquals Equals
3 NotEquals Equals
</table>
<tableCaption confidence="0.792702666666667">
Table 4: Confusion matrix for Java using non-
convertible dependency trees with LEFT, ordered
by descending frequency.
</tableCaption>
<bodyText confidence="0.999539">
2008), the figures reported in tables 1 and 3 are
also much better. It is however worth noting that
natural languages are more complex and less reg-
ular compared to programming languages. Al-
though it remains to be shown, we conjecture that
these figures are sufficiently high for a large num-
ber of program comprehension tasks.
</bodyText>
<subsectionHeader confidence="0.988217">
4.4 Error Analysis
</subsectionHeader>
<bodyText confidence="0.997469066666667">
This subsection will study the result for Java with
non-convertible dependency trees (NC) and LEFT,
in order to get a deeper insight into the types of
errors that the parser commits. Specifically, the
labeling mistakes caused by the parser are investi-
gated here. This is done by producing a confusion
matrix based on the dependency labels. That is,
how often does a parser confuse label X with la-
bel Y . This is shown in table 4 for the 15 most
common errors.
The two most frequent errors show that the
parser confuses FieldReference and VariableRef-
erence. A FieldReference refers to a class attribute
whereas a VariableReference could refer to either
an attribute or a local variable. The parser mixes a
reference to a class attribute with a reference that
could also be a local variable or vice versa. The
error is understandable, since the parser obviously
has no knowledge about where the variables are
declared. This is an error that type and name anal-
ysis can easily resolve. On the use-occurrence of a
name (reference), analysis looks up for both pos-
sible define-occurrences of the name (declaration),
first a LocalVariableDeclaration and then a Field-
Declaration. It uses the one that is found first.
Another type of confusion involves declara-
tions, where a MethodDeclaration is misinter-
preted as a LocalVariableDeclaration. This type
of error can be resolved by a simple post-
processing step: a LocalVariableDeclaration fol-
lowed by opening parenthesis (always recognized
correctly) is a MethodDeclaration.
Errors that involve binary operators, e.g., Con-
ditional, NotEqual, Plus, are at rank 4 and below
in the list of the most frequent errors. They are
most likely a result of the incremental left-to-right
parsing strategy. The whole expression should be
labeled in accordance with its binary operator (see
count &gt; 0 in figure 4 for LEFT), but is incor-
rectly labeled as either MethodReference, Field-
Reference or some other operator instead. The ref-
erences actually occur in the left-hand side sub-
expression of the binary operators. This means
that subexpressions and bracketing were recog-
nized correctly, but the type of the top expression
node was mixed up. Extending the lookahead in
the list of remaining input tokens, making it pos-
sible for the classifier in the parser to look at even
more yet unparsed tokens, might be one possible
solution. However, these errors are by and large
relatively harmless anyway. Hence, no correction
is in practice needed.
Figure 5 displays some typical mistakes for the
example program fragment
return (fw.unitIndex == unitIndex &amp;&amp;
fw.unitIndex.equals(unitList));
The parser mixes up a ParenthesizedExpression
with a Conditional, a boolean ParenthesizedEx-
pression only occurring in conditional statements
and expressions. Then it incorrectly assigns the
label Equals to the arc between the first left paren-
thesis and the first fw instead of the correct la-
bel LogicalAnd. It mixes up the type of the whole
expression, an Equals- (i.e., ==) is taken for an
LogicalAnd-expression (i.e., &amp;&amp;). Finally, the two
FieldReferences are taken as more general Vari-
ableReferences, which is corrigible as discussed.
In addition to a possible error correction in a
post-processing step, the parsing errors could dis-
appear due to the abstraction of subsequent anal-
yses as commonly used in software maintenance
tools. For instance, without any error correction,
the type reference graphs of our test program, the
correct one and the one constructed using the not
quite correct parsing results, are identical.
</bodyText>
<page confidence="0.993981">
57
</page>
<figure confidence="0.8254425">
Correct:
Parsed:
</figure>
<figureCaption confidence="0.990213">
Figure 5: Typical errors for LEFT using by non-convertible dependency trees.
</figureCaption>
<sectionHeader confidence="0.999836" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999957015625">
Classical parsers for formal languages have been
known for many years. They (conventionally) ac-
cept a context-free language defined by a context-
free grammar. For each program, the parsers
produce a phrase structure referred to as an ab-
stract syntax tree (AST) which is also defined by a
context-free language. Parsers including error sta-
bilization and AST-constructors can be generated
from context-free grammars for parsers (Kastens
et al., 2007). A parser for a new language still
requires the development of a complex specifica-
tion. Moreover, error stabilization often throws
away large parts of the source – it is robust but
does not care about maximizing accuracy.
Breadth-First Parsing (Ophel, 1997) was de-
signed to provide better error stabilization than tra-
ditional parsers and parser generators. It uses a
two phase approach: the first phase identifies high-
level entities – the second phase parses the struc-
ture with these entities as root nonterminals (ax-
ioms).
Fuzzy Parsing (Koppler, 1997) was designed
to efficiently develop parsers by performing the
analysis on selected parts of the source instead
of the whole input. It is specified by a set of
(sub)grammars each with their own axioms. The
actual approach is then similar to Breadth-First
Parsing: it scans for instances of the axioms and
then parses according to the grammar. It makes
parsing more robust in the sense that it ignores
source fragments – including missing parts, errors
and deviations therein – that subsequent analyses
abstract from anyway. A prominent tool using
the fuzzy parsing approach for information extrac-
tion in reverse-engineering tools is Sniff (Bischof-
berger, 1992) for analyzing C++ code.
Island grammars (Moonen, 2001) generalize on
Fuzzy Parsing. Parsing is controlled by two gram-
mar levels (island and sea) where the sea-level is
used when no island-level production applies. The
island-level corresponds to the sub-grammars of
fuzzy parsing. Island grammars have been applied
in reverse-engineering, specifically, to bank soft-
ware (Moonen, 2002).
Syntactic approximation based on lexical anal-
ysis was developed with the same motivation as
our work: when maintenance tools need syntac-
tic information but the documents could not be
parsed for some reason, hierarchies of regular ex-
pression analyses could be used to approximate
the information with high accuracy (Murphy and
Notkin, 1995; Cox and Clarke, 2003). Their in-
formation extraction approach is characterized as
“lightweight” in the sense that it requires little
specification effort.
A similar robust and light-weight approach for
information extraction constructs XML formats
(JavaML and srcML) from C/C++/Java programs
first, before further processing with XML tools
like Xpath (Badros, 2000; Collard et al., 2003). It
combines lexical and context free analyses. Lex-
ical pattern matching is also used in combination
with context free parsing in order to extract facts
from semi-structured specific comments and con-
</bodyText>
<page confidence="0.993444">
58
</page>
<bodyText confidence="0.9998774">
figuration specifications in frameworks (Knodel
and Pinzger, 2003).
TXL is a rule-based language defining informa-
tion extraction and transformation rules for formal
languages (Cordy et al., 1991). It makes it possible
to incrementally extend the rule base and to adapt
to language dialects and extensions. As the rules
are context-sensitive, TXL goes beyond the lexical
and context-free approaches discussed before.
The fundamental difference of our approach
compared to lexical, context-free, and context-
sensitive approaches (and combinations thereof) is
that we use automated machine learning instead of
manual specification for defining and adapting the
information extraction.
General NLP techniques have been applied for
extracting facts from general source code com-
ments to support software maintenance (Etzkorn
et al., 1999). Comments are extracted from source
code using classical lexical analysis; additional in-
formation is extracted (and then added) with clas-
sical compiler front-end technology.
NLP has also been applied to other informa-
tion extraction tasks in software maintenance to
analyze unstructured or very large information
sources, e.g., for analyzing requirement speci-
fications (Sawyer et al., 2002), in clone detec-
tion (Marcus and Maletic, 2001; Grant and Cordy,
2009), and to connect program documentation to
source code (Marcus and Maletic, 2003).
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99995912">
In this paper, we applied natural language parsing
techniques to programming languages. One ad-
vantage is that it offers robustness, since it always
produces some output even if the input is incorrect
or incomplete. Completely correct analysis can,
however, not be guaranteed even for correct input.
However, the experiments showed that accuracy is
in fact close to 100%.
In contrast to robust information extractors used
so far for formal languages, the approach pre-
sented here is rapidly adaptable to new languages.
We automatically generate the language specific
information extractor using machine learning and
training of a generic parsing, instead of explicitly
specifying the information extractor using gram-
mar and transformation rules. Also the training
data can be generated automatically. This could
increase the development efficiency of parsers,
since no language specification has to be provided,
only examples.
Regarding accuracy, the experiments showed
that selecting the syntactic base representation
used by the parser internally has a major impact.
Incorporating, for instance, class, method and
variable names in the set of features of the parser
improves the accuracy more than expected. The
detailed error analysis showed that many errors
committed by the parser are forgivable, as they
are anyway abstracted in later processing phases.
Other errors are easily corrigible. We can also
see that the best results presented here are much
higher than the best parsing results for natural lan-
guages.
Besides efficient information extractor develop-
ment, efficient parsing itself is important. Applied
to programs which can easily contain several mil-
lion lines of code, a parser with more than linear
time complexity is not acceptable. The data-driven
parser utilized here has linear parsing time.
These results are only the first (promising) step
towards natural language parsing leveraging infor-
mation extraction for software maintenance. How-
ever, the only way to really evaluate the usefulness
of the approach is to use its output as input to client
analyses, e.g., software measurement and archi-
tecture recovery, which we plan to do in the fu-
ture. Another direction for future work is to apply
the approach to more dialects of C/C++, such as
analyzing correct, incomplete, and erroneous pro-
grams for both standard C and its dialects.
</bodyText>
<sectionHeader confidence="0.99953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999357">
Paul Anderson. 2008. 90 % Perspiration: Engineering
Static Analysis Techniques for Industrial Applica-
tions. In Proceedings of the 8th IEEE International
Working Conference on Source Code Analysis and
Manipulation, pages 3–12.
Greg J. Badros. 2000. JavaML: a Markup Language
for Java Source Code. In Proceedings of the 9th
International World Wide Web conference on Com-
puter networks : the international journal of com-
puter and telecommunications networking, pages
159–177.
Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing
Model. Computational Linguistics, 30(4):479–511.
Steven Bird, Edward Loper, and Ewan Klein.
2008. Natural Language Toolkit (NLTK) 0.9.5.
http://nltk.org/.
Walter R. Bischofberger. 1992. Sniff: A Pragmatic
Approach to a C++ Programming Environment. In
USENIX C++ Conference, pages 67–82.
</reference>
<page confidence="0.982952">
59
</page>
<reference confidence="0.999858792079208">
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Michael L. Collard, Huzefa H. Kagdi, and Jonathan I.
Maletic. 2003. An XML-Based Lightweight C++
Fact Extractor. In 11th IEEE International Work-
shop on Program Comprehension, pages 134–143.
Michael Collins and Satoshi Sekine. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
James R. Cordy, Charles D. Halpern-Hamu, and Eric
Promislow. 1991. TXL: a Rapid Prototyping Sys-
tem for Programming Language Dialects. Computer
Languages, 16(1):97–107.
Anthony Cox and Charles L. A. Clarke. 2003. Syntac-
tic Approximation Using Iterative Lexical Analysis.
In Proceedings of the 11th IEEE International Work-
shop on Program Comprehension, pages 154–163.
Letha H. Etzkorn, Lisa L. Bowen, and Carl G. Davis.
1999. An Approach to Program Understanding by
Natural Language Understanding. Natural Lan-
guage Engineering, 5(3):219–236.
Scott Grant and James R. Cordy. 2009. Vector Space
Analysis of Software Clones. In Proceedings of
the IEEE 17th International Conference on Program
Comprehension, pages 233–237.
Tobias Gutzmann, Dirk Heuzeroth, and Mircea Trifu.
2007. Recoder 0.83. http://recoder.sourceforge.net/.
Johan Hall and Joakim Nivre. 2008. Parsing Discon-
tinuous Phrase Structure with Grammatical Func-
tions. In Proceedings of GoTAL, pages 169–180.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In Proceedings of COLING-ACL,
pages 316–323.
Uwe Kastens, Anthony M. Sloane, and William M.
Waite. 2007. Generating Software from Specifica-
tions. Jones and Bartlett Publishers.
Jens Knodel and Martin Pinzger. 2003. Improving
Fact Extraction of Framework-Based Software Sys-
tems. In Proceedings of 10th Working Conference
on Reverse Engineering, pages 186–195.
Rainer Koppler. 1997. A Systematic Approach to
Fuzzy Parsing. Software - Practice and Experience,
27(6):637–649.
David M. Magerman. 1995. Statistical Decision-tree
Models for Parsing. In Proceedings of ACL, pages
276–283.
Andrian Marcus and Jonathan I. Maletic. 2001. Iden-
tification of High-Level Concept Clones in Source
Code. In Proceedings of the 16th IEEE interna-
tional conference on Automated software engineer-
ing, page 107.
Andrian Marcus and Jonathan I. Maletic. 2003. Re-
covering Documentation-to-Source-Code Traceabil-
ity Links using Latent Semantic Indexing. In Pro-
ceedings of the 25th International Conference on
Software Engineering, pages 125–135.
Scott McPeak. 2005. Elsa: The
Elkhound-based C/C++ Parser.
http://www.cs.berkeley.edu/∼smcpeak.
Leon Moonen. 2001. Generating Robust Parsers using
Island Grammars. In Proceedings of the 8th Work-
ing Conference on Reverse Engineering, pages 13–
22.
Leon Moonen. 2002. Lightweight Impact Analysis us-
ing Island Grammars. In Proceedings of the 10th In-
ternational Workshop on Program Comprehension,
pages 219–228.
Gail C. Murphy and David Notkin. 1995. Lightweight
Source Model Extraction. SIGSOFT Software Engi-
neering Notes, 20(4):116–127.
Jens Nilsson, Welf L¨owe, Johan Hall, and Joakim
Nivre. 2009. Natural Language Parsing for Fact Ex-
traction from Source Code. In Proceedings of 17th
IEEE International Conference on Program Com-
prehension, pages 223–227.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based Dependency Parsing. In Proceed-
ings of CoNLL, pages 49–56.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In Proceedings of LREC,
pages 2216–2219.
Joakim Nivre, Johan Hall, Sanda K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proceedings of CoNLL/ACL,
pages 915–932.
Joakim Nivre. 2003. An Efficient Algorithm for
Projective Dependency Parsing. In Proceedings of
IWPT, pages 149–160.
John Ophel. 1997. Breadth-First
Parsing. citeseerx.ist.psu.edu/view-
doc/summary?doi=10.1.1.50.3035.
Pete Sawyer, Paul Rayson, and Roger Garside. 2002.
REVERE: Support for Requirements Synthesis
from Documents. Information Systems Frontiers,
4(11):343–353.
Dennis Strein, R¨udiger Lincke, Jonas Lundberg, and
Welf L¨owe. 2007. An Extensible Meta-Model for
Program Analysis. IEEE Transactions on Software
Engineering, 33(9):592–607.
</reference>
<page confidence="0.998404">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.528195">
<title confidence="0.999969">Parsing Formal Languages using Natural Language Parsing Techniques</title>
<author confidence="0.999459">Welf Johan Joakim</author>
<affiliation confidence="0.7479375">University, School of Mathematics and Systems Engineering, Sweden University, Department of Linguistics and Philology, Sweden</affiliation>
<abstract confidence="0.9993797">Program analysis tools used in software maintenance must be robust and ought to be accurate. Many data-driven parsing approaches developed for natural languages are robust and have quite high accuracy when applied to parsing of software. We show this for the programming languages Java, C/C++, and Python. Further studies indicate that post-processing can almost completely remove the remaining errors. Finally, the training data for instantiating the generic data-driven parser can be generated automatically for formal languages, as opposed to the manually development of treebanks for natural languages. Hence, our approach could improve the robustness of software maintenance tools, probably without showing a significant negative effect on their accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paul Anderson</author>
</authors>
<title>90 % Perspiration: Engineering Static Analysis Techniques for Industrial Applications.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th IEEE International Working Conference on Source Code Analysis and Manipulation,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="2484" citStr="Anderson, 2008" startWordPosition="364" endWordPosition="365">ms to analyze can be incomplete, erroneous, or conform to a (yet unknown) dialect or version of the language. Despite error stabilization, classical parsers then lose a lot of information or simply break down. This is unsatisfactory for tools supporting maintenance. Therefore, quite some effort has gone into the development of robust parsers of programs for these tools (cf. our related work section 5). This effort, however, has to be repeated for every programming language. The development of robust parsers is of special interest for languages like C/C++ due to their numerous dialects in use (Anderson, 2008). Also, tools for languages frequently coming in new versions, like Java, benefit from robust parsing. Finally, there are languages like HTML where existing browsers are forgiving if documents do not adhere to the formal standard with the consequence that there exist many formally erroneous documents. In such cases, robust parsing is even a prerequisite for tool-supported maintenance. The accuracy of parsing is a secondary goal in the context of software maintenance. Tasks like program comprehension, quality assessment, and reverse-engineering are fuzzy by their nature. There is no well-define</context>
</contexts>
<marker>Anderson, 2008</marker>
<rawString>Paul Anderson. 2008. 90 % Perspiration: Engineering Static Analysis Techniques for Industrial Applications. In Proceedings of the 8th IEEE International Working Conference on Source Code Analysis and Manipulation, pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg J Badros</author>
</authors>
<title>JavaML: a Markup Language for Java Source Code.</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th International World Wide Web conference on Computer networks : the international journal of computer and telecommunications networking,</booktitle>
<pages>159--177</pages>
<contexts>
<context position="37927" citStr="Badros, 2000" startWordPosition="6187" endWordPosition="6188"> our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to approximate the information with high accuracy (Murphy and Notkin, 1995; Cox and Clarke, 2003). Their information extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuration specifications in frameworks (Knodel and Pinzger, 2003). TXL is a rule-based language defining information extraction and transformation rules for formal languages (Cordy et al., 1991). It makes it possible to incrementally extend the rule base and to adapt to language dialects and extensions. As the rules are context-sensitive, TXL goes beyond the lexical and </context>
</contexts>
<marker>Badros, 2000</marker>
<rawString>Greg J. Badros. 2000. JavaML: a Markup Language for Java Source Code. In Proceedings of the 9th International World Wide Web conference on Computer networks : the international journal of computer and telecommunications networking, pages 159–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ Parsing Model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="27708" citStr="Bikel, 2004" startWordPosition="4564" endWordPosition="4565">in the same table, showing the accuracies when training and parsing with non-convertible dependency trees. One observation is that each figure in NC is higher than its corresponding figure in CO (even ASU for RIGHT with more decimals), probably attributed to the lower burden on the parser. Both ASU and ASL are above 99% for the non-convertible dependency trees using LEFT. We can see that choosing an appropriate representation of syntactic structure to be used during parsing is just as important for programming languages as for natural languages, when using datadriven natural language parsers (Bikel, 2004). The parser output in table 1 can more easily be used as input to existing program comprehension tools, normally requiring abstract syntax trees. However, the highly accurate output for LEFT using non-convertible dependency trees could be worth using instead, but it requires some additional processing. In order to investigate the language independence of our approach, table 3 contains the corresponding figures as in table 1 for Python and C/C++, restricted to LEFT, which is the best head-finding strategy for these languages as well. Again, each lexicalized feature set (L) outperforms its corr</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ Parsing Model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2008</date>
<journal>Natural Language Toolkit (NLTK)</journal>
<volume>0</volume>
<pages>http://nltk.org/.</pages>
<contexts>
<context position="21042" citStr="Bird et al., 2008" startWordPosition="3413" endWordPosition="3416"> languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all source files in the directory “src” (having 400 source files with 92k LOC and 335k tokens). • For C/C++: Elsa 2005.08.22b (McPeak, 2005), where 1389 source files were used, including the 978 C/C++ benchmark files in the distribution (thus comprising 1389 source files with 265k LOC and 691k tokens). • For Python: Natural Language Toolkit 0.9.5 (Bird et al., 2008), where all source files in the directory “nltk” were used (having 160 source files with 65k LOC and 280k tokens). To construct the syntax tree for the source code file of Recoder, we have used Recoder. It creates an abstract syntax tree for a source file, but we are currently interested in the concrete syntax tree with all the original tokens. In this first conversion step, the tokens of the syntax trees are thus retained. For example, the syntax trees in figure 2 are generated by Recoder. 54 The same strategy was adopted for Elsa with the difference that CDT 4.0.3, a plug-in to the Eclipse I</context>
</contexts>
<marker>Bird, Loper, Klein, 2008</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2008. Natural Language Toolkit (NLTK) 0.9.5. http://nltk.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter R Bischofberger</author>
</authors>
<title>Sniff: A Pragmatic Approach to a C++ Programming Environment.</title>
<date>1992</date>
<booktitle>In USENIX C++ Conference,</booktitle>
<pages>67--82</pages>
<contexts>
<context position="36837" citStr="Bischofberger, 1992" startWordPosition="6026" endWordPosition="6028">sers by performing the analysis on selected parts of the source instead of the whole input. It is specified by a set of (sub)grammars each with their own axioms. The actual approach is then similar to Breadth-First Parsing: it scans for instances of the axioms and then parses according to the grammar. It makes parsing more robust in the sense that it ignores source fragments – including missing parts, errors and deviations therein – that subsequent analyses abstract from anyway. A prominent tool using the fuzzy parsing approach for information extraction in reverse-engineering tools is Sniff (Bischofberger, 1992) for analyzing C++ code. Island grammars (Moonen, 2001) generalize on Fuzzy Parsing. Parsing is controlled by two grammar levels (island and sea) where the sea-level is used when no island-level production applies. The island-level corresponds to the sub-grammars of fuzzy parsing. Island grammars have been applied in reverse-engineering, specifically, to bank software (Moonen, 2002). Syntactic approximation based on lexical analysis was developed with the same motivation as our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierar</context>
</contexts>
<marker>Bischofberger, 1992</marker>
<rawString>Walter R. Bischofberger. 1992. Sniff: A Pragmatic Approach to a C++ Programming Environment. In USENIX C++ Conference, pages 67–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A Library for Support Vector Machines.</title>
<date>2001</date>
<contexts>
<context position="20367" citStr="Chang and Lin, 2001" startWordPosition="3301" endWordPosition="3304"> analysis no matter the input, but it can commit errors even for correct input. This section investigates the accuracy for correct input, when varying feature set, head-finding rules and language. We begin with the experimental setup. 4.1 Experimental Setup The open-source software MaltParser (maltparser.org) (Nivre et al., 2006) is used in the experiments. It contains an implementation of the parsing algorithm, as well as an implementation of the conversion strategy from syntax trees to dependency trees and back, presented in subsections 3.3 and 3.4. It comes with the machine learner LIBSVM (Chang and Lin, 2001), producing the most accurate results for parsing natural languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all source files in the directory “src” (having 400 source files with 92k LOC and 335k tokens). • For C/C++: Elsa 2005.08.22b (McPeak, 2005), where 1389 source files were used, including the 978 C/C++ benchmark files in the distribution (thus comprising 1389 source files with 265k LOC and 691</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: A Library for Support Vector Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Collard</author>
<author>Huzefa H Kagdi</author>
<author>Jonathan I Maletic</author>
</authors>
<title>An XML-Based Lightweight C++ Fact Extractor.</title>
<date>2003</date>
<booktitle>In 11th IEEE International Workshop on Program Comprehension,</booktitle>
<pages>134--143</pages>
<contexts>
<context position="13623" citStr="Collard et al. (2003)" startWordPosition="2181" endWordPosition="2184">while (count &gt; 0) { stack[--count]=null; } We also map the output of the lexical analyzer to the parts-of-speech for the words (e.g., Identifier for String and getName). All source code comments and indentation information (except for Python where the indentation conveys hierarchical information) have been excluded from the syntax trees. All string and character literals have also been mapped to “string” and “char”, respectively. This does not entail that the approach is lossy, since all this information can be retained in a post-processing step, if necessary. As pointed out by, for instance, Collard et al. (2003), comments and indentation may among other things be of interest when trying to understand source code. 3.3 Syntax Tree ⇒ Dependency Tree Here we will discuss the conversion of syntax trees into dependency trees. We use a method that has been successfully applied for natural languages for converting syntax trees into a convertible dependency tree that makes it possible to perform the inverse conversion, meaning that information about the syntax tree is saved in complex arc labels (Hall and Nivre, 2008). We also present results in section 4 using the dependency trees that cannot be used for the</context>
<context position="37950" citStr="Collard et al., 2003" startWordPosition="6189" endWordPosition="6192">n maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to approximate the information with high accuracy (Murphy and Notkin, 1995; Cox and Clarke, 2003). Their information extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuration specifications in frameworks (Knodel and Pinzger, 2003). TXL is a rule-based language defining information extraction and transformation rules for formal languages (Cordy et al., 1991). It makes it possible to incrementally extend the rule base and to adapt to language dialects and extensions. As the rules are context-sensitive, TXL goes beyond the lexical and context-free approaches</context>
</contexts>
<marker>Collard, Kagdi, Maletic, 2003</marker>
<rawString>Michael L. Collard, Huzefa H. Kagdi, and Jonathan I. Maletic. 2003. An XML-Based Lightweight C++ Fact Extractor. In 11th IEEE International Workshop on Program Comprehension, pages 134–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Satoshi Sekine</author>
</authors>
<date>2008</date>
<note>Evalb. http://nlp.cs.nyu.edu/evalb/.</note>
<contexts>
<context position="23720" citStr="Collins and Sekine, 2008" startWordPosition="3874" endWordPosition="3877">terminals i and j – derived from the test data with those derived from the parser. A constituent in the parser output matches a constituent in the test data when they span over the same terminals in the input string. Recall is the ratio of matched constituents over all constituents in the test data. Precision is the ratio of matched constituents over all constituents found by the parser. F-score comes in two versions, one unlabeled (FU) and one labeled (FL), where each correct constituent in the latter also must have the correct nonterminal name (i.e., XP). The metric is implemented in Evalb (Collins and Sekine, 2008). 1It is worth noting that CDT failed to produce syntax trees for 2.2% of these source files, which were consequently excluded from the experiments. This again indicates the difficult of parsing C/C++ due to its different dialects. FR FL RI FR FU RI LE LE UL 82.1 93.5 74.6 92.3 97.9 90.6 L 89.7 97.7 80.8 95.8 99.3 92.1 Table 1: F-score for various parser models and head-finding rules for Java, where FR = FREQ, LE = LEFT and RI = RIGHT. The standard evaluation metric measuring accuracy for dependency parsing for natural language is, on the other hand, labeled (ASL) and unlabeled (ASU) attachmen</context>
</contexts>
<marker>Collins, Sekine, 2008</marker>
<rawString>Michael Collins and Satoshi Sekine. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Cordy</author>
<author>Charles D Halpern-Hamu</author>
<author>Eric Promislow</author>
</authors>
<title>TXL: a Rapid Prototyping System for Programming Language Dialects.</title>
<date>1991</date>
<journal>Computer Languages,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="38348" citStr="Cordy et al., 1991" startWordPosition="6248" endWordPosition="6251">obust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuration specifications in frameworks (Knodel and Pinzger, 2003). TXL is a rule-based language defining information extraction and transformation rules for formal languages (Cordy et al., 1991). It makes it possible to incrementally extend the rule base and to adapt to language dialects and extensions. As the rules are context-sensitive, TXL goes beyond the lexical and context-free approaches discussed before. The fundamental difference of our approach compared to lexical, context-free, and contextsensitive approaches (and combinations thereof) is that we use automated machine learning instead of manual specification for defining and adapting the information extraction. General NLP techniques have been applied for extracting facts from general source code comments to support softwar</context>
</contexts>
<marker>Cordy, Halpern-Hamu, Promislow, 1991</marker>
<rawString>James R. Cordy, Charles D. Halpern-Hamu, and Eric Promislow. 1991. TXL: a Rapid Prototyping System for Programming Language Dialects. Computer Languages, 16(1):97–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Cox</author>
<author>Charles L A Clarke</author>
</authors>
<title>Syntactic Approximation Using Iterative Lexical Analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th IEEE International Workshop on Program Comprehension,</booktitle>
<pages>154--163</pages>
<contexts>
<context position="37585" citStr="Cox and Clarke, 2003" startWordPosition="6138" endWordPosition="6141">s (island and sea) where the sea-level is used when no island-level production applies. The island-level corresponds to the sub-grammars of fuzzy parsing. Island grammars have been applied in reverse-engineering, specifically, to bank software (Moonen, 2002). Syntactic approximation based on lexical analysis was developed with the same motivation as our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to approximate the information with high accuracy (Murphy and Notkin, 1995; Cox and Clarke, 2003). Their information extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuration specifications in fra</context>
</contexts>
<marker>Cox, Clarke, 2003</marker>
<rawString>Anthony Cox and Charles L. A. Clarke. 2003. Syntactic Approximation Using Iterative Lexical Analysis. In Proceedings of the 11th IEEE International Workshop on Program Comprehension, pages 154–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Letha H Etzkorn</author>
<author>Lisa L Bowen</author>
<author>Carl G Davis</author>
</authors>
<title>An Approach to Program Understanding by Natural Language Understanding.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>3</issue>
<contexts>
<context position="38984" citStr="Etzkorn et al., 1999" startWordPosition="6339" endWordPosition="6342">ossible to incrementally extend the rule base and to adapt to language dialects and extensions. As the rules are context-sensitive, TXL goes beyond the lexical and context-free approaches discussed before. The fundamental difference of our approach compared to lexical, context-free, and contextsensitive approaches (and combinations thereof) is that we use automated machine learning instead of manual specification for defining and adapting the information extraction. General NLP techniques have been applied for extracting facts from general source code comments to support software maintenance (Etzkorn et al., 1999). Comments are extracted from source code using classical lexical analysis; additional information is extracted (and then added) with classical compiler front-end technology. NLP has also been applied to other information extraction tasks in software maintenance to analyze unstructured or very large information sources, e.g., for analyzing requirement specifications (Sawyer et al., 2002), in clone detection (Marcus and Maletic, 2001; Grant and Cordy, 2009), and to connect program documentation to source code (Marcus and Maletic, 2003). 6 Conclusions and Future Work In this paper, we applied na</context>
</contexts>
<marker>Etzkorn, Bowen, Davis, 1999</marker>
<rawString>Letha H. Etzkorn, Lisa L. Bowen, and Carl G. Davis. 1999. An Approach to Program Understanding by Natural Language Understanding. Natural Language Engineering, 5(3):219–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Grant</author>
<author>James R Cordy</author>
</authors>
<title>Vector Space Analysis of Software Clones.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE 17th International Conference on Program Comprehension,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="39444" citStr="Grant and Cordy, 2009" startWordPosition="6407" endWordPosition="6410">extraction. General NLP techniques have been applied for extracting facts from general source code comments to support software maintenance (Etzkorn et al., 1999). Comments are extracted from source code using classical lexical analysis; additional information is extracted (and then added) with classical compiler front-end technology. NLP has also been applied to other information extraction tasks in software maintenance to analyze unstructured or very large information sources, e.g., for analyzing requirement specifications (Sawyer et al., 2002), in clone detection (Marcus and Maletic, 2001; Grant and Cordy, 2009), and to connect program documentation to source code (Marcus and Maletic, 2003). 6 Conclusions and Future Work In this paper, we applied natural language parsing techniques to programming languages. One advantage is that it offers robustness, since it always produces some output even if the input is incorrect or incomplete. Completely correct analysis can, however, not be guaranteed even for correct input. However, the experiments showed that accuracy is in fact close to 100%. In contrast to robust information extractors used so far for formal languages, the approach presented here is rapidly</context>
</contexts>
<marker>Grant, Cordy, 2009</marker>
<rawString>Scott Grant and James R. Cordy. 2009. Vector Space Analysis of Software Clones. In Proceedings of the IEEE 17th International Conference on Program Comprehension, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Gutzmann</author>
<author>Dirk Heuzeroth</author>
<author>Mircea Trifu</author>
</authors>
<date>2007</date>
<note>Recoder 0.83. http://recoder.sourceforge.net/.</note>
<contexts>
<context position="20666" citStr="Gutzmann et al., 2007" startWordPosition="3351" endWordPosition="3354">ltparser.org) (Nivre et al., 2006) is used in the experiments. It contains an implementation of the parsing algorithm, as well as an implementation of the conversion strategy from syntax trees to dependency trees and back, presented in subsections 3.3 and 3.4. It comes with the machine learner LIBSVM (Chang and Lin, 2001), producing the most accurate results for parsing natural languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all source files in the directory “src” (having 400 source files with 92k LOC and 335k tokens). • For C/C++: Elsa 2005.08.22b (McPeak, 2005), where 1389 source files were used, including the 978 C/C++ benchmark files in the distribution (thus comprising 1389 source files with 265k LOC and 691k tokens). • For Python: Natural Language Toolkit 0.9.5 (Bird et al., 2008), where all source files in the directory “nltk” were used (having 160 source files with 65k LOC and 280k tokens). To construct the syntax tree for the source code file of Recoder, we have used Recoder. It creates an abstrac</context>
</contexts>
<marker>Gutzmann, Heuzeroth, Trifu, 2007</marker>
<rawString>Tobias Gutzmann, Dirk Heuzeroth, and Mircea Trifu. 2007. Recoder 0.83. http://recoder.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>Parsing Discontinuous Phrase Structure with Grammatical Functions.</title>
<date>2008</date>
<booktitle>In Proceedings of GoTAL,</booktitle>
<pages>169--180</pages>
<contexts>
<context position="14130" citStr="Hall and Nivre, 2008" startWordPosition="2265" endWordPosition="2268">on can be retained in a post-processing step, if necessary. As pointed out by, for instance, Collard et al. (2003), comments and indentation may among other things be of interest when trying to understand source code. 3.3 Syntax Tree ⇒ Dependency Tree Here we will discuss the conversion of syntax trees into dependency trees. We use a method that has been successfully applied for natural languages for converting syntax trees into a convertible dependency tree that makes it possible to perform the inverse conversion, meaning that information about the syntax tree is saved in complex arc labels (Hall and Nivre, 2008). We also present results in section 4 using the dependency trees that cannot be used for the inverse conversion, which we call non-convertible dependency trees. The conversion is performed in a two-step approach. First, the algorithm traverses the syntax tree from the root and identifies the head-child and the terminal head for all nonterminals in a recursive depth-first search. To identify the head-child for each nonterminal, the algorithm uses heuristics called head-finding rules, inspired by, for instance, Magerman (1995). Three head-finding strategies have been investigated. For each nont</context>
</contexts>
<marker>Hall, Nivre, 2008</marker>
<rawString>Johan Hall and Joakim Nivre. 2008. Parsing Discontinuous Phrase Structure with Grammatical Functions. In Proceedings of GoTAL, pages 169–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Discriminative Classifiers for Deterministic Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>316--323</pages>
<contexts>
<context position="20499" citStr="Hall et al., 2006" startWordPosition="3323" endWordPosition="3326">ut, when varying feature set, head-finding rules and language. We begin with the experimental setup. 4.1 Experimental Setup The open-source software MaltParser (maltparser.org) (Nivre et al., 2006) is used in the experiments. It contains an implementation of the parsing algorithm, as well as an implementation of the conversion strategy from syntax trees to dependency trees and back, presented in subsections 3.3 and 3.4. It comes with the machine learner LIBSVM (Chang and Lin, 2001), producing the most accurate results for parsing natural languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all source files in the directory “src” (having 400 source files with 92k LOC and 335k tokens). • For C/C++: Elsa 2005.08.22b (McPeak, 2005), where 1389 source files were used, including the 978 C/C++ benchmark files in the distribution (thus comprising 1389 source files with 265k LOC and 691k tokens). • For Python: Natural Language Toolkit 0.9.5 (Bird et al., 2008), where all source files in the directory “nltk” were use</context>
</contexts>
<marker>Hall, Nivre, Nilsson, 2006</marker>
<rawString>Johan Hall, Joakim Nivre, and Jens Nilsson. 2006. Discriminative Classifiers for Deterministic Dependency Parsing. In Proceedings of COLING-ACL, pages 316–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uwe Kastens</author>
<author>Anthony M Sloane</author>
<author>William M Waite</author>
</authors>
<date>2007</date>
<booktitle>Generating Software from Specifications. Jones and</booktitle>
<publisher>Bartlett Publishers.</publisher>
<contexts>
<context position="35623" citStr="Kastens et al., 2007" startWordPosition="5831" endWordPosition="5834">d using the not quite correct parsing results, are identical. 57 Correct: Parsed: Figure 5: Typical errors for LEFT using by non-convertible dependency trees. 5 Related Work Classical parsers for formal languages have been known for many years. They (conventionally) accept a context-free language defined by a contextfree grammar. For each program, the parsers produce a phrase structure referred to as an abstract syntax tree (AST) which is also defined by a context-free language. Parsers including error stabilization and AST-constructors can be generated from context-free grammars for parsers (Kastens et al., 2007). A parser for a new language still requires the development of a complex specification. Moreover, error stabilization often throws away large parts of the source – it is robust but does not care about maximizing accuracy. Breadth-First Parsing (Ophel, 1997) was designed to provide better error stabilization than traditional parsers and parser generators. It uses a two phase approach: the first phase identifies highlevel entities – the second phase parses the structure with these entities as root nonterminals (axioms). Fuzzy Parsing (Koppler, 1997) was designed to efficiently develop parsers b</context>
</contexts>
<marker>Kastens, Sloane, Waite, 2007</marker>
<rawString>Uwe Kastens, Anthony M. Sloane, and William M. Waite. 2007. Generating Software from Specifications. Jones and Bartlett Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Knodel</author>
<author>Martin Pinzger</author>
</authors>
<title>Improving Fact Extraction of Framework-Based Software Systems.</title>
<date>2003</date>
<booktitle>In Proceedings of 10th Working Conference on Reverse Engineering,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="38219" citStr="Knodel and Pinzger, 2003" startWordPosition="6229" endWordPosition="6232">nformation extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuration specifications in frameworks (Knodel and Pinzger, 2003). TXL is a rule-based language defining information extraction and transformation rules for formal languages (Cordy et al., 1991). It makes it possible to incrementally extend the rule base and to adapt to language dialects and extensions. As the rules are context-sensitive, TXL goes beyond the lexical and context-free approaches discussed before. The fundamental difference of our approach compared to lexical, context-free, and contextsensitive approaches (and combinations thereof) is that we use automated machine learning instead of manual specification for defining and adapting the informati</context>
</contexts>
<marker>Knodel, Pinzger, 2003</marker>
<rawString>Jens Knodel and Martin Pinzger. 2003. Improving Fact Extraction of Framework-Based Software Systems. In Proceedings of 10th Working Conference on Reverse Engineering, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rainer Koppler</author>
</authors>
<title>A Systematic Approach to Fuzzy Parsing. Software - Practice and Experience,</title>
<date>1997</date>
<contexts>
<context position="36177" citStr="Koppler, 1997" startWordPosition="5922" endWordPosition="5923">rom context-free grammars for parsers (Kastens et al., 2007). A parser for a new language still requires the development of a complex specification. Moreover, error stabilization often throws away large parts of the source – it is robust but does not care about maximizing accuracy. Breadth-First Parsing (Ophel, 1997) was designed to provide better error stabilization than traditional parsers and parser generators. It uses a two phase approach: the first phase identifies highlevel entities – the second phase parses the structure with these entities as root nonterminals (axioms). Fuzzy Parsing (Koppler, 1997) was designed to efficiently develop parsers by performing the analysis on selected parts of the source instead of the whole input. It is specified by a set of (sub)grammars each with their own axioms. The actual approach is then similar to Breadth-First Parsing: it scans for instances of the axioms and then parses according to the grammar. It makes parsing more robust in the sense that it ignores source fragments – including missing parts, errors and deviations therein – that subsequent analyses abstract from anyway. A prominent tool using the fuzzy parsing approach for information extraction</context>
</contexts>
<marker>Koppler, 1997</marker>
<rawString>Rainer Koppler. 1997. A Systematic Approach to Fuzzy Parsing. Software - Practice and Experience, 27(6):637–649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical Decision-tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="14661" citStr="Magerman (1995)" startWordPosition="2349" endWordPosition="2350">ormation about the syntax tree is saved in complex arc labels (Hall and Nivre, 2008). We also present results in section 4 using the dependency trees that cannot be used for the inverse conversion, which we call non-convertible dependency trees. The conversion is performed in a two-step approach. First, the algorithm traverses the syntax tree from the root and identifies the head-child and the terminal head for all nonterminals in a recursive depth-first search. To identify the head-child for each nonterminal, the algorithm uses heuristics called head-finding rules, inspired by, for instance, Magerman (1995). Three head-finding strategies have been investigated. For each nonterminal: 1. FREQ: Let the element with the most frequently occurring name be the head, but exclude the token ‘;’ as a potential head. If two tokens have the same frequency, let the leftmost occurring element be the head. 2. LEFT: let the leftmost terminal in the entire subtree of the nonterminal be the head of all other elements. 3. RIGHT: let the rightmost terminal in the entire subtree of the nonterminal be the head of all other elements. The dependency trees in figures 3 and 4 use LEFT and FREQ. LEFT and RIGHT induce that </context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical Decision-tree Models for Parsing. In Proceedings of ACL, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrian Marcus</author>
<author>Jonathan I Maletic</author>
</authors>
<title>Identification of High-Level Concept Clones in Source Code.</title>
<date>2001</date>
<booktitle>In Proceedings of the 16th IEEE international conference on Automated software engineering,</booktitle>
<pages>107</pages>
<contexts>
<context position="39420" citStr="Marcus and Maletic, 2001" startWordPosition="6403" endWordPosition="6406"> adapting the information extraction. General NLP techniques have been applied for extracting facts from general source code comments to support software maintenance (Etzkorn et al., 1999). Comments are extracted from source code using classical lexical analysis; additional information is extracted (and then added) with classical compiler front-end technology. NLP has also been applied to other information extraction tasks in software maintenance to analyze unstructured or very large information sources, e.g., for analyzing requirement specifications (Sawyer et al., 2002), in clone detection (Marcus and Maletic, 2001; Grant and Cordy, 2009), and to connect program documentation to source code (Marcus and Maletic, 2003). 6 Conclusions and Future Work In this paper, we applied natural language parsing techniques to programming languages. One advantage is that it offers robustness, since it always produces some output even if the input is incorrect or incomplete. Completely correct analysis can, however, not be guaranteed even for correct input. However, the experiments showed that accuracy is in fact close to 100%. In contrast to robust information extractors used so far for formal languages, the approach p</context>
</contexts>
<marker>Marcus, Maletic, 2001</marker>
<rawString>Andrian Marcus and Jonathan I. Maletic. 2001. Identification of High-Level Concept Clones in Source Code. In Proceedings of the 16th IEEE international conference on Automated software engineering, page 107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrian Marcus</author>
<author>Jonathan I Maletic</author>
</authors>
<title>Recovering Documentation-to-Source-Code Traceability Links using Latent Semantic Indexing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 25th International Conference on Software Engineering,</booktitle>
<pages>125--135</pages>
<contexts>
<context position="39524" citStr="Marcus and Maletic, 2003" startWordPosition="6419" endWordPosition="6422">m general source code comments to support software maintenance (Etzkorn et al., 1999). Comments are extracted from source code using classical lexical analysis; additional information is extracted (and then added) with classical compiler front-end technology. NLP has also been applied to other information extraction tasks in software maintenance to analyze unstructured or very large information sources, e.g., for analyzing requirement specifications (Sawyer et al., 2002), in clone detection (Marcus and Maletic, 2001; Grant and Cordy, 2009), and to connect program documentation to source code (Marcus and Maletic, 2003). 6 Conclusions and Future Work In this paper, we applied natural language parsing techniques to programming languages. One advantage is that it offers robustness, since it always produces some output even if the input is incorrect or incomplete. Completely correct analysis can, however, not be guaranteed even for correct input. However, the experiments showed that accuracy is in fact close to 100%. In contrast to robust information extractors used so far for formal languages, the approach presented here is rapidly adaptable to new languages. We automatically generate the language specific inf</context>
</contexts>
<marker>Marcus, Maletic, 2003</marker>
<rawString>Andrian Marcus and Jonathan I. Maletic. 2003. Recovering Documentation-to-Source-Code Traceability Links using Latent Semantic Indexing. In Proceedings of the 25th International Conference on Software Engineering, pages 125–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott McPeak</author>
</authors>
<title>Elsa: The Elkhound-based C/C++</title>
<date>2005</date>
<note>Parser. http://www.cs.berkeley.edu/∼smcpeak.</note>
<contexts>
<context position="20814" citStr="McPeak, 2005" startWordPosition="3378" endWordPosition="3379">nversion strategy from syntax trees to dependency trees and back, presented in subsections 3.3 and 3.4. It comes with the machine learner LIBSVM (Chang and Lin, 2001), producing the most accurate results for parsing natural languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all source files in the directory “src” (having 400 source files with 92k LOC and 335k tokens). • For C/C++: Elsa 2005.08.22b (McPeak, 2005), where 1389 source files were used, including the 978 C/C++ benchmark files in the distribution (thus comprising 1389 source files with 265k LOC and 691k tokens). • For Python: Natural Language Toolkit 0.9.5 (Bird et al., 2008), where all source files in the directory “nltk” were used (having 160 source files with 65k LOC and 280k tokens). To construct the syntax tree for the source code file of Recoder, we have used Recoder. It creates an abstract syntax tree for a source file, but we are currently interested in the concrete syntax tree with all the original tokens. In this first conversion </context>
</contexts>
<marker>McPeak, 2005</marker>
<rawString>Scott McPeak. 2005. Elsa: The Elkhound-based C/C++ Parser. http://www.cs.berkeley.edu/∼smcpeak.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Moonen</author>
</authors>
<title>Generating Robust Parsers using Island Grammars.</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th Working Conference on Reverse Engineering,</booktitle>
<pages>13--22</pages>
<contexts>
<context position="36892" citStr="Moonen, 2001" startWordPosition="6035" endWordPosition="6036">e instead of the whole input. It is specified by a set of (sub)grammars each with their own axioms. The actual approach is then similar to Breadth-First Parsing: it scans for instances of the axioms and then parses according to the grammar. It makes parsing more robust in the sense that it ignores source fragments – including missing parts, errors and deviations therein – that subsequent analyses abstract from anyway. A prominent tool using the fuzzy parsing approach for information extraction in reverse-engineering tools is Sniff (Bischofberger, 1992) for analyzing C++ code. Island grammars (Moonen, 2001) generalize on Fuzzy Parsing. Parsing is controlled by two grammar levels (island and sea) where the sea-level is used when no island-level production applies. The island-level corresponds to the sub-grammars of fuzzy parsing. Island grammars have been applied in reverse-engineering, specifically, to bank software (Moonen, 2002). Syntactic approximation based on lexical analysis was developed with the same motivation as our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to a</context>
</contexts>
<marker>Moonen, 2001</marker>
<rawString>Leon Moonen. 2001. Generating Robust Parsers using Island Grammars. In Proceedings of the 8th Working Conference on Reverse Engineering, pages 13– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Moonen</author>
</authors>
<title>Lightweight Impact Analysis using Island Grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the 10th International Workshop on Program Comprehension,</booktitle>
<pages>219--228</pages>
<contexts>
<context position="37222" citStr="Moonen, 2002" startWordPosition="6083" endWordPosition="6084">ssing parts, errors and deviations therein – that subsequent analyses abstract from anyway. A prominent tool using the fuzzy parsing approach for information extraction in reverse-engineering tools is Sniff (Bischofberger, 1992) for analyzing C++ code. Island grammars (Moonen, 2001) generalize on Fuzzy Parsing. Parsing is controlled by two grammar levels (island and sea) where the sea-level is used when no island-level production applies. The island-level corresponds to the sub-grammars of fuzzy parsing. Island grammars have been applied in reverse-engineering, specifically, to bank software (Moonen, 2002). Syntactic approximation based on lexical analysis was developed with the same motivation as our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to approximate the information with high accuracy (Murphy and Notkin, 1995; Cox and Clarke, 2003). Their information extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and</context>
</contexts>
<marker>Moonen, 2002</marker>
<rawString>Leon Moonen. 2002. Lightweight Impact Analysis using Island Grammars. In Proceedings of the 10th International Workshop on Program Comprehension, pages 219–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gail C Murphy</author>
<author>David Notkin</author>
</authors>
<title>Lightweight Source Model Extraction.</title>
<date>1995</date>
<journal>SIGSOFT Software Engineering Notes,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="37562" citStr="Murphy and Notkin, 1995" startWordPosition="6134" endWordPosition="6137">lled by two grammar levels (island and sea) where the sea-level is used when no island-level production applies. The island-level corresponds to the sub-grammars of fuzzy parsing. Island grammars have been applied in reverse-engineering, specifically, to bank software (Moonen, 2002). Syntactic approximation based on lexical analysis was developed with the same motivation as our work: when maintenance tools need syntactic information but the documents could not be parsed for some reason, hierarchies of regular expression analyses could be used to approximate the information with high accuracy (Murphy and Notkin, 1995; Cox and Clarke, 2003). Their information extraction approach is characterized as “lightweight” in the sense that it requires little specification effort. A similar robust and light-weight approach for information extraction constructs XML formats (JavaML and srcML) from C/C++/Java programs first, before further processing with XML tools like Xpath (Badros, 2000; Collard et al., 2003). It combines lexical and context free analyses. Lexical pattern matching is also used in combination with context free parsing in order to extract facts from semi-structured specific comments and con58 figuratio</context>
</contexts>
<marker>Murphy, Notkin, 1995</marker>
<rawString>Gail C. Murphy and David Notkin. 1995. Lightweight Source Model Extraction. SIGSOFT Software Engineering Notes, 20(4):116–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Welf L¨owe</author>
<author>Johan Hall</author>
<author>Joakim Nivre</author>
</authors>
<title>Natural Language Parsing for Fact Extraction from Source Code.</title>
<date>2009</date>
<booktitle>In Proceedings of 17th IEEE International Conference on Program Comprehension,</booktitle>
<pages>223--227</pages>
<marker>Nilsson, L¨owe, Hall, Nivre, 2009</marker>
<rawString>Jens Nilsson, Welf L¨owe, Johan Hall, and Joakim Nivre. 2009. Natural Language Parsing for Fact Extraction from Source Code. In Proceedings of 17th IEEE International Conference on Program Comprehension, pages 223–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based Dependency Parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="7272" citStr="Nivre et al., 2004" startWordPosition="1112" endWordPosition="1115"> a sequence of transitions. The parser starts with an empty stack and terminates when the input queue is empty, parsing input from left to right. It has four transitions (LeftArc, Right-Arc, Reduce and Shift), manipulating these data structures. The algorithm has a linear time complexity as it is guaranteed to terminate after at most 2n transitions, given that the length of the input sentence is n. In contrast to a parser guided by a grammar (e.g., ordinary shift-reduce parsing for contextfree grammars), this parser is guided by a classifier induced from empirical data using machine learning (Nivre et al., 2004). Hence, the parser requires training data containing dependency trees. In other words, the parser has a training phase where the training data is used by the training module in order to learn the correct sequence of transitions. The training data can contain dependency trees for sentences of any language irrespectively of whether the language is a natural or formal one. The training module produces the correct transition sequences using the dependency trees of the training data. These correct parser configurations and transition sequences are then provided as training data to a classifier, wh</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based Dependency Parsing. In Proceedings of CoNLL, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>MaltParser: A Data-Driven Parser-Generator for Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="20078" citStr="Nivre et al., 2006" startWordPosition="3252" endWordPosition="3255">erminal head and the attachment nonterminal. 4 Experiments We will in this section present parsing experiments and evaluate the accuracy of the syntax trees produced by the parser. As mentioned in section 2, the parsing algorithm is robust in the sense that it always produces a syntactic analysis no matter the input, but it can commit errors even for correct input. This section investigates the accuracy for correct input, when varying feature set, head-finding rules and language. We begin with the experimental setup. 4.1 Experimental Setup The open-source software MaltParser (maltparser.org) (Nivre et al., 2006) is used in the experiments. It contains an implementation of the parsing algorithm, as well as an implementation of the conversion strategy from syntax trees to dependency trees and back, presented in subsections 3.3 and 3.4. It comes with the machine learner LIBSVM (Chang and Lin, 2001), producing the most accurate results for parsing natural languages compared to other evaluated machine learners (Hall et al., 2006). LIBSVM requires training data. The source files of the following projects have been converted into dependency trees: • For Java: Recoder 0.83 (Gutzmann et al., 2007), using all </context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. MaltParser: A Data-Driven Parser-Generator for Dependency Parsing. In Proceedings of LREC, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sanda K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>Shared Task on Dependency Parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of CoNLL/ACL,</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sanda K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of CoNLL/ACL, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An Efficient Algorithm for Projective Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="6209" citStr="Nivre, 2003" startWordPosition="940" endWordPosition="941">led, directed and rooted trees, as shown in figure 1. One essential difference compared to context-free grammar is the absence of nonterminals. Another difference is that the syntactic structure is composed of lexical tokens (also called terminals or words) linked by binary and directed relations called dependencies. Each token in the figure is labeled with a part-of-speech, shown at the bottom of the figure. Each dependency relation is also labeled. The parsing algorithm used in the experiments of section 4, known as the Nivre’s arc-eager alFigure 1: Sentence with a dependency tree. gorithm (Nivre, 2003), can produce such dependency trees. It bears a resemblance to the shiftreduce parser for context-free grammars, with the most apparent difference being that terminals (not nonterminals) are pushed onto the stack. Parser configurations are represented by a stack, a list of (remaining) input tokens, and the (current) set of arcs for the dependency tree. Similar to the shift-reduce parser, the construction of syntactic structure is created by a sequence of transitions. The parser starts with an empty stack and terminates when the input queue is empty, parsing input from left to right. It has fou</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An Efficient Algorithm for Projective Dependency Parsing. In Proceedings of IWPT, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ophel</author>
</authors>
<date>1997</date>
<note>Breadth-First Parsing. citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3035.</note>
<contexts>
<context position="35881" citStr="Ophel, 1997" startWordPosition="5874" endWordPosition="5875">accept a context-free language defined by a contextfree grammar. For each program, the parsers produce a phrase structure referred to as an abstract syntax tree (AST) which is also defined by a context-free language. Parsers including error stabilization and AST-constructors can be generated from context-free grammars for parsers (Kastens et al., 2007). A parser for a new language still requires the development of a complex specification. Moreover, error stabilization often throws away large parts of the source – it is robust but does not care about maximizing accuracy. Breadth-First Parsing (Ophel, 1997) was designed to provide better error stabilization than traditional parsers and parser generators. It uses a two phase approach: the first phase identifies highlevel entities – the second phase parses the structure with these entities as root nonterminals (axioms). Fuzzy Parsing (Koppler, 1997) was designed to efficiently develop parsers by performing the analysis on selected parts of the source instead of the whole input. It is specified by a set of (sub)grammars each with their own axioms. The actual approach is then similar to Breadth-First Parsing: it scans for instances of the axioms and</context>
</contexts>
<marker>Ophel, 1997</marker>
<rawString>John Ophel. 1997. Breadth-First Parsing. citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pete Sawyer</author>
<author>Paul Rayson</author>
<author>Roger Garside</author>
</authors>
<title>REVERE: Support for Requirements Synthesis from Documents. Information Systems Frontiers,</title>
<date>2002</date>
<pages>4--11</pages>
<contexts>
<context position="39374" citStr="Sawyer et al., 2002" startWordPosition="6395" endWordPosition="6398">d of manual specification for defining and adapting the information extraction. General NLP techniques have been applied for extracting facts from general source code comments to support software maintenance (Etzkorn et al., 1999). Comments are extracted from source code using classical lexical analysis; additional information is extracted (and then added) with classical compiler front-end technology. NLP has also been applied to other information extraction tasks in software maintenance to analyze unstructured or very large information sources, e.g., for analyzing requirement specifications (Sawyer et al., 2002), in clone detection (Marcus and Maletic, 2001; Grant and Cordy, 2009), and to connect program documentation to source code (Marcus and Maletic, 2003). 6 Conclusions and Future Work In this paper, we applied natural language parsing techniques to programming languages. One advantage is that it offers robustness, since it always produces some output even if the input is incorrect or incomplete. Completely correct analysis can, however, not be guaranteed even for correct input. However, the experiments showed that accuracy is in fact close to 100%. In contrast to robust information extractors us</context>
</contexts>
<marker>Sawyer, Rayson, Garside, 2002</marker>
<rawString>Pete Sawyer, Paul Rayson, and Roger Garside. 2002. REVERE: Support for Requirements Synthesis from Documents. Information Systems Frontiers, 4(11):343–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Strein</author>
<author>R¨udiger Lincke</author>
<author>Jonas Lundberg</author>
<author>Welf L¨owe</author>
</authors>
<title>An Extensible Meta-Model for Program Analysis.</title>
<date>2007</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>33</volume>
<issue>9</issue>
<marker>Strein, Lincke, Lundberg, L¨owe, 2007</marker>
<rawString>Dennis Strein, R¨udiger Lincke, Jonas Lundberg, and Welf L¨owe. 2007. An Extensible Meta-Model for Program Analysis. IEEE Transactions on Software Engineering, 33(9):592–607.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>