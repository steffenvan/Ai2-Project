<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.889068">
Robust Entity Clustering via Phylogenetic Inference
</title>
<author confidence="0.989203">
Nicholas Andrews and Jason Eisner and Mark Dredze
</author>
<affiliation confidence="0.885716">
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
</affiliation>
<email confidence="0.999349">
{noa,eisner,mdredze}@jhu.edu
</email>
<sectionHeader confidence="0.994831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999555842105263">
Entity clustering must determine when two
named-entity mentions refer to the same
entity. Typical approaches use a pipeline ar-
chitecture that clusters the mentions using
fixed or learned measures of name and con-
text similarity. In this paper, we propose a
model for cross-document coreference res-
olution that achieves robustness by learn-
ing similarity from unlabeled data. The
generative process assumes that each entity
mention arises from copying and option-
ally mutating an earlier name from a sim-
ilar context. Clustering the mentions into
entities depends on recovering this copying
tree jointly with estimating models of the
mutation process and parent selection pro-
cess. We present a block Gibbs sampler for
posterior inference and an empirical evalu-
ation on several datasets.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9980298">
Variation poses a serious challenge for determin-
ing who or what a name refers to. For instance,
Wikipedia contains more than 100 variations of the
name Barack Obama as redirects to the U.S. Presi-
dent article, including:
</bodyText>
<subsectionHeader confidence="0.4387625">
President Obama Barack H. Obama, Jr.
Barak Obamba Barry Soetoro
</subsectionHeader>
<bodyText confidence="0.99993590625">
To relate different names, one solution is to use
specifically tailored measures of name similarity
such as Jaro-Winkler similarity (Winkler, 1999; Co-
hen et al., 2003). This approach is brittle, however,
and fails to adapt to the test data. Another option is
to train a model like stochastic edit distance from
known pairs of similar names (Ristad and Yian-
ilos, 1998; Green et al., 2012), but this requires
supervised data in the test domain.
Even the best model of name similarity is not
enough by itself, since two names that are similar—
even identical—do not necessarily corefer. Docu-
ment context is needed to determine whether they
may be talking about two different people.
In this paper, we propose a method for jointly
(1) learning similarity between names and (2) clus-
tering name mentions into entities, the two major
components of cross-document coreference reso-
lution systems (Baron and Freedman, 2008; Finin
et al., 2009; Rao et al., 2010; Singh et al., 2011;
Lee et al., 2012; Green et al., 2012). Our model
is an evolutionary generative process based on the
name variation model of Andrews et al. (2012),
which stipulates that names are often copied from
previously generated names, perhaps with mutation
(spelling edits). This can deduce that rather than
being names for different entities, Barak Obamba
and Barock obama more likely arose from the fre-
quent name Barack Obama as a common ancestor,
which accounts for most of their letters. This can
also relate seemingly dissimilar names via multiple
steps in the generative process:
</bodyText>
<subsectionHeader confidence="0.634314">
Taylor Swift → T-Swift → T-Swizzle
</subsectionHeader>
<bodyText confidence="0.99783">
Our model learns without supervision that these all
refer to the the same entity. Such creative spellings
are especially common on Twitter and other so-
cial media; we give more examples of coreferents
learned by our model in Section 8.4.
Our primary contributions are improvements on
Andrews et al. (2012) for the entity clustering task.
Their inference procedure only clustered types (dis-
tinct names) rather than tokens (mentions in con-
text), and relied on expensive matrix inversions for
learning. Our novel approach features:
§4.1 A topical model of which entities from previ-
ously written text an author tends to mention
from previously written text.
</bodyText>
<listItem confidence="0.6632046">
§4.2 A name mutation model that is sensitive to
features of the input and output characters and
takes a reader’s comprehension into account.
§5 A scalable Markov chain Monte Carlo sam-
pler used in training and inference.
</listItem>
<page confidence="0.97072">
775
</page>
<note confidence="0.8470855">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775–785,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.939498714285714">
§7 A minimum Bayes risk decoding procedure
to pick an output clustering. The procedure is
applicable to any model capable of producing
a posterior over coreference decisions.
We evaluate our approach by comparing to sev-
eral baselines on datasets from three different gen-
res: Twitter, newswire, and blogs.
</bodyText>
<sectionHeader confidence="0.969857" genericHeader="introduction">
2 Overview and Related Work
</sectionHeader>
<bodyText confidence="0.999947142857143">
Cross-document coreference resolution (CDCR)
was first introduced by Bagga and Baldwin (1998b).
Most approaches since then are based on the intu-
itions that coreferent names tend to have “similar”
spellings and tend to appear in “similar” contexts.
The distinguishing feature of our system is that both
notions of similarity are learned together without
supervision.
We adopt a “phylogenetic” generative model of
coreference. The basic insight is that coreference is
created when an author thinks of an entity that was
mentioned earlier in a similar context, and men-
tions it again in a similar way. The author may
alter the name mention string when copying it, but
both names refer to the same entity. Either name
may later be copied further, leading to an evolution-
ary tree of mentions—a phylogeny. Phylogenetic
models are new to information extraction. In com-
putational historical linguistics, Bouchard-Cˆot´e et
al. (2013) have also modeled the mutation of strings
along the edges of a phylogeny; but for them the
phylogeny is observed and most mentions are not,
while we observe the mentions only.
To apply our model to the CDCR task, we ob-
serve that the probability that two name mentions
are coreferent is the probability that they arose from
a common ancestor in the phylogeny. So we design
a Monte Carlo sampler to reconstruct likely phylo-
genies. A phylogeny must explain every observed
name. While our model is capable of generating
each name independently, a phylogeny will gener-
ally achieve higher probability if it explains similar
names as being similar by mutation (rather than
by coincidence). Thus, our sampled phylogenies
tend to make similar names coreferent—especially
long or unusual names that would be expensive to
generate repeatedly, and especially in contexts that
are topically similar and therefore have a higher
prior probability of coreference.
For learning, we iteratively adjust our model’s
parameters to better explain our samples. That is,
we do unsupervised training via Monte Carlo EM.
What is learned? An important component of
a CDCR system is its model of name similarity
(Winkler, 1999; Porter and Winkler, 1997), which
is often fixed up front. This role is played in our sys-
tem by the name mutation model, which we take to
be a variant of stochastic edit distance (Ristad and
Yianilos, 1996). Rather than fixing its parameters
before we begin CDCR, we learn them (without
supervision) as part of CDCR, by training from
samples of reconstructed phylogenies.
Name similarity is also an important component
of within-document coreference resolution, and ef-
forts in that area bear resemblance to our approach.
Haghighi and Klein (2010) describe an “entity-
centered” model where a distance-dependent Chi-
nese restaurant process is used to pick previous
coreferent mentions within a document. Similarly,
Durrett and Klein (2013) learn a mention similarity
model based on labeled data. Our cross-document
setting has no observed mention ordering and no
observed entities: we must sum over all possibili-
ties, a challenging inference problem.
The second major component of CDCR is
context-based disambiguation of similar or iden-
tical names that refer to the same entity. Like
Kozareva and Ravi (2011) and Green et al. (2012)
we use topics as the contexts, but learn mention
topics jointly with other model parameters.
</bodyText>
<sectionHeader confidence="0.999783" genericHeader="method">
3 Generative Model of Coreference
</sectionHeader>
<bodyText confidence="0.99950955">
Let x = (x1,... , xN) denote an ordered sequence
of distinct named-entity mentions in documents
d = (d1, ... , dD). We assume that each doc-
ument has a (single) known language, and that
its mentions and their types have been identified
by a named-entity recognizer. We use the object-
oriented notation x.v for attribute v of mention x.
Our model generates an ordered sequence x al-
though we do not observe its order. Thus each men-
tion x has latent position x.i (e.g., x729.i = 729).
The entire corpus, including these entities, is gen-
erated according to standard topic model assump-
tions; we first generate a topic distribution for a
document, then sample topics and words for the
document (Blei et al., 2003). However, any topic
may generate an entity type, e.g. PERSON, which is
then replaced by a specific name: when PERSON is
generated, the model chooses a previous mention
of any person and copies it, perhaps mutating its
name.1 Alternatively, the model may manufacture
</bodyText>
<footnote confidence="0.986845">
1We make the closed-world assumption that the author is
</footnote>
<page confidence="0.998223">
776
</page>
<bodyText confidence="0.999757387096774">
a name for a new person, though the name itself
may not be new.
If all previous mentions were equally likely, this
would be a Chinese Restaurant Process (CRP) in
which frequently mentioned entities are more likely
to be mentioned again (“the rich get richer”). We
refine that idea by saying that the current topic, lan-
guage, and document influence the choice of which
previous mention to copy, similar to the distance-
dependent CRP (Blei and Frazier, 2011).2 This will
help distinguish multiple John Smith entities if they
tend to appear in different contexts.
Formally, each mention x is derived from a par-
ent mention x.p where x.p.i &lt; x.i (the parent
came first), x.e = x.p.e (same entity) and x.n is
a copy or mutation of x.p.n. In the special case
where x is a first mention of x.e, x.p is the special
symbol Q, x.e is a newly allocated entity of some
appropriate type, and the name x.n is generated
from scratch.
Our goal is to reconstruct mappings p, i, z that
specify the latent properties of the mentions x. The
mapping p : x H x.p forms a phylogenetic tree on
the mentions, with root Q. Each entity corresponds
to a subtree that is rooted at some child of Q. The
mapping i : x H x.i gives an ordering consistent
with that tree in the sense that (bx)x.p.i &lt; x.i.
Finally, the mapping z : x H x.z specifies, for
each mention, the topic that generated it. While i
and z are not necessary for creating coref clusters,
they are needed to produce p.
</bodyText>
<sectionHeader confidence="0.990927" genericHeader="method">
4 Detailed generative story
</sectionHeader>
<bodyText confidence="0.999921428571429">
Given a few constants that are referenced in the
main text, we assume that the corpus d was gener-
ated as follows.
First, for each topic z = 1,... K and each lan-
guage E, choose a multinomial )3zt over the word
vocabulary, from a symmetric Dirichlet with con-
centration parameter q. Then set m = 0 (entity
</bodyText>
<footnote confidence="0.630261461538462">
only aware of previous mentions from our corpus. This means
that two mentions cannot be derived from a common ancestor
outside our corpus. To mitigate this unrealistic assumption, we
allow any ordering x of the observed mentions, not respecting
document timestamps or forcing the mentions from a given
document to be generated as a contiguous subsequence of x.
2Unlike the ddCRP, our generative story is careful to pro-
hibit derivational cycles: each mention is copied from a previ-
ous mention in the latent ordering. This is why our phylogeny
is a tree, and why our sampler is more complex. Also unlike
the ddCRP, we permit asymmetric “distances”: if a certain
topic or language likes to copy mentions from another, the
compliment is not necessarily returned.
</footnote>
<equation confidence="0.672498">
count), i = 0 (mention count), and for each docu-
ment index d = 1, ... , D:
</equation>
<listItem confidence="0.9900208">
1. Choose the document’s length L and language
E. (The distributions used to choose these
are unimportant because these variables are
always observed.)
2. Choose its topic distribution ψd from an
asymmetric Dirichlet prior with parameters
m (Wallach et al., 2009).3
3. For each token position k = 1, ... , L:
(a) Choose a topic zdk — ψd.
(b) Choose a word conditioned on the topic
and language, wdk — )3zdkt.
(c) If wdk is a named entity type (PERSON,
PLACE, ORG, ...) rather than an ordinary
word, then increment i and:
i. create a new mention x with
</listItem>
<equation confidence="0.990032">
x.e.t = wdk x.d = d x.E = E
x.i = i x.z=zdk x.k = k
</equation>
<bodyText confidence="0.984685454545455">
ii. Choose the parent x.p from a distri-
bution conditioned on the attributes
just set (see §4.1).
iii. If x.p = Q, increment m and set
x.e = a new entity em. Else set
x.e = x.p.e.
iv. Choose x.n from a distribution con-
ditioned on x.p.n and x.E (see §4.2).
Notice that the tokens wdk in document d are
exchangeable: by collapsing out ψd, we can re-
gard them as having been generated from a CRP.
Thus, for fixed values of the non-mention tokens
and their topics, the probability of generating the
mention sequence x is proportional to the prod-
uct of the probabilities of the choices in step 3 at
the positions dk where mentions were generated.
These choices generate a topic x.z (from the CRP
for document d), a type x.e.t (from βx.z), a par-
ent mention (from the distribution over previous
mentions), and a name string (conditioned on the
parent’s name if any). §5 uses this fact to construct
an MCMC sampler for the latent parts of x.
</bodyText>
<subsectionHeader confidence="0.959743">
4.1 Sub-model for parent selection
</subsectionHeader>
<bodyText confidence="0.9954655">
To select a parent for a mention x of type t = x.e.t,
a simple model (as mentioned above) would be a
CRP: each previous mention of the same type is
selected with probability proportional to 1, and Q is
</bodyText>
<footnote confidence="0.805199">
3Extension: This choice could depend on the language d.f.
</footnote>
<page confidence="0.994885">
777
</page>
<bodyText confidence="0.999943428571429">
selected with probability proportional to αt &gt; 0. A
larger choice of αt results in smaller entity clusters,
because it prefers to create new entities of type t
rather than copying old ones.
We modify this story by re-weighting Q and
previous mentions according to their relative suit-
ability as the parent of x:
</bodyText>
<equation confidence="0.997799">
exp(φ · f(x.p, x))
Prφ(x.p  |x) = (1)
Z(x)
</equation>
<bodyText confidence="0.99925325">
where x.p ranges over Q and all previous mentions
of the same type as x, that is, mentions p such that
p.i &lt; x.i and p.e.t = x.e.t. The normalizing con-
stant Z(x) def= exp (φ · f(x.p, x)) is chosen
</bodyText>
<subsectionHeader confidence="0.96678">
4.2 Sub-model for name mutation
</subsectionHeader>
<bodyText confidence="0.999957666666667">
Let x denote a mention with parent p = x.p. As in
Andrews et al. (2012), its name x.n is a stochastic
transduction of its parent’s name p.n. That is,
</bodyText>
<equation confidence="0.9785">
Prθ(x.n  |p.n) (2)
</equation>
<bodyText confidence="0.950215793103448">
is given by the probability that applying a random
sequence of edits to the characters of p.n would
yield x.n. The contextual probabilities of different
edits depend on learned parameters θ.
(2) is the total probability of all edit sequences
that derive x.n from p.n. It can be computed in
time O(|x.n |· |p.n|) by dynamic programming.
The probability of a single edit sequence, which
corresponds to a monotonic alignment of x.n to
p.n, is a product of individual edit probabilities of
the form Prθ((ab)  |ˆa), which is conditioned on the
next input character ˆa. The edit (ab) replaces input
a E {E, ˆa} with output b E {E} U Σ (where E is
4Many other features could be added. In a multilingual
setting, one would similarly want to model whether English
authors select Arabic mentions. One could also imagine fea-
tures that reward proximity in the generative order (x.p.i ≈
x.i), local linguistic relationships (when x.p.d = x.d and
x.p.k ≈ x.k), or social information flow (e.g., from main-
stream media to Twitter). One could also make more specific
versions of any feature by conjoining it with the entity type t.
the empty string and Σ is the alphabet of language
x. E). Insertions and deletions are the cases where
respectively a = E or b = E—we do not allow both
at once. All other edits are substitutions. When
aˆ is the special end-of-string symbol #, the only
allowed edits are the insertion (b) and the substi-
tution (##). We define the edit probability using a
locally normalized log-linear model:
</bodyText>
<equation confidence="0.986478">
exp(θ · f(ˆa, a, b))
Prθ((a b)  |ˆa) = (3)
Ea/,b/ exp(θ · f(ˆa, a&apos;, b&apos;))
</equation>
<bodyText confidence="0.976596431372549">
We use a small set of simple feature functions f,
which consider conjunctions of the attributes of the
characters aˆ and b: character, character class (letter,
digit, etc.), and case (upper vs. lower).
More generally, the probability (2) may also be
conditioned on other variables such as on the lan-
guages p. E and x. E—this leaves room for a translit-
eration model when x. E =� p. E—and on the entity
type x.t. The features in (3) may then depend on
these variables as well.
Notice that we use a locally normalized proba-
bility for each edit. This enables faster and sim-
pler training than the similar model of Dreyer et al.
(2008), which uses a globally normalized probabil-
ity for the whole edit sequence.
When p = Q, we are generating a new name x.n.
We use the same model, taking Q.n to be the empty
string (but with #p rather than # as the end-of-
string symbol). This yields a feature-based unigram
language model (whose character probabilities may
differ from usual insertion probabilities because
they see #p as the lookahead character).
Pragmatics. We can optionally make the model
more sophisticated. Authors tend to avoid names
x.n that readers would misinterpret (given the pre-
viously generated names). The edit model thinks
that Prθ(CIA  |Q) is relatively high (because CIA is
a short string) and so is Prθ(CIA  |Chuck’s Ice Art).
But in fact, if CIA has already been frequently used
to refer to the Central Intelligence Agency, then an
author is unlikely to use it for a different entity.
To model this pragmatic effect, we multiply
our definition of Prθ(x.n  |p.n) by an extra fac-
tor Pr(x.e  |x)γ, where γ &gt; 0 is the effect
strength.5 Here Pr(x.e  |x) is the probability that
a reader correctly identifies the entity x.e. We
take this to be the probability that a reader who
knows our sub-models would guess some parent
5Currently we omit the step of renormalizing this deficient
model. Our training procedure also ignores the extra factor.
p
so that the probabilities sum to 1.
This is a conditional log-linear model parameter-
ized by φ, where φk — N(0, σ2k). The features f
are extracted from the attributes of x and x.p. Our
most important feature tests whether x.p.z = x.z.
This binary feature has a high weight if authors
mainly choose mentions from the same topic. To
model which (other) topics tend to be selected, we
also have a binary feature for each parent topic
x.p.z and each topic pair (x.p.z, x.z).4
</bodyText>
<page confidence="0.98804">
778
</page>
<bodyText confidence="0.999659666666667">
having the correct entity (or 0 if x is a first men-
tion): Ep1:p1.e=x.e w(p&apos;, x)/ Ep1 w(p&apos;, x). Here p&apos;
ranges over mentions (including 0) that precede
x in the ordering i, and w(p&apos;, x)—defined later in
sec. 5.3—is proportional to the posterior probabil-
ity that x.p = p&apos;, given name x.n and topic x.z.6
</bodyText>
<sectionHeader confidence="0.899228" genericHeader="method">
5 Inference by Block Gibbs Sampling
</sectionHeader>
<bodyText confidence="0.9990055">
We use a block Gibbs sampler, which from an ini-
tial state (p0, i0, z0) repeats these steps:
</bodyText>
<listItem confidence="0.999858">
1. Sample the ordering i from its conditional
distribution given all other variables.
2. Sample the topic vector z likewise.
3. Sample the phylogeny p likewise.
4. Output the current sample st = (p, i, z).
</listItem>
<bodyText confidence="0.999892571428571">
It is difficult to draw exact samples at steps 1
and 2. Thus, we sample i or z from a simpler
proposal distribution, but correct the discrepancy
using the Independent Metropolis-Hastings (IMH)
strategy: with an appropriate probability, reject the
proposed new value and instead use another copy
of the current value (Tierney, 1994).
</bodyText>
<subsectionHeader confidence="0.996026">
5.1 Resampling the ordering i
</subsectionHeader>
<bodyText confidence="0.99976025">
We resample the ordering i of the mentions x,
conditioned on the other variables. The current
phylogeny p already defines a partial order on x,
since each parent must precede its children. For
instance, phylogeny (a) below requires 0 � x and
0 � y. This partial order is compatible with 2
total orderings, 0 � x � y and 0 � y � x. By
contrast, phylogeny (b) requires the total ordering
</bodyText>
<equation confidence="0.99115725">
0 � x � y. 0
0
x y
(a)
</equation>
<bodyText confidence="0.999966375">
We first sample an ordering i0 (the ordering
of mentions with parent 0, i.e. all mentions) uni-
formly at random from the set of orderings compat-
ible with the current p. (We provide details about
this procedure in Appendix A.)7 However, such or-
derings are not in fact equiprobable given the other
variables—some orderings better explain why that
phylogeny was chosen in the first place, according
</bodyText>
<footnote confidence="0.920426">
6Better, one could integrate over the reader’s guess of x.z.
7The full version of this paper is available at
http://cs.jhu.edu/˜noa/publications/
phylo-acl-14.pdf
</footnote>
<bodyText confidence="0.999270666666667">
to our competitive parent selection model (§4.1).
To correct for this bias using IMH, we accept the
proposed ordering i0 with probability
</bodyText>
<equation confidence="0.994562333333333">
� 1, Pr(p, i♦, z, x θ, φ)
a = min (4)
Pr(p, i, z, x θ, φ)
</equation>
<bodyText confidence="0.9988555">
where i is the current ordering. Otherwise we reject
i0 and reuse i for the new sample.
</bodyText>
<subsectionHeader confidence="0.998151">
5.2 Resampling the topics z
</subsectionHeader>
<bodyText confidence="0.998591333333333">
Each context word and each named entity is asso-
ciated with a latent topic. The topics of context
words are assumed exchangeable, and so we re-
sample them using Gibbs sampling (Griffiths and
Steyvers, 2004).
Unfortunately, this is prohibitively expensive for
the (non-exchangeable) topics of the named men-
tions x. A Gibbs sampler would have to choose
a new value for x.z with probability proportional
to the resulting joint probability of the full sample.
This probability is expensive to evaluate because
changing x.z will change the probability of many
edges in the current phylogeny p. (Equation (1)
puts x is in competition with other parents, so ev-
ery mention y that follows x must recompute how
happy it is with its current parent y.p.)
Rather than resampling one topic at a time, we re-
sample z as a block. We use a proposal distribution
for which block sampling is efficient, and use IMH
to correct the error in this proposal distribution.
Our proposal distribution is an undirected graph-
ical model whose random variables are the topics
z and whose graph structure is given by the current
phylogeny p:
</bodyText>
<equation confidence="0.990463">
Q(z) a rl Tx(x.z)Tx.p,x(x.p.z, x.z) (5)
x�0
</equation>
<bodyText confidence="0.997376571428571">
Q(z) is an approximation to the posterior distri-
bution over z. As detailed below, a proposal can
be sampled from Q(z) in time O( z K2) where K
is the number of topics, because the only interac-
tions among topics are along the edges of the tree
p. The unary factor Tx gives a weight for each
possible value of x.z, and the binary factor Tx.p,x
gives a weight for each possible value of the pair
(x.p.z,x.z).
The Tx(x.z) factors in (5) approximate the topic
model’s prior distribution over z. Tx(x.z) is pro-
portional to the probability that a Gibbs sampling
step for an ordinary topic model would choose this
value of x.z. This depends on whether—in the
</bodyText>
<figure confidence="0.898028333333333">
x
y
(b)
</figure>
<page confidence="0.988547">
779
</page>
<bodyText confidence="0.9997020625">
current sample—x.z is currently common in x’s
document and x.t is commonly generated by x.z.
It ignores the fact that we will also be resampling
the topics of the other mentions.
The XFx.p,x factors in (5) approximate Pr(p |
z, i) (up to a constant factor), where p is the current
phylogeny. Specifically, XFx.p,x approximates the
probability of a single edge. It ought to be given
by (1), but we use only the numerator of (1), which
avoids modeling the competition among parents.
We sample from Q using standard methods, sim-
ilar to sampling from a linear-chain CRF by run-
ning the backward algorithm followed by forward
sampling. Specifically, we run the sum-product
algorithm from the leaves up to the root ♦, at each
node x computing the following for each topic z:
</bodyText>
<equation confidence="0.9887705">
βx (z) def &amp;x (z) · rl
Y yEchildren(x)
</equation>
<bodyText confidence="0.999571333333333">
Then we sample from the root down to the leaves,
first sampling ♦.z from β♦, then at each x =6 ♦
sampling the topic x.z to be z with probability
proportional to XFx.p,x(x.p.z, z) · βx(z).
Again we use IMH to correct for the bias in Q:
we accept the resulting proposal zˆ with probability
</bodyText>
<equation confidence="0.993996">
minC1, Pr(p, i, ˆz, x  |θ, φ) Q(z)1 (6)
Pr(p, i, z, x  |θ, φ) Q(ˆz) )
</equation>
<bodyText confidence="0.996430857142857">
While Pr(p, i, ˆz, x  |θ, φ) might seem slow to
compute because it contains many factors (1) with
different denominators Z(x), one can share work
by visiting the mentions x in their order i. Most
summands in Z(x) were already included in Z(x&apos;),
where x&apos; is the latest previous mention having the
same attributes as x (e.g., same topic).
</bodyText>
<subsectionHeader confidence="0.994965">
5.3 Resampling the phylogeny p
</subsectionHeader>
<bodyText confidence="0.999925470588235">
It is easy to resample the phylogeny. For each x, we
must choose a parent x.p from among the possible
parents p (having p.i &lt; x.i and p.e.t = x.e.t).
Since the ordering i prevents cycles, the resulting
phylogeny p is indeed a tree.
Given the topics z, the ordering i, and the ob-
served names, we choose an x.p value according
to its posterior probability. This is proportional to
w(x.p, x) def = PrO(x.p  |x) · Prg(x.n  |x.p.n),
independent of any other mention’s choice of par-
ent. The two factors here are given by (1) and (2)
respectively. As in the previous section, the de-
nominators Z(x) in the Pr(x.p  |x) factors can be
computed efficiently with shared work.
With the pragmatic model (section 4.2), the par-
ent choices are no longer independent; then the
samples of p should be corrected by IMH as usual.
</bodyText>
<subsectionHeader confidence="0.983496">
5.4 Initializing the sampler
</subsectionHeader>
<bodyText confidence="0.999990875">
The initial sampler state (z0, p0, i0) is obtained as
follows. (1) We fix topics z0 via collapsed Gibbs
sampling (Griffiths and Steyvers, 2004). The sam-
pler is run for 1000 iterations, and the final sam-
pler state is taken to be z0. This process treats all
topics as exchangeable, including those associated
with named entities.(2) Given the topic assignment
z0, initialize p0 to the phylogeny rooted at ♦ that
maximizes Ex log w(x.p, x). This is a maximum
rooted directed spanning tree problem that can be
solved in time O(n2) (Tarjan, 1977). The weight
w(x.p, x) is defined as in section 5.3—except that
since we do not yet have an ordering i, we do not
restrict the possible values of x.p to mentions p
with p.i &lt; x.p.i. (3) Given p0, sample an ordering
i0 using the procedure described in §5.1.
</bodyText>
<sectionHeader confidence="0.992201" genericHeader="method">
6 Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999181111111111">
Evaluating the likelihood and its partial derivatives
with respect to the parameters of the model requires
marginalizing over our latent variables. As this
marginalization is intractable, we resort to Monte
Carlo EM procedure (Levine and Casella, 2001)
which iterates the following two steps:
E-step: Collect samples by MCMC simulation as
in §5, given current model parameters θ and φ.
M-step: Improve θ and φ to increase8
</bodyText>
<equation confidence="0.960162">
log Prg,O(x,ps, is, zs) (7)
</equation>
<bodyText confidence="0.9997844">
It is not necessary to locally maximize L at each
M-step, merely to improve it if it is not already
at a local maximum (Dempster et al., 1977). We
improve it by a single update: at the tth M-step, we
update our parameters to 4&apos;t = (θt, φt)
</bodyText>
<equation confidence="0.9915">
4&apos;t = 4&apos;t−1 + aEt∇,DL(x, 4&apos;t−1) (8)
</equation>
<bodyText confidence="0.9999828">
where a is a fixed scaling term and Et is an adap-
tive learning rate given by AdaGrad (Duchi et al.,
2011).
We now describe how to compute the gradient
∇,DL. The gradient with respect to the parent se-
</bodyText>
<footnote confidence="0.5497205">
8We actually do MAP-EM, which augments (7) by adding
the log-likelihoods of θ and φ under a Gaussian prior.
</footnote>
<equation confidence="0.95549475">
� XFx,y(z, z&apos;) · βy(z&apos;)
z�
L def = 1
S
S
s=1
780
lection parameters φ is
⎛ ⎞
�1
S f (p, x) − E Prφ (p0  |x)f (p0, x) (9)
p0
</equation>
<bodyText confidence="0.965520157894737">
samples. The other variables in (9) are associ-
ated with the edge being summed over. That edge
explains a mention x as a mutation of some parent
p in the context of a particular sample (ps, is, zs).
The possible parents
range over
and the men-
tions that precede x according to the ordering is,
while the features f and distribution
depend
on the topics zs.
As for the mutation parameters, let
be the
fraction of samples in which p is the parent of x.
This is the expected number of times that the string
p.n mutated into x.n. Given this weighted set of
string pairs, let
be the expected number of
times that edit
</bodyText>
<equation confidence="0.929172454545454">
was chosen in context
this
can
S
p0
♦
Prφ
cp,x
cˆa,a,b
(ab)
ˆa:
</equation>
<bodyText confidence="0.8832804">
be computed using dynamic programming to
marginalize over the latent edit sequence that maps
p.n to x.n, for each (p, x). The gradient of L with
respect to θ is
function L for use with (11):
</bodyText>
<equation confidence="0.953531541666667">
R(e0,e) def = TP+FP+TN+FN
TP + TN TP + TN
the clustering
to evaluate how well
classi-
fies the
More similar clusterings achieve larger R, with
e) = 1 iff
= e. In all cases, 0
The MBR decision rule for the (negated) Ran
e
e0
�N � mention pairs as coreferent or not.
2
R(e0,
e0
≤R(e0, e) = R(e, e0) ≤ 1.
d
index is easily seen to be equivalent to
e∗ = argmax E[TP] + (12)
e0
� �
= argmax sij + (1 − sij)
e0 i,j: xi∼xj i,j: xi6∼xj
</equation>
<bodyText confidence="0.9984658">
where ∼ denotes coreference according to e0. As
explained above, the sij are coreference probabil-
ities sij that can be estimated from a sample of
clusterings e.
This objective corresponds to min-max graph
</bodyText>
<equation confidence="0.998307">
� Prθ(a0,b0|ˆa)f(ˆa, a0, b0)) approximate solution (Nie et al., 2010).9
ˆa,a,b (10)
Pr(e
�e∗ = argminL(e0,e)
 |x, θ, φ) (11)
e0 e
</equation>
<bodyText confidence="0.834894">
This minimizes our expected loss, where
</bodyText>
<figure confidence="0.336975">
e)
denotes the loss associated with picking
when
the true clustering is e. In practice, we again esti-
L(e0,
e0
</figure>
<bodyText confidence="0.857053833333333">
ng e values.
efficient choice of loss
consist of entity annotations, and so we foll
ow the
evaluation procedure of Yogatama et al. (2012).
The outer summation ranges over all edges in the
</bodyText>
<equation confidence="0.932721">
�cˆa,a,b(f(ˆa, a, b) −
a0,b0
</equation>
<sectionHeader confidence="0.96947" genericHeader="method">
7 Consensus Clustering
</sectionHeader>
<bodyText confidence="0.97742275">
From a single phylogeny p, we deterministically
obtain a clustering
by removing the root
Each
of the resulting connected components corresponds
to a cluster of mentions. Our model gives a distribu-
tion over phylogenies
(given observations x and
learned parameters
thus gives a posterior
distribution over clusterings e, which can be used
to answer various queries.
A traditional query is to request a single cluster-
ing e. We prefer the clustering
that minimizes
Bayes risk (MBR) (Bickel an
</bodyText>
<equation confidence="0.676293333333333">
e
♦.
p
Φ)—and
e∗
d Doksum, 1977):
</equation>
<bodyText confidence="0.970576666666667">
mate the expectation by sampli
The Rand index (Rand, 1971)—unlike our actual
evaluation measure—is an
</bodyText>
<equation confidence="0.904885">
�N �
2
</equation>
<bodyText confidence="0.99504475">
where the true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN) use
cut (Ding et al., 2001), an NP-hard problem with
an
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.998818538461538">
In this section, we describe experiments on three
different datasets. Our main results are described
first: Twitter features many instances of name vari-
ation that we would like our model to be able to
learn. We also report the performance of different
ablations of our full approach, in order to see which
consistently helped across the different splits. We
report additional experiments on the ACE 2008 cor-
pus, and on a political blog corpus, to demonstrate
that our approach is applicable in different settings.
For Twitter and ACE 2008, we report the stan-
dard B3 metric (Bagga and Baldwin, 1998a). For
the political blog dataset, the reference does not
</bodyText>
<subsectionHeader confidence="0.987669">
8.1 Twitter
</subsectionHeader>
<bodyText confidence="0.95733675">
Data. We use a novel corpus of Twitter posts dis-
cussing the 2013 Grammy Award ceremony. This
is a challenging corpus, featuring many instan
ces
</bodyText>
<footnote confidence="0.9836936">
9In our experiments, we run the clustering algorithm five
times, initialized from samples chosen at random from the last
10% of the sampler run, and keep the clustering that achieved
d score.
highest expected Ran
</footnote>
<page confidence="0.9963">
781
</page>
<bodyText confidence="0.999851066666667">
of name variation. The dataset consists of five splits
(by entity), the smallest of which is 604 mentions
and the largest is 1374. We reserve the largest split
for development purposes, and report our results
on the remaining four. Appendix B provides more
detail about the dataset.
Baselines. We use the discriminative entity cluster-
ing algorithm of Green et al. (2012) as our baseline;
their approach was found to outperform another
generative model which produced a flat cluster-
ing of mentions via a Dirichlet process mixture
model. Their method uses Jaro-Winkler string sim-
ilarity to match names, then clusters mentions with
matching names (for disambiguation) by compar-
ing their unigram context distributions using the
Jenson-Shannon metric. We also compare to the
EXACT-MATCH baseline, which assigns all strings
with the same name to the same entity.
Procedure. We run four test experiments in which
one split is used to pick model hyperparameters
and the remaining three are used for test. For the
discriminative baseline, we tune the string match
threshold, context threshold, and the weight of the
context model prior (all via grid search). For our
model, we tune only the fixed weight of the root
feature, which determines the precision/recall trade-
off (larger values of this feature result in more
attachments to Q and hence more entities). We
leave other hyperparameters fixed: 16 latent top-
ics, and Gaussian priors N(0,1) on all log-linear
parameters. For PHYLO, the entity clustering is
the result of (1) training the model using EM, (2)
sampling from the posterior to obtain a distribu-
tion over clusterings, and (3) finding a consensus
clustering. We use 20 iterations of EM with 100
samples per E-step for training, and use 1000 sam-
ples after training to estimate the posterior. We
report results using three variations of our model:
PHYLO does not consider mention context (all men-
tions effectively have the same topic) and deter-
mines mention entities from a single sample of
P (the last); PHYLO+TOPIC adds context (§5.2);
PHYLO+TOPIC+MBR uses the full posterior and
consensus clustering to pick the output clustering
(§7). Our results are shown in Table 1.10
</bodyText>
<footnote confidence="0.9043168">
10Our single-threaded implementation took around 15 min-
utes per fold of the Twitter corpus on a personal laptop with
a 2.3 Ghz Intel Core i7 processor (including time required to
parse the data files). Typical acceptance rates for ordering and
topic proposals ranged from 0.03 to 0.08.
</footnote>
<table confidence="0.999815">
Mean Test B3
P R F1
EXACT-MATCH 99.6 53.7 69.8
Green et al. (2012) 92.1 69.8 79.3
PHYLO 85.3 91.4 88.7
PHYLO+TOPIC 92.8 90.8 91.8
PHYLO+TOPIC+MBR 92.9 90.9 91.9
</table>
<tableCaption confidence="0.9392402">
Table 1: Results for the Twitter dataset. Higher B3 scores
are better. Note that each number is averaged over four
different test splits. In three out of four experiments,
PHYLO+TOPIC+MBR achieved the highest F1 score; in one
case PHYLO+TOPIC won by a small margin.
</tableCaption>
<table confidence="0.999911125">
P Test B3 F1
R
EXACT-MATCH 98.0 81.2 88.8
PER Green et al. (2012) 95.0 88.9 91.9
PHYLO+TOPIC+MBR 97.2 88.6 92.7
EXACT-MATCH 98.2 78.3 87.1
ORG Green et al. (2012) 92.1 88.5 90.3
PHYLO+TOPIC+MBR 95.5 80.9 87.6
</table>
<tableCaption confidence="0.992472">
Table 2: Results for the ACE 2008 newswire dataset.
</tableCaption>
<subsectionHeader confidence="0.993149">
8.2 Newswire
</subsectionHeader>
<bodyText confidence="0.9999435">
Data. We use the ACE 2008 dataset, which is
described in detail in Green et al. (2012). It is
split into a development portion and a test portion.
The baseline system took the first mention from
each (gold) within-document coreference chain as
the canonical mention, ignoring other mentions in
the chain; we follow the same procedure in our
experiments.11
Baselines &amp; Procedure. We use the same base-
lines as in §8.1. On development data, modeling
pragmatics as in §4.2 gave large improvements for
organizations (8 points in F-measure), correcting
the tendency to assume that short names like CIA
were coincidental homonyms. Hence we allowed
γ &gt; 0 and tuned it on development data.12 Results
are in Table 2.
</bodyText>
<subsectionHeader confidence="0.997803">
8.3 Blogs
</subsectionHeader>
<bodyText confidence="0.7919015">
Data. The CMU political blogs dataset consists of
3000 documents about U.S. politics (Yano et al.,
2009). Preprocessed as described in Yogatama et al.
(2012), the data consists of 10647 entity mentions.
</bodyText>
<footnote confidence="0.990661857142857">
11That is, each within-document coreference chain is
mapped to a single mention as a preprocessing step.
12We used only a simplified version of the pragmatic model,
approximating w(p , x) as 1 or 0 according to whether p .n =
x.n. We also omitted the IMH step from section 5.3. The
other results we report do not use pragmatics at all, since we
found that it gave only a slight improvement on Twitter.
</footnote>
<page confidence="0.9939">
782
</page>
<bodyText confidence="0.999861393939394">
Unlike our other datasets, mentions are not anno-
tated with entities: the reference consists of a table
of 126 entities, where each row is the canonical
name of one entity.
Baselines. We compare to the system results
reported in Figure 2 of Yogatama et al. (2012).
This includes a baseline hierarchical clustering ap-
proach, the “EEA” name canonicalization system
of Eisenstein et al. (2011), as well the model pro-
posed by Yogatama et al. (2012). Like the output
of our model, the output of their hierarchical clus-
tering baseline is a mention clustering, and there-
fore must be mapped to a table of canonical entity
names to compare to the reference table.
Procedure &amp; Results We tune our method as in
previous experiments, on the initialization data
used by Yogatama et al. (2012) which consists of
a subset of 700 documents of the full dataset. The
tuned model then produced a mention clustering
on the full political blog corpus. As the mapping
from clusters to a table is not fully detailed in Yo-
gatama et al. (2012), we used a simple heuristic:
the most frequent name in each cluster is taken as
the canonical name, augmented by any titles from
a predefined list appearing in any other name in
the cluster. The resulting table is then evaluated
against the reference, as described in Yogatama et
al. (2012). We achieved a response score of 0.17
and a reference score of 0.61. Though not state-of-
the-art, this result is close to the score of the “EEA”
system of Eisenstein et al. (2011), as reported in
Figure 2 of Yogatama et al. (2012), which is specif-
ically designed for the task of canonicalization.
</bodyText>
<subsectionHeader confidence="0.718839">
8.4 Discussion
</subsectionHeader>
<bodyText confidence="0.921126888888889">
On the Twitter dataset, we obtained a 12.6-point F1
improvement over the baseline. To understand our
model’s behavior, we looked at the sampled phy-
logenetic trees on development data. One reason
our model does well in this noisy domain is that
it is able to relate seemingly dissimilar names via
successive steps. For instance, our model learned
to relate many variations of LL Cool J:
Cool James LLCoJ El-El Cool John
</bodyText>
<sectionHeader confidence="0.71799" genericHeader="method">
LL LL COOL JAMES LLCOOLJ
</sectionHeader>
<bodyText confidence="0.999990414634146">
In the sample we inspected, these mentions were
also assigned the same topic, further boosting the
probability of the configuration.
The ACE dataset, consisting of editorialized
newswire, naturally contains less name variation
than Twitter data. Nonetheless, we find that the
variation that does appear is often properly handled
by our model. For instance, we see several in-
stances of variation due to transliteration that were
all correctly grouped together, such as Megawati
Soekarnoputri and Megawati Sukarnoputri. The prag-
matic model was also effective in grouping com-
mon acronyms into the same entity.
We found that multiple samples tend to give dif-
ferent phylogenies (so the sampler is mobile), but
essentially the same clustering into entities (which
is why consensus clustering did not improve much
over simply using the last sample). Random restarts
of EM might create more variety by choosing dif-
ferent locally optimal parameter settings. It may
also be beneficial to explore other sampling tech-
niques (Bouchard-Cˆot´e, 2014).
Our method assembles observed names into an
evolutionary tree. However, the true tree must in-
clude many names that fall outside our small ob-
served corpora, so our model would be a more
appropriate fit for a far larger corpus. Larger cor-
pora also offer stronger signals that might enable
our Monte Carlo methods to mix faster and detect
regularities more accurately.
A common error of our system is to connect
mentions that share long substrings, such as dif-
ferent PERSONs who share a last name, or differ-
ent ORGANIZATIONs that contain University of. A
more powerful name mutation than the one we use
here would recognize entire words, for example
inserting a common title or replacing a first name
with its common nickname. Modeling the internal
structure of names (Johnson, 2010; Eisenstein et
al., 2011; Yogatama et al., 2012) in the mutation
model is a promising future direction.
</bodyText>
<sectionHeader confidence="0.996341" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999772692307692">
Our primary contribution consists of new model-
ing ideas, and associated inference techniques, for
the problem of cross-document coreference resolu-
tion. We have described how writers systematically
plunder (φ) and then systematically modify (θ) the
work of past writers. Inference under such models
could also play a role in tracking evolving memes
and social influence, not merely in establishing
strict coreference. Our model also provides an al-
ternative to the distance-dependent CRP.2
Our implementation is available for re-
search use at: https://bitbucket.org/
noandrews/phyloinf.
</bodyText>
<page confidence="0.997951">
783
</page>
<sectionHeader confidence="0.983139" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99947887735849">
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 344–355, Jeju, Korea, July.
Amit Bagga and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In In The First In-
ternational Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference,
pages 563–566.
Amit Bagga and Breck Baldwin. 1998b. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ’98, pages
79–85, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 274–283, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics : Basic Ideas and Selected Topics.
Holden-Day, Inc.
David M. Blei and Peter I. Frazier. 2011. Distance
dependent chinese restaurant processes. J. Mach.
Learn. Res., 12:2461–2488, November.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022.
Alexandre Bouchard-Cˆot´e, David Hall, Thomas L.
Griffiths, and Dan Klein. 2013. Automated re-
construction of ancient languages using probabilis-
tic models of sound change. Proceedings of the Na-
tional Academy of Sciences.
Alexandre Bouchard-Cˆot´e. 2014. Sequential Monte
Carlo (SMC) for Bayesian phylogenetics. Bayesian
phylogenetics: methods, algorithms, and applica-
tions.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string metrics for
matching names and records. In KDD Workshop on
data cleaning and object consolidation.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1–38.
C.H.Q. Ding, Xiaofeng He, Hongyuan Zha, Ming Gu,
and H.D. Simon. 2001. A min-max cut algorithm
for graph partitioning and data clustering. In Data
Mining, 2001. ICDM 2001, Proceedings IEEE Inter-
national Conference on, pages 107 –114.
Mark Dredze, Michael J Paul, Shane Bergsma, and
Hieu Tran. 2013. Carmen: A twitter geolocation
system with applications to public health. In AAAI
Workshop on Expanding the Boundaries of Health
Informatics Using AI (HIAI).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1080–1089, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971–1982.
Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William W. Cohen,
Noah A. Smith, and Eric P. Xing. 2011. Structured
databases of named entities from bayesian nonpara-
metrics. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ’11, pages
2–12, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
T. Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Pi-
atko. 2009. Using Wikitology for cross-document
entity coreference resolution. In AAAI Spring Sym-
posium on Learning by Reading and Learning to
Read.
Spence Green, Nicholas Andrews, Matthew R. Gorm-
ley, Mark Dredze, and Christopher D. Manning.
2012. Entity clustering across languages. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ’12, pages 60–69, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228–5235.
Stephen Guo, Ming-Wei Chang, and Emre Kıcıman.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of NAACL-HLT,
pages 1020–1030.
</reference>
<page confidence="0.981712">
784
</page>
<reference confidence="0.999836009345795">
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 385–393,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Mark Johnson. 2010. Pcfgs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 1148–1157,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsuper-
vised name ambiguity resolution using a generative
model. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ’11, pages
105–112, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Richard A. Levine and George Casella. 2001. Im-
plementations of the Monte Carlo EM Algorithm.
Journal of Computational and Graphical Statistics,
10(3):422–439.
Feiping Nie, Chris H. Q. Ding, Dijun Luo, and Heng
Huang. 2010. Improved minmax cut graph cluster-
ing with nonnegative relaxation. In Jos´e L. Balc´azar,
Francesco Bonchi, Aristides Gionis, and Mich`ele
Sebag, editors, ECML/PKDD (2), volume 6322 of
Lecture Notes in Computer Science, pages 451–466.
Springer.
E. H. Porter and W. E. Winkler, 1997. Approximate
String Comparison and its Effect on an Advanced
Record Linkage System, chapter 6, pages 190–199.
U.S. Bureau of the Census.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846–850.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 1050–1058, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science.
Eric Sven Ristad and Peter N. Yianilos. 1998.
Learning string edit distance. IEEE Transactions
on Pattern Recognition and Machine Intelligence,
20(5):522–532, May.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524–1534. Association for Computational Linguis-
tics.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-scale
cross-document coreference using distributed infer-
ence and hierarchical models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 793–803, Portland, Oregon, USA, June.
Association for Computational Linguistics.
R E Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25–35.
Luke Tierney. 1994. Markov Chains for Exploring
Posterior Distributions. The Annals of Statistics,
22(4):1701–1728.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
Advances in Neural Information Processing Systems,
pages 1973–1981.
Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A discriminative hierarchical model for fast
coreference at large scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
’12, pages 379–388, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
William E. Winkler. 1999. The state of record link-
age and current research problems. Technical report,
Statistical Research Division, U.S. Census Bureau.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ’09, pages
477–485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dani Yogatama, Yanchuan Sim, and Noah A. Smith.
2012. A probabilistic model for canonicalizing
named entity mentions. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers - Volume 1, ACL
’12, pages 685–693, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.998511">
785
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.602231">
<title confidence="0.996235">Robust Entity Clustering via Phylogenetic Inference</title>
<author confidence="0.999579">Andrews Eisner</author>
<affiliation confidence="0.999742">Department of Computer Science and Human Language Technology Center of</affiliation>
<address confidence="0.813751">Johns Hopkins 3400 N. Charles St., Baltimore, MD 21218</address>
<abstract confidence="0.9981224">Entity clustering must determine when two named-entity mentions refer to the same entity. Typical approaches use a pipeline architecture that clusters the mentions using fixed or learned measures of name and context similarity. In this paper, we propose a model for cross-document coreference resolution that achieves robustness by learning similarity from unlabeled data. The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context. Clustering the mentions into entities depends on recovering this copying tree jointly with estimating models of the mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Andrews</author>
<author>Jason Eisner</author>
<author>Mark Dredze</author>
</authors>
<title>Name phylogeny: A generative model of string variation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>344--355</pages>
<location>Jeju, Korea,</location>
<contexts>
<context position="2490" citStr="Andrews et al. (2012)" startWordPosition="391" endWordPosition="394">by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our model learns without supervision that these all refer to the the same entity. Such creative spellings are especially common on Twi</context>
<context position="13879" citStr="Andrews et al. (2012)" startWordPosition="2341" endWordPosition="2344">tional to αt &gt; 0. A larger choice of αt results in smaller entity clusters, because it prefers to create new entities of type t rather than copying old ones. We modify this story by re-weighting Q and previous mentions according to their relative suitability as the parent of x: exp(φ · f(x.p, x)) Prφ(x.p |x) = (1) Z(x) where x.p ranges over Q and all previous mentions of the same type as x, that is, mentions p such that p.i &lt; x.i and p.e.t = x.e.t. The normalizing constant Z(x) def= exp (φ · f(x.p, x)) is chosen 4.2 Sub-model for name mutation Let x denote a mention with parent p = x.p. As in Andrews et al. (2012), its name x.n is a stochastic transduction of its parent’s name p.n. That is, Prθ(x.n |p.n) (2) is given by the probability that applying a random sequence of edits to the characters of p.n would yield x.n. The contextual probabilities of different edits depend on learned parameters θ. (2) is the total probability of all edit sequences that derive x.n from p.n. It can be computed in time O(|x.n |· |p.n|) by dynamic programming. The probability of a single edit sequence, which corresponds to a monotonic alignment of x.n to p.n, is a product of individual edit probabilities of the form Prθ((ab)</context>
</contexts>
<marker>Andrews, Eisner, Dredze, 2012</marker>
<rawString>Nicholas Andrews, Jason Eisner, and Mark Dredze. 2012. Name phylogeny: A generative model of string variation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 344–355, Jeju, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference,</booktitle>
<pages>563--566</pages>
<contexts>
<context position="4454" citStr="Bagga and Baldwin (1998" startWordPosition="696" endWordPosition="699">5 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775–785, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics §7 A minimum Bayes risk decoding procedure to pick an output clustering. The procedure is applicable to any model capable of producing a posterior over coreference decisions. We evaluate our approach by comparing to several baselines on datasets from three different genres: Twitter, newswire, and blogs. 2 Overview and Related Work Cross-document coreference resolution (CDCR) was first introduced by Bagga and Baldwin (1998b). Most approaches since then are based on the intuitions that coreferent names tend to have “similar” spellings and tend to appear in “similar” contexts. The distinguishing feature of our system is that both notions of similarity are learned together without supervision. We adopt a “phylogenetic” generative model of coreference. The basic insight is that coreference is created when an author thinks of an entity that was mentioned earlier in a similar context, and mentions it again in a similar way. The author may alter the name mention string when copying it, but both names refer to the same</context>
<context position="29825" citStr="Bagga and Baldwin, 1998" startWordPosition="5189" endWordPosition="5192">lem with an 8 Experiments In this section, we describe experiments on three different datasets. Our main results are described first: Twitter features many instances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 corpus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings. For Twitter and ACE 2008, we report the standard B3 metric (Bagga and Baldwin, 1998a). For the political blog dataset, the reference does not 8.1 Twitter Data. We use a novel corpus of Twitter posts discussing the 2013 Grammy Award ceremony. This is a challenging corpus, featuring many instan ces 9In our experiments, we run the clustering algorithm five times, initialized from samples chosen at random from the last 10% of the sampler run, and keep the clustering that achieved d score. highest expected Ran 781 of name variation. The dataset consists of five splits (by entity), the smallest of which is 604 mentions and the largest is 1374. We reserve the largest split for deve</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998a. Algorithms for scoring coreference chains. In In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Entitybased cross-document coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98,</booktitle>
<pages>79--85</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4454" citStr="Bagga and Baldwin (1998" startWordPosition="696" endWordPosition="699">5 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775–785, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics §7 A minimum Bayes risk decoding procedure to pick an output clustering. The procedure is applicable to any model capable of producing a posterior over coreference decisions. We evaluate our approach by comparing to several baselines on datasets from three different genres: Twitter, newswire, and blogs. 2 Overview and Related Work Cross-document coreference resolution (CDCR) was first introduced by Bagga and Baldwin (1998b). Most approaches since then are based on the intuitions that coreferent names tend to have “similar” spellings and tend to appear in “similar” contexts. The distinguishing feature of our system is that both notions of similarity are learned together without supervision. We adopt a “phylogenetic” generative model of coreference. The basic insight is that coreference is created when an author thinks of an entity that was mentioned earlier in a similar context, and mentions it again in a similar way. The author may alter the name mention string when copying it, but both names refer to the same</context>
<context position="29825" citStr="Bagga and Baldwin, 1998" startWordPosition="5189" endWordPosition="5192">lem with an 8 Experiments In this section, we describe experiments on three different datasets. Our main results are described first: Twitter features many instances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 corpus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings. For Twitter and ACE 2008, we report the standard B3 metric (Bagga and Baldwin, 1998a). For the political blog dataset, the reference does not 8.1 Twitter Data. We use a novel corpus of Twitter posts discussing the 2013 Grammy Award ceremony. This is a challenging corpus, featuring many instan ces 9In our experiments, we run the clustering algorithm five times, initialized from samples chosen at random from the last 10% of the sampler run, and keep the clustering that achieved d score. highest expected Ran 781 of name variation. The dataset consists of five splits (by entity), the smallest of which is 604 mentions and the largest is 1374. We reserve the largest split for deve</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998b. Entitybased cross-document coreferencing using the vector space model. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL ’98, pages 79–85, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Baron</author>
<author>Marjorie Freedman</author>
</authors>
<title>Who is who and what is what: Experiments in crossdocument co-reference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>274--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2285" citStr="Baron and Freedman, 2008" startWordPosition="353" endWordPosition="356">tic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multip</context>
</contexts>
<marker>Baron, Freedman, 2008</marker>
<rawString>Alex Baron and Marjorie Freedman. 2008. Who is who and what is what: Experiments in crossdocument co-reference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 274–283, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Bickel</author>
<author>Kjell A Doksum</author>
</authors>
<title>Mathematical Statistics : Basic Ideas and Selected Topics.</title>
<date>1977</date>
<publisher>Holden-Day, Inc.</publisher>
<marker>Bickel, Doksum, 1977</marker>
<rawString>Peter J. Bickel and Kjell A. Doksum. 1977. Mathematical Statistics : Basic Ideas and Selected Topics. Holden-Day, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Peter I Frazier</author>
</authors>
<title>Distance dependent chinese restaurant processes.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2461</pages>
<contexts>
<context position="9229" citStr="Blei and Frazier, 2011" startWordPosition="1478" endWordPosition="1481">s mention of any person and copies it, perhaps mutating its name.1 Alternatively, the model may manufacture 1We make the closed-world assumption that the author is 776 a name for a new person, though the name itself may not be new. If all previous mentions were equally likely, this would be a Chinese Restaurant Process (CRP) in which frequently mentioned entities are more likely to be mentioned again (“the rich get richer”). We refine that idea by saying that the current topic, language, and document influence the choice of which previous mention to copy, similar to the distancedependent CRP (Blei and Frazier, 2011).2 This will help distinguish multiple John Smith entities if they tend to appear in different contexts. Formally, each mention x is derived from a parent mention x.p where x.p.i &lt; x.i (the parent came first), x.e = x.p.e (same entity) and x.n is a copy or mutation of x.p.n. In the special case where x is a first mention of x.e, x.p is the special symbol Q, x.e is a newly allocated entity of some appropriate type, and the name x.n is generated from scratch. Our goal is to reconstruct mappings p, i, z that specify the latent properties of the mentions x. The mapping p : x H x.p forms a phylogen</context>
</contexts>
<marker>Blei, Frazier, 2011</marker>
<rawString>David M. Blei and Peter I. Frazier. 2011. Distance dependent chinese restaurant processes. J. Mach. Learn. Res., 12:2461–2488, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8447" citStr="Blei et al., 2003" startWordPosition="1347" endWordPosition="1350">ocuments d = (d1, ... , dD). We assume that each document has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the objectoriented notation x.v for attribute v of mention x. Our model generates an ordered sequence x although we do not observe its order. Thus each mention x has latent position x.i (e.g., x729.i = 729). The entire corpus, including these entities, is generated according to standard topic model assumptions; we first generate a topic distribution for a document, then sample topics and words for the document (Blei et al., 2003). However, any topic may generate an entity type, e.g. PERSON, which is then replaced by a specific name: when PERSON is generated, the model chooses a previous mention of any person and copies it, perhaps mutating its name.1 Alternatively, the model may manufacture 1We make the closed-world assumption that the author is 776 a name for a new person, though the name itself may not be new. If all previous mentions were equally likely, this would be a Chinese Restaurant Process (CRP) in which frequently mentioned entities are more likely to be mentioned again (“the rich get richer”). We refine th</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>David Hall</author>
<author>Thomas L Griffiths</author>
<author>Dan Klein</author>
</authors>
<title>Automated reconstruction of ancient languages using probabilistic models of sound change.</title>
<date>2013</date>
<booktitle>Proceedings of the National Academy of Sciences.</booktitle>
<marker>Bouchard-Cˆot´e, Hall, Griffiths, Klein, 2013</marker>
<rawString>Alexandre Bouchard-Cˆot´e, David Hall, Thomas L. Griffiths, and Dan Klein. 2013. Automated reconstruction of ancient languages using probabilistic models of sound change. Proceedings of the National Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Bouchard-Cˆot´e</author>
</authors>
<title>Sequential Monte Carlo (SMC) for Bayesian phylogenetics. Bayesian phylogenetics: methods, algorithms, and applications.</title>
<date>2014</date>
<marker>Bouchard-Cˆot´e, 2014</marker>
<rawString>Alexandre Bouchard-Cˆot´e. 2014. Sequential Monte Carlo (SMC) for Bayesian phylogenetics. Bayesian phylogenetics: methods, algorithms, and applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Pradeep Ravikumar</author>
<author>Stephen E Fienberg</author>
</authors>
<title>A comparison of string metrics for matching names and records.</title>
<date>2003</date>
<booktitle>In KDD Workshop on data cleaning and object consolidation.</booktitle>
<contexts>
<context position="1540" citStr="Cohen et al., 2003" startWordPosition="229" endWordPosition="233">mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets. 1 Introduction Variation poses a serious challenge for determining who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names an</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>William W. Cohen, Pradeep Ravikumar, and Stephen E. Fienberg. 2003. A comparison of string metrics for matching names and records. In KDD Workshop on data cleaning and object consolidation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="25847" citStr="Dempster et al., 1977" startWordPosition="4457" endWordPosition="4460">§5.1. 6 Parameter Estimation Evaluating the likelihood and its partial derivatives with respect to the parameters of the model requires marginalizing over our latent variables. As this marginalization is intractable, we resort to Monte Carlo EM procedure (Levine and Casella, 2001) which iterates the following two steps: E-step: Collect samples by MCMC simulation as in §5, given current model parameters θ and φ. M-step: Improve θ and φ to increase8 log Prg,O(x,ps, is, zs) (7) It is not necessary to locally maximize L at each M-step, merely to improve it if it is not already at a local maximum (Dempster et al., 1977). We improve it by a single update: at the tth M-step, we update our parameters to 4&apos;t = (θt, φt) 4&apos;t = 4&apos;t−1 + aEt∇,DL(x, 4&apos;t−1) (8) where a is a fixed scaling term and Et is an adaptive learning rate given by AdaGrad (Duchi et al., 2011). We now describe how to compute the gradient ∇,DL. The gradient with respect to the parent se8We actually do MAP-EM, which augments (7) by adding the log-likelihoods of θ and φ under a Gaussian prior. � XFx,y(z, z&apos;) · βy(z&apos;) z� L def = 1 S S s=1 780 lection parameters φ is ⎛ ⎞ �1 S f (p, x) − E Prφ (p0 |x)f (p0, x) (9) p0 samples. The other variables in (9) </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Q Ding</author>
<author>Xiaofeng He</author>
<author>Hongyuan Zha</author>
<author>Ming Gu</author>
<author>H D Simon</author>
</authors>
<title>A min-max cut algorithm for graph partitioning and data clustering.</title>
<date>2001</date>
<booktitle>In Data Mining,</booktitle>
<pages>107--114</pages>
<contexts>
<context position="29185" citStr="Ding et al., 2001" startWordPosition="5082" endWordPosition="5085">nds to a cluster of mentions. Our model gives a distribution over phylogenies (given observations x and learned parameters thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single clustering e. We prefer the clustering that minimizes Bayes risk (MBR) (Bickel an e ♦. p Φ)—and e∗ d Doksum, 1977): mate the expectation by sampli The Rand index (Rand, 1971)—unlike our actual evaluation measure—is an �N � 2 where the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) use cut (Ding et al., 2001), an NP-hard problem with an 8 Experiments In this section, we describe experiments on three different datasets. Our main results are described first: Twitter features many instances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 corpus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings. For Twitter and ACE 2008, we report the sta</context>
</contexts>
<marker>Ding, He, Zha, Gu, Simon, 2001</marker>
<rawString>C.H.Q. Ding, Xiaofeng He, Hongyuan Zha, Ming Gu, and H.D. Simon. 2001. A min-max cut algorithm for graph partitioning and data clustering. In Data Mining, 2001. ICDM 2001, Proceedings IEEE International Conference on, pages 107 –114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Michael J Paul</author>
<author>Shane Bergsma</author>
<author>Hieu Tran</author>
</authors>
<title>Carmen: A twitter geolocation system with applications to public health.</title>
<date>2013</date>
<booktitle>In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</booktitle>
<marker>Dredze, Paul, Bergsma, Tran, 2013</marker>
<rawString>Mark Dredze, Michael J Paul, Shane Bergsma, and Hieu Tran. 2013. Carmen: A twitter geolocation system with applications to public health. In AAAI Workshop on Expanding the Boundaries of Health Informatics Using AI (HIAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Latent-variable modeling of string transductions with finite-state methods.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1080--1089</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="16182" citStr="Dreyer et al. (2008)" startWordPosition="2750" endWordPosition="2753">small set of simple feature functions f, which consider conjunctions of the attributes of the characters aˆ and b: character, character class (letter, digit, etc.), and case (upper vs. lower). More generally, the probability (2) may also be conditioned on other variables such as on the languages p. E and x. E—this leaves room for a transliteration model when x. E =� p. E—and on the entity type x.t. The features in (3) may then depend on these variables as well. Notice that we use a locally normalized probability for each edit. This enables faster and simpler training than the similar model of Dreyer et al. (2008), which uses a globally normalized probability for the whole edit sequence. When p = Q, we are generating a new name x.n. We use the same model, taking Q.n to be the empty string (but with #p rather than # as the end-ofstring symbol). This yields a feature-based unigram language model (whose character probabilities may differ from usual insertion probabilities because they see #p as the lookahead character). Pragmatics. We can optionally make the model more sophisticated. Authors tend to avoid names x.n that readers would misinterpret (given the previously generated names). The edit model thin</context>
</contexts>
<marker>Dreyer, Smith, Eisner, 2008</marker>
<rawString>Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-variable modeling of string transductions with finite-state methods. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1080–1089, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="26086" citStr="Duchi et al., 2011" startWordPosition="4506" endWordPosition="4509">EM procedure (Levine and Casella, 2001) which iterates the following two steps: E-step: Collect samples by MCMC simulation as in §5, given current model parameters θ and φ. M-step: Improve θ and φ to increase8 log Prg,O(x,ps, is, zs) (7) It is not necessary to locally maximize L at each M-step, merely to improve it if it is not already at a local maximum (Dempster et al., 1977). We improve it by a single update: at the tth M-step, we update our parameters to 4&apos;t = (θt, φt) 4&apos;t = 4&apos;t−1 + aEt∇,DL(x, 4&apos;t−1) (8) where a is a fixed scaling term and Et is an adaptive learning rate given by AdaGrad (Duchi et al., 2011). We now describe how to compute the gradient ∇,DL. The gradient with respect to the parent se8We actually do MAP-EM, which augments (7) by adding the log-likelihoods of θ and φ under a Gaussian prior. � XFx,y(z, z&apos;) · βy(z&apos;) z� L def = 1 S S s=1 780 lection parameters φ is ⎛ ⎞ �1 S f (p, x) − E Prφ (p0 |x)f (p0, x) (9) p0 samples. The other variables in (9) are associated with the edge being summed over. That edge explains a mention x as a mutation of some parent p in the context of a particular sample (ps, is, zs). The possible parents range over and the mentions that precede x according to </context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1971--1982</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7221" citStr="Durrett and Klein (2013)" startWordPosition="1139" endWordPosition="1142">on model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1,... , xN) denote an ordered sequence of distinct named-entity menti</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971–1982. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Structured databases of named entities from bayesian nonparametrics.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11,</booktitle>
<pages>2--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="35136" citStr="Eisenstein et al. (2011)" startWordPosition="6071" endWordPosition="6074">imating w(p , x) as 1 or 0 according to whether p .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter. 782 Unlike our other datasets, mentions are not annotated with entities: the reference consists of a table of 126 entities, where each row is the canonical name of one entity. Baselines. We compare to the system results reported in Figure 2 of Yogatama et al. (2012). This includes a baseline hierarchical clustering approach, the “EEA” name canonicalization system of Eisenstein et al. (2011), as well the model proposed by Yogatama et al. (2012). Like the output of our model, the output of their hierarchical clustering baseline is a mention clustering, and therefore must be mapped to a table of canonical entity names to compare to the reference table. Procedure &amp; Results We tune our method as in previous experiments, on the initialization data used by Yogatama et al. (2012) which consists of a subset of 700 documents of the full dataset. The tuned model then produced a mention clustering on the full political blog corpus. As the mapping from clusters to a table is not fully detail</context>
<context position="38652" citStr="Eisenstein et al., 2011" startWordPosition="6660" endWordPosition="6663">ore appropriate fit for a far larger corpus. Larger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately. A common error of our system is to connect mentions that share long substrings, such as different PERSONs who share a last name, or different ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names (Johnson, 2010; Eisenstein et al., 2011; Yogatama et al., 2012) in the mutation model is a promising future direction. 9 Conclusions Our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an alternative to the distance-dependent CRP.2 Our implementation</context>
</contexts>
<marker>Eisenstein, Yano, Cohen, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Tae Yano, William W. Cohen, Noah A. Smith, and Eric P. Xing. 2011. Structured databases of named entities from bayesian nonparametrics. In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11, pages 2–12, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finin</author>
<author>Z Syed</author>
<author>J Mayfield</author>
<author>P McNamee</author>
<author>C Piatko</author>
</authors>
<title>Using Wikitology for cross-document entity coreference resolution.</title>
<date>2009</date>
<booktitle>In AAAI Spring Symposium on Learning by Reading and Learning to Read.</booktitle>
<contexts>
<context position="2305" citStr="Finin et al., 2009" startWordPosition="357" endWordPosition="360">wn pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the gene</context>
</contexts>
<marker>Finin, Syed, Mayfield, McNamee, Piatko, 2009</marker>
<rawString>T. Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Piatko. 2009. Using Wikitology for cross-document entity coreference resolution. In AAAI Spring Symposium on Learning by Reading and Learning to Read.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Nicholas Andrews</author>
<author>Matthew R Gormley</author>
<author>Mark Dredze</author>
<author>Christopher D Manning</author>
</authors>
<title>Entity clustering across languages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>60--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1760" citStr="Green et al., 2012" startWordPosition="269" endWordPosition="272">who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 201</context>
<context position="7614" citStr="Green et al. (2012)" startWordPosition="1202" endWordPosition="1205">r approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1,... , xN) denote an ordered sequence of distinct named-entity mentions in documents d = (d1, ... , dD). We assume that each document has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the objectoriented notation x.v for attribute v of mention x. Our model generates an ordered sequence x although we do not observe its order. Thus each mention x has latent position x.i (e.g., x729.i </context>
<context position="30627" citStr="Green et al. (2012)" startWordPosition="5322" endWordPosition="5325">rpus, featuring many instan ces 9In our experiments, we run the clustering algorithm five times, initialized from samples chosen at random from the last 10% of the sampler run, and keep the clustering that achieved d score. highest expected Ran 781 of name variation. The dataset consists of five splits (by entity), the smallest of which is 604 mentions and the largest is 1374. We reserve the largest split for development purposes, and report our results on the remaining four. Appendix B provides more detail about the dataset. Baselines. We use the discriminative entity clustering algorithm of Green et al. (2012) as our baseline; their approach was found to outperform another generative model which produced a flat clustering of mentions via a Dirichlet process mixture model. Their method uses Jaro-Winkler string similarity to match names, then clusters mentions with matching names (for disambiguation) by comparing their unigram context distributions using the Jenson-Shannon metric. We also compare to the EXACT-MATCH baseline, which assigns all strings with the same name to the same entity. Procedure. We run four test experiments in which one split is used to pick model hyperparameters and the remainin</context>
<context position="32786" citStr="Green et al. (2012)" startWordPosition="5674" endWordPosition="5677">ively have the same topic) and determines mention entities from a single sample of P (the last); PHYLO+TOPIC adds context (§5.2); PHYLO+TOPIC+MBR uses the full posterior and consensus clustering to pick the output clustering (§7). Our results are shown in Table 1.10 10Our single-threaded implementation took around 15 minutes per fold of the Twitter corpus on a personal laptop with a 2.3 Ghz Intel Core i7 processor (including time required to parse the data files). Typical acceptance rates for ordering and topic proposals ranged from 0.03 to 0.08. Mean Test B3 P R F1 EXACT-MATCH 99.6 53.7 69.8 Green et al. (2012) 92.1 69.8 79.3 PHYLO 85.3 91.4 88.7 PHYLO+TOPIC 92.8 90.8 91.8 PHYLO+TOPIC+MBR 92.9 90.9 91.9 Table 1: Results for the Twitter dataset. Higher B3 scores are better. Note that each number is averaged over four different test splits. In three out of four experiments, PHYLO+TOPIC+MBR achieved the highest F1 score; in one case PHYLO+TOPIC won by a small margin. P Test B3 F1 R EXACT-MATCH 98.0 81.2 88.8 PER Green et al. (2012) 95.0 88.9 91.9 PHYLO+TOPIC+MBR 97.2 88.6 92.7 EXACT-MATCH 98.2 78.3 87.1 ORG Green et al. (2012) 92.1 88.5 90.3 PHYLO+TOPIC+MBR 95.5 80.9 87.6 Table 2: Results for the ACE 2</context>
</contexts>
<marker>Green, Andrews, Gormley, Dredze, Manning, 2012</marker>
<rawString>Spence Green, Nicholas Andrews, Matthew R. Gormley, Mark Dredze, and Christopher D. Manning. 2012. Entity clustering across languages. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 60–69, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl</booktitle>
<pages>1--5228</pages>
<contexts>
<context position="20458" citStr="Griffiths and Steyvers, 2004" startWordPosition="3502" endWordPosition="3505">ss of x.z. 7The full version of this paper is available at http://cs.jhu.edu/˜noa/publications/ phylo-acl-14.pdf to our competitive parent selection model (§4.1). To correct for this bias using IMH, we accept the proposed ordering i0 with probability � 1, Pr(p, i♦, z, x θ, φ) a = min (4) Pr(p, i, z, x θ, φ) where i is the current ordering. Otherwise we reject i0 and reuse i for the new sample. 5.2 Resampling the topics z Each context word and each named entity is associated with a latent topic. The topics of context words are assumed exchangeable, and so we resample them using Gibbs sampling (Griffiths and Steyvers, 2004). Unfortunately, this is prohibitively expensive for the (non-exchangeable) topics of the named mentions x. A Gibbs sampler would have to choose a new value for x.z with probability proportional to the resulting joint probability of the full sample. This probability is expensive to evaluate because changing x.z will change the probability of many edges in the current phylogeny p. (Equation (1) puts x is in competition with other parents, so every mention y that follows x must recompute how happy it is with its current parent y.p.) Rather than resampling one topic at a time, we resample z as a </context>
<context position="24576" citStr="Griffiths and Steyvers, 2004" startWordPosition="4237" endWordPosition="4240">lity. This is proportional to w(x.p, x) def = PrO(x.p |x) · Prg(x.n |x.p.n), independent of any other mention’s choice of parent. The two factors here are given by (1) and (2) respectively. As in the previous section, the denominators Z(x) in the Pr(x.p |x) factors can be computed efficiently with shared work. With the pragmatic model (section 4.2), the parent choices are no longer independent; then the samples of p should be corrected by IMH as usual. 5.4 Initializing the sampler The initial sampler state (z0, p0, i0) is obtained as follows. (1) We fix topics z0 via collapsed Gibbs sampling (Griffiths and Steyvers, 2004). The sampler is run for 1000 iterations, and the final sampler state is taken to be z0. This process treats all topics as exchangeable, including those associated with named entities.(2) Given the topic assignment z0, initialize p0 to the phylogeny rooted at ♦ that maximizes Ex log w(x.p, x). This is a maximum rooted directed spanning tree problem that can be solved in time O(n2) (Tarjan, 1977). The weight w(x.p, x) is defined as in section 5.3—except that since we do not yet have an ordering i, we do not restrict the possible values of x.p to mentions p with p.i &lt; x.p.i. (3) Given p0, sample</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1):5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Guo</author>
<author>Ming-Wei Chang</author>
<author>Emre Kıcıman</author>
</authors>
<title>To link or not to link? a study on end-to-end tweet entity linking.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1020--1030</pages>
<marker>Guo, Chang, Kıcıman, 2013</marker>
<rawString>Stephen Guo, Ming-Wei Chang, and Emre Kıcıman. 2013. To link or not to link? a study on end-to-end tweet entity linking. In Proceedings of NAACL-HLT, pages 1020–1030.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference resolution in a modular, entity-centered model.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="7032" citStr="Haghighi and Klein (2010)" startWordPosition="1112" endWordPosition="1115">tant component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as </context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference resolution in a modular, entity-centered model. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385–393, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1148--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="38627" citStr="Johnson, 2010" startWordPosition="6658" endWordPosition="6659">el would be a more appropriate fit for a far larger corpus. Larger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately. A common error of our system is to connect mentions that share long substrings, such as different PERSONs who share a last name, or different ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names (Johnson, 2010; Eisenstein et al., 2011; Yogatama et al., 2012) in the mutation model is a promising future direction. 9 Conclusions Our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an alternative to the distance-dependent</context>
</contexts>
<marker>Johnson, 2010</marker>
<rawString>Mark Johnson. 2010. Pcfgs, topic models, adaptor grammars and learning topical collocations and the structure of proper names. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1148–1157, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Sujith Ravi</author>
</authors>
<title>Unsupervised name ambiguity resolution using a generative model.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11,</booktitle>
<pages>105--112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7590" citStr="Kozareva and Ravi (2011)" startWordPosition="1197" endWordPosition="1200">t area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1,... , xN) denote an ordered sequence of distinct named-entity mentions in documents d = (d1, ... , dD). We assume that each document has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the objectoriented notation x.v for attribute v of mention x. Our model generates an ordered sequence x although we do not observe its order. Thus each mention x has latent pos</context>
</contexts>
<marker>Kozareva, Ravi, 2011</marker>
<rawString>Zornitsa Kozareva and Sujith Ravi. 2011. Unsupervised name ambiguity resolution using a generative model. In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ’11, pages 105–112, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint entity and event coreference resolution across documents.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</booktitle>
<contexts>
<context position="2361" citStr="Lee et al., 2012" startWordPosition="369" endWordPosition="372">n et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our m</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint entity and event coreference resolution across documents. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Levine</author>
<author>George Casella</author>
</authors>
<title>Implementations of the Monte Carlo EM Algorithm.</title>
<date>2001</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="25506" citStr="Levine and Casella, 2001" startWordPosition="4395" endWordPosition="4398">maximum rooted directed spanning tree problem that can be solved in time O(n2) (Tarjan, 1977). The weight w(x.p, x) is defined as in section 5.3—except that since we do not yet have an ordering i, we do not restrict the possible values of x.p to mentions p with p.i &lt; x.p.i. (3) Given p0, sample an ordering i0 using the procedure described in §5.1. 6 Parameter Estimation Evaluating the likelihood and its partial derivatives with respect to the parameters of the model requires marginalizing over our latent variables. As this marginalization is intractable, we resort to Monte Carlo EM procedure (Levine and Casella, 2001) which iterates the following two steps: E-step: Collect samples by MCMC simulation as in §5, given current model parameters θ and φ. M-step: Improve θ and φ to increase8 log Prg,O(x,ps, is, zs) (7) It is not necessary to locally maximize L at each M-step, merely to improve it if it is not already at a local maximum (Dempster et al., 1977). We improve it by a single update: at the tth M-step, we update our parameters to 4&apos;t = (θt, φt) 4&apos;t = 4&apos;t−1 + aEt∇,DL(x, 4&apos;t−1) (8) where a is a fixed scaling term and Et is an adaptive learning rate given by AdaGrad (Duchi et al., 2011). We now describe ho</context>
</contexts>
<marker>Levine, Casella, 2001</marker>
<rawString>Richard A. Levine and George Casella. 2001. Implementations of the Monte Carlo EM Algorithm. Journal of Computational and Graphical Statistics, 10(3):422–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feiping Nie</author>
<author>Chris H Q Ding</author>
<author>Dijun Luo</author>
<author>Heng Huang</author>
</authors>
<title>Improved minmax cut graph clustering with nonnegative relaxation.</title>
<date>2010</date>
<journal>ECML/PKDD</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>2</volume>
<pages>451--466</pages>
<editor>In Jos´e L. Balc´azar, Francesco Bonchi, Aristides Gionis, and Mich`ele Sebag, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="27977" citStr="Nie et al., 2010" startWordPosition="4878" endWordPosition="4881">ifies the More similar clusterings achieve larger R, with e) = 1 iff = e. In all cases, 0 The MBR decision rule for the (negated) Ran e e0 �N � mention pairs as coreferent or not. 2 R(e0, e0 ≤R(e0, e) = R(e, e0) ≤ 1. d index is easily seen to be equivalent to e∗ = argmax E[TP] + (12) e0 � � = argmax sij + (1 − sij) e0 i,j: xi∼xj i,j: xi6∼xj where ∼ denotes coreference according to e0. As explained above, the sij are coreference probabilities sij that can be estimated from a sample of clusterings e. This objective corresponds to min-max graph � Prθ(a0,b0|ˆa)f(ˆa, a0, b0)) approximate solution (Nie et al., 2010).9 ˆa,a,b (10) Pr(e �e∗ = argminL(e0,e) |x, θ, φ) (11) e0 e This minimizes our expected loss, where e) denotes the loss associated with picking when the true clustering is e. In practice, we again estiL(e0, e0 ng e values. efficient choice of loss consist of entity annotations, and so we foll ow the evaluation procedure of Yogatama et al. (2012). The outer summation ranges over all edges in the �cˆa,a,b(f(ˆa, a, b) − a0,b0 7 Consensus Clustering From a single phylogeny p, we deterministically obtain a clustering by removing the root Each of the resulting connected components corresponds to a c</context>
</contexts>
<marker>Nie, Ding, Luo, Huang, 2010</marker>
<rawString>Feiping Nie, Chris H. Q. Ding, Dijun Luo, and Heng Huang. 2010. Improved minmax cut graph clustering with nonnegative relaxation. In Jos´e L. Balc´azar, Francesco Bonchi, Aristides Gionis, and Mich`ele Sebag, editors, ECML/PKDD (2), volume 6322 of Lecture Notes in Computer Science, pages 451–466. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Porter</author>
<author>W E Winkler</author>
</authors>
<title>Approximate String Comparison and its Effect on an Advanced Record Linkage System, chapter 6,</title>
<date>1997</date>
<journal>U.S. Bureau of the Census.</journal>
<pages>190--199</pages>
<contexts>
<context position="6512" citStr="Porter and Winkler, 1997" startWordPosition="1027" endWordPosition="1030"> explains similar names as being similar by mutation (rather than by coincidence). Thus, our sampled phylogenies tend to make similar names coreferent—especially long or unusual names that would be expensive to generate repeatedly, and especially in contexts that are topically similar and therefore have a higher prior probability of coreference. For learning, we iteratively adjust our model’s parameters to better explain our samples. That is, we do unsupervised training via Monte Carlo EM. What is learned? An important component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restauran</context>
</contexts>
<marker>Porter, Winkler, 1997</marker>
<rawString>E. H. Porter and W. E. Winkler, 1997. Approximate String Comparison and its Effect on an Advanced Record Linkage System, chapter 6, pages 190–199. U.S. Bureau of the Census.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>66</volume>
<issue>336</issue>
<contexts>
<context position="29008" citStr="Rand, 1971" startWordPosition="5055" endWordPosition="5056"> a0,b0 7 Consensus Clustering From a single phylogeny p, we deterministically obtain a clustering by removing the root Each of the resulting connected components corresponds to a cluster of mentions. Our model gives a distribution over phylogenies (given observations x and learned parameters thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single clustering e. We prefer the clustering that minimizes Bayes risk (MBR) (Bickel an e ♦. p Φ)—and e∗ d Doksum, 1977): mate the expectation by sampli The Rand index (Rand, 1971)—unlike our actual evaluation measure—is an �N � 2 where the true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) use cut (Ding et al., 2001), an NP-hard problem with an 8 Experiments In this section, we describe experiments on three different datasets. Our main results are described first: Twitter features many instances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional expe</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Paul McNamee</author>
<author>Mark Dredze</author>
</authors>
<title>Streaming cross document entity coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1050--1058</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2323" citStr="Rao et al., 2010" startWordPosition="361" endWordPosition="364">names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Ta</context>
</contexts>
<marker>Rao, McNamee, Dredze, 2010</marker>
<rawString>Delip Rao, Paul McNamee, and Mark Dredze. 2010. Streaming cross document entity coreference resolution. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1050–1058, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1996</date>
<tech>Technical Report CS-TR532-96,</tech>
<institution>Princeton University, Department of Computer Science.</institution>
<contexts>
<context position="6692" citStr="Ristad and Yianilos, 1996" startWordPosition="1061" endWordPosition="1064">ames that would be expensive to generate repeatedly, and especially in contexts that are topically similar and therefore have a higher prior probability of coreference. For learning, we iteratively adjust our model’s parameters to better explain our samples. That is, we do unsupervised training via Monte Carlo EM. What is learned? An important component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-docu</context>
</contexts>
<marker>Ristad, Yianilos, 1996</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string edit distance. Technical Report CS-TR532-96, Princeton University, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Peter N Yianilos</author>
</authors>
<title>Learning string edit distance.</title>
<date>1998</date>
<journal>IEEE Transactions on Pattern Recognition and Machine Intelligence,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="1739" citStr="Ristad and Yianilos, 1998" startWordPosition="264" endWordPosition="268"> challenge for determining who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., </context>
</contexts>
<marker>Ristad, Yianilos, 1998</marker>
<rawString>Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string edit distance. IEEE Transactions on Pattern Recognition and Machine Intelligence, 20(5):522–532, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524–1534. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Large-scale cross-document coreference using distributed inference and hierarchical models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>793--803</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2343" citStr="Singh et al., 2011" startWordPosition="365" endWordPosition="368">Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift</context>
</contexts>
<marker>Singh, Subramanya, Pereira, McCallum, 2011</marker>
<rawString>Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2011. Large-scale cross-document coreference using distributed inference and hierarchical models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 793–803, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="24974" citStr="Tarjan, 1977" startWordPosition="4308" endWordPosition="4309"> of p should be corrected by IMH as usual. 5.4 Initializing the sampler The initial sampler state (z0, p0, i0) is obtained as follows. (1) We fix topics z0 via collapsed Gibbs sampling (Griffiths and Steyvers, 2004). The sampler is run for 1000 iterations, and the final sampler state is taken to be z0. This process treats all topics as exchangeable, including those associated with named entities.(2) Given the topic assignment z0, initialize p0 to the phylogeny rooted at ♦ that maximizes Ex log w(x.p, x). This is a maximum rooted directed spanning tree problem that can be solved in time O(n2) (Tarjan, 1977). The weight w(x.p, x) is defined as in section 5.3—except that since we do not yet have an ordering i, we do not restrict the possible values of x.p to mentions p with p.i &lt; x.p.i. (3) Given p0, sample an ordering i0 using the procedure described in §5.1. 6 Parameter Estimation Evaluating the likelihood and its partial derivatives with respect to the parameters of the model requires marginalizing over our latent variables. As this marginalization is intractable, we resort to Monte Carlo EM procedure (Levine and Casella, 2001) which iterates the following two steps: E-step: Collect samples by </context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R E Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Tierney</author>
</authors>
<title>Markov Chains for Exploring Posterior Distributions.</title>
<date>1994</date>
<journal>The Annals of Statistics,</journal>
<volume>22</volume>
<issue>4</issue>
<contexts>
<context position="18943" citStr="Tierney, 1994" startWordPosition="3232" endWordPosition="3233">Gibbs sampler, which from an initial state (p0, i0, z0) repeats these steps: 1. Sample the ordering i from its conditional distribution given all other variables. 2. Sample the topic vector z likewise. 3. Sample the phylogeny p likewise. 4. Output the current sample st = (p, i, z). It is difficult to draw exact samples at steps 1 and 2. Thus, we sample i or z from a simpler proposal distribution, but correct the discrepancy using the Independent Metropolis-Hastings (IMH) strategy: with an appropriate probability, reject the proposed new value and instead use another copy of the current value (Tierney, 1994). 5.1 Resampling the ordering i We resample the ordering i of the mentions x, conditioned on the other variables. The current phylogeny p already defines a partial order on x, since each parent must precede its children. For instance, phylogeny (a) below requires 0 � x and 0 � y. This partial order is compatible with 2 total orderings, 0 � x � y and 0 � y � x. By contrast, phylogeny (b) requires the total ordering 0 � x � y. 0 0 x y (a) We first sample an ordering i0 (the ordering of mentions with parent 0, i.e. all mentions) uniformly at random from the set of orderings compatible with the cu</context>
</contexts>
<marker>Tierney, 1994</marker>
<rawString>Luke Tierney. 1994. Markov Chains for Exploring Posterior Distributions. The Annals of Statistics, 22(4):1701–1728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
<author>David Mimno</author>
<author>Andrew McCallum</author>
</authors>
<title>Rethinking lda: Why priors matter.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1973--1981</pages>
<contexts>
<context position="11654" citStr="Wallach et al., 2009" startWordPosition="1914" endWordPosition="1917">revious mention in the latent ordering. This is why our phylogeny is a tree, and why our sampler is more complex. Also unlike the ddCRP, we permit asymmetric “distances”: if a certain topic or language likes to copy mentions from another, the compliment is not necessarily returned. count), i = 0 (mention count), and for each document index d = 1, ... , D: 1. Choose the document’s length L and language E. (The distributions used to choose these are unimportant because these variables are always observed.) 2. Choose its topic distribution ψd from an asymmetric Dirichlet prior with parameters m (Wallach et al., 2009).3 3. For each token position k = 1, ... , L: (a) Choose a topic zdk — ψd. (b) Choose a word conditioned on the topic and language, wdk — )3zdkt. (c) If wdk is a named entity type (PERSON, PLACE, ORG, ...) rather than an ordinary word, then increment i and: i. create a new mention x with x.e.t = wdk x.d = d x.E = E x.i = i x.z=zdk x.k = k ii. Choose the parent x.p from a distribution conditioned on the attributes just set (see §4.1). iii. If x.p = Q, increment m and set x.e = a new entity em. Else set x.e = x.p.e. iv. Choose x.n from a distribution conditioned on x.p.n and x.E (see §4.2). Noti</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>Hanna Wallach, David Mimno, and Andrew McCallum. 2009. Rethinking lda: Why priors matter. In Advances in Neural Information Processing Systems, pages 1973–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Sameer Singh</author>
<author>Andrew McCallum</author>
</authors>
<title>A discriminative hierarchical model for fast coreference at large scale.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>379--388</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Wick, Singh, McCallum, 2012</marker>
<rawString>Michael Wick, Sameer Singh, and Andrew McCallum. 2012. A discriminative hierarchical model for fast coreference at large scale. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 379–388, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>The state of record linkage and current research problems.</title>
<date>1999</date>
<tech>Technical report, Statistical Research</tech>
<institution>Division, U.S. Census Bureau.</institution>
<contexts>
<context position="1519" citStr="Winkler, 1999" startWordPosition="227" endWordPosition="228"> models of the mutation process and parent selection process. We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets. 1 Introduction Variation poses a serious challenge for determining who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning simila</context>
<context position="6485" citStr="Winkler, 1999" startWordPosition="1025" endWordPosition="1026">obability if it explains similar names as being similar by mutation (rather than by coincidence). Thus, our sampled phylogenies tend to make similar names coreferent—especially long or unusual names that would be expensive to generate repeatedly, and especially in contexts that are topically similar and therefore have a higher prior probability of coreference. For learning, we iteratively adjust our model’s parameters to better explain our samples. That is, we do unsupervised training via Monte Carlo EM. What is learned? An important component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-</context>
</contexts>
<marker>Winkler, 1999</marker>
<rawString>William E. Winkler. 1999. The state of record linkage and current research problems. Technical report, Statistical Research Division, U.S. Census Bureau.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tae Yano</author>
<author>William W Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting response to political blog posts with topic models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>477--485</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="34242" citStr="Yano et al., 2009" startWordPosition="5918" endWordPosition="5921">ld) within-document coreference chain as the canonical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments.11 Baselines &amp; Procedure. We use the same baselines as in §8.1. On development data, modeling pragmatics as in §4.2 gave large improvements for organizations (8 points in F-measure), correcting the tendency to assume that short names like CIA were coincidental homonyms. Hence we allowed γ &gt; 0 and tuned it on development data.12 Results are in Table 2. 8.3 Blogs Data. The CMU political blogs dataset consists of 3000 documents about U.S. politics (Yano et al., 2009). Preprocessed as described in Yogatama et al. (2012), the data consists of 10647 entity mentions. 11That is, each within-document coreference chain is mapped to a single mention as a preprocessing step. 12We used only a simplified version of the pragmatic model, approximating w(p , x) as 1 or 0 according to whether p .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter. 782 Unlike our other datasets, mentions are not annotated with entities: the reference consists of </context>
</contexts>
<marker>Yano, Cohen, Smith, 2009</marker>
<rawString>Tae Yano, William W. Cohen, and Noah A. Smith. 2009. Predicting response to political blog posts with topic models. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 477–485, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dani Yogatama</author>
<author>Yanchuan Sim</author>
<author>Noah A Smith</author>
</authors>
<title>A probabilistic model for canonicalizing named entity mentions.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>685--693</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="28324" citStr="Yogatama et al. (2012)" startWordPosition="4940" endWordPosition="4943"> where ∼ denotes coreference according to e0. As explained above, the sij are coreference probabilities sij that can be estimated from a sample of clusterings e. This objective corresponds to min-max graph � Prθ(a0,b0|ˆa)f(ˆa, a0, b0)) approximate solution (Nie et al., 2010).9 ˆa,a,b (10) Pr(e �e∗ = argminL(e0,e) |x, θ, φ) (11) e0 e This minimizes our expected loss, where e) denotes the loss associated with picking when the true clustering is e. In practice, we again estiL(e0, e0 ng e values. efficient choice of loss consist of entity annotations, and so we foll ow the evaluation procedure of Yogatama et al. (2012). The outer summation ranges over all edges in the �cˆa,a,b(f(ˆa, a, b) − a0,b0 7 Consensus Clustering From a single phylogeny p, we deterministically obtain a clustering by removing the root Each of the resulting connected components corresponds to a cluster of mentions. Our model gives a distribution over phylogenies (given observations x and learned parameters thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single clustering e. We prefer the clustering that minimizes Bayes risk (MBR) (Bickel an e ♦. p Φ</context>
<context position="34295" citStr="Yogatama et al. (2012)" startWordPosition="5926" endWordPosition="5929">nical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments.11 Baselines &amp; Procedure. We use the same baselines as in §8.1. On development data, modeling pragmatics as in §4.2 gave large improvements for organizations (8 points in F-measure), correcting the tendency to assume that short names like CIA were coincidental homonyms. Hence we allowed γ &gt; 0 and tuned it on development data.12 Results are in Table 2. 8.3 Blogs Data. The CMU political blogs dataset consists of 3000 documents about U.S. politics (Yano et al., 2009). Preprocessed as described in Yogatama et al. (2012), the data consists of 10647 entity mentions. 11That is, each within-document coreference chain is mapped to a single mention as a preprocessing step. 12We used only a simplified version of the pragmatic model, approximating w(p , x) as 1 or 0 according to whether p .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter. 782 Unlike our other datasets, mentions are not annotated with entities: the reference consists of a table of 126 entities, where each row is the canoni</context>
<context position="35525" citStr="Yogatama et al. (2012)" startWordPosition="6140" endWordPosition="6143"> of one entity. Baselines. We compare to the system results reported in Figure 2 of Yogatama et al. (2012). This includes a baseline hierarchical clustering approach, the “EEA” name canonicalization system of Eisenstein et al. (2011), as well the model proposed by Yogatama et al. (2012). Like the output of our model, the output of their hierarchical clustering baseline is a mention clustering, and therefore must be mapped to a table of canonical entity names to compare to the reference table. Procedure &amp; Results We tune our method as in previous experiments, on the initialization data used by Yogatama et al. (2012) which consists of a subset of 700 documents of the full dataset. The tuned model then produced a mention clustering on the full political blog corpus. As the mapping from clusters to a table is not fully detailed in Yogatama et al. (2012), we used a simple heuristic: the most frequent name in each cluster is taken as the canonical name, augmented by any titles from a predefined list appearing in any other name in the cluster. The resulting table is then evaluated against the reference, as described in Yogatama et al. (2012). We achieved a response score of 0.17 and a reference score of 0.61. </context>
<context position="38676" citStr="Yogatama et al., 2012" startWordPosition="6664" endWordPosition="6667"> far larger corpus. Larger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately. A common error of our system is to connect mentions that share long substrings, such as different PERSONs who share a last name, or different ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names (Johnson, 2010; Eisenstein et al., 2011; Yogatama et al., 2012) in the mutation model is a promising future direction. 9 Conclusions Our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an alternative to the distance-dependent CRP.2 Our implementation is available for resear</context>
</contexts>
<marker>Yogatama, Sim, Smith, 2012</marker>
<rawString>Dani Yogatama, Yanchuan Sim, and Noah A. Smith. 2012. A probabilistic model for canonicalizing named entity mentions. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 685–693, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>