<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998205">
Comparison of Classification and Ranking Approaches
to Pronominal Anaphora Resolution in Czech*
</title>
<author confidence="0.998094">
Ngu.y Giang Linh, V´aclav Nov´ak, Zdenˇek ˇZabokrtsk´y
</author>
<affiliation confidence="0.997259">
Charles University in Prague
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.877608">
Malostransk´e n´amˇestf25, CZ-11800
</address>
<email confidence="0.953411">
{linh,novak,zabokrtsky}.ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999509571428572">
In this paper we compare two Ma-
chine Learning approaches to the task
of pronominal anaphora resolution: a
conventional classification system based
on C5.0 decision trees, and a novel
perceptron-based ranker. We use coref-
erence links annotated in the Prague De-
pendency Treebank 2.0 for training and
evaluation purposes. The perceptron sys-
tem achieves f-score 79.43% on recogniz-
ing coreference of personal and possessive
pronouns, which clearly outperforms the
classifier and which is the best result re-
ported on this data set so far.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999798849056604">
Anaphora Resolution (AR) is a well established
task in Natural Language Processing (Mitkov,
2002). Classification techniques (e.g., single can-
didate model aimed at answering: “Is there a
coreference link between the anaphor and this
antecedent candidate, or not?”) are very often
used for the task, e.g. in Mccarthy and Lehnert
(1995) and Soon et al. (2001). However, as ar-
gued already in Yang et al. (2003), better results
are achieved when the candidates can compete in
a pairwise fashion. It can be explained by the
fact that in this approach (called twin-candidate
model), more information is available for the de-
cision making. If we proceed further along this
direction, we come to the ranking approach de-
scribed in Denis and Baldridge (2007), in which
the entire candidate set is considered at once and
The work on this project was supported by the
grants MSM 0021620838, GAAV ˇCR 1ET101120503 and
1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009
which leads to further significant shift in perfor-
mance, more recently documented in Denis and
Baldridge (2008).
In this paper we deal with supervised ap-
proaches to pronominal anaphora in Czech.1 For
training and evaluation purposes, we use corefer-
ences links annotated in the Prague Dependency
Treebank, (Jan Hajiˇc, et al., 2006). We limit our-
selves only to textual coreference (see Section 2)
and to personal and possessive pronouns. We
make use of a rich set of features available thanks
to the complex annotation scenario of the tree-
bank.
We experiment with two of the above men-
tioned techniques for AR: a classifier and a ranker.
The former is based on a top-down induction of
decision trees (Quinlan, 1993). The latter uses
a simple scoring function whose optimal weight
vector is estimated using perceptron learning in-
spired by Collins (2002). We try to provide both
implementations with as similar input information
as possible in order to be able to compare their
performance for the given task.
Performance of the presented systems can be
compared with several already published works,
namely with a rule-based system described in
Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “clas-
sical” algorithms implemented in Nˇemˇcfk (2006),
a system based on decision trees (Ngu.y, 2006),
and a rule-based system evaluated in Ngu.y and
ˇZabokrtsk´y (2007). To illustrate the real complex-
ity of the task, we also provide performance eval-
uation of a baseline solution.
</bodyText>
<footnote confidence="0.698486666666667">
1Currently one can see a growing interest in unsupervised
techniques, e.g. Charniak and Elsner (2009) and Ng (2008).
However, we make only a very tiny step in this direction:
we use a probabilistic feature based on collocation counts in
large unannotated data (namely in the Czech National Cor-
pus).
</footnote>
<note confidence="0.841118">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285,
</note>
<affiliation confidence="0.602379">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998409">
276
</page>
<bodyText confidence="0.99991116">
The most important result claimed in this pa-
per is that, to the best of our knowledge, the pre-
sented ranker system outperforms all the previ-
ously published systems evaluated on the PDT
data. Moreover, the performance of our ranker (f-
score 79.43%) for Czech data is not far from the
performance of the state-of-the-art system for En-
glish described in Denis and Baldridge (2008) (f-
score for 3rd person pronouns 82.2 %).2
A side product of this work lies in bringing
empirical evidence – for a different language and
different data set – for the claim of Denis and
Baldridge (2007) that the ranking approach is
more appropriate for the task of AR than the clas-
sification approach.
The paper is structured as follows. The data
with manually annotated links we use are de-
scribed in Section 2. Section 3 outlines prepro-
cessing the data for training and evaluation pur-
poses. The classifier-based and ranker-based sys-
tems are described in Section 4 and Section 5 re-
spectively. Section 6 summarizes the achieved re-
sults by evaluating both approaches using the test
data. Conclusions and final remarks follow in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.675334" genericHeader="introduction">
2 Coreference links in the Prague
Dependency Treebank 2.0
</sectionHeader>
<bodyText confidence="0.990859217391304">
The Prague Dependency Treebank 2.03 (PDT 2.0,
Jan Hajiˇc, et al. (2006)) is a large collection of
linguistically annotated data and documentation,
based on the theoretical framework of Functional
Generative Description (FGD; introduced by Sgall
(1967) and later elaborated, e.g. in by Sgall et al.
(1986)). The PDT 2.0 data are Czech newspaper
texts selected from the Czech National Corpus4
(CNC).
The PDT 2.0 has a three-level structure. On the
lowest morphological level, a lemma and a posi-
tional morphological tag are added to each token.
The middle analytical level represents each sen-
tence as a surface-syntactic dependency tree. On
the highest tectogrammatical level, each sentence
is represented as a complex deep-syntactic depen-
2However, it should be noted that exact comparison is not
possible here, since the tasks are slightly different for the
two languages, especially because of typological differences
between Czech and English (frequent pro-drop in Czech)
and different information available in the underlying data re-
source on the other hand (manually annotated morphological
and syntactical information available for Czech).
</bodyText>
<footnote confidence="0.997729">
3http://ufal.mff.cuni.cz/pdt2.0/
4http://ucnk.ff.cuni.cz/
</footnote>
<bodyText confidence="0.999756066666667">
dency tree, see Mikulov´a and others (2005) for de-
tails. This level includes also annotation of coref-
erential links.
The PDT 2.0 contains 3,168 newspaper texts
(49,431 sentences) annotated on the tectogram-
matical level. Coreference has been annotated
manually in all this data. Following the FGD,
there are two types of coreference distinguished:
grammatical coreference and textual coreference
(Panevov´a, 1991). The main difference between
the two coreference types is that the antecedent in
grammatical coreference can be identified using
grammatical rules and sentence syntactic struc-
ture, whereas the antecedent in textual coreference
can not.
The further division of grammatical and textual
coreference is based on types of anaphors:
Grammatical anaphors: relative pronouns, re-
flexive pronouns, reciprocity pronouns, re-
stored (surface-unexpressed) “subjects” of
infinitive verbs below verbs of control,
Textual anaphors: personal and possessive pro-
nouns, demonstrative pronouns.
The data in the PDT 2.0 are divided into three
groups: training set (80%), development test set
(10%), and evaluation test set (10%). The training
and development test set can be freely exploited,
while the evaluation test data should serve only for
the very final evaluation of developed tools.
Table 1 shows the distribution of each anaphor
type. The total number of coreference links in the
PDT 2.0 data is 45,174.5 Personal pronouns in-
cluding those zero ones and possessive pronouns
form 37.4% of all anaphors in the entire corpus
(16,888 links).
An example tectogrammatical tree with de-
picted coreference links (arrows) is presented in
Figure 1. For the sake of simplicity, only three
node attributes are displayed below the nodes: tec-
togrammatical lemma, functor, and semantic part
of speech (tectogrammatical nodes themselves are
complex data structures and around twenty at-
tributes might be stored with them).
Tectogrammatical lemma is a canonical word
form or an artificial value of a newly created node
</bodyText>
<footnote confidence="0.998623666666667">
5In terms of the number of coreference links, PDT 2.0
is one of the largest existing manually annotated resources.
Another comparably large resource is BBN Pronoun Coref-
erence and Entity Type Corpus (Weischedel and Brunstein,
2005), which contains a stand-off annotation of coreference
links in the Penn Treebank texts.
</footnote>
<page confidence="0.990387">
277
</page>
<table confidence="0.999830555555555">
Type/Count train dtest etest
Personal pron. 12,913 1,945 2,030
Relative pron. 6,957 948 1,034
Under-control pron. 6,598 874 907
Reflexive pron. 3,381 452 571
Demonstrative pron. 2,582 332 344
Reciprocity pron. 882 110 122
Other 320 35 42
Total 34,983 4,909 5,282
</table>
<tableCaption confidence="0.9132235">
Table 1: Distribution of the different anaphor
types in the PDT 2.0.
</tableCaption>
<bodyText confidence="0.998521909090909">
on the tectogrammatical level. E.g. the (artifi-
cial) tectogrammatical lemma #PersPron stands
for personal (and possessive) pronouns, be they
expressed on the surface (i.e., present in the orig-
inal sentence) or restored during the annotation
of the tectogrammatical tree structure (zero pro-
nouns).
Functor captures the deep-syntactic dependency
relation between a node and its governor in the
tectogrammatical tree. According to FGD, func-
tors are divided into actants (ACT – actor, PAT –
patient, ADDR – addressee, etc.) and free modi-
fiers (LOC – location, BEN – benefactor, RHEM
– rhematizer, TWHEN – temporal modifier, APP
– appurtenance, etc.).
Semantic parts of speech correspond to ba-
sic onomasiological categories (substance, fea-
ture, factor, event). The main semantic POS dis-
tinguished in PDT 2.0 are: semantic nouns, se-
mantic adjectives, semantic adverbs and semantic
verbs (for example, personal and possessive pro-
nouns belong to semantic nouns).
</bodyText>
<sectionHeader confidence="0.934939" genericHeader="method">
3 Training data preparation
</sectionHeader>
<bodyText confidence="0.9993185">
The training phase of both presented AR systems
can be outlined as follows:
</bodyText>
<listItem confidence="0.997018">
1. detect nodes which are anaphors (Sec-
tion 3.1),
2. for each anaphor ai, collect the set of an-
tecedent candidates Cand(ai) (Section 3.2),
3. for each anaphor ai, divide the set of
candidates into positive instances (true an-
tecedents) and negative instances (Sec-
tion 3.3),
4. for each pair of an anaphor ai and an an-
tecedent candidate cj E Cand(ai), compute
</listItem>
<bodyText confidence="0.992239">
the feature vector 4b(c, ai) (Section 3.4),
5. given the anaphors, their sets of antecedent
candidates (with related feature vectors), and
the division into positive and negative candi-
dates, train the system for identifying the true
antecedents among the candidates.
Steps 1-4 can be seen as training data prepro-
cessing, and are very similar for both systems.
System-specific details are described in Section 4
and Section 5 respectively.
</bodyText>
<subsectionHeader confidence="0.999719">
3.1 Anaphor selection
</subsectionHeader>
<bodyText confidence="0.999996357142857">
In the presented work, only third person per-
sonal (and possessive) pronouns are considered,6
be they expressed on the surface or reconstructed.
We treat as anaphors all tectogrammatical nodes
with lemma #PersPron and third person stored in
the gram/person grammateme. More than 98 %
of such nodes have their antecedents (in the sense
of textual coreference) marked in the training data.
Therefore we decided to rely only on this highly
precise rule when detecting anaphors.7
In our example tree, the node #PersPron rep-
resenting his on the surface and the node #Per-
sPron representing the zero personal pronoun he
will be recognized as anaphors.
</bodyText>
<subsectionHeader confidence="0.999766">
3.2 Candidate selection
</subsectionHeader>
<bodyText confidence="0.910046545454545">
In both systems, the predicted antecedent of a
given anaphor ai is selected from an easy-to-
compute set of antecedent candidates denoted as
Cand(ai). We limit the set of candidates to se-
mantic nouns which are located either in the same
sentence before the anaphor, or in the preced-
ing sentence. Table 2 shows that if we disregard
cataphoric and longer anaphoric links, we loose
a chance for correct answer with only 6 % of
anaphors.
6The reason is that antecedents of most other types of
anaphors annotated in PDT 2.0 can be detected – given
the tree topology and basic node attributes – with precision
higher than 90 %, as it was shown already in Kuˇcov´a and
ˇZabokrtsk´y (2005). For instance, antecedents of reflexive
pronouns are tree-nearest clause subjects in most cases, while
antecedents of relative pronouns are typically parents of the
relative clause heads.
7It is not surprising that no discourse status model (as used
e.g. in Denis and Baldridge (2008)) is practically needed
here, since we limit ourselves to personal pronouns, which
are almost always “discourse-old”.
</bodyText>
<page confidence="0.989799">
278
</page>
<table confidence="0.9975862">
Antecedent location Percnt.
Previous sentence 37 %
Same sentence, preceding the anaphor 57 %
Same sentence, following the anaphor 5 %
Other 1 %
</table>
<tableCaption confidence="0.985304">
Table 2: Location of antecedents with respect to
anaphors in the training section of PDT 2.0.
</tableCaption>
<subsectionHeader confidence="0.997108">
3.3 Generating positive and negative
instances
</subsectionHeader>
<bodyText confidence="0.99996832">
If the true antecedent of ai is not present in
Cand(ai), no training instance is generated. If it is
present, the sets of negative and positive instances
are generated based on the anaphor. This prepro-
cessing step differs for the two systems, because
the classifier can be easily provided with more
than one positive instance per anaphor, whereas
the ranker can not.
In the classification-based system, all candi-
dates belonging to the coreferential chain are
marked as positive instances in the training data.
The remaining candidates are marked as negative
instances.
In the ranking-based system, the coreferential
chain is followed from the anaphor to the nearest
antecedent which itself is not an anaphor in gram-
matical coreference.8 The first such node is put on
the top of the training rank list, as it should be pre-
dicted as the winner (E.g., the nearest antecedent
of the zero personal pronoun he in the example
tree is the relative pronoun who, however, it is a
grammatical anaphor, so its antecedent Brien is
chosen as the winner instead). All remaining (neg-
ative) candidates are added to the list, without any
special ordering.
</bodyText>
<subsectionHeader confidence="0.994238">
3.4 Feature extraction
</subsectionHeader>
<bodyText confidence="0.999973857142857">
Our model makes use of a wide range of features
that are obtained not only from all three levels of
the PDT 2.0 but also from the Czech National Cor-
pus and the EuroWordNet. Each training or test-
ing instance is represented by a feature vector. The
features describe the anaphor, its antecedent can-
didate and their relationship, as well as their con-
</bodyText>
<footnote confidence="0.641810333333333">
8Grammatical anaphors are skipped because they usually
do not provide sufficient information (e.g., reflexive pronouns
provide almost no cues at all). The classification approach
does not require such adaptation – it is more robust against
such lack of information as it treats the whole chain as posi-
tive instances.
</footnote>
<bodyText confidence="0.989239611111111">
texts. All features are listed in Table 4 in the Ap-
pendix.
When designing the feature set on personal pro-
nouns, we take into account the fact that Czech
personal pronouns stand for persons, animals and
things, therefore they agree with their antecedents
in many attributes and functions. Further we use
the knowledge from the Lappin and Leass’s al-
gorithm (Lappin and Leass, 1994), the Mitkov’s
robust, knowledge-poor approach (Mitkov, 2002),
and the theory of topic-focus articulation (Kuˇcov´a
et al., 2005). We want to take utmost advantage of
information from the antecedent’s and anaphor’s
node on all three levels as well.
Distance: Numeric features capturing the dis-
tance between the anaphor and the candidate, mea-
sured by the number of sentences, clauses, tree
nodes and candidates between them.
Morphological agreement: Categorial features
created from the values of tectogrammatical gen-
der and number9 and from selected morphological
categories from the positional tag10 of the anaphor
and of the candidate. In addition, there are features
indicating the strict agreement between these pairs
and features formed by concatenating the pair of
values of the given attribute in the two nodes (e.g.,
masc neut).
Agreement in dependency functions: Catego-
rial features created from the values of tec-
togrammatical functor and analytical functor (with
surface-syntactic values such as Sb, Pred, Obj) of
the anaphor and of the candidate, their agreement
and joint feature. There are two more features in-
dicating whether the candidate/anaphor is an ac-
tant and whether the candidate/anaphor is a sub-
ject on the tectogrammatical level.11
</bodyText>
<listItem confidence="0.7657135">
Context: Categorial features describing the con-
text of the anaphor and of the candidate:
• parent – tectogrammatical functor and the se-
mantic POS of the effective parent12 of the
</listItem>
<footnote confidence="0.915237727272727">
9Sometimes gender and number are unknown, but we can
identify the gender and number of e.g. relative or reflexive
pronouns on the tectogrammatical level thanks to their an-
tecedent.
10A positional tag from the morphological level is a string
of 15 characters. Every positions encodes one morphological
category using one character.
11A subject on the tectogrammatical level can be a node
with the analytical functor Sb or with the tectogrammatical
functor Actor in a clause without a subject.
12The ”true governor” in terms of dependency relations.
</footnote>
<page confidence="0.99637">
279
</page>
<bodyText confidence="0.999856666666667">
anaphor and the candidate, their agreement
and joint feature; a feature indicating the
agreement of both parents’ tectogrammatical
lemma and their joint feature; a joint feature
of the pair of the tectogrammatical lemma
of the candidate and the effective parent’s
lemma of the anaphor; and a feature indicat-
ing whether the candidate and the anaphor are
siblings.13
</bodyText>
<listItem confidence="0.919812210526316">
• coordination – a feature that indicates
whether the candidate is a member of a coor-
dination and a feature indicating whether the
anaphor is a possessive pronoun and is in the
coordination with the candidate
• collocation – a feature indicating whether the
candidate has appeared in the same colloca-
tion as the anaphor within the text14 and a
feature that indicates the collocation assumed
from the Czech National Corpus.15
• boundness – features assigned on the ba-
sis of contextual boundness (available in the
tectogrammatical trees) {contextually bound,
contrastively contextually bound, or contex-
tually non-bound}16 for the anaphor and the
candidate; their agreement and joint feature.
• frequency – 1 if the candidate is a denotative
semantic noun and occurs more than once
within the text; otherwise 0.
</listItem>
<bodyText confidence="0.99061437037037">
Semantics: Semantically oriented feature that
indicates whether the candidate is a person name
for the present and a set of 63 binary ontologi-
cal attributes obtained from the EuroWordNet.17
These attributes determine the positive or negative
13Both have the same effective parent.
14If the anaphor’s effective parent is a verb and the can-
didate is a denotative semantic noun and has appeared as a
child of the same verb and has had the same functor as the
anaphor.
15The probability of the candidate being a subject preced-
ing the verb, which is the effective parent of the anaphor.
16Contextual boundness is a property of an expression (be
it expressed or absent in the surface structure of the sentence)
which determines whether the speaker (author) uses the ex-
pression as given (for the recipient), i.e. uniquely determined
by the context.
17The Top Ontology used in EuroWordNet (EWN) con-
tains the (structured) set of 63 basic semantic concepts like
Place, Time, Human, Group, Living, etc. For the majority of
English synsets (set of synonyms, the basic unit of EWN), the
appropriate subset of these concepts are listed. Using the In-
ter Lingual Index that links the synsets of different languages,
the set of relevant concepts can be found also for Czech lem-
mas.
relation between the candidate’s lemma and the se-
mantic concepts.
</bodyText>
<sectionHeader confidence="0.99293" genericHeader="method">
4 Classifier-based system
</sectionHeader>
<bodyText confidence="0.99996612">
Our classification approach uses C5.0, a succes-
sor of C4.5 (Quinlan, 1993), which is probably the
most widely used program for inducing decision
trees. Decision trees are used in many AR sys-
tems such as Aone and Bennett (1995), Mccarthy
and Lehnert (1995), Soon et al. (2001), and Ng
and Cardie (2002).18
Our classifier-based system takes as input a set
of feature vectors as described in Section 3.4 and
their classifications (1 – true antecedent, 0 – non-
antecedent) and produces a decision tree that is
further used for classifying new pairs of candidate
and anaphor.
The classifier antecedent selection algorithm
works as follows. For each anaphor ai, feature
vectors 4b(c, ai) are computed for all candidates
c E Cand(ai) and passed to the trained decision
tree. The candidate classified as positive is re-
turned as the predicted antecedent. If there are
more candidates classified as positive, the nearest
one is chosen.
If no candidate is classified as positive, a sys-
tem of handwritten fallback rules can be used. The
fallback rules are the same rules as those used in
the baseline system in Section 6.2.
</bodyText>
<sectionHeader confidence="0.995494" genericHeader="method">
5 Ranker-based system
</sectionHeader>
<bodyText confidence="0.999925">
In the ranker-based AR system, every training ex-
ample is a pair (ai, yi), where ai is the anaphoric
expression and yi is the true antecedent. Using
the candidate extraction function Cand, we aim
to rank the candidates so that the true antecedent
would always be the first candidate on the list. The
ranking is modeled by a linear model of the fea-
tures described in Section 3.4. According to the
model, the antecedent yi for an anaphoric expres-
sion ai is found as:
</bodyText>
<equation confidence="0.995111">
yi = argmax -b(c, ai) · w
cECand(ai)
</equation>
<bodyText confidence="0.996594">
The weights w of the linear model are trained
using a modification of the averaged perceptron al-
</bodyText>
<footnote confidence="0.465562333333333">
18Besides C5.0, we plan to use also other classifiers in the
future (especially Support Vector Machine, which is often
employed in AR experiments, e.g. by Ng (2005) and Yang
et al. (2006)) in order to study how the classifier choice in-
fluences the AR system performance on our data and feature
sets.
</footnote>
<page confidence="0.991533">
280
</page>
<bodyText confidence="0.908092666666667">
gorithm (Collins, 2002). This is averaged percep-
tron learning with a modified loss function adapted
to the ranking scenario. The loss function is tai-
lored to the task of correctly ranking the true an-
tecedent, the ranking of other candidates is irrel-
evant. The algorithm (without averaging the pa-
rameters) is listed as Algorithm 1. Note that the
training instances where yi ∈� Cand(ai) were ex-
cluded from the training.
Algorithm 1: Modified perceptron algorithm
for ranking. 4&gt; is the feature extraction func-
tion, ai is the anaphoric expression, yi is the
true antecedent.
Antecedent selection algorithm using a ranker:
For each third person pronoun create a feature vec-
tor from the pronoun and the semantic noun pre-
ceding the pronoun and is in the same sentence or
in the previous sentence. Use the trained ranking
features weight model to get out the candidate’s
total weight. The candidate with the highest fea-
tures weight is identified as the antecedent.
</bodyText>
<sectionHeader confidence="0.998486" genericHeader="evaluation">
6 Experiments and evaluation
</sectionHeader>
<subsectionHeader confidence="0.998525">
6.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.973512">
For the evaluation we use the standard metrics:19
</bodyText>
<equation confidence="0.918567">
Precision = number of correctly predicted anaphoric third person pronouns
number of all predicted third person pronouns
Recall = number of correctly predicted anaphoric third person pronouns
number of all anaphoric third person pronouns
F-measure = 2xPrecisionxRecall
Precision+Recall
</equation>
<bodyText confidence="0.8498826">
We consider an anaphoric third person pronoun
to be correctly predicted when we can success-
19Using simple accuracy would not be adequate, as there
can be no link (or more than one) leading from an anaphor
in the annotated data. In other words, finding whether a pro-
noun has an antecedent or not is a part of the task. A deeper
discussion about coreference resolution metrics can be found
in Luo (2005).
fully indicate its antecedent, which can be any an-
tecedent from the same coreferential chain as the
anaphor.
Both the AR systems were developed and tested
on PDT 2.0 training and development test data. Fi-
nally they were tested on evaluation test data for
the final scoring, summarized in Section 6.3.
</bodyText>
<subsectionHeader confidence="0.999304">
6.2 Baseline system
</subsectionHeader>
<bodyText confidence="0.992757142857143">
We have made some baseline rules for the task of
AR and tested them on the PDT 2.0 evaluation test
data. Their results are reported in Table 3. Base-
line rules are following: For each third person pro-
noun, consider all semantic nouns which precede
the pronoun and are not further than the previous
sentence, and:
</bodyText>
<listItem confidence="0.962832181818182">
• select the nearest one as its antecedent
(BASE 1),
• select the nearest one which is a clause sub-
ject (BASE 2),
• select the nearest one which agrees in gender
and number (BASE 3),
• select the nearest one which agrees in gen-
der and number; if there is no such noun,
choose the nearest clause subject; if no clause
subject was found, choose the nearest noun
(BASE 3+2+1).
</listItem>
<subsectionHeader confidence="0.999716">
6.3 Experimental results and discussion
</subsectionHeader>
<bodyText confidence="0.999970052631579">
Scores for all three systems (baseline, clasifier
with and without fallback, ranker) are given in Ta-
ble 3. Our baseline system based on the combina-
tion of three rules (BASE 3+2+1) reports results
superior to the ones of the rule-based system de-
scribed in Kuˇcov´a and ˇZabokrtsk´y (2005). Kuˇcov´a
and ˇZabokrtsk´y proposed a set of filters for per-
sonal pronominal anaphora resolution. The list of
candidates was built from the preceding and the
same sentence as the personal pronoun. After ap-
plying each filter, improbable candidates were cut
off. If there was more than one candidate left at
the end, the nearest one to the anaphor was cho-
sen as its antecedent. The reported final success
rate was 60.4 % (counted simply as the number of
correctly predicted links divided by the number of
pronoun anaphors in the test data section).
An interesting point of the classifier-based sys-
tem lies in the comparison with the rule-based
</bodyText>
<figure confidence="0.976691416666667">
input : N training examples (ai, yi),
number of iterations T
for t ← 1 to T, i ← 1 to N do
Pi ← argmaxcECand(ai) 4&gt;(c, ai) · →−w;
if yi =6 yi then
→− w = →−w + 4&gt;(yi, ai) − 4&gt;(yi, ai);
end
end
output: weights →−w
→−
init : →−w ←
0 ;
</figure>
<page confidence="0.990354">
281
</page>
<table confidence="0.9998135">
Rule P R F
BASE 1 17.82% 18.00% 17.90%
BASE 2 41.69% 42.06% 41.88%
BASE 3 59.00% 59.50% 59.24%
BASE 3+2+1 62.55% 63.03% 62.79%
CLASS 69.9% 70.44% 70.17%
CLASS+3+2+1 76.02% 76.60% 76.30%
RANK 79.13% 79.74% 79.43%
</table>
<tableCaption confidence="0.8844095">
Table 3: Precision (P), Recall (R) and F-measure
(F) results for the presented AR systems.
</tableCaption>
<bodyText confidence="0.998430421052632">
system of Ngu.y and ˇZabokrtsk´y (2007). With-
out the rule-based fallback (CLASS), the clas-
sifier falls behind the Ngu.y and ˇZabokrtsk´y’s
system (74.2%), while including the fallback
(CLASS+3+2+1) it gives better results.
Overall, the ranker-based system (RANK) sig-
nificantly outperforms all other AR systems for
Czech with the f-score of 79.43%. Comparing
with the model for third person pronouns of Denis
and Baldridge (2008), which reports the f-score of
82.2%, our ranker is not so far behind. It is im-
portant to say that our system relies on manually
annotated information20 and we solve the task of
anaphora resolution for third person pronouns on
the tectogrammatical level of the PDT 2.0. That
means these pronouns are not only those expressed
on the surface, but also artificially added (recon-
structed) into the structure according to the princi-
ples of FGD.
</bodyText>
<sectionHeader confidence="0.994075" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.998357625">
In this paper we report two systems for AR in
Czech: the classifier-based system and the ranker-
based system. The latter system reaches f-score
79.43% on the Prague Dependency Treebank test
data and significantly outperforms all previously
published results. Our results support the hypoth-
esis that ranking approaches are more appropriate
for the AR task than classification approaches.
</bodyText>
<sectionHeader confidence="0.857413" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.68386">
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
</bodyText>
<footnote confidence="0.934396166666667">
20In the near future, we plan to re-run the experiments us-
ing sentence analyses created by automatic tools (all needed
tools are available in the TectoMT software framework
(ˇZabokrtsk´y et al., 2008)) instead of manually created analy-
ses, in order to examine the sensitivity of the AR system to
annotation quality.
</footnote>
<reference confidence="0.97660287037037">
anaphora resolution strategies. In Proceedings of the
33rd annual meeting on Association for Computa-
tional Linguistics, pages 122–129, Morristown, NJ,
USA. Association for Computational Linguistics.
Ant´onio Branco, Tony McEnery, Ruslan Mitkov, and
F´atima Silva, editors. 2007. Proceedings of the 6th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2007), Lagos (Algarve), Portugal.
CLUP-Center for Linguistics of the University of
Oporto.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148–156, Athens, Greece,
March. Association for Computational Linguistics.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of EMNLP, volume 10, pages 1–8.
Pascal Denis and Jason Baldridge. 2007. A ranking
approach to pronoun resolution. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI2007), pages 1588–1593, Hyder-
abad, India, January 6–12.
Pascal Denis and Jason Baldridge. 2008. Special-
ized models and ranking for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2008), pages 660–669, Honolulu, Hawaii,
USA, October 25–27.
Jan Hajiˇc, et al. 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Lucie Kuˇcov´a and Zdenˇek ˇZabokrtsk´y. 2005.
Anaphora in Czech: Large Data and Experiments
with Automatic Anaphora. LNCS/Lecture Notes in
Artificial Intelligence/Proceedings of Text, Speech
and Dialogue, 3658:93–98.
Lucie Kuˇcov´a, Kateˇrina Vesel´a, Eva Hajiˇcov´a, and
Jiˇr´ı Havelka. 2005. Topic-focus articulation and
anaphoric relations: A corpus based probe. In Klaus
Heusinger and Carla Umbach, editors, Proceedings
of Discourse Domains and Information Structure
workshop, pages 37–46, Edinburgh, Scotland, UK,
Aug. 8-12.
Shalom Lappin and Herbert J. Leass. 1994. ”an algo-
rithm for pronominal anaphora resolution”. Compu-
tational Linguistics, 20(4):535–561.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ’05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25–32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.988408">
282
</page>
<reference confidence="0.964496658227848">
J Mccarthy and Wendy G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In In Pro-
ceedings of the Fourteenth International Joint Con-
ference on Artificial Intelligence, pages 1050–1055.
Marie Mikulov´a et al. 2005. Anotace na tektogra-
matick´e rovinˇe Praˇzsk´eho z´avislostn ho korpusu.
Anot´atorsk´a pˇr ruˇcka (t-layer annotation guide-
lines). Technical Report TR-2005-28, ´UFAL MFF
UK, Prague, Prague.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
V´aclav Nˇemˇc´ık. 2006. Anaphora Resolution. Mas-
ter’s thesis, Faculty of Informatics, Masaryk Univer-
sity.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ’02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104–111, Morristown, NJ, USA. Association
for Computational Linguistics.
Vincent Ng. 2005. Supervised ranking for pro-
noun resolution: Some recent improvements. In
Manuela M. Veloso and Subbarao Kambhampati,
editors, AAAI, pages 1081–1086. AAAI Press / The
MIT Press.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2008), pages 640–649, Hon-
olulu, Hawaii, USA.
Giang Linh Ngu.y and Zdenˇek ˇZabokrtsk´y. 2007.
Rule-based approach to pronominal anaphora reso-
lution applied on the prague dependency treebank
2.0 data. In Branco et al. (Branco et al., 2007), pages
77–81.
Giang Linh Ngu.y. 2006. Proposal of a Set of Rules
for Anaphora Resolution in Czech. Master’s thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity.
Jarmila Panevov´a. 1991. Koreference gramatick´a nebo
textov´a? In Etudes de linguistique romane et slave.
Krakow.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn´ı popis jazyka a ˇcesk´a
deklinace. Academia, Prague, Czech Republic.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Comput. Linguist., 27(4):521–544.
Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus. CD-
ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2005T33, Philadelphia.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 176–
183, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL2006), pages 41–48, Sydney, Australia, July
17–21.
</reference>
<page confidence="0.997408">
283
</page>
<figure confidence="0.98103425">
A Appendix
deset - TEN
RSTR
adj.quant.def
</figure>
<figureCaption confidence="0.888993">
Figure 1: Simplified tectogrammatical tree representing the sentence O’Brien, ktery Louganise trenoval
deset let, o jeho onemocnenivedel, ale zavdzal se mlcenim. (Lit.: O’Brien, who Louganis trained for
ten years, about his injury knew, but (he) tied himself to secrecy.) Note two coreferential chains {Brien,
who, (he)} and {Louganis, his}.
</figureCaption>
<figure confidence="0.999166585365854">
rok - YEAR
THL
n.denot
který - WHO
ACT
n.pron.indef
#PersPron - HIS
ACT
n.pron.def.pers
#PersPron - (HE)
ACT
n.pron.def.pers
Louganis - LOUGANIS
PAT
n.denot
t-ln95049-047-p3s1
root
Brien - BRIEN
ACT
n.denot
ale - BUT.enunc
ADVS
coap
v✂dt - TO KNOW
PRED
v
zav✄zat_se - TO TIE SOMEONE&apos;S SELF
PRED
v
O - O
RSTR
n.denot
tr novat - TO TRAIN
RSTR
v
onemocněn - INJURY
PAT
n.denot.neg
mlčen - SECRECY
PAT
n.denot.neg
</figure>
<page confidence="0.970005">
284
</page>
<sectionHeader confidence="0.709923" genericHeader="acknowledgments">
Distance
</sectionHeader>
<bodyText confidence="0.981363">
sent dist sentence distance between c and az
clause dist clause distance between c and az
node dist tree node distance between c and az
cand ord mention distance between c and az
</bodyText>
<subsectionHeader confidence="0.941179">
Morphological Agreement
</subsectionHeader>
<bodyText confidence="0.9985656">
gender t-gender of c and az, agreement, joint
number t-number of c and az, agreement, joint
apos m-POS of c and az, agreement, joint
asubpos detailed POS of c and az, agreement, joint
agen m-gender of c and az, agreement, joint
anum m-number of c and az, agreement, joint
acase m-case of c and az, agreement, joint
apossgen m-possessor’s gender of c and az, agreement, joint
apossnum m-possessor’s number of c and az, agreement, joint
apers m-person of c and az, agreement, joint
</bodyText>
<subsectionHeader confidence="0.917404">
Functional Agreement
</subsectionHeader>
<bodyText confidence="0.99634725">
afun a-functor of c and az, agreement, joint
fun t-functor of c and az, agreement, joint
act c/az is an actant, agreement
subj c/az is a subject, agreement
</bodyText>
<sectionHeader confidence="0.461068" genericHeader="references">
Context
</sectionHeader>
<bodyText confidence="0.999046181818182">
par fun t-functor of the parent of c and az, agreement, joint
par pos t-POS of the parent of c and az, agreement, joint
par lemma agreement between the parent’s lemma of c and az, joint
clem aparlem joint between the lemma of c and the parent’s lemma of az
c coord c is a member of a coordination
app coord c and az are in coordination &amp; az is a possessive pronoun
sibl c and az are siblings
coll c and az have the same collocation
cnk coll c and az have the same CNC collocation
tfa contextual boundness of c and az, agreement, joint
c freq c is a frequent word
</bodyText>
<subsectionHeader confidence="0.392176">
Semantics
</subsectionHeader>
<bodyText confidence="0.946619">
cand pers c is a person name
cand ewn semantic position of c’s lemma within the EuroWordNet Top Ontology
</bodyText>
<tableCaption confidence="0.989405">
Table 4: Features used by the perceptron-based model
</tableCaption>
<page confidence="0.997383">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324943">
<title confidence="0.770737">Comparison of Classification and Ranking Pronominal Anaphora Resolution in</title>
<author confidence="0.956949">Giang Linh</author>
<author confidence="0.956949">V´aclav Nov´ak</author>
<author confidence="0.956949">Zdenˇek</author>
<affiliation confidence="0.9663755">Charles University in Institute of Formal and Applied</affiliation>
<address confidence="0.684535">Malostransk´e n´amˇestf25,</address>
<abstract confidence="0.993487133333333">In this paper we compare two Machine Learning approaches to the task of pronominal anaphora resolution: a conventional classification system based on C5.0 decision trees, and a novel perceptron-based ranker. We use coreference links annotated in the Prague Dependency Treebank 2.0 for training and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>anaphora resolution strategies.</title>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>122--129</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker></marker>
<rawString>anaphora resolution strategies. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 122–129, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>Proceedings of the 6th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2007),</booktitle>
<editor>Ant´onio Branco, Tony McEnery, Ruslan Mitkov, and F´atima Silva, editors.</editor>
<location>Lagos (Algarve),</location>
<contexts>
<context position="1605" citStr="(2007)" startWordPosition="241" endWordPosition="241">ndidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009 which leads to further significant shift in performance, more recently documented in Denis and Baldridge (2008). In this paper we deal with supervised approaches to pronominal anaphora in Czech.1 For training and evaluation purposes, we use coreferences links annotated in the Prague Dependency Treebank, (Jan Hajiˇc, et al., 2006). We limit ourselves only to textual coreference (see Section </context>
<context position="3169" citStr="(2007)" startWordPosition="493" endWordPosition="493">ction whose optimal weight vector is estimated using perceptron learning inspired by Collins (2002). We try to provide both implementations with as similar input information as possible in order to be able to compare their performance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based system evaluated in Ngu.y and ˇZabokrtsk´y (2007). To illustrate the real complexity of the task, we also provide performance evaluation of a baseline solution. 1Currently one can see a growing interest in unsupervised techniques, e.g. Charniak and Elsner (2009) and Ng (2008). However, we make only a very tiny step in this direction: we use a probabilistic feature based on collocation counts in large unannotated data (namely in the Czech National Corpus). Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285, Queen Mary University of London, September 2009. c�2009 Associat</context>
<context position="4385" citStr="(2007)" startWordPosition="695" endWordPosition="695">mputational Linguistics 276 The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated on the PDT data. Moreover, the performance of our ranker (fscore 79.43%) for Czech data is not far from the performance of the state-of-the-art system for English described in Denis and Baldridge (2008) (fscore for 3rd person pronouns 82.2 %).2 A side product of this work lies in bringing empirical evidence – for a different language and different data set – for the claim of Denis and Baldridge (2007) that the ranking approach is more appropriate for the task of AR than the classification approach. The paper is structured as follows. The data with manually annotated links we use are described in Section 2. Section 3 outlines preprocessing the data for training and evaluation purposes. The classifier-based and ranker-based systems are described in Section 4 and Section 5 respectively. Section 6 summarizes the achieved results by evaluating both approaches using the test data. Conclusions and final remarks follow in Section 7. 2 Coreference links in the Prague Dependency Treebank 2.0 The Pra</context>
<context position="25808" citStr="(2007)" startWordPosition="4178" endWordPosition="4178">arison with the rule-based input : N training examples (ai, yi), number of iterations T for t ← 1 to T, i ← 1 to N do Pi ← argmaxcECand(ai) 4&gt;(c, ai) · →−w; if yi =6 yi then →− w = →−w + 4&gt;(yi, ai) − 4&gt;(yi, ai); end end output: weights →−w →− init : →−w ← 0 ; 281 Rule P R F BASE 1 17.82% 18.00% 17.90% BASE 2 41.69% 42.06% 41.88% BASE 3 59.00% 59.50% 59.24% BASE 3+2+1 62.55% 63.03% 62.79% CLASS 69.9% 70.44% 70.17% CLASS+3+2+1 76.02% 76.60% 76.30% RANK 79.13% 79.74% 79.43% Table 3: Precision (P), Recall (R) and F-measure (F) results for the presented AR systems. system of Ngu.y and ˇZabokrtsk´y (2007). Without the rule-based fallback (CLASS), the classifier falls behind the Ngu.y and ˇZabokrtsk´y’s system (74.2%), while including the fallback (CLASS+3+2+1) it gives better results. Overall, the ranker-based system (RANK) significantly outperforms all other AR systems for Czech with the f-score of 79.43%. Comparing with the model for third person pronouns of Denis and Baldridge (2008), which reports the f-score of 82.2%, our ranker is not so far behind. It is important to say that our system relies on manually annotated information20 and we solve the task of anaphora resolution for third per</context>
</contexts>
<marker>2007</marker>
<rawString>Ant´onio Branco, Tony McEnery, Ruslan Mitkov, and F´atima Silva, editors. 2007. Proceedings of the 6th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 2007), Lagos (Algarve), Portugal. CLUP-Center for Linguistics of the University of Oporto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Micha Elsner</author>
</authors>
<title>EM works for pronoun anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>148--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="3382" citStr="Charniak and Elsner (2009)" startWordPosition="524" endWordPosition="527">r to be able to compare their performance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based system evaluated in Ngu.y and ˇZabokrtsk´y (2007). To illustrate the real complexity of the task, we also provide performance evaluation of a baseline solution. 1Currently one can see a growing interest in unsupervised techniques, e.g. Charniak and Elsner (2009) and Ng (2008). However, we make only a very tiny step in this direction: we use a probabilistic feature based on collocation counts in large unannotated data (namely in the Czech National Corpus). Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 276 The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated o</context>
</contexts>
<marker>Charniak, Elsner, 2009</marker>
<rawString>Eugene Charniak and Micha Elsner. 2009. EM works for pronoun anaphora resolution. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 148–156, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<volume>10</volume>
<pages>1--8</pages>
<contexts>
<context position="2662" citStr="Collins (2002)" startWordPosition="414" endWordPosition="415"> use coreferences links annotated in the Prague Dependency Treebank, (Jan Hajiˇc, et al., 2006). We limit ourselves only to textual coreference (see Section 2) and to personal and possessive pronouns. We make use of a rich set of features available thanks to the complex annotation scenario of the treebank. We experiment with two of the above mentioned techniques for AR: a classifier and a ranker. The former is based on a top-down induction of decision trees (Quinlan, 1993). The latter uses a simple scoring function whose optimal weight vector is estimated using perceptron learning inspired by Collins (2002). We try to provide both implementations with as similar input information as possible in order to be able to compare their performance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based system evaluated in Ngu.y and ˇZabokrtsk´y (2007). To illustrate the real complexity of the task, we also provide performance evaluation of a </context>
<context position="21527" citStr="Collins, 2002" startWordPosition="3439" endWordPosition="3440">modeled by a linear model of the features described in Section 3.4. According to the model, the antecedent yi for an anaphoric expression ai is found as: yi = argmax -b(c, ai) · w cECand(ai) The weights w of the linear model are trained using a modification of the averaged perceptron al18Besides C5.0, we plan to use also other classifiers in the future (especially Support Vector Machine, which is often employed in AR experiments, e.g. by Ng (2005) and Yang et al. (2006)) in order to study how the classifier choice influences the AR system performance on our data and feature sets. 280 gorithm (Collins, 2002). This is averaged perceptron learning with a modified loss function adapted to the ranking scenario. The loss function is tailored to the task of correctly ranking the true antecedent, the ranking of other candidates is irrelevant. The algorithm (without averaging the parameters) is listed as Algorithm 1. Note that the training instances where yi ∈� Cand(ai) were excluded from the training. Algorithm 1: Modified perceptron algorithm for ranking. 4&gt; is the feature extraction function, ai is the anaphoric expression, yi is the true antecedent. Antecedent selection algorithm using a ranker: For </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP, volume 10, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>A ranking approach to pronoun resolution.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI2007),</booktitle>
<pages>1588--1593</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="1605" citStr="Denis and Baldridge (2007)" startWordPosition="238" endWordPosition="241">ues (e.g., single candidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009 which leads to further significant shift in performance, more recently documented in Denis and Baldridge (2008). In this paper we deal with supervised approaches to pronominal anaphora in Czech.1 For training and evaluation purposes, we use coreferences links annotated in the Prague Dependency Treebank, (Jan Hajiˇc, et al., 2006). We limit ourselves only to textual coreference (see Section </context>
<context position="4385" citStr="Denis and Baldridge (2007)" startWordPosition="692" endWordPosition="695">9 Association for Computational Linguistics 276 The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated on the PDT data. Moreover, the performance of our ranker (fscore 79.43%) for Czech data is not far from the performance of the state-of-the-art system for English described in Denis and Baldridge (2008) (fscore for 3rd person pronouns 82.2 %).2 A side product of this work lies in bringing empirical evidence – for a different language and different data set – for the claim of Denis and Baldridge (2007) that the ranking approach is more appropriate for the task of AR than the classification approach. The paper is structured as follows. The data with manually annotated links we use are described in Section 2. Section 3 outlines preprocessing the data for training and evaluation purposes. The classifier-based and ranker-based systems are described in Section 4 and Section 5 respectively. Section 6 summarizes the achieved results by evaluating both approaches using the test data. Conclusions and final remarks follow in Section 7. 2 Coreference links in the Prague Dependency Treebank 2.0 The Pra</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis and Jason Baldridge. 2007. A ranking approach to pronoun resolution. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI2007), pages 1588–1593, Hyderabad, India, January 6–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008),</booktitle>
<pages>660--669</pages>
<location>Honolulu, Hawaii, USA,</location>
<contexts>
<context position="1923" citStr="Denis and Baldridge (2008)" startWordPosition="289" endWordPosition="292">en the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009 which leads to further significant shift in performance, more recently documented in Denis and Baldridge (2008). In this paper we deal with supervised approaches to pronominal anaphora in Czech.1 For training and evaluation purposes, we use coreferences links annotated in the Prague Dependency Treebank, (Jan Hajiˇc, et al., 2006). We limit ourselves only to textual coreference (see Section 2) and to personal and possessive pronouns. We make use of a rich set of features available thanks to the complex annotation scenario of the treebank. We experiment with two of the above mentioned techniques for AR: a classifier and a ranker. The former is based on a top-down induction of decision trees (Quinlan, 199</context>
<context position="4183" citStr="Denis and Baldridge (2008)" startWordPosition="655" endWordPosition="658">zech National Corpus). Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 276 The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated on the PDT data. Moreover, the performance of our ranker (fscore 79.43%) for Czech data is not far from the performance of the state-of-the-art system for English described in Denis and Baldridge (2008) (fscore for 3rd person pronouns 82.2 %).2 A side product of this work lies in bringing empirical evidence – for a different language and different data set – for the claim of Denis and Baldridge (2007) that the ranking approach is more appropriate for the task of AR than the classification approach. The paper is structured as follows. The data with manually annotated links we use are described in Section 2. Section 3 outlines preprocessing the data for training and evaluation purposes. The classifier-based and ranker-based systems are described in Section 4 and Section 5 respectively. Section</context>
<context position="12342" citStr="Denis and Baldridge (2008)" startWordPosition="1934" endWordPosition="1937">c and longer anaphoric links, we loose a chance for correct answer with only 6 % of anaphors. 6The reason is that antecedents of most other types of anaphors annotated in PDT 2.0 can be detected – given the tree topology and basic node attributes – with precision higher than 90 %, as it was shown already in Kuˇcov´a and ˇZabokrtsk´y (2005). For instance, antecedents of reflexive pronouns are tree-nearest clause subjects in most cases, while antecedents of relative pronouns are typically parents of the relative clause heads. 7It is not surprising that no discourse status model (as used e.g. in Denis and Baldridge (2008)) is practically needed here, since we limit ourselves to personal pronouns, which are almost always “discourse-old”. 278 Antecedent location Percnt. Previous sentence 37 % Same sentence, preceding the anaphor 57 % Same sentence, following the anaphor 5 % Other 1 % Table 2: Location of antecedents with respect to anaphors in the training section of PDT 2.0. 3.3 Generating positive and negative instances If the true antecedent of ai is not present in Cand(ai), no training instance is generated. If it is present, the sets of negative and positive instances are generated based on the anaphor. Thi</context>
<context position="26197" citStr="Denis and Baldridge (2008)" startWordPosition="4233" endWordPosition="4236"> 62.55% 63.03% 62.79% CLASS 69.9% 70.44% 70.17% CLASS+3+2+1 76.02% 76.60% 76.30% RANK 79.13% 79.74% 79.43% Table 3: Precision (P), Recall (R) and F-measure (F) results for the presented AR systems. system of Ngu.y and ˇZabokrtsk´y (2007). Without the rule-based fallback (CLASS), the classifier falls behind the Ngu.y and ˇZabokrtsk´y’s system (74.2%), while including the fallback (CLASS+3+2+1) it gives better results. Overall, the ranker-based system (RANK) significantly outperforms all other AR systems for Czech with the f-score of 79.43%. Comparing with the model for third person pronouns of Denis and Baldridge (2008), which reports the f-score of 82.2%, our ranker is not so far behind. It is important to say that our system relies on manually annotated information20 and we solve the task of anaphora resolution for third person pronouns on the tectogrammatical level of the PDT 2.0. That means these pronouns are not only those expressed on the surface, but also artificially added (reconstructed) into the structure according to the principles of FGD. 7 Conclusions In this paper we report two systems for AR in Czech: the classifier-based system and the rankerbased system. The latter system reaches f-score 79.</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008), pages 660–669, Honolulu, Hawaii, USA, October 25–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01,</booktitle>
<location>Philadelphia.</location>
<marker>Hajiˇc, 2006</marker>
<rawString>Jan Hajiˇc, et al. 2006. Prague Dependency Treebank 2.0. CD-ROM, Linguistic Data Consortium, LDC Catalog No.: LDC2006T01, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucie Kuˇcov´a</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Anaphora in Czech: Large Data and Experiments with Automatic Anaphora.</title>
<date>2005</date>
<booktitle>LNCS/Lecture Notes in Artificial Intelligence/Proceedings of Text, Speech and Dialogue,</booktitle>
<pages>3658--93</pages>
<marker>Kuˇcov´a, ˇZabokrtsk´y, 2005</marker>
<rawString>Lucie Kuˇcov´a and Zdenˇek ˇZabokrtsk´y. 2005. Anaphora in Czech: Large Data and Experiments with Automatic Anaphora. LNCS/Lecture Notes in Artificial Intelligence/Proceedings of Text, Speech and Dialogue, 3658:93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucie Kuˇcov´a</author>
<author>Kateˇrina Vesel´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Jiˇr´ı Havelka</author>
</authors>
<title>Topic-focus articulation and anaphoric relations: A corpus based probe.</title>
<date>2005</date>
<booktitle>Proceedings of Discourse Domains and Information Structure workshop,</booktitle>
<pages>37--46</pages>
<editor>In Klaus Heusinger and Carla Umbach, editors,</editor>
<location>Edinburgh, Scotland, UK,</location>
<marker>Kuˇcov´a, Vesel´a, Hajiˇcov´a, Havelka, 2005</marker>
<rawString>Lucie Kuˇcov´a, Kateˇrina Vesel´a, Eva Hajiˇcov´a, and Jiˇr´ı Havelka. 2005. Topic-focus articulation and anaphoric relations: A corpus based probe. In Klaus Heusinger and Carla Umbach, editors, Proceedings of Discourse Domains and Information Structure workshop, pages 37–46, Edinburgh, Scotland, UK, Aug. 8-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shalom Lappin</author>
<author>Herbert J Leass</author>
</authors>
<title>an algorithm for pronominal anaphora resolution”.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="14953" citStr="Lappin and Leass, 1994" startWordPosition="2365" endWordPosition="2368">ficient information (e.g., reflexive pronouns provide almost no cues at all). The classification approach does not require such adaptation – it is more robust against such lack of information as it treats the whole chain as positive instances. texts. All features are listed in Table 4 in the Appendix. When designing the feature set on personal pronouns, we take into account the fact that Czech personal pronouns stand for persons, animals and things, therefore they agree with their antecedents in many attributes and functions. Further we use the knowledge from the Lappin and Leass’s algorithm (Lappin and Leass, 1994), the Mitkov’s robust, knowledge-poor approach (Mitkov, 2002), and the theory of topic-focus articulation (Kuˇcov´a et al., 2005). We want to take utmost advantage of information from the antecedent’s and anaphor’s node on all three levels as well. Distance: Numeric features capturing the distance between the anaphor and the candidate, measured by the number of sentences, clauses, tree nodes and candidates between them. Morphological agreement: Categorial features created from the values of tectogrammatical gender and number9 and from selected morphological categories from the positional tag10</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Shalom Lappin and Herbert J. Leass. 1994. ”an algorithm for pronominal anaphora resolution”. Computational Linguistics, 20(4):535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="23250" citStr="Luo (2005)" startWordPosition="3720" endWordPosition="3721">mber of all predicted third person pronouns Recall = number of correctly predicted anaphoric third person pronouns number of all anaphoric third person pronouns F-measure = 2xPrecisionxRecall Precision+Recall We consider an anaphoric third person pronoun to be correctly predicted when we can success19Using simple accuracy would not be adequate, as there can be no link (or more than one) leading from an anaphor in the annotated data. In other words, finding whether a pronoun has an antecedent or not is a part of the task. A deeper discussion about coreference resolution metrics can be found in Luo (2005). fully indicate its antecedent, which can be any antecedent from the same coreferential chain as the anaphor. Both the AR systems were developed and tested on PDT 2.0 training and development test data. Finally they were tested on evaluation test data for the final scoring, summarized in Section 6.3. 6.2 Baseline system We have made some baseline rules for the task of AR and tested them on the PDT 2.0 evaluation test data. Their results are reported in Table 3. Baseline rules are following: For each third person pronoun, consider all semantic nouns which precede the pronoun and are not furthe</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 25–32, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mccarthy</author>
<author>Wendy G Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution. In</title>
<date>1995</date>
<booktitle>In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1192" citStr="Mccarthy and Lehnert (1995)" startWordPosition="167" endWordPosition="170">e Dependency Treebank 2.0 for training and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far. 1 Introduction Anaphora Resolution (AR) is a well established task in Natural Language Processing (Mitkov, 2002). Classification techniques (e.g., single candidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536,</context>
<context position="19725" citStr="Mccarthy and Lehnert (1995)" startWordPosition="3126" endWordPosition="3129">, etc. For the majority of English synsets (set of synonyms, the basic unit of EWN), the appropriate subset of these concepts are listed. Using the Inter Lingual Index that links the synsets of different languages, the set of relevant concepts can be found also for Czech lemmas. relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai, feature vectors 4b(c, ai) are computed for all candidates c E Cand(ai) and passed to the trained decision tree. The candidate classified as positive is returned as the predicted antecedent. If there a</context>
</contexts>
<marker>Mccarthy, Lehnert, 1995</marker>
<rawString>J Mccarthy and Wendy G. Lehnert. 1995. Using decision trees for coreference resolution. In In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Mikulov´a</author>
</authors>
<title>Anotace na tektogramatick´e rovinˇe Praˇzsk´eho z´avislostn ho korpusu. Anot´atorsk´a pˇr ruˇcka (t-layer annotation guidelines).</title>
<date>2005</date>
<tech>Technical Report TR-2005-28, ´UFAL</tech>
<publisher>MFF UK,</publisher>
<location>Prague, Prague.</location>
<marker>Mikulov´a, 2005</marker>
<rawString>Marie Mikulov´a et al. 2005. Anotace na tektogramatick´e rovinˇe Praˇzsk´eho z´avislostn ho korpusu. Anot´atorsk´a pˇr ruˇcka (t-layer annotation guidelines). Technical Report TR-2005-28, ´UFAL MFF UK, Prague, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Anaphora Resolution.</title>
<date>2002</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="955" citStr="Mitkov, 2002" startWordPosition="132" endWordPosition="133">hine Learning approaches to the task of pronominal anaphora resolution: a conventional classification system based on C5.0 decision trees, and a novel perceptron-based ranker. We use coreference links annotated in the Prague Dependency Treebank 2.0 for training and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far. 1 Introduction Anaphora Resolution (AR) is a well established task in Natural Language Processing (Mitkov, 2002). Classification techniques (e.g., single candidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the rankin</context>
<context position="15014" citStr="Mitkov, 2002" startWordPosition="2374" endWordPosition="2375"> all). The classification approach does not require such adaptation – it is more robust against such lack of information as it treats the whole chain as positive instances. texts. All features are listed in Table 4 in the Appendix. When designing the feature set on personal pronouns, we take into account the fact that Czech personal pronouns stand for persons, animals and things, therefore they agree with their antecedents in many attributes and functions. Further we use the knowledge from the Lappin and Leass’s algorithm (Lappin and Leass, 1994), the Mitkov’s robust, knowledge-poor approach (Mitkov, 2002), and the theory of topic-focus articulation (Kuˇcov´a et al., 2005). We want to take utmost advantage of information from the antecedent’s and anaphor’s node on all three levels as well. Distance: Numeric features capturing the distance between the anaphor and the candidate, measured by the number of sentences, clauses, tree nodes and candidates between them. Morphological agreement: Categorial features created from the values of tectogrammatical gender and number9 and from selected morphological categories from the positional tag10 of the anaphor and of the candidate. In addition, there are </context>
</contexts>
<marker>Mitkov, 2002</marker>
<rawString>Ruslan Mitkov. 2002. Anaphora Resolution. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´aclav Nˇemˇc´ık</author>
</authors>
<title>Anaphora Resolution. Master’s thesis,</title>
<date>2006</date>
<institution>Faculty of Informatics, Masaryk University.</institution>
<marker>Nˇemˇc´ık, 2006</marker>
<rawString>V´aclav Nˇemˇc´ık. 2006. Anaphora Resolution. Master’s thesis, Faculty of Informatics, Masaryk University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19771" citStr="Ng and Cardie (2002)" startWordPosition="3135" endWordPosition="3138">nonyms, the basic unit of EWN), the appropriate subset of these concepts are listed. Using the Inter Lingual Index that links the synsets of different languages, the set of relevant concepts can be found also for Czech lemmas. relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai, feature vectors 4b(c, ai) are computed for all candidates c E Cand(ai) and passed to the trained decision tree. The candidate classified as positive is returned as the predicted antecedent. If there are more candidates classified as positive, the</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In ACL ’02: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 104–111, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Supervised ranking for pronoun resolution: Some recent improvements.</title>
<date>2005</date>
<pages>1081--1086</pages>
<editor>In Manuela M. Veloso and Subbarao Kambhampati, editors, AAAI,</editor>
<publisher>AAAI Press / The MIT Press.</publisher>
<contexts>
<context position="21364" citStr="Ng (2005)" startWordPosition="3410" endWordPosition="3411">candidate extraction function Cand, we aim to rank the candidates so that the true antecedent would always be the first candidate on the list. The ranking is modeled by a linear model of the features described in Section 3.4. According to the model, the antecedent yi for an anaphoric expression ai is found as: yi = argmax -b(c, ai) · w cECand(ai) The weights w of the linear model are trained using a modification of the averaged perceptron al18Besides C5.0, we plan to use also other classifiers in the future (especially Support Vector Machine, which is often employed in AR experiments, e.g. by Ng (2005) and Yang et al. (2006)) in order to study how the classifier choice influences the AR system performance on our data and feature sets. 280 gorithm (Collins, 2002). This is averaged perceptron learning with a modified loss function adapted to the ranking scenario. The loss function is tailored to the task of correctly ranking the true antecedent, the ranking of other candidates is irrelevant. The algorithm (without averaging the parameters) is listed as Algorithm 1. Note that the training instances where yi ∈� Cand(ai) were excluded from the training. Algorithm 1: Modified perceptron algorithm</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Vincent Ng. 2005. Supervised ranking for pronoun resolution: Some recent improvements. In Manuela M. Veloso and Subbarao Kambhampati, editors, AAAI, pages 1081–1086. AAAI Press / The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008),</booktitle>
<pages>640--649</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="3396" citStr="Ng (2008)" startWordPosition="529" endWordPosition="530">erformance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based system evaluated in Ngu.y and ˇZabokrtsk´y (2007). To illustrate the real complexity of the task, we also provide performance evaluation of a baseline solution. 1Currently one can see a growing interest in unsupervised techniques, e.g. Charniak and Elsner (2009) and Ng (2008). However, we make only a very tiny step in this direction: we use a probabilistic feature based on collocation counts in large unannotated data (namely in the Czech National Corpus). Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–285, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 276 The most important result claimed in this paper is that, to the best of our knowledge, the presented ranker system outperforms all the previously published systems evaluated on the PDT data</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Vincent Ng. 2008. Unsupervised models for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP2008), pages 640–649, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giang Linh Ngu y</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<title>Rule-based approach to pronominal anaphora resolution applied on the prague dependency treebank 2.0 data.</title>
<date>2007</date>
<journal>In Branco</journal>
<pages>77--81</pages>
<marker>y, ˇZabokrtsk´y, 2007</marker>
<rawString>Giang Linh Ngu.y and Zdenˇek ˇZabokrtsk´y. 2007. Rule-based approach to pronominal anaphora resolution applied on the prague dependency treebank 2.0 data. In Branco et al. (Branco et al., 2007), pages 77–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giang Linh Ngu y</author>
</authors>
<title>Proposal of a Set of Rules for Anaphora Resolution in Czech. Master’s thesis,</title>
<date>2006</date>
<institution>Faculty of Mathematics and Physics, Charles University.</institution>
<contexts>
<context position="3101" citStr="y, 2006" startWordPosition="482" endWordPosition="483">f decision trees (Quinlan, 1993). The latter uses a simple scoring function whose optimal weight vector is estimated using perceptron learning inspired by Collins (2002). We try to provide both implementations with as similar input information as possible in order to be able to compare their performance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based system evaluated in Ngu.y and ˇZabokrtsk´y (2007). To illustrate the real complexity of the task, we also provide performance evaluation of a baseline solution. 1Currently one can see a growing interest in unsupervised techniques, e.g. Charniak and Elsner (2009) and Ng (2008). However, we make only a very tiny step in this direction: we use a probabilistic feature based on collocation counts in large unannotated data (namely in the Czech National Corpus). Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276–2</context>
</contexts>
<marker>y, 2006</marker>
<rawString>Giang Linh Ngu.y. 2006. Proposal of a Set of Rules for Anaphora Resolution in Czech. Master’s thesis, Faculty of Mathematics and Physics, Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jarmila Panevov´a</author>
</authors>
<title>Koreference gramatick´a nebo textov´a? In Etudes de linguistique romane et slave.</title>
<date>1991</date>
<publisher>Krakow.</publisher>
<marker>Panevov´a, 1991</marker>
<rawString>Jarmila Panevov´a. 1991. Koreference gramatick´a nebo textov´a? In Etudes de linguistique romane et slave. Krakow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers Inc.,</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2525" citStr="Quinlan, 1993" startWordPosition="393" endWordPosition="394">ridge (2008). In this paper we deal with supervised approaches to pronominal anaphora in Czech.1 For training and evaluation purposes, we use coreferences links annotated in the Prague Dependency Treebank, (Jan Hajiˇc, et al., 2006). We limit ourselves only to textual coreference (see Section 2) and to personal and possessive pronouns. We make use of a rich set of features available thanks to the complex annotation scenario of the treebank. We experiment with two of the above mentioned techniques for AR: a classifier and a ranker. The former is based on a top-down induction of decision trees (Quinlan, 1993). The latter uses a simple scoring function whose optimal weight vector is estimated using perceptron learning inspired by Collins (2002). We try to provide both implementations with as similar input information as possible in order to be able to compare their performance for the given task. Performance of the presented systems can be compared with several already published works, namely with a rule-based system described in Kuˇcov´a and ˇZabokrtsk´y (2005), some of the “classical” algorithms implemented in Nˇemˇcfk (2006), a system based on decision trees (Ngu.y, 2006), and a rule-based syste</context>
<context position="19544" citStr="Quinlan, 1993" startWordPosition="3097" endWordPosition="3098">termined by the context. 17The Top Ontology used in EuroWordNet (EWN) contains the (structured) set of 63 basic semantic concepts like Place, Time, Human, Group, Living, etc. For the majority of English synsets (set of synonyms, the basic unit of EWN), the appropriate subset of these concepts are listed. Using the Inter Lingual Index that links the synsets of different languages, the set of relevant concepts can be found also for Czech lemmas. relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai, feature vectors 4b(</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajiˇcov´a</author>
<author>Jarmila Panevov´a</author>
</authors>
<title>The Meaning of the Sentence in Its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<editor>D.</editor>
<publisher>Reidel Publishing Company,</publisher>
<location>Dordrecht.</location>
<marker>Sgall, Hajiˇcov´a, Panevov´a, 1986</marker>
<rawString>Petr Sgall, Eva Hajiˇcov´a, and Jarmila Panevov´a. 1986. The Meaning of the Sentence in Its Semantic and Pragmatic Aspects. D. Reidel Publishing Company, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
</authors>
<title>Generativn´ı popis jazyka a ˇcesk´a deklinace.</title>
<date>1967</date>
<location>Academia, Prague, Czech Republic.</location>
<contexts>
<context position="5229" citStr="Sgall (1967)" startWordPosition="829" endWordPosition="830">processing the data for training and evaluation purposes. The classifier-based and ranker-based systems are described in Section 4 and Section 5 respectively. Section 6 summarizes the achieved results by evaluating both approaches using the test data. Conclusions and final remarks follow in Section 7. 2 Coreference links in the Prague Dependency Treebank 2.0 The Prague Dependency Treebank 2.03 (PDT 2.0, Jan Hajiˇc, et al. (2006)) is a large collection of linguistically annotated data and documentation, based on the theoretical framework of Functional Generative Description (FGD; introduced by Sgall (1967) and later elaborated, e.g. in by Sgall et al. (1986)). The PDT 2.0 data are Czech newspaper texts selected from the Czech National Corpus4 (CNC). The PDT 2.0 has a three-level structure. On the lowest morphological level, a lemma and a positional morphological tag are added to each token. The middle analytical level represents each sentence as a surface-syntactic dependency tree. On the highest tectogrammatical level, each sentence is represented as a complex deep-syntactic depen2However, it should be noted that exact comparison is not possible here, since the tasks are slightly different for</context>
</contexts>
<marker>Sgall, 1967</marker>
<rawString>Petr Sgall. 1967. Generativn´ı popis jazyka a ˇcesk´a deklinace. Academia, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Comput. Linguist.,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1215" citStr="Soon et al. (2001)" startWordPosition="172" endWordPosition="175">aining and evaluation purposes. The perceptron system achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far. 1 Introduction Anaphora Resolution (AR) is a well established task in Natural Language Processing (Mitkov, 2002). Classification techniques (e.g., single candidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009 whi</context>
<context position="19745" citStr="Soon et al. (2001)" startWordPosition="3130" endWordPosition="3133">glish synsets (set of synonyms, the basic unit of EWN), the appropriate subset of these concepts are listed. Using the Inter Lingual Index that links the synsets of different languages, the set of relevant concepts can be found also for Czech lemmas. relation between the candidate’s lemma and the semantic concepts. 4 Classifier-based system Our classification approach uses C5.0, a successor of C4.5 (Quinlan, 1993), which is probably the most widely used program for inducing decision trees. Decision trees are used in many AR systems such as Aone and Bennett (1995), Mccarthy and Lehnert (1995), Soon et al. (2001), and Ng and Cardie (2002).18 Our classifier-based system takes as input a set of feature vectors as described in Section 3.4 and their classifications (1 – true antecedent, 0 – nonantecedent) and produces a decision tree that is further used for classifying new pairs of candidate and anaphor. The classifier antecedent selection algorithm works as follows. For each anaphor ai, feature vectors 4b(c, ai) are computed for all candidates c E Cand(ai) and passed to the trained decision tree. The candidate classified as positive is returned as the predicted antecedent. If there are more candidates c</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Comput. Linguist., 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdenˇek ˇZabokrtsk´y</author>
<author>Jan Pt´aˇcek</author>
<author>Petr Pajas</author>
</authors>
<title>TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation, ACL.</booktitle>
<marker>ˇZabokrtsk´y, Pt´aˇcek, Pajas, 2008</marker>
<rawString>Zdenˇek ˇZabokrtsk´y, Jan Pt´aˇcek, and Petr Pajas. 2008. TectoMT: Highly Modular MT System with Tectogrammatics Used as Transfer Layer. In Proceedings of the 3rd Workshop on Statistical Machine Translation, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<date>2005</date>
<booktitle>BBN Pronoun Coreference and Entity Type Corpus. CDROM, Linguistic Data Consortium, LDC Catalog No.: LDC2005T33,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="8407" citStr="Weischedel and Brunstein, 2005" startWordPosition="1300" endWordPosition="1303">ks (arrows) is presented in Figure 1. For the sake of simplicity, only three node attributes are displayed below the nodes: tectogrammatical lemma, functor, and semantic part of speech (tectogrammatical nodes themselves are complex data structures and around twenty attributes might be stored with them). Tectogrammatical lemma is a canonical word form or an artificial value of a newly created node 5In terms of the number of coreference links, PDT 2.0 is one of the largest existing manually annotated resources. Another comparably large resource is BBN Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), which contains a stand-off annotation of coreference links in the Penn Treebank texts. 277 Type/Count train dtest etest Personal pron. 12,913 1,945 2,030 Relative pron. 6,957 948 1,034 Under-control pron. 6,598 874 907 Reflexive pron. 3,381 452 571 Demonstrative pron. 2,582 332 344 Reciprocity pron. 882 110 122 Other 320 35 42 Total 34,983 4,909 5,282 Table 1: Distribution of the different anaphor types in the PDT 2.0. on the tectogrammatical level. E.g. the (artificial) tectogrammatical lemma #PersPron stands for personal (and possessive) pronouns, be they expressed on the surface (i.e., pr</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN Pronoun Coreference and Entity Type Corpus. CDROM, Linguistic Data Consortium, LDC Catalog No.: LDC2005T33, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Coreference resolution using competition learning approach.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>176--183</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1265" citStr="Yang et al. (2003)" startWordPosition="182" endWordPosition="185">tem achieves f-score 79.43% on recognizing coreference of personal and possessive pronouns, which clearly outperforms the classifier and which is the best result reported on this data set so far. 1 Introduction Anaphora Resolution (AR) is a well established task in Natural Language Processing (Mitkov, 2002). Classification techniques (e.g., single candidate model aimed at answering: “Is there a coreference link between the anaphor and this antecedent candidate, or not?”) are very often used for the task, e.g. in Mccarthy and Lehnert (1995) and Soon et al. (2001). However, as argued already in Yang et al. (2003), better results are achieved when the candidates can compete in a pairwise fashion. It can be explained by the fact that in this approach (called twin-candidate model), more information is available for the decision making. If we proceed further along this direction, we come to the ranking approach described in Denis and Baldridge (2007), in which the entire candidate set is considered at once and The work on this project was supported by the grants MSM 0021620838, GAAV ˇCR 1ET101120503 and 1ET201120505, MˇSMT ˇCR LC536, and GAUK 4383/2009 which leads to further significant shift in performan</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2003</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2003. Coreference resolution using competition learning approach. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 176– 183, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Kernel-based pronoun resolution with structured syntactic knowledge.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL2006),</booktitle>
<pages>41--48</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="21387" citStr="Yang et al. (2006)" startWordPosition="3413" endWordPosition="3416">action function Cand, we aim to rank the candidates so that the true antecedent would always be the first candidate on the list. The ranking is modeled by a linear model of the features described in Section 3.4. According to the model, the antecedent yi for an anaphoric expression ai is found as: yi = argmax -b(c, ai) · w cECand(ai) The weights w of the linear model are trained using a modification of the averaged perceptron al18Besides C5.0, we plan to use also other classifiers in the future (especially Support Vector Machine, which is often employed in AR experiments, e.g. by Ng (2005) and Yang et al. (2006)) in order to study how the classifier choice influences the AR system performance on our data and feature sets. 280 gorithm (Collins, 2002). This is averaged perceptron learning with a modified loss function adapted to the ranking scenario. The loss function is tailored to the task of correctly ranking the true antecedent, the ranking of other candidates is irrelevant. The algorithm (without averaging the parameters) is listed as Algorithm 1. Note that the training instances where yi ∈� Cand(ai) were excluded from the training. Algorithm 1: Modified perceptron algorithm for ranking. 4&gt; is the</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006. Kernel-based pronoun resolution with structured syntactic knowledge. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLINGACL2006), pages 41–48, Sydney, Australia, July 17–21.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>