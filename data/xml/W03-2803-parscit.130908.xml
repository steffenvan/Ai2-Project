<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000623">
<title confidence="0.990719">
Some statistical methods for evaluating information extraction systems
</title>
<author confidence="0.993667">
Will Lowe Gary King
</author>
<affiliation confidence="0.971211666666667">
Computer Science Department Center for Basic Research
Bath University in the Social Sciences
wlowe@latte.harvard.edu Harvard University
</affiliation>
<email confidence="0.996119">
king@harvard.edu
</email>
<sectionHeader confidence="0.995612" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999761541666667">
We present new statistical methods for
evaluating information extraction sys-
tems. The methods were developed
to evaluate a system used by polit-
ical scientists to extract event infor-
mation from news leads about inter-
national politics. The nature of this
data presents two problems for evalu-
ators: 1) the frequency distribution of
event types in international event data
is strongly skewed, so a random sample
of newsleads will typically fail to con-
tain any low frequency events. 2) Man-
ual information extraction necessary to
create evaluation sets is costly, and most
effort is wasted coding high frequency
categories .
We present an evaluation scheme that
overcomes these problems with consid-
erably less manual effort than traditional
methods, and also allows us to interpret
an information extraction system as an
estimator (in the statistical sense) and to
estimate its bias.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903428571429">
This paper introduces a statistical approach we
developed to evaluate information extraction sys-
tems used to study international relations. Event
extraction is a form of categorization, but the
highly skewed frequency profile of international
event categories in real data generates severe prob-
lems for evaluators. We discuss these problems in
section 3, show how to circumvent using a novel
sampling scheme in section 4, and briefly describe
our application. Finally we discuss the advantages
and disadvantages of the methods, and their rela-
tions to standard evaluation procedure. We start
with a brief review of information extraction in in-
ternational relations.
</bodyText>
<sectionHeader confidence="0.997156" genericHeader="method">
2 Event Analysis in International
Relations
</sectionHeader>
<bodyText confidence="0.9997777">
Researchers in quantitative international relations
have been performing manual information ex-
traction since the mid-1970s (McClelland, 1978;
Azar, 1982). The information extracted has re-
mained fairly simple; a researcher fills a ’who did
what to whom’ template, usually from historical
documents, a list of countries and international
organizations to describe the actors, and a more
or less articulated ontology of international events
to describe what occurred (McClelland, 1978).
In the early 1990s automated information extrac-
tion tools mostly replaced manual coding efforts
(Schrodt et al., 1994). Information extraction sys-
tems in international relations perform a similar
task to those competing in early Message Under-
standing Competitions (Sundheim, 1991, 1992).
With machine extracted events data it is now pos-
sible to do near real-time conflict forecasting with
data based on newswire leads, and detailed politi-
cal analysis afterwards.
</bodyText>
<sectionHeader confidence="0.992881" genericHeader="method">
3 Event Category Distributions
</sectionHeader>
<bodyText confidence="0.999953166666667">
We wanted to evaluate an information extraction
system from Virtual Research Associates1. This
system bundles extraction and visualization soft-
ware with a custom event ontology containing, at
last count, about 200 categories of international
event.
We found two problems with the nature of inter-
national events data. First, the frequency distribu-
tion over the system’s ontology, or indeed several
other ontologies we considered, is heavily skewed.
A handful of mostly diplomatic event types pre-
dominate, and the frequency of other event types
falls of very sharply: we ran the system over all
the newsleads in Reuters’ coverage of the Bosnia
conflict, and of the approximately 45,000 events
it extracted, 10,605 were in the category of ’neu-
tral comment’, 4 of ’apology’ and 35 of ’threat of
force’. Thus the relative frequencies of event cat-
egories in this data can be 2,500 to 1.
Also, as these figures suggest, the more inter-
esting and politically relevant events tend to be of
low frequency. This problem is quite general in
categorization systems with reasonably articulated
category systems, and not specific to international
relations. But any dataset with these properties
causes an immediate problem for evaluation.
Ideally we would choose a random subset of
leads whose events are known with certainty (be-
cause we have coded them manually beforehand),
run the system over them, and then compute var-
ious sample statistics such as precision and re-
call2. However, a small randomly chosen subset
is very unlikely contain instances of most interest-
ing events, and so the system’s performance will
not be evaluated on them. Given the possible fre-
quency ratios above, the size of subset necessary
to ensure reasonable coverage of lower frequency
event categories is enormous. Put more concretely,
to construct a test set of news leads the evaluator
will on average have to code around 2,500 com-
ments to reach a single apology and about 300
comments to find a single threat of force.
</bodyText>
<footnote confidence="0.973797">
1http://www.vranet.com
2This paper only evaluates extraction performance on
event types, though there would seem to be no reason why
a similar approach would not work for actors etc.
</footnote>
<subsectionHeader confidence="0.998453">
3.1 Standard Evalution Methods
</subsectionHeader>
<bodyText confidence="0.999967526315789">
The standard evaluation methods developed over
the course of the Message Understanding Compe-
titions consist mainly in sample statistics to com-
pute over the evaluation materials e.g. precision
and recall, but do not give any guidance for choos-
ing the materials themselves (Cowie and Lehnert,
1996; Grishman, 1997). This is just done by hand
by the judges. Perhaps because the selection ques-
tion is neglected, it is seldom clear what larger
population the test materials are from (save that it
is the same one as the training examples), and as a
consequence it is unclear what the implications for
generalization are when a system obtains a partic-
ular set of scores for precision and recall (Lehnert
and Sundheim, 1991).
Since this literature did not help us generate
a suitable evaluation sample, we approached the
problem from scratch, and developed a statistical
framework specific to our needs.
</bodyText>
<sectionHeader confidence="0.984739" genericHeader="evaluation">
4 Method
</sectionHeader>
<bodyText confidence="0.999611">
One reasonable-sounding but wrong way to ad-
dress the problem of creating a test set without
having to code tens of thousands of irrelevant sto-
ries is the following:
</bodyText>
<listItem confidence="0.955476833333333">
1. Use the extraction system itself to perform an
initial coding,
2. Take a sample of the output that covers all the
event types in reasonable quantities,
3. Examine each coding to see whether the sys-
tem assigned the correct event code.
</listItem>
<bodyText confidence="0.999799818181818">
This looks like it can guarantee a good sample of
low frequency events at much lower cost to the
manual coder; we can just pick a fixed number
of events from each category and evaluate them.
However, this method exhibits selection bias. To
see this, let M and T be variables indicating which
event category the Machine (that is, the informa-
tion extraction system) codes an event into, and
the True category to which the event actually be-
longs. Statistically, the quantity of interest to us is
the probability that the machine is correct:
</bodyText>
<equation confidence="0.9963">
P(M=iIT=i) (1)
</equation>
<bodyText confidence="0.999991130434783">
This is the probability that the machine classifies
an event into category i given that the true event
coding is indeed i. A full characterization of the
success of the machine requires knowing P(M = i |
T = i) for i = 0,... ,J, which includes all J event
categories and where i = 0 denotes the situation
where the machine is unable to classify an event
into any category. In short, the quantity of interest
is the full probability density P(M  |T).
In statistical terms, this distribution is a likeli-
hood function for the information extraction sys-
tem. This observation allows us to treat the system
like any other statistical estimator and offers the
interesting possibility of analyzing generalization
via its sampling properties, e.g. its bias, variance,
mean squared error, or risk.
Unfortunately, the problem with the reasonable-
sounding approach described above is that it does
not in fact allow us to estimate P(M  |T) because
it is implicitly conditioning on M, not T. In par-
ticular, the proportion of events that are actually in
category i among those the machine put in cate-
gory i gives us instead an estimate of
</bodyText>
<equation confidence="0.997698">
P(T  |M) (2)
</equation>
<bodyText confidence="0.99989175">
which is not the quantity of interest. (2) is the
probability of the truth being in some event cate-
gory rather than the machine’s response whereas
in fact the true event category is fixed and it is
the machine’s response that is uncertain3. Worse,
P(T  |M) is a systematically biased estimate of
P(M  |T) because these two quantities are related
by Bayes theorem:
</bodyText>
<equation confidence="0.999505">
P(M  |T) = P(M,T) = P(T  |M)P(M), (3)
P(T) P(T)
</equation>
<bodyText confidence="0.999151">
and the only circumstances under which they
would be equal is when P(M) is uniform. But
the figures in section 3 suggest that P(M) is highly
skewed.
However this last observation suggests a better
method for unbiased estimation of (1).
</bodyText>
<listItem confidence="0.792467444444445">
1. Estimate P(T  |M) as described above
3This is due to changes in the journalist’s choice of vocab-
ulary and syntactic construction that are uncorrelated with the
identity of the event being described.
2. Compute P(M) by running the system over
the entire data set and normalizing the fre-
quency histogram of event categories
3. Estimate P(M  |T) by correcting P(T  |M)
with P(M) using Bayes theorem
</listItem>
<bodyText confidence="0.999923264705882">
Our implementation of this scheme was to first
run the system over 45,000 leads about the Bosnia
conflict, and normalize the frequency histogram of
events extracted to create P(M). Then, randomly
choose 5 leads assigned to each event category,
and manually determine which event type the in-
stantiate. Then normalize to estimate P(T  |M).
And finally, use (3) to create P(M  |T). We chose
four times as many uncategorized leads as from
each true category in addition. A larger sample
here is advisable to see what sort of categories the
system misses. These sample sizes are fixed, but
it may also be possible to use active learning tech-
niques to tune them (as in e.g. Argamon-Engelson
and Dagan, 1999) for even more efficient sam-
pling.
The advantage of this roundabout route to (1) is
that it requires many fewer events to be manually
coded. We ran the system over 45,000 leads but
only manually coded a handful of events for each
category. This guaranteed us even coverage of the
lowest frequency event categories whilst not bias-
ing the end result – for an ontology with about 200
categories this is a substantial decrease in evalua-
tor effort.
This method works by making use of the ex-
traction system itself to produce one important
marginal: P(M). If we assume that the aim is to
evaluate the system on the Bosnia conflict, P(M)
is not estimated, but is rather an exact population
marginal4. Then we can guarantee that our esti-
mate of P(M  |T) is unbiased because the method
for estimating P(T  |M) is clearly unbiased, and
P(M) adds no error.
</bodyText>
<subsectionHeader confidence="0.996533">
4.1 Summary Measures
</subsectionHeader>
<bodyText confidence="0.999654">
P(M  |T) allows the computation of a number
of useful summary measures5. For example, we
</bodyText>
<footnote confidence="0.8307538">
4We might consider the Bosnian conflict to be a sample
point from the larger population of all wars, but that popula-
tion – if it exists at all – is certainly difficult to quantify.
5Detailed discussion of several summary measures for the
system we evaluated can be found in King and Lowe (2002).
</footnote>
<bodyText confidence="0.997951727272727">
can easily compute P(M,T) from quantities al-
ready available, so EJ P(M = i,T = i) is the pro-
portion of time the system extracts the correct
category. Alternatively, if it is more important
to extract some categories than others, then var-
ious weighted measures can be constructed e.g.
EJ P(M = i  |T = i)wi where ws are non-negative
and sum to 1, representing the relative importance
of extracting each category. Some more graphi-
cal methods of evaluation using P(M  |T) are pre-
sented below.
</bodyText>
<subsectionHeader confidence="0.870241">
4.2 Estimator Properties
</subsectionHeader>
<bodyText confidence="0.999931083333333">
Given a likelihood function for the extraction sys-
tem we can investigate its properties as an esti-
mator. It is particularly useful to know the bias
of an estimator, defined in this case as the dif-
ference between the expected category response
from the system when the true event category is
i, and i itself, where the expectation is taken of re-
peated information extraction tasks that instantiate
the same event categories. We do not examine the
corresponding variance here, and a more complete
evaluation might also address the question of con-
sistency.
</bodyText>
<subsubsectionHeader confidence="0.631979">
4.2.1 Conflict and Cooperation
</subsubsectionHeader>
<bodyText confidence="0.999915333333333">
The machines response and the true category
is best seen as a set of multinomial probabilities
(with a unit vector with the value 1 at the index
of the system’s extracted category or the true cate-
gory respectively. Estimator properties are cum-
bersome to represent in this format, so here we
map the system’s response to a single real value
corresponding to the level of conflict or coopera-
tion of the event category. This re-representation
is usual in international relations and allows stan-
dard econometric time series methods to be ap-
plied (Schrodt and Gerner, 1994; Goldstein and
Freeman, 1990; Goldstein and Pevehouse, 1997).
For our purposes it also allows the straightfor-
ward graphical presentation of the main ideas. We
define the level of conflict or cooperation level
of an event category i as Gi, a real number be-
tween -10 (most conflictual) to 10 (most coopera-
tive) (see Goldstein, 1992, for the full mapping).
For example, according to this scheme, when i
denotes the event category ‘extending economic
</bodyText>
<figure confidence="0.997407615384615">
10
8
6
4
2
0
2
−4
−6
−8
10
−10 −8 −6 −4 −2 0 2 4 6 8 10
G�
</figure>
<figureCaption confidence="0.8096645">
Figure 1: Expected (gi) versus true (Gi) conflict-
cooperation level for each event category.
</figureCaption>
<bodyText confidence="0.999940777777778">
aid’, Gi = 7.4, ‘policy endorsement’ maps to 3.6,
‘halt negotiations’ maps to -3.8, and a ‘military en-
gagement’ maps to -10, the maximally conflictual
event. The mapping allows univariate, and polit-
ically relevant comparison between the true con-
flict level and that of the event categories the sys-
tem extracts.
The expected system response when the true
category has conflict/cooperation level Gi is:
</bodyText>
<equation confidence="0.9368045">
J
gi = jGjP(M = j  |T = i,M = 0) (4)
</equation>
<bodyText confidence="0.921071">
where
</bodyText>
<equation confidence="0.9999575">
P(M  |T)1(M = 0)
P(M = j  |T = i,M = 0) = P(M = 0  |T) .
</equation>
<bodyText confidence="0.996939125">
and 1(M = 0) is an indicator function equaling 1
if M = 0 and 0 otherwise.
A plot of Gi against gi for each event category is
shown in Figure 1. An unbiased estimator would
show expected values on the main diagonal. Esti-
mator bias for event category i is simply gi − Gi.
Estimator variance is simply the spread around the
diagonal.
</bodyText>
<subsectionHeader confidence="0.99279">
4.3 Comparison
</subsectionHeader>
<bodyText confidence="0.9998440625">
We also compared the system’s performance to
3 undergraduate coders (U1-3) working on the
same data set. To examine undergraduate perfor-
mance requires first P(U,T), from which we can
get P(U  |T). However, we cannot simply count
the proportion of times each undergraduate assigns
a lead to category i when it is in fact in category i
because this ignores the fact that we have sampled
the leads themselves using the system, and must
therefore condition on M. On the other hand we
do have access to the relevant conditional distri-
bution P(U,T  |M = i). This is the distribution of
undergraduate and true categories, conditioned on
the fact the the system assigns an event to cate-
gory i. The desired P(U,T) is a weighted average
of these distributions:
</bodyText>
<figure confidence="0.97834385106383">
P(U,T) = Y P(U,T  |M = i)P(M = i).
i
Machine
G.
U2
10
5
0
Gi−gi
−5
−10
15
−10 −5
−
0 5 10
10
5
0
Gi−gi
−5
−10
15
−10 −5
−
0 5 10
U1
G.
U3
G.
10
5
Gi−gi
0
−5
−10
−10 −5
0 5 10
10
5
0
Gi−gi
−5
−10
−15
−10 −5
0 5 10
Gi−gi
</figure>
<bodyText confidence="0.95643971875">
P(U  |T) is then obtained by marginalization6.
Clearly these calculations can also be used to com-
pare other systems with the same ontology using
the same materials.
Summary statistics similar to those described
above can be easily computed (King and Lowe,
2002). Here we provide graphical results: Fig-
ure 2 plots the bias of the system and that of the un-
dergraduates over the category set (with smoothed
estimates superimposed). In the figure, the bias
Gi − gi is plotted against Gi, so deflections from
the horizontal are systematic bias. In almost all
cases we find that more conflictual (negative val-
ued) categories are mistaken for more cooperative
ones, with some suggestion of a similar effect at
the cooperative end too. Of most interest is the ba-
sic similarity in performance between undergrad-
uates and the information extraction system.
It would be helpful if the bias that appears in
these plots were systematically related to the ex-
pected system response. If this was the case, in
future use we could simply adjust the system’s
response up or down by some coefficient deter-
mined in the evaluation process and remove the
bias. However, figure 3 shows that there is no
systematic relation between the expected reponses
and the level of bias, so no such coefficient canbe
computed. This is a rather pessimistic result for
this system, suggesting a level of bias that can-
not be straightforwardly removed. On the other
6We would normally expect to use P(U  |T,U =� 0), but
the undergraduates never failed to assign categories.
</bodyText>
<figure confidence="0.712685">
G.
</figure>
<figureCaption confidence="0.860007666666667">
Figure 2: System (M) versus undergraduate coder
(U1-3) bias. Connected lines are generated by
smoothing Gi − gi.
</figureCaption>
<figure confidence="0.994878">
Machine U1
10
5
0
0
−5
−5
−
0 5 10
g;
U2 U3
0 5 10
g;
</figure>
<figureCaption confidence="0.608978">
Figure 3: Bias plotted against expected system and
undergraduate response. Deviations from the hori-
zontal suggest the possibility of a post-output cor-
rection to correct for bias in subsequent applica-
tion.
</figureCaption>
<figure confidence="0.998128688888889">
10
5
0
Gi−gi
−5
−10
−10
10
5
15
−10 −5
0 5 10
g;
Gi−gi
−10
−10 −5
g;
Gi−gi
−10
−
−5
10
15
−10 −5
5
0
0 5 10
−15
−10 −5
Machine U1
1
0.8
0.6
0.4
0.2
0 5 10
Gi
U2 U3
1
0.8
0.6
0.4
0.2
0 5 10
G.
</figure>
<figureCaption confidence="0.7672645">
Figure 4: The probability that the system, or un-
dergraduate fails to assign an event to a category,
plotted against the level of conflict/cooperation of
that category.
</figureCaption>
<bodyText confidence="0.9997145">
hand, one of the advantages of the methods pre-
sented here is that this bias is now estimated, and,
since bias estimates are available on a category-
by-category basis, redesigning effort can be di-
rected in a way that maximizes generalization per-
formance.
Finally, figure 4 plots the probability that the
machine failed to assign an event category, P(M =
0 1 T = i) (denoted p(null) in the figure), as a func-
tion of that category’s conflict/cooperation value,
Gi. Our interest in Gi reflects the use this data is
typically put to, since we are most concerned with
errors that make the world look systematically
more (or less) cooperative than it really is. But
we might equally have plotted P(M = 0 1 T = i)
against i itself, or any other property of events that
might be suspected to generate difficult to catego-
rize event descriptions.
Like the previous figures, plotting P(M = 0
T = i) against other quantities is a useful diagnos-
tic, indicating where future work should best be
applied. In this case there appears to be no sys-
tematic relationship between the true level of con-
flict/cooperation and the probability that either the
system or the undergraduates will fail to assign the
event to a category.
</bodyText>
<sectionHeader confidence="0.997701" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999986578947368">
We have presented a set of statistical methods
for evaluating an information extraction system
without unreasonable manual labour when the dis-
tribution of categories to be extracted is heav-
ily skewed. The scheme uses a form of biased
sampling and subsequent correction to estimate
a probability distribution of system responses for
each true category in the data. This distribution
costitutes a likelihood function for the system. We
then show how functions of this distribution can
be used for evaluation, and estimate the system’s
statistical bias.
The two main ideas: using estimates of P(M
T) as the basis for evaluation, and using a non-
standard sampling scheme for the estimation, are
separate. Emphasis on using P(M I T) comes from
standard statistical theory, and if correct, suggests
how evaluation in information extraction might be
integrated in to that body of theory. When a sam-
ple of leads is randomly chosen and can be ex-
pected to be reasonably representative, then the
sampling machinery described above, the compu-
tation of P(M), and the application of Bayes the-
orem will not be necessary. But when the distri-
bution of categories to be extracted is so highly
skewed then our method is the only one that will
make it feasible to evaluate a system on all of its
categories in an unbiased way.
The principle difference between these and
standard evaluation methods is in our explicitly
statistical framework, and our consideration of
how to sample in a representative way, and meth-
ods to get around cases where we cannot. The ex-
act relationship to precision, recall etc. is the topic
of current research. In the meantime we hope that
the methods presented might advance understand-
ing of effective evaluation methods in computa-
tional linguistics.
</bodyText>
<sectionHeader confidence="0.998328" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976655166666667">
We thank Doug Bond, Craig Jenkins, Dylan
Balch-Lindsay, Phil Schrodt, and two anonymous
reviewers for helpful comments, and the National
Science Foundation (IIS-9874747), the National
Institutes of Aging (P01 AG17625-01), the Weath-
erhead Center for International Relations, and the
</bodyText>
<figure confidence="0.989297333333333">
Gi
p(null)
0.8
0.6
0.4
0.2
0
−10 −5
1
0 5 10
p(null)
0
−10 −5
G.
p(null)
0.8
0.6
0.4
0.2
0
−10 −5
1
0 5 10
p(null)
0
−10 −5
World Health Organization for research support.
</figure>
<sectionHeader confidence="0.906158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999311236363636">
Argamon-Engelson, S. and Dagan, I. (1999).
Committee-based sample selection for proba-
bilistic classifiers. Journal of Artificial Intelli-
gence Research, 11:335–360.
Azar, E. E. (1982). Codebook of the Conflict and
Peace Databank. Center for International De-
velopment, University of Maryland.
Cowie, J. and Lehnert, W. (1996). Informa-
tion extraction. Communications of the ACM,
39(1):80–91.
Goldstein, J. S. (1992). A conflict-cooperation
scale for WEIS events data. Journal of Conflict
Resolution, 36(2).
Goldstein, J. S. and Freeman, J. R. (1990). Three-
Way Street: Strategic Reciprocity in World Pol-
itics. Chicago University Press.
Goldstein, J. S. and Pevehouse, J. C. (1997).
Reciprocity, bullying and international conflict:
Time-series analysis of the Bosnia conflict.
American Political Science Review, 91(3):515–
529.
Grishman, R. (1997). Information extraction:
Techniques and challenges. In Pazienza, M. T.,
editor, Information Extraction: A Multidisci-
plinary Approach to an Emerging Information
Technology, volume 1299 of Lecture Notes in
Artificial Intelligence, chapter 2, pages 10–27.
Springer Verlag.
King, G. and Lowe, W. (2002). An automated in-
formation extraction tool for international con-
flict data with performance as good as hu-
man coders: A rare events evaluation design.
http://gking.harvard.edu/infoex.pdf.
Lehnert, W. and Sundheim, B. (1991). A perfor-
mance evaluation of text-analysis technologies.
AI Magazine, pages 81–95.
McClelland, C. (1978). World Event / Interac-
tion Survey (WEIS) 1966-1978. Inter-University
Consortium for Political and Social Research,
University of Southern California.
Schrodt, P. A., Davis, S. G., and Weddle, J. L.
(1994). Political science: KEDS — a program
for the machine coding of event data. Social
Science Computer Review, 12.
Schrodt, P. A. and Gerner, D. J. (1994). Validity
assessment of a machine-coded event data set
for the Middle East, 1982-92. American Journal
of Political Science, 38(3).
Sundheim, B. (1992). Overview of the fourth mes-
sage understanding evaluation and conference.
In Proceedings of the Fourth Message Under-
standing Conference, pages 3–22.
Sundheim, S., editor (1991). Proceedings of the
Third Message Understanding Conference, San
Mateo, CA. Morgan Kaufmann.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.249486">
<title confidence="0.999667">Some statistical methods for evaluating information extraction systems</title>
<author confidence="0.999404">Will Gary</author>
<affiliation confidence="0.944503">Computer Science Center for Basic</affiliation>
<author confidence="0.541206">Bath in the Social</author>
<affiliation confidence="0.489864">wlowe@latte.harvard.edu Harvard</affiliation>
<email confidence="0.999268">king@harvard.edu</email>
<abstract confidence="0.99958212">We present new statistical methods for evaluating information extraction systems. The methods were developed to evaluate a system used by political scientists to extract event information from news leads about international politics. The nature of this data presents two problems for evaluators: 1) the frequency distribution of event types in international event data is strongly skewed, so a random sample of newsleads will typically fail to contain any low frequency events. 2) Manual information extraction necessary to create evaluation sets is costly, and most effort is wasted coding high frequency categories . We present an evaluation scheme that overcomes these problems with considerably less manual effort than traditional methods, and also allows us to interpret an information extraction system as an estimator (in the statistical sense) and to estimate its bias.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon-Engelson</author>
<author>I Dagan</author>
</authors>
<title>Committee-based sample selection for probabilistic classifiers.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--335</pages>
<contexts>
<context position="9765" citStr="Argamon-Engelson and Dagan, 1999" startWordPosition="1580" endWordPosition="1583"> 45,000 leads about the Bosnia conflict, and normalize the frequency histogram of events extracted to create P(M). Then, randomly choose 5 leads assigned to each event category, and manually determine which event type the instantiate. Then normalize to estimate P(T |M). And finally, use (3) to create P(M |T). We chose four times as many uncategorized leads as from each true category in addition. A larger sample here is advisable to see what sort of categories the system misses. These sample sizes are fixed, but it may also be possible to use active learning techniques to tune them (as in e.g. Argamon-Engelson and Dagan, 1999) for even more efficient sampling. The advantage of this roundabout route to (1) is that it requires many fewer events to be manually coded. We ran the system over 45,000 leads but only manually coded a handful of events for each category. This guaranteed us even coverage of the lowest frequency event categories whilst not biasing the end result – for an ontology with about 200 categories this is a substantial decrease in evaluator effort. This method works by making use of the extraction system itself to produce one important marginal: P(M). If we assume that the aim is to evaluate the system</context>
</contexts>
<marker>Argamon-Engelson, Dagan, 1999</marker>
<rawString>Argamon-Engelson, S. and Dagan, I. (1999). Committee-based sample selection for probabilistic classifiers. Journal of Artificial Intelligence Research, 11:335–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E E Azar</author>
</authors>
<title>Codebook of the Conflict and Peace Databank.</title>
<date>1982</date>
<institution>Center for International Development, University of Maryland.</institution>
<contexts>
<context position="2008" citStr="Azar, 1982" startWordPosition="294" endWordPosition="295">tegories in real data generates severe problems for evaluators. We discuss these problems in section 3, show how to circumvent using a novel sampling scheme in section 4, and briefly describe our application. Finally we discuss the advantages and disadvantages of the methods, and their relations to standard evaluation procedure. We start with a brief review of information extraction in international relations. 2 Event Analysis in International Relations Researchers in quantitative international relations have been performing manual information extraction since the mid-1970s (McClelland, 1978; Azar, 1982). The information extracted has remained fairly simple; a researcher fills a ’who did what to whom’ template, usually from historical documents, a list of countries and international organizations to describe the actors, and a more or less articulated ontology of international events to describe what occurred (McClelland, 1978). In the early 1990s automated information extraction tools mostly replaced manual coding efforts (Schrodt et al., 1994). Information extraction systems in international relations perform a similar task to those competing in early Message Understanding Competitions (Sund</context>
</contexts>
<marker>Azar, 1982</marker>
<rawString>Azar, E. E. (1982). Codebook of the Conflict and Peace Databank. Center for International Development, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>W Lehnert</author>
</authors>
<title>Information extraction.</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="5327" citStr="Cowie and Lehnert, 1996" startWordPosition="815" endWordPosition="818"> to code around 2,500 comments to reach a single apology and about 300 comments to find a single threat of force. 1http://www.vranet.com 2This paper only evaluates extraction performance on event types, though there would seem to be no reason why a similar approach would not work for actors etc. 3.1 Standard Evalution Methods The standard evaluation methods developed over the course of the Message Understanding Competitions consist mainly in sample statistics to compute over the evaluation materials e.g. precision and recall, but do not give any guidance for choosing the materials themselves (Cowie and Lehnert, 1996; Grishman, 1997). This is just done by hand by the judges. Perhaps because the selection question is neglected, it is seldom clear what larger population the test materials are from (save that it is the same one as the training examples), and as a consequence it is unclear what the implications for generalization are when a system obtains a particular set of scores for precision and recall (Lehnert and Sundheim, 1991). Since this literature did not help us generate a suitable evaluation sample, we approached the problem from scratch, and developed a statistical framework specific to our needs</context>
</contexts>
<marker>Cowie, Lehnert, 1996</marker>
<rawString>Cowie, J. and Lehnert, W. (1996). Information extraction. Communications of the ACM, 39(1):80–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Goldstein</author>
</authors>
<title>A conflict-cooperation scale for WEIS events data.</title>
<date>1992</date>
<journal>Journal of Conflict Resolution,</journal>
<volume>36</volume>
<issue>2</issue>
<contexts>
<context position="13008" citStr="Goldstein, 1992" startWordPosition="2140" endWordPosition="2141"> here we map the system’s response to a single real value corresponding to the level of conflict or cooperation of the event category. This re-representation is usual in international relations and allows standard econometric time series methods to be applied (Schrodt and Gerner, 1994; Goldstein and Freeman, 1990; Goldstein and Pevehouse, 1997). For our purposes it also allows the straightforward graphical presentation of the main ideas. We define the level of conflict or cooperation level of an event category i as Gi, a real number between -10 (most conflictual) to 10 (most cooperative) (see Goldstein, 1992, for the full mapping). For example, according to this scheme, when i denotes the event category ‘extending economic 10 8 6 4 2 0 2 −4 −6 −8 10 −10 −8 −6 −4 −2 0 2 4 6 8 10 G� Figure 1: Expected (gi) versus true (Gi) conflictcooperation level for each event category. aid’, Gi = 7.4, ‘policy endorsement’ maps to 3.6, ‘halt negotiations’ maps to -3.8, and a ‘military engagement’ maps to -10, the maximally conflictual event. The mapping allows univariate, and politically relevant comparison between the true conflict level and that of the event categories the system extracts. The expected system </context>
</contexts>
<marker>Goldstein, 1992</marker>
<rawString>Goldstein, J. S. (1992). A conflict-cooperation scale for WEIS events data. Journal of Conflict Resolution, 36(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Goldstein</author>
<author>J R Freeman</author>
</authors>
<title>ThreeWay Street: Strategic Reciprocity in World Politics.</title>
<date>1990</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="12707" citStr="Goldstein and Freeman, 1990" startWordPosition="2087" endWordPosition="2090">4.2.1 Conflict and Cooperation The machines response and the true category is best seen as a set of multinomial probabilities (with a unit vector with the value 1 at the index of the system’s extracted category or the true category respectively. Estimator properties are cumbersome to represent in this format, so here we map the system’s response to a single real value corresponding to the level of conflict or cooperation of the event category. This re-representation is usual in international relations and allows standard econometric time series methods to be applied (Schrodt and Gerner, 1994; Goldstein and Freeman, 1990; Goldstein and Pevehouse, 1997). For our purposes it also allows the straightforward graphical presentation of the main ideas. We define the level of conflict or cooperation level of an event category i as Gi, a real number between -10 (most conflictual) to 10 (most cooperative) (see Goldstein, 1992, for the full mapping). For example, according to this scheme, when i denotes the event category ‘extending economic 10 8 6 4 2 0 2 −4 −6 −8 10 −10 −8 −6 −4 −2 0 2 4 6 8 10 G� Figure 1: Expected (gi) versus true (Gi) conflictcooperation level for each event category. aid’, Gi = 7.4, ‘policy endors</context>
</contexts>
<marker>Goldstein, Freeman, 1990</marker>
<rawString>Goldstein, J. S. and Freeman, J. R. (1990). ThreeWay Street: Strategic Reciprocity in World Politics. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Goldstein</author>
<author>J C Pevehouse</author>
</authors>
<title>Reciprocity, bullying and international conflict: Time-series analysis of the Bosnia conflict.</title>
<date>1997</date>
<journal>American Political Science Review,</journal>
<volume>91</volume>
<issue>3</issue>
<pages>529</pages>
<contexts>
<context position="12739" citStr="Goldstein and Pevehouse, 1997" startWordPosition="2091" endWordPosition="2094">n The machines response and the true category is best seen as a set of multinomial probabilities (with a unit vector with the value 1 at the index of the system’s extracted category or the true category respectively. Estimator properties are cumbersome to represent in this format, so here we map the system’s response to a single real value corresponding to the level of conflict or cooperation of the event category. This re-representation is usual in international relations and allows standard econometric time series methods to be applied (Schrodt and Gerner, 1994; Goldstein and Freeman, 1990; Goldstein and Pevehouse, 1997). For our purposes it also allows the straightforward graphical presentation of the main ideas. We define the level of conflict or cooperation level of an event category i as Gi, a real number between -10 (most conflictual) to 10 (most cooperative) (see Goldstein, 1992, for the full mapping). For example, according to this scheme, when i denotes the event category ‘extending economic 10 8 6 4 2 0 2 −4 −6 −8 10 −10 −8 −6 −4 −2 0 2 4 6 8 10 G� Figure 1: Expected (gi) versus true (Gi) conflictcooperation level for each event category. aid’, Gi = 7.4, ‘policy endorsement’ maps to 3.6, ‘halt negoti</context>
</contexts>
<marker>Goldstein, Pevehouse, 1997</marker>
<rawString>Goldstein, J. S. and Pevehouse, J. C. (1997). Reciprocity, bullying and international conflict: Time-series analysis of the Bosnia conflict. American Political Science Review, 91(3):515– 529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
</authors>
<title>Information extraction: Techniques and challenges.</title>
<date>1997</date>
<booktitle>Information Extraction: A Multidisciplinary Approach to an Emerging Information Technology,</booktitle>
<volume>1299</volume>
<pages>10--27</pages>
<editor>In Pazienza, M. T., editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="5344" citStr="Grishman, 1997" startWordPosition="819" endWordPosition="820">ments to reach a single apology and about 300 comments to find a single threat of force. 1http://www.vranet.com 2This paper only evaluates extraction performance on event types, though there would seem to be no reason why a similar approach would not work for actors etc. 3.1 Standard Evalution Methods The standard evaluation methods developed over the course of the Message Understanding Competitions consist mainly in sample statistics to compute over the evaluation materials e.g. precision and recall, but do not give any guidance for choosing the materials themselves (Cowie and Lehnert, 1996; Grishman, 1997). This is just done by hand by the judges. Perhaps because the selection question is neglected, it is seldom clear what larger population the test materials are from (save that it is the same one as the training examples), and as a consequence it is unclear what the implications for generalization are when a system obtains a particular set of scores for precision and recall (Lehnert and Sundheim, 1991). Since this literature did not help us generate a suitable evaluation sample, we approached the problem from scratch, and developed a statistical framework specific to our needs. 4 Method One re</context>
</contexts>
<marker>Grishman, 1997</marker>
<rawString>Grishman, R. (1997). Information extraction: Techniques and challenges. In Pazienza, M. T., editor, Information Extraction: A Multidisciplinary Approach to an Emerging Information Technology, volume 1299 of Lecture Notes in Artificial Intelligence, chapter 2, pages 10–27. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G King</author>
<author>W Lowe</author>
</authors>
<title>An automated information extraction tool for international conflict data with performance as good as human coders: A rare events evaluation design.</title>
<date>2002</date>
<note>http://gking.harvard.edu/infoex.pdf.</note>
<contexts>
<context position="11011" citStr="King and Lowe (2002)" startWordPosition="1801" endWordPosition="1804">(M) is not estimated, but is rather an exact population marginal4. Then we can guarantee that our estimate of P(M |T) is unbiased because the method for estimating P(T |M) is clearly unbiased, and P(M) adds no error. 4.1 Summary Measures P(M |T) allows the computation of a number of useful summary measures5. For example, we 4We might consider the Bosnian conflict to be a sample point from the larger population of all wars, but that population – if it exists at all – is certainly difficult to quantify. 5Detailed discussion of several summary measures for the system we evaluated can be found in King and Lowe (2002). can easily compute P(M,T) from quantities already available, so EJ P(M = i,T = i) is the proportion of time the system extracts the correct category. Alternatively, if it is more important to extract some categories than others, then various weighted measures can be constructed e.g. EJ P(M = i |T = i)wi where ws are non-negative and sum to 1, representing the relative importance of extracting each category. Some more graphical methods of evaluation using P(M |T) are presented below. 4.2 Estimator Properties Given a likelihood function for the extraction system we can investigate its properti</context>
<context position="15354" citStr="King and Lowe, 2002" startWordPosition="2592" endWordPosition="2595">ries, conditioned on the fact the the system assigns an event to category i. The desired P(U,T) is a weighted average of these distributions: P(U,T) = Y P(U,T |M = i)P(M = i). i Machine G. U2 10 5 0 Gi−gi −5 −10 15 −10 −5 − 0 5 10 10 5 0 Gi−gi −5 −10 15 −10 −5 − 0 5 10 U1 G. U3 G. 10 5 Gi−gi 0 −5 −10 −10 −5 0 5 10 10 5 0 Gi−gi −5 −10 −15 −10 −5 0 5 10 Gi−gi P(U |T) is then obtained by marginalization6. Clearly these calculations can also be used to compare other systems with the same ontology using the same materials. Summary statistics similar to those described above can be easily computed (King and Lowe, 2002). Here we provide graphical results: Figure 2 plots the bias of the system and that of the undergraduates over the category set (with smoothed estimates superimposed). In the figure, the bias Gi − gi is plotted against Gi, so deflections from the horizontal are systematic bias. In almost all cases we find that more conflictual (negative valued) categories are mistaken for more cooperative ones, with some suggestion of a similar effect at the cooperative end too. Of most interest is the basic similarity in performance between undergraduates and the information extraction system. It would be hel</context>
</contexts>
<marker>King, Lowe, 2002</marker>
<rawString>King, G. and Lowe, W. (2002). An automated information extraction tool for international conflict data with performance as good as human coders: A rare events evaluation design. http://gking.harvard.edu/infoex.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>B Sundheim</author>
</authors>
<title>A performance evaluation of text-analysis technologies.</title>
<date>1991</date>
<journal>AI Magazine,</journal>
<pages>81--95</pages>
<contexts>
<context position="5749" citStr="Lehnert and Sundheim, 1991" startWordPosition="888" endWordPosition="891">petitions consist mainly in sample statistics to compute over the evaluation materials e.g. precision and recall, but do not give any guidance for choosing the materials themselves (Cowie and Lehnert, 1996; Grishman, 1997). This is just done by hand by the judges. Perhaps because the selection question is neglected, it is seldom clear what larger population the test materials are from (save that it is the same one as the training examples), and as a consequence it is unclear what the implications for generalization are when a system obtains a particular set of scores for precision and recall (Lehnert and Sundheim, 1991). Since this literature did not help us generate a suitable evaluation sample, we approached the problem from scratch, and developed a statistical framework specific to our needs. 4 Method One reasonable-sounding but wrong way to address the problem of creating a test set without having to code tens of thousands of irrelevant stories is the following: 1. Use the extraction system itself to perform an initial coding, 2. Take a sample of the output that covers all the event types in reasonable quantities, 3. Examine each coding to see whether the system assigned the correct event code. This look</context>
</contexts>
<marker>Lehnert, Sundheim, 1991</marker>
<rawString>Lehnert, W. and Sundheim, B. (1991). A performance evaluation of text-analysis technologies. AI Magazine, pages 81–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C McClelland</author>
</authors>
<title>World Event / Interaction Survey (WEIS)</title>
<date>1978</date>
<institution>Inter-University Consortium for Political and Social Research, University of Southern California.</institution>
<contexts>
<context position="1995" citStr="McClelland, 1978" startWordPosition="292" endWordPosition="293">rnational event categories in real data generates severe problems for evaluators. We discuss these problems in section 3, show how to circumvent using a novel sampling scheme in section 4, and briefly describe our application. Finally we discuss the advantages and disadvantages of the methods, and their relations to standard evaluation procedure. We start with a brief review of information extraction in international relations. 2 Event Analysis in International Relations Researchers in quantitative international relations have been performing manual information extraction since the mid-1970s (McClelland, 1978; Azar, 1982). The information extracted has remained fairly simple; a researcher fills a ’who did what to whom’ template, usually from historical documents, a list of countries and international organizations to describe the actors, and a more or less articulated ontology of international events to describe what occurred (McClelland, 1978). In the early 1990s automated information extraction tools mostly replaced manual coding efforts (Schrodt et al., 1994). Information extraction systems in international relations perform a similar task to those competing in early Message Understanding Compe</context>
</contexts>
<marker>McClelland, 1978</marker>
<rawString>McClelland, C. (1978). World Event / Interaction Survey (WEIS) 1966-1978. Inter-University Consortium for Political and Social Research, University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Schrodt</author>
<author>S G Davis</author>
<author>J L Weddle</author>
</authors>
<title>Political science: KEDS — a program for the machine coding of event data.</title>
<date>1994</date>
<journal>Social Science Computer Review,</journal>
<volume>12</volume>
<contexts>
<context position="2457" citStr="Schrodt et al., 1994" startWordPosition="359" endWordPosition="362">national Relations Researchers in quantitative international relations have been performing manual information extraction since the mid-1970s (McClelland, 1978; Azar, 1982). The information extracted has remained fairly simple; a researcher fills a ’who did what to whom’ template, usually from historical documents, a list of countries and international organizations to describe the actors, and a more or less articulated ontology of international events to describe what occurred (McClelland, 1978). In the early 1990s automated information extraction tools mostly replaced manual coding efforts (Schrodt et al., 1994). Information extraction systems in international relations perform a similar task to those competing in early Message Understanding Competitions (Sundheim, 1991, 1992). With machine extracted events data it is now possible to do near real-time conflict forecasting with data based on newswire leads, and detailed political analysis afterwards. 3 Event Category Distributions We wanted to evaluate an information extraction system from Virtual Research Associates1. This system bundles extraction and visualization software with a custom event ontology containing, at last count, about 200 categories</context>
</contexts>
<marker>Schrodt, Davis, Weddle, 1994</marker>
<rawString>Schrodt, P. A., Davis, S. G., and Weddle, J. L. (1994). Political science: KEDS — a program for the machine coding of event data. Social Science Computer Review, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Schrodt</author>
<author>D J Gerner</author>
</authors>
<title>Validity assessment of a machine-coded event data set for the Middle East,</title>
<date>1994</date>
<journal>American Journal of Political Science,</journal>
<volume>38</volume>
<issue>3</issue>
<contexts>
<context position="12678" citStr="Schrodt and Gerner, 1994" startWordPosition="2083" endWordPosition="2086"> question of consistency. 4.2.1 Conflict and Cooperation The machines response and the true category is best seen as a set of multinomial probabilities (with a unit vector with the value 1 at the index of the system’s extracted category or the true category respectively. Estimator properties are cumbersome to represent in this format, so here we map the system’s response to a single real value corresponding to the level of conflict or cooperation of the event category. This re-representation is usual in international relations and allows standard econometric time series methods to be applied (Schrodt and Gerner, 1994; Goldstein and Freeman, 1990; Goldstein and Pevehouse, 1997). For our purposes it also allows the straightforward graphical presentation of the main ideas. We define the level of conflict or cooperation level of an event category i as Gi, a real number between -10 (most conflictual) to 10 (most cooperative) (see Goldstein, 1992, for the full mapping). For example, according to this scheme, when i denotes the event category ‘extending economic 10 8 6 4 2 0 2 −4 −6 −8 10 −10 −8 −6 −4 −2 0 2 4 6 8 10 G� Figure 1: Expected (gi) versus true (Gi) conflictcooperation level for each event category. a</context>
</contexts>
<marker>Schrodt, Gerner, 1994</marker>
<rawString>Schrodt, P. A. and Gerner, D. J. (1994). Validity assessment of a machine-coded event data set for the Middle East, 1982-92. American Journal of Political Science, 38(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sundheim</author>
</authors>
<title>Overview of the fourth message understanding evaluation and conference.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth Message Understanding Conference,</booktitle>
<pages>3--22</pages>
<marker>Sundheim, 1992</marker>
<rawString>Sundheim, B. (1992). Overview of the fourth message understanding evaluation and conference. In Proceedings of the Fourth Message Understanding Conference, pages 3–22.</rawString>
</citation>
<citation valid="true">
<date>1991</date>
<booktitle>Proceedings of the Third Message Understanding Conference,</booktitle>
<editor>Sundheim, S., editor</editor>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<marker>1991</marker>
<rawString>Sundheim, S., editor (1991). Proceedings of the Third Message Understanding Conference, San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>