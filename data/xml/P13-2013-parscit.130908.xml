<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056335">
<title confidence="0.9949045">
Aggregated Word Pair Features for Implicit Discourse Relation
Disambiguation
</title>
<author confidence="0.994629">
Or Biran
</author>
<affiliation confidence="0.9992885">
Columbia University
Department of Computer Science
</affiliation>
<email confidence="0.998467">
orb@cs.columbia.edu
</email>
<sectionHeader confidence="0.997386" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998875">
We present a reformulation of the word
pair features typically used for the task
of disambiguating implicit relations in the
Penn Discourse Treebank. Our word pair
features achieve significantly higher per-
formance than the previous formulation
when evaluated without additional fea-
tures. In addition, we present results
for a full system using additional features
which achieves close to state of the art per-
formance without resorting to gold syntac-
tic parses or to context outside the relation.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986432">
Discourse relations such as contrast and causal-
ity are part of what makes a text coherent. Be-
ing able to automatically identify these relations
is important for many NLP tasks such as gener-
ation, question answering and textual entailment.
In some cases, discourse relations contain an ex-
plicit marker such as but or because which makes
it easy to identify the relation. Prior work (Pitler
and Nenkova, 2009) showed that where explicit
markers exist, the class of the relation can be dis-
ambiguated with f-scores higher than 90%.
Predicting the class of implicit discourse rela-
tions, however, is much more difficult. Without an
explicit marker to rely on, work on this task ini-
tially focused on using lexical cues in the form
of word pairs mined from large corpora where
they appear around an explicit marker (Marcu and
Echihabi, 2002). The intuition is that these pairs
will tend to represent semantic relationships which
are related to the discourse marker (for example,
word pairs often appearing around but may tend
to be antonyms). While this approach showed
some success and has been used extensively in
later work, it has been pointed out by multiple
authors that many of the most useful word pairs
</bodyText>
<author confidence="0.824171">
Kathleen McKeown
</author>
<affiliation confidence="0.9968935">
Columbia University
Department of Computer Science
</affiliation>
<email confidence="0.991008">
kathy@cs.columbia.edu
</email>
<bodyText confidence="0.99907868">
are pairs of very common functional words, which
contradicts the original intuition, and it is hard to
explain why these are useful.
In this work we focus on the task of identi-
fying and disambiguating implicit discourse rela-
tions which have no explicit marker. In particular,
we present a reformulation of the word pair fea-
tures that have most often been used for this task
in the past, replacing the sparse lexical features
with dense aggregated score features. This is the
main contribution of our paper. We show that our
formulation outperforms the original one while re-
quiring less features, and that using a stop list of
functional words does not significantly affect per-
formance, suggesting that these features indeed
represent semantically related content word pairs.
In addition, we present a system which com-
bines these word pairs with additional features to
achieve near state of the art performance without
the use of syntactic parse features and of context
outside the arguments of the relation. Previous
work has attributed much of the achieved perfor-
mance to these features, which are easy to get in
the experimental setting but would be less reliable
or unavailable in other applications.1
</bodyText>
<sectionHeader confidence="0.999886" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998829">
This line of research began with (Marcu and Echi-
habi, 2002), who used a small number of unam-
biguous explicit markers and patterns involving
them, such as [Arg1, but Arg2] to collect sets of
word pairs from a large corpus using the cross-
product of the words in Arg1 and Arg2. The au-
thors created a feature out of each pair and built a
naive bayes model directly from the unannotated
corpus, updating the priors and posteriors using
maximum likelihood. While they demonstrated
</bodyText>
<footnote confidence="0.861533">
1Reliable syntactic parses are not always available in do-
mains other than newswire, and context (preceding relations,
especially explicit relations) is not always available in some
applications such as generation and question answering.
</footnote>
<page confidence="0.988288">
69
</page>
<bodyText confidence="0.972492705882353">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 69–73,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
some success, their experiments were run on data
that is unnatural in two ways. First, it is balanced.
Second, it is constructed with the same unsuper-
vised method they use to extract the word pairs -
by assuming that the patterns correspond to a par-
ticular relation and collecting the arguments from
an unannotated corpus. Even if the assumption is
correct, these arguments are really taken from ex-
plicit relations with their markers removed, which
as others have pointed out (Blair-Goldensohn et
al., 2007; Pitler et al., 2009) may not look like true
implicit relations.
More recently, implicit relation prediction has
been evaluated on annotated implicit relations
from the Penn Discourse Treebank (Prasad et al.,
2008). PDTB uses hierarchical relation types
which abstract over other theories of discourse
such as RST (Mann and Thompson, 1987) and
SDRT (Asher and Lascarides, 2003). It contains
40, 600 annotated relations from the WSJ corpus.
Each relation has two arguments, Arg1 and Arg2,
and the annotators decide whether it is explicit or
implicit.
The first to evaluate directly on PDTB in a re-
alistic setting were Pitler et al. (2009). They used
word pairs as well as additional features to train
four binary classifiers, each corresponding to one
of the high-level PDTB relation classes. Although
other features proved to be useful, word pairs were
still the major contributor to most of these clas-
sifiers. In fact, their best system for comparison
included only the word pair features, and for all
other classes other than expansion the word pair
features alone achieved an f-score within 2 points
of the best system. Interestingly, they found that
training the word pair features on PDTB itself was
more useful than training them on an external cor-
pus like Marcu and Echihabi (2002), although in
some cases they resort to information gain in the
external corpus for filtering the word pairs.
Zhou et al. (2010) used a similar method and
added features that explicitly try to predict the
implicit marker in the relation, increasing perfor-
mance. Most recently to the best of our knowl-
edge, Park and Cardie (2012) achieved the highest
performance by optimizing the feature set. An-
other work evaluating on PDTB is (Lin et al.,
2009), who are unique in evaluating on the more
fine-grained second-level relation classes.
</bodyText>
<sectionHeader confidence="0.996142" genericHeader="method">
3 Word Pairs
</sectionHeader>
<subsectionHeader confidence="0.999287">
3.1 The Problem: Sparsity
</subsectionHeader>
<bodyText confidence="0.999984481481481">
While Marcu and Echihabi (2002)’s approach of
training a classifier from an unannotated corpus
provides a relatively large amount of training data,
this data does not consist of true implicit relations.
However, the approach taken by Pitler et al. (2009)
and repeated in more recent work (training directly
on PDTB) is problematic as well: when training a
model with so many sparse features on a dataset
the size of PDTB (there are 22,141 non-explicit
relations overall), it is likely that many important
word pairs will not be seen in training.
In fact, even the larger corpus of Marcu and
Echihabi (2002) may not be quite large enough
to solve the sparsity issue, given that the num-
ber of word pairs is quadratic in the vocabulary.
Blair-Goldensohn et al. (2007) report that using
even a very small stop list (25 words) significantly
reduces performance, which is counter-intuitive.
They attribute this finding to the sparsity of the
feature space. An analysis in (Pitler et al., 2009)
also shows that the top word pairs (ranked by
information gain) all contain common functional
words, and are not at all the semantically-related
content words that were imagined. In the case
of some reportedly useful word pairs (the-and; in-
the; the-of...) it is hard to explain how they might
affect performance except through overfitting.
</bodyText>
<subsectionHeader confidence="0.999622">
3.2 The Solution: Aggregation
</subsectionHeader>
<bodyText confidence="0.99999235">
Representing each word pair as a single feature has
the advantage of allowing the weights for each pair
to be learned directly from the data. While pow-
erful, this approach requires large amounts of data
to be effective.
Another possible approach is to aggregate some
of the pairs together and learn weights from the
data only for the aggregated sets of words. For this
approach to be effective, the pairs we choose to
group together should have similar meaning with
regard to predicting the relation.
Biran and Rambow (2011) is to our knowledge
the only other work utilizing a similar approach.
They used aggregated word pair set features to
predict whether or not a sentence is argumentative.
Their method is to group together word pairs that
have been collected around the same explicit dis-
course marker: for every discourse marker such
as therefore or however, they have a single fea-
ture whose value depends only on the word pairs
</bodyText>
<page confidence="0.990147">
70
</page>
<bodyText confidence="0.999981857142857">
collected around that marker. This is reasonable
given the intuition that the marker pattern is unam-
biguous and points at a particular relation. Using
one feature per marker can be seen as analogous
(yet complementary) to Zhou et al. (2010)’s ap-
proach of trying to predict the implicit connective
by giving a score to each marker using a language
model.
This work uses binary features which only in-
dicate the appearance of one or more of the pairs.
The original frequencies of the word pairs are not
used anywhere. A more powerful approach is to
use an informed function to weight the word pairs
used inside each feature.
</bodyText>
<subsectionHeader confidence="0.99725">
3.3 Our Approach
</subsectionHeader>
<bodyText confidence="0.999980393939394">
Our approach is similar in that we choose to ag-
gregate word pairs that were collected around the
same explicit marker. We first assembled a list of
all 102 discourse markers used in PDTB, in both
explicit and implicit relations.2
Next, we extract word pairs for each marker
from the Gigaword corpus by taking the cross
product of words that appear in a sentence around
that marker. This is a simpler approach than us-
ing patterns - for example, the marker because can
appear in two patterns: [Arg1 because Arg2] and
[because Arg1, Arg2], and we only use the first.
We leave the task of listing the possible patterns
for each of the 102 markers to future work because
of the significant manual effort required. Mean-
while, we rely on the fact that we use a very large
corpus and hope that the simple pattern [Arg1
marker Arg2] is enough to make our features use-
ful. There are, of course, markers for which this
pattern does not normally apply, such as by com-
parison or on one hand. We expect these features
to be down-weighted by the final classifier, as ex-
plained at the end of this section. When collect-
ing the pairs, we stem the words and discard pairs
which appear only once around the marker.
We can think of each discourse marker as hav-
ing a corresponding unordered “document”, where
each word pair is a term with an associated fre-
quency. We want to create a feature for each
marker such that for each data instance (that is,
for each potential relation in the PDTB data) the
value for the feature is the relevance of the marker
document to the data instance.
</bodyText>
<footnote confidence="0.8018855">
2in implicit relations, there is no marker in the text but the
implicit marker is provided by the human annotators
</footnote>
<bodyText confidence="0.994787571428572">
Each data instance in PDTB consists of two ar-
guments, and can therefore also be represented
as a set of word pairs extracted from the cross-
product of the two arguments. To represent the rel-
evance of the instance to each marker, we set the
value of the marker feature to the cosine similarity
of the data instance and the marker’s “document”,
where each word pair is a dimension.
While the terms (i.e. word pairs) of the
data instance are weighted by simple occurence
count, we weight the terms in each marker’s
document with tf-idf, where tf is defined in
one of two ways: normalized term frequency
(
</bodyText>
<equation confidence="0.98919975">
count(t)
max{count(s,d):s∈d}) and pointwise mutual infor-
mation (log count(t) ), where w1 and w2
count(.1) ∗count(.2 )
</equation>
<bodyText confidence="0.999431916666667">
are the member words of the pair. Idf is calculated
normally given that the set of all documents is de-
fined as the 102 marker documents.
We then train a binary classifier (logistic regres-
sion) using these 102 features for each of the four
high-level relations in PDTB: comparison, con-
tingency, expansion and temporal. To make sure
our results are comparable to previous work, we
treat EntRel relations as instances of expansion
and use sections 2-20 for training and sections 21-
22 for testing. We use a ten fold stratified cross-
validation of the training set for development. Ex-
plicit relations are excluded from all data sets.
As mentioned earlier, there are markers that do
not fit the simple pattern we use. In particular,
some markers always or often appear as the first
term of a sentence. For these, we expect the list of
word pairs to be empty or almost empty, since in
most sentences there are no words on the left (and
recall that we discard pairs that appear only once).
Since the features created for these markers will
be uninformative, we expect them to be weighted
down by the classifier and have no significant ef-
fect on prediction.
</bodyText>
<sectionHeader confidence="0.96688" genericHeader="method">
4 Evaluation of Word Pairs
</sectionHeader>
<bodyText confidence="0.9998925">
For our main evaluation, we evaluate the perfor-
mance of word pair features when used with no
additional features. Results are shown in Table 1.
Our word pair features outperform the previous
formulation (represented by the results reported by
(Pitler et al., 2009), but used by virtually all previ-
ous work on this task). For most relation classes,
tf is significantly better than pmi. 3
</bodyText>
<footnote confidence="0.9195955">
3Significance was verified for our own results in all exper-
iments shown in this paper with a standard t-test
</footnote>
<page confidence="0.996902">
71
</page>
<table confidence="0.9942026">
Comparison Contingency Expansion Temporal
Pitler et al., 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98)
tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09)
pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53)
tf-idf, with stop list 23.77 44.33 65.33 16.98
</table>
<tableCaption confidence="0.999015">
Table 1: Main evaluation. F-measure (accuracy) for various implementations of the word pairs features
</tableCaption>
<table confidence="0.999717833333333">
Comparison Contingency Expansion Temporal
Best System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35)
features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9
Pitler et al., 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49)
Zhou et al., 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48)
Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32)
</table>
<tableCaption confidence="0.991226">
Table 2: Secondary evaluation. F-measure (accuracy) for the best systems. tf and pmi refer to the word
pair features used (by tf implementation), and the numbers refer to the indeces of Table 3
</tableCaption>
<table confidence="0.9982473">
Comp. Cont. Exp. Temp.
1 WordNet 20.07 34.07 52.96 11.58
2 Verb Class 14.24 24.84 49.6 10.04
3 MPN 23.84 38.58 49.97 13.16
4 Modality 17.49 28.92 13.84 10.72
5 Polarity 16.46 26.36 65.15 11.58
6 Affect 18.62 31.59 59.8 13.37
7 Similarity 20.68 34.5 43.16 12.1
8 Negation 8.28 22.47 75.87 11.1
9 Length 20.75 31.28 65.72 10.19
</table>
<tableCaption confidence="0.996866">
Table 3: F-measure for each feature category
</tableCaption>
<bodyText confidence="0.999973">
We also show results using a stop list of 50 com-
mon functional words. The stop list has only a
small effect on performance except in the tempo-
ral class. This may be because of functional words
like was and will which have a temporal effect.
</bodyText>
<sectionHeader confidence="0.98963" genericHeader="method">
5 Other Features
</sectionHeader>
<bodyText confidence="0.999408777777778">
For our secondary evaluation, we include addi-
tional features to complement the word pairs. Pre-
vious work has relied on features based on the gold
parse trees of the Penn Treebank (which overlaps
with PDTB) and on contextual information from
relations preceding the one being disambiguated.
We intentionally limit ourselves to features that do
not require either so that our system can be readily
used on arbitrary argument pairs.
WordNet Features: We define four features
based on WordNet (Fellbaum, 1998) - Synonyms,
Antonyms, Hypernyms and Hyponyms. The values
are the counts of word pairs in the cross-product of
the words in the arguments that have the particular
relation (synonymy, antonymy etc) between them.
Verb Class: This is the count of pairs of verbs
from Arg1 and Arg2 that share the same class, de-
fined as the highest level Levin verb class (Levin,
1993) from the LCS database (Dorr, 2001).
Money, Percentages and Numbers (MPN): The
counts of currency symbols/abbreviations, per-
centage signs or cues (“percent”, “BPS”...) and
numbers in each argument.
Modality: Presence or absence of each English
modal in each argument.
Polarity: Based on MPQA (Wilson et al., 2005).
We include the counts of positive and negative
words according to the MPQA subjectivity lexicon
for both arguments. Unlike Pitler et al. (2009), we
do not use neutral polarity features. We also do not
explicitly group negation with polarity (although
we do have separate negation features).
Affect: Based on the Dictionary of Affect in Lan-
guage (Whissell, 1989). Each word in the DAL
gets a score for three dimensions - pleasantness
(pleasant - unpleasant), activation (passive - ac-
tive) and imagery (hard to imagine - easy to imag-
ine). We use the average score for each dimension
in each argument as a feature.
Content Similarity: We use the cosine similarity
and word overlap of the arguments as features.
Negation: Presence or absence of negation terms
in each of the arguments.
Length: The ratio between the lengths (counts of
words) of the arguments.
</bodyText>
<sectionHeader confidence="0.987076" genericHeader="evaluation">
6 Evaluation of Additional Features
</sectionHeader>
<bodyText confidence="0.9998362">
For our secondary evaluation, we present results
for each feature category on its own in Table 3 and
for our best system for each of the relation classes
in Table 2. We show results for the best systems
from (Pitler et al., 2009), (Zhou et al., 2010) and
</bodyText>
<page confidence="0.993169">
72
</page>
<note confidence="0.632114">
(Park and Cardie, 2012) for comparison. and Documentation. University Of Maryland Col-
lege Park.
</note>
<sectionHeader confidence="0.994991" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999856823529412">
We presented an aggregated approach to word pair
features and showed that it outperforms the previ-
ous formulation for all relation types but contin-
gency. This is our main contribution. With this
approach, using a stop list does not have a major
effect on results for most relation classes, which
suggests most of the word pairs affecting perfor-
mance are content word pairs which may truly be
semantically related to the discourse structure.
In addition, we introduced the new and useful
WordNet, Affect, Length and Negation feature cat-
egories. Our final system outperformed the best
system from Pitler et al. (2009), who used mostly
similar features, for comparison and temporal and
is competitive with the most recent state of the
art systems for contingency and expansion with-
out using any syntactic or context features.
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998974384615385">
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99973905">
Nicholas Asher and Alex Lascarides. 2003. Logics of
Conversation. Studies in Natural Language Process-
ing Series. Cambridge University Press.
Or Biran and Owen Rambow. 2011. Identifying justifi-
cations in written dialog by classifying text as argu-
mentative. International Journal of Semantic Com-
puting, 5(4):363–381, December.
Sasha Blair-Goldensohn, Kathleen McKeown, and
Owen Rambow. 2007. Building and refin-
ing rhetorical-semantic relation models. In HLT-
NAACL, pages 428–435. The Association for Com-
putational Linguistics.
Bonnie J. Dorr. 2001. LCS Verb Database, Online
Software Database of Lexical Conceptual Structures
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University Of
Chicago Press.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 343–351.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse re-
lations. In ACL, pages 368–375. ACL.
Joonsuk Park and Claire Cardie. 2012. Improving im-
plicit discourse relation recognition through feature
set optimization. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 108–112.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text.
In ACL/IJCNLP (Short Papers), pages 13–16. The
Association for Computer Linguistics.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In ACL/IJCNLP, pages 683–691. The
Association for Computer Linguistics.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
In Proceedings of LREC.
Cynthia M. Whissell. 1989. The dictionary of affect in
language.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347–354.
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian
Su, and Chew Lim Tan. 2010. Predicting discourse
connectives for implicit discourse relation recogni-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
</reference>
<page confidence="0.999298">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.781920">
<title confidence="0.9969635">Aggregated Word Pair Features for Implicit Discourse Disambiguation</title>
<author confidence="0.831286">Or</author>
<affiliation confidence="0.9493155">Columbia Department of Computer</affiliation>
<email confidence="0.998653">orb@cs.columbia.edu</email>
<abstract confidence="0.998322615384615">We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation. Studies in Natural Language Processing Series.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4975" citStr="Asher and Lascarides, 2003" startWordPosition="787" endWordPosition="790">relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al. (2009). They used word pairs as well as additional features to train four binary classifiers, each corresponding to one of the high-level PDTB relation classes. Although other features proved to be useful, word pairs were still the major contributor to most of these classifiers. In fact, their best system for comparison included only the word p</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Studies in Natural Language Processing Series. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Owen Rambow</author>
</authors>
<title>Identifying justifications in written dialog by classifying text as argumentative.</title>
<date>2011</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="8340" citStr="Biran and Rambow (2011)" startWordPosition="1346" endWordPosition="1349">xplain how they might affect performance except through overfitting. 3.2 The Solution: Aggregation Representing each word pair as a single feature has the advantage of allowing the weights for each pair to be learned directly from the data. While powerful, this approach requires large amounts of data to be effective. Another possible approach is to aggregate some of the pairs together and learn weights from the data only for the aggregated sets of words. For this approach to be effective, the pairs we choose to group together should have similar meaning with regard to predicting the relation. Biran and Rambow (2011) is to our knowledge the only other work utilizing a similar approach. They used aggregated word pair set features to predict whether or not a sentence is argumentative. Their method is to group together word pairs that have been collected around the same explicit discourse marker: for every discourse marker such as therefore or however, they have a single feature whose value depends only on the word pairs 70 collected around that marker. This is reasonable given the intuition that the marker pattern is unambiguous and points at a particular relation. Using one feature per marker can be seen a</context>
</contexts>
<marker>Biran, Rambow, 2011</marker>
<rawString>Or Biran and Owen Rambow. 2011. Identifying justifications in written dialog by classifying text as argumentative. International Journal of Semantic Computing, 5(4):363–381, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathleen McKeown</author>
<author>Owen Rambow</author>
</authors>
<title>Building and refining rhetorical-semantic relation models.</title>
<date>2007</date>
<booktitle>In HLTNAACL,</booktitle>
<pages>428--435</pages>
<contexts>
<context position="4597" citStr="Blair-Goldensohn et al., 2007" startWordPosition="730" endWordPosition="733">Computational Linguistics, pages 69–73, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics some success, their experiments were run on data that is unnatural in two ways. First, it is balanced. Second, it is constructed with the same unsupervised method they use to extract the word pairs - by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a reali</context>
<context position="7222" citStr="Blair-Goldensohn et al. (2007)" startWordPosition="1164" endWordPosition="1167">raining data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22,141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. In fact, even the larger corpus of Marcu and Echihabi (2002) may not be quite large enough to solve the sparsity issue, given that the number of word pairs is quadratic in the vocabulary. Blair-Goldensohn et al. (2007) report that using even a very small stop list (25 words) significantly reduces performance, which is counter-intuitive. They attribute this finding to the sparsity of the feature space. An analysis in (Pitler et al., 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. In the case of some reportedly useful word pairs (the-and; inthe; the-of...) it is hard to explain how they might affect performance except through overfitting. 3.2 The Solution: Aggregation Repres</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Rambow, 2007</marker>
<rawString>Sasha Blair-Goldensohn, Kathleen McKeown, and Owen Rambow. 2007. Building and refining rhetorical-semantic relation models. In HLTNAACL, pages 428–435. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie J Dorr</author>
</authors>
<title>WordNet An Electronic Lexical Database.</title>
<date>2001</date>
<booktitle>LCS Verb Database, Online Software Database of Lexical Conceptual Structures Christiane Fellbaum,</booktitle>
<publisher>The MIT Press.</publisher>
<location>editor.</location>
<contexts>
<context position="15986" citStr="Dorr, 2001" startWordPosition="2665" endWordPosition="2666">intentionally limit ourselves to features that do not require either so that our system can be readily used on arbitrary argument pairs. WordNet Features: We define four features based on WordNet (Fellbaum, 1998) - Synonyms, Antonyms, Hypernyms and Hyponyms. The values are the counts of word pairs in the cross-product of the words in the arguments that have the particular relation (synonymy, antonymy etc) between them. Verb Class: This is the count of pairs of verbs from Arg1 and Arg2 that share the same class, defined as the highest level Levin verb class (Levin, 1993) from the LCS database (Dorr, 2001). Money, Percentages and Numbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation features). Affect: Based on the Dictiona</context>
</contexts>
<marker>Dorr, 2001</marker>
<rawString>Bonnie J. Dorr. 2001. LCS Verb Database, Online Software Database of Lexical Conceptual Structures Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University Of Chicago Press.</publisher>
<contexts>
<context position="15951" citStr="Levin, 1993" startWordPosition="2659" endWordPosition="2660">ing the one being disambiguated. We intentionally limit ourselves to features that do not require either so that our system can be readily used on arbitrary argument pairs. WordNet Features: We define four features based on WordNet (Fellbaum, 1998) - Synonyms, Antonyms, Hypernyms and Hyponyms. The values are the counts of word pairs in the cross-product of the words in the arguments that have the particular relation (synonymy, antonymy etc) between them. Verb Class: This is the count of pairs of verbs from Arg1 and Arg2 that share the same class, defined as the highest level Levin verb class (Levin, 1993) from the LCS database (Dorr, 2001). Money, Percentages and Numbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation featu</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University Of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>343--351</pages>
<contexts>
<context position="6332" citStr="Lin et al., 2009" startWordPosition="1016" endWordPosition="1019">stem. Interestingly, they found that training the word pair features on PDTB itself was more useful than training them on an external corpus like Marcu and Echihabi (2002), although in some cases they resort to information gain in the external corpus for filtering the word pairs. Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 3 Word Pairs 3.1 The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22,141 non-explicit relations overall), it</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 343–351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/RS-87-190, ISI.</tech>
<contexts>
<context position="4937" citStr="Mann and Thompson, 1987" startWordPosition="781" endWordPosition="784">atterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al. (2009). They used word pairs as well as additional features to train four binary classifiers, each corresponding to one of the high-level PDTB relation classes. Although other features proved to be useful, word pairs were still the major contributor to most of these classifiers. In fact, their best system f</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical Structure Theory: A theory of text organization. Technical Report ISI/RS-87-190, ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>368--375</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1510" citStr="Marcu and Echihabi, 2002" startWordPosition="234" endWordPosition="237">wering and textual entailment. In some cases, discourse relations contain an explicit marker such as but or because which makes it easy to identify the relation. Prior work (Pitler and Nenkova, 2009) showed that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult. Without an explicit marker to rely on, work on this task initially focused on using lexical cues in the form of word pairs mined from large corpora where they appear around an explicit marker (Marcu and Echihabi, 2002). The intuition is that these pairs will tend to represent semantic relationships which are related to the discourse marker (for example, word pairs often appearing around but may tend to be antonyms). While this approach showed some success and has been used extensively in later work, it has been pointed out by multiple authors that many of the most useful word pairs Kathleen McKeown Columbia University Department of Computer Science kathy@cs.columbia.edu are pairs of very common functional words, which contradicts the original intuition, and it is hard to explain why these are useful. In thi</context>
<context position="3250" citStr="Marcu and Echihabi, 2002" startWordPosition="515" endWordPosition="519">gnificantly affect performance, suggesting that these features indeed represent semantically related content word pairs. In addition, we present a system which combines these word pairs with additional features to achieve near state of the art performance without the use of syntactic parse features and of context outside the arguments of the relation. Previous work has attributed much of the achieved performance to these features, which are easy to get in the experimental setting but would be less reliable or unavailable in other applications.1 2 Related Work This line of research began with (Marcu and Echihabi, 2002), who used a small number of unambiguous explicit markers and patterns involving them, such as [Arg1, but Arg2] to collect sets of word pairs from a large corpus using the crossproduct of the words in Arg1 and Arg2. The authors created a feature out of each pair and built a naive bayes model directly from the unannotated corpus, updating the priors and posteriors using maximum likelihood. While they demonstrated 1Reliable syntactic parses are not always available in domains other than newswire, and context (preceding relations, especially explicit relations) is not always available in some app</context>
<context position="5886" citStr="Marcu and Echihabi (2002)" startWordPosition="940" endWordPosition="943">well as additional features to train four binary classifiers, each corresponding to one of the high-level PDTB relation classes. Although other features proved to be useful, word pairs were still the major contributor to most of these classifiers. In fact, their best system for comparison included only the word pair features, and for all other classes other than expansion the word pair features alone achieved an f-score within 2 points of the best system. Interestingly, they found that training the word pair features on PDTB itself was more useful than training them on an external corpus like Marcu and Echihabi (2002), although in some cases they resort to information gain in the external corpus for filtering the word pairs. Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 3 Word Pairs 3.1 The Problem: Sparsity While Marcu and Echihabi (20</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In ACL, pages 368–375. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Improving implicit discourse relation recognition through feature set optimization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,</booktitle>
<pages>108--112</pages>
<contexts>
<context position="6214" citStr="Park and Cardie (2012)" startWordPosition="996" endWordPosition="999"> for all other classes other than expansion the word pair features alone achieved an f-score within 2 points of the best system. Interestingly, they found that training the word pair features on PDTB itself was more useful than training them on an external corpus like Marcu and Echihabi (2002), although in some cases they resort to information gain in the external corpus for filtering the word pairs. Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 3 Word Pairs 3.1 The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a </context>
<context position="14202" citStr="Park and Cardie, 2012" startWordPosition="2358" endWordPosition="2361">tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09) pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53) tf-idf, with stop list 23.77 44.33 65.33 16.98 Table 1: Main evaluation. F-measure (accuracy) for various implementations of the word pairs features Comparison Contingency Expansion Temporal Best System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35) features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9 Pitler et al., 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49) Zhou et al., 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48) Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32) Table 2: Secondary evaluation. F-measure (accuracy) for the best systems. tf and pmi refer to the word pair features used (by tf implementation), and the numbers refer to the indeces of Table 3 Comp. Cont. Exp. Temp. 1 WordNet 20.07 34.07 52.96 11.58 2 Verb Class 14.24 24.84 49.6 10.04 3 MPN 23.84 38.58 49.97 13.16 4 Modality 17.49 28.92 13.84 10.72 5 Polarity 16.46 26.36 65.15 11.58 6 Affect 18.62 31.59 59.8 13.37 7 Similarity 20.68 34.5 43.16 12.1 8 Negation 8.28 22.47 75.87 11.1 9 Length 20.75 31.28 65.72 10.19 Table 3: F-measure for </context>
<context position="17439" citStr="Park and Cardie, 2012" startWordPosition="2904" endWordPosition="2907">rage score for each dimension in each argument as a feature. Content Similarity: We use the cosine similarity and word overlap of the arguments as features. Negation: Presence or absence of negation terms in each of the arguments. Length: The ratio between the lengths (counts of words) of the arguments. 6 Evaluation of Additional Features For our secondary evaluation, we present results for each feature category on its own in Table 3 and for our best system for each of the relation classes in Table 2. We show results for the best systems from (Pitler et al., 2009), (Zhou et al., 2010) and 72 (Park and Cardie, 2012) for comparison. and Documentation. University Of Maryland College Park. 7 Conclusion We presented an aggregated approach to word pair features and showed that it outperforms the previous formulation for all relation types but contingency. This is our main contribution. With this approach, using a stop list does not have a major effect on results for most relation classes, which suggests most of the word pairs affecting performance are content word pairs which may truly be semantically related to the discourse structure. In addition, we introduced the new and useful WordNet, Affect, Length and</context>
</contexts>
<marker>Park, Cardie, 2012</marker>
<rawString>Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set optimization. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 108–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using syntax to disambiguate explicit discourse connectives in text.</title>
<date>2009</date>
<booktitle>In ACL/IJCNLP (Short Papers),</booktitle>
<pages>13--16</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="1084" citStr="Pitler and Nenkova, 2009" startWordPosition="162" endWordPosition="165">dition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation. 1 Introduction Discourse relations such as contrast and causality are part of what makes a text coherent. Being able to automatically identify these relations is important for many NLP tasks such as generation, question answering and textual entailment. In some cases, discourse relations contain an explicit marker such as but or because which makes it easy to identify the relation. Prior work (Pitler and Nenkova, 2009) showed that where explicit markers exist, the class of the relation can be disambiguated with f-scores higher than 90%. Predicting the class of implicit discourse relations, however, is much more difficult. Without an explicit marker to rely on, work on this task initially focused on using lexical cues in the form of word pairs mined from large corpora where they appear around an explicit marker (Marcu and Echihabi, 2002). The intuition is that these pairs will tend to represent semantic relationships which are related to the discourse marker (for example, word pairs often appearing around bu</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In ACL/IJCNLP (Short Papers), pages 13–16. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In ACL/IJCNLP,</booktitle>
<pages>683--691</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="4619" citStr="Pitler et al., 2009" startWordPosition="734" endWordPosition="737">s 69–73, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics some success, their experiments were run on data that is unnatural in two ways. First, it is balanced. Second, it is constructed with the same unsupervised method they use to extract the word pairs - by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitl</context>
<context position="6712" citStr="Pitler et al. (2009)" startWordPosition="1075" endWordPosition="1078">mplicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 3 Word Pairs 3.1 The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22,141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. In fact, even the larger corpus of Marcu and Echihabi (2002) may not be quite large enough to solve the sparsity issue, given that the number of word pairs is quadratic in the vocabulary. Blair-Goldensohn et al. (2007) report that using even a very small stop list (25 words) significantly reduces performanc</context>
<context position="13230" citStr="Pitler et al., 2009" startWordPosition="2206" endWordPosition="2209"> list of word pairs to be empty or almost empty, since in most sentences there are no words on the left (and recall that we discard pairs that appear only once). Since the features created for these markers will be uninformative, we expect them to be weighted down by the classifier and have no significant effect on prediction. 4 Evaluation of Word Pairs For our main evaluation, we evaluate the performance of word pair features when used with no additional features. Results are shown in Table 1. Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al., 2009), but used by virtually all previous work on this task). For most relation classes, tf is significantly better than pmi. 3 3Significance was verified for our own results in all experiments shown in this paper with a standard t-test 71 Comparison Contingency Expansion Temporal Pitler et al., 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98) tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09) pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53) tf-idf, with stop list 23.77 44.33 65.33 16.98 Table 1: Main evaluation. F-measure (accuracy) for va</context>
<context position="16409" citStr="Pitler et al. (2009)" startWordPosition="2727" endWordPosition="2730">en them. Verb Class: This is the count of pairs of verbs from Arg1 and Arg2 that share the same class, defined as the highest level Levin verb class (Levin, 1993) from the LCS database (Dorr, 2001). Money, Percentages and Numbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation features). Affect: Based on the Dictionary of Affect in Language (Whissell, 1989). Each word in the DAL gets a score for three dimensions - pleasantness (pleasant - unpleasant), activation (passive - active) and imagery (hard to imagine - easy to imagine). We use the average score for each dimension in each argument as a feature. Content Similarity: We use the cosine similarity and word overlap of the arguments as features. Negation: Presence or absence of ne</context>
<context position="18140" citStr="Pitler et al. (2009)" startWordPosition="3017" endWordPosition="3020">sion We presented an aggregated approach to word pair features and showed that it outperforms the previous formulation for all relation types but contingency. This is our main contribution. With this approach, using a stop list does not have a major effect on results for most relation classes, which suggests most of the word pairs affecting performance are content word pairs which may truly be semantically related to the discourse structure. In addition, we introduced the new and useful WordNet, Affect, Length and Negation feature categories. Our final system outperformed the best system from Pitler et al. (2009), who used mostly similar features, for comparison and temporal and is competitive with the most recent state of the art systems for contingency and expansion without using any syntactic or context features. Acknowledgments This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20153. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained </context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In ACL/IJCNLP, pages 683–691. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In</title>
<date>2008</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="4812" citStr="Prasad et al., 2008" startWordPosition="762" endWordPosition="765">. Second, it is constructed with the same unsupervised method they use to extract the word pairs - by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al. (2009). They used word pairs as well as additional features to train four binary classifiers, each corresponding to one of the high-level PDTB relation classes. Although other feature</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The penn discourse treebank 2.0. In In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia M Whissell</author>
</authors>
<title>The dictionary of affect in language.</title>
<date>1989</date>
<contexts>
<context position="16627" citStr="Whissell, 1989" startWordPosition="2764" endWordPosition="2765">umbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation features). Affect: Based on the Dictionary of Affect in Language (Whissell, 1989). Each word in the DAL gets a score for three dimensions - pleasantness (pleasant - unpleasant), activation (passive - active) and imagery (hard to imagine - easy to imagine). We use the average score for each dimension in each argument as a feature. Content Similarity: We use the cosine similarity and word overlap of the arguments as features. Negation: Presence or absence of negation terms in each of the arguments. Length: The ratio between the lengths (counts of words) of the arguments. 6 Evaluation of Additional Features For our secondary evaluation, we present results for each feature cat</context>
</contexts>
<marker>Whissell, 1989</marker>
<rawString>Cynthia M. Whissell. 1989. The dictionary of affect in language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="16264" citStr="Wilson et al., 2005" startWordPosition="2704" endWordPosition="2707"> are the counts of word pairs in the cross-product of the words in the arguments that have the particular relation (synonymy, antonymy etc) between them. Verb Class: This is the count of pairs of verbs from Arg1 and Arg2 that share the same class, defined as the highest level Levin verb class (Levin, 1993) from the LCS database (Dorr, 2001). Money, Percentages and Numbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation features). Affect: Based on the Dictionary of Affect in Language (Whissell, 1989). Each word in the DAL gets a score for three dimensions - pleasantness (pleasant - unpleasant), activation (passive - active) and imagery (hard to imagine - easy to imagine). We use the average score for each dimension in each argument </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Min Zhou</author>
<author>Yu Xu</author>
<author>Zheng-Yu Niu</author>
<author>Man Lan</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Predicting discourse connectives for implicit discourse relation recognition.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6014" citStr="Zhou et al. (2010)" startWordPosition="962" endWordPosition="965">gh other features proved to be useful, word pairs were still the major contributor to most of these classifiers. In fact, their best system for comparison included only the word pair features, and for all other classes other than expansion the word pair features alone achieved an f-score within 2 points of the best system. Interestingly, they found that training the word pair features on PDTB itself was more useful than training them on an external corpus like Marcu and Echihabi (2002), although in some cases they resort to information gain in the external corpus for filtering the word pairs. Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 3 Word Pairs 3.1 The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this dat</context>
<context position="8993" citStr="Zhou et al. (2010)" startWordPosition="1457" endWordPosition="1460"> work utilizing a similar approach. They used aggregated word pair set features to predict whether or not a sentence is argumentative. Their method is to group together word pairs that have been collected around the same explicit discourse marker: for every discourse marker such as therefore or however, they have a single feature whose value depends only on the word pairs 70 collected around that marker. This is reasonable given the intuition that the marker pattern is unambiguous and points at a particular relation. Using one feature per marker can be seen as analogous (yet complementary) to Zhou et al. (2010)’s approach of trying to predict the implicit connective by giving a score to each marker using a language model. This work uses binary features which only indicate the appearance of one or more of the pairs. The original frequencies of the word pairs are not used anywhere. A more powerful approach is to use an informed function to weight the word pairs used inside each feature. 3.3 Our Approach Our approach is similar in that we choose to aggregate word pairs that were collected around the same explicit marker. We first assembled a list of all 102 discourse markers used in PDTB, in both expli</context>
<context position="14125" citStr="Zhou et al., 2010" startWordPosition="2346" endWordPosition="2349">itler et al., 2009 21.96 (56.59) 45.6 (67.1) 63.84 (60.28) 16.21 (61.98) tf-idf, no stop list 23 (61.72) 44.03 (66.78) 66.48 (60.93) 19.54 (68.09) pmi-idf, no stop list 24.38 (61.72) 38.96 (61.52) 62.22 (57.26) 16 (65.53) tf-idf, with stop list 23.77 44.33 65.33 16.98 Table 1: Main evaluation. F-measure (accuracy) for various implementations of the word pairs features Comparison Contingency Expansion Temporal Best System 25.4 (63.36) 46.94 (68.09) 75.87 (62.84) 20.23 (68.35) features used pmi+1,2,3,6 tf+ALL tf+8 tf+3,9 Pitler et al., 2009 21.96 (56.59) 47.13 (67.3) 76.42 (63.62) 16.76 (63.49) Zhou et al., 2010 31.79 (58.22) 47.16 (48.96) 70.11 (54.54) 20.3 (55.48) Park and Cardie, 2012 31.32 (74.66) 49.82 (72.09) 79.22 (69.14) 26.57 (79.32) Table 2: Secondary evaluation. F-measure (accuracy) for the best systems. tf and pmi refer to the word pair features used (by tf implementation), and the numbers refer to the indeces of Table 3 Comp. Cont. Exp. Temp. 1 WordNet 20.07 34.07 52.96 11.58 2 Verb Class 14.24 24.84 49.6 10.04 3 MPN 23.84 38.58 49.97 13.16 4 Modality 17.49 28.92 13.84 10.72 5 Polarity 16.46 26.36 65.15 11.58 6 Affect 18.62 31.59 59.8 13.37 7 Similarity 20.68 34.5 43.16 12.1 8 Negation 8</context>
<context position="17408" citStr="Zhou et al., 2010" startWordPosition="2898" endWordPosition="2901">to imagine). We use the average score for each dimension in each argument as a feature. Content Similarity: We use the cosine similarity and word overlap of the arguments as features. Negation: Presence or absence of negation terms in each of the arguments. Length: The ratio between the lengths (counts of words) of the arguments. 6 Evaluation of Additional Features For our secondary evaluation, we present results for each feature category on its own in Table 3 and for our best system for each of the relation classes in Table 2. We show results for the best systems from (Pitler et al., 2009), (Zhou et al., 2010) and 72 (Park and Cardie, 2012) for comparison. and Documentation. University Of Maryland College Park. 7 Conclusion We presented an aggregated approach to word pair features and showed that it outperforms the previous formulation for all relation types but contingency. This is our main contribution. With this approach, using a stop list does not have a major effect on results for most relation classes, which suggests most of the word pairs affecting performance are content word pairs which may truly be semantically related to the discourse structure. In addition, we introduced the new and use</context>
</contexts>
<marker>Zhou, Xu, Niu, Lan, Su, Tan, 2010</marker>
<rawString>Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recognition. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>