<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.999177">
A Bootstrapping Algorithm for
Automatically Harvesting Semantic Relations
</title>
<author confidence="0.985897">
Marco Pennacchiotti Patrick Pantel
</author>
<affiliation confidence="0.995757">
Department of Computer Science Information Sciences Institute
University of Rome “Tor Vergata” University of Southern California
</affiliation>
<address confidence="0.682483">
Viale del Politecnico 1 4676 Admiralty Way
Rome, Italy Marina del Rey, CA 90292
</address>
<email confidence="0.988602">
pennacchiotti@info.uniroma2.it pantel@isi.edu
</email>
<sectionHeader confidence="0.993556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999700166666667">
In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a
web-based knowledge expansion technique, for extracting binary semantic relations. Given a
small set of seed instances for a particular relation, the system learns lexical patterns, applies
them to extract new instances, and then uses the Web to filter and expand the instances.
Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of
semantic relations when compared with two state of the art systems.
</bodyText>
<sectionHeader confidence="0.997725" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999949541666667">
Recent attention to knowledge-rich problems such as question answering [18] and textual
entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop
algorithms for automatically harvesting shallow semantic resources. With seemingly endless
amounts of textual data at our disposal, we have a tremendous opportunity to automatically
grow semantic term banks and ontological resources. Methods must be accurate, adaptable
and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web),
and independent or weakly dependent on human supervision.
In this paper we present Espresso, a novel bootstrapping algorithm for automatically
harvesting semantic relations, aiming at effectively supporting NLP applications,
emphasizing two major points that have been partially neglected by previous systems:
generality and weak supervision.
From the one side, Espresso is intended as a general-purpose system able to extract a wide
variety of binary semantic relations, from the classical is-a and part-of relations, to more
specific and domain oriented ones like chemical reactants in a chemistry domain and position
succession in political texts. The system architecture is designed with generality in mind,
avoiding any relation-specific inference technique. Indeed, for each semantic relation, the
system builds specific lexical patterns inferred from textual corpora.
From the other side, Espresso requires only weak human supervision. In order to start the
extraction process, a user provides only a small set of seed instances of a target relation (e.g.
Italy-country and Canada-country for the is-a relation.) In our experience, a handful of seed
instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger
set is required. To guarantee weakest supervision, Espresso combines its bootstrapping
approach with a web-based knowledge expansion technique and linguistic analysis,
exploiting the seeds as much as possible.
</bodyText>
<sectionHeader confidence="0.99842" genericHeader="introduction">
2. Relevant Work
</sectionHeader>
<bodyText confidence="0.999110755555556">
To date, most research on lexical relation harvesting has focused on is-a and part-of relations.
Approaches fall into two main categories: pattern- and clustering-based.
Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract
hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a
bootstrapping algorithm to learn more patterns from instances, which has served as the model
for most subsequent pattern-based algorithms.
Berland and Charniak [1] propose a system for part-of relation extraction, based on the
Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used
to extract new instances, ranked according to various statistical measures. While this study
introduces statistical measures to evaluate instance reliability, it remains vulnerable to data
sparseness and has the limitation of taking into consideration only one-word terms.
Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning
algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s part-
NP]. This study is the first extensive attempt to solve the problem of generic relational
patterns, that is, those expressive patterns that have high recall while suffering low precision,
as they subsume a large set of instances. In order to discard incorrect instances, Girju et al.
learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)].
While making huge grounds on improving precision/recall, the system requires heavy
supervision through manual semantic annotations.
Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to
terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from
a small set of instance seeds by extracting all substrings relating seeds in corpus sentences.
The frequencies of the substrings in the corpus are then used to retain the best patterns. The
approach gives good results on specific relations such as birthdates, however it has low
precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly
scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing
both good performances and efficiency. Espresso uses a similar approach to infer patterns,
but we then apply refining techniques to deal with various types of relations.
Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic
method for discovering similar words using a few seed examples by using pattern-based
techniques and human supervision, KnowItAll [7] that performs large-scale extraction of
facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to
extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who
formalized the problem of relation extraction in a coherent and effective combinatorial model
that is shown to outperform previous probabilistic frameworks.
Clustering approaches to relation extraction are less common and have insofar been applied
only to is-a extraction. These methods employ clustering algorithms to group words
according to their meanings in text, label the clusters using its members’ lexical or syntactic
dependencies, and then extract an is-a relation between each cluster member and the cluster
label. Caraballo [3] proposed the first attempt, which used conjunction and apposition
features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this
approach by making use of all syntactic dependency features for each noun. The advantage of
clustering approaches is that they permit algorithms to identify is-a relations that do not
explicitly appear in text, however they generally fail to produce coherent clusters from fewer
than 100 million words; hence they are unreliable for small corpora.
</bodyText>
<sectionHeader confidence="0.849036" genericHeader="method">
3. The Espresso Algorithm
</sectionHeader>
<bodyText confidence="0.996891923076923">
The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a
specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of
seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the
relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm
begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that
captures as many of the seed instances as possible in C. In the second phase, we define a
reliability measure to select the best set of patterns P&apos;⊆P. In phase three, the patterns in P&apos; are
used to extract a set of instances I. Finally, in phase four, Espresso scores each instance and
then selects the best instances I&apos; as input seeds for the next iteration. The algorithm terminates
when a predefined stopping condition is met (for our preliminary experiments, the stopping
condition is set according to the size of the corpus). For each induced pattern p and instance i,
the information theoretic scores, rπ(p) and rι(i) respectively, aim to express their reliability.
Below, Sections 3.2–3.5 describe in detail these different phases of Espresso.
</bodyText>
<subsectionHeader confidence="0.983583">
3.1. Term definition
</subsectionHeader>
<bodyText confidence="0.983463083333334">
Before one can extract relation instances from a corpus, it is necessary to define a
tokenization procedure for extracting terms. Terms are commonly defined as surface
representations of stable and key domain concepts [19]. Defining regular expressions over
POS-tagged corpora is the most commonly used technique to both define and extract terms.
We adopt a slightly modified version of the term definition given in [13], as it is one of the
most commonly used in the literature:
((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun
We operationally extend the definition of Adj to include present and past participles as most
noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus,
unlike many approaches for automatic relation extraction, we allow complex multi-word
terms as anchor points. Hence, we can capture relations between complex terms, such as
“record of a criminal conviction” part-of “FBI report”.
</bodyText>
<subsectionHeader confidence="0.998124">
3.2. Phase 1: Pattern discovery
</subsectionHeader>
<bodyText confidence="0.999939333333333">
The pattern discovery phase takes as input a set of instances I&apos; and produces as output a set of
lexical patterns P. For the first iteration I&apos; = Is, the set of initial seeds. In order to induce P, we
apply a slight modification to the approach presented in [20]. For each input instance i = {x,
y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then
generalized into a set of new sentences SGx,y by replacing all terminological expressions by a
terminological label (TR). For example:
</bodyText>
<equation confidence="0.894062">
“Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y”
is generalized as:
“Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y”
</equation>
<bodyText confidence="0.999711857142857">
All substrings linking terms x and y are then extracted from the set SGx,y, and overall
frequencies are computed. The most frequent substrings then represent the set of new patterns
P, where the frequency cutoff is experimentally set. Term generalization is particularly useful
for small corpora, where generalization is vital to ease the data sparseness. However, the
generalized patterns are naturally less precise. Hence, when dealing with bigger corpora, the
system allows the use of Sx,y∪SGx,y in order to extract substrings. For our experiments, we
used the set SGx,y .
</bodyText>
<subsectionHeader confidence="0.996066">
3.3. Phase 2: Pattern filtering
</subsectionHeader>
<bodyText confidence="0.999982625">
In this phase, Espresso selects among the patterns P those that are most reliable. Intuitively, a
reliable pattern is one that is both highly precise and one that extracts many instances. The
recall of a pattern p can be approximated by the fraction of input instances in I&apos; that are
extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are
weary of keeping patterns that generate many instances (i.e., patterns that generate high recall
but potentially disastrous precision). We thus prefer patterns that are highly associated with
the input patterns I&apos;. Pointwise mutual information [4] is a commonly used metric for
measuring the strength of association between two events x and y:
</bodyText>
<equation confidence="0.9996046">
() ( )
P x y
,
pmi x y
, = log
</equation>
<bodyText confidence="0.999733217391304">
where rι(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum
pointwise mutual information between all patterns and all instances. rπ(p) ranges from [0,1].
The reliability of the manually supplied seed instances are rι(i) = 1. The pointwise mutual in-
formation between instance i = {x, y} and pattern p is estimated using the following formula:
where fix, p, yfi is the frequency of pattern p instantiated with terms x and y and where the
asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual
information is biased towards infrequent events. To address this, we multiply pmi(i, p) with
the discounting factor suggested in [16].
The set of highest n scoring patterns P&apos;, according to rπ(p), are then selected and retained for
the next phase, where n is the number of patterns of the previous iteration incremented by 1.
In general, we expect that the set of patterns is formed by those of the previous iteration plus
a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was
previously discovered.
Moreover, to further discourage too generic patterns that might have low precision, a
threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than
t instances are then discarded, no matter what their score is. In this paper, we experimentally
set t to a value dependent on the size of the corpus. In future work, this parameter can be
learned using a development corpus.
Our reliability measure ensures that overly generic patterns, which may potentially have very
low precision, are discarded. However, we are currently exploring a web-expansion algorithm
that could both help detect generic patterns and also filter out their incorrect instances. We
estimate the precision of the instance set generated by a new pattern p by looking at the
number of these instances that are instantiated on the Web by previously accepted patterns.
</bodyText>
<equation confidence="0.961742869565217">
∑′∈ max
⎛ ( , ) ⎞
⎜⎜ pmi i p ∗ r i
( ) ⎟⎟
i I pmi
ι⎝ ⎠
I′
=
rπ
( )
p
pmi i p =
( )
,
log
x,
p y
,
x,*
, *,
y p
,*
P(x)P(y)
</equation>
<bodyText confidence="0.9999234">
We define the reliability of a pattern p, rπ(p), as its average strength of association across
each input instance i in I&apos;, weighted by the reliability of each instance i:
Generic patterns will generate instances with higher Web counts than incorrect patterns.
Then, the Web counts can also be used to filter out incorrect instances from the generic
patterns’ instantiations. More details are discussed in Section 4.3.
</bodyText>
<subsectionHeader confidence="0.992984">
3.4. Phase 3: Instance discovery
</subsectionHeader>
<bodyText confidence="0.9999566875">
In this phase, Espresso retrieves from the corpus the set of instances I that match any of the
lexical patterns in P&apos;.
In small corpora, the number of extracted instances can be too low to guarantee sufficient
statistical evidence for the pattern discovery phase of the next iteration. In such cases, the
system enters a web expansion phase, in which new instances for the given patterns are
retrieved from the Web, using the Google search engine. Specifically, for each instance i∈ I,
the system creates a set of queries, using each pattern in P&apos; with its y term instantiated with i’s
y term. For example, given the instance “Italy ; country” and the pattern [Y such as XJ , the
resulting Google query will be “country such as *”. New instances are then created from the
retrieved Web results (e.g. “Canada ; country”) and added to I. We are currently exploring
filtering mechanisms to avoid retrieving too much noise.
Moreover, to cope with data sparsity, a syntactic expansion phase is also carried out. A set of
new instances is created for each instance i∈ I by extracting sub-terminological expressions
from x corresponding to the syntactic head of terms. For example, expanding the relation
“new record of a criminal conviction” part-of “FBI report”, the following new instances are
obtained: “new record” part-of “FBI report”, and “record” part-of “FBI report”.
</bodyText>
<subsectionHeader confidence="0.983207">
3.5. Phase 4: Instance filtering
</subsectionHeader>
<bodyText confidence="0.999899">
Estimating the reliability of an instance is similar to estimating the reliability of a pattern.
Intuitively, a reliable instance is one that is highly associated with as many reliable patterns
as possible (i.e., we have more confidence in an instance when multiple reliable patterns
instantiate it.) Hence, analogous to our pattern reliability measure in Section 3.3, we define
the reliability of an instance i, rι(i), as:
where rπ(p) is the reliability of pattern p (defined in Section 3.3) and maxpmi is the maximum
pointwise mutual information between all patterns and all instances, as in Section 3.3.
Espresso finally selects the highest scoring m instances, I&apos;, and retains them as input for the
subsequent iteration. In this paper, we experimentally set m = 200.
</bodyText>
<sectionHeader confidence="0.994154" genericHeader="method">
4. Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.812759">
4.1. Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9930115">
In this section, we present a preliminary comparison of Espresso with two state of the art
systems on the task of extracting various semantic relations.
</bodyText>
<footnote confidence="0.388807">
4.1.1. Datasets
</footnote>
<bodyText confidence="0.979733">
We perform our experiments using the following two datasets:
</bodyText>
<equation confidence="0.831714166666667">
∑′∈ max
p P pmi
P′
pmi i p
( , ) ∗
rπ
( )
p
rι
=
( )
i
</equation>
<listItem confidence="0.927554666666667">
■ TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9)
newswire text collection. The sample consists of 5,951,432 words extracted from the
following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310
– AP890319.
■ CHEM: This small dataset of 313,590 words consists of a college level textbook of
introductory chemistry [2].
</listItem>
<bodyText confidence="0.977053">
We preprocess the corpora using the Alembic Workbench POS-tagger [5].
</bodyText>
<subsectionHeader confidence="0.314507">
4.1.2. Systems
</subsectionHeader>
<bodyText confidence="0.982894">
We compare the results of Espresso with the following two state of the art extraction
systems:
</bodyText>
<listItem confidence="0.9623286">
■ RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction
patterns from a set of seed instances of a particular relation (see Section 2.)
■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first
automatically induces concepts (clusters) from a raw corpus, names the concepts, and
then extracts an is-a relation between each cluster member and its cluster label. For
each cluster member, the system may generate multiple possible is-a relations, but in
this evaluation we only keep the highest scoring one. To apply this algorithm, both
datasets were first analyzed using the Minipar parser [14].
■ ESP: This is the algorithm described in this paper (details in Section 3).
4.1.3. Semantic Relations
</listItem>
<bodyText confidence="0.875658">
Espresso is designed to extract various semantic relations exemplified by a given small set of
seed instances. For our preliminary evaluation, we consider the standard is-a and part-of
relations as well as three novel relations:
</bodyText>
<listItem confidence="0.9867241">
■ succession: This relation indicates that one proper noun succeeds another in a position
or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI
succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus.
■ reaction: This relation occurs between chemical elements/molecules that can be
combined in a chemical reaction. For example, hydrogen gas reacts-with oxygen gas
and zinc reacts-with hydrochloric acid. We evaluate this relation on the CHEM
corpus.
■ production: This relation occurs when a process or element/object produces a result.
For example, ammonia produces nitric oxide. We evaluate this relation on the CHEM
corpus.
</listItem>
<bodyText confidence="0.803238">
For each semantic relation, we manually extracted a set of seed examples. The seeds were
used for both Espresso as well as RH021. Table 1 lists a sample of the seeds as well as sample
outputs from Espresso.
</bodyText>
<subsectionHeader confidence="0.93024">
4.2. Precision and Recall
</subsectionHeader>
<bodyText confidence="0.999176333333333">
We implemented each of the three systems outlined in Section 4.1.2 and applied them to the
TREC and CHEM datasets. For each output set, per relation, we evaluate the precision of the
system by extracting a random sample of instances (50 for the TREC corpus and 20 for the
</bodyText>
<table confidence="0.358344">
1 PR04 does not require any seeds.
</table>
<tableCaption confidence="0.9205305">
Table 1. Sample seeds used for each semantic relation and sample outputs from Espresso. The
number in the parentheses for each relation denotes the total number of seeds.
</tableCaption>
<figure confidence="0.7709989">
Is-a (12)
T
R
E
C
9
Part-Of (12)
Succession (12)
SEEDS
ESP
</figure>
<table confidence="0.9405875">
wheat :: crop
George Wendt :: star
Miami :: city
shark :: predator
Khrushchev :: Stalin
Carla Hills :: Yeutter
George Bush :: Ronald Reagan
Julio Barbosa de Aquino :: Mendes
leader :: panel
city :: region
plastic :: explosive
United States :: alliance
Picasso :: artist
tax :: charge
drug dealers :: felons
Italy :: country
shield :: nuclear missile
biblical quotations :: book
trees :: land
material :: FBI report
Ford :: Nixon
Setrakian :: John Griesemer
Camero Cardiel :: Camacho
Susan Weiss :: editor
</table>
<figure confidence="0.904231428571428">
Is-a (12)
C Part-Of (12)
H
E
M
Reaction (13)
Production (14)
</figure>
<figureCaption confidence="0.79979075">
NaCl :: ionic compounds
diborane :: substance
nitrogen :: element
gold :: precious metal
ion :: matter
oxygen :: water
light particle :: gas
element :: substance
magnesium :: oxygen
hydrazine :: water
aluminum metal :: oxygen
lithium metal :: fluorine gas
bright flame :: flares
hydrogen :: solid metal hydrides
ammonia :: nitric oxide
copper :: brown gas
</figureCaption>
<table confidence="0.9732770625">
Na :: element
protein :: biopolymer
HCl :: strong acid
electromagnetic radiation :: energy
oxygen :: air
powdered zinc metal :: battery
atom :: molecule
ethylene glycol :: automotive antifreeze
hydrogen :: oxygen
Ni :: HCl
carbon dioxide :: methane
boron :: fluorine
electron :: ions
glycerin :: nitroglycerin
kidneys :: kidney stones
ions :: charge
</table>
<bodyText confidence="0.998914">
CHEM corpus) and evaluating their quality manually using one human judge2. For each
instance, the judge may assign a score of 1 for correct, 0 for incorrect, and 1/2 for partially
correct. Example instances that were judged partially correct include “analyst is-a manager”
and “pilot is-a teacher”. The precision for a given set of relation instances is the sum of the
judge’s scores divided by the number of instances.
Although knowing the total number of instances of a particular relation in any non-trivial
corpus is impossible, it is possible to compute the recall of a system relative to another
system’s recall. The recall of a system A, RA, is given by the following formula:
</bodyText>
<equation confidence="0.9449205">
C A
R A = C
</equation>
<bodyText confidence="0.9918124">
where CA is the number of correct instances of a particular relation extracted by A and C is
the total number of correct instances in the corpus. Following [17], we define the relative
recall of system A given system B, RA|B, as:
Using the precision estimates, PA, from our precision experiments, we can estimate CA ≈ PA ×
|A|, where A is the total number of instances of a particular relation discovered by system A.
</bodyText>
<figure confidence="0.973519375">
×
|
=
=
B
RA
PA
×
B
CB
RB
PB
RA
CA
=
A
</figure>
<footnote confidence="0.9377695">
2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and
agreement scores.
</footnote>
<tableCaption confidence="0.930692">
Table 2. System performance on the is-a
</tableCaption>
<table confidence="0.976729714285714">
relation on the TREC-9 dataset.
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 57,525 28.0% 5.31
PR04 1,504 47.0% 0.23
ESP 4,154 73.0% 1.00
* Precision estimated from 50 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.972267">
Table 4. System performance on the part-of
relation on the TREC-9 dataset.
</tableCaption>
<table confidence="0.9855448">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 12,828 35.0% 42.52
ESP 132 80.0% 1.00
* Precision estimated from 50 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.7170705">
Table 6. System performance on the succession
relation on the TREC-9 dataset.
</tableCaption>
<table confidence="0.9770404">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 49,798 2.0% 36.96
ESP 55 49.0% 1.00
* Precision estimated from 50 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.979324">
Table 3. System performance on the is-a
relation on the CHEM dataset.
</tableCaption>
<table confidence="0.985453166666667">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 2556 25.0% 3.76
PR04 108 40.0% 0.25
ESP 200 85.0% 1.00
* Precision estimated from 20 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.98964">
Table 5. System performance on the part-of
relation on the CHEM dataset.
</tableCaption>
<table confidence="0.9798892">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 11,582 33.8% 58.78
ESP 111 60.0% 1.00
* Precision estimated from 20 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.9858425">
Table 7. System performance on the reaction
relation on the CHEM dataset.
</tableCaption>
<table confidence="0.9775958">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 6,083 30% 53.67
ESP 40 85% 1.00
* Precision estimated from 20 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<tableCaption confidence="0.758387">
Table 8. System performance on the production
relation on the CHEM dataset.
</tableCaption>
<table confidence="0.9737134">
SYSTEM INSTANCES PRECISION* REL RECALL†
RH02 197 57.5% 0.80
ESP 196 72.5% 1.00
* Precision estimated from 20 randomly sampled instances.
† Relative recall is given in relation to ESP.
</table>
<bodyText confidence="0.998635090909091">
Tables 2 – 8 reports the total number of
instances, precision, and relative recall of
each system on the TREC-9 and CHEM
corpora. The relative recall is always given in
relation to the Espresso system. For example,
in Table 2, RH02 has a relative recall of 5.31
with Espresso, which means that the RH02
system output 5.31 times more correct
relations than Espresso (at a cost of much
lower precision). Similarly, PR04 has a relative recall of 0.23 with Espresso, which means
that PR04 outputs 4.35 fewer correct relations than Espresso (also with a smaller precision).
</bodyText>
<subsectionHeader confidence="0.719581">
4.3. Discussion
</subsectionHeader>
<bodyText confidence="0.999989527777778">
Experimental results, for all relations and the two different corpus sizes, show that Espresso
greatly outperforms the other two methods on precision. However, Espresso fails to match
the recall level of RH02 in all but the experiment on the production relation. Indeed, the
filtering of unreliable patterns and instances during the bootstrapping algorithm not only
discards the patterns that are unrelated to the actual relation, but also patterns that are too
generic and ambiguous – hence resulting in a loss of recall.
As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise
in the system (e.g, the pattern [X of YJ can ambiguously refer to a part-of, is-a or possession
relation). However, generic patterns, while having low precision, yield a high recall, as also
reported by [11]. We ran an experiment on the reaction relation, retaining the generic patterns
produced during Espresso’s selection process. As expected, we obtained 1923 instances
instead of the 40 reported in Table 7, but precision dropped from 85% to 30%.
The challenge, then, is to harness the expressive power of the generic patterns whilst
maintaining the precision of Espresso. We propose the following solution that helps both in
distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances
produced by generic patterns. Unlike Girju et al. [11] that propose a highly supervised
machine learning approach based on selectional restriction, ours is an unsupervised method
based on statistical evidence obtained from the Web. At a given iteration in Espresso, the
intuition behind our solution is that the Web is large enough that correct instances will be
instantiated by many of the currently accepted patterns P. Hence, we can distinguish between
generic patterns and incorrect patterns by inspecting the relative frequency distribution of
their instances using the patterns in P. More formally, given an instance i produced by a
generic or incorrect pattern, we count how many times i instantiates on the Web with every
pattern in P, using Google. The instance i is then considered correct if its web count surpasses
a given threshold. The pattern in question is accepted as a generic pattern if a sufficient
number of its instances are considered correct, otherwise it is rejected as an incorrect pattern.
Although our results in Section 4.2 do not include this algorithm, we performed a small
experiment by adding an a-posteriori generic pattern recovery phase to Espresso. We tested
the 7,634 instances extracted by the generic pattern [X of YJ on the CHEM corpus for the
part-of relation. We randomly sample 200 of these instances and then queried Google for
these instances using the pattern [X consists of Y]. Manual evaluation of the 25 instances that
occurred at least once on Google showed 50% precision. Adding these instances to the results
from Table 5 decreases the system precision from 60% to 51%, but dramatically increases
Espresso’s recall by a factor of 8.16. Furthermore, it is important to note that there are several
other generic patterns, like [X’s Y], from which we expect a similar precision of 50% with a
continual increase of recall. This is a very exciting avenue of further investigation.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.99974125">
We proposed a weakly supervised bootstrapping algorithm, called Espresso, for
automatically extracting a wide variety of binary semantic relations from raw text. Given a
small set of seed instances for a particular relation, the system learns reliable lexical patterns,
applies them to extract new instances ranked by an information theoretic definition of
reliability, and then uses the Web to filter and expand the instances.
There are many avenues of future work. Preliminary results show that Espresso generates
highly precise relations, but at the expense of lower recall. As mentioned above in Section
4.3, we are working on improving system recall with a web-based method to identify generic
patterns and filter their instances. Early results appear very promising. We also plan to
investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here
that negative instances will play a key role in determining the selectional restriction on
generic patterns.
Espresso is the first system, to our knowledge, to emphasize both minimal supervision and
generality, both in identification of a wide variety of relations and in extensibility to various
corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations
harvested by Espresso, and if these relations can benefit NLP applications such as QA.
</bodyText>
<sectionHeader confidence="0.991612" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999959">
The authors wish to thank the reviewers for their helpful comments and Andrew Philpot for
evaluating the outputs of the systems.
</bodyText>
<sectionHeader confidence="0.998209" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999693902439025">
[1] Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In Proceedings of ACL-1999. pp.
57-64. College Park, MD.
[2] Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 2003. Chemistry: The Central Science, Ninth
Edition. Prentice Hall.
[3] Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled noun hierarchy from text. In Proceedings
of ACL-99. pp 120-126, Baltimore, MD.
[4] Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. John Wiley &amp; Sons.
[5] Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; Robinson, P.; and Vilain, M. 1997. Mixed-initiative
development of language processing systems. In Proceedings of ANLP-1997. Washington D.C.
[6] Downey, D.; Etzioni, O.; and Soderland, S. 2005. A Probabilistic model of redundancy in information
extraction. In Proceedings of IJCAI-2005. pp. 1034-1041. Edinburgh, Scotland.
[7] Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; Shaked, T.; Soderland, S.; Weld, D.S.; and
Yates, A. 2005. Unsupervised named-entity extraction from the Web: An experimental study. Artificial
Intelligence, 165(1): 91-134.
[8] Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press.
[9] Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies for online question answering:
Answering questions before they are asked. In Proceedings of ACL-03. pp. 1-7. Sapporo, Japan.
[10] Geffet, M. and Dagan, I. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. In
Proceedings of ACL-2005. Ann Arbor, MI.
[11] Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of HLT/NAACL-03. pp. 80-87. Edmonton, Canada.
[12] Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92. pp. 539-545.
Nantes, France.
[13] Justeson J.S. and Katz S.M. 1995. Technical Terminology: some linguistic properties and algorithms for
identification in text. In Proceedings of ICCL-1995. pp.539-545. Nantes, France.
[14] Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING-
94. pp. 42-48. Kyoto, Japan.
[15] Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. In Proceedings of
SemaNet’ 02: Building and Using Semantic Networks, Taipei, Taiwan.
[16] Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In Proceedings of
HLT/NAACL-04. pp. 321-328. Boston, MA.
[17] Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards terascale knowledge acquisition. In Proceedings of
COLING-04. pp. 771-777. Geneva, Switzerland.
[18] Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering.
In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138-143. Pittsburgh,
PA.
[19] Pazienza M.T. 2000. A domain-specific terminology-extraction system. In Terminology, 5:2.
[20] Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns for a question answering system. In
Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA.
[21] Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings
of EMNLP-1997.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805259">
<title confidence="0.998739">A Bootstrapping Algorithm Automatically Harvesting Semantic Relations</title>
<author confidence="0.999958">Marco Pennacchiotti Patrick Pantel</author>
<affiliation confidence="0.9999635">Department of Computer Information Sciences University of Rome “Tor University of Southern</affiliation>
<address confidence="0.9319125">Viale del Politecnico 4676 Admiralty Rome, Marina del Rey, CA</address>
<email confidence="0.936099">pennacchiotti@info.uniroma2.itpantel@isi.edu</email>
<abstract confidence="0.99805">In this paper, we present Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-1999.</booktitle>
<pages>57--64</pages>
<location>College Park, MD.</location>
<contexts>
<context position="3460" citStr="[1]" startWordPosition="484" endWordPosition="484"> and linguistic analysis, exploiting the seeds as much as possible. 2. Relevant Work To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-</context>
<context position="11335" citStr="[0,1]" startWordPosition="1726" endWordPosition="1726">timate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision). We thus prefer patterns that are highly associated with the input patterns I&apos;. Pointwise mutual information [4] is a commonly used metric for measuring the strength of association between two events x and y: () ( ) P x y , pmi x y , = log where rι(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances. rπ(p) ranges from [0,1]. The reliability of the manually supplied seed instances are rι(i) = 1. The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: where fix, p, yfi is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual information is biased towards infrequent events. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P&apos;, according to rπ(p), are then selected and retained for th</context>
</contexts>
<marker>[1]</marker>
<rawString>Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In Proceedings of ACL-1999. pp. 57-64. College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Brown</author>
<author>H E LeMay</author>
<author>B E Bursten</author>
<author>J R Burdge</author>
</authors>
<title>Chemistry: The Central Science, Ninth Edition.</title>
<date>2003</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="16560" citStr="[2]" startWordPosition="2617" endWordPosition="2617">rison of Espresso with two state of the art systems on the task of extracting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets: ∑′∈ max p P pmi P′ pmi i p ( , ) ∗ rπ ( ) p rι = ( ) i ■ TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319. ■ CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: ■ RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) ■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each clus</context>
</contexts>
<marker>[2]</marker>
<rawString>Brown, T.L.; LeMay, H.E.; Bursten, B.E.; and Burdge, J.R. 2003. Chemistry: The Central Science, Ninth Edition. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
</authors>
<title>Automatic acquisition of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-99.</booktitle>
<pages>120--126</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="6379" citStr="[3]" startWordPosition="916" endWordPosition="916">s-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted</context>
</contexts>
<marker>[3]</marker>
<rawString>Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled noun hierarchy from text. In Proceedings of ACL-99. pp 120-126, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="11019" citStr="[4]" startWordPosition="1666" endWordPosition="1666">elects among the patterns P those that are most reliable. Intuitively, a reliable pattern is one that is both highly precise and one that extracts many instances. The recall of a pattern p can be approximated by the fraction of input instances in I&apos; that are extracted by p. Since it is difficult at run-time to estimate the precision of a pattern, we are weary of keeping patterns that generate many instances (i.e., patterns that generate high recall but potentially disastrous precision). We thus prefer patterns that are highly associated with the input patterns I&apos;. Pointwise mutual information [4] is a commonly used metric for measuring the strength of association between two events x and y: () ( ) P x y , pmi x y , = log where rι(i) is the reliability of instance i (defined in Section 3.5) and maxpmi is the maximum pointwise mutual information between all patterns and all instances. rπ(p) ranges from [0,1]. The reliability of the manually supplied seed instances are rι(i) = 1. The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: where fix, p, yfi is the frequency of pattern p instantiated with terms x and y and where the </context>
</contexts>
<marker>[4]</marker>
<rawString>Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Day</author>
<author>J Aberdeen</author>
<author>L Hirschman</author>
<author>R Kozierok</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mixed-initiative development of language processing systems.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP-1997. Washington D.C.</booktitle>
<contexts>
<context position="16630" citStr="[5]" startWordPosition="2627" endWordPosition="2627">racting various semantic relations. 4.1.1. Datasets We perform our experiments using the following two datasets: ∑′∈ max p P pmi P′ pmi i p ( , ) ∗ rπ ( ) p rι = ( ) i ■ TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319. ■ CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: ■ RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) ■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, </context>
</contexts>
<marker>[5]</marker>
<rawString>Day, D.; Aberdeen, J.; Hirschman, L.; Kozierok, R.; Robinson, P.; and Vilain, M. 1997. Mixed-initiative development of language processing systems. In Proceedings of ANLP-1997. Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>O Etzioni</author>
<author>S Soderland</author>
</authors>
<title>A Probabilistic model of redundancy in information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-2005.</booktitle>
<pages>1034--1041</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="5835" citStr="[6]" startWordPosition="837" endWordPosition="837"> good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and </context>
</contexts>
<marker>[6]</marker>
<rawString>Downey, D.; Etzioni, O.; and Soderland, S. 2005. A Probabilistic model of redundancy in information extraction. In Proceedings of IJCAI-2005. pp. 1034-1041. Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M J Cafarella</author>
<author>D Downey</author>
<author>A-M Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the Web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<pages>91--134</pages>
<contexts>
<context position="5622" citStr="[7]" startWordPosition="800" endWordPosition="800"> however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clus</context>
</contexts>
<marker>[7]</marker>
<rawString>Etzioni, O.; Cafarella, M.J.; Downey, D.; Popescu, A.-M.; Shaked, T.; Soderland, S.; Weld, D.S.; and Yates, A. 2005. Unsupervised named-entity extraction from the Web: An experimental study. Artificial Intelligence, 165(1): 91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4005" citStr="[8]" startWordPosition="564" endWordPosition="564">sequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] fo</context>
</contexts>
<marker>[8]</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>E Hovy</author>
<author>A Echihabi</author>
</authors>
<title>Offline strategies for online question answering: Answering questions before they are asked.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03.</booktitle>
<pages>1--7</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="5718" citStr="[9]" startWordPosition="816" endWordPosition="816"> a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation bet</context>
</contexts>
<marker>[9]</marker>
<rawString>Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies for online question answering: Answering questions before they are asked. In Proceedings of ACL-03. pp. 1-7. Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Geffet</author>
<author>I Dagan</author>
</authors>
<title>The Distributional Inclusion Hypotheses and Lexical Entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005.</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1025" citStr="[10]" startWordPosition="140" endWordPosition="140">ervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. 1. Introduction Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatically harvesting semantic rela</context>
</contexts>
<marker>[10]</marker>
<rawString>Geffet, M. and Dagan, I. 2005. The Distributional Inclusion Hypotheses and Lexical Entailment. In Proceedings of ACL-2005. Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL-03.</booktitle>
<pages>80--87</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3954" citStr="[11]" startWordPosition="557" endWordPosition="557">nstances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate instance reliability, it remains vulnerable to data sparseness and has the limitation of taking into consideration only one-word terms. Improving upon Berland and Charniak [1], Girju et al. [11] employ machine learning algorithms and WordNet [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual </context>
<context position="24981" citStr="[11]" startWordPosition="4016" endWordPosition="4016">t the experiment on the production relation. Indeed, the filtering of unreliable patterns and instances during the bootstrapping algorithm not only discards the patterns that are unrelated to the actual relation, but also patterns that are too generic and ambiguous – hence resulting in a loss of recall. As underlined in Section 3.2, the ambiguity of generic patterns often introduces much noise in the system (e.g, the pattern [X of YJ can ambiguously refer to a part-of, is-a or possession relation). However, generic patterns, while having low precision, yield a high recall, as also reported by [11]. We ran an experiment on the reaction relation, retaining the generic patterns produced during Espresso’s selection process. As expected, we obtained 1923 instances instead of the 40 reported in Table 7, but precision dropped from 85% to 30%. The challenge, then, is to harness the expressive power of the generic patterns whilst maintaining the precision of Espresso. We propose the following solution that helps both in distinguishing generic patterns from incorrect patterns and also in filtering incorrect instances produced by generic patterns. Unlike Girju et al. [11] that propose a highly su</context>
<context position="28310" citStr="[11]" startWordPosition="4542" endWordPosition="4542">patterns, applies them to extract new instances ranked by an information theoretic definition of reliability, and then uses the Web to filter and expand the instances. There are many avenues of future work. Preliminary results show that Espresso generates highly precise relations, but at the expense of lower recall. As mentioned above in Section 4.3, we are working on improving system recall with a web-based method to identify generic patterns and filter their instances. Early results appear very promising. We also plan to investigate the use of WordNet selectional constraints, as proposed by [11]. We expect here that negative instances will play a key role in determining the selectional restriction on generic patterns. Espresso is the first system, to our knowledge, to emphasize both minimal supervision and generality, both in identification of a wide variety of relations and in extensibility to various corpus sizes. It remains to be seen whether one could enrich existing ontologies with relations harvested by Espresso, and if these relations can benefit NLP applications such as QA. Acknowledgements The authors wish to thank the reviewers for their helpful comments and Andrew Philpot </context>
</contexts>
<marker>[11]</marker>
<rawString>Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of HLT/NAACL-03. pp. 80-87. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING-92.</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="3165" citStr="[12]" startWordPosition="445" endWordPosition="445"> is-a relation.) In our experience, a handful of seed instances, in general, is sufficient for large corpora while for smaller corpora, a slightly larger set is required. To guarantee weakest supervision, Espresso combines its bootstrapping approach with a web-based knowledge expansion technique and linguistic analysis, exploiting the seeds as much as possible. 2. Relevant Work To date, most research on lexical relation harvesting has focused on is-a and part-of relations. Approaches fall into two main categories: pattern- and clustering-based. Most common are pattern-based approaches. Hearst [12] pioneered using patterns to extract hyponym (is-a) relations. Manually building three lexico-syntactic patterns, Hearst sketched a bootstrapping algorithm to learn more patterns from instances, which has served as the model for most subsequent pattern-based algorithms. Berland and Charniak [1] propose a system for part-of relation extraction, based on the Hearst approach [12]. Seed instances are used to infer linguistic patterns that, in turn, are used to extract new instances, ranked according to various statistical measures. While this study introduces statistical measures to evaluate insta</context>
<context position="6987" citStr="[12]" startWordPosition="1010" endWordPosition="1010">posed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a corpus C. An instance is a pair of terms x and y governed by the relation at hand (e.g., Pablo Picasso is-a artist). Starting from these seeds, the algorithm begins a four-phase loop. In the first phase, the algorithm infers a set of patterns P that captures as many of the seed instances as possible in C. In the second phase, we define a reliability measure to select the best set of patterns P&apos;⊆P. In phase three, the patterns in P&apos; are used to extract a set of instance</context>
</contexts>
<marker>[12]</marker>
<rawString>Hearst, M. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING-92. pp. 539-545. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S M Katz</author>
</authors>
<title>Technical Terminology: some linguistic properties and algorithms for identification in text.</title>
<date>1995</date>
<booktitle>In Proceedings of ICCL-1995.</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="8556" citStr="[13]" startWordPosition="1270" endWordPosition="1270">n theoretic scores, rπ(p) and rι(i) respectively, aim to express their reliability. Below, Sections 3.2–3.5 describe in detail these different phases of Espresso. 3.1. Term definition Before one can extract relation instances from a corpus, it is necessary to define a tokenization procedure for extracting terms. Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as “record of a criminal conviction” part-of “FBI report”. 3.2. Phase 1: Pattern discovery The pattern discovery phase takes as input</context>
</contexts>
<marker>[13]</marker>
<rawString>Justeson J.S. and Katz S.M. 1995. Technical Terminology: some linguistic properties and algorithms for identification in text. In Proceedings of ICCL-1995. pp.539-545. Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principar - an efficient, broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING94.</booktitle>
<pages>42--48</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="17379" citStr="[14]" startWordPosition="2745" endWordPosition="2745">ndran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) ■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14]. ■ ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small set of seed instances. For our preliminary evaluation, we consider the standard is-a and part-of relations as well as three novel relations: ■ succession: This relation indicates that one proper noun succeeds another in a position or title. For example, George Bush succeeded Bill Clinton and Pope Benedict XVI succeeded Pope John Paul II. We evaluate this relation on the TREC-9 corpus. ■ reaction: This re</context>
</contexts>
<marker>[14]</marker>
<rawString>Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based parser. In Proceedings of COLING94. pp. 42-48. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
</authors>
<title>Fine-Grained Proper Noun Ontologies for Question Answering.</title>
<date>2002</date>
<booktitle>In Proceedings of SemaNet’ 02: Building and Using Semantic Networks,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="5692" citStr="[15]" startWordPosition="811" endWordPosition="811">Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then ex</context>
</contexts>
<marker>[15]</marker>
<rawString>Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question Answering. In Proceedings of SemaNet’ 02: Building and Using Semantic Networks, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL-04.</booktitle>
<pages>321--328</pages>
<location>Boston, MA.</location>
<contexts>
<context position="6517" citStr="[16]" startWordPosition="935" endWordPosition="935"> combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and have insofar been applied only to is-a extraction. These methods employ clustering algorithms to group words according to their meanings in text, label the clusters using its members’ lexical or syntactic dependencies, and then extract an is-a relation between each cluster member and the cluster label. Caraballo [3] proposed the first attempt, which used conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran [16] extended this approach by making use of all syntactic dependency features for each noun. The advantage of clustering approaches is that they permit algorithms to identify is-a relations that do not explicitly appear in text, however they generally fail to produce coherent clusters from fewer than 100 million words; hence they are unreliable for small corpora. 3. The Espresso Algorithm The Espresso algorithm is based on a similar framework to the one adopted in [12]. For a specific semantic binary relation (e.g., is-a), the algorithm requires as input a small set of seed instances Is and a cor</context>
<context position="11834" citStr="[16]" startWordPosition="1809" endWordPosition="1809"> the maximum pointwise mutual information between all patterns and all instances. rπ(p) ranges from [0,1]. The reliability of the manually supplied seed instances are rι(i) = 1. The pointwise mutual information between instance i = {x, y} and pattern p is estimated using the following formula: where fix, p, yfi is the frequency of pattern p instantiated with terms x and y and where the asterisk (*) represents a wildcard. A well-known problem is that pointwise mutual information is biased towards infrequent events. To address this, we multiply pmi(i, p) with the discounting factor suggested in [16]. The set of highest n scoring patterns P&apos;, according to rπ(p), are then selected and retained for the next phase, where n is the number of patterns of the previous iteration incremented by 1. In general, we expect that the set of patterns is formed by those of the previous iteration plus a new one. Yet, new statistical evidence can lead the algorithm to discard a pattern that was previously discovered. Moreover, to further discourage too generic patterns that might have low precision, a threshold t is set for the number of instances that a pattern retrieves. Patterns firing more than t instan</context>
<context position="16973" citStr="[16]" startWordPosition="2682" endWordPosition="2682">g data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319. ■ CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: ■ RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) ■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14]. ■ ESP: This is the algorithm described in this paper (details in Section 3). 4.1.3. Semantic Relations Espresso is designed to extract various semantic relations exemplified by a given small s</context>
</contexts>
<marker>[16]</marker>
<rawString>Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In Proceedings of HLT/NAACL-04. pp. 321-328. Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E H Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<pages>771--777</pages>
<location>Geneva,</location>
<contexts>
<context position="5106" citStr="[17]" startWordPosition="727" endWordPosition="727">he system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischma</context>
<context position="21284" citStr="[17]" startWordPosition="3402" endWordPosition="3402">nalyst is-a manager” and “pilot is-a teacher”. The precision for a given set of relation instances is the sum of the judge’s scores divided by the number of instances. Although knowing the total number of instances of a particular relation in any non-trivial corpus is impossible, it is possible to compute the recall of a system relative to another system’s recall. The recall of a system A, RA, is given by the following formula: C A R A = C where CA is the number of correct instances of a particular relation extracted by A and C is the total number of correct instances in the corpus. Following [17], we define the relative recall of system A given system B, RA|B, as: Using the precision estimates, PA, from our precision experiments, we can estimate CA ≈ PA × |A|, where A is the total number of instances of a particular relation discovered by system A. × | = = B RA PA × B CB RB PB RA CA = A 2 In future work, we will perform this evaluation using multiple judges in order to obtain confidence bounds and agreement scores. Table 2. System performance on the is-a relation on the TREC-9 dataset. SYSTEM INSTANCES PRECISION* REL RECALL† RH02 57,525 28.0% 5.31 PR04 1,504 47.0% 0.23 ESP 4,154 73.0%</context>
</contexts>
<marker>[17]</marker>
<rawString>Pantel, P.; Ravichandran, D.; Hovy, E.H. 2004. Towards terascale knowledge acquisition. In Proceedings of COLING-04. pp. 771-777. Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
<author>S Harabagiu</author>
</authors>
<title>The informative role of WordNet in Open-Domain Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources.</booktitle>
<pages>138--143</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="997" citStr="[18]" startWordPosition="136" endWordPosition="136">esent Espresso, a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique, for extracting binary semantic relations. Given a small set of seed instances for a particular relation, the system learns lexical patterns, applies them to extract new instances, and then uses the Web to filter and expand the instances. Preliminary experiments show that Espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems. 1. Introduction Recent attention to knowledge-rich problems such as question answering [18] and textual entailment [10] has encouraged Natural Language Processing (NLP) researchers to develop algorithms for automatically harvesting shallow semantic resources. With seemingly endless amounts of textual data at our disposal, we have a tremendous opportunity to automatically grow semantic term banks and ontological resources. Methods must be accurate, adaptable and scalable to the varying sizes of domain corpora (e.g., textbooks vs. World Wide Web), and independent or weakly dependent on human supervision. In this paper we present Espresso, a novel bootstrapping algorithm for automatica</context>
</contexts>
<marker>[18]</marker>
<rawString>Pasca, M. and Harabagiu, S. 2001. The informative role of WordNet in Open-Domain Question Answering. In Proceedings of NAACL-01 Workshop on WordNet and Other Lexical Resources. pp. 138-143. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Pazienza</author>
</authors>
<title>A domain-specific terminology-extraction system.</title>
<date>2000</date>
<journal>In Terminology,</journal>
<volume>5</volume>
<contexts>
<context position="8358" citStr="[19]" startWordPosition="1239" endWordPosition="1239">a predefined stopping condition is met (for our preliminary experiments, the stopping condition is set according to the size of the corpus). For each induced pattern p and instance i, the information theoretic scores, rπ(p) and rι(i) respectively, aim to express their reliability. Below, Sections 3.2–3.5 describe in detail these different phases of Espresso. 3.1. Term definition Before one can extract relation instances from a corpus, it is necessary to define a tokenization procedure for extracting terms. Terms are commonly defined as surface representations of stable and key domain concepts [19]. Defining regular expressions over POS-tagged corpora is the most commonly used technique to both define and extract terms. We adopt a slightly modified version of the term definition given in [13], as it is one of the most commonly used in the literature: ((Adj|Noun)+|((Adj|Noun)*(NounPrep)?)(Adj|Noun)*)Noun We operationally extend the definition of Adj to include present and past participles as most noun phrases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor poi</context>
</contexts>
<marker>[19]</marker>
<rawString>Pazienza M.T. 2000. A domain-specific terminology-extraction system. In Terminology, 5:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>E H Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2002.</booktitle>
<pages>41--47</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4602" citStr="[20]" startWordPosition="645" endWordPosition="645">Net [8] to disambiguate part-of generic patterns, like [whole-NP’s partNP]. This study is the first extensive attempt to solve the problem of generic relational patterns, that is, those expressive patterns that have high recall while suffering low precision, as they subsume a large set of instances. In order to discard incorrect instances, Girju et al. learn WordNet-based selectional restrictions, like [whole-NP(scene#4)’s part-NP(movie#1)]. While making huge grounds on improving precision/recall, the system requires heavy supervision through manual semantic annotations. Ravichandran and Hovy [20] focus on efficiency issues for scaling relation extraction to terabytes of data. A simple and effective algorithm is proposed to infer surface patterns from a small set of instance seeds by extracting all substrings relating seeds in corpus sentences. The frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lex</context>
<context position="9376" citStr="[20]" startWordPosition="1400" endWordPosition="1400">ases composed of them are usually intended as terms (e.g., boiling point). Thus, unlike many approaches for automatic relation extraction, we allow complex multi-word terms as anchor points. Hence, we can capture relations between complex terms, such as “record of a criminal conviction” part-of “FBI report”. 3.2. Phase 1: Pattern discovery The pattern discovery phase takes as input a set of instances I&apos; and produces as output a set of lexical patterns P. For the first iteration I&apos; = Is, the set of initial seeds. In order to induce P, we apply a slight modification to the approach presented in [20]. For each input instance i = {x, y}, we first retrieve all sentences Sx,y containing the two terms x and y. Sentences are then generalized into a set of new sentences SGx,y by replacing all terminological expressions by a terminological label (TR). For example: “Because/IN HF/NNP is/VBZ a/DT weak/JJ acid/NN and/CC x is/VBZ a/DT y” is generalized as: “Because/IN TR is/VBZ a/DT TR and/CC x is/VBZ a/DT y” All substrings linking terms x and y are then extracted from the set SGx,y, and overall frequencies are computed. The most frequent substrings then represent the set of new patterns P, where th</context>
<context position="16794" citStr="[20]" startWordPosition="2654" endWordPosition="2654"> ) i ■ TREC-9: This dataset consists of a sample of articles from the Aquaint (TREC-9) newswire text collection. The sample consists of 5,951,432 words extracted from the following data files: AP890101 – AP890131, AP890201 – AP890228, and AP890310 – AP890319. ■ CHEM: This small dataset of 313,590 words consists of a college level textbook of introductory chemistry [2]. We preprocess the corpora using the Alembic Workbench POS-tagger [5]. 4.1.2. Systems We compare the results of Espresso with the following two state of the art extraction systems: ■ RH02: This algorithm by Ravichandran and Hovy [20] learns lexical extraction patterns from a set of seed instances of a particular relation (see Section 2.) ■ PR04: This is-a extraction algorithm from Pantel and Ravichandran [16] first automatically induces concepts (clusters) from a raw corpus, names the concepts, and then extracts an is-a relation between each cluster member and its cluster label. For each cluster member, the system may generate multiple possible is-a relations, but in this evaluation we only keep the highest scoring one. To apply this algorithm, both datasets were first analyzed using the Minipar parser [14]. ■ ESP: This i</context>
</contexts>
<marker>[20]</marker>
<rawString>Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL-2002. pp. 41-47. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A corpus-based approach for building semantic lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-1997.</booktitle>
<contexts>
<context position="5461" citStr="[21]" startWordPosition="777" endWordPosition="777">e frequencies of the substrings in the corpus are then used to retain the best patterns. The approach gives good results on specific relations such as birthdates, however it has low precision on generic ones like is-a and part-of. Pantel et al. [17] proposed a similar, highly scalable approach, based on an edit-distance technique, to learn lexico-POS patterns, showing both good performances and efficiency. Espresso uses a similar approach to infer patterns, but we then apply refining techniques to deal with various types of relations. Other pattern-based algorithms include Riloff and Shepherd [21], who used a semi-automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision, KnowItAll [7] that performs large-scale extraction of facts from the Web, Mann [15] and Fleischman et al. [9] who used part of speech patterns to extract a subset of is-a relations involving proper nouns, and Downey et al. [6] who formalized the problem of relation extraction in a coherent and effective combinatorial model that is shown to outperform previous probabilistic frameworks. Clustering approaches to relation extraction are less common and</context>
</contexts>
<marker>[21]</marker>
<rawString>Riloff, E. and Shepherd, J. 1997. A corpus-based approach for building semantic lexicons. In Proceedings of EMNLP-1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>