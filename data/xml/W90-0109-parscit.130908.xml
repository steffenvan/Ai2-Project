<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000082">
<title confidence="0.500081">
Abstract Linguistic Resources for Text Planning
</title>
<note confidence="0.713942">
Marie W. Meteer
BBN Systems &amp; Technologies Corporation
</note>
<address confidence="0.8690725">
10 Moulton Street
Cambridge, Massachusetts 02138
</address>
<email confidence="0.971983">
MMETEER@BBN.COM
</email>
<sectionHeader confidence="0.994225" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940181818182">
In this paper, I define the notion of an abstract
linguistic resource which reifies as a term for use by
the text planner just those combinations of concrete
linguistic resources (the words, morphological
markings, syntactic structures, etc. that actually
appear in a stream of text) that are expressible. I
present a representational level, the Text Structure,
which is defined in these abstract linguistic terms
and which mediates and constrains the commitments
of a text planner to ensure that the utterance being
planned will be expressible in language.
</bodyText>
<sectionHeader confidence="0.997055" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.99994264935065">
Natural language generation is the deliberate production
of text to meet the communicative goals of some
underlying application program. It consists of the
following major activities: (1) determining what
information is to be communicated, (2) imposing a
suitable order on the elements of this information
consistent with the constituent structure of language and
expressing the relative salience and newness of the
elements, and (3) determining what wording and syntactic
constructions to use. The first two of these activities
are generally considered &amp;quot;text planning&amp;quot; and its output is
the &amp;quot;plan&amp;quot;. The third activity is &amp;quot;realization&amp;quot; and
generally handles all of the linguistic decision making.
While it is recognized that this division is problematic
(Hovy, et al. 1988), nearly all generation systems today
make this division. One of the chief accomplishments
of my work has been to bridge the gap between these
two activities through the introduction of a new
representational level that simplifies both their
responsibilities. It both provides the choices available to
the text planner to allow it to take advantage of the
expressiveness of natural language and, through that
control of the choices, prevents it from composing an
utterance that is not expressible in the language.
Most state of the art text planning systems follow a
common design (see for example McKeown 1985, Derr
&amp; McKeown 1984, Paris 1987, or Hovy 1988). They
start from a set of propositions, each typically verb-based
and able to be realized independently as a simple
sentence. Then they organize the propositions into a
coherent discourse by combining them according to the
dictates of predefined &amp;quot;schemas&amp;quot; representing plausible
discourse relationships. Subsequent choices of concrete
surface resources1 are all local to the propositions and
not sensitive to the schemas or other context, except for
the discourse-level connectives used in combining the
propositions into complex sentences and occasionally a
shallow discourse history governing the use of pronouns.
In these approaches to natural language generation
there is a gap between the plan, which is usually
represented in the terms of the application program, and
the resources used by the realization component to carry
out that plan, which are the concrete words, syntax,
morphemes, etc. The gap occurs because the text
planner selects units from the application program and
organizes them without simultaneously considering what
linguistic resources are available for expressing them.
These systems thus have no principled way of ensuring
that their message is expressible in language.
Of course, they do successfully produce texts: they
ensure their plans are expressible by accepting limits to
their expressive competence, e.g. each atomic unit in the
plan is required to be a proposition, and thus can always
be realized as a clause. Each unit can be independently
translated into language using the linguistic realization
component since there are few restrictions on the
connection between clauses. Clauses can be connected
with coordinate or subordinate conjunctions (e.g. &amp;quot;and&amp;quot;,
&amp;quot;because&amp;quot;) or simply made into separate sentences.
However, this kind of approach does not take
advantage of the full expressive power of language, in
which units can be much more tightly composed. In
order to exercise the full expressiveness of language, text
planning needs to address the internal composition of
clauses and not just their organization into larger
structures. Clauses in actual texts reflect a combination
of multiple atomic units. Systems that ignore this and
begin with units that are inevitably realized as kernel
clauses (e.g. Mann &amp; Moore 1981, Derr &amp; McKeown
1984, Hovy 1988) have two major deficiencies: (1) they
are presuming underlying programs have units of this
size that may be simply selected for inclusion in the
message and then realized intact, and (2) they are under-
utilizing the power of natural language, which can use
complex noun phrases, nominalizations, adverbial
phrases, and other adjuncts to pack information from
multiple units into one clause.
</bodyText>
<footnote confidence="0.992507">
1 The surface linguistic resources are all the syntactic
structures, words, and grammatical features of the language
available to the speaker.
</footnote>
<page confidence="0.999389">
62
</page>
<bodyText confidence="0.999976806451613">
Moreover, the process of composing multiple units
into one clause is a much more complex problem than
simply ordering propositions in a text plan. What
compositions are possible depends on what linguistic
resources are available to realize the units involved. For
example, an Army battalion that has been assigned a
defensive mission can be said to be &amp;quot;defending&amp;quot;, but if
we say that a battalion that has been assigned an
offensive mission is &amp;quot;offending&amp;quot; we mean something
very different. There is no comparable resource available
to fit that textual niche and either a different, more
complex resource must be used or the whole text changed
(e.g. &amp;quot;attack&amp;quot;. Furthermore, different types of resources
have different constraints on their composition: one can
&amp;quot;make an important decision&amp;quot;, but one cannot &amp;quot;decide
importantly&amp;quot;.
In this paper, I address the problem of how we can
constrain the actions of a text planner to ensure that it
will never compose an utterance that cannot be realized
and can still make use of the full expressive power of
language. To do this, I introduce the notion of an
abstract linguistic resource which groups together as the
reified terms to be used by the planner just those
combinations of concrete linguistic resources that are
expressible. I have defined a level of representation in
terms of these abstract resources, the Text Structure,
which is used by the text planner to state its &amp;quot;plan&amp;quot;.
This intermediate level of representation bridges the
&amp;quot;generation gap&amp;quot; between the representation of the world
in the application program and the linguistic resources
provided by the language.
The terms and expressions in the Text Structure are
abstractions over the concrete resources of language.
They provide the text planner with terms that define the
possible combinations of features that express the
semantic categories available in the language, such as
event, process, or instance-of-a-kind. By providing the
text planner with a set of semantic categories, rather than
letting it freely choose from the individual linguistic
features that define the categories, the planner is
prevented from choosing a combination of features that
is not realizable. These abstractions in Text Structure
further constrain the composition of the utterance by
defining what kinds of constituents can be extended and
how the semantic categories can compose.
In this paper, I define the notion of an abstract
linguistic resource for text planning by looking at (1)
what are the concrete resources that the language provides
and what are their abstract properties, and (2) which of
those properties are appropriate to a text planner trying
to compose an utterance from an application program. I
then show a preliminary set of abstractions which is used
in the text planner of the Spokesman generation system.
This set was arrived at by both applying research results
from linguistics and lexical semantics and empirically by
connecting the generation system to an application
program and producing text. The long range challenge
of this work will be the continued development and
refinement of this vocabulary of abstract linguistic terms
to cover the full expressive power of natural language
and determining the compositional constraints which
will maintain the expressibility criteria.
</bodyText>
<sectionHeader confidence="0.936966" genericHeader="method">
2. Linguistic Resources
</sectionHeader>
<bodyText confidence="0.999980625">
In this section, I address the question of what the
linguistic resources are and what abstractions we can and
should make over them. I begin by looking at the
concrete resources, that is, those that actually appear in a
stream of text. I then look at what various complexes of
these resources express taken as a group. In Section 2.3,
I look more generally at how work in linguistics can
help develop a more complete vocabulary of abstractions.
</bodyText>
<subsectionHeader confidence="0.996309">
2.1 The concrete resources of language
</subsectionHeader>
<bodyText confidence="0.911489902439025">
The concrete linguistic resources are all the syntactic
structures, words, and grammatical features available to
the speaker of the language. We can divide linguistic
resources into two general classes:
• The lexical resources: These are what are often
called the open class words (the nouns, verbs, and
adjectives), and they carry most of the content.
• The grammatical resources: These include the closed
class words, morphological markings, and phrase
structure.
In what follows we ground the notion of concrete
resources by looking closely at one fairly simple
sentence:
Karen likes watching movies.
This sentence has lexical resources, such as &amp;quot;Karen&amp;quot; and
&amp;quot;watch&amp;quot;, and morphological resources, such as
the gerund marker on the verb &amp;quot;watch&amp;quot; which emphasizes
the process aspect of the action, and &amp;quot;-s&amp;quot;, the plural
marker on the noun &amp;quot;movie&amp;quot;. The phrase structure is
also a concrete resource, which expresses how the
constituents group together and certain kinds of relations
between them; in this example, the phrase structure tells
us that &amp;quot;movies&amp;quot; are what is watched, that &amp;quot;watching
movies&amp;quot; is what is liked, and that &amp;quot;Karen&amp;quot; is the one that
likes watching movies.
What is not there also expresses information. The fact
that there is no determiner (&amp;quot;a&amp;quot; or &amp;quot;the&amp;quot;) with &amp;quot;movies&amp;quot;
indicates that it is not a particular set of movies being
referred to (as in &amp;quot;the movies&amp;quot;) but a general sample of
movies. Note that it is not just the lack of the
determiner that provides this information, but the
features of the whole constituent the particular noun is in
and the fact that it is plural: if the noun phrase were
singular, then there would have to be a determiner before
&amp;quot;movie&amp;quot; (*&amp;quot;Karen likes watching movie&amp;quot;). For other
nouns in head position, the lack of a determiner can
mean other things. For example, there is also no
determiner in the first noun phrase in the sentence
(&amp;quot;Karen&amp;quot;); however, in this case, since the head is a
proper noun, it does refer to a unique individual. If a
determiner is used with a proper noun, it has a more
</bodyText>
<page confidence="0.997031">
63
</page>
<bodyText confidence="0.999935375">
general meaning of &amp;quot;an entity with that name&amp;quot; (as in
&amp;quot;All the Karens I had ever met had dark hair and then I
met a Karen with red hair&amp;quot;).
We will term this kind of composition, where the
same resource means different things in different
contexts, &amp;quot;non-linear&amp;quot; composition; this is in contrast to
&amp;quot;linear&amp;quot; composition, where each resource contributes an
identifiable part of the whole and what it contributes is
not context dependent. The identification of which
grammatical resources non-linearly co-occur and
grouping those sets into single abstract resources is a
powerful method of constraining the text planner to keep
its choices only those that are expressible in language, as
we shall see in the next section where we develop
abstraction resources for the sets of concrete resources
that appear in the example.
</bodyText>
<subsectionHeader confidence="0.999292">
2.2 Abstractions over concrete resources
</subsectionHeader>
<bodyText confidence="0.999451548387097">
Allowing a generation system to select concrete
resources directly, as is done in virtually all other
generation systems, makes available many more degrees
of freedom than the language actually permits. As we
saw in the previous section, some combinations of
concrete resources occur in language, while others do
not. Furthermore, we saw that the combination of the
lexical resource in the head of a constituent and the
grammatical resources in the constituent as a whole can
combine non-linearly, so that the choice of the lexical
and grammatical resources cannot be made independently
of each other.
In this section, we look at how we can abstract over
combinations of concrete resources by treating a
particular set as a whole and naming it, rather than
treating the resources as a set of independent features that
happen to have appeared together. The vocabulary of
abstractions we derive then becomes the terms in which
the text planner makes its decisions. It is incapable of
selecting a set of resources that is not expressible
because it is not allowed to choose them independently.
For example, the two noun phrase constituents in our
example (&amp;quot;Karen&amp;quot; and &amp;quot;movies&amp;quot;), express two different
perspectives on the entities they refer to. &amp;quot;Karen&amp;quot; is
expressed with the perspective NAMED-INDIVIDUAL and
&amp;quot;movies&amp;quot; is expressed as a &amp;quot;SAMPLE-OF-A-KIND&amp;quot;.2 We
can think of these perspectives as semantic
categories; &amp;quot;semantic&amp;quot; because they represent
something about the meaning of a constituent, not just
its form, e.g. &amp;quot;Karen&amp;quot; is referring to a person as a unique
individual with a name, in contrast to referring to her as
an anonymous individual (e.g. &amp;quot;a woman&amp;quot;). A surface
constituent can then be abstractly represented in the Text
2 &amp;quot;Sample&amp;quot; intended to mean &amp;quot;indefinite set&amp;quot;; the choice of
names for categories is meant to be evocative of what they
mean, while staying away from terms that have special
meanings in other theories. Within Text Structure, these terms
only need to be consistent The names themselves do no work,
except to help the observer understand the system.
Structure for the purposes of the text planner as the
combination of a lexical item and a semantic category.
Figure 1 shows the &amp;quot;abstract&amp;quot; resources for the two
noun phrases of our example and the other constituents
of our sentence: Karen Likes watching movies. 3
The upward arrows begin at the surface constituent
being abstracted over and point to the boxes showing
abstract resources: the lexical item in italics and
semantic category following it. This tree of boxes is an
example of the Text Structure intermediate level of
representation. We will return to how to develop a
complete set of semantic categories in the next section.
In addition to abstracting over combinations of
concrete resources by only representing the semantic type
of a constituent, we can also represent the structural
(syntactic) relations between the constituents. In Figure
1 the concrete relations of subject, direct object, etc. are
represented abstractly as arguments and marked with a
semantic relation.4
In this example, we have identified three kinds of
information that are essential to an abstract
representation of the concrete resources language
provides:
</bodyText>
<listItem confidence="0.99934">
• the constituency,
• the semantic category of the constituent, and
• the structural relations among the constituents.
</listItem>
<bodyText confidence="0.97863675">
In the next section we look at some of the motivations
for these abstractions, and in Section 3, show how they
can be used for text planning.
3 The notation of the phrase structure in the diagram is the
notation used in the linguistic component Mumble-86. While
it is slightly unconventional in that it explicitly represents the
path that will be followed in traversing the structure, it is in
other respects fairly standard in its terminology. I use it here
since it is the notation I work with and because it lends a
concreteness and reality to the diagrams since this is the
structure the linguistic component will actually build when
generating this sentence.
4 The semantic relations shown here, &amp;quot;agent&amp;quot; and &amp;quot;patient&amp;quot;, are
capturing internally consistent relations. They are not
attempting to carry the kind of weight and meaning as, say, the
terms in theta-theory.
</bodyText>
<page confidence="0.985377">
64
</page>
<figure confidence="0.984289363636364">
HEAD
Like::State
ARGUMENT
Arg-relation: Agent
Kam ::Named-individual
ARGUMENT
Arg-relation: Patient
Watch::Activity
ARGUMENT
Arg-relation: Patient
movie:Sample-of-a-kind
CLAUSE /
[SUBJECT] [PREDICATE]
NP VP
[NP-HEAD]
ICaren [VERB] [COMPLEMENT]
likes VP
■■■
[VERB] [D-OBJECT]
watching
[NP-HEAD]
movies
</figure>
<figureCaption confidence="0.99978">
Figure 1 Abstractions over concrete resources
</figureCaption>
<subsectionHeader confidence="0.999812">
2.3 Developing a set of abstractions
</subsectionHeader>
<bodyText confidence="0.901142142857143">
The development of the full vocabulary of particular
abstract resources is an ongoing process. The
motivation for determining the abstractions comes from
analysis of the language and what is expressible. A great
deal of work has already been done in linguistics that can
contribute to defining the vocabulary of abstractions. In
this section, I look at the work of four linguists in
particular who have influenced my development of the
current set of semantic categories: Jackendoff, Talmy,
Pustejovsky, and Grimshaw. While their work is very
different in character, all explore regularities in language
using a more semantic than syntactic vocabulary.
The notion of a semantic category used here was
initially influenced by the work of Jackendoff (1983)
who makes the following claim about the relationship
between language structure and meaning:
Each contenocul major syntactic constituent of a
sentence maps into a conceptual constituent in the
meaning of the sentence.5
Included in his vocabulary of conceptual categories are
Thing, Path, Action, Event, and Place.
</bodyText>
<page confidence="0.678088">
5 Jackendoff, 1983, p. 76.
</page>
<bodyText confidence="0.999464619047619">
However, while Jackendoffs categories are useful in
that they span the entire language (since they are
projections from the syntactic categories), they are not
discriminatory enough to capture the constraints
necessary to ensure expressibility. For example, two of
the semantic categories in the example above, NAMED-
INDIVIDUAL and SAMPLE-OF-A-KIND, are subsumed by
the same category in Jackendoffs set, OBJECT.
Similarly, his category EVENT has finer distinctions
available in the actual resources: a finite verb (one
which expresses tense) with its arguments expresses
what I call an EVENT (Peter decided to go to the beach),
whereas a nonfinite verb can express a generic ACTIVITY
(to decide to go to the beach). Nominalizations make the
event or activity into an OBJECT and different forms of
nominalizations can pick out different aspects of the
event, such as the PROCESS (Deciding to go to the
beach took Peter all morning) or the RESULT (The
decision to go to the beach caused controversy).
Figure 2 shows a partial hierarchy of semantic
categories that reflects these distinctions.
</bodyText>
<page confidence="0.997583">
65
</page>
<figure confidence="0.996754">
EVENT
Time-Anchored-Event Object
.0000,000ee°°.
Transition-Event State
Process-Event Activity
Process-Activity Event-Activity
</figure>
<figureCaption confidence="0.941336">
Figure 2 Partial hierarchy of semantic categories
</figureCaption>
<subsectionHeader confidence="0.369077">
Result-Object Process-Object
</subsectionHeader>
<bodyText confidence="0.999961875">
In using these finer semantic categories in the
planning vocabulary for generation, we are making a
stronger claim than Jackendoffs, namely that these
categories defme what combinations of surface resources
are possible in the language. For example, an
ACTIVITY cannot have a tense marker, since by
definition it is not grounded in time. The categories
also serve to constrain how constituents are cnnimsed.
For example, if we choose the EVENT perspective (e.g.
Michael decided to go to the beach), we can add an
adjunct of type MANNER to it (Michael quickly
decided to go to the beach) but we cannot add an adjunct
of type PROPERTY (*Michael important(ly) decided
to go to the beach)6. However, if we choose an
OBJECT perspective (Michael made a decision), the
PROPERTY adjunct oan be added (Michael made an
important decision). Both perspectives are available,
and the text planner&apos;s choice must be consistent with
the kinds of adjunctions it intends to make.
Research in lexical semantics has contributed a great
deal to defining these finer grained semantic categories.
Talmy&apos;s (1987) extensive cross language research
resulted in a set of categories for the notions expressed
grammatically in a language. Pustejovsky&apos;s (1989)
Event Semantic Structure makes a three way distinction
of event types (state, process, transition) which both
captures the effects of nonlinear composition of
resources and provides constraints on the composition
of these types with other resources. Grimshaw&apos;s
analysis of nominals (1988) contributed to the
definition of object types which convey particular
perspectives on events, such as result and process.
</bodyText>
<sectionHeader confidence="0.7787305" genericHeader="method">
3. Using Abstract Resources for Text
Planning
</sectionHeader>
<bodyText confidence="0.999744">
In order to plan complex utterances and ensure they are
expressible in language, i.e. can be successfully realized
as grammatical utterances, the text planning process
</bodyText>
<footnote confidence="0.664846666666667">
6 Following the general convention in linguistics, we use a
&amp;quot;*&amp;quot; to mark ungrammatical sentences, and a&amp;quot;?&amp;quot; to mark
questionable ones.
</footnote>
<bodyText confidence="0.991193">
must know (1) what realizations are available to an
element, that is, what resources are available for it, (2)
the constraints on the composition of the resources, and
(3) what has been committed to so far in the utterance
that may constrain the choice of resource. The first two
points are addressed by the the use of abstract linguistic
resources discussed in the previous section. The third is
addressed by the ongoing Text Structure representation
of the utterance being planned, which is also in abstract
linguistic terms. In this section, I describe the Text
Structure and how it mediates and constrains the text
planning process.
</bodyText>
<subsectionHeader confidence="0.994662">
3.1 Text Structure7
</subsectionHeader>
<bodyText confidence="0.895943925925926">
Text Structure is a tree in which each node represents a
constituent in the utterance being planned. Figure 3
shows an example of the Text Structure representation
for the utterance: &amp;quot;Karen likes watching movies on
Sundays&amp;quot;.
Text Structure represents the following information:
Constituency: The nodes in the Text Structure tree
reflect the constituency of the utterance. A
constituent may range in size from a paragraph to a
single word.
Structural relations among constituents: Each
node is marked with its structural relation to its
parent (the top label) and to its children (the
bottom label on nodes with children). Structural
relations indicate where the tree can be expanded:
composite nodes may be incrementally extended
whereas a head/argument structure is built in a
single action by the planner, reflecting the
atomicity of predicate/argument structure.
7 Note that I will not attempt a formal definition. I agree
with the text linguist Beaugrande that &amp;quot;Formalism should not
be undertaken too early. Unwieldy constructs borrowed from
mathematics and logic are out of place in domains where the
basic concepts are still highly approximative. Such constructs
give a false sense of security of having explained what has in
fact only been rewritten in a formal language.&amp;quot; Beaugrande &amp;
Dressler, 1981, p.14.
</bodyText>
<page confidence="0.975732">
66
</page>
<table confidence="0.996761538461538">
MATRIX
Like::State
HEAD
ARGUMENT ARGUMENT
Arg-relation: Agent Arg-relation: Patient
Karen ::Named. individual activity
COMPOSITE
MATRIX ADJUNCT
Watch ::Activity on ::temporal-relation
HEAD HEAD
ARGUMENT ARGUMENT
Arg-relation: Patient sunday::sample-of-a-kind
movie ::Sample-of-a-kind
</table>
<figureCaption confidence="0.88913">
Figure 3 Text Structure for &amp;quot;Karen
</figureCaption>
<bodyText confidence="0.966388">
Semantic category the constituent expresses:
The labels in the center of the node (in bold) show
the lexical head (when applicable, in italics) and the
semantic category the constituent expresses.
</bodyText>
<subsectionHeader confidence="0.9995095">
3.2 Using the Text Structure for Text
Planning
</subsectionHeader>
<bodyText confidence="0.996567153846154">
The abstract linguistic terms of our planning vocabulary
can provide constraints on the composition of the
message to ensure that it will continue to be expressible
as we add more information. For example, the semantic
category of a constituent can constrain the kind of
information that can be composed into that constituent.
Consider the earlier example contrasting &amp;quot;decide&amp;quot; and
&amp;quot;make a decision&amp;quot;, where in order to add an adjunct of
type PROPERTY, the RESULT perspective of the
EVENT must be explicit in the utterance, as shown in
Figure 4.
In summary, the Text Structure can constrain the
following types of decisions within the text planner:
</bodyText>
<listItem confidence="0.9545004">
• where additional information may be added (e.g.
structure can only be added at leaves and nodes of
type COMPOSITE; furthermore, in an incremental
pipeline architecture such as this, information can
only be added ahead of the point of speech)
• what functions and positions are available for the
elements being added in (e.g. matrix or adjunct)
• what form the added element must be in (e.g. an
object of type property can be added to a thing but
not to an event)
</listItem>
<subsectionHeader confidence="0.899193">
likes watching movies on Sundays&amp;quot;
</subsectionHeader>
<bodyText confidence="0.896504857142857">
The Text Structure representation is used in the text
planner of my SPOKESMAN generation system (Meteer
1989). It serves as an intermediate representation
between a variety of application programs and the
linguistic realization component Mumble-86
(McDonald 1984, Meteer, etal 1987). Portions of the
outputs for three of these applications are shown below.
</bodyText>
<note confidence="0.844798">
THE MAIN STREET SIMULATION PROGRAM
(ABRE1T, ET AL 1989)
</note>
<construct confidence="0.810374214285714">
Karen 10:49 AM: Karen is at International-
conglomerate, which is at 1375 Main Street. Her
skills are managing and cooking. Karen likes
watching movies. She watched &amp;quot;The Lady
Vanishes&amp;quot; on Sunday.
SEMI-AUTOMATED FORCES (SAF) PROJECT8
C11 TB is to the east and its mission is to attack
Objective GAMMA from ES646905 to ES758911
at 141423 Apr. All TB is to the south. B11 TB
and HHC12 are to the east.
AIRLAND BATTLE MANAGEMENT PROJECT&apos;)
Conduct covering force operations along avenues B
and C to defeat the lead regiments of the first
tactical echelon in the CFA in assigned sector.
</construct>
<footnote confidence="0.9953265">
8 SAF is part of DARPA&apos;s SIMNET project, contract
number MDA972-89-0600.
9 Sponsorship of ALBM is by the Defense Advanced Research
Projects Agency, the US Army Ballistic Research Laboratory,
and the US Army Center for Communications, contract
number DAAA15-87-C-0006.
</footnote>
<page confidence="0.997643">
67
</page>
<figure confidence="0.986831535714286">
COMPLEX-EVENT
COMPOSITE
V
MATRIX
DECIDE::EVENT
HEAD
ADJUNCT
IMPORTANT: :PROPERTY
\
• • •
/
ARGUMENT
MICHAEL:INDIVIDUAL
&amp;quot;Michael decided ...&amp;quot;
COMPLEX-EVENT
COMPOSITE
/
MATRIX
MAICE::EVENT
HEAD
z N
ARGUMENT ARGUMENT
MICHEAL:INDIVIDUAL DECISION::RESULT
COMPOSITE
7
MATRIX ADJUNCT
DECISION::OBJECT IMPORTANT::PROPERTY
&amp;quot;Michael made an important decision&amp;quot;
</figure>
<figureCaption confidence="0.987839">
Figure 4
</figureCaption>
<sectionHeader confidence="0.926912" genericHeader="method">
4. Contrasting Approaches
</sectionHeader>
<bodyText confidence="0.999964948717948">
The greatest difference between other approaches to NLG
and ours is that they work directly in terms of concrete
resources rather than introducing an abstract intermediate
level as I have proposed here. Approaches fall into two
classes: (1) those that use a two component architecture
in which a text planner chooses and organizes the
information to be expressed and passes it to a separate
linguistic component that chooses the concrete resources
to express the plan (e.g. McKeown 1985, Paris 1987, or
Hovy 1988); and (2) those that use a single component
which does the planning of the text directly in terms of
the concrete resources (e.g. Nirenburg et al. 1989,
Danlos 1987).
The limitation of the two component architecture is
that the text planner is not working in linguistic terms,
and so it cannot be sure that the plan it builds is
expressible, i.e. can have a successful realization. Most
such systems avoid this problem by limiting the
expressiveness of the system overall. The planner
begins with a set of propositions, each verb-based and
able to be realized independently as a simple sentence. It
then organizes the propositions into a coherent discourse
by combining them according to predefined &amp;quot;schemas&amp;quot;
representing plausible discourse relationships.
Subsequent choices of linguistic resources are all local to
the propositions and not sensitive to the schemas or
other context, except for the discourse-level connectives
used in combining the propositions and occasionally a
discourse history governing the use of pronouns.
However, clauses in actual texts by people reflect a
combination of multiple atomic units. Systems that
ignore this and begin with units that are inevitably
realized as kernel clauses under-utilize the expressive
power of natural language, which can use complex noun
phrases, nominalizations, adverbial phrases, and other
adjuncts to pack information from multiple units into
one clause.
The second approach, using single component
architecture, recognizes the limitation of separating text
</bodyText>
<page confidence="0.997924">
68
</page>
<bodyText confidence="0.999990375">
planning from the choice of linguistic resources, and
removes this division, letting the text planner
manipulate concrete resources directly. However, this
increase in complexity for the text planner has
repercussions for the complexity of the architecture
overall. For example, Nirenburg uses a blackboard
architecture that must backtrack when the text planner
has chosen incompatible concrete resources.
</bodyText>
<sectionHeader confidence="0.997607" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.99996924137931">
I have argued that an intermediate level of representation
is needed within the text planner in which to compose
the utterance and that this representation should be in
abstract linguistic terms. Making the vocabulary in
which the text planner makes its decisions be an
abstraction over the concrete resources of the language
simplifies the decision making in the composition
process, since the text planner need not deal with the
particular grammatical details of the language.
Furthermore, since the abstract vocabulary captures all
and only those combinations of resources that occur in
the language and since its terms constrain the
composition with other terms, the representation serves
to ensure that the decisions that the text planner makes
when composing the utterance will not have to be
retracted, that is, that the utterance the text planner
composes will be expressible in language.
I have shown how a preliminary planning vocabulary
can be developed by approaching the problem from two
sides: (1) using research in linguistics and text analysis
to determine a set of abstractions over concrete linguistic
resources and (2) using these terms in a text planner
generating text from a real application to empirically test
the usefulness of this set for generating. The long rang
challenge of this work will be continuing this
bidirectional development and testing process to define an
intermediate representation that both covers the
expressiveness of natural language and ensures the
expressibility of the generator&apos;s text plan.
</bodyText>
<sectionHeader confidence="0.990604" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999829016129033">
Beaugrande Robert de, &amp; Wolfgang Dressler (1981)
Introduction to Text Linguistics. Longman. London,
England.
Abrett, Glen, Mark Burstein, &amp; Stephen Deutsch (1989)
Tan: Tactical Action Representation Language, an
environment for building goal directed knowledge
based simulation. BBN Technical Report No. 7062.
June 1989.
Danlos, Laurence (1987) The Linguistic Basis of Text
Generation, Cambridge University Press, Cambridge,
England.
Derr &amp; McKeown (1984) &amp;quot;Using Focus to Generate
Complex and Simple Sentences&amp;quot;, Proceedings of
Coling-84, Stanford University, July 2-6 1984. p.319-
326.
Grimshaw, Jane (1988) &amp;quot;On the Representation of Two
Kinds of Noun&amp;quot; Presented at Theoretical Issues in
Computation and Lexical Semantics Workshop,
Brandeis University, April 1988.
Hovy, Eduard (1988) &amp;quot;Planning Coherent Multisentential
Paragraphs&amp;quot; In Proceedings of the 26th Annual
Meeting of the Association for Computational
Linguistics, Buffalo, New York, June 7-10, 1988, p.
163-169.
Jackendoff, Ray (1983) Semantics and Cognition, MIT
Press, Cambridge, Massachusetts.
McDonald, David D. (1984) &amp;quot;Description Directed
Control&amp;quot;, Computers and Mathematics 9(1) Reprinted
in Grosz, et al. (eds.) Readings in Natural Language
Processing, Morgan Kaufman Publishers, California,
1986, pp.519-538.
McDonald, David D. &amp; Marie Meteer &amp;quot;Adapting Tree
Adjoining Grammar to Generation&amp;quot;, submitted to 5th
International Workshop on Natural Language
Generation.
McKeown, Kathleen (1985) Text Generation, Cambridge
University Press, Cambridge, England.
Meteer, Marie W. (1990) The Generation Gap: The
problem of expressibility in text planning. Ph.D.
thesis, Computer and Information Sciences
Department, University of Massachusetts, Amherst,
Massachusetts. February 1990.
Meteer, Marie W. (1989) The SPOKESMAN Natural
Language Generation System, BBN Technical Report
7090.
Meteer, Marie W., David D. McDonald, Scott Anderson,
David Forster, Linda Gay, Alison Huettner, Penelope
Sibun (1987) Mumble-86: Design and
Implementation, UMass Technical Report 87-87, 173
pgs.
Nirenburg, Sergei, Victor Lessor, &amp; Eric Nyberg (1989)
&amp;quot;Controlling a Language Generation Planner&amp;quot;,
Proceedings of IJCAI-89, Detroit, Michigan.
Paris, Cecile L. (1987) The Use of Explicit User
Models in Text Generation: Tailoring to a User&apos;s
Level of Expertise, PhD Thesis, Columbia University,
Department of Computer Science.
Pustejovsky, James (1989) &amp;quot;The Generative Lexicon&amp;quot;,
submitted to Computational Linguistics.
Talmy, Leonard (1987) &amp;quot;The Relation of Grammar to
Cognition&amp;quot; (ed) B. Rudzka-Ostyn, Topics in Cognitive
Linguistics, John Benjamins.
</reference>
<page confidence="0.999318">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918644">
<title confidence="0.999288">Abstract Linguistic Resources for Text Planning</title>
<author confidence="0.997955">W Marie</author>
<affiliation confidence="0.98297">amp; Technologies</affiliation>
<address confidence="0.9813995">10 Moulton Cambridge, Massachusetts</address>
<email confidence="0.987899">MMETEER@BBN.COM</email>
<abstract confidence="0.99861825">In this paper, I define the notion of an abstract linguistic resource which reifies as a term for use by the text planner just those combinations of concrete linguistic resources (the words, morphological markings, syntactic structures, etc. that actually appear in a stream of text) that are expressible. I present a representational level, the Text Structure, which is defined in these abstract linguistic terms and which mediates and constrains the commitments of a text planner to ensure that the utterance being planned will be expressible in language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beaugrande Robert de</author>
<author>Wolfgang Dressler</author>
</authors>
<title>Introduction to Text Linguistics.</title>
<date>1981</date>
<location>Longman. London, England.</location>
<contexts>
<context position="22682" citStr="de &amp; Dressler, 1981" startWordPosition="3552" endWordPosition="3555">osite nodes may be incrementally extended whereas a head/argument structure is built in a single action by the planner, reflecting the atomicity of predicate/argument structure. 7 Note that I will not attempt a formal definition. I agree with the text linguist Beaugrande that &amp;quot;Formalism should not be undertaken too early. Unwieldy constructs borrowed from mathematics and logic are out of place in domains where the basic concepts are still highly approximative. Such constructs give a false sense of security of having explained what has in fact only been rewritten in a formal language.&amp;quot; Beaugrande &amp; Dressler, 1981, p.14. 66 MATRIX Like::State HEAD ARGUMENT ARGUMENT Arg-relation: Agent Arg-relation: Patient Karen ::Named. individual activity COMPOSITE MATRIX ADJUNCT Watch ::Activity on ::temporal-relation HEAD HEAD ARGUMENT ARGUMENT Arg-relation: Patient sunday::sample-of-a-kind movie ::Sample-of-a-kind Figure 3 Text Structure for &amp;quot;Karen Semantic category the constituent expresses: The labels in the center of the node (in bold) show the lexical head (when applicable, in italics) and the semantic category the constituent expresses. 3.2 Using the Text Structure for Text Planning The abstract linguistic te</context>
</contexts>
<marker>de, Dressler, 1981</marker>
<rawString>Beaugrande Robert de, &amp; Wolfgang Dressler (1981) Introduction to Text Linguistics. Longman. London, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glen Abrett</author>
<author>Mark Burstein</author>
<author>Stephen Deutsch</author>
</authors>
<title>Tan: Tactical Action Representation Language, an environment for building goal directed knowledge based simulation.</title>
<date>1989</date>
<tech>BBN Technical Report No. 7062.</tech>
<marker>Abrett, Burstein, Deutsch, 1989</marker>
<rawString>Abrett, Glen, Mark Burstein, &amp; Stephen Deutsch (1989) Tan: Tactical Action Representation Language, an environment for building goal directed knowledge based simulation. BBN Technical Report No. 7062. June 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Danlos</author>
</authors>
<title>The Linguistic Basis of Text Generation,</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="26729" citStr="Danlos 1987" startWordPosition="4181" endWordPosition="4182">s that they work directly in terms of concrete resources rather than introducing an abstract intermediate level as I have proposed here. Approaches fall into two classes: (1) those that use a two component architecture in which a text planner chooses and organizes the information to be expressed and passes it to a separate linguistic component that chooses the concrete resources to express the plan (e.g. McKeown 1985, Paris 1987, or Hovy 1988); and (2) those that use a single component which does the planning of the text directly in terms of the concrete resources (e.g. Nirenburg et al. 1989, Danlos 1987). The limitation of the two component architecture is that the text planner is not working in linguistic terms, and so it cannot be sure that the plan it builds is expressible, i.e. can have a successful realization. Most such systems avoid this problem by limiting the expressiveness of the system overall. The planner begins with a set of propositions, each verb-based and able to be realized independently as a simple sentence. It then organizes the propositions into a coherent discourse by combining them according to predefined &amp;quot;schemas&amp;quot; representing plausible discourse relationships. Subseque</context>
</contexts>
<marker>Danlos, 1987</marker>
<rawString>Danlos, Laurence (1987) The Linguistic Basis of Text Generation, Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derr</author>
<author>McKeown</author>
</authors>
<title>Using Focus to Generate Complex and Simple Sentences&amp;quot;,</title>
<date>1984</date>
<booktitle>Proceedings of Coling-84,</booktitle>
<pages>319--326</pages>
<institution>Stanford University,</institution>
<contexts>
<context position="2155" citStr="Derr &amp; McKeown 1984" startWordPosition="325" endWordPosition="328"> generation systems today make this division. One of the chief accomplishments of my work has been to bridge the gap between these two activities through the introduction of a new representational level that simplifies both their responsibilities. It both provides the choices available to the text planner to allow it to take advantage of the expressiveness of natural language and, through that control of the choices, prevents it from composing an utterance that is not expressible in the language. Most state of the art text planning systems follow a common design (see for example McKeown 1985, Derr &amp; McKeown 1984, Paris 1987, or Hovy 1988). They start from a set of propositions, each typically verb-based and able to be realized independently as a simple sentence. Then they organize the propositions into a coherent discourse by combining them according to the dictates of predefined &amp;quot;schemas&amp;quot; representing plausible discourse relationships. Subsequent choices of concrete surface resources1 are all local to the propositions and not sensitive to the schemas or other context, except for the discourse-level connectives used in combining the propositions into complex sentences and occasionally a shallow disco</context>
<context position="4470" citStr="Derr &amp; McKeown 1984" startWordPosition="677" endWordPosition="680">nate conjunctions (e.g. &amp;quot;and&amp;quot;, &amp;quot;because&amp;quot;) or simply made into separate sentences. However, this kind of approach does not take advantage of the full expressive power of language, in which units can be much more tightly composed. In order to exercise the full expressiveness of language, text planning needs to address the internal composition of clauses and not just their organization into larger structures. Clauses in actual texts reflect a combination of multiple atomic units. Systems that ignore this and begin with units that are inevitably realized as kernel clauses (e.g. Mann &amp; Moore 1981, Derr &amp; McKeown 1984, Hovy 1988) have two major deficiencies: (1) they are presuming underlying programs have units of this size that may be simply selected for inclusion in the message and then realized intact, and (2) they are underutilizing the power of natural language, which can use complex noun phrases, nominalizations, adverbial phrases, and other adjuncts to pack information from multiple units into one clause. 1 The surface linguistic resources are all the syntactic structures, words, and grammatical features of the language available to the speaker. 62 Moreover, the process of composing multiple units i</context>
</contexts>
<marker>Derr, McKeown, 1984</marker>
<rawString>Derr &amp; McKeown (1984) &amp;quot;Using Focus to Generate Complex and Simple Sentences&amp;quot;, Proceedings of Coling-84, Stanford University, July 2-6 1984. p.319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Grimshaw</author>
</authors>
<title>On the Representation of Two Kinds of Noun&amp;quot; Presented at Theoretical Issues</title>
<date>1988</date>
<booktitle>in Computation and Lexical Semantics Workshop,</booktitle>
<institution>Brandeis University,</institution>
<marker>Grimshaw, 1988</marker>
<rawString>Grimshaw, Jane (1988) &amp;quot;On the Representation of Two Kinds of Noun&amp;quot; Presented at Theoretical Issues in Computation and Lexical Semantics Workshop, Brandeis University, April 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
</authors>
<title>Planning Coherent Multisentential Paragraphs&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>163--169</pages>
<location>Buffalo, New York,</location>
<contexts>
<context position="2182" citStr="Hovy 1988" startWordPosition="332" endWordPosition="333">division. One of the chief accomplishments of my work has been to bridge the gap between these two activities through the introduction of a new representational level that simplifies both their responsibilities. It both provides the choices available to the text planner to allow it to take advantage of the expressiveness of natural language and, through that control of the choices, prevents it from composing an utterance that is not expressible in the language. Most state of the art text planning systems follow a common design (see for example McKeown 1985, Derr &amp; McKeown 1984, Paris 1987, or Hovy 1988). They start from a set of propositions, each typically verb-based and able to be realized independently as a simple sentence. Then they organize the propositions into a coherent discourse by combining them according to the dictates of predefined &amp;quot;schemas&amp;quot; representing plausible discourse relationships. Subsequent choices of concrete surface resources1 are all local to the propositions and not sensitive to the schemas or other context, except for the discourse-level connectives used in combining the propositions into complex sentences and occasionally a shallow discourse history governing the </context>
<context position="4482" citStr="Hovy 1988" startWordPosition="681" endWordPosition="682">g. &amp;quot;and&amp;quot;, &amp;quot;because&amp;quot;) or simply made into separate sentences. However, this kind of approach does not take advantage of the full expressive power of language, in which units can be much more tightly composed. In order to exercise the full expressiveness of language, text planning needs to address the internal composition of clauses and not just their organization into larger structures. Clauses in actual texts reflect a combination of multiple atomic units. Systems that ignore this and begin with units that are inevitably realized as kernel clauses (e.g. Mann &amp; Moore 1981, Derr &amp; McKeown 1984, Hovy 1988) have two major deficiencies: (1) they are presuming underlying programs have units of this size that may be simply selected for inclusion in the message and then realized intact, and (2) they are underutilizing the power of natural language, which can use complex noun phrases, nominalizations, adverbial phrases, and other adjuncts to pack information from multiple units into one clause. 1 The surface linguistic resources are all the syntactic structures, words, and grammatical features of the language available to the speaker. 62 Moreover, the process of composing multiple units into one clau</context>
<context position="26564" citStr="Hovy 1988" startWordPosition="4152" endWordPosition="4153">JECT IMPORTANT::PROPERTY &amp;quot;Michael made an important decision&amp;quot; Figure 4 4. Contrasting Approaches The greatest difference between other approaches to NLG and ours is that they work directly in terms of concrete resources rather than introducing an abstract intermediate level as I have proposed here. Approaches fall into two classes: (1) those that use a two component architecture in which a text planner chooses and organizes the information to be expressed and passes it to a separate linguistic component that chooses the concrete resources to express the plan (e.g. McKeown 1985, Paris 1987, or Hovy 1988); and (2) those that use a single component which does the planning of the text directly in terms of the concrete resources (e.g. Nirenburg et al. 1989, Danlos 1987). The limitation of the two component architecture is that the text planner is not working in linguistic terms, and so it cannot be sure that the plan it builds is expressible, i.e. can have a successful realization. Most such systems avoid this problem by limiting the expressiveness of the system overall. The planner begins with a set of propositions, each verb-based and able to be realized independently as a simple sentence. It t</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Hovy, Eduard (1988) &amp;quot;Planning Coherent Multisentential Paragraphs&amp;quot; In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, Buffalo, New York, June 7-10, 1988, p. 163-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="17089" citStr="Jackendoff (1983)" startWordPosition="2693" endWordPosition="2694">tions comes from analysis of the language and what is expressible. A great deal of work has already been done in linguistics that can contribute to defining the vocabulary of abstractions. In this section, I look at the work of four linguists in particular who have influenced my development of the current set of semantic categories: Jackendoff, Talmy, Pustejovsky, and Grimshaw. While their work is very different in character, all explore regularities in language using a more semantic than syntactic vocabulary. The notion of a semantic category used here was initially influenced by the work of Jackendoff (1983) who makes the following claim about the relationship between language structure and meaning: Each contenocul major syntactic constituent of a sentence maps into a conceptual constituent in the meaning of the sentence.5 Included in his vocabulary of conceptual categories are Thing, Path, Action, Event, and Place. 5 Jackendoff, 1983, p. 76. However, while Jackendoffs categories are useful in that they span the entire language (since they are projections from the syntactic categories), they are not discriminatory enough to capture the constraints necessary to ensure expressibility. For example, </context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Jackendoff, Ray (1983) Semantics and Cognition, MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>Description Directed Control&amp;quot;,</title>
<date>1984</date>
<journal>Computers and Mathematics</journal>
<booktitle>Readings in Natural Language Processing,</booktitle>
<volume>9</volume>
<issue>1</issue>
<pages>519--538</pages>
<editor>Grosz, et al. (eds.)</editor>
<publisher>Morgan Kaufman Publishers,</publisher>
<location>California,</location>
<note>Reprinted in</note>
<contexts>
<context position="24664" citStr="McDonald 1984" startWordPosition="3859" endWordPosition="3860">pipeline architecture such as this, information can only be added ahead of the point of speech) • what functions and positions are available for the elements being added in (e.g. matrix or adjunct) • what form the added element must be in (e.g. an object of type property can be added to a thing but not to an event) likes watching movies on Sundays&amp;quot; The Text Structure representation is used in the text planner of my SPOKESMAN generation system (Meteer 1989). It serves as an intermediate representation between a variety of application programs and the linguistic realization component Mumble-86 (McDonald 1984, Meteer, etal 1987). Portions of the outputs for three of these applications are shown below. THE MAIN STREET SIMULATION PROGRAM (ABRE1T, ET AL 1989) Karen 10:49 AM: Karen is at Internationalconglomerate, which is at 1375 Main Street. Her skills are managing and cooking. Karen likes watching movies. She watched &amp;quot;The Lady Vanishes&amp;quot; on Sunday. SEMI-AUTOMATED FORCES (SAF) PROJECT8 C11 TB is to the east and its mission is to attack Objective GAMMA from ES646905 to ES758911 at 141423 Apr. All TB is to the south. B11 TB and HHC12 are to the east. AIRLAND BATTLE MANAGEMENT PROJECT&apos;) Conduct covering</context>
</contexts>
<marker>McDonald, 1984</marker>
<rawString>McDonald, David D. (1984) &amp;quot;Description Directed Control&amp;quot;, Computers and Mathematics 9(1) Reprinted in Grosz, et al. (eds.) Readings in Natural Language Processing, Morgan Kaufman Publishers, California, 1986, pp.519-538.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David D McDonald</author>
<author>Marie Meteer</author>
</authors>
<title>Adapting Tree Adjoining Grammar to Generation&amp;quot;,</title>
<booktitle>submitted to 5th International Workshop on Natural Language Generation.</booktitle>
<marker>McDonald, Meteer, </marker>
<rawString>McDonald, David D. &amp; Marie Meteer &amp;quot;Adapting Tree Adjoining Grammar to Generation&amp;quot;, submitted to 5th International Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
</authors>
<title>Text Generation,</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2134" citStr="McKeown 1985" startWordPosition="323" endWordPosition="324">8), nearly all generation systems today make this division. One of the chief accomplishments of my work has been to bridge the gap between these two activities through the introduction of a new representational level that simplifies both their responsibilities. It both provides the choices available to the text planner to allow it to take advantage of the expressiveness of natural language and, through that control of the choices, prevents it from composing an utterance that is not expressible in the language. Most state of the art text planning systems follow a common design (see for example McKeown 1985, Derr &amp; McKeown 1984, Paris 1987, or Hovy 1988). They start from a set of propositions, each typically verb-based and able to be realized independently as a simple sentence. Then they organize the propositions into a coherent discourse by combining them according to the dictates of predefined &amp;quot;schemas&amp;quot; representing plausible discourse relationships. Subsequent choices of concrete surface resources1 are all local to the propositions and not sensitive to the schemas or other context, except for the discourse-level connectives used in combining the propositions into complex sentences and occasio</context>
<context position="26537" citStr="McKeown 1985" startWordPosition="4147" endWordPosition="4148">7 MATRIX ADJUNCT DECISION::OBJECT IMPORTANT::PROPERTY &amp;quot;Michael made an important decision&amp;quot; Figure 4 4. Contrasting Approaches The greatest difference between other approaches to NLG and ours is that they work directly in terms of concrete resources rather than introducing an abstract intermediate level as I have proposed here. Approaches fall into two classes: (1) those that use a two component architecture in which a text planner chooses and organizes the information to be expressed and passes it to a separate linguistic component that chooses the concrete resources to express the plan (e.g. McKeown 1985, Paris 1987, or Hovy 1988); and (2) those that use a single component which does the planning of the text directly in terms of the concrete resources (e.g. Nirenburg et al. 1989, Danlos 1987). The limitation of the two component architecture is that the text planner is not working in linguistic terms, and so it cannot be sure that the plan it builds is expressible, i.e. can have a successful realization. Most such systems avoid this problem by limiting the expressiveness of the system overall. The planner begins with a set of propositions, each verb-based and able to be realized independently</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, Kathleen (1985) Text Generation, Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
</authors>
<title>The Generation Gap: The problem of expressibility in text planning.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Sciences Department, University of Massachusetts,</institution>
<location>Amherst, Massachusetts.</location>
<marker>Meteer, 1990</marker>
<rawString>Meteer, Marie W. (1990) The Generation Gap: The problem of expressibility in text planning. Ph.D. thesis, Computer and Information Sciences Department, University of Massachusetts, Amherst, Massachusetts. February 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
</authors>
<title>The SPOKESMAN Natural Language Generation System, BBN</title>
<date>1989</date>
<tech>Technical Report 7090.</tech>
<contexts>
<context position="24511" citStr="Meteer 1989" startWordPosition="3839" endWordPosition="3840">er: • where additional information may be added (e.g. structure can only be added at leaves and nodes of type COMPOSITE; furthermore, in an incremental pipeline architecture such as this, information can only be added ahead of the point of speech) • what functions and positions are available for the elements being added in (e.g. matrix or adjunct) • what form the added element must be in (e.g. an object of type property can be added to a thing but not to an event) likes watching movies on Sundays&amp;quot; The Text Structure representation is used in the text planner of my SPOKESMAN generation system (Meteer 1989). It serves as an intermediate representation between a variety of application programs and the linguistic realization component Mumble-86 (McDonald 1984, Meteer, etal 1987). Portions of the outputs for three of these applications are shown below. THE MAIN STREET SIMULATION PROGRAM (ABRE1T, ET AL 1989) Karen 10:49 AM: Karen is at Internationalconglomerate, which is at 1375 Main Street. Her skills are managing and cooking. Karen likes watching movies. She watched &amp;quot;The Lady Vanishes&amp;quot; on Sunday. SEMI-AUTOMATED FORCES (SAF) PROJECT8 C11 TB is to the east and its mission is to attack Objective GAMM</context>
</contexts>
<marker>Meteer, 1989</marker>
<rawString>Meteer, Marie W. (1989) The SPOKESMAN Natural Language Generation System, BBN Technical Report 7090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie W Meteer</author>
<author>David D McDonald</author>
<author>Scott Anderson</author>
<author>David Forster</author>
</authors>
<date>1987</date>
<booktitle>Mumble-86: Design and Implementation, UMass</booktitle>
<tech>Technical Report 87-87, 173 pgs.</tech>
<institution>Linda Gay, Alison Huettner, Penelope Sibun</institution>
<marker>Meteer, McDonald, Anderson, Forster, 1987</marker>
<rawString>Meteer, Marie W., David D. McDonald, Scott Anderson, David Forster, Linda Gay, Alison Huettner, Penelope Sibun (1987) Mumble-86: Design and Implementation, UMass Technical Report 87-87, 173 pgs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Victor Lessor</author>
</authors>
<title>Eric Nyberg</title>
<date>1989</date>
<booktitle>Proceedings of IJCAI-89,</booktitle>
<location>Detroit, Michigan.</location>
<marker>Nirenburg, Lessor, 1989</marker>
<rawString>Nirenburg, Sergei, Victor Lessor, &amp; Eric Nyberg (1989) &amp;quot;Controlling a Language Generation Planner&amp;quot;, Proceedings of IJCAI-89, Detroit, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile L Paris</author>
</authors>
<title>The Use of Explicit User Models in Text Generation: Tailoring to a User&apos;s Level of Expertise, PhD Thesis,</title>
<date>1987</date>
<institution>Columbia University, Department of Computer Science.</institution>
<contexts>
<context position="2167" citStr="Paris 1987" startWordPosition="329" endWordPosition="330">oday make this division. One of the chief accomplishments of my work has been to bridge the gap between these two activities through the introduction of a new representational level that simplifies both their responsibilities. It both provides the choices available to the text planner to allow it to take advantage of the expressiveness of natural language and, through that control of the choices, prevents it from composing an utterance that is not expressible in the language. Most state of the art text planning systems follow a common design (see for example McKeown 1985, Derr &amp; McKeown 1984, Paris 1987, or Hovy 1988). They start from a set of propositions, each typically verb-based and able to be realized independently as a simple sentence. Then they organize the propositions into a coherent discourse by combining them according to the dictates of predefined &amp;quot;schemas&amp;quot; representing plausible discourse relationships. Subsequent choices of concrete surface resources1 are all local to the propositions and not sensitive to the schemas or other context, except for the discourse-level connectives used in combining the propositions into complex sentences and occasionally a shallow discourse history</context>
<context position="26549" citStr="Paris 1987" startWordPosition="4149" endWordPosition="4150">CT DECISION::OBJECT IMPORTANT::PROPERTY &amp;quot;Michael made an important decision&amp;quot; Figure 4 4. Contrasting Approaches The greatest difference between other approaches to NLG and ours is that they work directly in terms of concrete resources rather than introducing an abstract intermediate level as I have proposed here. Approaches fall into two classes: (1) those that use a two component architecture in which a text planner chooses and organizes the information to be expressed and passes it to a separate linguistic component that chooses the concrete resources to express the plan (e.g. McKeown 1985, Paris 1987, or Hovy 1988); and (2) those that use a single component which does the planning of the text directly in terms of the concrete resources (e.g. Nirenburg et al. 1989, Danlos 1987). The limitation of the two component architecture is that the text planner is not working in linguistic terms, and so it cannot be sure that the plan it builds is expressible, i.e. can have a successful realization. Most such systems avoid this problem by limiting the expressiveness of the system overall. The planner begins with a set of propositions, each verb-based and able to be realized independently as a simple</context>
</contexts>
<marker>Paris, 1987</marker>
<rawString>Paris, Cecile L. (1987) The Use of Explicit User Models in Text Generation: Tailoring to a User&apos;s Level of Expertise, PhD Thesis, Columbia University, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The Generative Lexicon&amp;quot;,</title>
<date>1989</date>
<note>submitted to Computational Linguistics.</note>
<marker>Pustejovsky, 1989</marker>
<rawString>Pustejovsky, James (1989) &amp;quot;The Generative Lexicon&amp;quot;, submitted to Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>The Relation of Grammar to Cognition&amp;quot; (ed) B. Rudzka-Ostyn, Topics in Cognitive Linguistics,</title>
<date>1987</date>
<location>John Benjamins.</location>
<marker>Talmy, 1987</marker>
<rawString>Talmy, Leonard (1987) &amp;quot;The Relation of Grammar to Cognition&amp;quot; (ed) B. Rudzka-Ostyn, Topics in Cognitive Linguistics, John Benjamins.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>