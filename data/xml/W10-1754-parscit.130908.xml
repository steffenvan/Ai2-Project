<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000096">
<title confidence="0.9970315">
TESLA: Translation Evaluation of Sentences with
Linear-programming-based Analysis
</title>
<author confidence="0.998023">
Chang Liu&apos; and Daniel Dahlmeier2 and Hwee Tou Ng&apos; ,2
</author>
<affiliation confidence="0.924051">
&apos;Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
</affiliation>
<email confidence="0.990399">
{liuchan1,danielhe,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997299" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946416666667">
We present TESLA-M and TESLA, two
novel automatic machine translation eval-
uation metrics with state-of-the-art perfor-
mances. TESLA-M builds on the suc-
cess of METEOR and MaxSim, but em-
ploys a more expressive linear program-
ming framework. TESLA further exploits
parallel texts to build a shallow seman-
tic representation. We evaluate both on
the WMT 2009 shared evaluation task and
show that they outperform all participating
systems in most tasks.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761466666667">
In recent years, many machine translation (MT)
evaluation metrics have been proposed, exploiting
varying amounts of linguistic resources.
Heavyweight linguistic approaches including
RTE (Pado et al., 2009) and ULC (Giménez and
Màrquez, 2008) performed the best in the WMT
2009 shared evaluation task. They exploit an ex-
tensive array of linguistic features such as parsing,
semantic role labeling, textual entailment, and dis-
course representation, which may also limit their
practical applications.
Lightweight linguistic approaches such as ME-
TEOR (Banerjee and Lavie, 2005), MaxSim
(Chan and Ng, 2008), wpF and wpBleu (Popovi´c
and Ney, 2009) exploit a limited range of linguis-
tic information that is relatively cheap to acquire
and to compute, including lemmatization, part-of-
speech (POS) tagging, and synonym dictionaries.
Non-linguistic approaches include BLEU (Pap-
ineni et al., 2002) and its variants, TER (Snover et
al., 2006), among others. They operate purely at
the surface word level and no linguistic resources
are required. Although still very popular with MT
researchers, they have generally shown inferior
performances than the linguistic approaches.
We believe that the lightweight linguistic ap-
proaches are a good compromise given the current
state of computational linguistics research and re-
sources. In this paper, we devise TESLA-M and
TESLA, two lightweight approaches to MT eval-
uation. Specifically: (1) the core features are F-
measures derived by matching bags of N-grams;
(2) both recall and precision are considered, with
more emphasis on recall; and (3) WordNet syn-
onyms feature prominently.
The main novelty of TESLA-M compared to
METEOR and MaxSim is that we match the N-
grams under a very expressive linear programming
framework, which allows us to assign weights to
the N-grams. This is in contrast to the greedy ap-
proach of METEOR, and the more restrictive max-
imum bipartite matching formulation of MaxSim.
In addition, we present a heavier version
TESLA, which combines the features using a lin-
ear model trained on development data, making
it easy to exploit features not on the same scale,
and leaving open the possibility of domain adapta-
tion. It also exploits parallel texts of the target lan-
guage with other languages as a shallow semantic
representation, which allows us to model phrase
synonyms and idioms. In contrast, METEOR and
MaxSim are capable of processing only word syn-
onyms from WordNet.
The rest of this paper is organized as follows.
Section 2 gives a high level overview of the eval-
uation task. Sections 3 and 4 describe TESLA-M
and TESLA, respectively. Section 5 presents ex-
perimental results in the setting of the WMT 2009
shared evaluation task. Finally, Section 6 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.996627" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.99991025">
We consider the task of evaluating machine trans-
lation systems in the direction of translating the
source language to the target language. Given a
reference translation and a system translation, the
</bodyText>
<page confidence="0.988125">
354
</page>
<note confidence="0.453184">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354–359,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999215">
goal of an automatic machine translation evalua-
tion algorithm such as TESLA(-M) is to output a
score predicting the quality of the system transla-
tion. Neither TESLA-M nor TESLA requires the
source text, but as additional linguistic resources,
TESLA makes use of phrase tables generated from
parallel texts of the target language and other lan-
guages, which we refer to as pivot languages. The
source language may or may not be one of the
pivot languages.
</bodyText>
<sectionHeader confidence="0.993927" genericHeader="method">
3 TESLA-M
</sectionHeader>
<bodyText confidence="0.999934473684211">
This section describes TESLA-M, the lighter
one among the two metrics. At the highest
level, TESLA-M is the arithmetic average of F-
measures between bags of N-grams (BNGs). A
BNG is a multiset of weighted N-grams. Math-
ematically, a BNG B consists of tuples (bi, bWi ),
where each bi is an N-gram and bWi is a posi-
tive real number representing its weight. In the
simplest case, a BNG contains every N-gram in a
translated sentence, and the weights are just the
counts of the respective N-grams. However, to
emphasize the content words over the function
words, we discount the weight of an N-gram by
a factor of 0.1 for every function word in the N-
gram. We decide whether a word is a function
word based on its POS tag.
In TESLA-M, the BNGs are extracted in the tar-
get language, so we call them bags of target lan-
guage N-grams (BTNGs).
</bodyText>
<subsectionHeader confidence="0.999345">
3.1 Similarity functions
</subsectionHeader>
<bodyText confidence="0.997209375">
To match two BNGs, we first need a similarity
measure between N-grams. In this section, we
define the similarity measures used in our exper-
iments.
We adopt the similarity measure from MaxSim
as sms. For unigrams x and y,
We define two other similarity functions be-
tween unigrams:
</bodyText>
<equation confidence="0.999702">
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
</equation>
<bodyText confidence="0.932406333333333">
All the three unigram similarity functions general-
ize to N-grams in the same way. For two N-grams
x = x1,2,...,n and y = y1,2,...,n,
</bodyText>
<equation confidence="0.992956">
�
0 if li, s(xi, yi) = 0
s(x, y) = n En 1 s(xi, yi) otherwise
</equation>
<subsectionHeader confidence="0.999867">
3.2 Matching two BNGs
</subsectionHeader>
<bodyText confidence="0.9990035">
Now we describe the procedure of matching two
BNGs. We take as input the following:
</bodyText>
<listItem confidence="0.98666175">
1. Two BNGs, X and Y . The ith entry in X
is xi and has weight xWi (analogously for yj
and yWj ).
2. A similarity measure, s, that gives a similar-
</listItem>
<bodyText confidence="0.961877705882353">
ity score between any two entries in the range
of 0 to 1.
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall simi-
larity. As translations often contain one-to-many
or many-to-many alignments, we allow one entry
to split its weight among multiple alignments. An
example matching problem is shown in Figure 1a,
where the weight of each node is shown, along
with the similarity for each edge. Edges with a
similarity of zero are not shown. The solution to
the matching problem is shown in Figure 1b, and
the overall similarity is 0.5 x 1.0 + 0.5 x 0.6 +
1.0 x 0.2 + 1.0 x 0.1 = 1.1.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
</bodyText>
<equation confidence="0.993295">
w(xi,yj) Vi, j
</equation>
<bodyText confidence="0.853971">
We maximize
</bodyText>
<listItem confidence="0.9930465">
• If lemma(x) = lemma(y), then sms = 1. I s(xi, yj)w(xi, yj)
• Otherwise, let ,
</listItem>
<equation confidence="0.782501">
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(�) is the indicator function, then
sms = (a + b)/2.
</equation>
<bodyText confidence="0.793069777777778">
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than En-
glish, a synonym dictionary is used instead.
subject to
w(xi,yj) ? 0 Vi, j
� w(xi,yj) :5 xW i Vi
j
� w(xi,yj) :5 yWj Vj
i
</bodyText>
<footnote confidence="0.9281725">
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
</footnote>
<page confidence="0.995721">
355
</page>
<figure confidence="0.969515">
(b) The solution
</figure>
<figureCaption confidence="0.99998">
Figure 1: A BNG matching problem
</figureCaption>
<bodyText confidence="0.9960298">
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
The F-measure is derived from the precision and
the recall:
</bodyText>
<equation confidence="0.7846775">
Precision x Recall
F= α x Precision + (1 − α) x Recall
</equation>
<bodyText confidence="0.999913666666667">
In this work, we set α = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
</bodyText>
<subsectionHeader confidence="0.99967">
3.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999901555555555">
The TESLA-M sentence-level score for a refer-
ence and a system translation is the arithmetic av-
erage of the BTNG F-measures for unigrams, bi-
grams, and trigrams based on similarity functions
sms and spos. We thus have 3 x 2 = 6 features for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
</bodyText>
<subsectionHeader confidence="0.972783">
3.4 Reduction
</subsectionHeader>
<bodyText confidence="0.999843454545454">
When every xWi and yWj is 1, the linear program-
ming problem proposed above reduces to weighted
bipartite matching. This is a well known result;
see for example, Cormen et al. (2001) for details.
This is the formalism of MaxSim, which precludes
the use of fractional weights.
If the similarity function is binary-valued
and transitive, such as slem and spos, then
we can use a much simpler and faster greedy
matching procedure: the best match is simply
Eg min(Exz=g xWi , Eyz=g yWi ).
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
4 TESLA
</sectionHeader>
<bodyText confidence="0.99994265">
Unlike the simple arithmetic average used in
TESLA-M, TESLA uses a general linear com-
bination of three types of features: BTNG F-
measures as in TESLA-M, F-measures between
bags of N-grams in each of the pivot languages,
called bags of pivot language N-grams (BPNGs),
and normalized language model scores of the sys-
tem translation, defined as n1 log P, where n is
the length of the translation, and P the language
model probability. The method of training the lin-
ear model depends on the development data. In
the case of WMT, the development data is in the
form of manual rankings, so we train SVMrank
(Joachims, 2006) on these instances to build the
linear model. In other scenarios, some form of re-
gression can be more appropriate.
The rest of this section focuses on the genera-
tion of the BPNGs. Their matching is done in the
same way as described for BTNGs in the previous
section.
</bodyText>
<subsectionHeader confidence="0.995426">
4.1 Phrase level semantic representation
</subsectionHeader>
<bodyText confidence="0.9999526">
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al., 2006; Haghighi et al., 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a se-
mantic representation of a target language phrase.
That is, if two target language phrases are often
aligned to the same pivot language phrase, then
they can be inferred to be similar in meaning.
Similar observations have been made by previous
researchers (Bannard and Callison-Burch, 2005;
Callison-Burch et al., 2006; Snover et al., 2009).
We note here two differences from WordNet
synonyms: (1) the relationship is not restricted to
the word level only, and (2) the relationship is not
binary. The degree of similarity can be measured
by the percentage of overlap between the seman-
tic representations. For example, at the word level,
</bodyText>
<figure confidence="0.963447">
w=1.0 w=0.8 w=0.2 1
w=0.2
s=0.5 s=0.5 s=1.0 s=1.0
w=1.0 w=0.8
w=0.1
(a) The matching problem
w=1.0 w=0.8 w=0.2 1
w=0.2
w=1.0 w=0.6 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
S
Precision =
E jyWj
S
Recall =
Ei xW i
</figure>
<page confidence="0.993467">
356
</page>
<bodyText confidence="0.999853">
the phrases good morning and hello are unrelated
even with a synonym dictionary, but they both very
often align to the same French phrase bonjour, and
we conclude they are semantically related to a high
degree.
</bodyText>
<subsectionHeader confidence="0.999677">
4.2 Segmenting a sentence into phrases
</subsectionHeader>
<bodyText confidence="0.999993">
To extend the concept of this semantic represen-
tation of phrases to sentences, we segment a sen-
tence in the target language into phrases. Given a
phrase table, we can approximate the probability
of a phrase p by:
</bodyText>
<equation confidence="0.992302333333333">
N(p)
Pr(p) = � (1)
p� N(p&apos;)
</equation>
<bodyText confidence="0.998585846153846">
where N(·) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, ... , pn) by:
where Z(S) is a normalizing constant. The seg-
mentation of S that maximizes the probability can
be determined efficiently using a dynamic pro-
gramming algorithm. The formula has a strong
preference for longer phrases, as every Pr(p) is
a small fraction. To deal with out-of-vocabulary
(OOV) words, we allow any single word w to be
considered a phrase, and if N(w) = 0, we set
N(w) = 0.5 instead.
</bodyText>
<subsectionHeader confidence="0.9773135">
4.3 BPNGs as sentence level semantic
representation
</subsectionHeader>
<bodyText confidence="0.968489666666667">
Simply merging the phrase-level semantic rep-
resentation is insufficient to produce a sensible
sentence-level semantic representation. As an ex-
ample, we consider two target language (English)
sentences segmented as follows:
A naive comparison of the bags of aligned pivot
language (French) phrases would likely conclude
that the two sentences are completely unrelated,
as the bags of aligned phrases are likely to be
completely disjoint. We tackle this problem by
constructing a confusion network representation
of the aligned phrases, as shown in Figures 2 and
</bodyText>
<figureCaption confidence="0.852456">
Figure 2: A confusion network as a semantic rep-
resentation
Bonjour , monsieur . / 1.0
Figure 3: A degenerate confusion network as a se-
mantic representation
</figureCaption>
<listItem confidence="0.48703">
3. A confusion network is a compact representa-
</listItem>
<bodyText confidence="0.993788866666667">
tion of a potentially exponentially large number of
weighted and likely malformed French sentences.
We can collect the N-gram statistics of this ensem-
ble of French sentences efficiently from the confu-
sion network representation. For example, the tri-
gram Bonjour, Querrien 2 would receive a weight
of 0.9 x 1.0 = 0.9 in Figure 2. As with BTNGs,
we discount the weight of an N-gram by a factor
of 0.1 for every function word in the N-gram, so
as to place more emphasis on the content words.
The collection of all such N-grams and their
corresponding weights forms the BPNG of a sen-
tence. The reference and system BPNGs are then
matched using the algorithm outlined in Section
3.2.
</bodyText>
<subsectionHeader confidence="0.997965">
4.4 Scoring
</subsectionHeader>
<bodyText confidence="0.999973769230769">
The TESLA sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, (2) BPNG F-measures for un-
igrams, bigrams, and trigrams based on similar-
ity functions slem and spos for each pivot lan-
guage, and (3) normalized language model scores.
In this work, we use two language models. We
thus have 3 x 2 features from the BTNGs, 3 x
2 x #pivot languages features from the BPNGs, and
2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.987579">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999636666666667">
We test our metrics in the setting of the WMT
2009 evaluation task (Callison-Burch et al., 2009).
The manual judgments from WMT 2008 are used
</bodyText>
<footnote confidence="0.666185">
2Note that the N-gram can span more than one segment.
</footnote>
<equation confidence="0.930098666666667">
Bonjour , / 0.9
Salut , / 0.1
Querrien / 1.0 . / 1.0
n
Pr(p1,p2,...,pn|S) = Z(S)JPr(pi) (2)
i=1
</equation>
<page confidence="0.992377">
357
</page>
<bodyText confidence="0.999905727272727">
as the development data and the metric is evalu-
ated on WMT 2009 manual judgments with re-
spect to two criteria: sentence level consistency
and system level correlation.
The sentence level consistency is defined as the
percentage of correctly predicted pairs among all
the manually judged pairs. Pairs judged as ties
by humans are excluded from the evaluation. The
system level correlation is defined as the average
Spearman’s rank correlation coefficient across all
translation tracks.
</bodyText>
<subsectionHeader confidence="0.999969">
5.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999828272727273">
We POS tag and lemmatize the texts using the fol-
lowing tools: for English, OpenNLP POS-tagger3
and WordNet lemmatizer; for French and German,
TreeTagger4; for Spanish, the FreeLing toolkit
(Atserias et al., 2006); and for Czech, the Morce
morphological tagger5.
For German, we additionally perform noun
compound splitting. For each noun, we choose the
split that maximizes the geometric mean of the fre-
quency counts of its parts, following the method in
(Koehn and Knight, 2003):
</bodyText>
<equation confidence="0.97163325">
� n
max N(pi)
nrp1rptr...rpn
i=1
</equation>
<bodyText confidence="0.999217571428571">
The resulting compound split sentence is then POS
tagged and lemmatized.
Finally, we remove all non-alphanumeric tokens
from the text in all languages. To generate the lan-
guage model features, we train SRILM (Stolcke,
2002) trigram models with modified Kneser-Ney
discounting on the supplied monolingual Europarl
and news commentary texts.
We build phrase tables from the supplied news
commentary bitexts. Word alignments are pro-
duced by the Berkeley aligner. The widely used
phrase extraction heuristic in (Koehn et al., 2003)
is used to extract phrase pairs and phrases of up to
4 words are collected.
</bodyText>
<subsectionHeader confidence="0.9877">
5.3 Into-English task
</subsectionHeader>
<bodyText confidence="0.9980352">
For each of the BNG features, we generate three
scores, for unigrams, bigrams, and trigrams re-
spectively. For BPNGs, we generate one such
triple for each of the four pivot languages supplied,
namely Czech, French, German, and Spanish.
</bodyText>
<footnote confidence="0.988968666666667">
3opennlp.sourceforge.net
4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5ufal.mff.cuni.cz/morce/index.php
</footnote>
<table confidence="0.999701714285714">
System Sentence
correlation consistency
TESLA 0.8993 0.6324
TESLA-M 0.8718 0.6097
ulc 0.83 0.63
maxsim 0.80 0.62
meteor-0.6 0.72 0.50
</table>
<tableCaption confidence="0.999921">
Table 1: Into-English task on WMT 2009 data
</tableCaption>
<bodyText confidence="0.96398725">
Table 1 compares the scores of TESLA and
TESLA-M against three participants in WMT
2009 under identical settings6: ULC (a heavy-
weight linguistic approach with the best per-
formance in WMT 2009), MaxSim, and ME-
TEOR. The results show that TESLA outperforms
all these systems by a substantial margin, and
TESLA-M is very competitive too.
</bodyText>
<subsectionHeader confidence="0.821314">
5.4 Out-of-English task
</subsectionHeader>
<bodyText confidence="0.99998965">
A synonym dictionary is required for target lan-
guages other than English. We use the freely avail-
able Wiktionary dictionary7 for each language.
For Spanish, we additionally use the Spanish
WordNet, a component of FreeLing.
Only one pivot language (English) is used for
the BPNG. For the English-Czech task, we only
have one language model instead of two, as the
Europarl language model is not available.
Tables 2 and 3 show the sentence-level consis-
tency and system-level correlation respectively of
TESLA and TESLA-M against the best reported
results in WMT 2009 under identical setting. The
results show that both TESLA and TESLA-M
give very competitive performances. Interestingly,
TESLA and TESLA-M obtain similar scores in the
out-of-English task. This could be because we use
only one pivot language (English), compared to
four in the into-English task. We plan to inves-
tigate this phenomenon in our future work.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999835">
This paper describes TESLA-M and TESLA. Our
main contributions are: (1) we generalize the
bipartite matching formalism of MaxSim into a
more expressive linear programming framework;
</bodyText>
<footnote confidence="0.9974995">
6The original WMT09 report contained erroneous results.
The scores here are the corrected results released after publi-
cation.
7www.wiktionary.org
</footnote>
<bodyText confidence="0.4150825">
i
n
</bodyText>
<page confidence="0.968936">
358
</page>
<table confidence="0.9998785">
en-fr en-de en-es en-cz Overall
TESLA 0.6828 0.5734 0.5940 0.5519 0.5796
TESLA-M 0.6390 0.5890 0.5927 0.5656 0.5847
wcd6p4er 0.67 0.58 0.61 0.59 0.60
wpF 0.66 0.60 0.61 n/a 0.61
terp 0.62 0.50 0.54 0.31 0.43
</table>
<tableCaption confidence="0.724598">
Table 2: Out-of-English task sentence-level con-
sistency on WMT 2009 data
</tableCaption>
<table confidence="0.9998755">
en-fr en-de en-es en-cz Overall
TESLA 0.8529 0.7857 0.7272 0.3141 0.6700
TESLA-M 0.9294 0.8571 0.7909 0.0857 0.6657
wcd6p4er -0.89 0.54 -0.45 -0.1 -0.22
wpF 0.90 -0.06 0.58 n/a n/a
terp -0.89 0.03 -0.58 -0.40 -0.46
</table>
<tableCaption confidence="0.949738">
Table 3: Out-of-English task system-level correla-
tion on WMT 2009 data
</tableCaption>
<bodyText confidence="0.99975175">
(2) we exploit parallel texts to create a shallow se-
mantic representation of the sentences; and (3) we
show that they outperform all participants in most
WMT 2009 shared evaluation tasks.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992298">
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999786">
J. Atserias, B. Casas, E. Comelles, M. González,
L. Padró, and M. Padró. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source NLP
library. In Proceedings of LREC.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or
Summarization.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of ACL.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of HLT-NAACL.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proceedings of ACL.
T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein,
2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.
J. Giménez and L. Màrquez. 2008. A smorgasbord of
features for automatic MT evaluation. In Proceed-
ings of the Third WMT.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG mod-
els. In Proceedings of ACL-IJCNLP.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of KDD.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of EACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.
F.J. Och and N. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
S. Pado, M. Galley, D. Jurafsky, and C.D. Man-
ning. 2009. Robust machine translation evaluation
with entailment features. In Proceedings of ACL-
IJCNLP.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
M. Popovi´c and H. Ney. 2009. Syntax-oriented eval-
uation measures for machine translation output. In
Proceedings of WMT.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of WMT.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP.
</reference>
<page confidence="0.999075">
359
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.594931">
<title confidence="0.994578">TESLA: Translation Evaluation of Sentences Linear-programming-based Analysis</title>
<author confidence="0.973251">Tou</author>
<affiliation confidence="0.984019">of Computer Science, National University of Graduate School for Integrative Sciences and</affiliation>
<email confidence="0.621319">liuchan1@comp.nus.edu.sg</email>
<email confidence="0.621319">danielhe@comp.nus.edu.sg</email>
<email confidence="0.621319">nght@comp.nus.edu.sg</email>
<abstract confidence="0.999689384615384">We present TESLA-M and TESLA, two novel automatic machine translation evaluation metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<institution>(MDA) of Singapore. References</institution>
<marker></marker>
<rawString>(MDA) of Singapore. References</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>B Casas</author>
<author>E Comelles</author>
<author>M González</author>
<author>L Padró</author>
<author>M Padró</author>
</authors>
<title>Freeling 1.3: Syntactic and semantic services in an open-source NLP library.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="14934" citStr="Atserias et al., 2006" startWordPosition="2528" endWordPosition="2531">to two criteria: sentence level consistency and system level correlation. The sentence level consistency is defined as the percentage of correctly predicted pairs among all the manually judged pairs. Pairs judged as ties by humans are excluded from the evaluation. The system level correlation is defined as the average Spearman’s rank correlation coefficient across all translation tracks. 5.2 Pre-processing We POS tag and lemmatize the texts using the following tools: for English, OpenNLP POS-tagger3 and WordNet lemmatizer; for French and German, TreeTagger4; for Spanish, the FreeLing toolkit (Atserias et al., 2006); and for Czech, the Morce morphological tagger5. For German, we additionally perform noun compound splitting. For each noun, we choose the split that maximizes the geometric mean of the frequency counts of its parts, following the method in (Koehn and Knight, 2003): � n max N(pi) nrp1rptr...rpn i=1 The resulting compound split sentence is then POS tagged and lemmatized. Finally, we remove all non-alphanumeric tokens from the text in all languages. To generate the language model features, we train SRILM (Stolcke, 2002) trigram models with modified Kneser-Ney discounting on the supplied monolin</context>
</contexts>
<marker>Atserias, Casas, Comelles, González, Padró, Padró, 2006</marker>
<rawString>J. Atserias, B. Casas, E. Comelles, M. González, L. Padró, and M. Padró. 2006. Freeling 1.3: Syntactic and semantic services in an open-source NLP library. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</booktitle>
<contexts>
<context position="1347" citStr="Banerjee and Lavie, 2005" startWordPosition="186" endWordPosition="189">participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. We believ</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="10206" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1724" endWordPosition="1727">tion Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, w=1.0 w=0.8 w=0.2 1 w=0.2 s=0.5 s=0.5 s=1.0 s=1.0 w=1.0 w=0.8 w=0.1 (a) The matching problem w=1.0 w=0.8 w=0.2 1 w=0.2 w=1.0 w=0.6 w=0.2 w=0.1 w=1.0 w=0.8 w=0.1 S Precision = E jyWj S Recall = Ei xW i 356 the phrases good morning and hello are unrelate</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>M Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="10235" citStr="Callison-Burch et al., 2006" startWordPosition="1728" endWordPosition="1731">xt between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, w=1.0 w=0.8 w=0.2 1 w=0.2 s=0.5 s=0.5 s=1.0 s=1.0 w=1.0 w=0.8 w=0.1 (a) The matching problem w=1.0 w=0.8 w=0.2 1 w=0.2 w=1.0 w=0.6 w=0.2 w=0.1 w=1.0 w=0.8 w=0.1 S Precision = E jyWj S Recall = Ei xW i 356 the phrases good morning and hello are unrelated even with a synonym diction</context>
</contexts>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>C. Callison-Burch, P. Koehn, and M. Osborne. 2006. Improved statistical machine translation using paraphrases. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of WMT.</booktitle>
<contexts>
<context position="14018" citStr="Callison-Burch et al., 2009" startWordPosition="2379" endWordPosition="2382">s for unigrams, bigrams, and trigrams based on similarity functions sms and spos, (2) BPNG F-measures for unigrams, bigrams, and trigrams based on similarity functions slem and spos for each pivot language, and (3) normalized language model scores. In this work, we use two language models. We thus have 3 x 2 features from the BTNGs, 3 x 2 x #pivot languages features from the BPNGs, and 2 features from the language models. Again, we can compute system-level scores by averaging the sentence-level scores. 5 Experiments 5.1 Setup We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al., 2009). The manual judgments from WMT 2008 are used 2Note that the N-gram can span more than one segment. Bonjour , / 0.9 Salut , / 0.1 Querrien / 1.0 . / 1.0 n Pr(p1,p2,...,pn|S) = Z(S)JPr(pi) (2) i=1 357 as the development data and the metric is evaluated on WMT 2009 manual judgments with respect to two criteria: sentence level consistency and system level correlation. The sentence level consistency is defined as the percentage of correctly predicted pairs among all the manually judged pairs. Pairs judged as ties by humans are excluded from the evaluation. The system level correlation is defined a</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1375" citStr="Chan and Ng, 2008" startWordPosition="191" endWordPosition="194">. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. We believe that the lightweight lingu</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cormen</author>
<author>C E Leiserson</author>
<author>R L Rivest</author>
<author>C Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8343" citStr="Cormen et al. (2001)" startWordPosition="1403" endWordPosition="1406">he recall than the precision. 3.3 Scoring The TESLA-M sentence-level score for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams, bigrams, and trigrams based on similarity functions sms and spos. We thus have 3 x 2 = 6 features for TESLA-M. We can compute a system-level score for a machine translation system by averaging its sentencelevel scores over the complete test set. 3.4 Reduction When every xWi and yWj is 1, the linear programming problem proposed above reduces to weighted bipartite matching. This is a well known result; see for example, Cormen et al. (2001) for details. This is the formalism of MaxSim, which precludes the use of fractional weights. If the similarity function is binary-valued and transitive, such as slem and spos, then we can use a much simpler and faster greedy matching procedure: the best match is simply Eg min(Exz=g xWi , Eyz=g yWi ). 4 TESLA Unlike the simple arithmetic average used in TESLA-M, TESLA uses a general linear combination of three types of features: BTNG Fmeasures as in TESLA-M, F-measures between bags of N-grams in each of the pivot languages, called bags of pivot language N-grams (BPNGs), and normalized language</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein, 2001. Introduction to Algorithms. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Giménez</author>
<author>L Màrquez</author>
</authors>
<title>A smorgasbord of features for automatic MT evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third WMT.</booktitle>
<contexts>
<context position="1015" citStr="Giménez and Màrquez, 2008" startWordPosition="137" endWordPosition="140">tion metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic a</context>
</contexts>
<marker>Giménez, Màrquez, 2008</marker>
<rawString>J. Giménez and L. Màrquez. 2008. A smorgasbord of features for automatic MT evaluation. In Proceedings of the Third WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J Blitzer</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="9826" citStr="Haghighi et al., 2009" startWordPosition="1662" endWordPosition="1665"> the form of manual rankings, so we train SVMrank (Joachims, 2006) on these instances to build the linear model. In other scenarios, some form of regression can be more appropriate. The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 4.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree o</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="9270" citStr="Joachims, 2006" startWordPosition="1566" endWordPosition="1567">nlike the simple arithmetic average used in TESLA-M, TESLA uses a general linear combination of three types of features: BTNG Fmeasures as in TESLA-M, F-measures between bags of N-grams in each of the pivot languages, called bags of pivot language N-grams (BPNGs), and normalized language model scores of the system translation, defined as n1 log P, where n is the length of the translation, and P the language model probability. The method of training the linear model depends on the development data. In the case of WMT, the development data is in the form of manual rankings, so we train SVMrank (Joachims, 2006) on these instances to build the linear model. In other scenarios, some form of regression can be more appropriate. The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 4.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligne</context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>T. Joachims. 2006. Training linear svms in linear time. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Empirical methods for compound splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="15200" citStr="Koehn and Knight, 2003" startWordPosition="2571" endWordPosition="2574">The system level correlation is defined as the average Spearman’s rank correlation coefficient across all translation tracks. 5.2 Pre-processing We POS tag and lemmatize the texts using the following tools: for English, OpenNLP POS-tagger3 and WordNet lemmatizer; for French and German, TreeTagger4; for Spanish, the FreeLing toolkit (Atserias et al., 2006); and for Czech, the Morce morphological tagger5. For German, we additionally perform noun compound splitting. For each noun, we choose the split that maximizes the geometric mean of the frequency counts of its parts, following the method in (Koehn and Knight, 2003): � n max N(pi) nrp1rptr...rpn i=1 The resulting compound split sentence is then POS tagged and lemmatized. Finally, we remove all non-alphanumeric tokens from the text in all languages. To generate the language model features, we train SRILM (Stolcke, 2002) trigram models with modified Kneser-Ney discounting on the supplied monolingual Europarl and news commentary texts. We build phrase tables from the supplied news commentary bitexts. Word alignments are produced by the Berkeley aligner. The widely used phrase extraction heuristic in (Koehn et al., 2003) is used to extract phrase pairs and p</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>P. Koehn and K. Knight. 2003. Empirical methods for compound splitting. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="15762" citStr="Koehn et al., 2003" startWordPosition="2657" endWordPosition="2660">s parts, following the method in (Koehn and Knight, 2003): � n max N(pi) nrp1rptr...rpn i=1 The resulting compound split sentence is then POS tagged and lemmatized. Finally, we remove all non-alphanumeric tokens from the text in all languages. To generate the language model features, we train SRILM (Stolcke, 2002) trigram models with modified Kneser-Ney discounting on the supplied monolingual Europarl and news commentary texts. We build phrase tables from the supplied news commentary bitexts. Word alignments are produced by the Berkeley aligner. The widely used phrase extraction heuristic in (Koehn et al., 2003) is used to extract phrase pairs and phrases of up to 4 words are collected. 5.3 Into-English task For each of the BNG features, we generate three scores, for unigrams, bigrams, and trigrams respectively. For BPNGs, we generate one such triple for each of the four pivot languages supplied, namely Czech, French, German, and Spanish. 3opennlp.sourceforge.net 4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger 5ufal.mff.cuni.cz/morce/index.php System Sentence correlation consistency TESLA 0.8993 0.6324 TESLA-M 0.8718 0.6097 ulc 0.83 0.63 maxsim 0.80 0.62 meteor-0.6 0.72 0.50 Table 1: Into-Engli</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="9802" citStr="Liang et al., 2006" startWordPosition="1658" endWordPosition="1661">velopment data is in the form of manual rankings, so we train SVMrank (Joachims, 2006) on these instances to build the linear model. In other scenarios, some form of regression can be more appropriate. The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 4.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>N Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9758" citStr="Och and Ney, 2003" startWordPosition="1650" endWordPosition="1653">development data. In the case of WMT, the development data is in the form of manual rankings, so we train SVMrank (Joachims, 2006) on these instances to build the linear model. In other scenarios, some form of regression can be more appropriate. The rest of this section focuses on the generation of the BPNGs. Their matching is done in the same way as described for BTNGs in the previous section. 4.1 Phrase level semantic representation Given a sentence-aligned bitext between the target language and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the w</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and N. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pado</author>
<author>M Galley</author>
<author>D Jurafsky</author>
<author>C D Manning</author>
</authors>
<title>Robust machine translation evaluation with entailment features.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP.</booktitle>
<contexts>
<context position="979" citStr="Pado et al., 2009" startWordPosition="131" endWordPosition="134">c machine translation evaluation metrics with state-of-the-art performances. TESLA-M builds on the success of METEOR and MaxSim, but employs a more expressive linear programming framework. TESLA further exploits parallel texts to build a shallow semantic representation. We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks. 1 Introduction In recent years, many machine translation (MT) evaluation metrics have been proposed, exploiting varying amounts of linguistic resources. Heavyweight linguistic approaches including RTE (Pado et al., 2009) and ULC (Giménez and Màrquez, 2008) performed the best in the WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and sy</context>
</contexts>
<marker>Pado, Galley, Jurafsky, Manning, 2009</marker>
<rawString>S. Pado, M. Galley, D. Jurafsky, and C.D. Manning. 2009. Robust machine translation evaluation with entailment features. In Proceedings of ACLIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1661" citStr="Papineni et al., 2002" startWordPosition="232" endWordPosition="236">he WMT 2009 shared evaluation task. They exploit an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. We believe that the lightweight linguistic approaches are a good compromise given the current state of computational linguistics research and resources. In this paper, we devise TESLA-M and TESLA, two lightweight approaches to MT evaluation. Specifically: (1) the core features are Fmeasures derived by matching bags of N-g</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popovi´c</author>
<author>H Ney</author>
</authors>
<title>Syntax-oriented evaluation measures for machine translation output.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT.</booktitle>
<marker>Popovi´c, Ney, 2009</marker>
<rawString>M. Popovi´c and H. Ney. 2009. Syntax-oriented evaluation measures for machine translation output. In Proceedings of WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA.</booktitle>
<contexts>
<context position="1705" citStr="Snover et al., 2006" startWordPosition="241" endWordPosition="244">it an extensive array of linguistic features such as parsing, semantic role labeling, textual entailment, and discourse representation, which may also limit their practical applications. Lightweight linguistic approaches such as METEOR (Banerjee and Lavie, 2005), MaxSim (Chan and Ng, 2008), wpF and wpBleu (Popovi´c and Ney, 2009) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute, including lemmatization, part-ofspeech (POS) tagging, and synonym dictionaries. Non-linguistic approaches include BLEU (Papineni et al., 2002) and its variants, TER (Snover et al., 2006), among others. They operate purely at the surface word level and no linguistic resources are required. Although still very popular with MT researchers, they have generally shown inferior performances than the linguistic approaches. We believe that the lightweight linguistic approaches are a good compromise given the current state of computational linguistics research and resources. In this paper, we devise TESLA-M and TESLA, two lightweight approaches to MT evaluation. Specifically: (1) the core features are Fmeasures derived by matching bags of N-grams; (2) both recall and precision are cons</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT.</booktitle>
<contexts>
<context position="10257" citStr="Snover et al., 2009" startWordPosition="1732" endWordPosition="1735">e and a pivot language, we can align the text at the word level using well known tools such as GIZA++ (Och and Ney, 2003) or the Berkeley aligner (Liang et al., 2006; Haghighi et al., 2009). We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase. That is, if two target language phrases are often aligned to the same pivot language phrase, then they can be inferred to be similar in meaning. Similar observations have been made by previous researchers (Bannard and Callison-Burch, 2005; Callison-Burch et al., 2006; Snover et al., 2009). We note here two differences from WordNet synonyms: (1) the relationship is not restricted to the word level only, and (2) the relationship is not binary. The degree of similarity can be measured by the percentage of overlap between the semantic representations. For example, at the word level, w=1.0 w=0.8 w=0.2 1 w=0.2 s=0.5 s=0.5 s=1.0 s=1.0 w=1.0 w=0.8 w=0.1 (a) The matching problem w=1.0 w=0.8 w=0.2 1 w=0.2 w=1.0 w=0.6 w=0.2 w=0.1 w=1.0 w=0.8 w=0.1 S Precision = E jyWj S Recall = Ei xW i 356 the phrases good morning and hello are unrelated even with a synonym dictionary, but they both ver</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of WMT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>