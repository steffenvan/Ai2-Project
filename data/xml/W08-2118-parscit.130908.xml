<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000257">
<title confidence="0.988924">
Context-based Arabic Morphological Analysis for Machine Translation
</title>
<author confidence="0.99246">
ThuyLinh Nguyen
</author>
<affiliation confidence="0.94620625">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.997117">
thuylinh@cs.cmu.edu
</email>
<author confidence="0.996909">
Stephan Vogel
</author>
<affiliation confidence="0.94698525">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998498">
vogel@cs.cmu.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823529411765">
In this paper, we present a novel morphol-
ogy preprocessing technique for Arabic-
English translation. We exploit the Arabic
morphology-English alignment to learn a
model removing nonaligned Arabic mor-
phemes. The model is an instance of
the Conditional Random Field (Lafferty et
al., 2001) model; it deletes a morpheme
based on the morpheme’s context. We
achieved around two BLEU points im-
provement over the original Arabic trans-
lation for both a travel-domain system
trained on 20K sentence pairs and a news
domain system trained on 177K sentence
pairs, and showed a potential improvement
for a large-scale SMT system trained on 5
million sentence pairs.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999741846153846">
Statistical machine translation (SMT) relies heav-
ily on the word alignment model of the source
and the target language. However, there is a
mismatch between a rich morphology language
(e.g Arabic, Czech) and a poor morphology lan-
guage (e.g English). An Arabic source word of-
ten corresponds to several English words. Pre-
vious research has focused on attempting to ap-
ply morphological analysis to machine translation
in order to reduce unknown words of highly in-
flected languages. Nießen and Ney (2004) rep-
resented a word as a vector of morphemes and
gained improvement over word-based system for
</bodyText>
<note confidence="0.723366">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967698">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999067921052632">
German-English translation. Goldwater and Mc-
closky (2005) improved Czech-English translation
by applying different heuristics to increase the
equivalence of Czech and English text.
Specially for Arabic-English translation, Lee
(2004) used the Arabic part of speech and English
parts of speech (POS) alignment probabilities to
retain an Arabic affix, drop it from the corpus or
merge it back to a stem. The resulting system
outperformed the original Arabic system trained
on 3.3 million sentence pairs corpora when using
monotone decoding. However, an improvement
in monotone decoding is no guarantee for an im-
provement over the best baseline achievable with
full word forms. Our experiments showed that an
SMT phrase-based translation using 4 words dis-
tance reordering could gain four BLEU points over
monotone decoding. Sadat and Habash (2006) ex-
plored a wide range of Arabic word-level prepro-
cessing and produced better translation results for
a system trained on 5 million Arabic words.
What all the above methodologies do not pro-
vide is a means to disambiguate morphologi-
cal analysis for machine translation based on the
words’ contexts. That is, for an Arabic word anal-
ysis of the form prefix*-stem-suffix* a morpheme
only is either always retained, always dropped off
or always merged to the stem regardless of its
surrounding text. In the example in Figure (1),
the Arabic word “AlnAfi*h”(“window” in English)
was segmented as “Al nAfi* ap”. The morpheme
“ap” is removed so that “Al nAfi*” aligned to “the
window” of the English sentence. In the sentence
“hl ldyk mqAEd bjwAr AlnAf*h ?” (“do you have
window tables ?” in English) the word “AlnAfi*h”
is also segmented as “Al nAfi* ap”. But in this
sentence, morphological preprocessing should re-
move both “Al” and “ap” so that only the remain-
</bodyText>
<page confidence="0.979408">
135
</page>
<reference confidence="0.267424">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 135–142
Manchester, August 2008
</reference>
<bodyText confidence="0.848012666666667">
nryd mA}dh bjAnb AlnAf*h .
we want to have a table near the window .
nu riyd u mA}id ap bi jAnib Al nAfi* .
</bodyText>
<figureCaption confidence="0.991601">
Figure 1: (a) Romanization of original Arabic sentence, (b) Output of morphological analysis toolkit—
words are separated by ‘ ’, (c) English translation and its alignment with full morphological analysis
</figureCaption>
<bodyText confidence="0.975582185185185">
(d) Morphological analysis after removing unaligned morphemes.
ing morpheme “nAfi*” aligned to the word “win-
dow” of the English translation. Thus an appropri-
ate preprocessing technique should be guided by
English translation and bring the word context into
account.
In this paper we describe a context-based mor-
phological analysis for Arabic-English translation
that take full account morphemes alignment to En-
glish text. The preprocessing uses the Arabic mor-
phology disambiguation in (Smith et al., 2005) for
full morphological analysis and learns the remov-
ing morphemes model based on the Viterbi align-
ment of English to full morphological analysis. We
tested the model with two training corpora of 5.2
millions Arabic words(177K sentences) in news
domain and 159K Arabic words (20K sentences)
in travel conversation domain and gain improve-
ment over the original Arabic translation in both
experiments. The system that trained on a sub-
sample corpora of 5 millions sentence pairs cor-
pora also showed one BLEU score improvement
over the original Arabic system on unseen test set.
We will explain our technique in the next section
and briefly review the phrase based SMT model in
section 3. The experiment results will be presented
in section 4.
</bodyText>
<sectionHeader confidence="0.993692" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999987549019608">
We first preprocess the Arabic training corpus and
segment words into morpheme sequences of the
form prefix* stem suffix*. Stems are verbs, adjec-
tives, nouns, pronouns, etc., carrying the content
of the sentence. Prefixes and suffixes are func-
tional morphemes such as gender and case mark-
ers, prepositions, etc. Because case makers do not
exist in English, we remove case marker suffixes
from the morphology output. The output of this
process is a full morphological analysis corpus.
Even after removing case markers, the token count
of the full morphology corpus still doubles the
original Arabic’s word token count and is approx-
imately 1.7 times the number of tokens of the En-
glish corpus. As stated above, using original Ara-
bic for translation introduces more unknown words
in test data and causes multiple English words to
map to one Arabic word. At the morpheme level,
an English word would correspond to a morpheme
in the full morphology corpus but some prefixes
and suffixes in the full morphology corpus may not
be aligned with any English words at all. For ex-
ample, the Arabic article “Al” (“the” in English)
prefixes to both adjectives and nouns, while En-
glish has only one determiner in a simple noun
phrase. Using the full morphological analysis cor-
pus for translation would introduce redundant mor-
phemes in the source side.
The goal of our morphological analysis method
for machine translation is removing nonaligned
prefixes and suffixes from the full morphology cor-
pus using a data-driven approach. We use the word
alignment output of the full morphology corpus to
the English corpus to delete morphemes in a sen-
tence. If an affix is not aligned to an English word
in the word alignment output, the affix should be
removed from the morphology corpus for better
one-to-one alignment of source and target corpora.
However, given an unseen test sentence, the En-
glish translation of the sentence is not available to
remove affixes based on the word alignment out-
put. We therefore learn a model removing non-
aligned morphemes from the full morphology Ara-
bic training corpus and its alignment to the English
corpus. To obtain consistency between training
corpus and test set, we applied the model to both
Arabic training corpus and test set, obtaining pre-
processed morphology corpora for the translation
task.
In this section, we will explain in detail each
steps of our preprocessing methodology:
</bodyText>
<page confidence="0.996109">
136
</page>
<listItem confidence="0.999573583333333">
• Apply word segmentation to the Arabic train-
ing corpus to get the full morphological anal-
ysis corpus.
• Annotate the full morphological analysis cor-
pus based on its word alignment to the En-
glish training corpus. We tag a morpheme as
“Deleted” if it should be removed from the
corpus, and “Retained” otherwise.
• Learn the morphology tagger model.
• Apply the model to both Arabic training cor-
pus and Arabic test corpus to get prepro-
cessed corpus for translation.
</listItem>
<subsectionHeader confidence="0.991701">
2.1 Arabic Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999995863636364">
Smith et al. (2005) applies a source-channel model
to the problem of morphology disambiguation.
The source model is a uniform model that de-
fines the set of analyses. For Arabic morphology
disambiguation, the source model uses the list of
un-weighted word analyses generated by BAMA
toolkit (Buckwalter, 2004). The channel model
disambiguates the morphology alternatives. It is a
log-linear combination of features, which capture
the morphemes’ context including tri-gram mor-
pheme histories, tri-gram part-of-speech histories
and combinations of the two.
The BAMA toolkit and hence (Smith et al.,
2005) do not specify if a morpheme is an affix or
a stem in the output. Given a segmentation of an
original Arabic word, we considered a morpheme
ai as a stem if its parts of speech pi is either a
noun, pronoun, verb, adjective, question, punctua-
tion, number or abbreviation. A morpheme on the
left of its word’s stem is a prefix and it is a suffix
if otherwise. We removed case marker morphemes
and got the full morphology corpus.
</bodyText>
<subsectionHeader confidence="0.999051">
2.2 Annotate Morphemes
</subsectionHeader>
<bodyText confidence="0.9964747">
To extract the Arabic morphemes that align to
English text, we use English as the source cor-
pus and aligned to Arabic morpheme corpus us-
ing GIZA++ (Och and Ney, 2003) toolkit. The
IBM3 and IBM4 (Brown et al., 1994) word align-
ment models select each word in the source sen-
tence, generate fertility and a list of target words
that connect to it. This generative process would
constrain source words to find alignments in the
target sentence. Using English as source corpus,
the alignment models force English words to gen-
erate their alignments in the Arabic morphemes.
GIZA++ outputs Viterbi alignment for every sen-
tence pair in the training corpus as depicted in (b)
and (c) of Figure (1). In our experiment, only 5%
of English words are not aligned to any Arabic
morpheme in the Viterbi alignment. From Viterbi
English-morpheme alignment output, we annotate
morphemes either to be deleted or retained as fol-
lows:
</bodyText>
<listItem confidence="0.998404666666667">
• Annotate stem morphemes as “Retained”(R),
in dependant of word alignment output.
• Annotate a prefix or a suffix as “Retained” (R)
if it is aligned to an English word.
• Annotate a prefix or a suffix as “Deleted” (D)
if it is not aligned to an English word.
</listItem>
<bodyText confidence="0.999904066666667">
Note that the model does not assume that
GIZA++ outputs accurate word alignments. We
lessen the impact of the GIZA++ errors by only
using the word alignment output of prefix and suf-
fix morphemes.
Furthermore, because the full morphology sen-
tence is longer, each English word could align to a
separate morpheme. Our procedure of annotating
morphemes also constrains morphemes tagged as
“Retained” to be aligned to English words. Thus
if we remove “Deleted” morphemes from the mor-
phology corpus, the reduced corpus and English
corpus have the property of one-to-one mapping
we prefer for source-target corpora in machine
translation.
</bodyText>
<subsectionHeader confidence="0.982738">
2.3 Reduced Morphology Model
</subsectionHeader>
<bodyText confidence="0.999974210526316">
The reduced morphology corpus would be the
best choice of morphological analysis for machine
translation. Because it is impossible to tag mor-
phemes of a test sentence without the English ref-
erence based on Viterbi word alignment, we need
to learn a morpheme tagging model. The model
estimates the distributions of tagging sequences
given a morphologically analysed sentence using
the previous step’s annotated training data.
The task of tagging morphemes to be either
“Deleted” or “Retained” belongs to the set of se-
quence labelling problems. The conditional ran-
dom fields (CRF) (Lafferty et al., 2001) model has
shown great benefits in similar applications of nat-
ural language processing such as part-of-speech
tagging, noun phrase chunking (Sha and Pereira,
2003), morphology disambiguation(Smith et al.,
2005). We apply the CRF model to our morpheme
tagging problem.
</bodyText>
<page confidence="0.991019">
137
</page>
<bodyText confidence="0.99974925">
Let A = {(A, T)} be the full morphology train-
ing corpus where A = a1Sp1 a2Sp2 ... amSpm is a
morphology Arabic sentence, ai is a morpheme in
the sentence and pi is its POS; T = t1 t2 ... tm is
the tag sequence of A, each ti is either “Deleted”
or “Retained” . The CRF model estimates param-
eter θ* maximizing the conditional probability of
the sequences of tags given the observed data:
</bodyText>
<equation confidence="0.997042333333333">
θ* = argmax
θ
p� ((A, T)) log p (TSA, θ)
</equation>
<bodyText confidence="0.9996088">
where p� ((A, T)) is the empirical distribution of
the sentence (A, T) in the training data, θ are the
model parameters. The model’s log conditional
probability log p (TSA, θ) is the linear combina-
tion of feature weights:
</bodyText>
<equation confidence="0.99633">
log p (TSA, θ) = � θkfk ((Aq, Tq)) (2)
k
</equation>
<bodyText confidence="0.999977466666667">
The feature functions {fk} are defined on any sub-
set of the sentence Aq C A and Tq C T. CRFs
can accommodate many closely related features
of the input. In our morpheme tagging model,
we use morpheme features, part-of-speech features
and combinations of both. The features capture
the local contexts of morphemes. The lexical mor-
pheme features are the combinations of the current
morpheme and up to 2 previous and 2 following
morphemes. The part-of-speech features are the
combinations of the current part of speech and up
to 3 previous part of speeches. The part of speech,
morpheme combination features capture the de-
pendencies of current morphemes and up to its 3
previous parts of speech.
</bodyText>
<subsectionHeader confidence="0.993427">
2.4 Preprocessed Data
</subsectionHeader>
<bodyText confidence="0.9999965">
Given a full morphology sentence A, we use the
morpheme tagging model learnt as described in the
previous section to decode A into the most proba-
ble sequence of tags T* = t1 t2 ... tm.
</bodyText>
<equation confidence="0.988672">
T* = argmax
T Pr (3)
�TSA, θ*�
</equation>
<bodyText confidence="0.999924818181818">
If a ti is “Deleted”, the morpheme ai is removed
from the morphology sentence A. The same pro-
cedure is applied to both training Arabic corpus
and test corpus to get preprocessed data for transla-
tion. We call a morphology sentence after remov-
ing “Deleted” tag a reduced morphology sentence.
In our experiments, we used the freely available
CRF++1 toolkit to train and decode with the mor-
pheme tagging model. The CRF model smoothed
the parameters by assigning them Gaussian prior
distributions.
</bodyText>
<sectionHeader confidence="0.981626" genericHeader="method">
3 Phrase-based SMT System
</sectionHeader>
<bodyText confidence="0.999983">
We used the open source Moses (Koehn, 2007)
phrase-based MT system to test the impact of the
preprocessing technique on translation results. We
kept the default parameter settings of Moses for
translation model generation. The system used the
“grow-diag-final” alignment combination heuris-
tic. The phrase table consisted of phrase pairs up to
seven words long. The system used a tri-gram lan-
guage model built from SRI (Stolcke, 2002) toolkit
with modified Kneser-Ney interpolation smooth-
ing technique (Chen and Goodman, 1996). By de-
fault, the Moses decoder uses 6 tokens distance re-
ordering windows.
</bodyText>
<sectionHeader confidence="0.996068" genericHeader="evaluation">
4 Experiment Results
</sectionHeader>
<bodyText confidence="0.9988655">
In this section we present experiment results using
our Arabic morphology preprocessing technique.
</bodyText>
<subsectionHeader confidence="0.992734">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999993666666667">
We tested our morphology technique on a small
data set of 20K sentence pairs and a medium size
data set of 177K sentence pairs.
</bodyText>
<subsubsectionHeader confidence="0.416799">
4.1.1 BTEC Data
</subsubsectionHeader>
<bodyText confidence="0.999215333333333">
As small training data set we used the BTEC
corpus (Takezawa et al., 2002) distributed by
the International Workshop on Spoken Language
Translation (IWSLT) (Eck and Hori, 2005). The
corpus is a collection of conversation transcripts
from the travel domain. Table 1 gives some de-
</bodyText>
<table confidence="0.9989268">
Arabic Eng
Ori Full Reduced
Sentences 19972
Tokens 159K 258K 183K 183K
Types 17084 8207 8207 7298
</table>
<tableCaption confidence="0.999857">
Table 1: BTEC corpus statistics
</tableCaption>
<bodyText confidence="0.9928005">
tails for this corpus, which consists of nearly 20K
sentence pairs with lower case on the English side.
There is an imbalance of word types and word to-
kens between original Arabic and English. The
</bodyText>
<footnote confidence="0.658633">
1http://crfpp.sourceforge.net/
</footnote>
<equation confidence="0.8111865">
� (1)
(A,T)EA
</equation>
<page confidence="0.99061">
138
</page>
<bodyText confidence="0.999831894736842">
original Arabic sentences are on average shorter
than the English sentences whereas the Arabic vo-
cabulary is more than twice the size of the English
vocabulary. The word segmentation reduced the
number of word types in the corpus to be closed
to English side but also increased word tokens
quite substantially. By removing nonaligned mor-
phemes, the reduced corpus is well balanced with
the English corpus.
The BTEC experiments used the 2004 IWSLT
Evaluation Test set as development set and 2005
IWSLT Evaluation Test set as unseen test data.
Table 2 gives the details of the two test sets. Both
of them had 16 reference translations per source
sentence. The English side of the training corpus
was used to build the language model. To optimize
the parameters of the decoder, we performed min-
imum error rate training on IWSLT04 optimizing
for the IBM-BLEU metric (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.45213">
4.1.2 Newswire Corpora
</subsectionHeader>
<bodyText confidence="0.97020325">
We also tested the impact of our morphology
technique on parallel corpus in the news domain.
The corpora were collected from LDC’s full Ara-
bic news translation corpora and a small portion
of UN data. The details of the data are give in
Table 3. The data consists of 177K sentence pairs,
5.2M words on the Arabic and 6M words on the
English side.
</bodyText>
<table confidence="0.9989096">
Arabic Eng
Ori Full Reduced
Sentences 177035
Tokens 5.2M 9.3M 6.2M 6.2M
Types 155K 47K 47K 68K
</table>
<tableCaption confidence="0.999848">
Table 3: Newswire corpus statistics
</tableCaption>
<bodyText confidence="0.999968">
We used two test sets from past NIST evalua-
tions as test data. NIST MT03 was used as devel-
opment set for optimizing parameters with respect
to the IBM-BLEU metric, NIST MT06 was used
as unseen test set. Both test sets have 4 references
per test sentence. Table 4 describes the data statis-
tics of the two test sets. All Newswire translation
experiments used the same language model esti-
mated from 200 million words collected from the
Xinhua section of the GIGA word corpus.
</bodyText>
<subsectionHeader confidence="0.9413065">
4.2 Translation Results
4.2.1 BTEC
</subsectionHeader>
<bodyText confidence="0.9981644">
We evaluated the machine translation accord-
ing to the case-insensitive BLEU metric. Table 5
shows the BTEC results when translated with de-
fault Moses setting of distance-based reordering
window size 6. The original Arabic word trans-
lation was the baseline of the evaluation. The
second row contains translation scores using the
full morphology translation. Our new technique of
context-based morphological analysis is shown in
the last row.
</bodyText>
<table confidence="0.9729615">
IWSLT04 IWSLT05
Ori 58.20 54.50
Full 58.55 55.87
Reduced 60.28 56.03
</table>
<tableCaption confidence="0.949932">
Table 5: BTEC translations results on IBM-BLEU
</tableCaption>
<bodyText confidence="0.99858895">
metrics(Case insensitive and 6 tokens distance re-
ordering window). The boldface marks scores sig-
nificantly higher than the original Arabic transla-
tion scores.
The full morphology translation performed sim-
ilar to the baseline on the development set but
outperformed the baseline on the unseen test set.
The reduced corpus showed significant improve-
ments over the baseline on the development set
(IWSLT04) and gave an additional small improve-
ment over the full morphology score over the un-
seen data (IWSLT05).
So why did the reduced morphology translation
not outperform more significantly the full mor-
phology translation on unseen set IWSLT05? To
analysis this in more detail, we selected good
full morphology translations and compared them
with the corresponding reduced morphology trans-
lations. Figure 2 shows one of these examples.
Typically, the reduced morphology translations
</bodyText>
<figureCaption confidence="0.846445">
Figure 2: An example of BTEC translation output.
</figureCaption>
<bodyText confidence="0.99788325">
are shorter than both the references and the full
morphology outputs. Table 2 shows that for the
IWSLT05 test set, the ratio of the average En-
glish reference sentence length and the source sen-
</bodyText>
<page confidence="0.9956">
139
</page>
<table confidence="0.999227285714286">
IWSLT04 (Dev set) IWSLT05 (Unseen set)
Arabic English English
Arabic
Ori Full Reduced Ori Full Reduced
Sentences 500 8000 506 8096
Words 3261 5243 3732 64896 3253 5155 3713 66286
Avg Sent Length 6.52 10.48 7.46 8.11 6.43 10.19 7.34 8.18
</table>
<tableCaption confidence="0.995869">
Table 2: BTEC test set statistics
</tableCaption>
<table confidence="0.999849857142857">
MT03 (Dev set) MT06 (Unseen set)
Arabic English English
Arabic
Ori Full Reduced Ori Full Reduced
Sentences 663 2652 1797 7188
Words 16268 27888 18888 79163 41059 71497 48716 222750
Avg Sent Length 24.53 42.06 28.49 29.85 22.85 39.79 27.1 30.98
</table>
<tableCaption confidence="0.999971">
Table 4: Newswire test set statistics
</tableCaption>
<bodyText confidence="0.999422894736842">
tence length is slightly higher than the correspond-
ing ratio for IWSLT04. Using the parameters op-
timised for IWSLT04 to translate IWSLT05 sen-
tences would generate hypotheses slightly shorter
than the IWSLT05 references resulting in brevity
penalties in the BLEU metric. The IWSLT05
brevity penalties for original Arabic, reduced mor-
phology and full morphology are 0.969, 0.978 and
0.988 respectively. Note that the BTEC corpus and
test sets are in the travel conversation domain, the
English reference sentences contain a large num-
ber of high frequency words. The full morpho-
logical analysis with additional prefixes and suf-
fixes outputs longer translations containing high
frequency words resulting in a high n-gram match
and lower BLEU brevity penalty. The reduced
translation method could generate translations that
are comparable but do not have the same effect on
BLEU metrics.
</bodyText>
<subsectionHeader confidence="0.792229">
4.2.2 Newswire results
</subsectionHeader>
<bodyText confidence="0.99878325">
Table 6 presents the translation results for the
Newswire corpus. Even though morphology seg-
mentation reduced the number of unseen words,
the translation results of full morphological anal-
ysis are slightly lower than the original Arabic
scores in both development set MT03 and unseen
test set MT06. This is consistent with the result
achieved in previous literature (Sadat and Habash,
2006). Morphology preprocessing only helps with
small corpora, but the advantage decreases for
larger data sets.
Our context dependent preprocessing technique
</bodyText>
<table confidence="0.9915545">
MT03 MT06
Ori 45.55 32.09
Full 45.30 31.54
Reduced 47.69 34.13
</table>
<tableCaption confidence="0.929937">
Table 6: Newswire translation results on IBM-
</tableCaption>
<bodyText confidence="0.979212727272727">
BLEU metrics(Case insensitive and 6 tokens dis-
tance reordering wondow). The boldface marks
scores significantly higher than the original Arabic
translation’s scores.
shows significant improvements on both develop-
ment and unseen test sets. Moreover, while the ad-
vantage of morphology segmentation diminishes
for the full morphology translation, we achieve an
improvement of more than two BLEU points over
the original Arabic translations in both develop-
ment set and unseen test set.
</bodyText>
<subsectionHeader confidence="0.999461">
4.3 Unknown Words Reduction
</subsectionHeader>
<bodyText confidence="0.999966625">
A clear advantage of using morphology based
translation over original word translation is the
reduction in the number of untranslated words.
Table 7 compares the number of unknown Arabic
tokens for original Arabic translation and reduced
morphology translation. In all the test sets, mor-
phology translations reduced the number of un-
known tokens by more than a factor of two.
</bodyText>
<subsectionHeader confidence="0.946609">
4.4 The Impact of Reordering Distance Limit
</subsectionHeader>
<bodyText confidence="0.999884666666667">
The reordering window length is determined based
on the movements of the source phrases. On an
average, an original Arabic word has two mor-
</bodyText>
<page confidence="0.987773">
140
</page>
<table confidence="0.999807857142857">
Reorder Window 0 2 3 4 5 6 7 8 9
IWSLT04 Ori 57.21 57.92 58.01 58.31 58.16 58.20 58.20 58.12 58.01
Full 56.89 57.54 58.62 58.39 58.32 58.55 58.55 58.55 58.57
Reduced 58.36 59.56 60.05 60.70 60.32 60.28 60.46 60.30 60.55
MT03 Ori 41.75 43.84 45.24 45.61 45.40 45.55 45.21 45.22 45.19
Full 41.45 43.12 44.32 44.71 45.30 45.80 45.88 45.82
Reduced 44.08 45.28 46.50 47.40 47.41 47.69 47.59 47.75 47.79
</table>
<tableCaption confidence="0.981538">
Table 8: The impact of reordering limits on BTEC ’s development set IWSLT04 and Newswire’s devel-
opment set MT03. The translation scores are IBM-BLEU metric
</tableCaption>
<table confidence="0.9995104">
Test Set Ori Reduced
IWSLT04 242 100
IWSLT05 219 97
MT03 1463 553
MT06 3734 1342
</table>
<tableCaption confidence="0.999278">
Table 7: Unknown tokens count
</tableCaption>
<bodyText confidence="0.9995059375">
phemes. The full morphology translation with a
6-word reordering window has the same impact
as a 3-word reordering when translating the orig-
inal Arabic. To fully benefit from word reorder-
ing, the full morphology translation requires a
longer reorder distance limit. However, in current
phrase based translations, reordering models are
not strong enough to guide long distance source-
word movements. This shows an additional advan-
tage of the nonaligned morpheme removal tech-
nique.
We carried out experiments from monotone de-
coding up to 9 word distance reordering limit for
the two development sets IWSLT04 and MT03.
The results are given in Table 8. The BTEC data
set does not benefit from a larger reordering win-
dow. Using only a 2-word reordering window
the score of the original Arabic translations(57.92)
was comparable to the best score (58.31) obtained
by using a 4-word reordering window. On the
other hand, the reordering limit showed a signifi-
cant impact on Newswire data. The MT03 original
Arabic translation using a 4-word re-ordering win-
dow resulted in an improvement of 4 BLEU points
over monotone decoding. Large Arabic corpora
usually contain data from the news domain. The
decoder might not effectively reorder very long
distance morphemes for these data sets. This ex-
plains why machine translation does not benefit
from word-based morphological segmentation for
large data sets which adequately cover the vocabu-
lary of the test set.
</bodyText>
<subsectionHeader confidence="0.995964">
4.5 Large Training Corpora Results
</subsectionHeader>
<bodyText confidence="0.9999595">
We wanted to test the impact of our preprocess-
ing technique on a system trained on 5 million
sentence pairs (128 million Arabic words). Un-
fortunately, the CRF++ toolkit exceeded memory
limits when executed even on a 24GB server. We
created smaller corpora by sub-sampling the large
corpus for the source side of MT03 and MT06
test sets. The sub-sampled corpus have 500K sen-
tence pairs and cover all source phrases of MT03
and MT06 which can be found in the large cor-
pus. In these experiments, we used a lexical re-
ordering model into translation model. The lan-
guage model was the 5-gram SRI language model
built from the whole GIGA word corpus. Table 9
</bodyText>
<table confidence="0.9977374">
MT03 MT06
5M Ori 56.22 42.17
Sub-sample Ori 54.54 41.59
Sub-sample Full 51.47 40.84
Sub-sample Reduced 54.78 43.20
</table>
<tableCaption confidence="0.982149">
Table 9: Translation results of large corpora(Case
</tableCaption>
<bodyText confidence="0.980538571428572">
insensitive, IBM-BLEU metric). The boldface
marks score significantly higher than the original
Arabic translation score.
presents the translation result of original Arabic
system trained on the full 5M sentence pairs cor-
pus and the three systems trained on the 500K sen-
tence pairs sub-sampled corpus. The sub-sampled
full morphology system scores degraded for both
development set and unseen test set. On devel-
opment set, the sub-sampled reduced morphology
system score was slightly better than baseline. On
the unseen test set, it significantly outperformed
both the baseline on sub-sampled training data and
even outperformed the system trained on the entire
</bodyText>
<page confidence="0.980304">
141
</page>
<figure confidence="0.684315">
5M sentence pairs.
5 Conclusion and Future Work
</figure>
<bodyText confidence="0.9754246">
Lafferty, John, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. In Proceed-
ings of 18th ICML, pages 282–289.
In this paper, we presented a context-dependent
morphology preprocessing technique for Arabic-
English translation. The model significantly out-
performed the original Arabic systems on small
and mid-size corpora and unseen test set on large
training corpora. The model treats morphology
processing task as a sequence labelling problem.
Therefore, other machine learning techniques such
as perceptron (Collins, 2002) could also be applied
for this problem.
The paper also discussed the relation between
the size of the reordering window and morphol-
ogy processing. In future investigations, we plan
to extend the model such that merging morphemes
is included. We also intent to study the impact of
phrase length and phrase extraction heuristics.
</bodyText>
<sectionHeader confidence="0.987618" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.999962166666667">
We thank Noah Smith for useful comments and
suggestions and providing us with the morphol-
ogy disambiguation toolkit. We also thank Sameer
Badaskar for help on editing the paper. We also
thank anonymous reviewers for helpful comments.
The research was supported by the GALE project.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999920175438597">
Brown, Peter F., Stephen Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The Mathematic of Statisti-
cal Machine Translation: Parameter Estimation. Compu-
tational Linguistics, 19(2):263–311.
Buckwalter, T. 2004. Arabic Morphological Analyzer ver-
sion 2.0. LDC2004L02.
Chen, Stanley F. and Joshua Goodman. 1996. An Empirical
Study of Smoothing Techniques for Language Modeling.
In Proceedings of the ACL, pages 310–318.
Collins, Michael. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proceedings of EMNLP ’02,
pages 1–8.
Eck, M. and C. Hori. 2005. Overview of the IWSLT 2005
Evaluation Campaign. In Proceedings of IWSLT, pages
11–17.
Goldwater, Sharon and David Mcclosky. 2005. Improv-
ing Statistical MT through Morphological Analysis. In
Proceedings of HLT/EMNLP, pages 676–683, Vancouver,
British Columbia, Canada.
Koehn, et al. 2007. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Annual Meeting of ACL,
demonstration session.
Lee, Young S. 2004. Morphological Analysis for Statistical
Machine Translation. In HLT-NAACL 2004: Short Papers,
pages 57–60, Boston, Massachusetts, USA.
Nießen, Sonja and Hermann Ney. 2004. Statistical Ma-
chine Translation with Scarce Resources Using Morpho-
Syntactic Information. Computational Linguistics, 30(2),
June.
Och, Franz Josef and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of the 40th ACL,
pages 311–318, Philadelphia.
Sadat, Fatiha and Nizar Habash. 2006. Combination of Ara-
bic Preprocessing Schemes for Statistical Machine Trans-
lation. In Proceedings of the ACL, pages 1–8, Sydney,
Australia. Association for Computational Linguistics.
Sha, Fei and Fernando Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of NAACL
’03, pages 134–141, Morristown, NJ, USA.
Smith, Noah A., David A. Smith, and Roy W. Tromble. 2005.
Context-Based Morphological Disambiguation with Ran-
dom Fields. In Proceedings of HLT/EMNLP, pages 475–
482, Vancouver, British Columbia, Canada, October. As-
sociation for Computational Linguistics.
Stolcke, A. 2002. SRILM – an Extensible Language Model-
ing Toolkit. In Intl. Conf. on Spoken Language Process-
ing.
Takezawa, Toshiyuki, Eiichiro Sumita, Fumiaki Sugaya, Hi-
rofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward
a Broad-Coverage Bilingual Corpus for Speech Transla-
tion of Travel Conversations in the Real World. In Pro-
ceedings ofLREC2002, pages 147–152.
</reference>
<page confidence="0.997728">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.778357">
<title confidence="0.999595">Context-based Arabic Morphological Analysis for Machine Translation</title>
<author confidence="0.998267">ThuyLinh Nguyen</author>
<affiliation confidence="0.998912">Language Technologies Institute School of Computer Science Carnegie Mellon University</affiliation>
<address confidence="0.998034">Pittsburgh, PA 15213, USA</address>
<email confidence="0.999653">thuylinh@cs.cmu.edu</email>
<author confidence="0.956401">Stephan</author>
<affiliation confidence="0.955290666666667">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.953207">Pittsburgh, PA 15213,</address>
<email confidence="0.994667">vogel@cs.cmu.edu</email>
<abstract confidence="0.9992215">In this paper, we present a novel morphology preprocessing technique for Arabic- English translation. We exploit the Arabic morphology-English alignment to learn a model removing nonaligned Arabic morphemes. The model is an instance of the Conditional Random Field (Lafferty et al., 2001) model; it deletes a morpheme based on the morpheme’s context. We achieved around two BLEU points improvement over the original Arabic translation for both a travel-domain system trained on 20K sentence pairs and a news domain system trained on 177K sentence pairs, and showed a potential improvement for a large-scale SMT system trained on 5 million sentence pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>135--142</pages>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 135–142</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manchester</author>
</authors>
<date>2008</date>
<marker>Manchester, 2008</marker>
<rawString>Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<date>1994</date>
<booktitle>The Mathematic of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>Brown, Peter F., Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1994. The Mathematic of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Arabic Morphological Analyzer version 2.0.</title>
<date>2004</date>
<pages>2004--02</pages>
<marker>Buckwalter, 2004</marker>
<rawString>Buckwalter, T. 2004. Arabic Morphological Analyzer version 2.0. LDC2004L02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>310--318</pages>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, Stanley F. and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modeling. In Proceedings of the ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP ’02,</booktitle>
<pages>1--8</pages>
<marker>Collins, 2002</marker>
<rawString>Collins, Michael. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP ’02, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eck</author>
<author>C Hori</author>
</authors>
<title>Evaluation Campaign.</title>
<date>2005</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proceedings of IWSLT,</booktitle>
<pages>11--17</pages>
<marker>Eck, Hori, 2005</marker>
<rawString>Eck, M. and C. Hori. 2005. Overview of the IWSLT 2005 Evaluation Campaign. In Proceedings of IWSLT, pages 11–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David Mcclosky</author>
</authors>
<title>Improving Statistical MT through Morphological Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>676--683</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1866" citStr="Goldwater and Mcclosky (2005)" startWordPosition="264" endWordPosition="268">h) and a poor morphology language (e.g English). An Arabic source word often corresponds to several English words. Previous research has focused on attempting to apply morphological analysis to machine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with</context>
</contexts>
<marker>Goldwater, Mcclosky, 2005</marker>
<rawString>Goldwater, Sharon and David Mcclosky. 2005. Improving Statistical MT through Morphological Analysis. In Proceedings of HLT/EMNLP, pages 676–683, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koehn</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation. In Annual Meeting of ACL, demonstration session.</title>
<date>2007</date>
<marker>Koehn, 2007</marker>
<rawString>Koehn, et al. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Annual Meeting of ACL, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young S Lee</author>
</authors>
<title>Morphological Analysis for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Short Papers,</booktitle>
<pages>57--60</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="2042" citStr="Lee (2004)" startWordPosition="289" endWordPosition="290">achine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with full word forms. Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding. Sadat and Ha</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Lee, Young S. 2004. Morphological Analysis for Statistical Machine Translation. In HLT-NAACL 2004: Short Papers, pages 57–60, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation with Scarce Resources Using MorphoSyntactic Information.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>2</issue>
<contexts>
<context position="1536" citStr="Nießen and Ney (2004)" startWordPosition="225" endWordPosition="228">irs, and showed a potential improvement for a large-scale SMT system trained on 5 million sentence pairs. 1 Introduction Statistical machine translation (SMT) relies heavily on the word alignment model of the source and the target language. However, there is a mismatch between a rich morphology language (e.g Arabic, Czech) and a poor morphology language (e.g English). An Arabic source word often corresponds to several English words. Previous research has focused on attempting to apply morphological analysis to machine translation in order to reduce unknown words of highly inflected languages. Nießen and Ney (2004) represented a word as a vector of morphemes and gained improvement over word-based system for © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. German-English translation. Goldwater and Mcclosky (2005) improved Czech-English translation by applying different heuristics to increase the equivalence of Czech and English text. Specially for Arabic-English translation, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to r</context>
</contexts>
<marker>Nießen, Ney, 2004</marker>
<rawString>Nießen, Sonja and Hermann Ney. 2004. Statistical Machine Translation with Scarce Resources Using MorphoSyntactic Information. Computational Linguistics, 30(2), June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia.</location>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th ACL, pages 311–318, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of Arabic Preprocessing Schemes for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>1--8</pages>
<institution>Sydney, Australia. Association for Computational Linguistics.</institution>
<contexts>
<context position="2653" citStr="Sadat and Habash (2006)" startWordPosition="385" endWordPosition="388">, Lee (2004) used the Arabic part of speech and English parts of speech (POS) alignment probabilities to retain an Arabic affix, drop it from the corpus or merge it back to a stem. The resulting system outperformed the original Arabic system trained on 3.3 million sentence pairs corpora when using monotone decoding. However, an improvement in monotone decoding is no guarantee for an improvement over the best baseline achievable with full word forms. Our experiments showed that an SMT phrase-based translation using 4 words distance reordering could gain four BLEU points over monotone decoding. Sadat and Habash (2006) explored a wide range of Arabic word-level preprocessing and produced better translation results for a system trained on 5 million Arabic words. What all the above methodologies do not provide is a means to disambiguate morphological analysis for machine translation based on the words’ contexts. That is, for an Arabic word analysis of the form prefix*-stem-suffix* a morpheme only is either always retained, always dropped off or always merged to the stem regardless of its surrounding text. In the example in Figure (1), the Arabic word “AlnAfi*h”(“window” in English) was segmented as “Al nAfi* </context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Sadat, Fatiha and Nizar Habash. 2006. Combination of Arabic Preprocessing Schemes for Statistical Machine Translation. In Proceedings of the ACL, pages 1–8, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL ’03,</booktitle>
<pages>134--141</pages>
<location>Morristown, NJ, USA.</location>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, Fei and Fernando Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of NAACL ’03, pages 134–141, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-Based Morphological Disambiguation with Random Fields.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP,</booktitle>
<pages>475--482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Smith, Noah A., David A. Smith, and Roy W. Tromble. 2005. Context-Based Morphological Disambiguation with Random Fields. In Proceedings of HLT/EMNLP, pages 475– 482, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. on Spoken Language Processing.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. 2002. SRILM – an Extensible Language Modeling Toolkit. In Intl. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World. In Pro-</title>
<date>2002</date>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Takezawa, Toshiyuki, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a Broad-Coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World. In Pro-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>