<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007068">
<title confidence="0.978017">
Design Challenges and Misconceptions in Named Entity Recognition∗ † ‡
</title>
<author confidence="0.998686">
Lev Ratinov Dan Roth
</author>
<affiliation confidence="0.998846">
Computer Science Department
University of Illinois
</affiliation>
<address confidence="0.574013">
Urbana, IL 61801 USA
</address>
<email confidence="0.998748">
{ratinov2,danr}@uiuc.edu
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999540785714286">
We analyze some of the fundamental design
challenges and misconceptions that underlie
the development of an efficient and robust
NER system. In particular, we address issues
such as the representation of text chunks, the
inference approach needed to combine local
NER decisions, the sources of prior knowl-
edge and how to use them within an NER
system. In the process of comparing several
solutions to these challenges we reach some
surprising conclusions, as well as develop an
NER system that achieves 90.8 Fl score on
the CoNLL-2003 NER shared task, the best
reported result for this dataset.
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999614125">
Natural Language Processing applications are char-
acterized by making complex interdependent deci-
sions that require large amounts of prior knowledge.
In this paper we investigate one such application–
Named Entity Recognition (NER). Figure 1 illus-
trates the necessity of using prior knowledge and
non-local decisions in NER. In the absence of mixed
case information it is difficult to understand that
</bodyText>
<footnote confidence="0.78869">
∗ The system and the Webpages dataset are available at:
http://l2r.cs.uiuc.edu/∼cogcomp/software.php
† This work was supported by NSF grant NSF SoD-HCER-
0613885, by MIAS, a DHS-IDS Center for Multimodal In-
formation Access and Synthesis at UIUC and by an NDIIPP
project from the National Library of Congress.
‡ We thank Nicholas Rizzolo for the baseline LBJ NER
system, Xavier Carreras for suggesting the word class models,
and multiple reviewers for insightful comments.
</footnote>
<construct confidence="0.9640903">
SOCCER - [PER BLINKER] BAN LIFTED.
[LOC LONDON] 1996-12-06 [MISC Dutch] forward
[PER Reggie Blinker] had his indefinite suspension
lifted by [ORG FIFA] on Friday and was set to make
his [ORG Sheffield Wednesday] comeback against
[ORG Liverpool] on Saturday . [PER Blinker] missed
his club’s last two games after [ORG FIFA] slapped a
worldwide ban on him for appearing to sign contracts for
both [ORG Wednesday] and [ORG Udinese] while he was
playing for [ORG Feyenoord].
</construct>
<figureCaption confidence="0.999203">
Figure 1: Example illustrating challenges in NER.
</figureCaption>
<bodyText confidence="0.99996925">
“BLINKER” is a person. Likewise, it is not obvi-
ous that the last mention of “Wednesday” is an orga-
nization (in fact, the first mention of “Wednesday”
can also be understood as a “comeback” which hap-
pens on Wednesday). An NER system could take ad-
vantage of the fact that “blinker” is also mentioned
later in the text as the easily identifiable “Reggie
Blinker”. It is also useful to know that Udinese
is a soccer club (an entry about this club appears
in Wikipedia), and the expression “both Wednesday
and Udinese” implies that “Wednesday” and “Udi-
nese” should be assigned the same label.
The above discussion focuses on the need for ex-
ternal knowledge resources (for example, that Udi-
nese can be a soccer club) and the need for non-
local features to leverage the multiple occurrences
of named entities in the text. While these two needs
have motivated some of the research in NER in
the last decade, several other fundamental decisions
must be made. These include: what model to use for
</bodyText>
<page confidence="0.972559">
147
</page>
<note confidence="0.989937">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147–155,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960757575758">
sequential inference, how to represent text chunks
and what inference (decoding) algorithm to use.
Despite the recent progress in NER, the effort has
been dispersed in several directions and there are no
published attempts to compare or combine the re-
cent advances, leading to some design misconcep-
tions and less than optimal performance. In this
paper we analyze some of the fundamental design
challenges and misconceptions that underlie the de-
velopment of an efficient and robust NER system.
We find that BILOU representation of text chunks
significantly outperforms the widely adopted BIO.
Surprisingly, naive greedy inference performs com-
parably to beamsearch or Viterbi, while being con-
siderably more computationally efficient. We ana-
lyze several approaches for modeling non-local de-
pendencies proposed in the literature and find that
none of them clearly outperforms the others across
several datasets. However, as we show, these contri-
butions are, to a large extent, independent and, as we
show, the approaches can be used together to yield
better results. Our experiments corroborate recently
published results indicating that word class models
learned on unlabeled text can significantly improve
the performance of the system and can be an al-
ternative to the traditional semi-supervised learning
paradigm. Combining recent advances, we develop
a publicly available NER system that achieves 90.8
F1 score on the CoNLL-2003 NER shared task, the
best reported result for this dataset. Our system is ro-
bust – it consistently outperforms all publicly avail-
able NER systems (e.g., the Stanford NER system)
on all three datasets.
</bodyText>
<sectionHeader confidence="0.964563" genericHeader="introduction">
2 Datasets and Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.99985825">
NER system should be robust across multiple do-
mains, as it is expected to be applied on a diverse set
of documents: historical texts, news articles, patent
applications, webpages etc. Therefore, we have con-
sidered three datasets: CoNLL03 shared task data,
MUC7 data and a set of Webpages we have anno-
tated manually. In the experiments throughout the
paper, we test the ability of the tagger to adapt to new
test domains. Throughout this work, we train on the
CoNLL03 data and test on the other datasets without
retraining. The differences in annotation schemes
across datasets created evaluation challenges. We
discuss the datasets and the evaluation methods be-
low.
The CoNLL03 shared task data is a subset of
Reuters 1996 news corpus annotated with 4 entity
types: PER,ORG, LOC, MISC. It is important to
notice that both the training and the development
datasets are news feeds from August 1996, while the
test set contains news feeds from December 1996.
The named entities mentioned in the test dataset are
considerably different from those that appear in the
training or the development set. As a result, the test
dataset is considerably harder than the development
set. Evaluation: Following the convention, we re-
port phrase-level F1 score.
The MUC7 dataset is a subset of the North
American News Text Corpora annotated with a wide
variety of entities including people, locations, or-
ganizations, temporal events, monetary units, and
so on. Since there was no direct mapping from
temporal events, monetary units, and other entities
from MUC7 and the MISC label in the CoNLL03
dataset, we measure performance only on PER,ORG
and LOC. Evaluation: There are several sources
of inconsistency in annotation between MUC7 and
CoNLL03. For example, since the MUC7 dataset
does not contain the MISC label, in the sentence
“balloon, called the Virgin Global Challenger” , the
expression Virgin Global Challenger should be la-
beled as MISC according to CoNLL03 guidelines.
However, the gold annotation in MUC7 is “balloon,
called the [ORG Virgin] Global Challenger”. These
and other annotation inconsistencies have prompted
us to relax the requirements of finding the exact
phrase boundaries and measure performance using
token-level F1.
Webpages - we have assembled and manually an-
notated a collection of 20 webpages, including per-
sonal, academic and computer-science conference
homepages. The dataset contains 783 entities (96-
loc, 223-org, 276-per, 188-misc). Evaluation: The
named entities in the webpages were highly am-
biguous and very different from the named entities
seen in the training data. For example, the data in-
cluded sentences such as : “Hear, O Israel, the Lord
our God, the Lord is one.” We could not agree on
whether “O Israel” should be labeled as ORG, LOC,
or PER. Similarly, we could not agree on whether
“God” and “Lord” is an ORG or PER. These issues
</bodyText>
<page confidence="0.997115">
148
</page>
<bodyText confidence="0.99965925">
led us to report token-level entity-identification F1
score for this dataset. That is, if a named entity to-
ken was identified as such, we counted it as a correct
prediction ignoring the named entity type.
</bodyText>
<sectionHeader confidence="0.988102" genericHeader="method">
3 Design Challenges in NER
</sectionHeader>
<bodyText confidence="0.982600634146341">
In this section we introduce the baseline NER sys-
tem, and raise the fundamental questions underlying
robust and efficient design. These questions define
the outline of this paper. NER is typically viewed
as a sequential prediction problem, the typical mod-
els include HMM (Rabiner, 1989), CRF (Lafferty
et al., 2001), and sequential application of Per-
ceptron or Winnow (Collins, 2002). That is, let
x = (x1,... , xN) be an input sequence and y =
(y1, ... , yN) be the output sequence. The sequential
prediction problem is to estimate the probabilities
P(yi|xi−k . . . xi+l, yi−m . . . yi−1),
where k, l and m are small numbers to allow
tractable inference and avoid overfitting. This con-
ditional probability distribution is estimated in NER
using the following baseline set of features (Zhang
and Johnson, 2003): (1) previous two predictions
yi−1 and yi−2 (2) current word xi (3) xi word type
(all-capitalized, is-capitalized, all-digits, alphanu-
meric, etc.) (4) prefixes and suffixes of xi (5) tokens
in the window c = (xi−2, xi−1, xi, xi+1, xi+2) (6)
capitalization pattern in the window c (7) conjunc-
tion of c and yi−1.
Most NER systems use additional features, such
as POS tags, shallow parsing information and
gazetteers. We discuss additional features in the fol-
lowing sections. We note that we normalize dates
and numbers, that is 12/3/2008 becomes *Date*,
1980 becomes *DDDD* and 212-325-4751 becomes
*DDD*-*DDD*-*DDDD*. This allows a degree of ab-
straction to years, phone numbers, etc.
Our baseline NER system uses a regularized aver-
aged perceptron (Freund and Schapire, 1999). Sys-
tems based on perceptron have been shown to be
competitive in NER and text chunking (Kazama and
Torisawa, 2007b; Punyakanok and Roth, 2001; Car-
reras et al., 2003) We specify the model and the fea-
tures with the LBJ (Rizzolo and Roth, 2007) mod-
eling language. We now state the four fundamental
design decisions in NER system which define the
structure of this paper.
</bodyText>
<table confidence="0.996889">
Algorithm Baseline system Final System
Greedy 83.29 90.57
Beam size=10 83.38 90.67
Beam size=100 83.38 90.67
Viterbi 83.71 N/A
</table>
<tableCaption confidence="0.994769333333333">
Table 1: Phrase-level Fl performance of different inference
methods on CoNLL03 test data. Viterbi cannot be used in the
end system due to non-local features.
</tableCaption>
<bodyText confidence="0.95971">
Key design decisions in an NER system.
</bodyText>
<listItem confidence="0.95998875">
1) How to represent text chunks in NER system?
2) What inference algorithm to use?
3) How to model non-local dependencies?
4) How to use external knowledge resources in NER?
</listItem>
<sectionHeader confidence="0.979668" genericHeader="method">
4 Inference &amp; Chunk Representation
</sectionHeader>
<bodyText confidence="0.999952161290323">
In this section we compare the performance of sev-
eral inference (decoding) algorithms: greedy left-
to-right decoding, Viterbi and beamsearch. It may
appear that beamsearch or Viterbi will perform
much better than naive greedy left-to-right decoding,
which can be seen as beamsearch of size one. The
Viterbi algorithm has the limitation that it does not
allow incorporating some of the non-local features
which will be discussed later, therefore, we cannot
use it in our end system. However, it has the appeal-
ing quality of finding the most likely assignment to
a second-order model, and since the baseline fea-
tures only have second order dependencies, we have
tested it on the baseline configuration.
Table 1 compares between the greedy decoding,
beamsearch with varying beam size, and Viterbi,
both for the system with baseline features and for the
end system (to be presented later). Surprisingly, the
greedy policy performs well, this phenmenon was
also observed in the POS tagging task (Toutanova
et al., 2003; Roth and Zelenko, 1998). The impli-
cations are subtle. First, due to the second-order of
the model, the greedy decoding is over 100 times
faster than Viterbi. The reason is that with the
BILOU encoding of four NE types, each token can
take 21 states (O, B-PER, I-PER , U-PER, etc.). To
tag a token, the greedy policy requires 21 compar-
isons, while the Viterbi requires 213, and this analy-
sis carries over to the number of classifier invoca-
tions. Furthermore, both beamsearch and Viterbi
require transforming the predictions of the classi-
</bodyText>
<page confidence="0.996218">
149
</page>
<table confidence="0.9780895">
Rep. CoNLL03 MUC7
Scheme Test Dev Dev Test
BIO 89.15 93.61 86.76 85.15
BILOU 90.57 93.28 88.09 85.62
</table>
<tableCaption confidence="0.922251">
Table 2: End system performance with BILOU and BIO
schemes. BILOU outperforms the more widely used BIO.
</tableCaption>
<bodyText confidence="0.999959090909091">
fiers to probabilities as discussed in (Niculescu-
Mizil and Caruana, 2005), incurring additional time
overhead. Second, this result reinforces the intuition
that global inference over the second-order HMM
features does not capture the non-local properties
of the task. The reason is that the NEs tend to
be short chunks separated by multiple “outside” to-
kens. This separation “breaks” the Viterbi decision
process to independent maximization of assignment
over short chunks, where the greedy policy performs
well. On the other hand, dependencies between iso-
lated named entity chunks have longer-range depen-
dencies and are not captured by second-order tran-
sition features, therefore requiring separate mecha-
nisms, which we discuss in Section 5.
Another important question that has been stud-
ied extensively in the context of shallow parsing and
was somewhat overlooked in the NER literature is
the representation of text segments (Veenstra, 1999).
Related works include voting between several rep-
resentation schemes (Shen and Sarkar, 2005), lex-
icalizing the schemes (Molina and Pla, 2002) and
automatically searching for best encoding (Edward,
2007). However, we are not aware of similar work
in the NER settings. Due to space limitations, we do
not discuss all the representation schemes and com-
bining predictions by voting. We focus instead on
two most popular schemes– BIO and BILOU. The
BIO scheme suggests to learn classifiers that iden-
tify the Beginning, the Inside and the Outside of
the text segments. The BILOU scheme suggests
to learn classifiers that identify the Beginning, the
Inside and the Last tokens of multi-token chunks
as well as Unit-length chunks. The BILOU scheme
allows to learn a more expressive model with only
a small increase in the number of parameters to be
learned. Table 2 compares the end system’s perfor-
mance with BIO and BILOU. Examining the results,
we reach two conclusions: (1) choice of encod-
ing scheme has a big impact on the system perfor-
mance and (2) the less used BILOU formalism sig-
nificantly outperforms the widely adopted BIO tag-
ging scheme. We use the BILOU scheme throughout
the paper.
</bodyText>
<sectionHeader confidence="0.998569" genericHeader="method">
5 Non-Local Features
</sectionHeader>
<bodyText confidence="0.999950285714286">
The key intuition behind non-local features in NER
has been that identical tokens should have identi-
cal label assignments. The sample text discussed
in the introduction shows one such example, where
all occurrences of “blinker” are assigned the PER
label. However, in general, this is not always the
case; for example we might see in the same doc-
ument the word sequences “Australia” and “The
bank of Australia”. The first instance should be la-
beled as LOC, and the second as ORG. We consider
three approaches proposed in the literature in the fol-
lowing sections. Before continuing the discussion,
we note that we found that adjacent documents in
the CoNLL03 and the MUC7 datasets often discuss
the same entities. Therefore, we ignore document
boundaries and analyze global dependencies in 200
and 1000 token windows. These constants were se-
lected by hand after trying a small number of val-
ues. We believe that this approach will also make
our system more robust in cases when the document
boundaries are not given.
</bodyText>
<subsectionHeader confidence="0.988327">
5.1 Context aggregation
</subsectionHeader>
<bodyText confidence="0.999952058823529">
(Chieu and Ng, 2003) used features that aggre-
gate, for each document, the context tokens appear
in. Sample features are: the longest capitilized se-
quence of words in the document which contains
the current token and the token appears before a
company marker such as ltd. elsewhere in text.
In this work, we call this type of features con-
text aggregation features. Manually designed con-
text aggregation features clearly have low coverage,
therefore we used the following approach. Recall
that for each token instance xi, we use as features
the tokens in the window of size two around it:
ci = (xi−2, xi−1, xi, xi+1, xi+2). When the same
token type t appears in several locations in the text,
say xi1, xi2, ... , xiN, for each instance xis, in ad-
dition to the context features cis, we also aggregate
the context across all instances within 200 tokens:
</bodyText>
<equation confidence="0.7799595">
C = ∪j=N
j=1 cis.
</equation>
<page confidence="0.977439">
150
</page>
<table confidence="0.999930571428571">
Component CoNLL03 CoNLL03 MUC7 MUC7 Web
Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Context Aggregation 85.40 89.99 79.16 71.53 70.76
3) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.19
4) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.72
5) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21
</table>
<tableCaption confidence="0.998179">
Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and
Webpages. No single technique outperformed the rest on all domains. The combination of all techniques is the most robust.
</tableCaption>
<subsectionHeader confidence="0.997441">
5.2 Two-stage prediction aggregation
</subsectionHeader>
<bodyText confidence="0.999974666666667">
Context aggregation as done above can lead to ex-
cessive number of features. (Krishnan and Manning,
2006) used the intuition that some instances of a to-
ken appear in easily-identifiable contexts. Therefore
they apply a baseline NER system, and use the re-
sulting predictions as features in a second level of in-
ference. We call the technique two-stage prediction
aggregation. We implemented the token-majority
and the entity-majority features discussed in (Krish-
nan and Manning, 2006); however, instead of docu-
ment and corpus majority tags, we used relative fre-
quency of the tags in a 1000 token window.
</bodyText>
<subsectionHeader confidence="0.960616">
5.3 Extended prediction history
</subsectionHeader>
<bodyText confidence="0.994320545454546">
Both context aggregation and two-stage prediction
aggregation treat all tokens in the text similarly.
However, we observed that the named entities in the
beginning of the documents tended to be more easily
identifiable and matched gazetteers more often. This
is due to the fact that when a named entity is intro-
duced for the first time in text, a canonical name is
used, while in the following discussion abbreviated
mentions, pronouns, and other references are used.
To break the symmetry, when using beamsearch or
greedy left-to-right decoding, we use the fact that
when we are making a prediction for token instance
xi, we have already made predictions y1, ... , yi−1
for token instances x1, ... , xi−1. When making the
prediction for token instance xi, we record the la-
bel assignment distribution for all token instances
for the same token type in the previous 1000 words.
That is, if the token instance is “Australia”, and in
the previous 1000 tokens, the token type “Australia”
was twice assigned the label L-ORG and three times
the label U-LOC, then the prediction history feature
will be: (L − ORG : 25; U − LOC: 35).
</bodyText>
<subsectionHeader confidence="0.998917">
5.4 Utility of non-local features
</subsectionHeader>
<bodyText confidence="0.999926875">
Table 3 summarizes the results. Surprisingly, no
single technique outperformed the others on all
datasets. The extended prediction history method
was the best on CoNLL03 data and MUC7 test set.
Context aggregation was the best method for MUC7
development set and two-stage prediction was the
best for Webpages. Non-local features proved less
effective for MUC7 test set and the Webpages. Since
the named entities in Webpages have less context,
this result is expected for the Webpages. However,
we are unsure why MUC7 test set benefits from non-
local features much less than MUC7 development
set. Our key conclusion is that no single approach
is better than the rest and that the approaches are
complimentary- their combination is the most stable
and best performing.
</bodyText>
<sectionHeader confidence="0.997081" genericHeader="method">
6 External Knowledge
</sectionHeader>
<bodyText confidence="0.999976">
As we have illustrated in the introduction, NER is
a knowledge-intensive task. In this section, we dis-
cuss two important knowledge resources– gazetteers
and unlabeled text.
</bodyText>
<subsectionHeader confidence="0.997878">
6.1 Unlabeled Text
</subsectionHeader>
<bodyText confidence="0.999428909090909">
Recent successful semi-supervised systems (Ando
and Zhang, 2005; Suzuki and Isozaki, 2008) have
illustrated that unlabeled text can be used to im-
prove the performance of NER systems. In this
work, we analyze a simple technique of using word
clusters generated from unlabeled text, which has
been shown to improve performance of dependency
parsing (Koo et al., 2008), Chinese word segmen-
tation (Liang, 2005) and NER (Miller et al., 2004).
The technique is based on word class models, pio-
neered by (Brown et al., 1992), which hierarchically
</bodyText>
<page confidence="0.995639">
151
</page>
<table confidence="0.999116">
Component CoNLL03 CoNLL03 MUC7 MUC7 Web
Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46
3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26
4) All External Knowledge 88.55 92.49 84.50 83.23 74.44
</table>
<tableCaption confidence="0.661436333333333">
Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages.
clusters words, producing a binary tree as in Fig-
ure 2.
</tableCaption>
<figureCaption confidence="0.991237">
Figure 2: An extract from word cluster hierarchy.
</figureCaption>
<bodyText confidence="0.999966">
The approach is related, but not identical, to dis-
tributional similarity (for details, see (Brown et al.,
1992) and (Liang, 2005)). For example, since the
words Friday and Tuesday appear in similar con-
texts, the Brown algorithm will assign them to the
same cluster. Successful abstraction of both as a
day of the week, addresses the data sparsity prob-
lem common in NLP tasks. In this work, we use the
implementation and the clusters obtained in (Liang,
2005) from running the algorithm on the Reuters
1996 dataset, a superset of the CoNLL03 NER
dataset. Within the binary tree produced by the al-
gorithm, each word can be uniquely identified by
its path from the root, and this path can be com-
pactly represented with a bit string. Paths of dif-
ferent depths along the path from the root to the
word provide different levels of word abstraction.
For example, paths at depth 4 closely correspond
to POS tags. Since word class models use large
amounts of unlabeled data, they are essentially a
semi-supervised technique, which we use to consid-
erably improve the performance of our system.
In this work, we used path prefixes of length
4,6,10, and 20. When Brown clusters are used as
features in the following sections, it implies that all
features in the system which contain a word form
will be duplicated and a new set of features con-
taining the paths of varying length will be intro-
duced. For example, if the system contains the fea-
ture concatenation of the current token and the sys-
tem prediction on the previous word, four new fea-
tures will be introduced which are concatenations
of the previous prediction and the 4,6,10,20 length
path-representations of the current word.
</bodyText>
<subsectionHeader confidence="0.991066">
6.2 Gazetteers
</subsectionHeader>
<bodyText confidence="0.99998903030303">
An important question at the inception of the NER
task was whether machine learning techniques are
necessary at all, and whether simple dictionary
lookup would be sufficient for good performance.
Indeed, the baseline for the CoNLL03 shared task
was essentially a dictionary lookup of the enti-
ties which appeared in the training data, and it
achieves 71.91 F1 score on the test set (Tjong and
De Meulder, 2003). It turns out that while prob-
lems of coverage and ambiguity prevent straightfor-
ward lookup, injection of gazetteer matches as fea-
tures in machine-learning based approaches is crit-
ical for good performance (Cohen, 2004; Kazama
and Torisawa, 2007a; Toral and Munoz, 2006; Flo-
rian et al., 2003). Given these findings, several ap-
proaches have been proposed to automatically ex-
tract comprehensive gazetteers from the web and
from large collections of unlabeled text (Etzioni
et al., 2005; Riloff and Jones, 1999) with lim-
ited impact on NER. Recently, (Toral and Munoz,
2006; Kazama and Torisawa, 2007a) have success-
fully constructed high quality and high coverage
gazetteers from Wikipedia.
In this work, we use a collection of 14 high-
precision, low-recall lists extracted from the web
that cover common names, countries, monetary
units, temporal expressions, etc. While these
gazetteers have excellent accuracy, they do not pro-
vide sufficient coverage. To further improve the
coverage, we have extracted 16 gazetteers from
Wikipedia, which collectively contain over 1.5M en-
tities. Overall, we have 30 gazetteers (available
for download with the system), and matches against
</bodyText>
<page confidence="0.993138">
152
</page>
<table confidence="0.999903">
Component CoNLL03 CoNLL03 MUC7 MUC7 Web
Test data Dev data Dev Test pages
1) Baseline 83.65 89.25 74.72 71.28 71.41
2) (1) + External Knowledge 88.55 92.49 84.50 83.23 74.44
3) (1) + Non-local 86.53 90.69 81.41 73.61 71.21
4) All Features 90.57 93.50 89.19 86.15 74.53
5) All Features (train with dev) 90.80 N/A 89.19 86.15 74.33
</table>
<tableCaption confidence="0.999871">
Table 5: End system performance by component. Results confirm that NER is a knowledge-intensive task.
</tableCaption>
<bodyText confidence="0.999267741935484">
each one are weighted as a separate feature in the
system (this allows us to trust each gazetteer to a dif-
ferent degree). We also note that we have developed
a technique for injecting non-exact string matching
to gazetteers, which has marginally improved the
performance, but is not covered in the paper due to
space limitations. In the rest of this section, we dis-
cuss the construction of gazetteers from Wikipedia.
Wikipedia is an open, collaborative encyclopedia
with several attractive properties. (1) It is kept up-
dated manually by it collaborators, hence new enti-
ties are constantly added to it. (2) Wikipedia con-
tains redirection pages, mapping several variations
of spelling of the same name to one canonical en-
try. For example, Suker is redirected to an entry
about Davor ˇSuker, the Croatian footballer (3) The
entries in Wikipedia are manually tagged with cate-
gories. For example, the entry about the Microsoft
in Wikipedia has the following categories: Companies
listed on NASDAQ; Cloud computing vendors; etc.
Both (Toral and Munoz, 2006) and (Kazama and
Torisawa, 2007a) used the free-text description of
the Wikipedia entity to reason about the entity type.
We use a simpler method to extract high coverage
and high quality gazetteers from Wikipedia. By
inspection of the CoNLL03 shared task annotation
guidelines and of the training set, we manually ag-
gregated several categories into a higher-level con-
cept (not necessarily NER type). When a Wikipedia
entry was tagged by one of the categories in the ta-
ble, it was added to the corresponding gazetteer.
</bodyText>
<subsectionHeader confidence="0.999899">
6.3 Utility of External Knowledge
</subsectionHeader>
<bodyText confidence="0.9994632">
Table 4 summarizes the results of the techniques
for injecting external knowledge. It is important
to note that, although the world class model was
learned on the superset of CoNLL03 data, and al-
though the Wikipedia gazetteers were constructed
</bodyText>
<table confidence="0.9994715">
Dataset Stanford-NER LBJ-NER
MUC7 Test 80.62 85.71
MUC7 Dev 84.67 87.99
Webpages 72.50 74.89
Reuters2003 test 87.04 90.74
Reuters2003 dev 92.36 93.94
</table>
<tableCaption confidence="0.951504">
Table 6: Comparison: token-based Fl score of LBJ-NER and
Stanford NER tagger across several domains
</tableCaption>
<bodyText confidence="0.999821611111111">
based on CoNLL03 annotation guidelines, these fea-
tures proved extremely good on all datasets. Word
class models discussed in Section 6.1 are computed
offline, are available online1, and provide an alter-
native to traditional semi-supervised learning. It is
important to note that the word class models and the
gazetteers and independednt and accumulative. Fur-
thermore, despite the number and the gigantic size
of the extracted gazetteers, the gazeteers alone are
not sufficient for adequate performance. When we
modified the CoNLL03 baseline to include gazetteer
matches, the performance went up from 71.91 to
82.3 on the CoNLL03 test set, below our baseline
system’s result of 83.65. When we have injected the
gazetteers into our system, the performance went up
to 87.22. Word class model and nonlocal features
further improve the performance to 90.57 (see Ta-
ble 5), by more than 3 F1 points.
</bodyText>
<sectionHeader confidence="0.912445" genericHeader="method">
7 Final System Performance Analysis
</sectionHeader>
<bodyText confidence="0.998012571428571">
As a final experiment, we have trained our system
both on the training and on the development set,
which gave us our best F1 score of 90.8 on the
CoNLL03 data, yet it failed to improve the perfor-
mance on other datasets. Table 5 summarizes the
performance of the system.
Next, we have compared the performance of our
</bodyText>
<footnote confidence="0.98866">
1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz
</footnote>
<page confidence="0.998551">
153
</page>
<bodyText confidence="0.99997772">
system to that of the Stanford NER tagger, across the
datasets discussed above. We have chosen to com-
pare against the Stanford tagger because to the best
of our knowledge, it is the best publicly available
system which is trained on the same data. We have
downloaded the Stanford NER tagger and used the
strongest provided model trained on the CoNLL03
data with distributional similarity features. The re-
sults we obtained on the CoNLL03 test set were
consistent with what was reported in (Finkel et al.,
2005). Our goal was to compare the performance of
the taggers across several datasets. For the most re-
alistic comparison, we have presented each system
with a raw text, and relied on the system’s sentence
splitter and tokenizer. When evaluating the systems,
we matched against the gold tokenization ignoring
punctuation marks. Table 6 summarizes the results.
Note that due to differences in sentence splitting, to-
kenization and evaluation, these results are not iden-
tical to those reported in Table 5. Also note that in
this experiment we have used token-level accuracy
on the CoNLL dataset as well. Finally, to complete
the comparison to other systems, in Table 7 we sum-
marize the best results reported for the CoNLL03
dataset in literature.
</bodyText>
<sectionHeader confidence="0.999106" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99998215">
We have presented a simple model for NER that
uses expressive features to achieve new state of the
art performance on the Named Entity recognition
task. We explored four fundamental design deci-
sions: text chunks representation, inference algo-
rithm, using non-local features and external knowl-
edge. We showed that BILOU encoding scheme sig-
nificantly outperforms BIO and that, surprisingly, a
conditional model that does not take into account in-
teractions at the output level performs comparably
to beamsearch or Viterbi, while being considerably
more efficient computationally. We analyzed sev-
eral approaches for modeling non-local dependen-
cies and found that none of them clearly outperforms
the others across several datasets. Our experiments
corroborate recently published results indicating that
word class models learned on unlabeled text can
be an alternative to the traditional semi-supervised
learning paradigm. NER proves to be a knowledge-
intensive task, and it was reassuring to observe that
</bodyText>
<table confidence="0.9994572">
System Resources Used Fl
+ LBJ-NER Wikipedia, Nonlocal Fea- 90.80
tures, Word-class Model
- (Suzuki and Semi-supervised on 1G- 89.92
Isozaki, 2008) word unlabeled data
- (Ando and Semi-supervised on 27M- 89.31
Zhang, 2005) word unlabeled data
- (Kazama and Wikipedia 88.02
Torisawa, 2007a)
- (Krishnan and Non-local Features 87.24
Manning, 2006)
- (Kazama and Non-local Features 87.17
Torisawa, 2007b)
+ (Finkel et al., Non-local Features 86.86
2005)
</table>
<tableCaption confidence="0.9676065">
Table 7: Results for CoNLL03 data reported in the literature.
publicly available systems marked by +.
</tableCaption>
<bodyText confidence="0.888021033333333">
knowledge-driven techniques adapt well across sev-
eral domains. We observed consistent performance
gains across several domains, most interestingly in
Webpages, where the named entities had less context
and were different in nature from the named entities
in the training set. Our system significantly outper-
forms the current state of the art and is available to
download under a research license.
Apendix– wikipedia gazetters &amp; categories
1)People: people, births, deaths. Extracts 494,699 Wikipedia
titles and 382,336 redirect links. 2)Organizations: cooper-
atives, federations, teams, clubs, departments, organizations,
organisations, banks, legislatures, record labels, constructors,
manufacturers, ministries, ministers, military units, military
formations, universities, radio stations, newspapers, broad-
casters, political parties, television networks, companies, busi-
nesses, agencies. Extracts 124,403 titles and 130,588 redi-
rects. 3)Locations: airports, districts, regions, countries, ar-
eas, lakes, seas, oceans, towns, villages, parks, bays, bases,
cities, landmarks, rivers, valleys, deserts, locations, places,
neighborhoods. Extracts 211,872 titles and 194,049 redirects.
4)Named Objects: aircraft, spacecraft, tanks, rifles, weapons,
ships, firearms, automobiles, computers, boats. Extracts 28,739
titles and 31,389 redirects. 5)Art Work: novels, books, paint-
ings, operas, plays. Extracts 39,800 titles and 34037 redirects.
6)Films: films, telenovelas, shows, musicals. Extracts 50,454
titles and 49,252 redirects. 7)Songs: songs, singles, albums.
Extracts 109,645 titles and 67,473 redirects. 8)Events: playoffs,
championships, races, competitions, battles. Extracts 20,176 ti-
tles and 15,182 redirects.
</bodyText>
<page confidence="0.999686">
154
</page>
<sectionHeader confidence="0.995819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999892684782609">
R. K. Ando and T. Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
ACL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D.
Pietra, and J. C. Lai. 1992. Class-based n-gram mod-
els of natural language. Computational Linguistics,
18(4):467–479.
X. Carreras, L. M`arquez, and L. Padr´o. 2003. Learn-
ing a perceptron-based named entity chunker via on-
line recognition feedback. In CoNLL.
H. Chieu and H. T. Ng. 2003. Named entity recognition
with a maximum entropy approach. In Proceedings of
CoNLL.
W. W. Cohen. 2004. Exploiting dictionaries in named
entity extraction: Combining semi-markov extraction
processes and data integration methods. In KDD.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In EMNLP.
L. Edward. 2007. Finding good sequential model struc-
tures using output transformations. In EMNLP).
O. Etzioni, M. J. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91–134.
J. R. Finkel, T. Grenager, and C. D. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by gibbs sampling. In ACL.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277–296.
J. Kazama and K. Torisawa. 2007a. Exploiting wikipedia
as external knowledge for named entity recognition. In
EMNLP.
J. Kazama and K. Torisawa. 2007b. A new perceptron al-
gorithm for sequence labeling with non-local features.
In EMNLP-CoNLL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In ACL.
V. Krishnan and C. D. Manning. 2006. An effective two-
stage model for exploiting non-local dependencies in
named entity recognition. In ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML. Mor-
gan Kaufmann.
P. Liang. 2005. Semi-supervised learning for natural
language. Masters thesis, Massachusetts Institute of
Technology.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative training.
In HLT-NAACL.
A. Molina and F. Pla. 2002. Shallow parsing using spe-
cialized hmms. The Journal of Machine Learning Re-
search, 2:595–613.
A. Niculescu-Mizil and R. Caruana. 2005. Predicting
good probabilities with supervised learning. In ICML.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
IEEE.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In AAAI.
N. Rizzolo and D. Roth. 2007. Modeling discriminative
global inference. In ICSC.
D. Roth and D. Zelenko. 1998. Part of speech tagging us-
ing a network of linear separators. In COLING-ACL.
H. Shen and A. Sarkar. 2005. Voting between multiple
data representations for text chunking. Advances in
Artificial Intelligence, pages 389–400.
J. Suzuki and H. Isozaki. 2008. Semi-supervised sequen-
tial labeling and segmentation using giga-word scale
unlabeled data. In ACL.
E. Tjong, K. and F. De Meulder. 2003. Introduction
to the conll-2003 shared task: Language-independent
named entity recognition. In CoNLL.
A. Toral and R. Munoz. 2006. A proposal to automat-
ically build and maintain gazetteers for named entity
recognition by using wikipedia. In EACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
J. Veenstra. 1999. Representing text chunks. In EACL.
T. Zhang and D. Johnson. 2003. A robust risk mini-
mization based named entity recognition system. In
CoNLL.
</reference>
<page confidence="0.99901">
155
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986808">
<title confidence="0.998322">Challenges and Misconceptions in Named Entity † ‡</title>
<author confidence="0.999608">Lev Ratinov Dan</author>
<affiliation confidence="0.9999185">Computer Science University of</affiliation>
<address confidence="0.995871">Urbana, IL 61801</address>
<abstract confidence="0.999495">We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an system that achieves 90.8 on the CoNLL-2003 NER shared task, the best reported result for this dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R K Ando</author>
<author>T Zhang</author>
</authors>
<title>A high-performance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19942" citStr="Ando and Zhang, 2005" startWordPosition="3232" endWordPosition="3235">s have less context, this result is expected for the Webpages. However, we are unsure why MUC7 test set benefits from nonlocal features much less than MUC7 development set. Our key conclusion is that no single approach is better than the rest and that the approaches are complimentary- their combination is the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>R. K. Ando and T. Zhang. 2005. A high-performance semi-supervised learning method for text chunking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>P V deSouza</author>
<author>R L Mercer</author>
<author>V J D Pietra</author>
<author>J C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="20395" citStr="Brown et al., 1992" startWordPosition="3309" endWordPosition="3312">ction, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, producing a binary tree as in Figure 2. Figure 2: An extract from word cluster hierarchy. The approach is related, but not identical, to distributional s</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
<author>L Padr´o</author>
</authors>
<title>Learning a perceptron-based named entity chunker via online recognition feedback.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<marker>Carreras, M`arquez, Padr´o, 2003</marker>
<rawString>X. Carreras, L. M`arquez, and L. Padr´o. 2003. Learning a perceptron-based named entity chunker via online recognition feedback. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chieu</author>
<author>H T Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="15605" citStr="Chieu and Ng, 2003" startWordPosition="2516" endWordPosition="2519"> labeled as LOC, and the second as ORG. We consider three approaches proposed in the literature in the following sections. Before continuing the discussion, we note that we found that adjacent documents in the CoNLL03 and the MUC7 datasets often discuss the same entities. Therefore, we ignore document boundaries and analyze global dependencies in 200 and 1000 token windows. These constants were selected by hand after trying a small number of values. We believe that this approach will also make our system more robust in cases when the document boundaries are not given. 5.1 Context aggregation (Chieu and Ng, 2003) used features that aggregate, for each document, the context tokens appear in. Sample features are: the longest capitilized sequence of words in the document which contains the current token and the token appears before a company marker such as ltd. elsewhere in text. In this work, we call this type of features context aggregation features. Manually designed context aggregation features clearly have low coverage, therefore we used the following approach. Recall that for each token instance xi, we use as features the tokens in the window of size two around it: ci = (xi−2, xi−1, xi, xi+1, xi+2)</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>H. Chieu and H. T. Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
</authors>
<title>Exploiting dictionaries in named entity extraction: Combining semi-markov extraction processes and data integration methods.</title>
<date>2004</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="23248" citStr="Cohen, 2004" startWordPosition="3795" endWordPosition="3796">stion at the inception of the NER task was whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, </context>
</contexts>
<marker>Cohen, 2004</marker>
<rawString>W. W. Cohen. 2004. Exploiting dictionaries in named entity extraction: Combining semi-markov extraction processes and data integration methods. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="8531" citStr="Collins, 2002" startWordPosition="1357" endWordPosition="1358">ed us to report token-level entity-identification F1 score for this dataset. That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type. 3 Design Challenges in NER In this section we introduce the baseline NER system, and raise the fundamental questions underlying robust and efficient design. These questions define the outline of this paper. NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1,... , xN) be an input sequence and y = (y1, ... , yN) be the output sequence. The sequential prediction problem is to estimate the probabilities P(yi|xi−k . . . xi+l, yi−m . . . yi−1), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes and suffixes</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Edward</author>
</authors>
<title>Finding good sequential model structures using output transformations.</title>
<date>2007</date>
<booktitle>In EMNLP).</booktitle>
<contexts>
<context position="13540" citStr="Edward, 2007" startWordPosition="2169" endWordPosition="2170">ependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5. Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999). Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007). However, we are not aware of similar work in the NER settings. Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting. We focus instead on two most popular schemes– BIO and BILOU. The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments. The BILOU scheme suggests to learn classifiers that identify the Beginning, the Inside and the Last tokens of multi-token chunks as well as Unit-length chunks. The BILOU scheme allows to learn a more expressive model with only a small incr</context>
</contexts>
<marker>Edward, 2007</marker>
<rawString>L. Edward. 2007. Finding good sequential model structures using output transformations. In EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M J Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="23513" citStr="Etzioni et al., 2005" startWordPosition="3835" endWordPosition="3838">y lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While these gazetteers have excellent accuracy, they do not provide sufficient coverage. To further improve the coverage, we have extracted 16 gazetteers from Wikipedia, which collectively contain over 1.5M entities. Overa</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. J. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C D Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28555" citStr="Finkel et al., 2005" startWordPosition="4644" endWordPosition="4647">ext, we have compared the performance of our 1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz 153 system to that of the Stanford NER tagger, across the datasets discussed above. We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data. We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features. The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). Our goal was to compare the performance of the taggers across several datasets. For the most realistic comparison, we have presented each system with a raw text, and relied on the system’s sentence splitter and tokenizer. When evaluating the systems, we matched against the gold tokenization ignoring punctuation marks. Table 6 summarizes the results. Note that due to differences in sentence splitting, tokenization and evaluation, these results are not identical to those reported in Table 5. Also note that in this experiment we have used token-level accuracy on the CoNLL dataset as well. Final</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. D. Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="23322" citStr="Florian et al., 2003" startWordPosition="3805" endWordPosition="3809">ing techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While these gazetteers have exc</context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003. Named entity recognition through classifier combination. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="9732" citStr="Freund and Schapire, 1999" startWordPosition="1555" endWordPosition="1558">(4) prefixes and suffixes of xi (5) tokens in the window c = (xi−2, xi−1, xi, xi+1, xi+2) (6) capitalization pattern in the window c (7) conjunction of c and yi−1. Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. Algorithm Baseline system Final System Greedy 83.29 90.57 Beam size=10 83.38 90.67 Beam size=100 83.38 90.67 Viterbi 83.71 N/A Table 1: Phrase-level Fl performance of different inference methods on CoNLL03 test data. Viterbi canno</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>Exploiting wikipedia as external knowledge for named entity recognition.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="9847" citStr="Kazama and Torisawa, 2007" startWordPosition="1575" endWordPosition="1578"> in the window c (7) conjunction of c and yi−1. Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. Algorithm Baseline system Final System Greedy 83.29 90.57 Beam size=10 83.38 90.67 Beam size=100 83.38 90.67 Viterbi 83.71 N/A Table 1: Phrase-level Fl performance of different inference methods on CoNLL03 test data. Viterbi cannot be used in the end system due to non-local features. Key design decisions in an NER system. 1) How to represent t</context>
<context position="23275" citStr="Kazama and Torisawa, 2007" startWordPosition="3797" endWordPosition="3800">inception of the NER task was whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal ex</context>
<context position="25719" citStr="Kazama and Torisawa, 2007" startWordPosition="4189" endWordPosition="4192">clopedia with several attractive properties. (1) It is kept updated manually by it collaborators, hence new entities are constantly added to it. (2) Wikipedia contains redirection pages, mapping several variations of spelling of the same name to one canonical entry. For example, Suker is redirected to an entry about Davor ˇSuker, the Croatian footballer (3) The entries in Wikipedia are manually tagged with categories. For example, the entry about the Microsoft in Wikipedia has the following categories: Companies listed on NASDAQ; Cloud computing vendors; etc. Both (Toral and Munoz, 2006) and (Kazama and Torisawa, 2007a) used the free-text description of the Wikipedia entity to reason about the entity type. We use a simpler method to extract high coverage and high quality gazetteers from Wikipedia. By inspection of the CoNLL03 shared task annotation guidelines and of the training set, we manually aggregated several categories into a higher-level concept (not necessarily NER type). When a Wikipedia entry was tagged by one of the categories in the table, it was added to the corresponding gazetteer. 6.3 Utility of External Knowledge Table 4 summarizes the results of the techniques for injecting external knowle</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>J. Kazama and K. Torisawa. 2007a. Exploiting wikipedia as external knowledge for named entity recognition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>K Torisawa</author>
</authors>
<title>A new perceptron algorithm for sequence labeling with non-local features.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<contexts>
<context position="9847" citStr="Kazama and Torisawa, 2007" startWordPosition="1575" endWordPosition="1578"> in the window c (7) conjunction of c and yi−1. Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. Algorithm Baseline system Final System Greedy 83.29 90.57 Beam size=10 83.38 90.67 Beam size=100 83.38 90.67 Viterbi 83.71 N/A Table 1: Phrase-level Fl performance of different inference methods on CoNLL03 test data. Viterbi cannot be used in the end system due to non-local features. Key design decisions in an NER system. 1) How to represent t</context>
<context position="23275" citStr="Kazama and Torisawa, 2007" startWordPosition="3797" endWordPosition="3800">inception of the NER task was whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal ex</context>
<context position="25719" citStr="Kazama and Torisawa, 2007" startWordPosition="4189" endWordPosition="4192">clopedia with several attractive properties. (1) It is kept updated manually by it collaborators, hence new entities are constantly added to it. (2) Wikipedia contains redirection pages, mapping several variations of spelling of the same name to one canonical entry. For example, Suker is redirected to an entry about Davor ˇSuker, the Croatian footballer (3) The entries in Wikipedia are manually tagged with categories. For example, the entry about the Microsoft in Wikipedia has the following categories: Companies listed on NASDAQ; Cloud computing vendors; etc. Both (Toral and Munoz, 2006) and (Kazama and Torisawa, 2007a) used the free-text description of the Wikipedia entity to reason about the entity type. We use a simpler method to extract high coverage and high quality gazetteers from Wikipedia. By inspection of the CoNLL03 shared task annotation guidelines and of the training set, we manually aggregated several categories into a higher-level concept (not necessarily NER type). When a Wikipedia entry was tagged by one of the categories in the table, it was added to the corresponding gazetteer. 6.3 Utility of External Knowledge Table 4 summarizes the results of the techniques for injecting external knowle</context>
</contexts>
<marker>Kazama, Torisawa, 2007</marker>
<rawString>J. Kazama and K. Torisawa. 2007b. A new perceptron algorithm for sequence labeling with non-local features. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semisupervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="20244" citStr="Koo et al., 2008" startWordPosition="3282" endWordPosition="3285">s the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, pr</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semisupervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Krishnan</author>
<author>C D Manning</author>
</authors>
<title>An effective twostage model for exploiting non-local dependencies in named entity recognition.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="17210" citStr="Krishnan and Manning, 2006" startWordPosition="2787" endWordPosition="2790">ontext Aggregation 85.40 89.99 79.16 71.53 70.76 3) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.19 4) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.72 5) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21 Table 3: The utility of non-local features. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. No single technique outperformed the rest on all domains. The combination of all techniques is the most robust. 5.2 Two-stage prediction aggregation Context aggregation as done above can lead to excessive number of features. (Krishnan and Manning, 2006) used the intuition that some instances of a token appear in easily-identifiable contexts. Therefore they apply a baseline NER system, and use the resulting predictions as features in a second level of inference. We call the technique two-stage prediction aggregation. We implemented the token-majority and the entity-majority features discussed in (Krishnan and Manning, 2006); however, instead of document and corpus majority tags, we used relative frequency of the tags in a 1000 token window. 5.3 Extended prediction history Both context aggregation and two-stage prediction aggregation treat all</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>V. Krishnan and C. D. Manning. 2006. An effective twostage model for exploiting non-local dependencies in named entity recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</title>
<date>2001</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="8463" citStr="Lafferty et al., 2001" startWordPosition="1345" endWordPosition="1348">d not agree on whether “God” and “Lord” is an ORG or PER. These issues 148 led us to report token-level entity-identification F1 score for this dataset. That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type. 3 Design Challenges in NER In this section we introduce the baseline NER system, and raise the fundamental questions underlying robust and efficient design. These questions define the outline of this paper. NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1,... , xN) be an input sequence and y = (y1, ... , yN) be the output sequence. The sequential prediction problem is to estimate the probabilities P(yi|xi−k . . . xi+l, yi−m . . . yi−1), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word type (all-capitalized, is-ca</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Masters thesis,</title>
<date>2005</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="20285" citStr="Liang, 2005" startWordPosition="3290" endWordPosition="3291">nal Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, producing a binary tree as in Figure 2. Fig</context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>P. Liang. 2005. Semi-supervised learning for natural language. Masters thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>J Guinness</author>
<author>A Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="20315" citStr="Miller et al., 2004" startWordPosition="3294" endWordPosition="3297">ve illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.46 3) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.26 4) All External Knowledge 88.55 92.49 84.50 83.23 74.44 Table 4: Utility of external knowledge. The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages. clusters words, producing a binary tree as in Figure 2. Figure 2: An extract from word cl</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>S. Miller, J. Guinness, and A. Zamanian. 2004. Name tagging with word clusters and discriminative training. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Molina</author>
<author>F Pla</author>
</authors>
<title>Shallow parsing using specialized hmms.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--595</pages>
<contexts>
<context position="13479" citStr="Molina and Pla, 2002" startWordPosition="2159" endWordPosition="2162">t chunks, where the greedy policy performs well. On the other hand, dependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5. Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999). Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007). However, we are not aware of similar work in the NER settings. Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting. We focus instead on two most popular schemes– BIO and BILOU. The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments. The BILOU scheme suggests to learn classifiers that identify the Beginning, the Inside and the Last tokens of multi-token chunks as well as Unit-length chunks. The BILOU scheme a</context>
</contexts>
<marker>Molina, Pla, 2002</marker>
<rawString>A. Molina and F. Pla. 2002. Shallow parsing using specialized hmms. The Journal of Machine Learning Research, 2:595–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Niculescu-Mizil</author>
<author>R Caruana</author>
</authors>
<title>Predicting good probabilities with supervised learning.</title>
<date>2005</date>
<booktitle>In ICML. V. Punyakanok</booktitle>
<marker>Niculescu-Mizil, Caruana, 2005</marker>
<rawString>A. Niculescu-Mizil and R. Caruana. 2005. Predicting good probabilities with supervised learning. In ICML. V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>In IEEE.</booktitle>
<contexts>
<context position="8434" citStr="Rabiner, 1989" startWordPosition="1342" endWordPosition="1343">R. Similarly, we could not agree on whether “God” and “Lord” is an ORG or PER. These issues 148 led us to report token-level entity-identification F1 score for this dataset. That is, if a named entity token was identified as such, we counted it as a correct prediction ignoring the named entity type. 3 Design Challenges in NER In this section we introduce the baseline NER system, and raise the fundamental questions underlying robust and efficient design. These questions define the outline of this paper. NER is typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1,... , xN) be an input sequence and y = (y1, ... , yN) be the output sequence. The sequential prediction problem is to estimate the probabilities P(yi|xi−k . . . xi+l, yi−m . . . yi−1), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="23538" citStr="Riloff and Jones, 1999" startWordPosition="3839" endWordPosition="3842">es which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While these gazetteers have excellent accuracy, they do not provide sufficient coverage. To further improve the coverage, we have extracted 16 gazetteers from Wikipedia, which collectively contain over 1.5M entities. Overall, we have 30 gazetteers</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Modeling discriminative global inference.</title>
<date>2007</date>
<booktitle>In ICSC.</booktitle>
<contexts>
<context position="9975" citStr="Rizzolo and Roth, 2007" startWordPosition="1599" endWordPosition="1602">ation and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. Our baseline NER system uses a regularized averaged perceptron (Freund and Schapire, 1999). Systems based on perceptron have been shown to be competitive in NER and text chunking (Kazama and Torisawa, 2007b; Punyakanok and Roth, 2001; Carreras et al., 2003) We specify the model and the features with the LBJ (Rizzolo and Roth, 2007) modeling language. We now state the four fundamental design decisions in NER system which define the structure of this paper. Algorithm Baseline system Final System Greedy 83.29 90.57 Beam size=10 83.38 90.67 Beam size=100 83.38 90.67 Viterbi 83.71 N/A Table 1: Phrase-level Fl performance of different inference methods on CoNLL03 test data. Viterbi cannot be used in the end system due to non-local features. Key design decisions in an NER system. 1) How to represent text chunks in NER system? 2) What inference algorithm to use? 3) How to model non-local dependencies? 4) How to use external kno</context>
</contexts>
<marker>Rizzolo, Roth, 2007</marker>
<rawString>N. Rizzolo and D. Roth. 2007. Modeling discriminative global inference. In ICSC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>D Zelenko</author>
</authors>
<title>Part of speech tagging using a network of linear separators.</title>
<date>1998</date>
<booktitle>In COLING-ACL.</booktitle>
<contexts>
<context position="11672" citStr="Roth and Zelenko, 1998" startWordPosition="1872" endWordPosition="1875">be discussed later, therefore, we cannot use it in our end system. However, it has the appealing quality of finding the most likely assignment to a second-order model, and since the baseline features only have second order dependencies, we have tested it on the baseline configuration. Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later). Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (Toutanova et al., 2003; Roth and Zelenko, 1998). The implications are subtle. First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi. The reason is that with the BILOU encoding of four NE types, each token can take 21 states (O, B-PER, I-PER , U-PER, etc.). To tag a token, the greedy policy requires 21 comparisons, while the Viterbi requires 213, and this analysis carries over to the number of classifier invocations. Furthermore, both beamsearch and Viterbi require transforming the predictions of the classi149 Rep. CoNLL03 MUC7 Scheme Test Dev Dev Test BIO 89.15 93.61 86.76 85.15 BILOU 90.57 </context>
</contexts>
<marker>Roth, Zelenko, 1998</marker>
<rawString>D. Roth and D. Zelenko. 1998. Part of speech tagging using a network of linear separators. In COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shen</author>
<author>A Sarkar</author>
</authors>
<title>Voting between multiple data representations for text chunking.</title>
<date>2005</date>
<booktitle>Advances in Artificial Intelligence,</booktitle>
<pages>389--400</pages>
<contexts>
<context position="13430" citStr="Shen and Sarkar, 2005" startWordPosition="2151" endWordPosition="2154">o independent maximization of assignment over short chunks, where the greedy policy performs well. On the other hand, dependencies between isolated named entity chunks have longer-range dependencies and are not captured by second-order transition features, therefore requiring separate mechanisms, which we discuss in Section 5. Another important question that has been studied extensively in the context of shallow parsing and was somewhat overlooked in the NER literature is the representation of text segments (Veenstra, 1999). Related works include voting between several representation schemes (Shen and Sarkar, 2005), lexicalizing the schemes (Molina and Pla, 2002) and automatically searching for best encoding (Edward, 2007). However, we are not aware of similar work in the NER settings. Due to space limitations, we do not discuss all the representation schemes and combining predictions by voting. We focus instead on two most popular schemes– BIO and BILOU. The BIO scheme suggests to learn classifiers that identify the Beginning, the Inside and the Outside of the text segments. The BILOU scheme suggests to learn classifiers that identify the Beginning, the Inside and the Last tokens of multi-token chunks </context>
</contexts>
<marker>Shen, Sarkar, 2005</marker>
<rawString>H. Shen and A. Sarkar. 2005. Voting between multiple data representations for text chunking. Advances in Artificial Intelligence, pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="19969" citStr="Suzuki and Isozaki, 2008" startWordPosition="3236" endWordPosition="3239">his result is expected for the Webpages. However, we are unsure why MUC7 test set benefits from nonlocal features much less than MUC7 development set. Our key conclusion is that no single approach is better than the rest and that the approaches are complimentary- their combination is the most stable and best performing. 6 External Knowledge As we have illustrated in the introduction, NER is a knowledge-intensive task. In this section, we discuss two important knowledge resources– gazetteers and unlabeled text. 6.1 Unlabeled Text Recent successful semi-supervised systems (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) have illustrated that unlabeled text can be used to improve the performance of NER systems. In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al., 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al., 2004). The technique is based on word class models, pioneered by (Brown et al., 1992), which hierarchically 151 Component CoNLL03 CoNLL03 MUC7 MUC7 Web Test data Dev data Dev Test pages 1) Baseline 83.65 89.25 74.72 71.28 71.41 2) (1) + Gazetteer Match 87.22 </context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>J. Suzuki and H. Isozaki. 2008. Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Tjong</author>
<author>K</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the conll-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<marker>Tjong, K, De Meulder, 2003</marker>
<rawString>E. Tjong, K. and F. De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Toral</author>
<author>R Munoz</author>
</authors>
<title>A proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="23299" citStr="Toral and Munoz, 2006" startWordPosition="3801" endWordPosition="3804">s whether machine learning techniques are necessary at all, and whether simple dictionary lookup would be sufficient for good performance. Indeed, the baseline for the CoNLL03 shared task was essentially a dictionary lookup of the entities which appeared in the training data, and it achieves 71.91 F1 score on the test set (Tjong and De Meulder, 2003). It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). Given these findings, several approaches have been proposed to automatically extract comprehensive gazetteers from the web and from large collections of unlabeled text (Etzioni et al., 2005; Riloff and Jones, 1999) with limited impact on NER. Recently, (Toral and Munoz, 2006; Kazama and Torisawa, 2007a) have successfully constructed high quality and high coverage gazetteers from Wikipedia. In this work, we use a collection of 14 highprecision, low-recall lists extracted from the web that cover common names, countries, monetary units, temporal expressions, etc. While th</context>
<context position="25688" citStr="Toral and Munoz, 2006" startWordPosition="4184" endWordPosition="4187"> an open, collaborative encyclopedia with several attractive properties. (1) It is kept updated manually by it collaborators, hence new entities are constantly added to it. (2) Wikipedia contains redirection pages, mapping several variations of spelling of the same name to one canonical entry. For example, Suker is redirected to an entry about Davor ˇSuker, the Croatian footballer (3) The entries in Wikipedia are manually tagged with categories. For example, the entry about the Microsoft in Wikipedia has the following categories: Companies listed on NASDAQ; Cloud computing vendors; etc. Both (Toral and Munoz, 2006) and (Kazama and Torisawa, 2007a) used the free-text description of the Wikipedia entity to reason about the entity type. We use a simpler method to extract high coverage and high quality gazetteers from Wikipedia. By inspection of the CoNLL03 shared task annotation guidelines and of the training set, we manually aggregated several categories into a higher-level concept (not necessarily NER type). When a Wikipedia entry was tagged by one of the categories in the table, it was added to the corresponding gazetteer. 6.3 Utility of External Knowledge Table 4 summarizes the results of the technique</context>
</contexts>
<marker>Toral, Munoz, 2006</marker>
<rawString>A. Toral and R. Munoz. 2006. A proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="11647" citStr="Toutanova et al., 2003" startWordPosition="1868" endWordPosition="1871">cal features which will be discussed later, therefore, we cannot use it in our end system. However, it has the appealing quality of finding the most likely assignment to a second-order model, and since the baseline features only have second order dependencies, we have tested it on the baseline configuration. Table 1 compares between the greedy decoding, beamsearch with varying beam size, and Viterbi, both for the system with baseline features and for the end system (to be presented later). Surprisingly, the greedy policy performs well, this phenmenon was also observed in the POS tagging task (Toutanova et al., 2003; Roth and Zelenko, 1998). The implications are subtle. First, due to the second-order of the model, the greedy decoding is over 100 times faster than Viterbi. The reason is that with the BILOU encoding of four NE types, each token can take 21 states (O, B-PER, I-PER , U-PER, etc.). To tag a token, the greedy policy requires 21 comparisons, while the Viterbi requires 213, and this analysis carries over to the number of classifier invocations. Furthermore, both beamsearch and Viterbi require transforming the predictions of the classi149 Rep. CoNLL03 MUC7 Scheme Test Dev Dev Test BIO 89.15 93.61</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL. J. Veenstra. 1999. Representing text chunks. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>D Johnson</author>
</authors>
<title>A robust risk minimization based named entity recognition system.</title>
<date>2003</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="8958" citStr="Zhang and Johnson, 2003" startWordPosition="1432" endWordPosition="1435"> typically viewed as a sequential prediction problem, the typical models include HMM (Rabiner, 1989), CRF (Lafferty et al., 2001), and sequential application of Perceptron or Winnow (Collins, 2002). That is, let x = (x1,... , xN) be an input sequence and y = (y1, ... , yN) be the output sequence. The sequential prediction problem is to estimate the probabilities P(yi|xi−k . . . xi+l, yi−m . . . yi−1), where k, l and m are small numbers to allow tractable inference and avoid overfitting. This conditional probability distribution is estimated in NER using the following baseline set of features (Zhang and Johnson, 2003): (1) previous two predictions yi−1 and yi−2 (2) current word xi (3) xi word type (all-capitalized, is-capitalized, all-digits, alphanumeric, etc.) (4) prefixes and suffixes of xi (5) tokens in the window c = (xi−2, xi−1, xi, xi+1, xi+2) (6) capitalization pattern in the window c (7) conjunction of c and yi−1. Most NER systems use additional features, such as POS tags, shallow parsing information and gazetteers. We discuss additional features in the following sections. We note that we normalize dates and numbers, that is 12/3/2008 becomes *Date*, 1980 becomes *DDDD* and 212-325-4751 becomes *D</context>
</contexts>
<marker>Zhang, Johnson, 2003</marker>
<rawString>T. Zhang and D. Johnson. 2003. A robust risk minimization based named entity recognition system. In CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>