<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001029">
<title confidence="0.9771665">
SVO triple based Latent Semantic Analysis for recognising textual
entailment
</title>
<author confidence="0.998704">
Gaston Burek Christian Pietsch Anne De Roeck
</author>
<affiliation confidence="0.900020666666667">
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, MK7 6AA, UK
</affiliation>
<email confidence="0.986346">
Ig.g.burek,c.pietsch,a.deroeckl@open.ac.uk
</email>
<sectionHeader confidence="0.998522" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999262866666667">
Latent Semantic Analysis has only recently
been applied to textual entailment recogni-
tion. However, these efforts have suffered
from inadequate bag of words vector repre-
sentations. Our prototype implementation
for the Third Recognising Textual Entail-
ment Challenge (RTE-3) improves the ap-
proach by applying it to vector represen-
tations that contain semi-structured repre-
sentations of words. It uses variable size
n-grams of word stems to model indepen-
dently verbs, subjects and objects displayed
in textual statements. The system perfor-
mance shows positive results and provides
insights about how to improve them further.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942592592593">
The Third Recognising Textual Entailment Chal-
lenge (RTE-3) task consists in developing a system
for automatically determining whether or not a hy-
pothesis (H) can be inferred from a text (T), which
could be up to a paragraph long.
Our entry to the RTE-3 challenge is a system
that takes advantage of Latent Semantic Analysis
(LSA) (Landauer and Dumais, 1997). This numer-
ical method for reducing noise generated by word
choices within texts is extensively used for docu-
ment indexing and word sense disambiguation. Re-
cently, there have also been efforts to use techniques
from LSA to recognise textual entailment (Clarke,
2006; de Marneffe et al., 2006). However, we argue
that these efforts (like most LSA approaches in the
past) suffer from an inadequate vector representation
for textual contexts as bags of words. In contrast,
we have applied LSA to vector representations of
semi-structured text. Our representation takes into
account the grammatical role (i.e. subject, verb or
object) a word occurs in.
Within this system report, we describe and dis-
cuss our methodology in section 2, our current im-
plementation in section 3, and system results in sec-
tion 4. We conclude in section 5 with a discussion
of the results obtained and with the presentation of
possible steps to improve our system’s performance.
</bodyText>
<sectionHeader confidence="0.9966935" genericHeader="method">
2 Methodology for detecting Textual
Entailment
</sectionHeader>
<subsectionHeader confidence="0.993424">
2.1 Textual entailment formalisation
</subsectionHeader>
<bodyText confidence="0.999804428571428">
Our approach addresses the problem of the semantic
gap that exists between low level linguistic entities
(words) and concepts. Concepts can be described
by means of predicate-argument structures or by a
set of alternative natural language realisations. In
this work we use terminology co-occurrence infor-
mation to identify when different spans of text have
common semantic content even if they do not share
vocabulary. To achieve this, we use variable size n-
grams to independently model subject, verb and ob-
ject, and capture semantics derived from grammati-
cal structure. In order to detect textual entailment we
measure the semantic similarity between n-grams in
each T–H pair.
</bodyText>
<subsectionHeader confidence="0.992712">
2.2 Using n-grams to align SVOs
</subsectionHeader>
<bodyText confidence="0.995925">
To align subjects, verbs and objects within H and T,
we build the set of all n-grams for T, and do the same
</bodyText>
<page confidence="0.988477">
113
</page>
<bodyText confidence="0.83516975">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 113–118,
Prague, June 2007. c�2007 Association for Computational Linguistics
for H. Section 3.5 describes this process in more de-
tail.
</bodyText>
<subsectionHeader confidence="0.9996695">
2.3 Deriving word senses with Latent Semantic
Analysis
</subsectionHeader>
<bodyText confidence="0.999922">
Our approach is based on the assumption that a
word sense can be derived from the textual contexts
in which that words occurs. This assumption was
formalised in the Distributional Hypothesis (Harris,
1954).
We implemented a vector space model (Salton et
al., 1975) to capture word semantics from linguis-
tic (i.e. grammatical role) and contextual (i.e. fre-
quency) information about each word. To avoid high
matrix sparsity our vector space model uses second
order co-occurrence (Widdows, 2004, p. 174).
We assumed that the corpus we generated the vec-
tor space model from has a probabilistic word distri-
bution that is characterised by a number of seman-
tic dimensions. The LSA literature seems to agree
that optimal number of dimensions is somewhere
between two hundred and one thousand depending
on corpus and domain. As specified by LSA we ap-
plied Singular Value Decomposition (SVD) (Berry,
1992) to identify the characteristic semantic dimen-
sions.
The resulting model is a lower dimensional pro-
jection of the original model that captures indi-
rect associations between the vectors in the original
model. SVD reduces the noise in word categori-
sations by producing the best approximation to the
original vector space model.
</bodyText>
<sectionHeader confidence="0.999579" genericHeader="method">
3 Implementation
</sectionHeader>
<subsectionHeader confidence="0.999977">
3.1 Development data set
</subsectionHeader>
<bodyText confidence="0.999846333333333">
The development data set consists of eight hundred
T–H pairs, half of them positive. By positive pair
we mean a T–H pair in which T entails H. All other
pairs we call negative. Each T–H pair belongs to a
particular sub-task. Those sub-tasks are Information
Extraction (IE), Information Retrieval (IR), Ques-
tion Answering (QA) and Summarisation (SUM). In
the current prototype, we ignored annotations about
sub-tasks.
</bodyText>
<subsectionHeader confidence="0.999381">
3.2 Corpus analysis
3.2.1 Corpora used
</subsectionHeader>
<bodyText confidence="0.999982833333333">
The only knowledge source we used in our imple-
mentation was a parsed newswire corpus (Reuters
News Corpus) (Lewis et al., 2004). To derive con-
textual information about the meaning of words con-
stituting the SVOs, we analysed the Reuters corpus
as explained below.
</bodyText>
<subsectionHeader confidence="0.906788">
3.2.2 SVO triple extraction
</subsectionHeader>
<bodyText confidence="0.99996852">
For parsing the corpus, we used Minipar1 because
of its speed and because its simple dependency triple
output format (-t option) contains word stems and
the grammatical relations between them. A simple
AWK script was used to convert the parse results into
Prolog facts, one file for each sentence. A straight-
forward Prolog program then identified SVOs in
each of these fact files, appending them to one big
structured text file.
Our algorithm currently recognises intransitive,
transitive, ditransitive, and predicative clauses. In-
transitive clauses are encoded as SVOs with an
empty object slot. Transitive clauses result in a fully
instantiated SVOs. Ditransitive clauses are encoded
as two different SVOs: the first containing subject,
verb and direct object; the second triple containing
the subject again, an empty verb slot, and the indi-
rect object. Predicatives (e.g. “somebody is some-
thing”) are encoded just like transitive clauses.
In this first prototype, we only used one word
(which could be a multi-word expression) for sub-
ject, verb and object slot respectively. We realise
that this approach ignores much information, but
given a large corpus, it might not be detrimental to
be selective.
</bodyText>
<subsectionHeader confidence="0.941837">
3.2.3 SVO Stemming and labeling
</subsectionHeader>
<bodyText confidence="0.9996954">
To reduce the dimensionality of our vector space
model we stem the SVOs using Snowball2. Then,
we calculate how many times stems co-occur as sub-
ject, verb or object with another stem within the
same SVO instance.
</bodyText>
<footnote confidence="0.998367833333333">
1Minipar can be downloaded from http://www.cs.ualberta.
ca/~lindek/minipar.htm. It is based on Principar, which is de-
scribed in Lin (1994).
2Snowball is freely available from http://snowball.tartarus.
org/. The English version is based on the original Porter Stem-
mer (Porter, 1980).
</footnote>
<page confidence="0.997546">
114
</page>
<bodyText confidence="0.999931666666667">
To keep track of the grammatical role (i.e. subject,
verb, object) of the words we stem them and label
the stems with the corresponding role.
</bodyText>
<subsectionHeader confidence="0.9916985">
3.3 Building vector spaces to represent stem
semantics
</subsectionHeader>
<bodyText confidence="0.999977689655172">
From the corpus, we built a model (S, V, O) of the
English (news) language consisting of three matri-
ces: S for subjects, V for verbs, and O for objects.
We built the three stem-to-stem matrices from
labeled stem co-occurrences within the extracted
triples. The entries to the matrices are the frequen-
cies of the co-occurrence of each labeled stem with
itself or with another labeled stem. In our current
prototype, due to technical restrictions explained in
Section 3.4, each matrix has 1000 rows and 5000
columns.
Columns of matrix S contain entries for stems la-
beled as subject, columns of matrix V contain en-
tries for stems labeled as verb, and columns of ma-
trix O contain entries for stems labeled as object.
The frequency entries of each matrix correspond to
the set of identically labeled stems with the highest
frequency.
Rows of the three matrices contain entries corre-
sponding to the same set of labeled stems. Those la-
beled stems are the ones with the highest frequency
in the set of all labeled stems. Of these, 347 stems
are labeled as subject, 356 are labeled as verb, and
297 are labeled as object. Each row entry is the fre-
quency of co-occurrence of two labeled stems within
the same triple.
Finally, each column entry is divided by the num-
ber of times the labeled stem associated with that
column occurs within all triples.
</bodyText>
<subsectionHeader confidence="0.9859">
3.4 Calculating the singular value
decomposition
</subsectionHeader>
<bodyText confidence="0.999970333333333">
We calculated the Singular Value Decompositions
(SVDs) for S, V and O. Each SVD of a matrix A is
defined as a product of three matrices:
</bodyText>
<equation confidence="0.947063">
A=Ux5xV (1)
</equation>
<bodyText confidence="0.99974325925926">
SVD is a standard matrix operation which is sup-
ported by many programming libraries and com-
puter algebra applications. The problem is that only
very few can handle the large matrices required for
real-world LSA. It is easy to see that the memory re-
quired for representing a full matrix of 64 bit float-
ing point values can easily exceed what current hard-
ware offers. Fortunately, our matrices are sparse, so
a library with sparse matrix support should be able
to cope. Unfortunately, these are hard to find out-
side the Fortran world. We failed to find any Java li-
brary that can perform SVD on sparse matrices.3 We
finally decided to use SVDLIBC, a modernised ver-
sion of SVDPACKC using only the LAS2 algorithm.
In pre-tests with a matrix derived from a different
text corpus (18371 rows x 3469 columns, density
0.73%), it completed the SVD task within ten min-
utes on typical current hardware. However, when
we try to use it for this task on a matrix S of dimen-
sion 5000 x 5000 (density 1.4%), SVDLIBC did not
terminate4. In theory, there is a Singular Value De-
composition for every given matrix, so we assume
this is an implementation flaw in either SVDLIBC or
GCC. With no time left to try Fortran alternatives,
we resorted to reducing the size of our three matri-
ces to 1000 x 5000, thus losing much information
in our language model.
</bodyText>
<subsectionHeader confidence="0.838979666666667">
3.5 Looking for evidence of H in T using
variable size n-grams
3.5.1 Building variable size n-grams
</subsectionHeader>
<bodyText confidence="0.942878083333333">
Our Minipar triple extraction algorithm is not able
to handle SVOs that are embedded within other
SVOs (as e.g. in “Our children play a new game that
involves three teams competing for a ball.”). There-
fore, in order to determine if SVOs displayed in H
are semantically similar to any of those displayed in
T, we generate all n-grams of all lengths for each T
and H: one set for subjects, one for verbs and another
one for objects.
Example: “The boy played tennis.”
Derived n-grams: the; the boy; the boy played; the
boy played tennis; boy; boy played; boy played
</bodyText>
<footnote confidence="0.895164222222222">
3The popular JAMA library and the related Jampack library
have no sparse matrix support at all. MTJ and Colt do support
sparse matrices but cannot perform SVD on them without first
converting them to full matrices.
4We tried various hardware / operating system / compiler
combinations. On Linux systems, SVDLIBC would abort after
about 15 minutes with an error message “imtqlb failed to con-
verge”. On Solaris and Mac OS X systems, the process would
not terminate within several days.
</footnote>
<page confidence="0.994865">
115
</page>
<bodyText confidence="0.840159">
tennis; played; played tennis; tennis.
We use n-grams to model subjects, verbs and ob-
jects of SVOs within T and H.
</bodyText>
<subsectionHeader confidence="0.843649">
3.5.2 How to compare n-grams
</subsectionHeader>
<bodyText confidence="0.997903">
We generate three vector representations for each
n-gram. To do this, we add up columns from the
Reuters Corpus derived matrices. To build the first
vector representation, we use the S matrix, to build
the second vector we use the V matrix, and to build
the third vector we use the O matrix. Each of
the three representations is the result of adding the
columns corresponding to each stem within the n-
gram.
To calculate the semantic similarity between n-
grams, we fold the three vector representations of
each n-gram into one of the dimensionally reduced
matrices S200, V200 or O200. Vector representation
originating from the S matrix are folded into S200.
We proceed analogously for vector representations
originating from V200 and O200. We apply equation
2 to fold vectors from Gr where r E IS, V, O1. G
is a matrix which consists of all the vector represen-
tations of all the n-grams modeling T or H. Sr200 and
Ur200 are SVD results reduced to 200 dimensions.
</bodyText>
<equation confidence="0.8860665">
Gr0 x Ur200 x (Sr200)−1 = Gr (2)
200
</equation>
<bodyText confidence="0.930160333333333">
For each T–H pair we calculate the dot product
between the G matrices for H and T as expressed in
equation 3
</bodyText>
<equation confidence="0.9946975">
textGr200 xhypothesis Gr0
200 = Or (3)
</equation>
<bodyText confidence="0.9998386">
The resulting matrix Or contains the dot product
similarity between all pairs of n-grams within the
same set. Finally, for each T–H pair we obtain three
similarity values s, v, o by selecting the entry of Or
with the highest value.
</bodyText>
<subsectionHeader confidence="0.918667">
3.5.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999854375">
Now we have calculated almost everything we
need to venture a guess about textual entailment.
For each T–H pair, we have three scores s, v, o
for for subject, verb and object slot respectively. The
remaining task is to combine them in a meaningful
way in order to make a decision. This requires some
amount of training which in the current prototype
is as simple as computing six average values: sp,
</bodyText>
<table confidence="0.830824666666667">
s� v� o�
positive 0.244 5.05 · 10−7 0.323
negative 0.196 4.76 · 10−7 0.277
</table>
<tableCaption confidence="0.998862">
Table 1: Values computed for SP vp, op, sn, vn, on
</tableCaption>
<figure confidence="0.980025">
0 100 200 300 400 500 600 700 800
n-grams
</figure>
<figureCaption confidence="0.937388">
Figure 1: Subject similarities s = max OS for all
H–T pairs
</figureCaption>
<bodyText confidence="0.999761368421052">
vp, op are the average scores of subject, verb and
object slots over those T–H pairs for which textual
entailment is known to hold. Conversely, sn, vn, on
are the averages for those pairs that do not stand in
a textual entailment relation. The (rounded) values
were determined are shown in table 1.
Note that the average values for non-entailment
are always lower than the average values for entail-
ment, which indicates that our system indeed tends
to discriminate correctly between these cases.
The very low values for the verb similarities (fig-
ure 3) compared to subject similarities (figure 1) and
object similarities (figure 2) remind us that before
we can combine slot scores, they should be scaled
to a comparable level. This is achieved by divid-
ing each slot score by its corresponding average. Ig-
noring the difference between positive and negative
pairs for a moment, the basic idea of our scoring al-
gorithm is to use the following threshold:
</bodyText>
<figure confidence="0.73994325">
= 3 (4)
0
0 100 200 300 400 500 600 700 800
n-grams
</figure>
<figureCaption confidence="0.916836">
Figure 2: Object similarities o = max OO for all
H–T pairs
</figureCaption>
<figure confidence="0.998293857142857">
0.9
0.8
dot product similarity
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
s
o
s�
v
�
�
o�
v�
0.8
0.7
dot product similarity
0.6
0.5
0.4
0.3
0.2
0.1
</figure>
<page confidence="0.857878">
116
</page>
<table confidence="0.635960333333333">
all IE IR QA SUM
accuracy 0.5500 0.4950 0.5750 0.5550 0.5750
av. prec. 0.5514 0.4929 0.5108 0.5842 0.6104
</table>
<tableCaption confidence="0.801062">
Table 2: Results on the test set
</tableCaption>
<figure confidence="0.998665916666667">
dot product similarity
1.6e-06
1.4e-06
1.2e-06
8e-07
6e-07
4e-07
2e-07
1e-06
0
0 100 200 300 400 500 600 700 800
r-grams
</figure>
<figureCaption confidence="0.984599">
Figure 3: Verb similarities v = max Ov for all H–T
pairs
</figureCaption>
<bodyText confidence="0.999844666666667">
At this point we observed that scaling the verb
similarities so much seemed to make results worse.
It seems to be necessary to introduce weights:
</bodyText>
<equation confidence="0.977854">
= σ + φ + ω (5)
</equation>
<bodyText confidence="0.600382">
Without loss of generality, we may assume φ = 1:
</bodyText>
<equation confidence="0.985908">
= σ + 1 + ω (6)
</equation>
<bodyText confidence="0.99996025">
The complete scoring formula with both positive
and negative scores is shown below. We assumed
that the weights σ and ω are the same in the positive
and in the negative case, so σ = σp = σn and ω =
</bodyText>
<equation confidence="0.803884333333333">
ωp = ωn.
= 2(σ+1+ω)
(7)
</equation>
<bodyText confidence="0.99750375">
At this point, some machine learning over the de-
velopment data set should be performed in order to
determine optimal values for σ and ω. For lack of
time, we simply performed a dozen or so of test runs
and finally set σ = ω = 3.
Our entailment threshold is thus simplified:
If q &gt; 14, our prototype predicts textual entail-
ment. Otherwise, it predicts non-entailment.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99995625">
Using the scoring function described in section
3.5.3, our system achieved an overall accuracy of
0.5638 on the development dataset. Table 2 shows
results for the system run on the test dataset. On this
unseen dataset, the overall accuracy decreased only
slightly to 0.5500. We take this as a strong indica-
tion that the thresholds we derived from the develop-
ment dataset work well on other comparable input.
Results show that our system has performed signifi-
cantly above the 0.5 baseline that would result from
a random decision.
As shown in section 3.5.3, the values in the three
similarity plots (see figures 1, 2 and 3) obtained with
the development set seem to be scattered around the
means. Therefore it seems that the threshold values
used to the decide whether or not T entails H do not
fully reflect the semantics underlying textual entail-
ment.
The nature of the SVD calculations do not allow
us directly to observe the performance of the vari-
able size n-grams in independently aligning subject,
verb and objects from T and from H. Nevertheless
we can infer from figures 1, 2 and 3 that many of
the values shown seem to be repeated. These value
configurations can be observed in the three horizon-
tal lines. These lines better visible in figures 2 and
3 are the effect of (a) many empty vectors resulting
from the rather low number of stems represented by
columns in our Reuters-derived matrices S, V and
O, and (b) the effect of folding the n-gram vector
representations into reduced matrices with two hun-
dred dimensions.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999884461538462">
Even though our system was developed from scratch
in a very short period of time, it has already out-
performed other LSA-based approaches to recognis-
ing textual entailment (Clarke, 2006), showing that
it is both feasible and desirable to move away from
a bag-of-words semantic representation to a semi-
structured (here, SVO) semantic representation even
when using LSA techniques.
Our system displays several shortcomings and
limitations owing to its immature implementation
state. These will be addressed in future work, and
we are confident that without changing its theoret-
ical basis, this will improve performance dramati-
</bodyText>
<figure confidence="0.996955891304348">
+ 3 o
�op
+ 3 s
�sn
+ 3 o
�
on
= q (8)
v
+
vn
�vp
v
+
3 s
�sp
s
σ
o
+ ω
v
+ φ_
v
s�
o�
s v o
σ + + ω
s� v� o�
s
σ
o
+ω
o
+ω
s
+σ
v
+
v
+
�sp
vn
�on
�sn
�op
�vp
</figure>
<page confidence="0.940361">
117
</page>
<bodyText confidence="0.890929">
cally. Envisaged changes include:
</bodyText>
<listItem confidence="0.993868666666667">
• using larger matrices as input to SVD
• using the complete Reuters corpus, and adding
Wikinews texts
• performing corpus look-up for unknown words
• extracting larger chunks from S and O slots
• using advanced data analysis and machine
</listItem>
<bodyText confidence="0.997650666666667">
learning techniques to improve our scoring
function
In addition, our approach currently does not take
into consideration the directionality of the entail-
ment relationship between the two text fragments. In
cases where T1 entails T2 but T2 does not entail T1,
our approach will treat (T1, T2) and (T2, T1) as the
same pair. We expect to correct this misrepresenta-
tion by evaluating the degree of specificity of words
composing the SVOs in asymmetric entailment rela-
tionships where the first text fragment is more gen-
eral than the second one. For that purpose, one can
use term frequencies as an indicator of specificity
(Sp¨arck Jones, 1972).
Obviously, system performance could be further
improved by taking a hybrid approach as e.g. in de
Marneffe et al. (2006), but we find it more instruc-
tive to take our pure LSA approach to its limits first.
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999348">
We are grateful to Prof. Donia Scott, head of the Nat-
ural Language Generation group within the Centre
for Research in Computing of the Open University,
who made us aware of the RTE-3 Challenge and en-
couraged us to participate.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999844065217391">
[Berry1992] M. W. Berry. 1992. Large-scale sparse sin-
gular value computations. The International Journal
of Supercomputer Applications, 6(1):13–49, Spring.
[Clarke2006] Daoud Clarke. 2006. Meaning as context
and subsequence analysis for entailment. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
[de Marneffe et al.2006] Marie-Catherine de Marneffe,
Bill MacCartney, Trond Grenager, Daniel Cer, Anna
Rafferty, and Christopher D. Manning. 2006. Learn-
ing to distinguish valid textual entailments. In Pro-
ceedings of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment, Venice, Italy.
[Harris1954] Zelig S. Harris. 1954. Distributional struc-
ture. WORD, 10:146–162. Reprinted in J. Fodor and J.
Katz, The structure of language: Readings in the phi-
losophy of language, pp. 33–49, Prentice-Hall, 1964.
[Landauer and Dumais1997] T. K. Landauer and S. T. Du-
mais. 1997. A solution to Plato’s Problem. The Latent
Semantic Analysis theory of the acquisition, induction
and representation of knowledge. Psychological Re-
view, 104(2):211–240.
[Lewis et al.2004] D. D. Lewis, Y. Yang, T. Rose, and
F. Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361–397.
[Lin1994] Dekang Lin. 1994. PRINCIPAR – an effi-
cient, broad-coverage, principle-based parser. In Proc.
COLING-94, pages 42–488, Kyoto, Japan.
[Porter1980] M. F. Porter. 1980. An algorithm for suffix
stripping. Program, 14(3):130–137.
[Salton et al.1975] G. Salton, A. Wong, and C. S. Yang.
1975. A vector space model for automatic indexing.
Commun. ACM, 18(11):613–620.
[Sp¨arck Jones1972] Karen Sp¨arck Jones. 1972. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Journal of Documentation, 28(1):11–
21. Reprinted 2004 in 60(5):493–502 and in Sp¨arck
Jones (1988).
[Sp¨arck Jones1988] Karen Sp¨arck Jones. 1988. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Document retrieval systems, pages
132–142.
[Widdows2004] Dominic Widdows. 2004. Geometry and
Meaning. Number 172 in CSLI Lecture Notes. Uni-
versity of Chicago Press.
</reference>
<page confidence="0.996204">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.435305">
<title confidence="0.8972355">SVO triple based Latent Semantic Analysis for recognising textual entailment</title>
<author confidence="0.994024">Gaston Burek Christian Pietsch Anne De_Roeck</author>
<affiliation confidence="0.758302">Centre for Research in The Open</affiliation>
<address confidence="0.693519">Walton Hall, Milton Keynes, MK7 6AA,</address>
<email confidence="0.962459">Ig.g.burek,c.pietsch,a.deroeckl@open.ac.uk</email>
<abstract confidence="0.999097375">Latent Semantic Analysis has only recently been applied to textual entailment recognition. However, these efforts have suffered from inadequate bag of words vector representations. Our prototype implementation for the Third Recognising Textual Entailment Challenge (RTE-3) improves the approach by applying it to vector representations that contain semi-structured representations of words. It uses variable size n-grams of word stems to model independently verbs, subjects and objects displayed in textual statements. The system performance shows positive results and provides insights about how to improve them further.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
</authors>
<title>Large-scale sparse singular value computations.</title>
<date>1992</date>
<journal>The International Journal of Supercomputer Applications,</journal>
<volume>6</volume>
<issue>1</issue>
<publisher>Spring.</publisher>
<marker>[Berry1992]</marker>
<rawString>M. W. Berry. 1992. Large-scale sparse singular value computations. The International Journal of Supercomputer Applications, 6(1):13–49, Spring.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Meaning as context and subsequence analysis for entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>[Clarke2006]</marker>
<rawString>Daoud Clarke. 2006. Meaning as context and subsequence analysis for entailment. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Daniel Cer</author>
<author>Anna Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to distinguish valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<marker>[de Marneffe et al.2006]</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, Trond Grenager, Daniel Cer, Anna Rafferty, and Christopher D. Manning. 2006. Learning to distinguish valid textual entailments. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. WORD,</booktitle>
<pages>10--146</pages>
<location>Prentice-Hall,</location>
<note>Reprinted in</note>
<marker>[Harris1954]</marker>
<rawString>Zelig S. Harris. 1954. Distributional structure. WORD, 10:146–162. Reprinted in J. Fodor and J. Katz, The structure of language: Readings in the philosophy of language, pp. 33–49, Prentice-Hall, 1964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s Problem. The Latent Semantic Analysis theory of the acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<marker>[Landauer and Dumais1997]</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s Problem. The Latent Semantic Analysis theory of the acquisition, induction and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<marker>[Lewis et al.2004]</marker>
<rawString>D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>PRINCIPAR – an efficient, broad-coverage, principle-based parser.</title>
<date>1994</date>
<booktitle>In Proc. COLING-94,</booktitle>
<pages>42--488</pages>
<location>Kyoto, Japan.</location>
<marker>[Lin1994]</marker>
<rawString>Dekang Lin. 1994. PRINCIPAR – an efficient, broad-coverage, principle-based parser. In Proc. COLING-94, pages 42–488, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>[Porter1980]</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<marker>[Salton et al.1975]</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<booktitle>in 60(5):493–502 and in Sp¨arck</booktitle>
<volume>28</volume>
<issue>1</issue>
<pages>21</pages>
<location>Jones</location>
<note>Reprinted</note>
<marker>[Sp¨arck Jones1972]</marker>
<rawString>Karen Sp¨arck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11– 21. Reprinted 2004 in 60(5):493–502 and in Sp¨arck Jones (1988).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨arck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval. Document retrieval systems,</title>
<date>1988</date>
<pages>132--142</pages>
<marker>[Sp¨arck Jones1988]</marker>
<rawString>Karen Sp¨arck Jones. 1988. A statistical interpretation of term specificity and its application in retrieval. Document retrieval systems, pages 132–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Geometry and Meaning. Number 172 in CSLI Lecture Notes.</title>
<date>2004</date>
<publisher>University of Chicago Press.</publisher>
<marker>[Widdows2004]</marker>
<rawString>Dominic Widdows. 2004. Geometry and Meaning. Number 172 in CSLI Lecture Notes. University of Chicago Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>