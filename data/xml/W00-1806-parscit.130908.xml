<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.93297">
Taking Primitive Optimality Theory Beyond the Finite State
</title>
<author confidence="0.57312">
Daniel M. Albro
Linguistics Department
</author>
<sectionHeader confidence="0.7888755" genericHeader="abstract">
UCLA
Abstract
</sectionHeader>
<bodyText confidence="0.996842346153846">
Primitive Optimality Theory (OTP) (Eisner,
1997a; Albro, 1998), a computational model
of Optimality Theory (Prince and Smolensky,
1993), employs a finite state machine to repre-
sent the set of active candidates at each stage
of an Optimality Theoretic derivation, as well
as weighted finite state machines to represent
the constraints themselves. For some purposes,
however, it would be convenient if the set of
candidates were limited by some set of crite-
ria capable of being described only in a higher-
level grammar formalism, such as a Context
Free Grammar, a Context Sensitive Grammar,
or a Multiple Context Free Grammar (Seki et
al., 1991). Examples include reduplication and
phrasal stress models. Here we introduce a
mechanism for OTP-like Optimality Theory in
which the constraints remain weighted finite
state machines, but sets of candidates are repre-
sented by higher-level grammars. In particular,
we use multiple context-free grammars to model
reduplication in the manner of Correspondence
Theory (McCarthy and Prince, 1995), and de-
velop an extended version of the Earley Algo-
rithm (Earley, 1970) to apply the constraints to
a reduplicating candidate set.
</bodyText>
<sectionHeader confidence="0.996863" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.979768">
The goals of this paper are as follows:
</bodyText>
<listItem confidence="0.984467272727273">
• To show how finite-state models of Opti-
mality Theoretic phonology (such as OTP)
can be extended to deal with non-finite
state phenomena (such as reduplication) in
a principled way.
• To provide an OTP treatment of redupli-
cation using the standard Correspondence
Theory account.
• To extend the Earley chart parsing algo-
rithm to multiple context free grammars
(MCFGs).
</listItem>
<bodyText confidence="0.998183805555556">
The basic idea of this approach is to begin
with a non-finite-state description of the space
of acceptable candidates (e.g., candidates with
some sort of reduplication inherent in them, or
candidates which are the outputs of a syntac-
tic grammar), and to repeatedly intersect the
high-level grammar representing those candi-
dates with finite state machines representing
constraints. The intersection operation is one
of weighted intersection (where only the set of
lowest-weighted candidates survive) in order to
model Optimality Theory, and will make use of
a modified version of the Earley parsing algo-
rithm.
There are at least two alternative approaches
to that which we will propose here: to aban-
don finite state models altogether and move to
uniformly higher-level approaches (e.g., Tesar
(1996)), or to modify finite state models mini-
mally to allow for (perhaps limited) reduplica-
tion (e.g., Walther (2000)). The first of these
alternative approaches deals with context free
grammars alone, so it would not be able to
model reduplicative effects. Besides this, it
seems preferable to stick with finite-state ap-
proaches as far as possible, because phonolog-
ical effects beyond the finite state seem quite
rare. The second of these approaches seems rea-
sonable in itself, but it is not suited for the
type of analyses for which the approach laid
out here is designed. In particular, Walther&apos;s
approach is tied to One-Level Phonology, a the-
ory which limits itself to surface-true generaliza-
tions, whereas the approach here is designed to
model Optimality Theory a system with vio-
lable constraints—and in particular Correspon-
</bodyText>
<page confidence="0.997285">
57
</page>
<bodyText confidence="0.999656625">
dence Theory. Tesar&apos;s approach as well, while
it is a model of Optimality Theory, does not
seem suited to Correspondence Theory. A final
argument for using this approach, in preference
to one similar to Walther&apos;s approach, is that it
can be extended to cover other non-finite-state
areas of phonology, such as phrasal stress pat-
terns, with no modification to the basic model.
</bodyText>
<equation confidence="0.916812666666667">
[ + + + + + + + ]
[ + I + + +
C: [ + I - [ + I +
V: - - [ +
C: - - - - [ + I +
V: - - [ +
</equation>
<figure confidence="0.995270666666667">
(b) OTP
C V C
(a) Conventional
</figure>
<sectionHeader confidence="0.591278" genericHeader="method">
2 Quick Overview of OTP
</sectionHeader>
<subsectionHeader confidence="0.987348">
2.1 Optimality Theory
</subsectionHeader>
<bodyText confidence="0.998831333333333">
Optimality Theory (OT), of which OTP is a
formalized computational model, is structured
as follows, with three components:
</bodyText>
<listItem confidence="0.990048">
1. Gen: a procedure that produces infinite
surface candidates from an underlying rep-
resentation (UR)
2. Con: a set of constraints, defined as func-
tions from representations to integers
3. Eva!: an evaluation procedure that, in suc-
cession, winnows out the candidates pro-
duced by Gen.
</listItem>
<bodyText confidence="0.9952955">
So OT is a theory that deals with potentially in-
finite sets of phonological representations. The
OT framework does not by itself specify the
character of these representations, however.
</bodyText>
<sectionHeader confidence="0.666167" genericHeader="method">
2.2 Primitive Optimality Theory
(OTP)
</sectionHeader>
<bodyText confidence="0.845662333333333">
The components of OT, as modeled by OTP
(see Eisner (1997a), Eisner (1997b), Albro
(1998)):
</bodyText>
<listItem confidence="0.999849833333333">
1. Gen: a procedure that produces from an
underlying representation a finite state ma-
chine that represents all possible surface
candidates that contain that UR (always
an infinite set)
2. Con: a set of constraints definable in
</listItem>
<bodyText confidence="0.9804605">
a restricted formalism internally repre-
sented as Weighted Deterministic Finite
Automata (WDFAs) which accept any
string in the representational alphabet.
The weights correspond to constraint vio-
lations. The weights passed through when
accepting a string are the violations in-
curred by that string.
</bodyText>
<figureCaption confidence="0.977431">
Figure 1: OTP Representation
</figureCaption>
<listItem confidence="0.9973082">
3. Eva!: the following procedure, where
/ represents the input FSM pro-
duced by Gen, and M is a machine
representing the output set of candi-
dates:
</listItem>
<equation confidence="0.957013">
M /
</equation>
<bodyText confidence="0.9629064">
for all C. E Con, taken in rank order
do
M intersection of M with Ci
Remove non-optimal paths from M
Zero out weights in M
</bodyText>
<subsectionHeader confidence="0.944314">
end for
</subsectionHeader>
<bodyText confidence="0.999634095238095">
Representations in OTP are gestural scores us-
ing symbols from the set {—,+, Lb 1}. See
Figure 1 for an example. This figure shows
a CVCC syllable in a conventional notation,
and also in OTP notation. The OTP nota-
tion is slightly more complex, though, in that
it also shows an underlying form for the sylla-
ble. The overlap relation of the conventional
notation&apos;s association lines is expressed in the
OTP notation by the presence of constituent in-
teriors (&amp;quot;+&amp;quot;) in the same vertical slice through
the diagram. This same-time-slice-membership
relation is also used to show correspondence.
Thus we see from this diagram that the sur-
face &amp;quot;CVCC&amp;quot; syllable corresponds to underly-
ing &amp;quot;VCC,&amp;quot; and that the initial &amp;quot;C&amp;quot; does not
correspond to any underlying segment. Note
that tiers with no special marking are used to
represent the surface level of representation, and
underlined tiers are used to represent the under-
lying level of representation.
</bodyText>
<sectionHeader confidence="0.897236" genericHeader="method">
3 Handling Reduplication: Overview
</sectionHeader>
<subsectionHeader confidence="0.975589">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.6456105">
Finite State Machines are useful in phonology
because it is possible to take any two finite
</bodyText>
<page confidence="0.976838">
58
</page>
<equation confidence="0.830166222222222">
S
0 1 0 0 1 0
A
(0 1 0, 0 1 0)
0 A 0
(1 0, 1 0)
1 A 1
(0, 0)
0 0
</equation>
<subsectionHeader confidence="0.9994205">
3.3 Representation of Reduplicative
Forms in OTP
</subsectionHeader>
<bodyText confidence="0.99928328">
OTP constraints are inherently local they can
only refer to overlap or non-overlap of interiors
or edges in an instant of time. Therefore, to en-
force correspondences between forms, they must
be juxtaposed so as to occur in the same time-
slices. In OTP, correspondence between the sur-
face and underlying forms is established by us-
ing one set of tiers for the surface form (each tier
represents either a feature or a type of prosodic
constituent) and another corresponding set for
the underlying form. For example, the tier son
might specify the distribution of the surface fea-
ture &amp;quot;sonorant&amp;quot; , while the tier son would specify
its underlying correspondent. Elements of those
tiers placed in the same time-slice are consid-
ered to be in correspondence with one another.
In order to create correspondence between two
portions of the same surface form, then, we need
to somehow have them simultaneously juxta-
posed so as to appear in the same segments of
time and separated in time as they will be on
the surface. This is accomplished by a represen-
tational trick: in the example of reduplication, a
copy of the reduplicant&apos;s surface form is placed
in a special set of tiers within the base:
</bodyText>
<equation confidence="0.990737857142857">
BASE RED2
URi UR2
REDi
or
RED2 BASE
UR2 URi
REDi
</equation>
<bodyText confidence="0.999942277777778">
In these representations SL stands for the sur-
face level of representation, UL for the under-
lying level, and RL for the special reduplicant
level (the place where a copy of the reduplicant
is kept). URi and UR2 are identical in the
input, and REDi and RED2 need to be kept
identical by other means. The means chosen
here is to use an MCFG enforcing the identity.
BASE-RED correspondence constraints operate
upon REDi while templatic and general sur-
face well-formedness constraints operate upon
RED2. An example of this sort of representa-
tion might help here. Suppose that there are
two surface tiers, C and V. Then a form such
as [CV+CVC] (with CV prefixing reduplication,
assuming that the base is CVC, and with the
underlying form RED±/VC/) might be repre-
sented as follows:
</bodyText>
<equation confidence="0.999943">
[ + ] - [
[ + ]
[ + 1
[ + + + ++ ]
[ + + + + + 1
</equation>
<bodyText confidence="0.977346523809524">
Note here that the special BASE and RED tiers
indicate the portions of the surface forms that
are the base and reduplicant, and that the redu-
plicant level of representation (that is, the level
that holds the copy of the reduplicant used for
correspondence) is present on the tiers labeled
with double underlines. The INS tier represents
a time-discrepancy between the levels of repre-
sentation where time does not exist on the un-
derlying level (so the period of time taken up by
the initial C in the surface reduplicant and base
doesn&apos;t correspond to anything in the underly-
ing level), and the DEL tier represents time that
does not exist on the surface level, so the time
taken up by the final C in the underlying form of
the reduplicant does not correspond to anything
on the surface. The RDEL tier is a mirror of the
contents of the DEL tier in the surface redupli-
cant, and thus represents time that does not
exist in the special reference copy of the redu-
plicant. This representation allows us to notice
that the reduplicant fits a CV template the
left edge of it is aligned with a surface C, the
right edge with a surface V, and there are no
other segments within it. (The relevant OTP
constraints to reinforce this would be &amp;quot;RED [ —&gt;
Cr &amp;quot;]RED ]V,&amp;quot; &amp;quot;]C I C[ I RED,&amp;quot; and &amp;quot;]C
I V[ _L RED,&amp;quot; if highly ranked and in that or-
der.)
In terms of translating these representations
to finite state machines (or to strings), we use
the alphabet { —, +, [, ], I}, so that each FSM
edge is labeled with a member of this alphabet.
This representation differs from that of earlier
accounts of OTP, in that the FSM edges in those
accounts represented entire time slices, whereas
SL:
UL:
RL:
SL:
UL:
RL:
</bodyText>
<figure confidence="0.997773142857143">
C:
V:
C:
V:
C:
V:
INS:
DEL:
RDEL:
RED:
BASE:
1 -
[ +
1 -
</figure>
<page confidence="0.971247">
60
</page>
<bodyText confidence="0.989293666666667">
an edge in this representation represents a single
tier in a time slice. As an example, the repre-
sentation of:
</bodyText>
<equation confidence="0.9918405">
C: [ ]
V: - - -
</equation>
<bodyText confidence="0.999887">
is as shown in Figure 3, where the &amp;quot;C&amp;quot; and &amp;quot;V&amp;quot;
labels are not part of the representation, but
just there to ease reading.
</bodyText>
<subsectionHeader confidence="0.988484">
3.4 The Grammar Used
</subsectionHeader>
<bodyText confidence="0.987944393442623">
The grammar used here is a bit complicated,
but the important thing to note about it is that
it generates exactly the set of possible OTP out-
put forms in which the special reduplicant ref-
erence level of representation contains an exact
copy of the surface reduplicant, placed within
the time-duration of the base. The grammar
for a situation in which there are two surface
tiers appears in Figure 4. Extending this gram-
mar to other numbers of tiers is straightforward.
The constituents of this grammar are as follows:
S The start symbol.
Non Non-reduplicating material (such as non-
reduplicating morphemes) before and/or
after the reduplicating material.
SSR The surface tiers in a time-slice.
UR The underlying tiers in a time-slice.
MRD The reduplicant reference-level tiers in a
time-slice where the tiers must contain the
value — (that is, outside of the base, which
is the only place where the reduplicant level
is used).
Rd/Rdl/Rd2 The reduplicating part of an
utterance.
BDR A right-facing boundary (allows any-
thing to be in the surface tiers during its
time-slice, and copies the right-facing half
of that material into the reduplicant).
BDL A left-facing boundary (see BDR).
B The surface tiers in a time-slice plus identical
material in the reduplicant tiers. Thus B
represents an item in the reduplicant plus
its copy in the special reduplicant reference
level.
The remaining non-terminals define different
values for the INS, DEL, RDEL, RED, and
BASE tiers, where INS and DEL are as de-
fined in Albro (1998), RDEL represents time
that does not exist in the reduplicant, RED rep-
resents the reduplicant (as a morpheme bound-
ary), and BASE represents the base as a mor-
pheme boundary:
NBR represents the state of not being in the
base or the reduplicant.
RLE represents the left edge of the reduplicant.
RRE represents the right edge of the redupli-
cant.
BLE represents the left edge of the base.
BRE represents the right edge of the base.
RB represents a boundary between a redupli-
cant and a base, where the reduplicant
comes first.
BR represents the reverse of RB.
RED represents the inside of the reduplicant.
BASE represents the inside of the base.
In this grammar any given time-slice will be de-
fined as SSR or the first component of one of
the B categories, followed by UR, followed by
MRD or the second component of one of the
B categories, followed by one of the NBR, etc.,
categories.
</bodyText>
<sectionHeader confidence="0.993587" genericHeader="method">
4 The Earley Algorithm
</sectionHeader>
<bodyText confidence="0.999963842105263">
The Earley algorithm is an efficient chart pars-
ing method. Chart parsing can be seen as a
method for taking the intersection of a string
or FSM with a CFG (later, an MCFG). Here
we take a CFG as a 4-tuple (V, N, P, S) where
V represents the set of terminals in the gram-
mar, N represents the set of non-terminals, P
represents the set of productions, and S E N
is the start symbol. In the definitions to fol-
low, a, 0, and 7 represent arbitrary members
of (V UN)*, A and C represent arbitrary mem-
bers of N, a and b represent arbitrary members
of V, p represents an arbitrary member of P,
and the indices i, j, and k represent positions
within the input string to be parsed, numbered
as in Figure 5.
In the standard definition, a member of the
chart is a 3-tuple (i, C j), where i repre-
sents the position at the beginning of the input
</bodyText>
<page confidence="0.985012">
61
</page>
<figure confidence="0.991123">
-V
-V
</figure>
<figureCaption confidence="0.999848">
Figure 3: FSM Representation Used Here
</figureCaption>
<bodyText confidence="0.9999485">
string covered by a and j represents the posi-
tion at the end of the covered portion of the
string. The parsing operation in the standard
definition, which parses a single input string,
is defined as a closure via the following three
inference rules of a chart initially consisting of
</bodyText>
<equation confidence="0.8371568">
(0, S •a, 0):
(i,C.—&gt;ce•A[3.,j)
predict: if A —&gt; ry E P (if 7 begins
with a terminal, that terminal must be the
symbol at position j in the input string)
(i,C-7-a.a13,j) .
scan: (i,C —acto,j +1) it a is the symbol after j
(i,C—&gt;ce•A[3,j) (j,A-ye,k)
complete:
,C &lt;=r 3,k)
</equation>
<bodyText confidence="0.964666333333333">
The input string is recognized if the chart con-
tains an element (0, S a., n), where n is the
final position of the input string.
</bodyText>
<sectionHeader confidence="0.986346" genericHeader="method">
5 Extending Earley
</sectionHeader>
<bodyText confidence="0.99993675">
The algorithm presented so far just checks to see
whether a particular string exists in a grammar.
In order for it to be useful for our purposes, the
following extensions must be made:
</bodyText>
<listItem confidence="0.9844638">
1. Intersection with an FSM, not just a string
2. Recovery of intersection grammar
3. Weights (intersection should allow lowest-
weight derivations only)
4. MCFGs
</listItem>
<subsectionHeader confidence="0.961246">
5.1 Intersection with an FSM
</subsectionHeader>
<bodyText confidence="0.9999585">
To modify the algorithm to intersect a grammar
with an FSM, we replace the input string with
an FSM, and change our definition of a chart
entry. Now, a chart entry is a 3-tuple (i, C
a • /3,j), where i represents the first FSM state
covered by a and j represents the last FSM state
covered. We define an FSM here as a 5-tuple
(Q, E, s, F, M), where Q is the set of states in
the FSM, E is the label alphabet for the FSM
(for our purposes E is always the same as V
for all grammars in use), s E Q is the start
state, F C Q is the set of final states of the
FSM, and M is a set of 3-tuples (i, a, j), which
represent transitions from state i to state j with
label a. Given these redefinitions we can then
just modify the scan rule:
</bodyText>
<equation confidence="0.858258666666667">
,C—olea13,j) . „ .
(z,Caa.,3,j+1) lt (3 al E M, where M
scan:
</equation>
<bodyText confidence="0.900349666666667">
is the input FSM.
and the predict rule in the obvious way:
predict: (z,C —oe•Al 3,j)
</bodyText>
<listItem confidence="0.617232">
- if A E P (if 7 is of
</listItem>
<equation confidence="0.626433">
, 1)
</equation>
<bodyText confidence="0.80738">
the form a ry&apos;, (j, a, k) E M must hold as
well)
Note that the initial entry in the chart is now
(s, S *a, s).
</bodyText>
<subsectionHeader confidence="0.98549">
5.2 Grammar Recovery
</subsectionHeader>
<bodyText confidence="0.999797125">
It is possible to recover the output of intersec-
tion by increasing slightly what is in the chart.
In particular, for every item on the chart, we
note how it got there (just the last step). Each
item on the chart may be referred to by its col-
umn number C and its position N within that
column. We annotate only items produced by
scan and complete steps, as follows:
</bodyText>
<listItem confidence="0.9997645">
• sC I N
• cCi I Ni; C2 /N2
</listItem>
<bodyText confidence="0.995891">
where Ci/Ni refers to the (j, A —&gt; ry., k) item
from the complete step, and C2/N2 refers to the
</bodyText>
<page confidence="0.982062">
62
</page>
<figure confidence="0.976767787234043">
Non Rd Non
Rd Non
Non Rd
Rd
Non SSR UR MRD NBR
Non SSR UR MRD NBR
SSR AA
UR AA
MRD
Rd Rdl 0 Rdl
BDR BDRO0 BDR10, )
BDROi BDR11
BDLO0 BDLlo, )
BDL BDLOi BDLli
B—
A—*
BDR0
BDLn
Bn
continuing with
NBR
RLE
RRE
BLE
BRE
RB
BR
RED
BAS
In cases where the reduplicant precedes the base, the
reduplication rules will appear as follows:
(BDR0 UR MRD RLE Rd20,
Rdl BDL 0 UR BDRi RB Rd21
SSR UR BDLi BRE
Rd2 Bo UR MRD RED, )
SSR UR B1 BAS
Rd20 Bo UR MRD RED,
Rd21 SSR UR B1 BAS
Otherwise, where the base precedes the reduplicant, the
rules will appear as follows:
UR BDRi BLE Rd20,
UR BDLi BR Rd21
UR MRD RRE
BAS, )
RED
BAS,
MRD RED
</figure>
<figureCaption confidence="0.999616">
Figure 4: Reduplication Grammar
</figureCaption>
<figure confidence="0.976864333333333">
a
t t t t
0 1 2 3
</figure>
<figureCaption confidence="0.937028">
Figure 5: Numbering of string positions in the
string &amp;quot;abc&amp;quot;
</figureCaption>
<bodyText confidence="0.978655117647059">
(i, C a • AO, j) item. A chart item is thus
now a 4-tuple (i, C a • )3, j, H), where H is a
set of history items of the type described here,
one for each scan or complete step that put the
item there.
Recovery of a grammar then starts from the
&amp;quot;success items,&amp;quot; that is items in the chart that
begin in state 1 and end with a final state and
represent a production from the start symbol
of the grammar, with the Earley position dot
at the end of the production. We then move
from right to left within those productions, fill-
ing in the state pairs for each constituent we
pass, and tracing through their productions as
well. Whenever we get to the left side of a pro-
duction, we output it. The exact algorithm is
as follows:
</bodyText>
<equation confidence="0.677780285714286">
GrammarRecovery(chart)
queue
for all success items (s,S f c
F, Ho) at (C, N) do
queue up (C, N) onto queue
while queue not empty do
(C, N) dequeue from queue
item item at (C, N): (i, A —&gt;
wb,j,H1)
pos pos. of • in item
RHSs GetRHSs([[]], item, pos,
queue)
for all RHS E RHSs do
output &amp;quot;A(i, j) RHS&amp;quot;
</equation>
<listItem confidence="0.947837666666667">
end for
end while
end for
</listItem>
<equation confidence="0.9085136">
GetRHSs(rhss, item, pos, queue)
if pos = 0 then
return rhss
end if
new_rhss
</equation>
<bodyText confidence="0.7838">
for all history path components hitem
of item do
</bodyText>
<figure confidence="0.922905074074074">
rhss&apos; &lt;— copy rhss
extend(rhss&apos;, hitem, pos, queue)
1, I,
1,
]
1, I,
I
A A A
A A A
A A A
A A A
A A A
A A
A A
A A
A A
Rdl SSR
Rd2 BDR0
BDL0
SSR
Bo
Rd20
Rd2i
UR B1
UR MRD
SSR UR
Bo UR
</figure>
<page confidence="0.937557">
63
</page>
<construct confidence="0.4695546">
add rhss&apos; to new_rhss
end for
return new_rhss
extend(rhss, hitem, pos, queue)
if hitem = s(C , N) then
</construct>
<bodyText confidence="0.675603125">
prepend scanned symbol to each rhs
E rhss
prey item at (C, N)
else if hitem = c(C Ni; C21 N2) then
(i, A ye, j, H) item at (C1, Ni)
prepend A(i, j) to each rhs E rhss
enter (C1, N1) into queue
prey item at (C2, N2)
</bodyText>
<equation confidence="0.942313666666667">
end if
return GetRHSs(rhss, item, pos-1,
queue)
</equation>
<subsectionHeader confidence="0.541083">
5.3 Weights
</subsectionHeader>
<bodyText confidence="0.999172090909091">
The basic idea for handling weights is an adap-
tion from the Viterbi algorithm, as used for
chart parsing of probabilistic grammars. Basi-
cally, we reduce the grammar to allow only the
lowest-weight derivations from each new cate-
gory.
Implementation: Each chart item has an as-
sociated weight, computed as follows:
predict: weight of the predicted rule A -y
scan: sum of the weight of the item scanned
from and the weight of the FSM edge
scanned across.
complete: sum of the weights of the two items
involved
We build new chart items whenever permitted
by the rules given in previous sections, assigning
weights to them by the above considerations. If
no equivalent item (equivalence ignores weight
and path to the item) is in the chart, we add
the item. If an equivalent item is in the chart,
there are three possible actions, according to the
weight of the new item:
</bodyText>
<listItem confidence="0.998790125">
1. Higher than the old item: do nothing
(don&apos;t add the new path).
2. Lower than the old item: remove all other
paths to the item, add this path to the
item. Adjust weights of all items built from
this one downward.
3. Same as the old item: add the new path to
the item.
</listItem>
<bodyText confidence="0.999597333333333">
A chart item is thus now a 5-tuple (w, i, C
a • )3, j, H), where w represents a weight, and
all the other items are as before.
</bodyText>
<subsectionHeader confidence="0.68237">
5.4 MCFGs
</subsectionHeader>
<bodyText confidence="0.999928125">
To extend the Earley algorithm to MCFGs, we
first reduce the chart-building part of the Earley
algorithm for MCFGs to the already-worked out
algorithm for CFGs by converting the MCFG
into a (not-equivalent) CFG. We then mod-
ify the grammar-recovery step to convert the
CFG produced into an MCFG, verifying that
the MCFG produced is a proper one.
</bodyText>
<subsubsectionHeader confidence="0.658905">
5.4.1 Adjustments to the
</subsubsectionHeader>
<subsectionHeader confidence="0.32622">
Chart-Building Algorithm:
</subsectionHeader>
<bodyText confidence="0.9792086875">
First, we treat each part of the rule as a sepa-
rate rule, and use the regular algorithm. Thus,
( 0 )
becomes B0—&gt; 0 and B1 1. Hav-
ing separated a single rule such as C (a, 0)
into two parts Co a and C1 —&gt; 0, we need to
keep track, when building the chart and after, of
which rule in the associated MCFG each chart
item refers to. These annotations will be useful
in Grammar Recovery (something like Co —&gt; a
can only be combined with C1 /3 if they both
come from the same MCFG rule). Thus, a chart
item is a 6-tuple (r, w, C a • 0, j, H), where
r is the rule number from the original MCFG to
which the production C a • corresponds,
and all the others are as before.
</bodyText>
<sectionHeader confidence="0.3618615" genericHeader="method">
5.4.2 Adjustments to Grammar
Recovery
</sectionHeader>
<bodyText confidence="0.994337714285714">
As before, followed by a final combinatory and
checking step:
for all non-terminals A with arity n do
for all possible combinations Ao(i, j)
N, Ai (k , 1) —&gt; , (m, n) do
if the MCFG condition applies to the
combination then
</bodyText>
<listItem confidence="0.904127">
output A(i, j)(k, 1) . . . (m, n) -4
end if
end for
end for
</listItem>
<bodyText confidence="0.899687">
where the MCFG condition is as follows:
All 7, on the right hand side of the
combination must be derived from the
</bodyText>
<page confidence="0.998694">
64
</page>
<bodyText confidence="0.999734">
same rule in the original set of rules
and their yields must not overlap each
other in the FSM.
</bodyText>
<equation confidence="0.69303925">
Column 1 (j = 1, i = 1)
r w EP
1 o •Ao
2 0 A0 •1
</equation>
<figure confidence="0.87332305">
3 0 A0 e0
4 0 Ao .0 Ao
5 0 Ao •1 Ao
5 0 A0 1 • Ao
4 0 Ao 0 • Ao
3 0 Ao 0.
1 0 S Ao • A1
{c1/7; 1/0, c1/10; 1/0,
5 0 Ao 1 Ao •
{c1/7; 1/5, c1/10; 1/5,
4 0 Ao 0 Ao•
{c1/7; 1/6, c1/10; 1/6,
2 0 • el
3 0 • -&gt; e0
4 0 A1 .0 A1
5 0 A1 •1 A1
5 0 • 1 • A1
4 0 • 0 • A1
3 0 A1 Oe
1 0 S Ao Ai•
</figure>
<equation confidence="0.921145">
{c1/17; 1/8, c1/20; 1/8, c1/19; 1/8, c1/21; 1/8}
5 0 A1 1 Ale
{c1/17; 1/14, c1/20; 1/14, c1/19; 1/14, c1/21; 1/14}
4 0 A1 0 Ale
{c1/17; 1/13, c1/20; 1/13, c1/19; 1/13, c1/21; 1/13}
2 0 A1 -&gt; le {s1/11}
2 0 Ao le {s1/1}
c1/9; 1/0, c1/22; 1/0}
c1/9; 1/5, c1/22; 1/51
c1/9; 1/6, c1/22; 1/6}
</equation>
<figure confidence="0.88682525">
0
0
0
0
</figure>
<bodyText confidence="0.999841545454546">
Given the way the chart-parsing and recovery
algorithms work, the MCFG condition will be
satisfied if we simply check that all the elements
of the combination come from the same rule in
the original MCFG. This will result in some
invalid rules in the output grammar, but this
simple check guarantees that these rules will be
such that they will be unable to participate in
derivations, since their right-hand sides will re-
fer to categories that do not head any produc-
tions.
</bodyText>
<subsectionHeader confidence="0.581899">
5.5 Example
</subsectionHeader>
<bodyText confidence="0.949408">
As an example, let&apos;s take the simple reduplica-
tion grammar from before:
</bodyText>
<table confidence="0.6777026">
Ao
(1,1)
(0,0)
(0 Ao, 0 AO
(1 Ao, 1 A1)
</table>
<bodyText confidence="0.819939833333333">
and intersect it with the machine
1/0
This machine generates the set of strings
{011}±, but weights all strings ending with 0.
The corresponding CFG-grammar used for
the chart-building step is as follows:
</bodyText>
<table confidence="0.916173555555556">
A0 A1
Ao -} 1
A1 1
Ao 0
A1 0
Ao 0A0
A1 0A1
Ao -1A0
A1 1 Ai
</table>
<bodyText confidence="0.992482">
The chart produced by the chart-building
part of the algorithm is as follows:
</bodyText>
<figure confidence="0.925945631578947">
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Column 2 (j = 2, i = 1)
W E P
5 0 Ao 1 • Ao {s1/4}
4 1 Ao 0 • Ao {s1/3}
3 1 Ao Oe {81/2}
1 0 S Ao • Ai
{c2/13; 1/0, c2/5; 1/0, c2P1; 1/0}
5 0 Ao 1 Ao•
{c2/13; 1/5, c2/5; 1/5, c2/4; 1/5}
4 0 A0 Ao•
{c2/13; 1/6, c2/5; 1/6, c2/4; 1/6}
5 0 A1 1 • A1 {81/14}
4 1 A1 0 • A1 {81/13}
3 1 A1 Oe {s1/12}
1 0 S Ao
</figure>
<equation confidence="0.683904">
{c2/12; 1/8, c2/11; 1/8, c2/10; 1/8}
5 0 A1 1 Al.
{c2/12; 1/15, c2/11; 1/15, c2/10; 1/15}
4 0 A1 0 Ale
{c2/12; 1/16, c2/11; 1/16, c2/10; 1/16}
</equation>
<bodyText confidence="0.792447333333334">
2 0 A1 le {81/11}
2 0 Ao le {s1/1}
In this chart the items with an empty history
list were entered by prediction steps. The &amp;quot;suc-
cess item&amp;quot; for this grammar is then item (2,9):
(r = 1,w = 0,i = 1,p = S Ao Ai., j =
</bodyText>
<figure confidence="0.910533222222222">
0
1
2
3
4
5
6
7
8
9
10
11
12
13
65
2, H = {c2/12; 1/8, c2/11; 1/8, c2/10; 1/8}), so {c1/7; 1/5, c1/10; 1/5, c1/9; 1/5, c1/22; 1/5}),
begin there:
S(1,2) Ao A1
</figure>
<bodyText confidence="0.999666333333333">
We then queue up (2,12), (2,11), and (2,10),
noting that for all of these the states for Ai
are (1,2), and we move to item (1,8): (r =
1,w = 0,i = 1,p = S Ao • Ai, j = 1,H =
{c1/7; 1/0, c1/10; 1/0, c1/9; 1/0, c1/22; 1/0}).
Here we queue up (1,7), (1,10), (1,9), and
(1,22), noting that for all of these the states for
Ao are (1,1). Moving to (1,0), we note that we
are done, and we thus output a complete rule:
</bodyText>
<equation confidence="0.972341">
(rl) S(1, 2) -&gt; A0(1, 1) Ai(1, 2).
</equation>
<bodyText confidence="0.9391145">
We then encounter (2,12) on the queue: (r =
2,w = 0,i = 1,p = 1.,j = 2,H =
{s1/11}), which can be output with no further
ado:
</bodyText>
<equation confidence="0.957887333333333">
(r2) A1(1, 2) 1
Moving to item (2,11) (r = 4,w
0,i = 1,p = 0 A.,j =
</equation>
<bodyText confidence="0.982470666666667">
2, H = {c2/12; 1/16, c2/11; 1/16, c2/10; 1/16})
we don&apos;t need to queue anything, and we can
see that the output will be:
</bodyText>
<equation confidence="0.873259">
(r4) Ai(1, 2) -&gt; 0*(1, 2)
Item (2,10) is (r = 5,w = 0,i
1,p = 1A1.,j = 2,H
{c2/12; 1/15, c2/11; 1/15, c2/10; 1/15}), so we
output
</equation>
<bodyText confidence="0.8254398">
(r5) Ai (I, 2) -&gt; 1 Ai (I, 2)
We now move on to item (1,7): (r = 3, w =
0,i = 1,p = Ao 0.,j = 1,H = {s1/2}),
which we output as
(r3) Ao(1, 1) 0.
Item (1,10) is (r = 4,w = 0,i
1,p = Ao 0 Ao•, j = 1,H
{c1/7; 1/6, c1/10; 1/6, c1/9; 1/6, c1/22; 1/6}).
In dealing with this we need to queue nothing,
and we output:
</bodyText>
<equation confidence="0.94114">
(r4) Ao(1, 1) -&gt; 0 Ao(1, I)
</equation>
<bodyText confidence="0.6419608">
Moving to (1,9), which is (r = 5,w
0,i = 1,p = Ao 1 Ao•, j = 1,H
we queue nothing and output
(r5) A0(1, 1) 1 Ao(1, 1)
Finally we get to (1,22): (r = 2,w = 0,i =
</bodyText>
<equation confidence="0.793047666666667">
1,p = Ao 1.,j = 1,H = {81/1}), which gets
output as
(r2) Ao(1, 1) -&gt; 1
</equation>
<bodyText confidence="0.9579865">
Collecting these together (for category A), we
get the following pairings:
</bodyText>
<figureCaption confidence="0.31773925">
Ao(1, 1) Ai (1, 2) -&gt; 1
Ao(1, 1) 0
Ao(1, 1) 0 Ao(1, 1) A1(1,2) 0 A1(1, 2) )
Ao(1, 1) 1 Ao(1, 1) Ai (1, 2) 1 A1(1, 2)
</figureCaption>
<bodyText confidence="0.96886225">
Note that the &amp;quot;pair&amp;quot; for (r3) has no second
member, so nothing will be output for it. Com-
bining the compatible rules, we get the following
grammar:
</bodyText>
<figure confidence="0.69593">
S(1, 2) A(1, 1)(1, 2)o A(1, 1)(1, 2)1
A(1,1)(1,2) (1,1)
(0 Ao(1, 1)(1, 2),0 A1(1, 1) (1, 2))
(1 Ao(1, 1)(1, 2), 1 A1(1, 1) (1, 2))
which is equivalent to the grammar:
S Ao
A (1,1)
I (0 Ao, A1)
(1 Ao, 1A1)
</figure>
<bodyText confidence="0.993012">
This grammar indeed represents the best out-
puts from the intersection-all reduplicating
forms which end in a 1.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997268666666667">
Daniel M. Albro. 1998. Evaluation, implemen-
tation, and extension of Primitive Optimality
Theory. Master&apos;s thesis, UCLA.
Jay Earley. 1970. An efficient context-free pars-
ing algorithm. Comm. of the ACM, 6(2):451-
455.
Jason Eisner. 1997a. Efficient generation in
primitive Optimality Theory. In Proceedings
of the ACL.
Jason Eisner. 1997b. What constraints should
OT allow? Handout for talk at LSA,
Chicago, January.
</reference>
<page confidence="0.855268">
66
</page>
<reference confidence="0.998605">
John McCarthy and Alan Prince. 1995. Faith-
fulness and reduplicative identity. In J. Beck-
man, S. Urbanczyk, and L. Walsh, edi-
tors, Papers in Optimality Theory, num-
ber 18 in University of Massachusetts Occa-
sional Papers, pages 259-384. GLSA, UMass,
Amherst.
Alan Prince and Paul Smolensky. 1993. Opti-
mality Theory: Constraint interaction in gen-
erative grammar. Technical Report 2, Center
for Cognitive Science, Rutgers University.
H. Seki, T. Matsumura, M. Fujii, and
T. Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science,
88:191-229.
Bruce Tesar. 1996. Computing optimal descrip-
tions for optimality theory grammars with
context-free position structures. In Proceed-
ings of ACL.
Nike van Vugt. 1996. Generalized context-free
grammars. Master&apos;s thesis, Universiteit Lei-
den. Internal Report 96-12.
Markus Walther. 2000. Finite-state redupli-
cation in one-level prosodic morphology. In
Proceedings of NAACL-2000, pages 296-302,
Seattle, WA.
</reference>
<page confidence="0.999503">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679693">
<title confidence="0.999498">Taking Primitive Optimality Theory Beyond the Finite State</title>
<author confidence="0.99995">Daniel M Albro</author>
<affiliation confidence="0.787683">Linguistics UCLA</affiliation>
<abstract confidence="0.997997148148148">Primitive Optimality Theory (OTP) (Eisner, 1997a; Albro, 1998), a computational model of Optimality Theory (Prince and Smolensky, 1993), employs a finite state machine to represent the set of active candidates at each stage of an Optimality Theoretic derivation, as well as weighted finite state machines to represent the constraints themselves. For some purposes, however, it would be convenient if the set of candidates were limited by some set of criteria capable of being described only in a higherlevel grammar formalism, such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple Context Free Grammar (Seki et al., 1991). Examples include reduplication and phrasal stress models. Here we introduce a mechanism for OTP-like Optimality Theory in which the constraints remain weighted finite state machines, but sets of candidates are represented by higher-level grammars. In particular, we use multiple context-free grammars to model reduplication in the manner of Correspondence Theory (McCarthy and Prince, 1995), and develop an extended version of the Earley Algorithm (Earley, 1970) to apply the constraints to a reduplicating candidate set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Albro</author>
</authors>
<title>Evaluation, implementation, and extension of Primitive Optimality Theory.</title>
<date>1998</date>
<tech>Master&apos;s thesis, UCLA.</tech>
<contexts>
<context position="4591" citStr="Albro (1998)" startWordPosition="753" endWordPosition="754">ree components: 1. Gen: a procedure that produces infinite surface candidates from an underlying representation (UR) 2. Con: a set of constraints, defined as functions from representations to integers 3. Eva!: an evaluation procedure that, in succession, winnows out the candidates produced by Gen. So OT is a theory that deals with potentially infinite sets of phonological representations. The OT framework does not by itself specify the character of these representations, however. 2.2 Primitive Optimality Theory (OTP) The components of OT, as modeled by OTP (see Eisner (1997a), Eisner (1997b), Albro (1998)): 1. Gen: a procedure that produces from an underlying representation a finite state machine that represents all possible surface candidates that contain that UR (always an infinite set) 2. Con: a set of constraints definable in a restricted formalism internally represented as Weighted Deterministic Finite Automata (WDFAs) which accept any string in the representational alphabet. The weights correspond to constraint violations. The weights passed through when accepting a string are the violations incurred by that string. Figure 1: OTP Representation 3. Eva!: the following procedure, where / r</context>
<context position="12217" citStr="Albro (1998)" startWordPosition="2115" endWordPosition="2116">cant level is used). Rd/Rdl/Rd2 The reduplicating part of an utterance. BDR A right-facing boundary (allows anything to be in the surface tiers during its time-slice, and copies the right-facing half of that material into the reduplicant). BDL A left-facing boundary (see BDR). B The surface tiers in a time-slice plus identical material in the reduplicant tiers. Thus B represents an item in the reduplicant plus its copy in the special reduplicant reference level. The remaining non-terminals define different values for the INS, DEL, RDEL, RED, and BASE tiers, where INS and DEL are as defined in Albro (1998), RDEL represents time that does not exist in the reduplicant, RED represents the reduplicant (as a morpheme boundary), and BASE represents the base as a morpheme boundary: NBR represents the state of not being in the base or the reduplicant. RLE represents the left edge of the reduplicant. RRE represents the right edge of the reduplicant. BLE represents the left edge of the base. BRE represents the right edge of the base. RB represents a boundary between a reduplicant and a base, where the reduplicant comes first. BR represents the reverse of RB. RED represents the inside of the reduplicant. </context>
</contexts>
<marker>Albro, 1998</marker>
<rawString>Daniel M. Albro. 1998. Evaluation, implementation, and extension of Primitive Optimality Theory. Master&apos;s thesis, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Comm. of the ACM,</journal>
<pages>6--2</pages>
<contexts>
<context position="1218" citStr="Earley, 1970" startWordPosition="184" endWordPosition="185"> a higherlevel grammar formalism, such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple Context Free Grammar (Seki et al., 1991). Examples include reduplication and phrasal stress models. Here we introduce a mechanism for OTP-like Optimality Theory in which the constraints remain weighted finite state machines, but sets of candidates are represented by higher-level grammars. In particular, we use multiple context-free grammars to model reduplication in the manner of Correspondence Theory (McCarthy and Prince, 1995), and develop an extended version of the Earley Algorithm (Earley, 1970) to apply the constraints to a reduplicating candidate set. 1 Introduction The goals of this paper are as follows: • To show how finite-state models of Optimality Theoretic phonology (such as OTP) can be extended to deal with non-finite state phenomena (such as reduplication) in a principled way. • To provide an OTP treatment of reduplication using the standard Correspondence Theory account. • To extend the Earley chart parsing algorithm to multiple context free grammars (MCFGs). The basic idea of this approach is to begin with a non-finite-state description of the space of acceptable candidat</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Comm. of the ACM, 6(2):451-455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient generation in primitive Optimality Theory.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="4559" citStr="Eisner (1997" startWordPosition="749" endWordPosition="750">s structured as follows, with three components: 1. Gen: a procedure that produces infinite surface candidates from an underlying representation (UR) 2. Con: a set of constraints, defined as functions from representations to integers 3. Eva!: an evaluation procedure that, in succession, winnows out the candidates produced by Gen. So OT is a theory that deals with potentially infinite sets of phonological representations. The OT framework does not by itself specify the character of these representations, however. 2.2 Primitive Optimality Theory (OTP) The components of OT, as modeled by OTP (see Eisner (1997a), Eisner (1997b), Albro (1998)): 1. Gen: a procedure that produces from an underlying representation a finite state machine that represents all possible surface candidates that contain that UR (always an infinite set) 2. Con: a set of constraints definable in a restricted formalism internally represented as Weighted Deterministic Finite Automata (WDFAs) which accept any string in the representational alphabet. The weights correspond to constraint violations. The weights passed through when accepting a string are the violations incurred by that string. Figure 1: OTP Representation 3. Eva!: th</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997a. Efficient generation in primitive Optimality Theory. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>What constraints should OT allow? Handout for talk at LSA,</title>
<date>1997</date>
<location>Chicago,</location>
<contexts>
<context position="4559" citStr="Eisner (1997" startWordPosition="749" endWordPosition="750">s structured as follows, with three components: 1. Gen: a procedure that produces infinite surface candidates from an underlying representation (UR) 2. Con: a set of constraints, defined as functions from representations to integers 3. Eva!: an evaluation procedure that, in succession, winnows out the candidates produced by Gen. So OT is a theory that deals with potentially infinite sets of phonological representations. The OT framework does not by itself specify the character of these representations, however. 2.2 Primitive Optimality Theory (OTP) The components of OT, as modeled by OTP (see Eisner (1997a), Eisner (1997b), Albro (1998)): 1. Gen: a procedure that produces from an underlying representation a finite state machine that represents all possible surface candidates that contain that UR (always an infinite set) 2. Con: a set of constraints definable in a restricted formalism internally represented as Weighted Deterministic Finite Automata (WDFAs) which accept any string in the representational alphabet. The weights correspond to constraint violations. The weights passed through when accepting a string are the violations incurred by that string. Figure 1: OTP Representation 3. Eva!: th</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997b. What constraints should OT allow? Handout for talk at LSA, Chicago, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John McCarthy</author>
<author>Alan Prince</author>
</authors>
<title>Faithfulness and reduplicative identity.</title>
<date>1995</date>
<booktitle>Papers in Optimality Theory, number 18 in University of Massachusetts Occasional Papers,</booktitle>
<pages>259--384</pages>
<editor>In J. Beckman, S. Urbanczyk, and L. Walsh, editors,</editor>
<location>Amherst.</location>
<contexts>
<context position="1146" citStr="McCarthy and Prince, 1995" startWordPosition="169" endWordPosition="172">of candidates were limited by some set of criteria capable of being described only in a higherlevel grammar formalism, such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple Context Free Grammar (Seki et al., 1991). Examples include reduplication and phrasal stress models. Here we introduce a mechanism for OTP-like Optimality Theory in which the constraints remain weighted finite state machines, but sets of candidates are represented by higher-level grammars. In particular, we use multiple context-free grammars to model reduplication in the manner of Correspondence Theory (McCarthy and Prince, 1995), and develop an extended version of the Earley Algorithm (Earley, 1970) to apply the constraints to a reduplicating candidate set. 1 Introduction The goals of this paper are as follows: • To show how finite-state models of Optimality Theoretic phonology (such as OTP) can be extended to deal with non-finite state phenomena (such as reduplication) in a principled way. • To provide an OTP treatment of reduplication using the standard Correspondence Theory account. • To extend the Earley chart parsing algorithm to multiple context free grammars (MCFGs). The basic idea of this approach is to begin</context>
</contexts>
<marker>McCarthy, Prince, 1995</marker>
<rawString>John McCarthy and Alan Prince. 1995. Faithfulness and reduplicative identity. In J. Beckman, S. Urbanczyk, and L. Walsh, editors, Papers in Optimality Theory, number 18 in University of Massachusetts Occasional Papers, pages 259-384. GLSA, UMass, Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality Theory: Constraint interaction in generative grammar.</title>
<date>1993</date>
<tech>Technical Report 2,</tech>
<institution>Center for Cognitive Science, Rutgers University.</institution>
<marker>Prince, Smolensky, 1993</marker>
<rawString>Alan Prince and Paul Smolensky. 1993. Optimality Theory: Constraint interaction in generative grammar. Technical Report 2, Center for Cognitive Science, Rutgers University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seki</author>
<author>T Matsumura</author>
<author>M Fujii</author>
<author>T Kasami</author>
</authors>
<title>On multiple context-free grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>88--191</pages>
<contexts>
<context position="754" citStr="Seki et al., 1991" startWordPosition="114" endWordPosition="117">ory (OTP) (Eisner, 1997a; Albro, 1998), a computational model of Optimality Theory (Prince and Smolensky, 1993), employs a finite state machine to represent the set of active candidates at each stage of an Optimality Theoretic derivation, as well as weighted finite state machines to represent the constraints themselves. For some purposes, however, it would be convenient if the set of candidates were limited by some set of criteria capable of being described only in a higherlevel grammar formalism, such as a Context Free Grammar, a Context Sensitive Grammar, or a Multiple Context Free Grammar (Seki et al., 1991). Examples include reduplication and phrasal stress models. Here we introduce a mechanism for OTP-like Optimality Theory in which the constraints remain weighted finite state machines, but sets of candidates are represented by higher-level grammars. In particular, we use multiple context-free grammars to model reduplication in the manner of Correspondence Theory (McCarthy and Prince, 1995), and develop an extended version of the Earley Algorithm (Earley, 1970) to apply the constraints to a reduplicating candidate set. 1 Introduction The goals of this paper are as follows: • To show how finite-</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>H. Seki, T. Matsumura, M. Fujii, and T. Kasami. 1991. On multiple context-free grammars. Theoretical Computer Science, 88:191-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Tesar</author>
</authors>
<title>Computing optimal descriptions for optimality theory grammars with context-free position structures.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2501" citStr="Tesar (1996)" startWordPosition="386" endWordPosition="387">or candidates which are the outputs of a syntactic grammar), and to repeatedly intersect the high-level grammar representing those candidates with finite state machines representing constraints. The intersection operation is one of weighted intersection (where only the set of lowest-weighted candidates survive) in order to model Optimality Theory, and will make use of a modified version of the Earley parsing algorithm. There are at least two alternative approaches to that which we will propose here: to abandon finite state models altogether and move to uniformly higher-level approaches (e.g., Tesar (1996)), or to modify finite state models minimally to allow for (perhaps limited) reduplication (e.g., Walther (2000)). The first of these alternative approaches deals with context free grammars alone, so it would not be able to model reduplicative effects. Besides this, it seems preferable to stick with finite-state approaches as far as possible, because phonological effects beyond the finite state seem quite rare. The second of these approaches seems reasonable in itself, but it is not suited for the type of analyses for which the approach laid out here is designed. In particular, Walther&apos;s appro</context>
</contexts>
<marker>Tesar, 1996</marker>
<rawString>Bruce Tesar. 1996. Computing optimal descriptions for optimality theory grammars with context-free position structures. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nike van Vugt</author>
</authors>
<title>Generalized context-free grammars.</title>
<date>1996</date>
<tech>Master&apos;s thesis,</tech>
<institution>Universiteit Leiden.</institution>
<marker>van Vugt, 1996</marker>
<rawString>Nike van Vugt. 1996. Generalized context-free grammars. Master&apos;s thesis, Universiteit Leiden. Internal Report 96-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Walther</author>
</authors>
<title>Finite-state reduplication in one-level prosodic morphology.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<pages>296--302</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="2613" citStr="Walther (2000)" startWordPosition="404" endWordPosition="405">r representing those candidates with finite state machines representing constraints. The intersection operation is one of weighted intersection (where only the set of lowest-weighted candidates survive) in order to model Optimality Theory, and will make use of a modified version of the Earley parsing algorithm. There are at least two alternative approaches to that which we will propose here: to abandon finite state models altogether and move to uniformly higher-level approaches (e.g., Tesar (1996)), or to modify finite state models minimally to allow for (perhaps limited) reduplication (e.g., Walther (2000)). The first of these alternative approaches deals with context free grammars alone, so it would not be able to model reduplicative effects. Besides this, it seems preferable to stick with finite-state approaches as far as possible, because phonological effects beyond the finite state seem quite rare. The second of these approaches seems reasonable in itself, but it is not suited for the type of analyses for which the approach laid out here is designed. In particular, Walther&apos;s approach is tied to One-Level Phonology, a theory which limits itself to surface-true generalizations, whereas the ap</context>
</contexts>
<marker>Walther, 2000</marker>
<rawString>Markus Walther. 2000. Finite-state reduplication in one-level prosodic morphology. In Proceedings of NAACL-2000, pages 296-302, Seattle, WA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>