<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000154">
<title confidence="0.826986">
DCU: Using Distributional Semantics and Domain Adaptation for the
Semantic Textual Similarity SemEval-2015 Task 2
</title>
<author confidence="0.976076">
Piyush Arora, Chris Hokamp, Jennifer Foster, Gareth J.F.Jones
</author>
<affiliation confidence="0.985888">
ADAPT Centre
School of Computing
Dublin City University
</affiliation>
<address confidence="0.696229">
Dublin, Ireland
</address>
<email confidence="0.998248">
{parora,chokamp,jfoster,gjones}@computing.dcu.ie
</email>
<sectionHeader confidence="0.996651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917357142857">
We describe the work carried out by the DCU
team on the Semantic Textual Similarity task
at SemEval-2015. We learn a regression
model to predict a semantic similarity score
between a sentence pair. Our system exploits
distributional semantics in combination with
tried-and-tested features from previous tasks
in order to compute sentence similarity. Our
team submitted 3 runs for each of the five En-
glish test sets. For two of the test sets, belief
and headlines, our best system ranked second
and fourth out of the 73 submitted systems.
Our best submission averaged over all test sets
ranked 26 out of the 73 systems.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999269428571429">
This paper describes DCU’s participation in the Se-
mEval 2015 English Semantic Textual Similarity
(STS) task, whose goal is to predict how similar
in meaning two sentences are (Agirre et al., 2014).
The semantic similarity between two sentences is
defined on a scale from 0 (no relation) to 5 (semantic
equivalence). Thus, given a sentence pair, our aim is
to learn a model which outputs a score between 0
and 5 reflecting the semantic similarity between the
two sentences.
We explore distributional representations of
words computed using neural networks – specifi-
cally Word2Vec vectors (Mikolov et al., 2013) – and
we design features which attempt to encode seman-
tic similarity at the sentence level. We also experi-
ment with several methods of data selection, both for
training word embeddings, and for selecting training
data for our regression models. We submitted three
runs for this task: for all three runs, the features used
are identical, and the only difference between them
is the training instance selection method used.
</bodyText>
<sectionHeader confidence="0.957576" genericHeader="introduction">
2 Data and Resources
</sectionHeader>
<bodyText confidence="0.994399217391305">
The training data for the task is comprised of all
the corpora from previous years STS tasks: STS-
2012, STS-2013 and STS-2014 (Agirre et al., 2012;
Agirre et al., 2013; Agirre et al., 2014). The test
data is taken from five domains: answers-forums,
answers-students, belief, headlines and images. Two
domains (headlines and images) have some training
data available from the previous STS tasks1 – the
other three have been introduced for the first time.
We use the Word2Vec (W2V) representation for
computing semantic similarity between two words.
We then expand to incorporate the similarity be-
tween two sentences. Using W2V, a word can
be represented as a vector of D dimensions, with
each dimension capturing some aspect of the word’s
meaning in the form of different concepts learnt
from the trained model. We use the gensim W2V
implementation (ˇReh˚uˇrek and Sojka, 2010).
We use the text8 Wikipedia corpus to train our
general W2V model. This corpus is comprised of
100MB of compressed Wikipedia data.2 We use
the UMBC corpus (Han et al., 2013) for building
domain-specific W2V models.
</bodyText>
<footnote confidence="0.999976">
1http://ixa2.si.ehu.es/stswiki/index.php/Main Page
2http://mattmahoney.net/dc/textdata.html
</footnote>
<page confidence="0.967734">
143
</page>
<note confidence="0.6139785">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 143–147,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.995488" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.999829">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.9997488">
We perform minimal pre-processing, replacing all
hyphens and apostrophes with spaces, and removing
all non-alphanumeric symbols from the data. Our
general domain model uses the NLTK3 stop word
list for stop word removal and the Porter stemming
algorithm (Porter, 1980). Word2Vec handles the
stem variations to some extent when it learns the
vector representation from the raw input data. Thus
for the domain-specific models, we only remove
stopwords and do not stem.
</bodyText>
<subsectionHeader confidence="0.99711">
3.2 Feature Design
</subsectionHeader>
<bodyText confidence="0.999728">
To predict a semantic similarity score, we learn a re-
gression model using the M5P algorithm.4 We rep-
resent a sentence pair using the features described in
the folowing subsections.
</bodyText>
<subsectionHeader confidence="0.882313">
3.2.1 Cosine Similarity
</subsectionHeader>
<bodyText confidence="0.999893111111111">
We have two features representing the cosine sim-
ilarity between two sentences, s1 and s2, where
the sentences are represented as binary vectors with
each dimension indicating the presence of a word.
The first feature is the basic cosine similarity be-
tween the two sentence vectors and the second is the
weighted cosine similarity between the two vectors,
where each word is weighted by its inverse collec-
tion frequency (ICF).5
</bodyText>
<subsectionHeader confidence="0.968987">
3.2.2 Word2Vec
</subsectionHeader>
<bodyText confidence="0.999987">
Sum W2V: For a given sentence we represent
each word by its W2V representation and then sum
each word vector in a sentence to find the centroid
of the word vectors representing the entire sentence.
The cosine of the centroids of the two sentences in-
dicates the similarity between them. Using the sum
approach, two features, sum and sum icf, are calcu-
lated, one corresponding to the basic cosine similar-
ity between the vectors, and the other representing
the weighted cosine similarity where, before calcu-
lating the centroid, each word vector is multiplied
</bodyText>
<footnote confidence="0.993913666666667">
3http://www.nltk.org/
4We used the weka implementation: http://www.cs
.waikato.ac.nz/ml/weka/ without performing any ex-
tra hyper-parameter optimization.
5ICF is calculated using word frequency from the wikipedia
2011 dump.
</footnote>
<bodyText confidence="0.924972428571428">
by its ICF weight.
Product W2V: Given s1 and s2, we take the
element-wise product of each word vector in s1 and
s2 and store the maximum product value for each
word in s1 and similarly for s2. The Product W2V
feature is the average of the maximum weights be-
tween each word of s1 with s2 and vice versa:
</bodyText>
<equation confidence="0.973935666666667">
En
1 max m (sli.s2/i)
Y (sli)2 )
�&apos; ( �j=1 Y �s2�)2
n
Em (max Pn / (s1i.s~2
� j) )
j 1 2 1 Y (s1i)2Y/ (s2j)2
m
</equation>
<bodyText confidence="0.99961775862069">
The sum and product W2V models are inspired
by the composition models of Mitchell and Lapata
(2008) and semantic similarity measures of Mihal-
cea et al. (2006).
Domain-specific Cosine Similarity: Good cover-
age is obtained using the text8 corpus to train the
W2V model. However, we also want to explore
the performance with respect to an in-domain W2V
model. So, for each of the test corpora, we first ex-
tract a corpus of similar sentences from the UMBC
corpus by selecting up to 500 sentences for each
content word in the test corpus and then use the ex-
tracted dataset to train a W2V model that has bet-
ter coverage of the test domain. Using the domain-
specific W2V corpus, we compute the feature do-
main w2v cosine similarity in a similar fashion to
the Sum W2V feature – we compute the centroid
vector of the content words in each sentence and
then compute the cosine between the two centroids.
Syntax: We also hypothesize that two semanti-
cally similar sentences should have high overlap be-
tween their nouns, verbs, adjectives and adverbs.
For each coarse-grained POS tag (NN*, VB*, JJ*
and RB*) we calculate the W2V cosine similarity
between all words from s1 and s2 which have the
same POS tag (using the Sum W2V combination
method). For each coarse-grained POS tag, we also
calculate the number of lexical matches with that
particular POS tag.
</bodyText>
<equation confidence="0.997387416666667">
sim(s1, s2) _
q(Einsii)2q(E ms~2j)2
En
i=1 ~s1i
n
Em ~s2j
j=1
.
m (1)
sim(s1, s2) _
+
(2)
</equation>
<page confidence="0.98754">
144
</page>
<bodyText confidence="0.999935666666667">
We also parse each sentence using the Stanford
parser (Manning et al., 2014) and look for depen-
dency relation overlap between s1 and s2.6 We
concentrate on six dependency relations – nsubj,
dobj, det, prep, amod and aux. For each re-
lation we calculate the degree of overlap between
the occurrences of this relation in the two sentences.
We have two notions of relation overlap: a non-
lexicalized version which just counts the relation it-
self (e.g. nsubj) and a lexicalized version which
counts the relation and the two tokens it connects
(e.g. nsubj word1 word2).
</bodyText>
<subsectionHeader confidence="0.651523">
3.2.3 Monolingual Alignment
</subsectionHeader>
<bodyText confidence="0.999958">
We compute the monolingual alignment between
the two sentences using the word aligner introduced
in (Sultan et al., 2014). Their system aligns re-
lated words in a sentence pair by exploiting seman-
tic and contextual similarities of the words. From
the aligned sentences, we then extract two features:
percent aligned source and percent aligned target,
which represent the fraction of tokens in each sen-
tence which have an alignment in the other sentence.
The intuition behind these features is that sentences
which are semantically similar should have a higher
fraction of aligned tokens, since alignments consti-
tute either identical strings or paraphrases.
</bodyText>
<subsectionHeader confidence="0.525789">
3.2.4 TakeLab
</subsectionHeader>
<bodyText confidence="0.999992111111111">
The Takelab system (ˇSari´c et al., 2012) was the
top performing system in STS-2012 task. Their
system used support vector regression models with
multiple features measuring word overlap similar-
ity and syntactic similarity. We find that adding the
Takelab features provides additional knowledge to
our system and improves performance for the train-
ing datasets. We add the 21 features of the Takelab
system to our feature set.
</bodyText>
<subsectionHeader confidence="0.999647">
3.3 Training instance selection
</subsectionHeader>
<bodyText confidence="0.999988166666667">
After designing features to model semantic similar-
ity between two sentences, the next important task is
to select the training corpus for learning the weights
for these features. Out of the five test sets for STS-
2015, we only have in-domain training corpora for
the headlines and images data sets. We hypothesize
</bodyText>
<footnote confidence="0.787945">
6Parsing is carried out on the raw sentences.
</footnote>
<bodyText confidence="0.9997525">
that finding vocabulary similarity between the en-
tire training and test corpus could be used to select
more similar corpus for training of the system. We
calculate the similarity between each of the corpora
we have from previous STS tasks and each of the
test corpora. Using the entire corpus vocabulary as a
vector we find the cosine similarity between differ-
ent corpora using the TFIDF (Manning et al., 2008),
LSI (Hofmann, 1999), LDA (Blei et al., 2003b) and
HLDA (Blei et al., 2003a) measures.
Next, we describe the mechanism we used for
training data selection for each run:
</bodyText>
<listItem confidence="0.69198425">
1. Run-1: For the two corpora for which we have
prior training data we took the previous years’
training data. For the other test corpora we se-
lect the most similar corpus from the previous
</listItem>
<bodyText confidence="0.8487345">
years’ training data based on the corpus vector
cosine similarity, where each word in a vector
is replaced by its TFIDF weight. The training
corpora we selected are as follows:
Images: Images 2014, Headlines: Headlines-
2014, Belief: Deft Forum 2014, Answers-stu-
dents: MSRVid 2012 train, Answers-forums:
Deft Forum 2014
</bodyText>
<listItem confidence="0.934267">
2. Run-2: We want to make sure that the train-
</listItem>
<bodyText confidence="0.867188142857143">
ing data has instances similar to the test sam-
ples. To capture diversity in our training cor-
pus we compute corpus vector cosine similar-
ity where each word is replaced by its TFIDF
weight, then we merge the top three most sim-
ilar training corpora for each test set as shown
below:
</bodyText>
<table confidence="0.887337583333333">
Images: Images 2014 + MSRVid 2012 train +
MSRVid 2012 test
Headlines: Headlines 2013 + Headlines 2014
+ MSRPar 2012 train
Belief: Deft Forum 2014 + Headlines 2014 +
Headlines 2013
Answers-students: OnWn 2012 test + MSR-
Vid 2012 train + MSRPar 2012 test
Answers-forums: Deft forum 2014 + SMT-
2012 train + MSRPar 2012 train
For each test set instance, we find the five7
most similar instances from the merged training
</table>
<footnote confidence="0.994276">
7Five was empirically chosen by experimenting with differ-
ent values on the training data.
</footnote>
<page confidence="0.993612">
145
</page>
<table confidence="0.999845857142857">
Test Set Baseline Run-1 Run-2 Run-3 Top System Our Rank
Images 0.6039 0.8394 0.835 0.8434 0.8713 19
Headlines 0.5312 0.8284 0.8187 0.8181 0.8417 4
Belief 0.6517 0.5464 0.7549 0.6977 0.7717 2
Answers-students 0.6647 0.6582 0.6233 0.6108 0.7879 47
Answers-forum 0.4453 0.5556 0.5628 0.653 0.739 30
Mean 0.7192 0.734 0.7369 26
</table>
<tableCaption confidence="0.999954">
Table 1: Results of our final runs compared to the baseline and the best system for each test set.
</tableCaption>
<bodyText confidence="0.999070857142857">
corpus (similar instances are computed using
cosine similarity between the feature vectors).
By combining these five training instances for
all test instances and removing duplicates, we
form a more focused training set which is ex-
pected to capture the test set diversity more ef-
fectively.
</bodyText>
<listItem confidence="0.5991744">
3. Run-3: In this variant, we do not want to limit
ourselves to just the top three corpora, so we
merge all the training data and then look for the
five most similar training instances for each test
instance to form a focused training set.
</listItem>
<sectionHeader confidence="0.999543" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999753344827586">
Table 1 shows the results of our systems on the five
test sets. For the test sets answers-forum and be-
lief there was a considerable difference in the re-
sults across the three runs, indicating that select-
ing training instances has a significant effect on per-
formance. For these two datasets across two runs
the absolute difference in the Pearson coefficient is
about 10% for answers-forum and about 20% for
the belief dataset. Overall, our best system rank is
26 out of 73. If we look at the results for individ-
ual test sets, it seems our approach works well for
the belief, headlines and image test set but performs
poorly for the answer-student and answer-forum test
sets. For the belief test set our Run-2 was ranked
2nd overall and for the headlines test set our Run-1
was ranked 4th overall. For the images test set, the
results are competitive – the absolute difference in
the Pearson value between our best run and the best
system is only 0.03. Thus, apart from two corpora,
answers-students and answer-forums, our approach
performed quite well.
We analyzed the features using GradientBoostin-
gRegressor8 for all the training sets. The feature
importance varies slightly across different domains.
For all datasets, we remove features with gini impor-
tance9 &lt; 0.01, then we look for the features which
are present in at least three of the different domain
for this year’s test set. The features that performed
well are shown in Table 2.
</bodyText>
<table confidence="0.712201111111111">
Our Features
sum icf, sum, product, domain w2v cosine,
percent aligned target, percent source target,
nn w2v, vb w2v, jj w2v, nsub 1, cosine, cosine icf
TakeLab Features
wn sim match, weighted word match,
weighted word match, dist sim,
weighted dist sim, weighted dist sim,
relative len difference, relative ic difference
</table>
<tableCaption confidence="0.995961">
Table 2: Important features.
</tableCaption>
<sectionHeader confidence="0.993498" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999978461538461">
All of our runs have the same features, but use dif-
ferent training corpora to learn the weights. We thus
show that training data selection can have an im-
pact on the performance of a model, especially for a
novel genre. Using Word2Vec to find semantic simi-
larity between a sentence pair proved to be effective.
Furthermore, composing W2V features in different
ways can help to reveal new information about se-
mantic similarity.
Investigating the test sets where we failed to per-
form well, answer-forums and answer-students, re-
veals that we need to handle phrasal information
more effectively by, for example, handling negation,
</bodyText>
<footnote confidence="0.9890914">
8http://scikit-learn.org/stable/
modules/generated/sklearn.ensemble.
GradientBoostingRegressor.html
9http://www.stat.berkeley.edu/˜breiman/
RandomForests/cc_home.htm
</footnote>
<page confidence="0.997547">
146
</page>
<bodyText confidence="0.997654">
devising measures to compare the sentences at the
entity level and making better use of parser output.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999077428571429">
We would like to thank Rasoul Kaljahi for helping
with running some of the preliminary experiments
using tree kernels and Debasis Ganguly for his sug-
gestions and guidance. This research is supported
by Science Foundation Ireland (SFI) as a part of the
CNGL Centre for Global Intelligent Content at DCU
(Grant No: 12/CE/I2267).
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848034883721">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. In Proceedings
of the First Joint Conference on Lexical and Computa-
tional Semantics - Volume 1: Proceedings of the Main
Conference and the Shared Task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 385–393, Montr´eal, Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Confer-
ence and the Shared Task: Semantic Textual Similarity,
pages 32–43, Atlanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. SemEval-2014 Task 10: Multilingual Seman-
tic Textual Similarity. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 81–91, Dublin, Ireland.
David M. Blei, Thomas L. Griffiths, Michael I. Jor-
dan, and Joshua B. Tenenbaum. 2003a. Hierarchical
Topic Models and the Nested Chinese Restaurant Pro-
cess. In Advances in Neural Information Processing
Systems 16 [Neural Information Processing Systems,
NIPS 2003], pages 17–24, Vancouver and Whistler,
British Columbia, Canada.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003b. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research - Volume 3, pages 993–1022.
Lushan Han, Abhay L. Kashyap, Tim Finin,
James Mayfield, and Johnathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic Textual Sim-
ilarity Systems. In Proceedings of the Second Joint
Conference on Lexical and Computational Semantics,
pages 42–54, Atlanta, Georgia, USA.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 50–57,
Berkeley, California, USA.
Christopher D Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Scoring, term weighting and the
vector space model. Introduction to Information Re-
trieval, pages 118–120.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Pro-
cessing Toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations, pages 55–60, Baltimore,
Maryland.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence - Vol-
ume 1, pages 775–780, Boston, Massachusetts.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting Similarities among Languages for Machine
Translation. CoRR, abs/1309.4168.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
Models of Semantic Composition. In Proceedings of
ACL-08: HLT, pages 236–244, Columbus, Ohio.
Martin F Porter. 1980. An algorithm for suffix stripping.
Program - Volume 14, pages 130–137.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta.
Arafat Md Sultan, Steven Bethard, and Tamara Sumner.
2014. Back to Basics for Monolingual Alignment:
Exploiting Word Similarity and Contextual Evidence.
Transactions of the Association of Computational Lin-
guistics – Volume 2, Issue 1, pages 219–230.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. In Proceed-
ings of the First Joint Conference on Lexical and Com-
putational Semantics - Volume 1: Proceedings of the
Main Conference and the Shared Task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, SemEval ’12, pages 441–448,
Montr´eal, Canada.
</reference>
<page confidence="0.998099">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.394078">
<title confidence="0.8371535">DCU: Using Distributional Semantics and Domain Adaptation for Semantic Textual Similarity SemEval-2015 Task 2</title>
<author confidence="0.995667">Piyush Arora</author>
<author confidence="0.995667">Chris Hokamp</author>
<author confidence="0.995667">Jennifer Foster</author>
<author confidence="0.995667">Gareth</author>
<affiliation confidence="0.963907333333333">ADAPT School of Dublin City</affiliation>
<address confidence="0.666451">Dublin,</address>
<abstract confidence="0.998625133333333">We describe the work carried out by the DCU team on the Semantic Textual Similarity task at SemEval-2015. We learn a regression model to predict a semantic similarity score between a sentence pair. Our system exploits distributional semantics in combination with tried-and-tested features from previous tasks in order to compute sentence similarity. Our team submitted 3 runs for each of the five Entest sets. For two of the test sets, our best system ranked second and fourth out of the 73 submitted systems. Our best submission averaged over all test sets ranked 26 out of the 73 systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2138" citStr="Agirre et al., 2012" startWordPosition="333" endWordPosition="336">vectors (Mikolov et al., 2013) – and we design features which attempt to encode semantic similarity at the sentence level. We also experiment with several methods of data selection, both for training word embeddings, and for selecting training data for our regression models. We submitted three runs for this task: for all three runs, the features used are identical, and the only difference between them is the training instance selection method used. 2 Data and Resources The training data for the task is comprised of all the corpora from previous years STS tasks: STS2012, STS-2013 and STS-2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). The test data is taken from five domains: answers-forums, answers-students, belief, headlines and images. Two domains (headlines and images) have some training data available from the previous STS tasks1 – the other three have been introduced for the first time. We use the Word2Vec (W2V) representation for computing semantic similarity between two words. We then expand to incorporate the similarity between two sentences. Using W2V, a word can be represented as a vector of D dimensions, with each dimension capturing some aspect of the word’s meaning </context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>SEM 2013 shared task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>32--43</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="2159" citStr="Agirre et al., 2013" startWordPosition="337" endWordPosition="340">l., 2013) – and we design features which attempt to encode semantic similarity at the sentence level. We also experiment with several methods of data selection, both for training word embeddings, and for selecting training data for our regression models. We submitted three runs for this task: for all three runs, the features used are identical, and the only difference between them is the training instance selection method used. 2 Data and Resources The training data for the task is comprised of all the corpora from previous years STS tasks: STS2012, STS-2013 and STS-2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). The test data is taken from five domains: answers-forums, answers-students, belief, headlines and images. Two domains (headlines and images) have some training data available from the previous STS tasks1 – the other three have been introduced for the first time. We use the Word2Vec (W2V) representation for computing semantic similarity between two words. We then expand to incorporate the similarity between two sentences. Using W2V, a word can be represented as a vector of D dimensions, with each dimension capturing some aspect of the word’s meaning in the form of differ</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic Textual Similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 32–43, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>81--91</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1136" citStr="Agirre et al., 2014" startWordPosition="168" endWordPosition="171">xploits distributional semantics in combination with tried-and-tested features from previous tasks in order to compute sentence similarity. Our team submitted 3 runs for each of the five English test sets. For two of the test sets, belief and headlines, our best system ranked second and fourth out of the 73 submitted systems. Our best submission averaged over all test sets ranked 26 out of the 73 systems. 1 Introduction This paper describes DCU’s participation in the SemEval 2015 English Semantic Textual Similarity (STS) task, whose goal is to predict how similar in meaning two sentences are (Agirre et al., 2014). The semantic similarity between two sentences is defined on a scale from 0 (no relation) to 5 (semantic equivalence). Thus, given a sentence pair, our aim is to learn a model which outputs a score between 0 and 5 reflecting the semantic similarity between the two sentences. We explore distributional representations of words computed using neural networks – specifically Word2Vec vectors (Mikolov et al., 2013) – and we design features which attempt to encode semantic similarity at the sentence level. We also experiment with several methods of data selection, both for training word embeddings, </context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81–91, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Hierarchical Topic Models and the Nested Chinese Restaurant Process.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS</booktitle>
<pages>17--24</pages>
<location>Vancouver and Whistler, British Columbia, Canada.</location>
<contexts>
<context position="9694" citStr="Blei et al., 2003" startWordPosition="1573" endWordPosition="1576">st sets for STS2015, we only have in-domain training corpora for the headlines and images data sets. We hypothesize 6Parsing is carried out on the raw sentences. that finding vocabulary similarity between the entire training and test corpus could be used to select more similar corpus for training of the system. We calculate the similarity between each of the corpora we have from previous STS tasks and each of the test corpora. Using the entire corpus vocabulary as a vector we find the cosine similarity between different corpora using the TFIDF (Manning et al., 2008), LSI (Hofmann, 1999), LDA (Blei et al., 2003b) and HLDA (Blei et al., 2003a) measures. Next, we describe the mechanism we used for training data selection for each run: 1. Run-1: For the two corpora for which we have prior training data we took the previous years’ training data. For the other test corpora we select the most similar corpus from the previous years’ training data based on the corpus vector cosine similarity, where each word in a vector is replaced by its TFIDF weight. The training corpora we selected are as follows: Images: Images 2014, Headlines: Headlines2014, Belief: Deft Forum 2014, Answers-students: MSRVid 2012 train,</context>
</contexts>
<marker>Blei, Griffiths, Jordan, Tenenbaum, 2003</marker>
<rawString>David M. Blei, Thomas L. Griffiths, Michael I. Jordan, and Joshua B. Tenenbaum. 2003a. Hierarchical Topic Models and the Nested Chinese Restaurant Process. In Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS 2003], pages 17–24, Vancouver and Whistler, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research -</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="9694" citStr="Blei et al., 2003" startWordPosition="1573" endWordPosition="1576">st sets for STS2015, we only have in-domain training corpora for the headlines and images data sets. We hypothesize 6Parsing is carried out on the raw sentences. that finding vocabulary similarity between the entire training and test corpus could be used to select more similar corpus for training of the system. We calculate the similarity between each of the corpora we have from previous STS tasks and each of the test corpora. Using the entire corpus vocabulary as a vector we find the cosine similarity between different corpora using the TFIDF (Manning et al., 2008), LSI (Hofmann, 1999), LDA (Blei et al., 2003b) and HLDA (Blei et al., 2003a) measures. Next, we describe the mechanism we used for training data selection for each run: 1. Run-1: For the two corpora for which we have prior training data we took the previous years’ training data. For the other test corpora we select the most similar corpus from the previous years’ training data based on the corpus vector cosine similarity, where each word in a vector is replaced by its TFIDF weight. The training corpora we selected are as follows: Images: Images 2014, Headlines: Headlines2014, Belief: Deft Forum 2014, Answers-students: MSRVid 2012 train,</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003b. Latent Dirichlet Allocation. Journal of Machine Learning Research - Volume 3, pages 993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>42--54</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="3042" citStr="Han et al., 2013" startWordPosition="480" endWordPosition="483"> for the first time. We use the Word2Vec (W2V) representation for computing semantic similarity between two words. We then expand to incorporate the similarity between two sentences. Using W2V, a word can be represented as a vector of D dimensions, with each dimension capturing some aspect of the word’s meaning in the form of different concepts learnt from the trained model. We use the gensim W2V implementation (ˇReh˚uˇrek and Sojka, 2010). We use the text8 Wikipedia corpus to train our general W2V model. This corpus is comprised of 100MB of compressed Wikipedia data.2 We use the UMBC corpus (Han et al., 2013) for building domain-specific W2V models. 1http://ixa2.si.ehu.es/stswiki/index.php/Main Page 2http://mattmahoney.net/dc/textdata.html 143 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 143–147, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 Methodology 3.1 Pre-processing We perform minimal pre-processing, replacing all hyphens and apostrophes with spaces, and removing all non-alphanumeric symbols from the data. Our general domain model uses the NLTK3 stop word list for stop word removal and the Porter stemming al</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, pages 42–54, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>50--57</pages>
<location>Berkeley, California, USA.</location>
<contexts>
<context position="9670" citStr="Hofmann, 1999" startWordPosition="1570" endWordPosition="1571">s. Out of the five test sets for STS2015, we only have in-domain training corpora for the headlines and images data sets. We hypothesize 6Parsing is carried out on the raw sentences. that finding vocabulary similarity between the entire training and test corpus could be used to select more similar corpus for training of the system. We calculate the similarity between each of the corpora we have from previous STS tasks and each of the test corpora. Using the entire corpus vocabulary as a vector we find the cosine similarity between different corpora using the TFIDF (Manning et al., 2008), LSI (Hofmann, 1999), LDA (Blei et al., 2003b) and HLDA (Blei et al., 2003a) measures. Next, we describe the mechanism we used for training data selection for each run: 1. Run-1: For the two corpora for which we have prior training data we took the previous years’ training data. For the other test corpora we select the most similar corpus from the previous years’ training data based on the corpus vector cosine similarity, where each word in a vector is replaced by its TFIDF weight. The training corpora we selected are as follows: Images: Images 2014, Headlines: Headlines2014, Belief: Deft Forum 2014, Answers-stud</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 50–57, Berkeley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Scoring, term weighting and the vector space model. Introduction to Information Retrieval,</title>
<date>2008</date>
<pages>118--120</pages>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Scoring, term weighting and the vector space model. Introduction to Information Retrieval, pages 118–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="7237" citStr="Manning et al., 2014" startWordPosition="1174" endWordPosition="1177">yntax: We also hypothesize that two semantically similar sentences should have high overlap between their nouns, verbs, adjectives and adverbs. For each coarse-grained POS tag (NN*, VB*, JJ* and RB*) we calculate the W2V cosine similarity between all words from s1 and s2 which have the same POS tag (using the Sum W2V combination method). For each coarse-grained POS tag, we also calculate the number of lexical matches with that particular POS tag. sim(s1, s2) _ q(Einsii)2q(E ms~2j)2 En i=1 ~s1i n Em ~s2j j=1 . m (1) sim(s1, s2) _ + (2) 144 We also parse each sentence using the Stanford parser (Manning et al., 2014) and look for dependency relation overlap between s1 and s2.6 We concentrate on six dependency relations – nsubj, dobj, det, prep, amod and aux. For each relation we calculate the degree of overlap between the occurrences of this relation in the two sentences. We have two notions of relation overlap: a nonlexicalized version which just counts the relation itself (e.g. nsubj) and a lexicalized version which counts the relation and the two tokens it connects (e.g. nsubj word1 word2). 3.2.3 Monolingual Alignment We compute the monolingual alignment between the two sentences using the word aligner</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence -</booktitle>
<volume>1</volume>
<pages>775--780</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="5880" citStr="Mihalcea et al. (2006)" startWordPosition="931" endWordPosition="935">sing word frequency from the wikipedia 2011 dump. by its ICF weight. Product W2V: Given s1 and s2, we take the element-wise product of each word vector in s1 and s2 and store the maximum product value for each word in s1 and similarly for s2. The Product W2V feature is the average of the maximum weights between each word of s1 with s2 and vice versa: En 1 max m (sli.s2/i) Y (sli)2 ) �&apos; ( �j=1 Y �s2�)2 n Em (max Pn / (s1i.s~2 � j) ) j 1 2 1 Y (s1i)2Y/ (s2j)2 m The sum and product W2V models are inspired by the composition models of Mitchell and Lapata (2008) and semantic similarity measures of Mihalcea et al. (2006). Domain-specific Cosine Similarity: Good coverage is obtained using the text8 corpus to train the W2V model. However, we also want to explore the performance with respect to an in-domain W2V model. So, for each of the test corpora, we first extract a corpus of similar sentences from the UMBC corpus by selecting up to 500 sentences for each content word in the test corpus and then use the extracted dataset to train a W2V model that has better coverage of the test domain. Using the domainspecific W2V corpus, we compute the feature domain w2v cosine similarity in a similar fashion to the Sum W2V</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1, pages 775–780, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting Similarities among Languages for Machine Translation.</title>
<date>2013</date>
<location>CoRR, abs/1309.4168.</location>
<contexts>
<context position="1549" citStr="Mikolov et al., 2013" startWordPosition="234" endWordPosition="237">ntroduction This paper describes DCU’s participation in the SemEval 2015 English Semantic Textual Similarity (STS) task, whose goal is to predict how similar in meaning two sentences are (Agirre et al., 2014). The semantic similarity between two sentences is defined on a scale from 0 (no relation) to 5 (semantic equivalence). Thus, given a sentence pair, our aim is to learn a model which outputs a score between 0 and 5 reflecting the semantic similarity between the two sentences. We explore distributional representations of words computed using neural networks – specifically Word2Vec vectors (Mikolov et al., 2013) – and we design features which attempt to encode semantic similarity at the sentence level. We also experiment with several methods of data selection, both for training word embeddings, and for selecting training data for our regression models. We submitted three runs for this task: for all three runs, the features used are identical, and the only difference between them is the training instance selection method used. 2 Data and Resources The training data for the task is comprised of all the corpora from previous years STS tasks: STS2012, STS-2013 and STS-2014 (Agirre et al., 2012; Agirre et</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013. Exploiting Similarities among Languages for Machine Translation. CoRR, abs/1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based Models of Semantic Composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5821" citStr="Mitchell and Lapata (2008)" startWordPosition="922" endWordPosition="925">ng any extra hyper-parameter optimization. 5ICF is calculated using word frequency from the wikipedia 2011 dump. by its ICF weight. Product W2V: Given s1 and s2, we take the element-wise product of each word vector in s1 and s2 and store the maximum product value for each word in s1 and similarly for s2. The Product W2V feature is the average of the maximum weights between each word of s1 with s2 and vice versa: En 1 max m (sli.s2/i) Y (sli)2 ) �&apos; ( �j=1 Y �s2�)2 n Em (max Pn / (s1i.s~2 � j) ) j 1 2 1 Y (s1i)2Y/ (s2j)2 m The sum and product W2V models are inspired by the composition models of Mitchell and Lapata (2008) and semantic similarity measures of Mihalcea et al. (2006). Domain-specific Cosine Similarity: Good coverage is obtained using the text8 corpus to train the W2V model. However, we also want to explore the performance with respect to an in-domain W2V model. So, for each of the test corpora, we first extract a corpus of similar sentences from the UMBC corpus by selecting up to 500 sentences for each content word in the test corpus and then use the extracted dataset to train a W2V model that has better coverage of the test domain. Using the domainspecific W2V corpus, we compute the feature domai</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program -</title>
<date>1980</date>
<journal>Volume</journal>
<volume>14</volume>
<pages>130--137</pages>
<contexts>
<context position="3664" citStr="Porter, 1980" startWordPosition="558" endWordPosition="559">ding domain-specific W2V models. 1http://ixa2.si.ehu.es/stswiki/index.php/Main Page 2http://mattmahoney.net/dc/textdata.html 143 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 143–147, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 3 Methodology 3.1 Pre-processing We perform minimal pre-processing, replacing all hyphens and apostrophes with spaces, and removing all non-alphanumeric symbols from the data. Our general domain model uses the NLTK3 stop word list for stop word removal and the Porter stemming algorithm (Porter, 1980). Word2Vec handles the stem variations to some extent when it learns the vector representation from the raw input data. Thus for the domain-specific models, we only remove stopwords and do not stem. 3.2 Feature Design To predict a semantic similarity score, we learn a regression model using the M5P algorithm.4 We represent a sentence pair using the features described in the folowing subsections. 3.2.1 Cosine Similarity We have two features representing the cosine similarity between two sentences, s1 and s2, where the sentences are represented as binary vectors with each dimension indicating th</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F Porter. 1980. An algorithm for suffix stripping. Program - Volume 14, pages 130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arafat Md Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence.</title>
<date>2014</date>
<journal>Transactions of the Association of Computational Linguistics –</journal>
<volume>2</volume>
<pages>219--230</pages>
<contexts>
<context position="7873" citStr="Sultan et al., 2014" startWordPosition="1279" endWordPosition="1282">pendency relation overlap between s1 and s2.6 We concentrate on six dependency relations – nsubj, dobj, det, prep, amod and aux. For each relation we calculate the degree of overlap between the occurrences of this relation in the two sentences. We have two notions of relation overlap: a nonlexicalized version which just counts the relation itself (e.g. nsubj) and a lexicalized version which counts the relation and the two tokens it connects (e.g. nsubj word1 word2). 3.2.3 Monolingual Alignment We compute the monolingual alignment between the two sentences using the word aligner introduced in (Sultan et al., 2014). Their system aligns related words in a sentence pair by exploiting semantic and contextual similarities of the words. From the aligned sentences, we then extract two features: percent aligned source and percent aligned target, which represent the fraction of tokens in each sentence which have an alignment in the other sentence. The intuition behind these features is that sentences which are semantically similar should have a higher fraction of aligned tokens, since alignments constitute either identical strings or paraphrases. 3.2.4 TakeLab The Takelab system (ˇSari´c et al., 2012) was the t</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Arafat Md Sultan, Steven Bethard, and Tamara Sumner. 2014. Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence. Transactions of the Association of Computational Linguistics – Volume 2, Issue 1, pages 219–230.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>TakeLab: Systems for Measuring Semantic Text Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12,</booktitle>
<pages>441--448</pages>
<location>Montr´eal, Canada.</location>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems for Measuring Semantic Text Similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval ’12, pages 441–448, Montr´eal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>