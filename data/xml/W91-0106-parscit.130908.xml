<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040190">
<title confidence="0.8187275">
REVERSIBLE NLP BY DERIVING THE GRAMMARS
FROM THE KNOWLEDGE BASE
</title>
<author confidence="0.985272">
David D. McDonald
</author>
<affiliation confidence="0.931273">
Content Technologies, Inc.
</affiliation>
<address confidence="0.971098">
14 Brantwood Road, Arlington, MA 02174
</address>
<email confidence="0.980801">
(617) 646-4124, MCDONALD@BRANDEIS.EDU
</email>
<sectionHeader confidence="0.991133" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999349">
We present a new architecture for reversible NLP.
Separate parsing and generation grammars are
constructed from the underlying application&apos;s
semantic model and knowledge base. By having two
grammars we are free to use process-specific
representations and control techniques, thereby
permitting highly efficient processing. The single
semantic source ensures the parsimony of
development and matched competence that make
reversible NLP attractive.
</bodyText>
<sectionHeader confidence="0.997901" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999931820895523">
Most natural language processing systems are
initially built in a single direction only; most are
parsers (understanding systems), a few are generators.
These systems are often then embedded in full, bi-
directional interfaces, whereupon a new, almost non-
technical kind of problem arises if differences in the
two uni-directional subsystems are not controlled.
The full system may not understand the same wording
or syntactic constructions that it can generate; or
generation and parsing development teams may both
have to work on extensions and modifications to their
grammars, with the likely result that still further
differences will be introduced
These practical problems bolster an intuition
that many have that knowledge of parsing and
generation is the same knowledge in a person&apos;s mind,
or at least that the two faculties draw on a single
representation of their language even if it is engaged
in different ways. This has led to the goal of
reversible NLP systems. The common approach has
been to take the computational artifact constructed by
one of the single-direction projects, typically its
grammar, and to adapt it for use in the other direction.
At ISI, for example, their massive systemic
grammar for generation, NIGEL, (Mann &amp;
Matdiiessen 1985) has since been adapted for use as a
parser (Casper 1989). With the conceptual basis of
the transformation in place, the development of
further extensions and modifications is done on the
generation grammar, and then that grammar is re-
transformed to yield the new parsing grammar.
The other well-known approach to reversible
NLP is of course to use the very same computational
artifact in both processing directions. Thus far this
artifact has invariably been a grammar, typically
some kind of specification of the text-stream --
logical form relation that can be used as a transducer
or can supply the data for it.
Parsers and generators draw on their grammars
as their predominant knowledge source. The grammar
thus becomes a bottleneck for the processing if it is
not designed with efficiency of processing in mind.
When virtually the same computational representation
of the grammar is used in both processes and it is
given an active role, e.g. when the grammar is
couched in a unification formalism, this bottleneck
can be substantial since the &amp;quot;common denominator&amp;quot;
processing architecture that must be employed in
order for the grammar to be literally usable by both
processes will be markedly less efficient than
architectures that work from single-direction
representations of the grammar.
By their nature as information processing
systems, language understanding and generation are
quite different kinds of processes. Understanding
proceeds from texts to intentions. The &amp;quot;known&amp;quot; is
the wording of the text and its intonation. From
these, the understanding process constructs and
deduces the propositional content conveyed by the
text and the probable intentions of the speaker in
producing it. Its primary effort is to scan the words
of the text in sequence, during which the form of the
text gradually unfolds. This requirement to scan
forces the adoption of algorithms based on the
management of multiple hypotheses and predictions
that feed a representation that must be expanded
dynamically. Major problems are caused by
</bodyText>
<page confidence="0.996852">
40
</page>
<bodyText confidence="0.999964018181818">
ambiguity and under-specification (i.e. the audience
typically receives more information from
situationally motivated inferences than is conveyed by
the actual text).
In generation, information flows in the opposite
direction from understanding. Generation proceeds
from content to form, from intentions and
perspectives to linearly arrayed words and syntactic
markers. A generatOr&apos;s &amp;quot;known&amp;quot; is its awareness of
its intentions, its plans, and the text it has already
produced. Coupled with a model of the audience, the
situation, and the discourse, this provides the basis
for making choices among the alternative wordings
and constructions that the language provides---the
principal activity in generation. Most generation
systems do produce , texts sequentially from left to
right---just like an understanding system would scan
it; but they do this only after having made decisions
about the content and form of the text as a whole.
Ambiguity in a generator&apos;s knowledge is not possible
(indeed one of its problems is to notice that it has
inadvertently introduced an ambiguity into the text).
And rather than under-specification, a generator&apos;s
problem is to choose from its over-supply of
information what to include and what to omit so as to
adequately signal its intended inferences to the
audience.
Our concern with efficiency---optimizing the
two processes to fit their differing information
processing characteristics---has led us to approach
reversible NLP by a compilation-style route where
the grammar that the processes use is not one artifact
but two, each with its own representation that is
deliberately tailored to the process that uses it. Like
the system at ISI, our reversible knowledge source is
grounded in the generation process and then projected,
via a compiler, to create the representation used by
the parser. The difference is that while ISI projected
the grammar that the generator used, i.e. the set of
system networks that is the model of the linguistic
resources provided by the language and their
dependencies, our system is a projection from the
underlying application&apos;s conceptual model.
In generation one starts with a set of objects
representing individuals, relations, propositions, etc.
that have been selected from the application program
as its representation of the information it wants to
communicate. Accordingly, the kind of knowledge
that a generator must draw on most frequently is what
are the options for realizing those objects
linguistically. In order to make this look-up
efficient, one is naturally led to an architecture where
this knowledge is stored directly with the definitions
of the objects or their classes, in effect distributing a
highly lexicalized grammar over the knowledge base.
</bodyText>
<sectionHeader confidence="0.570109" genericHeader="method">
A SIMPLE EXAMPLE
</sectionHeader>
<bodyText confidence="0.999909222222222">
To be concrete, consider the example in Figure
One below, a simple definition of die object class
(category) for generic months. This expression says
that a month is a kind of time stuff that can be
viewed either as a point or an interval; that it has a
specific number of days, a position within the year,
and, especially, that it has a name and abbreviations---
the primary options for realizing references to the
month in natural language.
</bodyText>
<figure confidence="0.995995666666667">
(def-category month
:specializes
time/interval-or-point
:slots
((name (word proper-name
:may-be-abbreviated))
(number-of-days number)
(position-in-the-year
number)))
</figure>
<figureCaption confidence="0.980076">
Figure One
</figureCaption>
<bodyText confidence="0.99151725">
In our system, CTI-1, the evaluation of this
expression causes a number of different things to be
constructed: the object representing the category,
indexing and printing functions for objects with that
category (instances of the class), and a defming form.
One then uses the form to create objects for the
twelve months, as shown in Figure Two for
December.
</bodyText>
<figure confidence="0.619491">
(define-month
:name &amp;quot;December&amp;quot;
:abbreviation &amp;quot;Dec&amp;quot;
:number-of-days 31
:position-in-the-year 12)
#&lt;month December
:name #&lt;word &amp;quot;December&amp;quot;›
:abbreviation #&lt;word &amp;quot;Dec&amp;quot;›
:number-of-days #&lt;number 31&gt;
:position-in-the-year
#&lt;number 12&gt;&gt;
</figure>
<figureCaption confidence="0.826948">
Figure Two
</figureCaption>
<bodyText confidence="0.999960076923077">
As a result of evaluating this form, we get the
object for December (Figure Two). When referring to
this object in generation we will look it its name
field and use the word object there, or perhaps the
abbreviated word.
When parsing, we will see an instance of the
word &amp;quot;December&amp;quot; or the phrase &amp;quot;Dec.&amp;quot; and want to
know what object it the application program&apos;s model
it refers to. In CTI-1 this is done by the phrase
structure rules in Figure Three. These rules were
written automatically (compiled) as one of the side-
effects of defining the object for December; the code
for constructing the rules was incorporated into the
</bodyText>
<page confidence="0.997294">
41
</page>
<bodyText confidence="0.99045575">
Define-month form by following the annotation in
the expression that defined the category month, the
same annotation that controls where the the generator
looks when it wants to realize a month object.
</bodyText>
<figure confidence="0.5113824">
#&lt;context-free-rule
:print-form Imonth -&gt; &amp;quot;December&amp;quot;I
:lefthand-side #&lt;category month&gt;
:righthand-side (#&lt;word &amp;quot;December&amp;quot;&gt;)
:syntactic-form
#&lt;category proper-noun&gt;
:referent #&lt;month December»
#&lt;context-free-rule
:print-form Imonth -&gt; &amp;quot;Dec.&amp;quot;I
:lefthand-side #&lt;category month&gt;
:righthand-side ( #&lt;word &amp;quot;Dec&amp;quot;›
#&lt;word &amp;quot;.&amp;quot;&gt; )
:syntactic-form
#&lt;category proper noun&gt;
:referent #&lt;month December&gt;&gt;
</figure>
<figureCaption confidence="0.877537">
Figure Three
</figureCaption>
<bodyText confidence="0.99756925">
These parsing rules are part of CTI-1&apos;s semantic
grammar. They are rewrite rules. When the word
&amp;quot;December&amp;quot; or the two word phrase &amp;quot;Dec&amp;quot; &amp;quot;.&amp;quot; is
scanned, the text segment is spanned with an edge of
the chart (a parse node), and the edge receives a three
part label: (1) the category &amp;quot;month&amp;quot;, which
participates in the semantic grammar, (2) the &amp;quot;form&amp;quot;
category &amp;quot;proper-noun&amp;quot;, which is available to the
syntactic grammar, and (3) the referent the edge picks
out in the application model, i.e. the very object
#&lt;month December&gt; that was defined by the form in
Figure Two.
</bodyText>
<sectionHeader confidence="0.996166" genericHeader="method">
SUMMARY OF THE APPROACH
</sectionHeader>
<bodyText confidence="0.99997">
Before going into a more elaborate example we
can briefly summarize the reversible NLP architecture
we have adopted. The grammar is developed on the
generation side by the linguist/semantic modeler as
part of defining the classes and individuals that
comprise the application&apos;s domain model. They
include with the definitions annotations about how
such objects can be realized in natural language.
A side-effect of definition is the automatic
inversion of the generation rules specified by the
annotation to construct the equivalent set of parsing
rules. Parsimony and uniformity of coverage, the
practical goals of reversible systems, are achieved by
having the parsing grammar constructed automatically
from the original forms that the linguist enters rather
than having them redundantly entered by hand.
Note that what we are projecting from as we
invert &amp;quot;the generator&apos;s rules&amp;quot; is the generator&apos;s
representation of the form-meaning relationship---its
rules for mapping from specific objects in the
underlying application&apos;s domain model to their (set
of) surface linguistic forms by warrant of how the
model has characterized them semantically. This is
not the same as a representation of the principles that
constrain the valid compositional forms of the
language: the constraints on how individual lexical
items and syntactic constructions can be combined,
what elements are requited if others are present, a
formal vocabulary of linguistic categories, and so on.
That representation provides the framework in which
the form-meaning relationship is couched, and it is
developed by hand. For CTI-1 the design choices as
to its categories and relations are taken from the
theory of Tree Adjoining Grammar (Joshi 1985).
The simplicity and immediacy of the automatic
inversion is possible because in our approach the task
of parsing (determining a text&apos;s form) has been
integrated with the task of understanding/semantic
interpretation (determining the denotations of the text
and its elements in some model). This integration is
brought about by using a semantic grammar. A
semantic grammar brings the categories of analysis
used by the parser into the same realm as those used
by the generator, namely the categories of the
application domain (in the present case personnel
changes), for example people, companies, dates, ages,
job titles, relations such as former, new, or has-title,
and event types such as appoint, succeed, retire, etc.
If the parser had been intended only to produce
syntactic structural descriptions of the text, then
projecting its rules from the generator would have
been either impossible or trivial. An application
supports a potentially vast number of categories; the
syntactic categories of natural languages are fixed and
relatively small. Collapsing the (Efferent kinds of
things that can be realized as noun phrases down to
that single category would lose the epistemological
structure of the application&apos;s model and provide only
minimal information to constrain or define the
grammar.
</bodyText>
<sectionHeader confidence="0.9991585" genericHeader="method">
TREES FOR GENERATION, BINARY
RULES FOR PARSING
</sectionHeader>
<bodyText confidence="0.9997485">
Consider the definition of the event type
&amp;quot;appoint-to-position, shown in Figure Four. It&apos;s
linguistic annotation amounts to the specification of
a tree family in a TAG. The features given in the
annotation are consulted to establish what trees the
family should contain, building on the basic
subcategorization frame of a verb that takes a subject
and two NP complements, e.g. that it includes a
passivized tree, one in participial form without its
subject, and so on.
</bodyText>
<page confidence="0.996544">
42
</page>
<figure confidence="0.996013">
(def-category appoint-to-position
:slots ((person person)
(company company)
(position title))
:tree-family
((personnel-change
company! !person!title
:verb &apos;appoint&amp;quot;
(subject -&gt; company)
(objectl -&gt; person)
(object2 -&gt; position)
:optional
((by-company -&gt; company)
(by-person -&gt; new-person))
:passivizes
:forms-participles )))
</figure>
<figureCaption confidence="0.973364">
Figure Four
</figureCaption>
<bodyText confidence="0.999629861111111">
The annotation is equivalent to binding a
specific lexeme to the verb position in the trees of the
family (this is a lexicalized TAG), as well as
restrictions on the denotations of the phrases that will
be substituted for the, other constituents of the clause,
e.g. that the subject &apos;picks out a company, the first
object a person, etc.
A tree family plus its bindings is how this
annotation looks from the generator&apos;s perspective.
For the parser, this same information is represented
quite differently, i.e. as a set of binary phrase
structure rules. Such rules are the more appropriate
representation for parsing (given the algorithm in
CTI-1) since parsing is a process of serial scanning
rather than the top-down refinement done in
generation. During the scan, constituents will
emerge successively bottom up, and the parser&apos;s most
frequent operation and reason for consulting the
grammar will be to judge whether two adjacent
constituents can compose to form a phrase. (The
rules are binary for efficiency concerns: CTI-1 does
the multiplication operation for determining whether
two adjacent constituents form a phrase in constant
time regardless of the size of the grammar.)
The tree family defines a set of rules that are
applicable to any verb and semantic bindings that
share the same subcategorization frame, such as
&amp;quot;name&amp;quot; or &amp;quot;elect&amp;quot;. In projecting the annotation on
the definition of appoint-to-position into parsing
rules, the compilation&apos; process will create the rules of
the family if it does not already exist, and also create
a set of unary rules for the immediate non-terminals
of the verb, one for each different morphological
variant. One of these rules is shown in Figure Five,
along with the general rule for the object-promotion
aspect of passivization.
</bodyText>
<figure confidence="0.989333117647059">
#&lt;context-free-rule
:lefthand-side
#&lt;category
pc/company! !person!title&gt;
:righthand-side
( #&lt;word &apos;appointed&amp;quot;› )
:form #&lt;category main-verb/-ed&gt;
:referent
#&lt;category personnel-
change/appoint-to-position&gt;&gt;
#&lt;context-free-rule/form
:righthand-side
( #&lt;category &apos;be&amp;quot;›
#&lt;category main-verb/-ed&gt; )
:head :second-constituent
:revised-mapping
((object -&gt; subject)))
</figure>
<figureCaption confidence="0.95354">
Figure Five
</figureCaption>
<bodyText confidence="0.999745136363636">
The first phrase structure rule, part of the
semantic grammar, ties the past participial form of
the verb into the family of rules. The long category
name is a convenient mnemonic for the family of
rules, since it shows by its spelling what semantic
categories of constituents are expected as sibling
constituents in the clause as a whole.
The object promotion rule is a syntactic
(&amp;quot;form&amp;quot;) rule that makes reference to the form label
on an edge rather than their semantic label. The rule
for &amp;quot;appointed&amp;quot; has a form label showing that it is a
main verb in past participle form, which is what the
syntactic rule is looking for. When a segment like
&amp;quot;was appointed&amp;quot; is scanned, the label &amp;quot;be&amp;quot; on the edge
spanning &amp;quot;was&amp;quot; will be checked against the label
&amp;quot;main-verb/-ed&amp;quot; on the edge over &amp;quot;appointed&amp;quot; and the
resulting edge will carry the semantic label and
referent of the phrase&apos;s head, i.e. the main verb.
Figure Six shows some of the other rules in the
family so that one can get an idea about how the
parsing of the whole clause will be done. The rules
are given just by their print forms.
</bodyText>
<page confidence="0.999719">
43
</page>
<reference confidence="0.996814454545454">
1. pc/company! !person!title
-&gt; appointed
2. pc/company! !title
-&gt; pc/company! !person!title
person
3. pc/company!
-&gt; pc/company! !title
title
4. personnel-change
-&gt; company
pc/company!
</reference>
<figureCaption confidence="0.626572">
Figure Six
</figureCaption>
<sectionHeader confidence="0.956" genericHeader="method">
STATE OF DEVELOPMENT
</sectionHeader>
<bodyText confidence="0.99991580952381">
The parsing side of this architecture for
reversible NLP is implemented and running in an
operational system, CTI-1. It has a mature domain
model for personnel changes, and has been running
the parsing grammar that is projected from that model
on hundreds of articles from the Wall Street Journal
(&amp;quot;Who&apos;s News&amp;quot;).
The generation side of the architecture is in its
infancy, waiting on a suitable domain and task where
the reasons for speaking and the situation models are
rich enough to motivate subtle nuances in phrasing.
By the same token, my prior experience with
generation leads me to believe that the design of the
linguistic annotations is well-founded for generation,
and that this side of the reversal will fall out once the
opportunity for implementation arises. When this
happens the &amp;quot;raw material&amp;quot; that the mappings
discussed here will supply will be fed to a text
planner like Meteer&apos;s RAVEL orchestrator in her
SPOKESMAN system, and then drive a TAG
realization component along the lines of Mumble-86.
</bodyText>
<sectionHeader confidence="0.996964" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9833233">
Joshi, A.K. (1985) How much context-sensitivity is
required to provide reasonable structural descriptions:
tree adjoining grammars. in Dowty et al. (eds)
Natural Language Processing, Cambridge University
Press.
Mann, W.C. &amp; C. Matthiessen (1985) A
demonstration of the Nigel text generation computer
program, in Benson &amp; Greaves (eds) Systemic
Perspectives in Discourse, Benjamins,
Amsterdam.
</reference>
<page confidence="0.999289">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908923">
<title confidence="0.983132">REVERSIBLE NLP BY DERIVING THE FROM THE KNOWLEDGE BASE</title>
<author confidence="0.999985">David D McDonald</author>
<affiliation confidence="0.999364">Content Technologies, Inc.</affiliation>
<address confidence="0.997736">14 Brantwood Road, Arlington, MA</address>
<abstract confidence="0.994015">We present a new architecture for reversible NLP. Separate parsing and generation grammars are constructed from the underlying application&apos;s semantic model and knowledge base. By having two grammars we are free to use process-specific representations and control techniques, thereby permitting highly efficient processing. The single semantic source ensures the parsimony of development and matched competence that make reversible NLP attractive.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>pc/company! !person!title -&gt; appointed</note>
<contexts>
<context position="9435" citStr="(1)" startWordPosition="1437" endWordPosition="1437">e #&lt;category month&gt; :righthand-side (#&lt;word &amp;quot;December&amp;quot;&gt;) :syntactic-form #&lt;category proper-noun&gt; :referent #&lt;month December» #&lt;context-free-rule :print-form Imonth -&gt; &amp;quot;Dec.&amp;quot;I :lefthand-side #&lt;category month&gt; :righthand-side ( #&lt;word &amp;quot;Dec&amp;quot;› #&lt;word &amp;quot;.&amp;quot;&gt; ) :syntactic-form #&lt;category proper noun&gt; :referent #&lt;month December&gt;&gt; Figure Three These parsing rules are part of CTI-1&apos;s semantic grammar. They are rewrite rules. When the word &amp;quot;December&amp;quot; or the two word phrase &amp;quot;Dec&amp;quot; &amp;quot;.&amp;quot; is scanned, the text segment is spanned with an edge of the chart (a parse node), and the edge receives a three part label: (1) the category &amp;quot;month&amp;quot;, which participates in the semantic grammar, (2) the &amp;quot;form&amp;quot; category &amp;quot;proper-noun&amp;quot;, which is available to the syntactic grammar, and (3) the referent the edge picks out in the application model, i.e. the very object #&lt;month December&gt; that was defined by the form in Figure Two. SUMMARY OF THE APPROACH Before going into a more elaborate example we can briefly summarize the reversible NLP architecture we have adopted. The grammar is developed on the generation side by the linguist/semantic modeler as part of defining the classes and individuals that comprise the application&apos;</context>
</contexts>
<marker>1.</marker>
<rawString>pc/company! !person!title -&gt; appointed</rawString>
</citation>
<citation valid="false">
<title>pc/company! !title -&gt; pc/company!</title>
<note>person!title person</note>
<contexts>
<context position="9505" citStr="(2)" startWordPosition="1447" endWordPosition="1447">rm #&lt;category proper-noun&gt; :referent #&lt;month December» #&lt;context-free-rule :print-form Imonth -&gt; &amp;quot;Dec.&amp;quot;I :lefthand-side #&lt;category month&gt; :righthand-side ( #&lt;word &amp;quot;Dec&amp;quot;› #&lt;word &amp;quot;.&amp;quot;&gt; ) :syntactic-form #&lt;category proper noun&gt; :referent #&lt;month December&gt;&gt; Figure Three These parsing rules are part of CTI-1&apos;s semantic grammar. They are rewrite rules. When the word &amp;quot;December&amp;quot; or the two word phrase &amp;quot;Dec&amp;quot; &amp;quot;.&amp;quot; is scanned, the text segment is spanned with an edge of the chart (a parse node), and the edge receives a three part label: (1) the category &amp;quot;month&amp;quot;, which participates in the semantic grammar, (2) the &amp;quot;form&amp;quot; category &amp;quot;proper-noun&amp;quot;, which is available to the syntactic grammar, and (3) the referent the edge picks out in the application model, i.e. the very object #&lt;month December&gt; that was defined by the form in Figure Two. SUMMARY OF THE APPROACH Before going into a more elaborate example we can briefly summarize the reversible NLP architecture we have adopted. The grammar is developed on the generation side by the linguist/semantic modeler as part of defining the classes and individuals that comprise the application&apos;s domain model. They include with the definitions annotations about ho</context>
</contexts>
<marker>2.</marker>
<rawString>pc/company! !title -&gt; pc/company! !person!title person</rawString>
</citation>
<citation valid="false">
<authors>
<author>pccompany</author>
</authors>
<title>pc/company! !title title</title>
<contexts>
<context position="9593" citStr="(3)" startWordPosition="1460" endWordPosition="1460">month -&gt; &amp;quot;Dec.&amp;quot;I :lefthand-side #&lt;category month&gt; :righthand-side ( #&lt;word &amp;quot;Dec&amp;quot;› #&lt;word &amp;quot;.&amp;quot;&gt; ) :syntactic-form #&lt;category proper noun&gt; :referent #&lt;month December&gt;&gt; Figure Three These parsing rules are part of CTI-1&apos;s semantic grammar. They are rewrite rules. When the word &amp;quot;December&amp;quot; or the two word phrase &amp;quot;Dec&amp;quot; &amp;quot;.&amp;quot; is scanned, the text segment is spanned with an edge of the chart (a parse node), and the edge receives a three part label: (1) the category &amp;quot;month&amp;quot;, which participates in the semantic grammar, (2) the &amp;quot;form&amp;quot; category &amp;quot;proper-noun&amp;quot;, which is available to the syntactic grammar, and (3) the referent the edge picks out in the application model, i.e. the very object #&lt;month December&gt; that was defined by the form in Figure Two. SUMMARY OF THE APPROACH Before going into a more elaborate example we can briefly summarize the reversible NLP architecture we have adopted. The grammar is developed on the generation side by the linguist/semantic modeler as part of defining the classes and individuals that comprise the application&apos;s domain model. They include with the definitions annotations about how such objects can be realized in natural language. A side-effect of definition is the a</context>
</contexts>
<marker>3.</marker>
<rawString>pc/company! -&gt; pc/company! !title title</rawString>
</citation>
<citation valid="false">
<authors>
<author>company pccompany Joshi</author>
<author>A K</author>
</authors>
<title>How much context-sensitivity is required to provide reasonable structural descriptions: tree adjoining grammars. in Dowty et al. (eds) Natural Language Processing,</title>
<date>1985</date>
<journal>Mann, W.C. &amp; C. Matthiessen</journal>
<booktitle>in Benson &amp; Greaves (eds) Systemic Perspectives in Discourse, Benjamins,</booktitle>
<publisher>Cambridge University Press.</publisher>
<location>Amsterdam.</location>
<marker>4.</marker>
<rawString>personnel-change -&gt; company pc/company! Joshi, A.K. (1985) How much context-sensitivity is required to provide reasonable structural descriptions: tree adjoining grammars. in Dowty et al. (eds) Natural Language Processing, Cambridge University Press. Mann, W.C. &amp; C. Matthiessen (1985) A demonstration of the Nigel text generation computer program, in Benson &amp; Greaves (eds) Systemic Perspectives in Discourse, Benjamins, Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>