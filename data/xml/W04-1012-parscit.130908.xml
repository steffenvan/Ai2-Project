<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006607">
<title confidence="0.983947">
Automatic Evaluation of Summaries Using Document Graphs
</title>
<author confidence="0.991046">
Eugene Santos Jr., Ahmed A. Mohamed, and Qunhua Zhao
</author>
<affiliation confidence="0.998323">
Computer Science and Engineering Department
University of Connecticut
</affiliation>
<address confidence="0.980082">
191 Auditorium Road, U-155, Storrs, CT 06269-3155
</address>
<email confidence="0.98405">
{eugene, amohamed, qzhao}@engr.uconn.edu
</email>
<sectionHeader confidence="0.993697" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999850833333333">
Summarization evaluation has been always a chal-
lenge to researchers in the document summariza-
tion field. Usually, human involvement is
necessary to evaluate the quality of a summary.
Here we present a new method for automatic
evaluation of text summaries by using document
graphs. Data from Document Understanding Con-
ference 2002 (DUC-2002) has been used in the
experiment. We propose measuring the similarity
between two summaries or between a summary
and a document based on the concepts/entities and
relations between them in the text.
</bodyText>
<sectionHeader confidence="0.998689" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999883655172414">
Document summarization has been the focus of
many researchers for the last decade, due to the
increase in on-line information and the need to
find the most important information in a (set of)
document(s). One of the biggest challenges in text
summarization research is how to evaluate the
quality of a summary or the performance of a
summarization tool. There are different approa-
ches to evaluate overall quality of a summariza-
tion system. In general, there are two types of
evaluation categories: intrinsic and extrinsic
(Sparck-Jones and Galliers, 1996). Extrinsic ap-
proaches measure the quality of a summary based
on how it affects certain tasks. In intrinsic approa-
ches, the quality of the summarization is evaluated
based on analysis of the content of a summary
itself. In both categories human involvement is
used to judge the summarization outputs. The
problem with having humans involved in evalua-
ting summaries is that we can not hire human jud-
ges every time we want to evaluate summaries
(Mani and Maybury, 1999). In this paper, we dis-
cuss a new automated way to evaluate machine-
generated summaries without the need to have
human judges being involved which decreases the
cost of determining which summarization system
is best. In our experiment, we used data from Do-
cument Understanding Conference 2002 (DUC-
2002).
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99983225">
Researchers in the field of document summariza-
tion have been trying for many years to define a
metric for evaluating the quality of a machine-
generated summary. Most of these attempts invol-
ve human interference, which make the process of
evaluation expensive and time-consuming. We
discuss some important work in the intrinsic cate-
gory.
</bodyText>
<subsectionHeader confidence="0.991683">
2.1 Sentence Precision-Recall Measure
</subsectionHeader>
<bodyText confidence="0.9999085">
Sentence precision and recall have been widely
used to evaluate the quality of a summarizer (Jing
et al., 1998). Sentence precision measures the per-
cent of the summary that contains sentences mat-
ched with the model summary. Recall, on the
other hand, measures the percent of sentences in
the ideal summary that have been recalled in the
summary. Even though sentence precision/recall
factors can give us an idea about a summary’s
quality, they are not the best metrics to evaluate a
system’s quality. This is due to the fact that a
small change in the output summary can dramati-
cally affect the quality of a summary (Jing et al.,
1998). For example, it is possible that a system
will pick a sentence that does not match with a
model sentence chosen by an assessor, but is
equivalent to it in meaning. This, of course, will
affect the score assigned to the system dramatica-
lly. It is also obvious that sentence precision/recall
is only applicable to the summaries that are gene-
rated by sentence extraction, not abstraction (Ma-
ni, 2001).
</bodyText>
<subsectionHeader confidence="0.978851">
2.2 Content-Based Measure
</subsectionHeader>
<bodyText confidence="0.99998158974359">
Content-based measure computes the similarity at
the vocabulary level (Donaway, 2000 and Mani,
2001). The evaluation is done by creating term
frequency vectors for both the summary and the
model summary, and measuring the cosine simila-
rity (Salton, 1988) between these two vectors. Of
course, the higher the cosine similarity measure,
the higher the quality of the summary is. Lin and
Hovy (2002) used accumulative n-gram matching
scores between model summaries and the summa-
ries to be evaluated as a performance indicator in
multi-document summaries. They achieved their
best results by giving more credit to longer n-
gram matches with the use of Porter stemmer.
A problem raised in the evaluation approaches
that use the cosine measure is that the summaries
may use different key terms than those in the ori-
ginal documents or model summaries. Since term
frequency is the base to score summaries, it is
possible that a high quality summary will get a
lower score if the terms used in the summary are
not the same terms used in most of the document’s
text. Donaway et al. (2000) discussed using a
common tool in information retrieval: latent se-
mantic indexing (LSI) (Deerwester et al., 1990) to
address this problem. The use of LSI reduces the
effect of near-synonymy problem on the similarity
score. This is done by penalizing the summary
less in the reduced dimension model when there
are infrequent terms synonymous to frequent
terms. LSI averages the weights of terms that co-
occur frequently with other mutual terms. For
example, both “bank” and “financial institution”
often occur with the term “account” (Deerwester
et al., 1990). Even though using LSI can be useful
in some cases, it can produce unexpected results
when the document contains terms that are not
synonymous to each other, but, however, they co-
occur with other mutual terms.
</bodyText>
<subsectionHeader confidence="0.770682666666667">
2.3 Document Graph
2.3.1 Representing Content by Document
Graph
</subsectionHeader>
<bodyText confidence="0.996088">
Current approaches in content-based summariza-
tion evaluation ignore the relations between the
keywords that are expressed in the document.
Here, we introduce our approach, which measures
the similarity between two summaries or a sum-
mary and a document based on the relations (bet-
ween the keywords). In our approach, each
document/summary is represented as a document
graph (DG), which is a directed graph of con-
cepts/entities and the relations between them. A
DG contains two kinds of nodes, concept/entity
nodes and relation nodes. Currently, only two
kinds of relations, “isa” and “related to”, are cap-
tured (Santos et al, 2001) for simplicity.
To generate a DG, a document/summary in
plain text format is first tokenized into sentences;
and then, each sentence is parsed using Link Par-
ser (Sleator and Temperley, 1993), and the noun
phrases (NP) are extracted from the parsing re-
sults. The relations are generated based on three
heuristic rules:
</bodyText>
<listItem confidence="0.99584572">
• The NP-heuristic helps to set up the hierar-
chical relations. For example, from a noun
phrase “folk hero stature”, we generate re-
lations “folk hero stature isa stature”, “folk
hero stature related to folk hero”, and “folk
hero isa hero”.
• The NP-PP-heuristic attaches all preposi-
tional phrases to adjacent noun phrases. For
example, from “workers at a coal mine”,
we generate a relation, “worker related to
coal mine”.
• The sentence-heuristic relates con-
cepts/entities contained in one sentence.
The relations created by sentence-heuristic
are then sensitive to verbs, since the inter-
val between two noun phrases usually con-
tains a verb. For example, from a sentence
“Workers at a coal mine went on strike”,
we generate a relation “worker related to
strike”. Another example, from “The usual
cause of heart attacks is a blockage of the
coronary arteries”, we generate “heart at-
tack cause related to coronary artery bloc-
kage”. Figure 1 shows a example of a
partial DG.
</listItem>
<figureCaption confidence="0.980034">
Figure 1: A partial DG.
</figureCaption>
<subsectionHeader confidence="0.9396895">
2.3.2 Similarity Comparison between two
Document Graphs
</subsectionHeader>
<bodyText confidence="0.999851181818182">
The similarity of DG1 to DG2 is given by the
equation:
which is modified from Montes-y-Gómez et al.
(2000). N is the number of concept/entity nodes in
DG1, and M stands for number of relations in
DG1; n is the number of matched concept/entity
nodes in two DGs, and m is the number of mat-
ched relations. We say we find a matched relation
in two different DGs, only when both of the two
concept/entity nodes linked to the relation node
are matched, and the relation node is also mat-
ched. Since we might compare two DGs that are
significantly different in size (for example, DGs
for an extract vs. its source document), we used
the number of concept/entity nodes and relation
nodes in the target DG as N and M, instead of the
total number of nodes in both DGs. The target DG
is the one for the extract in comparing an extract
with its source text. Otherwise, the similarity will
always be very low. Currently, we weight all the
concepts/entities and relations equally. This can
be fine tuned in the future.
</bodyText>
<sectionHeader confidence="0.94762" genericHeader="method">
3 Data, and Experimental Design
</sectionHeader>
<subsectionHeader confidence="0.686105">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9980451">
Because the data from DUC-2003 were short
(~100 words per extract for multi-document task),
we chose to use multi-document extracts from
DUC-2002 (~200 words and ~400 words per ex-
tract for multi-document task) in our experiment.
In this corpus, each of ten information analysts
from the National Institute of Standards and
Technology (NIST) chose one set of newswi-
re/paper articles in the following topics (Over and
Liggett, 2002):
</bodyText>
<listItem confidence="0.99911375">
• A single natural disaster event with docu-
ments created within at most a 7-day win-
dow
• A single event of any type with documents
created within at most a 7-day window
• Multiple distinct events of the same type (no
time limit)
• Biographical (discuss a single person)
</listItem>
<bodyText confidence="0.9998962">
Each assessor chose 2 more sets of articles so
that we ended up with a total of 15 document sets
of each type. Each set contains about 10 docu-
ments. All documents in a set are mainly about a
specific “concept.”
A total of ten automatic summarizers participa-
ted to produce machine-generated summaries.
Two extracts of different lengths, 200 and 400
words, have been generated for each document-
set.
</bodyText>
<subsectionHeader confidence="0.996677">
3.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999989857142857">
A total of 10 different automatic summarization
systems submitted their summaries to DUC. We
obtained a ranking order of these 10 systems ba-
sed on sentence precision/recall by comparing the
machine generated extracts to the human genera-
ted model summaries. The F-factor is calculated
from the following equation (Rijsbergen, 1979):
</bodyText>
<equation confidence="0.852279">
2xPxR
( P+R)
</equation>
<bodyText confidence="0.999974388888889">
where P is the precision and R is the recall. We
think this ranking order gives us some idea on
how human judges think about the performance of
different systems.
For our evaluation based on DGs, we also
calculated F-factors based on precision and recall,
where P = Sim(DG1, DG2) and R = Sim(DG2,
DG1). In the first experiment, we ranked the 10
automatic summarization systems by comparing
DGs generated from their outputs to the DGs gen-
erated from model summaries. In this case, DG1 is
the machine generated extract and DG2 is the hu-
man generated extract. In the second experiment,
we ranked the systems by comparing machine
generated extracts to the original documents. In
this case, DG1 is an extract and DG2 is the corre-
sponding original document. Since the extracts
were generated from multi-document sets, we
</bodyText>
<equation confidence="0.977242333333333">
n
Sim DG DG
( 1 , 2) = +
2N 2M
m
F
</equation>
<table confidence="0.998442533333333">
System Sentence- Sentence- DG- DG-
rank based based based based
Ranking F-factor Ranking F-factor
1 22 0.000 22 0.122
(worst)
2 16 0.062 16 0.167
3 31 0.081 31 0.180
4 25 0.081 25 0.188
5 29 0.090 29 0.200
6 20 0.125 20 0.226
7 28 0.138 28 0.255
8 24 0.171 19 0.283
9 19 0.184 24 0.283
10 21 0.188 21 0.308
(best)
</table>
<tableCaption confidence="0.930093666666667">
Table 1: Model Summaries vs. machine-
generated summaries. Ranking results for 200
words extracts
</tableCaption>
<table confidence="0.9996946">
System Sentence- Sentence- DG- DG-
rank based based based based
Ranking F-factor Ranking F-factor
1 22 0.000 22 0.181
(worst)
2 16 0.128 16 0.235
3 25 0.147 25 0.256
4 31 0.150 31 0.266
5 29 0.155 20 0.273
6 20 0.172 29 0.279
7 28 0.197 28 0.316
8 24 0.223 19 0.337
9 19 0.224 24 0.355
10 21 0.258 21 0.372
(best)
</table>
<tableCaption confidence="0.980551333333333">
Table 2: Model Summaries vs. machine-
generated summaries. Ranking results for 400
words extracts
</tableCaption>
<bodyText confidence="0.9683575">
used the average of the F-factors for ranking pur-
poses.
</bodyText>
<sectionHeader confidence="0.999937" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999889741379311">
The ranking orders obtained based on sentence
precisions and recalls are shown in Tables 1 and
2. The results indicate that for sentence precision
and recall, the ranking order for different summa-
rization systems is not affected by the summariza-
tion compression ratio. The ranking results for
200-word extracts and 400-word extracts are ex-
actly the same.
Since the comparison is between the machine
generated extracts and the human created model
extracts, we believe that the rankings should rep-
resent the performance of 10 different automated
summarization systems, to some degree. The ex-
periments using DGs instead of sentence matching
give two very similar ranking orders (Spearman
rank correlation coefficient [Myers and Well,
1995] is 0.988) where only systems 24 and 19 are
reversed in their ranks (Tables 1 and 2). The re-
sults show that when the evaluation is based on
the comparison between machine generated ex-
tracts and the model extracts, our DG-based
evaluation approach will provide roughly the
same ranking results as the sentence precision and
recall approach. Notice that the F-factors obtained
by experiments using DGs are higher than those
calculated based on sentence matching. This is
because our DG-based evaluation approach com-
pares the two extracts at a more fine grained level
than sentence matching does since we compare
the similarity at the level of concepts/entities and
their relations, not just whole sentences. The simi-
larity of the two extracts should actually be higher
than the score obtained with sentence matching
because there are sentences that are equivalent in
meaning but not syntactically identical.
Since we believe that the DGs captures the se-
mantic information content contained in the res-
pective documents, we rank the automatic
summarization systems by comparing the DGs of
their extract outputs against the DGs of the origi-
nal documents. This approach does not need the
model summaries, and hence no human involve-
ment is needed in the evaluation. The results are
shown in Tables 3 and 4. As we can see, our ran-
kings are different from the ranking results based
on comparison against the model extracts. System
28 has the largest change in rank in both 200-word
and 400-word summaries. It was ranked as the
worst by our DG based approach instead of num-
ber 7 (10 is the best) by the approaches comparing
to the model extracts. We investigated the extract
content of system 28 and found that many extracts
generated by system 28 included sentences that
contain little information, e.g., author’s names,
publishers, date of publication, etc. The following
are sample extracts produced for document 120 by
systems 28, 29 (the best ranked) and a human jud-
ge, at 200-words.
</bodyText>
<figure confidence="0.844504789473684">
[Extract for Document 120 by System 28]
John Major, endorsed by Margaret Thatcher as
the politician closest to her heart, was elected by
the Conservative Party Tuesday night to succeed
her as prime minister.
Hong Kong WEN WEI PO
By MICHAEL CASSELL and IVOR OWEN
By MICHAEL THOMPSON-NOEL
By DOMINIC LAWSON
From Times Wire Services
By WILLIAM TUOHY, TIMES STAFF WRITER
From Associated Press
[Extract for Document 120 by System 29]
John Major, endorsed by Margaret Thatcher as
the politician closest to her heart, was elected by
the Conservative Party Tuesday night to succeed
her as prime minister.
Aides said Thatcher is &amp;quot;thrilled&amp;quot;.
Hurd also quickly conceded.
</figure>
<bodyText confidence="0.884147304347826">
ONE year ago tomorrow, Mr John Major surpri-
sed everyone but himself by winning the general
election.
It has even been suggested that the recording of
the prime minister&apos;s conversation with Michael
Brunson, ITN&apos;s political editor, in which Major
used a variety of four-, six- and eight-letter words
to communicate his lack of fondness for certain
colleagues, may do him good.
BFN
[Colin Brown article: &amp;quot;Cabinet Allies Close Ranks
But Bring
Right-wing MPs confirmed the findings in an
INDEPENDENT ON SUNDAY/NOP [National
Opinion Poll] poll that Michael Heseltine was the
favourite to replace Mr Major, if he is forced out.
The Labour Party controls 90 local councils, whe-
reas the Conservatives only control 13, with a
sharp contrast in strength between the two sides.
If he did not see the similarity, that is still more
revealing.
[Extract for Document 120 by a human judge --
model extract]
John Major, endorsed by Margaret Thatcher as
the politician closest to her heart, was elected by
the Conservative Party Tuesday night to succeed
her as prime minister.
While adopting a gentler tone on the contentious
issue of Britain&apos;s involvement in Europe, he shares
her opposition to a single European currency and
shares her belief in tight restraint on government
spending.
FT 08 APR 93 / John Major&apos;s Year: Major&apos;s blue
period - A year on from success at the polls, the
prime minister&apos;s popularity has plunged.
The past 12 months have been hijacked by inter-
nal party differences over Europe, by the debacle
surrounding UK withdrawal from the exchange
rates mechanism of the European Monetary Sys-
tem, and by a continuing, deep recession which
has disappointed and alienated many traditional
Tory supporters in business.
Its Leader&amp;quot;] [Text] In local government elections
across Britain yesterday, the Conservatives suffe-
red their worst defeat ever, losing control of 17
regional councils and 444 seats.
Even before all of the results were known, some
Tories openly announced their determination to
challenge John Major&apos;s position and remove him
from office as early as possible.
The extract generated by system 28 has 8 sen-
tences of which only one of them contained rele-
vant information. When comparing using sentence
precision and recall, all three extracts only have
one sentence match which is the first sentence. If
we calculate the F-factors based on the model ex-
tract shown above, system 28 has a score of 0.143
and system 29 has a lower score of 0.118. After
reading all three extracts, the extract generated by
system 29 contains much more relevant informa-
tion than that generated by system 28. The mis-
sing information in system 28 is ---John Major
and the Conservatives were losing the popularity
in 1993, after John Major won the election one
year ago,-- which should be the most important
content in the extract. In our DG-based approach,
the scores assigned to system 28 and 29 are 0.063
and 0.100, respectively; which points out that sys-
tems 29 did a better job than system 28.
</bodyText>
<table confidence="0.999403066666667">
System Sentence- Sentence- DG- DG-
rank based based based based
Ranking F-factor Ranking F-factor
1 22 0.000 28 0.092
(worst)
2 16 0.062 22 0.101
3 31 0.081 16 0.111
4 25 0.081 20 0.115
5 29 0.090 25 0.115
6 20 0.125 21 0.122
7 28 0.138 31 0.124
8 24 0.171 24 0.125
9 19 0.184 19 0.129
10 21 0.188 29 0.132
(best)
</table>
<tableCaption confidence="0.951504">
Table 3: Machine-generated summaries vs.
source documents. Ranking results for 200
words extracts
</tableCaption>
<table confidence="0.9993632">
System Sentence- Sentence- DG- DG-
rank based based based based
Ranking F-factor Ranking F-factor
1 22 0.000 22 0.137
(worst)
2 16 0.128 28 0.141
3 25 0.147 16 0.160
4 31 0.150 25 0.163
5 29 0.155 20 0.164
6 20 0.172 31 0.165
7 28 0.197 21 0.167
8 24 0.223 29 0.168
9 19 0.224 19 0.168
10 21 0.258 24 0.169
(best)
</table>
<tableCaption confidence="0.950114333333333">
Table 4: Machine-generated summaries vs.
source documents. Ranking results for 400
words extracts
</tableCaption>
<table confidence="0.999278461538462">
200-word 400-word
System F-factor System F-factor
28 0.092 22 0.137
22 0.101 28 0.141
16 0.111 16 0.160
20 0.115 25 0.163
25 0.115 20 0.164
21 0.122 31 0.165
Model 0.124 Model 0.165
31 0.124 21 0.167
24 0.125 29 0.168
19 0.129 19 0.168
29 0.132 24 0.169
</table>
<tableCaption confidence="0.995592">
Table 5: Average F-factors for the model sum-
maries and machine-generated summaries.
</tableCaption>
<bodyText confidence="0.999990521739131">
Of the 59 submitted 200-word extracts by sys-
tem 28, 39 extracts suffer the problem of having
less informative sentences. The number of such
sentences is 103, where the total number of sen-
tences is 406 from all the extracts for system 28.
On average, each extract contains 1.75 such sen-
tences, where each extract has 6.88 sentences. For
the 400-words extracts, we found 54 extracts
among the 59 submitted summaries also have this
problem. The total number of such sentences was
206, and the total number of sentences was 802
sentences. So, about 3.49 sentences do not con-
tain much information, where the average length
of each extract is 13.59 sentences. Thus, a large
portion of each extract does not contribute to the
do example, will not be considered a good sum-
mary, either on the criterion of summary coheren-
ce or summary informativeness, where coherence
is how the summary reads and informativeness is
how much information from the source is preser-
ved in the summary (Mani, 2001).
From the results based on comparing extracts
against original documents, we found that several
systems perform very similarly, especially in the
experiments with 400-word extracts (Table 4).
The results show that except for systems 22 and
28 which perform significantly worse, all other
systems are very similar, from the point of view of
informativeness.
Finally, we generated DGs for the model extra-
cts and then compared them against their original
documents. The average F-factors are calculated,
which are listed in Table 5 along with the scores
for different automatic summarization systems.
Intuitively, a system provides extracts that contain
more information than other systems will get a
higher score. As we can see from the data, at 200-
words, the extracts generated by systems 21, 31,
24, 19, and 29 contain roughly the same amount
of information as those created by humans, while
the other five systems performed worse than
human judges. At 400-words, when the compres-
sion ratio of the extracts is decreased, more sys-
tems perform well; only systems 22 and 28
generated summaries that contain much less in-
formation than the model summaries.
</bodyText>
<sectionHeader confidence="0.98769" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999952847457627">
In DUC 2002 data collection, 9 human judges
were involved in creating model extracts; how-
ever, there are only 2 model extracts generated for
each document set. The sentence precisions and
recalls obtained from comparing the machine gen-
erated extracts and human generated model ex-
tracts are distributed along with raw data (DUC-
2002. http://www-nlpir.nist.gov/projects/duc),
with the intent to use them in system performance
comparison. Van Halteren (2002) argued that only
two manually created extracts could not be used to
form a sufficient basis for a good benchmark. To
explore this issue, we obtained a ranking order for
each human judge based on the extracts he/she
generated. The results showed that the ranking
orders obtained from 9 different judges are actu-
ally similar to each other, with the average
Spearman correlation efficient to be 0.901. From
this point of view, if the ranking orders obtained
by sentence precision and recall based on the
model extracts could not form a good basis for a
benchmark, it is because of its binary nature (Jing
et al., 1998), not the lack of sufficient model ex-
tracts in DUC 2002 data.
Van Halteren and Teufel (2003) proposed to
evaluate summaries via factoids, a pseudo-
semantic representation based on atomic informa-
tion units. However, sufficient manually created
model summaries are need; and factoids are also
manually annotated. Donaway et al. (2000) sug-
gested that it might be possible to use content-
based measures for summarization evaluation wit-
hout generating model summaries. Here, we pre-
sented our approach to evaluate the summaries
base on document graphs, which is generated au-
tomatically. It is not very surprising that different
measures rank summaries differently. A similar
observation has been reported previously (Radev,
et al, 2003). Our document graph approach on
summarization evaluation is a new automatic way
to evaluate machine-generated summaries, which
measures the summaries from the point of view of
informativeness. It has the potential to evaluate
the quality of summaries, including extracts, abs-
tracts, and multi-document summaries, without
human involvement. To improve the performance
of our system and better represent the content of
the summaries and source documents, we are
working in several areas: 1) Improve the results of
natural language processing to capture informa-
tion more accurately; 2) Incorporate a knowledge
base, such as WordNet (Fellbaum, 1998), to ad-
dress the synonymy problem; and, 3) Use more
heuristics in our relation extraction and genera-
tion. We are also going to extend our experiments
by comparing our approach to content-based mea-
sure approaches, such as cosine similarity based
on term frequencies and LSI approaches, in both
extracts and abstracts.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999675833333333">
This work was supported in part by the Advanced
Research and Development Activity (ARDA) U.S.
Government. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the U. S. Government.
</bodyText>
<sectionHeader confidence="0.998757" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998686626373626">
Scott Deerwester, Susan T. Dumais, George W.
Furnas, Thomas K. Landauer, and Richard
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society for In-
formation Science, 41(6): 391-407.
Robert L. Donaway, Kevin W. Drummey, and
Laura A. Mather. 2000. A comparison of rank-
ings produced by summarization evaluation
measures. In Proceedings of the Workshop on
Automatic Summarization, pages 69-78.
Christiane Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal,
and Jaime Carbonell. 1999. Summarizing text
documents: Sentence selection and evaluation
metrics. In Proceedings the 24th Annual Interna-
tional ACM SIGIR Conference on Research and
Development in Information Retrieval, pages
121-128, ACM, New York..
Hans van Haltern. 2002. Writing style recognition
and sentence extraction. DUC’02 Conference
Proceedings.
Hans Van Halteren and Simone Teufel, 2003. Ex-
amining the consensus between human summa-
ries: Initial experiments with factoid analysis. In
HLT/NAACL-2003 Workshop on Automatic
Summarization.
Hongyan Jing, Kathleen McKeown, Regina Barzi-
lay, and Michael Elhadad. 1998. Summarization
evaluation methods: experiments and analysis.
In American Association for Artificial Intelli-
gence Spring Symposium Series, pages 60-68.
Chen-Yew Lin. 2001. Summary evaluation
environment. http://www.isi.edu/~cyl/SEE.
Chin-Yew Lin and Eduard Hovy. 2002. Manual
and automatic evaluation of summaries. In Pro-
ceedings of the Workshop on Automatic Summa-
rization post conference workshop of ACL-02,
Philadelphia, PA, U.S.A., July 11-12 (DUC
2002).
Inderjeet Mani. 2001. Summarization evaluation:
An overview. In Proceedings of the NTCIR
Workshop 2 Meeting on Evaluation of Chinese
and Japanese Text Retrieval and Test Summari-
zation, National Institute of Informatics.
Inderjeet Mani and Mark T. Maybury. 1999. Ad-
vances in Automatic Text Summarization. The
MIT Press.
Manuel Montes-y-Gómez, Alexander Gelbukh,
and Aurelio López-López. 2000. Comparison of
conceptual graphs. In Proceedings of MICAI-
2000, 1st Mexican International Conference on
Artificial Intelligence, Acapulco, Mexico.
Jerome L. Myers and Arnold D. Well. 1995. Re-
search Design and Statistical Analysis, pages,
488-490, Lawrence Erlbaum Associates, New
Jersey
Paul Over and Walter Liggett. 2002. Introduction
to DUC-2002: An intrinsic evaluation of generic
news text summarization systems. Document
Understanding Conferences website
(http://duc.nist.gov/)
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: A Method for Auto-
matic Evaluation of Machine Translation.
Technical Report RC22176 (W0109-022), IBM
Research Division, Thomas J. Watson Research
Center.
Dragomir R. Radev, Simone Teufel, Horacio Sag-
gion, Wai Lam, John Blitzer, Hong Qi, Arda
Celebi, Danyu Liu, and Elliott Drabek. 2003.
Evaluation challenges in large-scale document
summarization. In Proceedings of the 41st An-
nual Meetring of the Association for Computa-
tional Linguistics, July 2003, pages 375-382.
Keith van Rijsbergen. 1979. Information Re-
trieval. Second Edition Butterworths, London.
Gerard Salton. 1988. Automatic Text Processing.
Addison-Wesley Publishing Company.
Eugene Santos Jr., Hien Nguyen, and Scott M.
Brown. 2001. Kavanah: An active user interface
Information Retrieval Agent Technology. Mae-
bashi, Japan, October 2001, pages 412-423.
Danny Sleator and Davy Temperley. 1993. Pars-
ing English with a link grammar. In Proceed-
ings of the Third International Workshop on
Parsing Technologies, pages 277-292.
Karen Sparck-Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing Sys-
tems: An Analysis and Review (Lecture Notes in
Artificial Intelligence 1083). Springer-Verlag
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947674">
<title confidence="0.999938">Automatic Evaluation of Summaries Using Document Graphs</title>
<author confidence="0.983183">Eugene Santos Jr</author>
<author confidence="0.983183">Ahmed A Mohamed</author>
<author confidence="0.983183">Qunhua</author>
<affiliation confidence="0.997936">Computer Science and Engineering University of</affiliation>
<address confidence="0.987064">191 Auditorium Road, U-155, Storrs, CT</address>
<email confidence="0.992963">eugene@engr.uconn.edu</email>
<email confidence="0.992963">amohamed@engr.uconn.edu</email>
<email confidence="0.992963">qzhao@engr.uconn.edu</email>
<abstract confidence="0.998725384615385">Summarization evaluation has been always a challenge to researchers in the document summarization field. Usually, human involvement is necessary to evaluate the quality of a summary. Here we present a new method for automatic evaluation of text summaries by using document graphs. Data from Document Understanding Conference 2002 (DUC-2002) has been used in the experiment. We propose measuring the similarity between two summaries or between a summary and a document based on the concepts/entities and relations between them in the text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan T Dumais</author>
<author>George W Furnas</author>
<author>Thomas K Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--407</pages>
<contexts>
<context position="4799" citStr="Deerwester et al., 1990" startWordPosition="775" endWordPosition="778">their best results by giving more credit to longer ngram matches with the use of Porter stemmer. A problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries. Since term frequency is the base to score summaries, it is possible that a high quality summary will get a lower score if the terms used in the summary are not the same terms used in most of the document’s text. Donaway et al. (2000) discussed using a common tool in information retrieval: latent semantic indexing (LSI) (Deerwester et al., 1990) to address this problem. The use of LSI reduces the effect of near-synonymy problem on the similarity score. This is done by penalizing the summary less in the reduced dimension model when there are infrequent terms synonymous to frequent terms. LSI averages the weights of terms that cooccur frequently with other mutual terms. For example, both “bank” and “financial institution” often occur with the term “account” (Deerwester et al., 1990). Even though using LSI can be useful in some cases, it can produce unexpected results when the document contains terms that are not synonymous to each othe</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert L Donaway</author>
<author>Kevin W Drummey</author>
<author>Laura A Mather</author>
</authors>
<title>A comparison of rankings produced by summarization evaluation measures.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization,</booktitle>
<pages>69--78</pages>
<contexts>
<context position="4686" citStr="Donaway et al. (2000)" startWordPosition="758" endWordPosition="761">aries and the summaries to be evaluated as a performance indicator in multi-document summaries. They achieved their best results by giving more credit to longer ngram matches with the use of Porter stemmer. A problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries. Since term frequency is the base to score summaries, it is possible that a high quality summary will get a lower score if the terms used in the summary are not the same terms used in most of the document’s text. Donaway et al. (2000) discussed using a common tool in information retrieval: latent semantic indexing (LSI) (Deerwester et al., 1990) to address this problem. The use of LSI reduces the effect of near-synonymy problem on the similarity score. This is done by penalizing the summary less in the reduced dimension model when there are infrequent terms synonymous to frequent terms. LSI averages the weights of terms that cooccur frequently with other mutual terms. For example, both “bank” and “financial institution” often occur with the term “account” (Deerwester et al., 1990). Even though using LSI can be useful in so</context>
<context position="22851" citStr="Donaway et al. (2000)" startWordPosition="3826" endWordPosition="3829"> each other, with the average Spearman correlation efficient to be 0.901. From this point of view, if the ranking orders obtained by sentence precision and recall based on the model extracts could not form a good basis for a benchmark, it is because of its binary nature (Jing et al., 1998), not the lack of sufficient model extracts in DUC 2002 data. Van Halteren and Teufel (2003) proposed to evaluate summaries via factoids, a pseudosemantic representation based on atomic information units. However, sufficient manually created model summaries are need; and factoids are also manually annotated. Donaway et al. (2000) suggested that it might be possible to use contentbased measures for summarization evaluation without generating model summaries. Here, we presented our approach to evaluate the summaries base on document graphs, which is generated automatically. It is not very surprising that different measures rank summaries differently. A similar observation has been reported previously (Radev, et al, 2003). Our document graph approach on summarization evaluation is a new automatic way to evaluate machine-generated summaries, which measures the summaries from the point of view of informativeness. It has th</context>
</contexts>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>Robert L. Donaway, Kevin W. Drummey, and Laura A. Mather. 2000. A comparison of rankings produced by summarization evaluation measures. In Proceedings of the Workshop on Automatic Summarization, pages 69-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23894" citStr="Fellbaum, 1998" startWordPosition="3984" endWordPosition="3985">n summarization evaluation is a new automatic way to evaluate machine-generated summaries, which measures the summaries from the point of view of informativeness. It has the potential to evaluate the quality of summaries, including extracts, abstracts, and multi-document summaries, without human involvement. To improve the performance of our system and better represent the content of the summaries and source documents, we are working in several areas: 1) Improve the results of natural language processing to capture information more accurately; 2) Incorporate a knowledge base, such as WordNet (Fellbaum, 1998), to address the synonymy problem; and, 3) Use more heuristics in our relation extraction and generation. We are also going to extend our experiments by comparing our approach to content-based measure approaches, such as cosine similarity based on term frequencies and LSI approaches, in both extracts and abstracts. 6 Acknowledgments This work was supported in part by the Advanced Research and Development Activity (ARDA) U.S. Government. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the U</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Proceedings the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>121--128</pages>
<publisher>ACM,</publisher>
<location>New York..</location>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and Jaime Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Proceedings the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 121-128, ACM, New York..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Haltern</author>
</authors>
<title>Writing style recognition and sentence extraction.</title>
<date>2002</date>
<booktitle>DUC’02 Conference Proceedings.</booktitle>
<marker>van Haltern, 2002</marker>
<rawString>Hans van Haltern. 2002. Writing style recognition and sentence extraction. DUC’02 Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Van Halteren</author>
<author>Simone Teufel</author>
</authors>
<title>Examining the consensus between human summaries: Initial experiments with factoid analysis.</title>
<date>2003</date>
<booktitle>In HLT/NAACL-2003 Workshop on Automatic Summarization.</booktitle>
<marker>Van Halteren, Teufel, 2003</marker>
<rawString>Hans Van Halteren and Simone Teufel, 2003. Examining the consensus between human summaries: Initial experiments with factoid analysis. In HLT/NAACL-2003 Workshop on Automatic Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization evaluation methods: experiments and analysis.</title>
<date>1998</date>
<contexts>
<context position="2658" citStr="Jing et al., 1998" startWordPosition="414" endWordPosition="417"> summarization system is best. In our experiment, we used data from Document Understanding Conference 2002 (DUC2002). 2 Related Work Researchers in the field of document summarization have been trying for many years to define a metric for evaluating the quality of a machinegenerated summary. Most of these attempts involve human interference, which make the process of evaluation expensive and time-consuming. We discuss some important work in the intrinsic category. 2.1 Sentence Precision-Recall Measure Sentence precision and recall have been widely used to evaluate the quality of a summarizer (Jing et al., 1998). Sentence precision measures the percent of the summary that contains sentences matched with the model summary. Recall, on the other hand, measures the percent of sentences in the ideal summary that have been recalled in the summary. Even though sentence precision/recall factors can give us an idea about a summary’s quality, they are not the best metrics to evaluate a system’s quality. This is due to the fact that a small change in the output summary can dramatically affect the quality of a summary (Jing et al., 1998). For example, it is possible that a system will pick a sentence that does n</context>
<context position="22520" citStr="Jing et al., 1998" startWordPosition="3775" endWordPosition="3778">2) argued that only two manually created extracts could not be used to form a sufficient basis for a good benchmark. To explore this issue, we obtained a ranking order for each human judge based on the extracts he/she generated. The results showed that the ranking orders obtained from 9 different judges are actually similar to each other, with the average Spearman correlation efficient to be 0.901. From this point of view, if the ranking orders obtained by sentence precision and recall based on the model extracts could not form a good basis for a benchmark, it is because of its binary nature (Jing et al., 1998), not the lack of sufficient model extracts in DUC 2002 data. Van Halteren and Teufel (2003) proposed to evaluate summaries via factoids, a pseudosemantic representation based on atomic information units. However, sufficient manually created model summaries are need; and factoids are also manually annotated. Donaway et al. (2000) suggested that it might be possible to use contentbased measures for summarization evaluation without generating model summaries. Here, we presented our approach to evaluate the summaries base on document graphs, which is generated automatically. It is not very surpri</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Kathleen McKeown, Regina Barzilay, and Michael Elhadad. 1998. Summarization evaluation methods: experiments and analysis.</rawString>
</citation>
<citation valid="false">
<booktitle>In American Association for Artificial Intelligence Spring Symposium Series,</booktitle>
<pages>60--68</pages>
<marker></marker>
<rawString>In American Association for Artificial Intelligence Spring Symposium Series, pages 60-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen-Yew Lin</author>
</authors>
<date>2001</date>
<note>Summary evaluation environment. http://www.isi.edu/~cyl/SEE.</note>
<marker>Lin, 2001</marker>
<rawString>Chen-Yew Lin. 2001. Summary evaluation environment. http://www.isi.edu/~cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Manual and automatic evaluation of summaries.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization post conference workshop of ACL-02,</booktitle>
<location>Philadelphia, PA, U.S.A.,</location>
<contexts>
<context position="4005" citStr="Lin and Hovy (2002)" startWordPosition="640" endWordPosition="643"> assigned to the system dramatically. It is also obvious that sentence precision/recall is only applicable to the summaries that are generated by sentence extraction, not abstraction (Mani, 2001). 2.2 Content-Based Measure Content-based measure computes the similarity at the vocabulary level (Donaway, 2000 and Mani, 2001). The evaluation is done by creating term frequency vectors for both the summary and the model summary, and measuring the cosine similarity (Salton, 1988) between these two vectors. Of course, the higher the cosine similarity measure, the higher the quality of the summary is. Lin and Hovy (2002) used accumulative n-gram matching scores between model summaries and the summaries to be evaluated as a performance indicator in multi-document summaries. They achieved their best results by giving more credit to longer ngram matches with the use of Porter stemmer. A problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries. Since term frequency is the base to score summaries, it is possible that a high quality summary will get a lower score if the terms used in the summary </context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2002. Manual and automatic evaluation of summaries. In Proceedings of the Workshop on Automatic Summarization post conference workshop of ACL-02, Philadelphia, PA, U.S.A., July 11-12 (DUC 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Summarization evaluation: An overview.</title>
<date>2001</date>
<booktitle>In Proceedings of the NTCIR Workshop 2 Meeting on Evaluation of Chinese and Japanese Text Retrieval and</booktitle>
<institution>Test Summarization, National Institute of Informatics.</institution>
<contexts>
<context position="3581" citStr="Mani, 2001" startWordPosition="575" endWordPosition="577">mary’s quality, they are not the best metrics to evaluate a system’s quality. This is due to the fact that a small change in the output summary can dramatically affect the quality of a summary (Jing et al., 1998). For example, it is possible that a system will pick a sentence that does not match with a model sentence chosen by an assessor, but is equivalent to it in meaning. This, of course, will affect the score assigned to the system dramatically. It is also obvious that sentence precision/recall is only applicable to the summaries that are generated by sentence extraction, not abstraction (Mani, 2001). 2.2 Content-Based Measure Content-based measure computes the similarity at the vocabulary level (Donaway, 2000 and Mani, 2001). The evaluation is done by creating term frequency vectors for both the summary and the model summary, and measuring the cosine similarity (Salton, 1988) between these two vectors. Of course, the higher the cosine similarity measure, the higher the quality of the summary is. Lin and Hovy (2002) used accumulative n-gram matching scores between model summaries and the summaries to be evaluated as a performance indicator in multi-document summaries. They achieved their </context>
<context position="20266" citStr="Mani, 2001" startWordPosition="3412" endWordPosition="3413">xtracts among the 59 submitted summaries also have this problem. The total number of such sentences was 206, and the total number of sentences was 802 sentences. So, about 3.49 sentences do not contain much information, where the average length of each extract is 13.59 sentences. Thus, a large portion of each extract does not contribute to the do example, will not be considered a good summary, either on the criterion of summary coherence or summary informativeness, where coherence is how the summary reads and informativeness is how much information from the source is preserved in the summary (Mani, 2001). From the results based on comparing extracts against original documents, we found that several systems perform very similarly, especially in the experiments with 400-word extracts (Table 4). The results show that except for systems 22 and 28 which perform significantly worse, all other systems are very similar, from the point of view of informativeness. Finally, we generated DGs for the model extracts and then compared them against their original documents. The average F-factors are calculated, which are listed in Table 5 along with the scores for different automatic summarization systems. I</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Inderjeet Mani. 2001. Summarization evaluation: An overview. In Proceedings of the NTCIR Workshop 2 Meeting on Evaluation of Chinese and Japanese Text Retrieval and Test Summarization, National Institute of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark T Maybury</author>
</authors>
<title>Advances in Automatic Text Summarization.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="1855" citStr="Mani and Maybury, 1999" startWordPosition="285" endWordPosition="288">erall quality of a summarization system. In general, there are two types of evaluation categories: intrinsic and extrinsic (Sparck-Jones and Galliers, 1996). Extrinsic approaches measure the quality of a summary based on how it affects certain tasks. In intrinsic approaches, the quality of the summarization is evaluated based on analysis of the content of a summary itself. In both categories human involvement is used to judge the summarization outputs. The problem with having humans involved in evaluating summaries is that we can not hire human judges every time we want to evaluate summaries (Mani and Maybury, 1999). In this paper, we discuss a new automated way to evaluate machinegenerated summaries without the need to have human judges being involved which decreases the cost of determining which summarization system is best. In our experiment, we used data from Document Understanding Conference 2002 (DUC2002). 2 Related Work Researchers in the field of document summarization have been trying for many years to define a metric for evaluating the quality of a machinegenerated summary. Most of these attempts involve human interference, which make the process of evaluation expensive and time-consuming. We d</context>
</contexts>
<marker>Mani, Maybury, 1999</marker>
<rawString>Inderjeet Mani and Mark T. Maybury. 1999. Advances in Automatic Text Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Montes-y-Gómez</author>
<author>Alexander Gelbukh</author>
<author>Aurelio López-López</author>
</authors>
<title>Comparison of conceptual graphs.</title>
<date>2000</date>
<booktitle>In Proceedings of MICAI2000, 1st Mexican International Conference on Artificial Intelligence,</booktitle>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="7624" citStr="Montes-y-Gómez et al. (2000)" startWordPosition="1235" endWordPosition="1238">nce-heuristic are then sensitive to verbs, since the interval between two noun phrases usually contains a verb. For example, from a sentence “Workers at a coal mine went on strike”, we generate a relation “worker related to strike”. Another example, from “The usual cause of heart attacks is a blockage of the coronary arteries”, we generate “heart attack cause related to coronary artery blockage”. Figure 1 shows a example of a partial DG. Figure 1: A partial DG. 2.3.2 Similarity Comparison between two Document Graphs The similarity of DG1 to DG2 is given by the equation: which is modified from Montes-y-Gómez et al. (2000). N is the number of concept/entity nodes in DG1, and M stands for number of relations in DG1; n is the number of matched concept/entity nodes in two DGs, and m is the number of matched relations. We say we find a matched relation in two different DGs, only when both of the two concept/entity nodes linked to the relation node are matched, and the relation node is also matched. Since we might compare two DGs that are significantly different in size (for example, DGs for an extract vs. its source document), we used the number of concept/entity nodes and relation nodes in the target DG as N and M</context>
</contexts>
<marker>Montes-y-Gómez, Gelbukh, López-López, 2000</marker>
<rawString>Manuel Montes-y-Gómez, Alexander Gelbukh, and Aurelio López-López. 2000. Comparison of conceptual graphs. In Proceedings of MICAI2000, 1st Mexican International Conference on Artificial Intelligence, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerome L Myers</author>
<author>Arnold D Well</author>
</authors>
<title>Erlbaum Associates,</title>
<date>1995</date>
<booktitle>Research Design and Statistical Analysis,</booktitle>
<pages>488--490</pages>
<location>Lawrence</location>
<contexts>
<context position="12498" citStr="Myers and Well, 1995" startWordPosition="2088" endWordPosition="2091">ate that for sentence precision and recall, the ranking order for different summarization systems is not affected by the summarization compression ratio. The ranking results for 200-word extracts and 400-word extracts are exactly the same. Since the comparison is between the machine generated extracts and the human created model extracts, we believe that the rankings should represent the performance of 10 different automated summarization systems, to some degree. The experiments using DGs instead of sentence matching give two very similar ranking orders (Spearman rank correlation coefficient [Myers and Well, 1995] is 0.988) where only systems 24 and 19 are reversed in their ranks (Tables 1 and 2). The results show that when the evaluation is based on the comparison between machine generated extracts and the model extracts, our DG-based evaluation approach will provide roughly the same ranking results as the sentence precision and recall approach. Notice that the F-factors obtained by experiments using DGs are higher than those calculated based on sentence matching. This is because our DG-based evaluation approach compares the two extracts at a more fine grained level than sentence matching does since </context>
</contexts>
<marker>Myers, Well, 1995</marker>
<rawString>Jerome L. Myers and Arnold D. Well. 1995. Research Design and Statistical Analysis, pages, 488-490, Lawrence Erlbaum Associates, New Jersey</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
<author>Walter Liggett</author>
</authors>
<title>Introduction to DUC-2002: An intrinsic evaluation of generic news text summarization systems. Document Understanding Conferences website (http://duc.nist.gov/)</title>
<date>2002</date>
<contexts>
<context position="8993" citStr="Over and Liggett, 2002" startWordPosition="1474" endWordPosition="1477">se, the similarity will always be very low. Currently, we weight all the concepts/entities and relations equally. This can be fine tuned in the future. 3 Data, and Experimental Design 3.1 Data Because the data from DUC-2003 were short (~100 words per extract for multi-document task), we chose to use multi-document extracts from DUC-2002 (~200 words and ~400 words per extract for multi-document task) in our experiment. In this corpus, each of ten information analysts from the National Institute of Standards and Technology (NIST) chose one set of newswire/paper articles in the following topics (Over and Liggett, 2002): • A single natural disaster event with documents created within at most a 7-day window • A single event of any type with documents created within at most a 7-day window • Multiple distinct events of the same type (no time limit) • Biographical (discuss a single person) Each assessor chose 2 more sets of articles so that we ended up with a total of 15 document sets of each type. Each set contains about 10 documents. All documents in a set are mainly about a specific “concept.” A total of ten automatic summarizers participated to produce machine-generated summaries. Two extracts of different l</context>
</contexts>
<marker>Over, Liggett, 2002</marker>
<rawString>Paul Over and Walter Liggett. 2002. Introduction to DUC-2002: An intrinsic evaluation of generic news text summarization systems. Document Understanding Conferences website (http://duc.nist.gov/)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: A Method for Automatic Evaluation of Machine Translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Simone Teufel</author>
<author>Horacio Saggion</author>
<author>Wai Lam</author>
<author>John Blitzer</author>
<author>Hong Qi</author>
<author>Arda Celebi</author>
<author>Danyu Liu</author>
<author>Elliott Drabek</author>
</authors>
<title>Evaluation challenges in large-scale document summarization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meetring of the Association for Computational Linguistics,</booktitle>
<pages>375--382</pages>
<contexts>
<context position="23248" citStr="Radev, et al, 2003" startWordPosition="3887" endWordPosition="3890">uate summaries via factoids, a pseudosemantic representation based on atomic information units. However, sufficient manually created model summaries are need; and factoids are also manually annotated. Donaway et al. (2000) suggested that it might be possible to use contentbased measures for summarization evaluation without generating model summaries. Here, we presented our approach to evaluate the summaries base on document graphs, which is generated automatically. It is not very surprising that different measures rank summaries differently. A similar observation has been reported previously (Radev, et al, 2003). Our document graph approach on summarization evaluation is a new automatic way to evaluate machine-generated summaries, which measures the summaries from the point of view of informativeness. It has the potential to evaluate the quality of summaries, including extracts, abstracts, and multi-document summaries, without human involvement. To improve the performance of our system and better represent the content of the summaries and source documents, we are working in several areas: 1) Improve the results of natural language processing to capture information more accurately; 2) Incorporate a kn</context>
</contexts>
<marker>Radev, Teufel, Saggion, Lam, Blitzer, Qi, Celebi, Liu, Drabek, 2003</marker>
<rawString>Dragomir R. Radev, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Hong Qi, Arda Celebi, Danyu Liu, and Elliott Drabek. 2003. Evaluation challenges in large-scale document summarization. In Proceedings of the 41st Annual Meetring of the Association for Computational Linguistics, July 2003, pages 375-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith van Rijsbergen</author>
</authors>
<title>Information Retrieval. Second Edition Butterworths,</title>
<date>1979</date>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>Keith van Rijsbergen. 1979. Information Retrieval. Second Edition Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing.</title>
<date>1988</date>
<publisher>Addison-Wesley Publishing Company.</publisher>
<contexts>
<context position="3863" citStr="Salton, 1988" startWordPosition="618" endWordPosition="619">t does not match with a model sentence chosen by an assessor, but is equivalent to it in meaning. This, of course, will affect the score assigned to the system dramatically. It is also obvious that sentence precision/recall is only applicable to the summaries that are generated by sentence extraction, not abstraction (Mani, 2001). 2.2 Content-Based Measure Content-based measure computes the similarity at the vocabulary level (Donaway, 2000 and Mani, 2001). The evaluation is done by creating term frequency vectors for both the summary and the model summary, and measuring the cosine similarity (Salton, 1988) between these two vectors. Of course, the higher the cosine similarity measure, the higher the quality of the summary is. Lin and Hovy (2002) used accumulative n-gram matching scores between model summaries and the summaries to be evaluated as a performance indicator in multi-document summaries. They achieved their best results by giving more credit to longer ngram matches with the use of Porter stemmer. A problem raised in the evaluation approaches that use the cosine measure is that the summaries may use different key terms than those in the original documents or model summaries. Since term</context>
</contexts>
<marker>Salton, 1988</marker>
<rawString>Gerard Salton. 1988. Automatic Text Processing. Addison-Wesley Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Santos Jr</author>
<author>Hien Nguyen</author>
<author>Scott M Brown</author>
</authors>
<title>Kavanah: An active user interface Information Retrieval Agent Technology.</title>
<date>2001</date>
<pages>412--423</pages>
<location>Maebashi, Japan,</location>
<marker>Jr, Nguyen, Brown, 2001</marker>
<rawString>Eugene Santos Jr., Hien Nguyen, and Scott M. Brown. 2001. Kavanah: An active user interface Information Retrieval Agent Technology. Maebashi, Japan, October 2001, pages 412-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danny Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third International Workshop on Parsing Technologies,</booktitle>
<pages>277--292</pages>
<contexts>
<context position="6339" citStr="Sleator and Temperley, 1993" startWordPosition="1022" endWordPosition="1025">milarity between two summaries or a summary and a document based on the relations (between the keywords). In our approach, each document/summary is represented as a document graph (DG), which is a directed graph of concepts/entities and the relations between them. A DG contains two kinds of nodes, concept/entity nodes and relation nodes. Currently, only two kinds of relations, “isa” and “related to”, are captured (Santos et al, 2001) for simplicity. To generate a DG, a document/summary in plain text format is first tokenized into sentences; and then, each sentence is parsed using Link Parser (Sleator and Temperley, 1993), and the noun phrases (NP) are extracted from the parsing results. The relations are generated based on three heuristic rules: • The NP-heuristic helps to set up the hierarchical relations. For example, from a noun phrase “folk hero stature”, we generate relations “folk hero stature isa stature”, “folk hero stature related to folk hero”, and “folk hero isa hero”. • The NP-PP-heuristic attaches all prepositional phrases to adjacent noun phrases. For example, from “workers at a coal mine”, we generate a relation, “worker related to coal mine”. • The sentence-heuristic relates concepts/entities </context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Danny Sleator and Davy Temperley. 1993. Parsing English with a link grammar. In Proceedings of the Third International Workshop on Parsing Technologies, pages 277-292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck-Jones</author>
<author>Julia R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review (Lecture Notes in Artificial Intelligence 1083).</title>
<date>1996</date>
<publisher>Springer-Verlag</publisher>
<contexts>
<context position="1388" citStr="Sparck-Jones and Galliers, 1996" startWordPosition="206" endWordPosition="209">the concepts/entities and relations between them in the text. 1 Introduction Document summarization has been the focus of many researchers for the last decade, due to the increase in on-line information and the need to find the most important information in a (set of) document(s). One of the biggest challenges in text summarization research is how to evaluate the quality of a summary or the performance of a summarization tool. There are different approaches to evaluate overall quality of a summarization system. In general, there are two types of evaluation categories: intrinsic and extrinsic (Sparck-Jones and Galliers, 1996). Extrinsic approaches measure the quality of a summary based on how it affects certain tasks. In intrinsic approaches, the quality of the summarization is evaluated based on analysis of the content of a summary itself. In both categories human involvement is used to judge the summarization outputs. The problem with having humans involved in evaluating summaries is that we can not hire human judges every time we want to evaluate summaries (Mani and Maybury, 1999). In this paper, we discuss a new automated way to evaluate machinegenerated summaries without the need to have human judges being in</context>
</contexts>
<marker>Sparck-Jones, Galliers, 1996</marker>
<rawString>Karen Sparck-Jones and Julia R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review (Lecture Notes in Artificial Intelligence 1083). Springer-Verlag</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>