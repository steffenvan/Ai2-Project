<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000604">
<title confidence="0.9979555">
Retrieval of Research-level Mathematical Information Needs:
A Test Collection and Technical Terminology Experiment
</title>
<author confidence="0.977155">
Yiannos A. Stathopoulos Simone Tuefel
</author>
<affiliation confidence="0.996384">
Computer Laboratory, University of Cambridge
</affiliation>
<address confidence="0.949568">
15 JJ Thomson Avenue, Cambridge, UK
</address>
<email confidence="0.999651">
{yiannos.stathopoulos,simone.teufel}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913">
In this paper, we present a test col-
lection for mathematical information re-
trieval composed of real-life, research-
level mathematical information needs.
Topics and relevance judgements have
been procured from the on-line collabo-
ration website MathOverflow by delegat-
ing domain-specific decisions to experts
on-line. With our test collection, we con-
struct a baseline using Lucene’s vector-
space model implementation and conduct
an experiment to investigate how prior ex-
traction of technical terms from mathe-
matical text can affect retrieval efficiency.
We show that by boosting the impor-
tance of technical terms, statistically sig-
nificant improvements in retrieval perfor-
mance can be obtained over the baseline.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998138588235294">
Since their introduction through the Cranfield ex-
periments (Cleverdon, 1960; Cleverdon, 1962;
Cleverdon et al., 1966a; Cleverdon et al., 1966b),
test collections have become the foundation of in-
formation retrieval (IR) evaluation.
Recent interest in Mathematical information re-
trieval (MIR) has prompted the construction of
the NTCIR Math IR test collection (Aizawa et
al., 2013). Like many general-purpose, domain-
specific IR test collections, the NTCIR collection
is composed of broad queries intended to test sys-
tems over a wide spectrum of query complexity.
In this paper we present a test collection
composed of real-life, research-level mathemat-
ical topics and associated relevance judgements
procured from the online collaboration web-site
MathOverflow1. The resulting test collection con-
</bodyText>
<footnote confidence="0.950666">
1http://mathoverflow.net/
</footnote>
<bodyText confidence="0.958856945945946">
tains 160 atomic questions - material derived from
120 MathOverflow discussion threads.
Topics in our test collection capture specialised
information needs that are complex to resolve and
often demand collective effort from multiple do-
main experts. For example2:
The ”most symmetric” Mukai-Umemura 3-fold
with automorphism group PGL(2, C) admits
a Kaehler-Einstein metric according to Donald-
son’s result. On the contrary, there are some arbi-
trarily small complex deformations of the above
3-fold which do not admit Kaehler-Einstein met-
rics, as shown by Tian. All examples considered
by Tian seem to have no symmetries at all. Is it
possible to find similarly arbitrarily small com-
plex deformations with C*-action and which do
not admit any Kaehler-Einstein metric?
Due to their specialised nature, our topics have
a relatively small number of relevant documents.
Fortunately, there is precedent of this from IR
tasks such as QA (Ishikawa et al., 2010) and
known-item search (Craswell et al., 2003).
With our test collection, we construct a baseline
using Lucene’s default implementation of the vec-
tor space model (VSM). Additionally, we conduct
an experiment designed to investigate the hypoth-
esis that technical terms in mathematics have ele-
vated retrieval significance.
Information in mathematics is communicated
by defining, manipulating and otherwise operat-
ing on mathematical structures and objects which
can be instantiated in the mathematical discourse.
In this sense, technical terminology in mathemat-
ics has an elevated role. This hypothesis stems
from the observation that the mathematical dis-
course is dense with named mathematical objects,
structures, properties and results.
</bodyText>
<footnote confidence="0.9987195">
2Adapted from MathOverflow post 68096, http://
mathoverflow.net/questions/68096/
</footnote>
<page confidence="0.827916">
334
</page>
<bodyText confidence="0.862395363636364">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 334–340,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
In the next section, we present our test collec-
tion and discuss the procedure for its construction
from crowd-sourced expertise on MathOverflow.
In section 3, we discuss related material in the lit-
erature and compare it to our work. Our experi-
mental setup and results are discussed in section
4, with a brief summary of our work presented in
section 5.
</bodyText>
<sectionHeader confidence="0.981522" genericHeader="method">
2 The Test Collection
</sectionHeader>
<bodyText confidence="0.99993512195122">
The main motivation behind this work comes
from our long-term goal to develop and evalu-
ate MIR models intended to satisfy research-level
mathematical information needs. Evaluation is an
important final step in the development of IR mod-
els and is preconditioned on the availability of a
test collection.
A test collection is a resource composed of (1)
a document collection (or corpus) with uniquely
identifiable documents (e.g., scientific papers,
news articles), (2) a set of topics from which
search queries can be produced and (3) a set of
relevance judgements: pairs connecting individual
topics to documents (in the corpus) known to sat-
isfy the corresponding information need.
General-purpose MIR test collections, such as
the one produced for NTCIR-10 (Aizawa et al.,
2013), are expected to contain both broad and nar-
row topics capturing a wide range of retrieval com-
plexity. In contrast, we require a collection of
topics characterised by a higher lower bound on
topic complexity with individual topics capturing
highly-specialised, real-world information needs.
Unfortunately, research-level mathematical in-
formation needs are hard to source from docu-
ments in a way that would not render them arti-
ficial. Furthermore, manual construction of topics
and relevance judgements is unrealistic due to the
large number of experts required to cover the var-
ious specialised sub-fields of mathematics. This,
coupled with limited access to numerous MIR sys-
tems, makes TREC-like pooling (Harman, 1993;
Voorhees and Harman, 2005) impractical.
We propose that topics and relevance judge-
ments be procured from the on-line collaboration
website MathOverflow (MO), an online QA site
for research mathematicians. A user (informa-
tion seeker) can post a question on the site, usu-
ally relating to a small niche field in mathemat-
ics. Colleagues can either post a candidate answer,
comment on the question, comment on and/or up-
</bodyText>
<table confidence="0.734591363636364">
Prelude 1) Apparently, physicist can calculate the GW
invariants of quintic CY 3-fold up to genus 51.
2) For each genus g, there is a lower
bound d(g) such that for every d &lt; d(g),
all genus g degree d
invariants of quintic are zero.
MT-1 I am looking for a reference that has a table
of these number for some low degrees
(say up to degree 5) and low genera
(at least until g = 3).
MT-2 Where can I found this lower bound?
</table>
<tableCaption confidence="0.996912">
Table 1: MO post 14655, prelude and micro-topics
</tableCaption>
<bodyText confidence="0.999624897435897">
vote existing answers. Ultimately, the information
seeker decides which answer satisfies the underly-
ing information need by marking it as “accepted”.
Material on MO is closely aligned with our re-
quirements. Specifically, Tausczik et al. (2014)
and Martin and Pease (2013) agree that MO ques-
tions (information needs) arise from doing mathe-
matics research and are novel to the mathemati-
cian involved. The authors conclude that, hav-
ing been produced by experts, MO answers are
authoritative and partially credit the website’s re-
ward system for their strong reliability.
MO questions often have multiple sub-parts,
which we refer to as micro-topics since they en-
code atomic information needs. Furthermore, in-
formation in MO questions is carried by two types
of sentences: prelude sentences, which are used to
set the mathematical context (introduce mathemat-
ical constructs and results) and query sentences,
which transcribe the information need itself and
are semantically bound to the accompanying pre-
lude.
As the underlying document collection, we have
used the Mathematical Retrieval Corpus (MREC)3
(L´ıˇska et al., 2011), which contains more than
439,000 mathematical publications, complete with
mathematical formulae converted to machine-
readable MathML. Similarly, we have made math-
ematical expressions in our topics accessible to
MIR systems by converting all LATEX embedded in
MO questions into MathML using the LaTeXML
tool-kit.
For the purpose of constructing our test collec-
tion we have adopted a multi-step process. All
steps in the process are systematically applicable
regardless of the subject material of the topic be-
ing considered for inclusion. As such, our test
collection can be as diverse, in terms of mathe-
matical subject and sub-fields, as MathOverflow.
</bodyText>
<footnote confidence="0.962985">
3version 2011.4.439
</footnote>
<page confidence="0.998274">
335
</page>
<bodyText confidence="0.97598846">
Decisions relating to relevance of material to a
given topic (MO question) are delegated to experts
on the website. However, the information seeker
(MO user posting the question) remains the ulti-
mate judge of relevance. This authority is typi-
cally exercised by either accepting an answer di-
rectly or, by explicitly commenting on the rele-
vance of posted material.
In the first step, all MO discussion threads4 with
at least one citation to the MREC in their accepted
answer were collected. Each identified thread was
examined by one of the authors for conformance to
two ideal-standard criteria: (1) Useful MO ques-
tions should not be too broad or vague but rather
express an information need that is clear and can
be satisfied by describing objects or properties,
stating conditions and/or producing examples or
counter-examples. (2) MREC documents cited in
MO accepted answers should address all sub-parts
of the question in a manner that requires minimal
deduction and do not synthesise mathematical re-
sults from multiple resources.
Subsequently, relevance of documents for each
micro-topic is decided using two criteria: totality
and directness. A cited resource is total if it con-
tains all necessary information to derive the an-
swer for the micro-topic and partial if it only ad-
dresses a special case. A cited resource is also said
to be direct, if the answer can be derived with lit-
tle intellectual effort from its text, or indirect if
the same information requires considerable effort
(such as mathematical deduction or reasoning) for
the information seeker to reproduce.
Making these determinations involves matching
the language of arguments and the symbolic con-
text of the answer to the cited resource. As part
of this step, we also examine the post-answer (PA)
comments for expressions of confirmation of the
usefulness of a cited resource from the informa-
tion seeker.
The completed test collection contains 160
micro-topics with 184 associated relevance judge-
ments (involving 224 unique MREC documents)
organised in 120 topics. Topic text in our test
collection is sentence tokenised, with relevance
judgements being represented conceptually as tu-
ples of the form:
(Topic ID , sentenceID , Micro−topic ID ,
relevant MREC document ID )
From Table 2 we observe that the vast majority of
</bodyText>
<footnote confidence="0.825022">
4from MathOverflow data-dump of 20/01/2014
</footnote>
<table confidence="0.99841">
Micro-topics 1 2 3 ≥ 4
Instances (topics) 88 24 8 0
Percentage 73.33% 20.00 % 6.67% 0%
</table>
<tableCaption confidence="0.998753">
Table 2: Topic/Micro-topic break-down
</tableCaption>
<bodyText confidence="0.992888090909091">
topics (93.33%) have either 1 or 2 micro-topics,
with the average being close to 1 (1.33). The ma-
jority of topics (97,80.83%) have only one rele-
vant document while a further 21 (17.5%) have
two relevant documents. Two topics have more
than 2 relevant documents: one with 3 and an-
other with 4. In terms of micro-topics, this cor-
responds to 140 micro-topics (87.50%) with 1 rel-
evant document, 17 (10.625%) with 2, 2 micro-
topics (1.25%) with 3 and just one (0.625%) with
4 relevant documents.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="method">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999574303030303">
Test collections over scientific publications
were first introduced for the Cranfield experiments
(Cleverdon, 1960; Cleverdon, 1962; Cleverdon et
al., 1966a; Cleverdon et al., 1966b). Despite crit-
icism for sourcing queries from collection docu-
ments, the Cranfield experiments highlighted the
importance of jointly reporting recall and preci-
sion, pioneered the practice of using authors and
citations for augmenting relevance judgements
and established the test collection paradigm.
Expert citations have already been exploited for
procuring relevance judgements. For example,
Ritchie et al. (2006) elicited relevance judgements
for citations in papers accepted in a scientific con-
ference from their authors and used these judge-
ments as part of their test collection of scientific
publications.
In terms of domain, our work is related to the
NTCIR-10 Math IR test collection (Aizawa et al.,
2013). Furthermore, the topics in our collec-
tion are analogous to those in the NTCIR full-text
search, in the sense that they take the form of co-
herent text interspersed with mathematical expres-
sions. Rather than being focused on accommodat-
ing information needs of varying complexity, how-
ever, our test collection has been designed to facil-
itate retrieval of highly specialised, mathematical
information needs of uniformly high complexity.
Similar use of crowd-sourced expertise has been
proposed in the context of QA. For example, Gy-
ongyi et al. (2008), examined 10 months-worth of
“Yahoo! Answers” material as part of an investi-
gation of QA data, which was later used for the
</bodyText>
<page confidence="0.995686">
336
</page>
<bodyText confidence="0.993093833333333">
NTCIR-8 Community QA pilot task (Ishikawa et
al., 2010; Sakai et al., 2011). Characterisation of
crowd-sourced answers in terms of totality (sec-
tion 2) has also been considered in the context of
QA. In particular, Sakai et al. (2011) describe a
relevance grading scheme of crowd-sourced an-
swers based on the total/partial/irrelevant scale,
but highlight that answers on “Yahoo! Answers”
vary in quality (e.g., due to instances of bias or
obscenity).
Finally, the idea of sourcing relevance judge-
ments from expert citations is an established prac-
tice in IR. In the context of patent search, for ex-
ample, Graf and Azzopardi (2008) utilised cita-
tions in patent office expert reports as relevance
judgements, while Fujii et al. (2006) automatically
extracted patent office expert citations used to re-
ject patent applications.
</bodyText>
<sectionHeader confidence="0.999885" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999828285714286">
In this section we conduct an experiment to
demonstrate the usefulness of our test collection
by investigating the impact of terminology boost-
ing on MIR effectiveness. An important assump-
tion of this experiment is that the retrieval of each
micro-topic is dependent only on the attached pre-
lude.
</bodyText>
<subsectionHeader confidence="0.97747">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999994916666667">
We first produced a Lucene index over all docu-
ments in the MREC. In order to normalise process-
ing of XHTML+MathML, topics and MREC doc-
uments were passed through the Tika framework5.
Lucene’s StandardAnalyzer was modified to
preserve stop-words since frequent words such as
the preposition “of” can be important parts of
technical terms (e.g., “set of vectors”). The ana-
lyzer was also modified to preserve dashes, which
are common in technical terms (e.g., “Calabi-Yau
manifold”). This analyzer is used during both in-
dexing and query processing for consistency.
</bodyText>
<subsectionHeader confidence="0.999103">
4.2 Building Queries
</subsectionHeader>
<bodyText confidence="0.999707857142857">
For each micro-topic in a given topic, we emit a
query string by concatenating all sentences in the
prelude with sentences associated with the micro-
topic. For example, query string for micro-topic
MT-1 in Table 1 is generated by concatenating its
text with that of the prelude. Using this strat-
egy, consistency with the assumption outlined at
</bodyText>
<footnote confidence="0.843483">
5https://tika.apache.org/
</footnote>
<bodyText confidence="0.999679">
the beginning of the section is achieved since no
overlap beyond the prelude is introduced between
queries generated for micro-topics attached to a
given topic.
</bodyText>
<subsectionHeader confidence="0.997649">
4.3 Systems
</subsectionHeader>
<bodyText confidence="0.999971">
Using Lucene as the indexing and searching back-
end, we compare the performance of two retrieval
methods. Underpinning both methods is Lucene’s
default similarity (project, 2013), which is based
on cosine similarity:
</bodyText>
<equation confidence="0.899483">
V (q).V (d)
sim(q, d) =
</equation>
<bodyText confidence="0.9911315">
where V (q) and V (d) are weighted vectors for the
query and candidate document respectively. As a
performance measure, we use mean average pre-
cision (MAP):
</bodyText>
<equation confidence="0.9992932">
1 1
|Q |m
MAP(Q) = E E Precision(Rik)
|Q |mi
i=1 k=1
</equation>
<subsectionHeader confidence="0.444053">
4.3.1 Baseline
</subsectionHeader>
<bodyText confidence="0.999856666666667">
Lucene’s VSM implementation with default TF-
IDF weighting and scoring is used as the baseline.
This is intended to emulate ageneral-purpose in-
formation retrieval scenario, which is the motiva-
tion behind the design of Lucene’s default config-
uration.
</bodyText>
<subsectionHeader confidence="0.605358">
4.3.2 Boosted Technical Terms
</subsectionHeader>
<bodyText confidence="0.980866272727273">
The alternative model is designed to give more
weight to technical terminology common to both
documents and queries. In order to construct this
model, all technical terms are extracted from the
document collection using an implementation of
the C-Value multi-word technical term extraction
method (Frantzi et al., 1996; Frantzi et al., 1998).
Given an input corpus, the C-Value method ex-
tracts multi-word terms by making use of a lin-
guistic and a statistical component.
The linguistic component is responsible for
eliminating multi-term strings that are unlikely to
be technical terms through the use of a stop-list
(composed of high-frequency corpus terms) and
linguistic filters (regular expressions) applied on
sequences of part-of-speech tags. The statistical
component assigns a
score to a candi-
date term sequence based on corpus-wide statisti-
cal characteristics of the sequence itself an
“termhood”
d those
</bodyText>
<equation confidence="0.72146">
of term sequences that contain it. The output of
|V (q)||V (d)|
</equation>
<page confidence="0.976884">
337
</page>
<table confidence="0.997013666666667">
class/Form 1 Form 2 ... Form 8 C-Value
riemannian Riemannian RIEMANNIAN 13236.6
manifold manifold MANIFOLDS
</table>
<tableCaption confidence="0.984737">
Table 3: C-Value technical-term list entry
</tableCaption>
<table confidence="0.940202636363636">
Original Text
a Riemannian manifold is a smooth manifold
Original Term vector
(a,2), (Riemannian,1),(manifold,2),(is,1),(smooth,1)
Technical terms
Riemannian manifold, smooth manifold
Re-Attributed Term Vector 1)
(a,2), (Riemannian manifold,1),(is,1),(smooth manifold,
Re-generated delta index text
a a a a Riemannian manifold Riemannian manifold
is is smooth manifold smooth manifold
</table>
<tableCaption confidence="0.982866">
Table 4: Example of re-attribution and delta index
</tableCaption>
<bodyText confidence="0.999354628571429">
the algorithm is a list of candidate technical terms
in the corpus, ordered by their C-Value termhood
score.
As shown in Table 3, each entry in the resulting
list represents a single technical term (the class)
and enumerates all forms of the candidate term
as observed in the input corpus. In total, 3 mil-
lion classes of technical terms have been detected
in the MREC. Using Lucene’s positional index-
ing mechanism, we retrieved the position of each
technical term (all forms), recorded its term fre-
quency (TF) and produced a new technical term
index. This technical term index contains 426 mil-
lion tuple entries of the form
&lt;class , form, MREC docid , TF , position
−of−occurrence list&gt;
The same re-indexing process is repeated for the
queries and the result is stored in a separate query
table (10,433 entries).
Subsequently, the indexed document and query
term vectors were modified by (1) adding new to-
kens to represent technical term phrases and (2) re-
attributing the TF of component terms to the term
vector of the phrase.
Finally, the text for each MREC document and
query is re-generated from the term vectors and
stored in a “delta index”. At this stage, the num-
ber of technical term instances emitted is twice
that recorded by the original term vector. This has
the effect of boosting the significance of technical
terms and phrases. An example of the application
of this process, from original text to delta index
generation is presented in Table 4. Rankings for
the alternative model can be obtained by search-
ing the delta index using the re-generated query.
</bodyText>
<tableCaption confidence="0.588756">
Table 5: Difference in MAP performance between
models (* statistically significant at α = 0.05)
</tableCaption>
<bodyText confidence="0.99675475">
Although the choice of boosting factor 2 is arbi-
trary, our intention is to demonstrate the presence
of a difference in retrieval efficiency, rather than
optimising the effect of boosting.
</bodyText>
<subsectionHeader confidence="0.874437">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999991">
The MAP scores obtained for the models are pre-
sented in Table 5. We observe that the difference
in MAP is in favour of the alternative model. This
difference is statistically significant at α = 0.05
using the Wilcoxon signed-rank test (p &lt; 0.05).
Therefore, we have sufficient evidence to conclude
that, in the context of the VSM, boosting techni-
cal terms improves retrieval efficiency of research
mathematics.
When compared to MAP scores produced by
the same systems in more traditional IR tasks, the
scores in Table 5 may seem poor. We attribute this
phenomenon to the fact that sense in written math-
ematics is communicated via a complex interac-
tion of text and mathematical expressions and is
thus hard to extract using shallow methods.
</bodyText>
<sectionHeader confidence="0.985975" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999715590909091">
We have constructed a Math IR test collection for
real-life, research-level mathematical information
needs. As part of the work of constructing our test
collection, we have developed a methodology for
compiling domain-specific test collections that re-
quires minimal expertise in the domain itself.
Using 160 micro-topics in our test collection,
we have shown experimentally that the perfor-
mance of VSM-based retrieval models with re-
search mathematics can be improved by boosting
the importance of technical terminology. Further-
more, our experimental work suggests that our test
collection can be used to identify statistically sig-
nificant differences between MIR systems. It is
our intention to make our collection available to
the IR community.
As part of on-going and future work, we will be
incorporating additional retrieval models, such as
the Okapi BM25, in our evaluation framework. In
addition, we are looking into investigating the sta-
tistical properties of our test collection along the
lines of Harman (2011) and Soboroff et al. (2001).
</bodyText>
<figure confidence="0.987903857142857">
Baseline
Tech-Term boosting
Difference
MAP
0.0602
0.0732
0.013* (17.7%)
</figure>
<page confidence="0.990603">
338
</page>
<sectionHeader confidence="0.994164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991549336448598">
Akiko Aizawa, Michael Kohlhase, and Iadh Ounis.
2013. Ntcir-10 math pilot task overview. In Pro-
ceedings of the 10th NTCIR Conference, June.
C.W. Cleverdon, J. Mills, M. Keen, and Aslib. Cran-
field Research Project. 1966a. Factors Determining
the Performance of Indexing Systems, Vol. 1: De-
sign. Number v. 1 in Factors Determining the Per-
formance of Indexing Systems. College of Aeronau-
tics.
C.W. Cleverdon, J. Mills, M. Keen, and Aslib. Cran-
field Research Project. 1966b. Factors determining
the performance of indexing systems, Vol 2: Test Re-
sults. Number v. 2 in Factors Determining the Per-
formance of Indexing Systems. College of Aeronau-
tics.
C. W. Cleverdon. 1960. Report on the first stage of
an investigation into the comparative efficiency of
indexing systems. Technical report.
C. W. Cleverdon. 1962. Aslib cranfield research
project: Report on the testing and analysis of an in-
vestigation into the comparative efficiency of index-
ing systems. Technical report, October.
Cyril W. Cleverdon. 1991. The significance of the
cranfield tests on index languages. In Proceedings
of the 14th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, SIGIR ’91, pages 3–12, New York,
NY, USA. ACM.
Nick Craswell, David Hawking, Ross Wilkinson, and
Mingfang Wu. 2003. Overview of the trec-2003
web track. In Proceedings of TREC-2003, Gaithers-
burg, Maryland USA, November.
K. Frantzi, S. Ananiadou, and J. Tsujii. 1996. Ex-
tracting terminological expressions. In The Special
Interest Group Notes of Information Processing So-
ciety of Japan, IPSJ SIG Notes, number 112 in Nat-
ural Language, page 8388.
Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi
Tsujii. 1998. The c-value/nc-value method of au-
tomatic recognition for multi-word terms. In Pro-
ceedings of the Second European Conference on
Research and Advanced Technology for Digital Li-
braries, ECDL ’98, pages 585–604, London, UK,
UK. Springer-Verlag.
Atsushi Fujii, Makoto Iwayama, and Noriko K. 2006.
Test collections for patent retrieval and patent clas-
sification in the fifth ntcir workshop.
E. Graf and L. Azzopardi. 2008. A methodology for
building a patent test collection for prior art search.
Zoltan Gyongyi, Georgia Koutrika, Jan Pedersen, and
Hector Garcia-Molina. 2008. Questioning yahoo!
answers.
Donna Harman. 1993. Overview of the first trec con-
ference. In Proceedings of the 16th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’93,
pages 36–47, New York, NY, USA. ACM.
Donna Harman. 2011. Information Retrieval Evalua-
tion. Synthesis Lectures on Information Concepts,
Retrieval, and Services. Morgan &amp; Claypool Pub-
lishers.
Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando.
2010. Overview of the ntcir-8 community qa pilot
task (part i): The test collection and the task.
Makoto Iwayama, Atsushi Fujii, Noriko Kando, and
Akihiko Takano. 2003. Overview of patent retrieval
task at ntcir-3. In Proceedings of the ACL-2003
Workshop on Patent Corpus Processing - Volume 20,
PATENT ’03, pages 24–32, Stroudsburg, PA, USA.
Association for Computational Linguistics.
K.S. Jones. 1981. Information retrieval experiment.
Butterworths.
Martin L´ıˇska, Petr Sojka, Michal R˚uˇziˇcka, and Petr
Mravec. 2011. Web interface and collection for
mathematical retrieval: Webmias and mrec. In Petr
Sojka and Thierry Bouche, editors, Towards a Dig-
ital Mathematics Library., pages 77–84, Bertinoro,
Italy, Jul. Masaryk University.
Ursula Martin and Alison Pease. 2013. What does
mathoverflow tell us about the production of mathe-
matics? CoRR, abs/1305.0904.
Lucene project. 2013. Lucene’s tf-idf similarity func-
tion.
Anna Ritchie, Simone Teufel, and Stephen Robertson.
2006. Creating a test collection for citation-based
ir experiments. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main
Conference, pages 391–398, New York City, USA,
June. Association for Computational Linguistics.
Tetsuya Sakai, Daisuke Ishikawa, Noriko Kando,
Yohei Seki, Kazuko Kuriyama, and Chin-Yew Lin.
2011. Using graded-relevance metrics for evaluat-
ing community qa answer selection. In Proceedings
of the Fourth ACM International Conference on Web
Search and Data Mining, WSDM ’11, pages 187–
196, New York, NY, USA. ACM.
Ian Soboroff, Charles Nicholas, and Patrick Cahan.
2001. Ranking retrieval systems without relevance
judgments. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’01,
pages 66–73, New York, NY, USA. ACM.
K. Sprck Jones and C.J. Van Rijsbergen. 1975. Re-
port on the need for and provision of an ’ideal’ in-
formation retrieval test collection. Technical report,
British Library Research and Development Report
5266, University Computer Laboratory, Cambridge.
</reference>
<page confidence="0.992803">
339
</page>
<reference confidence="0.999323239130435">
K. Sprck Jones and C.J. Van Rijsbergen. 1976. Infor-
mation retrieval test collections. Journal of Docu-
mentation, 32(1):59–75.
Heinrich Stamerjohanns and Michael Kohlhase. 2008.
Transforming the ariv to xml. In Serge Autex-
ier, John Campbell, Julio Rubio, Volker Sorge,
Masakazu Suzuki, and Freek Wiedijk, editors, In-
telligent Computer Mathematics, volume 5144 of
Lecture Notes in Computer Science, pages 574–582.
Springer Berlin Heidelberg.
Heinrich Stamerjohanns, Michael Kohlhase, Deyan
Ginev, Catalin David, and Bruce R. Miller. 2010.
Transforming large collections of scientific publica-
tions to xml. Mathematics in Computer Science,
3(3):299–307.
Yla R. Tausczik and James W. Pennebaker. 2011. Pre-
dicting the perceived quality of online mathematics
contributions from users’ reputations. In Proceed-
ings of the SIGCHI Conference on Human Factors
in Computing Systems, CHI ’11, pages 1885–1888,
New York, NY, USA. ACM.
Yla R. Tausczik and James W. Pennebaker. 2012. Par-
ticipation in an online mathematics community: Dif-
ferentiating motivations to add. In Proceedings of
the ACM 2012 Conference on Computer Supported
Cooperative Work, CSCW ’12, pages 207–216, New
York, NY, USA. ACM.
Yla R. Tausczik, Aniket Kittur, and Robert E. Kraut.
2014. Collaborative problem solving: A study of
mathoverflow. In Proceedings of the 17th ACM
Conference on Computer Supported Cooperative
Work &amp; Social Computing, CSCW ’14, pages
355–367, New York, NY, USA. ACM.
Ellen M. Voorhees and Donna K. Harman. 2005.
TREC: Experiment and Evaluation in Information
Retrieval (Digital Libraries and Electronic Publish-
ing). The MIT Press.
Ellen M. Voorhees. 1998. Variations in relevance
judgments and the measurement of retrieval effec-
tiveness. In Proceedings of the 21st Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’98,
pages 315–323, New York, NY, USA. ACM.
Ellen M. Voorhees. 2004. Overview of the trec 2001
question answering track. In Proceedings of the
Thirteenth Text REtreival Conference (TREC 2004).
</reference>
<page confidence="0.998229">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962211">
<title confidence="0.998878">Retrieval of Research-level Mathematical Information A Test Collection and Technical Terminology Experiment</title>
<author confidence="0.998268">Yiannos A Stathopoulos Simone Tuefel</author>
<affiliation confidence="0.996141">Computer Laboratory, University of</affiliation>
<address confidence="0.976678">15 JJ Thomson Avenue, Cambridge, UK</address>
<abstract confidence="0.999439947368421">In this paper, we present a test collection for mathematical information retrieval composed of real-life, researchlevel mathematical information needs. Topics and relevance judgements have been procured from the on-line collaboration website MathOverflow by delegating domain-specific decisions to experts on-line. With our test collection, we construct a baseline using Lucene’s vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Akiko Aizawa</author>
<author>Michael Kohlhase</author>
<author>Iadh Ounis</author>
</authors>
<title>Ntcir-10 math pilot task overview.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th NTCIR Conference,</booktitle>
<contexts>
<context position="1396" citStr="Aizawa et al., 2013" startWordPosition="189" endWordPosition="192">ical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline. 1 Introduction Since their introduction through the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b), test collections have become the foundation of information retrieval (IR) evaluation. Recent interest in Mathematical information retrieval (MIR) has prompted the construction of the NTCIR Math IR test collection (Aizawa et al., 2013). Like many general-purpose, domainspecific IR test collections, the NTCIR collection is composed of broad queries intended to test systems over a wide spectrum of query complexity. In this paper we present a test collection composed of real-life, research-level mathematical topics and associated relevance judgements procured from the online collaboration web-site MathOverflow1. The resulting test collection con1http://mathoverflow.net/ tains 160 atomic questions - material derived from 120 MathOverflow discussion threads. Topics in our test collection capture specialised information needs tha</context>
<context position="5040" citStr="Aizawa et al., 2013" startWordPosition="732" endWordPosition="735">on needs. Evaluation is an important final step in the development of IR models and is preconditioned on the availability of a test collection. A test collection is a resource composed of (1) a document collection (or corpus) with uniquely identifiable documents (e.g., scientific papers, news articles), (2) a set of topics from which search queries can be produced and (3) a set of relevance judgements: pairs connecting individual topics to documents (in the corpus) known to satisfy the corresponding information need. General-purpose MIR test collections, such as the one produced for NTCIR-10 (Aizawa et al., 2013), are expected to contain both broad and narrow topics capturing a wide range of retrieval complexity. In contrast, we require a collection of topics characterised by a higher lower bound on topic complexity with individual topics capturing highly-specialised, real-world information needs. Unfortunately, research-level mathematical information needs are hard to source from documents in a way that would not render them artificial. Furthermore, manual construction of topics and relevance judgements is unrealistic due to the large number of experts required to cover the various specialised sub-fi</context>
<context position="12304" citStr="Aizawa et al., 2013" startWordPosition="1890" endWordPosition="1893"> highlighted the importance of jointly reporting recall and precision, pioneered the practice of using authors and citations for augmenting relevance judgements and established the test collection paradigm. Expert citations have already been exploited for procuring relevance judgements. For example, Ritchie et al. (2006) elicited relevance judgements for citations in papers accepted in a scientific conference from their authors and used these judgements as part of their test collection of scientific publications. In terms of domain, our work is related to the NTCIR-10 Math IR test collection (Aizawa et al., 2013). Furthermore, the topics in our collection are analogous to those in the NTCIR full-text search, in the sense that they take the form of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test collection has been designed to facilitate retrieval of highly specialised, mathematical information needs of uniformly high complexity. Similar use of crowd-sourced expertise has been proposed in the context of QA. For example, Gyongyi et al. (2008), examined 10 months-worth of “Yahoo! Answers” mater</context>
</contexts>
<marker>Aizawa, Kohlhase, Ounis, 2013</marker>
<rawString>Akiko Aizawa, Michael Kohlhase, and Iadh Ounis. 2013. Ntcir-10 math pilot task overview. In Proceedings of the 10th NTCIR Conference, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cranfield Research Project</author>
</authors>
<title>Factors Determining the Performance of Indexing Systems,</title>
<date>1966</date>
<volume>1</volume>
<marker>Project, 1966</marker>
<rawString>C.W. Cleverdon, J. Mills, M. Keen, and Aslib. Cranfield Research Project. 1966a. Factors Determining the Performance of Indexing Systems, Vol. 1: Design. Number v. 1 in Factors Determining the Performance of Indexing Systems. College of Aeronautics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cranfield Research Project</author>
</authors>
<title>Factors determining the performance of indexing systems, Vol 2: Test Results. Number v. 2 in Factors Determining the Performance of Indexing Systems. College of Aeronautics.</title>
<date>1966</date>
<marker>Project, 1966</marker>
<rawString>C.W. Cleverdon, J. Mills, M. Keen, and Aslib. Cranfield Research Project. 1966b. Factors determining the performance of indexing systems, Vol 2: Test Results. Number v. 2 in Factors Determining the Performance of Indexing Systems. College of Aeronautics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Cleverdon</author>
</authors>
<title>Report on the first stage of an investigation into the comparative efficiency of indexing systems.</title>
<date>1960</date>
<tech>Technical report.</tech>
<contexts>
<context position="1092" citStr="Cleverdon, 1960" startWordPosition="146" endWordPosition="147">have been procured from the on-line collaboration website MathOverflow by delegating domain-specific decisions to experts on-line. With our test collection, we construct a baseline using Lucene’s vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline. 1 Introduction Since their introduction through the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b), test collections have become the foundation of information retrieval (IR) evaluation. Recent interest in Mathematical information retrieval (MIR) has prompted the construction of the NTCIR Math IR test collection (Aizawa et al., 2013). Like many general-purpose, domainspecific IR test collections, the NTCIR collection is composed of broad queries intended to test systems over a wide spectrum of query complexity. In this paper we present a test collection composed of real-life, research-level mathematical topics and associate</context>
<context position="11523" citStr="Cleverdon, 1960" startWordPosition="1776" endWordPosition="1777">cs (93.33%) have either 1 or 2 micro-topics, with the average being close to 1 (1.33). The majority of topics (97,80.83%) have only one relevant document while a further 21 (17.5%) have two relevant documents. Two topics have more than 2 relevant documents: one with 3 and another with 4. In terms of micro-topics, this corresponds to 140 micro-topics (87.50%) with 1 relevant document, 17 (10.625%) with 2, 2 microtopics (1.25%) with 3 and just one (0.625%) with 4 relevant documents. 3 Related Work Test collections over scientific publications were first introduced for the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b). Despite criticism for sourcing queries from collection documents, the Cranfield experiments highlighted the importance of jointly reporting recall and precision, pioneered the practice of using authors and citations for augmenting relevance judgements and established the test collection paradigm. Expert citations have already been exploited for procuring relevance judgements. For example, Ritchie et al. (2006) elicited relevance judgements for citations in papers accepted in a scientific conference from their authors and use</context>
</contexts>
<marker>Cleverdon, 1960</marker>
<rawString>C. W. Cleverdon. 1960. Report on the first stage of an investigation into the comparative efficiency of indexing systems. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C W Cleverdon</author>
</authors>
<title>Aslib cranfield research project: Report on the testing and analysis of an investigation into the comparative efficiency of indexing systems.</title>
<date>1962</date>
<tech>Technical report,</tech>
<contexts>
<context position="1109" citStr="Cleverdon, 1962" startWordPosition="148" endWordPosition="149">d from the on-line collaboration website MathOverflow by delegating domain-specific decisions to experts on-line. With our test collection, we construct a baseline using Lucene’s vectorspace model implementation and conduct an experiment to investigate how prior extraction of technical terms from mathematical text can affect retrieval efficiency. We show that by boosting the importance of technical terms, statistically significant improvements in retrieval performance can be obtained over the baseline. 1 Introduction Since their introduction through the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b), test collections have become the foundation of information retrieval (IR) evaluation. Recent interest in Mathematical information retrieval (MIR) has prompted the construction of the NTCIR Math IR test collection (Aizawa et al., 2013). Like many general-purpose, domainspecific IR test collections, the NTCIR collection is composed of broad queries intended to test systems over a wide spectrum of query complexity. In this paper we present a test collection composed of real-life, research-level mathematical topics and associated relevance judge</context>
<context position="11540" citStr="Cleverdon, 1962" startWordPosition="1778" endWordPosition="1779">either 1 or 2 micro-topics, with the average being close to 1 (1.33). The majority of topics (97,80.83%) have only one relevant document while a further 21 (17.5%) have two relevant documents. Two topics have more than 2 relevant documents: one with 3 and another with 4. In terms of micro-topics, this corresponds to 140 micro-topics (87.50%) with 1 relevant document, 17 (10.625%) with 2, 2 microtopics (1.25%) with 3 and just one (0.625%) with 4 relevant documents. 3 Related Work Test collections over scientific publications were first introduced for the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b). Despite criticism for sourcing queries from collection documents, the Cranfield experiments highlighted the importance of jointly reporting recall and precision, pioneered the practice of using authors and citations for augmenting relevance judgements and established the test collection paradigm. Expert citations have already been exploited for procuring relevance judgements. For example, Ritchie et al. (2006) elicited relevance judgements for citations in papers accepted in a scientific conference from their authors and used these judgement</context>
</contexts>
<marker>Cleverdon, 1962</marker>
<rawString>C. W. Cleverdon. 1962. Aslib cranfield research project: Report on the testing and analysis of an investigation into the comparative efficiency of indexing systems. Technical report, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril W Cleverdon</author>
</authors>
<title>The significance of the cranfield tests on index languages.</title>
<date>1991</date>
<booktitle>In Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’91,</booktitle>
<pages>3--12</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Cleverdon, 1991</marker>
<rawString>Cyril W. Cleverdon. 1991. The significance of the cranfield tests on index languages. In Proceedings of the 14th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’91, pages 3–12, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Craswell</author>
<author>David Hawking</author>
<author>Ross Wilkinson</author>
<author>Mingfang Wu</author>
</authors>
<title>Overview of the trec-2003 web track.</title>
<date>2003</date>
<booktitle>In Proceedings of TREC-2003,</booktitle>
<location>Gaithersburg, Maryland USA,</location>
<contexts>
<context position="2834" citStr="Craswell et al., 2003" startWordPosition="403" endWordPosition="406">ording to Donaldson’s result. On the contrary, there are some arbitrarily small complex deformations of the above 3-fold which do not admit Kaehler-Einstein metrics, as shown by Tian. All examples considered by Tian seem to have no symmetries at all. Is it possible to find similarly arbitrarily small complex deformations with C*-action and which do not admit any Kaehler-Einstein metric? Due to their specialised nature, our topics have a relatively small number of relevant documents. Fortunately, there is precedent of this from IR tasks such as QA (Ishikawa et al., 2010) and known-item search (Craswell et al., 2003). With our test collection, we construct a baseline using Lucene’s default implementation of the vector space model (VSM). Additionally, we conduct an experiment designed to investigate the hypothesis that technical terms in mathematics have elevated retrieval significance. Information in mathematics is communicated by defining, manipulating and otherwise operating on mathematical structures and objects which can be instantiated in the mathematical discourse. In this sense, technical terminology in mathematics has an elevated role. This hypothesis stems from the observation that the mathematic</context>
</contexts>
<marker>Craswell, Hawking, Wilkinson, Wu, 2003</marker>
<rawString>Nick Craswell, David Hawking, Ross Wilkinson, and Mingfang Wu. 2003. Overview of the trec-2003 web track. In Proceedings of TREC-2003, Gaithersburg, Maryland USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Frantzi</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>Extracting terminological expressions.</title>
<date>1996</date>
<booktitle>In The Special Interest Group Notes of Information Processing Society of Japan, IPSJ SIG Notes, number 112 in Natural Language,</booktitle>
<pages>8388</pages>
<contexts>
<context position="16321" citStr="Frantzi et al., 1996" startWordPosition="2530" endWordPosition="2533"> i=1 k=1 4.3.1 Baseline Lucene’s VSM implementation with default TFIDF weighting and scoring is used as the baseline. This is intended to emulate ageneral-purpose information retrieval scenario, which is the motivation behind the design of Lucene’s default configuration. 4.3.2 Boosted Technical Terms The alternative model is designed to give more weight to technical terminology common to both documents and queries. In order to construct this model, all technical terms are extracted from the document collection using an implementation of the C-Value multi-word technical term extraction method (Frantzi et al., 1996; Frantzi et al., 1998). Given an input corpus, the C-Value method extracts multi-word terms by making use of a linguistic and a statistical component. The linguistic component is responsible for eliminating multi-term strings that are unlikely to be technical terms through the use of a stop-list (composed of high-frequency corpus terms) and linguistic filters (regular expressions) applied on sequences of part-of-speech tags. The statistical component assigns a score to a candidate term sequence based on corpus-wide statistical characteristics of the sequence itself an “termhood” d those of te</context>
</contexts>
<marker>Frantzi, Ananiadou, Tsujii, 1996</marker>
<rawString>K. Frantzi, S. Ananiadou, and J. Tsujii. 1996. Extracting terminological expressions. In The Special Interest Group Notes of Information Processing Society of Japan, IPSJ SIG Notes, number 112 in Natural Language, page 8388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina T Frantzi</author>
<author>Sophia Ananiadou</author>
<author>Jun-ichi Tsujii</author>
</authors>
<title>The c-value/nc-value method of automatic recognition for multi-word terms.</title>
<date>1998</date>
<booktitle>In Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries, ECDL ’98,</booktitle>
<pages>585--604</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="16344" citStr="Frantzi et al., 1998" startWordPosition="2534" endWordPosition="2537">e Lucene’s VSM implementation with default TFIDF weighting and scoring is used as the baseline. This is intended to emulate ageneral-purpose information retrieval scenario, which is the motivation behind the design of Lucene’s default configuration. 4.3.2 Boosted Technical Terms The alternative model is designed to give more weight to technical terminology common to both documents and queries. In order to construct this model, all technical terms are extracted from the document collection using an implementation of the C-Value multi-word technical term extraction method (Frantzi et al., 1996; Frantzi et al., 1998). Given an input corpus, the C-Value method extracts multi-word terms by making use of a linguistic and a statistical component. The linguistic component is responsible for eliminating multi-term strings that are unlikely to be technical terms through the use of a stop-list (composed of high-frequency corpus terms) and linguistic filters (regular expressions) applied on sequences of part-of-speech tags. The statistical component assigns a score to a candidate term sequence based on corpus-wide statistical characteristics of the sequence itself an “termhood” d those of term sequences that conta</context>
</contexts>
<marker>Frantzi, Ananiadou, Tsujii, 1998</marker>
<rawString>Katerina T. Frantzi, Sophia Ananiadou, and Jun-ichi Tsujii. 1998. The c-value/nc-value method of automatic recognition for multi-word terms. In Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries, ECDL ’98, pages 585–604, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>K Noriko</author>
</authors>
<title>Test collections for patent retrieval and patent classification in the fifth ntcir workshop.</title>
<date>2006</date>
<contexts>
<context position="13711" citStr="Fujii et al. (2006)" startWordPosition="2118" endWordPosition="2121">answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of sourcing relevance judgements from expert citations is an established practice in IR. In the context of patent search, for example, Graf and Azzopardi (2008) utilised citations in patent office expert reports as relevance judgements, while Fujii et al. (2006) automatically extracted patent office expert citations used to reject patent applications. 4 Experiments In this section we conduct an experiment to demonstrate the usefulness of our test collection by investigating the impact of terminology boosting on MIR effectiveness. An important assumption of this experiment is that the retrieval of each micro-topic is dependent only on the attached prelude. 4.1 Experimental Setup We first produced a Lucene index over all documents in the MREC. In order to normalise processing of XHTML+MathML, topics and MREC documents were passed through the Tika frame</context>
</contexts>
<marker>Fujii, Iwayama, Noriko, 2006</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko K. 2006. Test collections for patent retrieval and patent classification in the fifth ntcir workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Graf</author>
<author>L Azzopardi</author>
</authors>
<title>A methodology for building a patent test collection for prior art search.</title>
<date>2008</date>
<contexts>
<context position="13609" citStr="Graf and Azzopardi (2008)" startWordPosition="2102" endWordPosition="2105">IR-8 Community QA pilot task (Ishikawa et al., 2010; Sakai et al., 2011). Characterisation of crowd-sourced answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of sourcing relevance judgements from expert citations is an established practice in IR. In the context of patent search, for example, Graf and Azzopardi (2008) utilised citations in patent office expert reports as relevance judgements, while Fujii et al. (2006) automatically extracted patent office expert citations used to reject patent applications. 4 Experiments In this section we conduct an experiment to demonstrate the usefulness of our test collection by investigating the impact of terminology boosting on MIR effectiveness. An important assumption of this experiment is that the retrieval of each micro-topic is dependent only on the attached prelude. 4.1 Experimental Setup We first produced a Lucene index over all documents in the MREC. In order</context>
</contexts>
<marker>Graf, Azzopardi, 2008</marker>
<rawString>E. Graf and L. Azzopardi. 2008. A methodology for building a patent test collection for prior art search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zoltan Gyongyi</author>
<author>Georgia Koutrika</author>
<author>Jan Pedersen</author>
<author>Hector Garcia-Molina</author>
</authors>
<date>2008</date>
<note>Questioning yahoo! answers.</note>
<contexts>
<context position="12852" citStr="Gyongyi et al. (2008)" startWordPosition="1976" endWordPosition="1980"> is related to the NTCIR-10 Math IR test collection (Aizawa et al., 2013). Furthermore, the topics in our collection are analogous to those in the NTCIR full-text search, in the sense that they take the form of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test collection has been designed to facilitate retrieval of highly specialised, mathematical information needs of uniformly high complexity. Similar use of crowd-sourced expertise has been proposed in the context of QA. For example, Gyongyi et al. (2008), examined 10 months-worth of “Yahoo! Answers” material as part of an investigation of QA data, which was later used for the 336 NTCIR-8 Community QA pilot task (Ishikawa et al., 2010; Sakai et al., 2011). Characterisation of crowd-sourced answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of </context>
</contexts>
<marker>Gyongyi, Koutrika, Pedersen, Garcia-Molina, 2008</marker>
<rawString>Zoltan Gyongyi, Georgia Koutrika, Jan Pedersen, and Hector Garcia-Molina. 2008. Questioning yahoo! answers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>Overview of the first trec conference.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’93,</booktitle>
<pages>36--47</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5757" citStr="Harman, 1993" startWordPosition="842" endWordPosition="843"> contrast, we require a collection of topics characterised by a higher lower bound on topic complexity with individual topics capturing highly-specialised, real-world information needs. Unfortunately, research-level mathematical information needs are hard to source from documents in a way that would not render them artificial. Furthermore, manual construction of topics and relevance judgements is unrealistic due to the large number of experts required to cover the various specialised sub-fields of mathematics. This, coupled with limited access to numerous MIR systems, makes TREC-like pooling (Harman, 1993; Voorhees and Harman, 2005) impractical. We propose that topics and relevance judgements be procured from the on-line collaboration website MathOverflow (MO), an online QA site for research mathematicians. A user (information seeker) can post a question on the site, usually relating to a small niche field in mathematics. Colleagues can either post a candidate answer, comment on the question, comment on and/or upPrelude 1) Apparently, physicist can calculate the GW invariants of quintic CY 3-fold up to genus 51. 2) For each genus g, there is a lower bound d(g) such that for every d &lt; d(g), all</context>
</contexts>
<marker>Harman, 1993</marker>
<rawString>Donna Harman. 1993. Overview of the first trec conference. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’93, pages 36–47, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
</authors>
<title>Information Retrieval Evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>Harman, 2011</marker>
<rawString>Donna Harman. 2011. Information Retrieval Evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Ishikawa</author>
<author>Tetsuya Sakai</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of the ntcir-8 community qa pilot task (part i): The test collection and the task.</title>
<date>2010</date>
<contexts>
<context position="2788" citStr="Ishikawa et al., 2010" startWordPosition="396" endWordPosition="399">PGL(2, C) admits a Kaehler-Einstein metric according to Donaldson’s result. On the contrary, there are some arbitrarily small complex deformations of the above 3-fold which do not admit Kaehler-Einstein metrics, as shown by Tian. All examples considered by Tian seem to have no symmetries at all. Is it possible to find similarly arbitrarily small complex deformations with C*-action and which do not admit any Kaehler-Einstein metric? Due to their specialised nature, our topics have a relatively small number of relevant documents. Fortunately, there is precedent of this from IR tasks such as QA (Ishikawa et al., 2010) and known-item search (Craswell et al., 2003). With our test collection, we construct a baseline using Lucene’s default implementation of the vector space model (VSM). Additionally, we conduct an experiment designed to investigate the hypothesis that technical terms in mathematics have elevated retrieval significance. Information in mathematics is communicated by defining, manipulating and otherwise operating on mathematical structures and objects which can be instantiated in the mathematical discourse. In this sense, technical terminology in mathematics has an elevated role. This hypothesis </context>
<context position="13035" citStr="Ishikawa et al., 2010" startWordPosition="2009" endWordPosition="2012"> that they take the form of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test collection has been designed to facilitate retrieval of highly specialised, mathematical information needs of uniformly high complexity. Similar use of crowd-sourced expertise has been proposed in the context of QA. For example, Gyongyi et al. (2008), examined 10 months-worth of “Yahoo! Answers” material as part of an investigation of QA data, which was later used for the 336 NTCIR-8 Community QA pilot task (Ishikawa et al., 2010; Sakai et al., 2011). Characterisation of crowd-sourced answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of sourcing relevance judgements from expert citations is an established practice in IR. In the context of patent search, for example, Graf and Azzopardi (2008) utilised citations in pat</context>
</contexts>
<marker>Ishikawa, Sakai, Kando, 2010</marker>
<rawString>Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando. 2010. Overview of the ntcir-8 community qa pilot task (part i): The test collection and the task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Iwayama</author>
<author>Atsushi Fujii</author>
<author>Noriko Kando</author>
<author>Akihiko Takano</author>
</authors>
<title>Overview of patent retrieval task at ntcir-3.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-2003 Workshop on Patent Corpus Processing - Volume 20, PATENT ’03,</booktitle>
<pages>24--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Iwayama, Fujii, Kando, Takano, 2003</marker>
<rawString>Makoto Iwayama, Atsushi Fujii, Noriko Kando, and Akihiko Takano. 2003. Overview of patent retrieval task at ntcir-3. In Proceedings of the ACL-2003 Workshop on Patent Corpus Processing - Volume 20, PATENT ’03, pages 24–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jones</author>
</authors>
<date>1981</date>
<journal>Information retrieval experiment. Butterworths.</journal>
<marker>Jones, 1981</marker>
<rawString>K.S. Jones. 1981. Information retrieval experiment. Butterworths.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin L´ıˇska</author>
<author>Petr Sojka</author>
<author>Michal R˚uˇziˇcka</author>
<author>Petr Mravec</author>
</authors>
<title>Web interface and collection for mathematical retrieval: Webmias and mrec.</title>
<date>2011</date>
<booktitle>Towards a Digital Mathematics Library.,</booktitle>
<pages>77--84</pages>
<editor>In Petr Sojka and Thierry Bouche, editors,</editor>
<publisher>Masaryk University.</publisher>
<location>Bertinoro, Italy,</location>
<marker>L´ıˇska, Sojka, R˚uˇziˇcka, Mravec, 2011</marker>
<rawString>Martin L´ıˇska, Petr Sojka, Michal R˚uˇziˇcka, and Petr Mravec. 2011. Web interface and collection for mathematical retrieval: Webmias and mrec. In Petr Sojka and Thierry Bouche, editors, Towards a Digital Mathematics Library., pages 77–84, Bertinoro, Italy, Jul. Masaryk University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ursula Martin</author>
<author>Alison Pease</author>
</authors>
<title>What does mathoverflow tell us about the production of mathematics? CoRR,</title>
<date>2013</date>
<contexts>
<context position="6916" citStr="Martin and Pease (2013)" startWordPosition="1041" endWordPosition="1044">g, there is a lower bound d(g) such that for every d &lt; d(g), all genus g degree d invariants of quintic are zero. MT-1 I am looking for a reference that has a table of these number for some low degrees (say up to degree 5) and low genera (at least until g = 3). MT-2 Where can I found this lower bound? Table 1: MO post 14655, prelude and micro-topics vote existing answers. Ultimately, the information seeker decides which answer satisfies the underlying information need by marking it as “accepted”. Material on MO is closely aligned with our requirements. Specifically, Tausczik et al. (2014) and Martin and Pease (2013) agree that MO questions (information needs) arise from doing mathematics research and are novel to the mathematician involved. The authors conclude that, having been produced by experts, MO answers are authoritative and partially credit the website’s reward system for their strong reliability. MO questions often have multiple sub-parts, which we refer to as micro-topics since they encode atomic information needs. Furthermore, information in MO questions is carried by two types of sentences: prelude sentences, which are used to set the mathematical context (introduce mathematical constructs an</context>
</contexts>
<marker>Martin, Pease, 2013</marker>
<rawString>Ursula Martin and Alison Pease. 2013. What does mathoverflow tell us about the production of mathematics? CoRR, abs/1305.0904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucene project</author>
</authors>
<title>Lucene’s tf-idf similarity function.</title>
<date>2013</date>
<contexts>
<context position="15437" citStr="project, 2013" startWordPosition="2388" endWordPosition="2389">de with sentences associated with the microtopic. For example, query string for micro-topic MT-1 in Table 1 is generated by concatenating its text with that of the prelude. Using this strategy, consistency with the assumption outlined at 5https://tika.apache.org/ the beginning of the section is achieved since no overlap beyond the prelude is introduced between queries generated for micro-topics attached to a given topic. 4.3 Systems Using Lucene as the indexing and searching backend, we compare the performance of two retrieval methods. Underpinning both methods is Lucene’s default similarity (project, 2013), which is based on cosine similarity: V (q).V (d) sim(q, d) = where V (q) and V (d) are weighted vectors for the query and candidate document respectively. As a performance measure, we use mean average precision (MAP): 1 1 |Q |m MAP(Q) = E E Precision(Rik) |Q |mi i=1 k=1 4.3.1 Baseline Lucene’s VSM implementation with default TFIDF weighting and scoring is used as the baseline. This is intended to emulate ageneral-purpose information retrieval scenario, which is the motivation behind the design of Lucene’s default configuration. 4.3.2 Boosted Technical Terms The alternative model is designed </context>
</contexts>
<marker>project, 2013</marker>
<rawString>Lucene project. 2013. Lucene’s tf-idf similarity function.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
<author>Simone Teufel</author>
<author>Stephen Robertson</author>
</authors>
<title>Creating a test collection for citation-based ir experiments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>391--398</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA,</location>
<contexts>
<context position="12006" citStr="Ritchie et al. (2006)" startWordPosition="1841" endWordPosition="1844">cuments. 3 Related Work Test collections over scientific publications were first introduced for the Cranfield experiments (Cleverdon, 1960; Cleverdon, 1962; Cleverdon et al., 1966a; Cleverdon et al., 1966b). Despite criticism for sourcing queries from collection documents, the Cranfield experiments highlighted the importance of jointly reporting recall and precision, pioneered the practice of using authors and citations for augmenting relevance judgements and established the test collection paradigm. Expert citations have already been exploited for procuring relevance judgements. For example, Ritchie et al. (2006) elicited relevance judgements for citations in papers accepted in a scientific conference from their authors and used these judgements as part of their test collection of scientific publications. In terms of domain, our work is related to the NTCIR-10 Math IR test collection (Aizawa et al., 2013). Furthermore, the topics in our collection are analogous to those in the NTCIR full-text search, in the sense that they take the form of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test coll</context>
</contexts>
<marker>Ritchie, Teufel, Robertson, 2006</marker>
<rawString>Anna Ritchie, Simone Teufel, and Stephen Robertson. 2006. Creating a test collection for citation-based ir experiments. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 391–398, New York City, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuya Sakai</author>
<author>Daisuke Ishikawa</author>
<author>Noriko Kando</author>
<author>Yohei Seki</author>
<author>Kazuko Kuriyama</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Using graded-relevance metrics for evaluating community qa answer selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM ’11,</booktitle>
<pages>187--196</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13056" citStr="Sakai et al., 2011" startWordPosition="2013" endWordPosition="2016">m of coherent text interspersed with mathematical expressions. Rather than being focused on accommodating information needs of varying complexity, however, our test collection has been designed to facilitate retrieval of highly specialised, mathematical information needs of uniformly high complexity. Similar use of crowd-sourced expertise has been proposed in the context of QA. For example, Gyongyi et al. (2008), examined 10 months-worth of “Yahoo! Answers” material as part of an investigation of QA data, which was later used for the 336 NTCIR-8 Community QA pilot task (Ishikawa et al., 2010; Sakai et al., 2011). Characterisation of crowd-sourced answers in terms of totality (section 2) has also been considered in the context of QA. In particular, Sakai et al. (2011) describe a relevance grading scheme of crowd-sourced answers based on the total/partial/irrelevant scale, but highlight that answers on “Yahoo! Answers” vary in quality (e.g., due to instances of bias or obscenity). Finally, the idea of sourcing relevance judgements from expert citations is an established practice in IR. In the context of patent search, for example, Graf and Azzopardi (2008) utilised citations in patent office expert rep</context>
</contexts>
<marker>Sakai, Ishikawa, Kando, Seki, Kuriyama, Lin, 2011</marker>
<rawString>Tetsuya Sakai, Daisuke Ishikawa, Noriko Kando, Yohei Seki, Kazuko Kuriyama, and Chin-Yew Lin. 2011. Using graded-relevance metrics for evaluating community qa answer selection. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, WSDM ’11, pages 187– 196, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Soboroff</author>
<author>Charles Nicholas</author>
<author>Patrick Cahan</author>
</authors>
<title>Ranking retrieval systems without relevance judgments.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01,</booktitle>
<pages>66--73</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Soboroff, Nicholas, Cahan, 2001</marker>
<rawString>Ian Soboroff, Charles Nicholas, and Patrick Cahan. 2001. Ranking retrieval systems without relevance judgments. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’01, pages 66–73, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sprck Jones</author>
<author>C J Van Rijsbergen</author>
</authors>
<title>Report on the need for and provision of an ’ideal’ information retrieval test collection.</title>
<date>1975</date>
<tech>Technical report,</tech>
<institution>British Library Research and Development Report 5266, University Computer Laboratory,</institution>
<location>Cambridge.</location>
<marker>Jones, Van Rijsbergen, 1975</marker>
<rawString>K. Sprck Jones and C.J. Van Rijsbergen. 1975. Report on the need for and provision of an ’ideal’ information retrieval test collection. Technical report, British Library Research and Development Report 5266, University Computer Laboratory, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sprck Jones</author>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information retrieval test collections.</title>
<date>1976</date>
<journal>Journal of Documentation,</journal>
<volume>32</volume>
<issue>1</issue>
<marker>Jones, Van Rijsbergen, 1976</marker>
<rawString>K. Sprck Jones and C.J. Van Rijsbergen. 1976. Information retrieval test collections. Journal of Documentation, 32(1):59–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Stamerjohanns</author>
<author>Michael Kohlhase</author>
</authors>
<title>Transforming the ariv to xml.</title>
<date>2008</date>
<journal>Intelligent Computer Mathematics,</journal>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>5144</volume>
<pages>574--582</pages>
<editor>In Serge Autexier, John Campbell, Julio Rubio, Volker Sorge, Masakazu Suzuki, and Freek Wiedijk, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Stamerjohanns, Kohlhase, 2008</marker>
<rawString>Heinrich Stamerjohanns and Michael Kohlhase. 2008. Transforming the ariv to xml. In Serge Autexier, John Campbell, Julio Rubio, Volker Sorge, Masakazu Suzuki, and Freek Wiedijk, editors, Intelligent Computer Mathematics, volume 5144 of Lecture Notes in Computer Science, pages 574–582. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heinrich Stamerjohanns</author>
<author>Michael Kohlhase</author>
<author>Deyan Ginev</author>
<author>Catalin David</author>
<author>Bruce R Miller</author>
</authors>
<title>Transforming large collections of scientific publications to xml.</title>
<date>2010</date>
<journal>Mathematics in Computer Science,</journal>
<volume>3</volume>
<issue>3</issue>
<marker>Stamerjohanns, Kohlhase, Ginev, David, Miller, 2010</marker>
<rawString>Heinrich Stamerjohanns, Michael Kohlhase, Deyan Ginev, Catalin David, and Bruce R. Miller. 2010. Transforming large collections of scientific publications to xml. Mathematics in Computer Science, 3(3):299–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>Predicting the perceived quality of online mathematics contributions from users’ reputations.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11,</booktitle>
<pages>1885--1888</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Tausczik, Pennebaker, 2011</marker>
<rawString>Yla R. Tausczik and James W. Pennebaker. 2011. Predicting the perceived quality of online mathematics contributions from users’ reputations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI ’11, pages 1885–1888, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>Participation in an online mathematics community: Differentiating motivations to add.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work, CSCW ’12,</booktitle>
<pages>207--216</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Tausczik, Pennebaker, 2012</marker>
<rawString>Yla R. Tausczik and James W. Pennebaker. 2012. Participation in an online mathematics community: Differentiating motivations to add. In Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work, CSCW ’12, pages 207–216, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>Aniket Kittur</author>
<author>Robert E Kraut</author>
</authors>
<title>Collaborative problem solving: A study of mathoverflow.</title>
<date>2014</date>
<booktitle>In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp;#38; Social Computing, CSCW ’14,</booktitle>
<pages>355--367</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6888" citStr="Tausczik et al. (2014)" startWordPosition="1036" endWordPosition="1039">enus 51. 2) For each genus g, there is a lower bound d(g) such that for every d &lt; d(g), all genus g degree d invariants of quintic are zero. MT-1 I am looking for a reference that has a table of these number for some low degrees (say up to degree 5) and low genera (at least until g = 3). MT-2 Where can I found this lower bound? Table 1: MO post 14655, prelude and micro-topics vote existing answers. Ultimately, the information seeker decides which answer satisfies the underlying information need by marking it as “accepted”. Material on MO is closely aligned with our requirements. Specifically, Tausczik et al. (2014) and Martin and Pease (2013) agree that MO questions (information needs) arise from doing mathematics research and are novel to the mathematician involved. The authors conclude that, having been produced by experts, MO answers are authoritative and partially credit the website’s reward system for their strong reliability. MO questions often have multiple sub-parts, which we refer to as micro-topics since they encode atomic information needs. Furthermore, information in MO questions is carried by two types of sentences: prelude sentences, which are used to set the mathematical context (introduc</context>
</contexts>
<marker>Tausczik, Kittur, Kraut, 2014</marker>
<rawString>Yla R. Tausczik, Aniket Kittur, and Robert E. Kraut. 2014. Collaborative problem solving: A study of mathoverflow. In Proceedings of the 17th ACM Conference on Computer Supported Cooperative Work &amp;#38; Social Computing, CSCW ’14, pages 355–367, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
<author>Donna K Harman</author>
</authors>
<title>TREC: Experiment and Evaluation</title>
<date>2005</date>
<booktitle>in Information Retrieval (Digital Libraries and Electronic Publishing).</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5785" citStr="Voorhees and Harman, 2005" startWordPosition="844" endWordPosition="847">require a collection of topics characterised by a higher lower bound on topic complexity with individual topics capturing highly-specialised, real-world information needs. Unfortunately, research-level mathematical information needs are hard to source from documents in a way that would not render them artificial. Furthermore, manual construction of topics and relevance judgements is unrealistic due to the large number of experts required to cover the various specialised sub-fields of mathematics. This, coupled with limited access to numerous MIR systems, makes TREC-like pooling (Harman, 1993; Voorhees and Harman, 2005) impractical. We propose that topics and relevance judgements be procured from the on-line collaboration website MathOverflow (MO), an online QA site for research mathematicians. A user (information seeker) can post a question on the site, usually relating to a small niche field in mathematics. Colleagues can either post a candidate answer, comment on the question, comment on and/or upPrelude 1) Apparently, physicist can calculate the GW invariants of quintic CY 3-fold up to genus 51. 2) For each genus g, there is a lower bound d(g) such that for every d &lt; d(g), all genus g degree d invariants</context>
</contexts>
<marker>Voorhees, Harman, 2005</marker>
<rawString>Ellen M. Voorhees and Donna K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval (Digital Libraries and Electronic Publishing). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Variations in relevance judgments and the measurement of retrieval effectiveness.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98,</booktitle>
<pages>315--323</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Voorhees, 1998</marker>
<rawString>Ellen M. Voorhees. 1998. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’98, pages 315–323, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the trec</title>
<date>2004</date>
<booktitle>In Proceedings of the Thirteenth Text REtreival Conference (TREC</booktitle>
<marker>Voorhees, 2004</marker>
<rawString>Ellen M. Voorhees. 2004. Overview of the trec 2001 question answering track. In Proceedings of the Thirteenth Text REtreival Conference (TREC 2004).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>