<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009206">
<title confidence="0.952707">
SIEL: Aspect Based Sentiment Analysis in Reviews
</title>
<author confidence="0.98238">
Satarupa Guha, Aditya Joshi, Vasudeva Varma
</author>
<affiliation confidence="0.908728666666667">
Search and Information Extraction Lab
International Institute of Information Technology, Hyderabad
Gachibowli, Hyderabad, Telengana, India
</affiliation>
<email confidence="0.9895625">
{satarupa.guha,aditya.joshi}@research.iiit.ac.in
vv@iiit.ac.in
</email>
<sectionHeader confidence="0.995504" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999490461538462">
Following the footsteps of SemEval-2014
Task 4 (Pontiki et al., 2014), SemEval-2015
too had a task dedicated to aspect-level senti-
ment analysis (Pontiki et al., 2015), which saw
participation from over 25 teams. In Aspect-
based Sentiment Analysis, the aim is to iden-
tify the aspects of entities and the sentiment
expressed for each aspect. In this paper, we
present a detailed description of our system,
that stood 4th in Aspect Category subtask (slot
1), 7th in Opinion Target Expression subtask
(slot 2) and 8th in Sentiment Polarity subtask
(slot 3) on the Restaurant datasets.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999760675">
When a review or a social media post talks about
a product or service, the user might want to dis-
cuss multiple aspects or sub-topics related to the
product or service being discussed. For example,
in a restaurant review, while the customer might
have good things to say about the food quality of-
fered at a restaurant, she might be disappointed with
the service offered to her, and she might think the
decor needs to be revamped. So a general senti-
ment analyzer that determines the overall sentiment
towards the product or service might not be able to
capture the full essence of the review. Hence the
need for Aspect-based Sentiment Analysis, for bet-
ter and more fine-grained analysis of user feedback,
which would enable service providers and product
manufacturers to identify those business aspects that
needs improvement. Specifically, SemEval-2015
Task 12 expects systems to automatically determine
the aspect categories present in the data and the sen-
timent expressed towards each of those categories,
given a customer review. For the Aspect Category
(Entity and Attribute) Detection subtask, one has to
identify every entity E and attribute A pair E#A to-
wards which an opinion is expressed in the given
text. E and A should be chosen from predefined
inventories of Entity types and Attribute labels per
domain. Each E#A pair together defines an aspect
category of the given text. The E#A inventories for
the restaurants domain has been shown in Table 1.
For the Opinion Target Expression (OTE) identi-
fication subtask (Slot 2), we need to identify an ex-
pression used in the given text that refers to the re-
viewed entity E of a pair E#A. The OTE is defined
by its starting and ending offsets in the given text.
The OTE slot takes the value “NULL” when there is
no explicit mention of the opinion entity or no men-
tion at all.
For Sentiment Polarity Detection task, each iden-
tified E#A pair of the given text has to be assigned a
polarity - positive, negative, or neutral.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9981603">
The Aspect Category Detection task can be thought
of as similar to document classification task, which
has a huge trove of excellent literature. Specif-
ically delving into classification of reviews, (Kir-
itchenko et al., 2014) showed state-of-art perfor-
mance, using interesting linguistic and lexicon fea-
tures. (Castellucci et al., 2014) used simple bag
of words based features, generalized using distribu-
tional vectors learnt from external data. (Brychc´ın et
al., 2014) employed MaxEnt classifiers using addi-
</bodyText>
<page confidence="0.979849">
759
</page>
<note confidence="0.9531315">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999748395833333">
tional features like word clusters learnt using various
methods like LDA.
(Hu and Liu, 2004b) initiated works on aspect
identification in product reviews using an associa-
tion rule based system. In his book (Liu, 2012)
specifies four methods for aspect extraction, namely,
frequent phrases, opinion and target relations, su-
pervised learning and topic models. (Jakob and
Gurevych, 2010) highlighted the use of Condi-
tional Random Fields to extract the aspect terms and
phrases and demonstrated a significant improvement
in the F-Measure compared to then state-of-the-art
by (Zhuang et al., 2006), which used a supervised
approach to extract feature-opinion pairs. There are
some approaches that utilize NLP semantics to ex-
tract aspect terms. Bhattacharyya (Mukherjee and
Bhattacharyya, 2012) created a system to discover
dependency parsing rules to extract opinion expres-
sions. Many new works use hybrid approaches com-
bining both NLP as well as statistical methods to
create improved systems. In SemEval 2014, (Kir-
itchenko et al., 2014) used an in-house entity tagging
system to find labels for Outside Term (O) and As-
pect Term (T). (Toh and Wang, 2014) used tagging
approach with more linguistic features and extra re-
sources like Wordnet and word clusters.
The task of Sentiment Analysis has been enriched
with some of the seminal works like (Pang and Lee,
2004) and (Wilson et al., 2005), and has reached
new heights with recent publications from (Socher
et al., 2013) which combines grammatical cues with
deep learning. (Carrascosa, 2014) presented inno-
vative techniques of ensemble learning for the task
of Sentiment Analysis, which we too have adopted
in concept. (Bakliwal et al., 2012) presents a sim-
ple sentiment scoring function which uses prior in-
formation to classify and weight various sentiment
bearing words/phrases in tweets. However, none of
these works are crafted to handle Aspect based Sen-
timent Analysis and it is not trivial to adapt them
for this task. Similar to the task at hand are works
done by (Mcauley et al., 2012) and (Lakkaraju et al.,
2011), both of whom mined great benefits from the
topic modelling paradigm. (Mohammad et al., 2013)
achieved the best performance in Aspect Category
Polarity Detection task in SemEval 2014 using vari-
ous innovative linguistic features and publicly avail-
able sentiment lexica and two automatically com-
</bodyText>
<table confidence="0.995622166666667">
Entities
RESTAURANT, FOOD, DRINKS,
SERVICE, AMBIENCE, LOCATION
Attributes
GENERAL, PRICES, QUALITY,
STYLE OPTIONS, MISCELLANEOUS
</table>
<tableCaption confidence="0.99985">
Table 1: Entities and Attributes in Restaurants dataset.
</tableCaption>
<bodyText confidence="0.998699285714286">
piled polarity lexica. (Brun et al., 2014) used infor-
mation from its syntactic parser, BoW features, and
an out-of-domain sentiment lexicon to train an SVM
model.
We have experimented with the techniques and
features from these previous works and have also
added some of our own.
</bodyText>
<sectionHeader confidence="0.981813" genericHeader="method">
3 Subtask 1: Aspect Category Detection
</sectionHeader>
<bodyText confidence="0.999076230769231">
The Aspect Category Detection task involves iden-
tifying every entity E and attribute A pair E#A to-
wards which an opinion is expressed in the given
review.
We take a supervised classification approach
where we use C one-vs-all Random Forest Classi-
fiers, for each of the C {entity,attribute} pairs or as-
pect categories in the training data, with basic bag
of words based approach. We have also tried other
features that we explain shortly, but surprisingly the
bag of words approach yielded us the best perfor-
mance. As a part of the pre-processing procedure,
we did the following:
</bodyText>
<listItem confidence="0.998201">
• Removed stop words, except pronouns, be-
cause we observed that the category SER-
VICE#GENERAL can easily be distinguished
from other categories by using pronouns as
cues
• Stemmed all words
• Removed punctuation
• Normalized all numbers by replacing them by
zeros, with the motivation that the exact figures
do not hold any semantic meaning and are not
of importance to us.
</listItem>
<bodyText confidence="0.38215">
Following is the list of features we experimented
with:
</bodyText>
<page confidence="0.867043">
760
</page>
<listItem confidence="0.921064636363636">
• Unigrams — For each word in a review, we
mark its corresponding position True if it is
present in the vocabulary.
• Presence of number — We check if a review
sample contains numbers or not, with the mo-
tivation reviews talking about the PRICES at-
tribute are more likely to have numbers in them.
• Presence of word in Food and Drinks list 1
— The motivation behind using this feature is,
sentences talking about say, FOOD#PRICES
and DRINKS#PRICES are likely to use sim-
</listItem>
<bodyText confidence="0.972512142857143">
ilar words like “cheap”, “expensive”, “value
for money”, “dollars”, etc., but we need to be
able to distinguish between the two (FOOD and
DRINKS). Hence we use look-up lists for food
and drinks with the hope that the customers
would explicitly use names of food and drinks
items in reviews, wherever applicable.
</bodyText>
<listItem confidence="0.817771">
• WordNet synsets — WordNet is a large lexi-
cal database of English. In Wordnet, synonyms
or words that denote the same concept and are
</listItem>
<bodyText confidence="0.98431475">
interchangeable in many contexts, are grouped
into unordered sets called synsets. Word forms
with several distinct meanings are represented
in as many distinct synsets, and hence this fea-
ture is useful for capturing semantic informa-
tion. For each word we find its corresponding
synset and use it as a feature for our classifier
in a bag of words fashion.
</bodyText>
<listItem confidence="0.948919428571429">
• TF-IDF — Instead of using binary values to de-
note absence or presence of a word in the sen-
tence, we put its corresponding TF-IDF score
pre-computed from the train data. Normally
for document classification tasks, TF-IDF per-
forms better than n-grams because the former
rightly penalizes common words that are not
helpful in distinguishing one topic from the
other. Although our Aspect Detection task is
very similar to document classification task,
this feature did not help much, probably be-
cause of the small size of the data set.
• Word2Vec — The Word2Vec is an efficient
implementation of skip-gram and continuous
</listItem>
<footnote confidence="0.9715865">
1Food list compiled from http://eatingatoz.com/food-list/
and Drinks list compiled manually
</footnote>
<bodyText confidence="0.999971977777778">
bag of words architectures that takes a text
corpus as input and produces the word vec-
tors of its constituent words as output. We
trained Word2Vec on a corpus comprising Yelp
Restaurant reviews data, SemEval 2014 data,
SemEval 2015 train data. Let the vector di-
mension to be D. For each word in a review
sentence, we get a vector representation of di-
mension D. We take an average over all words
and end up with a single D-dimensional vector.
We experiment with the value of D, which is
essentially an optimization over time required
to train, and the performance and finally set it
to be 30. However, vectors averaged over all
words in a sentence are not very good repre-
sentations for the sentence, which is possibly
why this feature did not add much value to our
system.
For train and test data were pre-processed and their
features extracted in the same way. As for the Ran-
dom Forest Classifier, we used 50 decision tree esti-
mators using Gini index criterion and at each step we
consider only S features when looking for the best
split, where S is the square root of the total number
of features.
We had also tried hierarchical 2-level classifica-
tion, i.e. first classifying a review sentence into
one of the entities and then classifying them fur-
ther into one of the pre-defined attributes. However,
this 2-level classification technique, with the same
set of features mentioned above, yielded poorer per-
formance. So we decided to not make any distinc-
tion between entities and attributes, and consider an
entity-attribute pair together as an aspect category.
This task required us to categorize reviews into
very fine-grained and inter-related categories, with
hierarchical dependencies among themselves. This
might have been one of the reasons why many of the
popular features used for regular document classifi-
cation did not perform as good as they promised to.
Another challenge was the small number of training
examples, as compared to the large number of cate-
gories to be classified into, which was not the case in
any of the previous works to the best of our knowl-
edge.
</bodyText>
<page confidence="0.994008">
761
</page>
<sectionHeader confidence="0.95854" genericHeader="method">
4 Subtask 2: Opinion Target Expression
</sectionHeader>
<bodyText confidence="0.99853005882353">
Given a review sentence, the aim of this task is
to find the Opinion Target Expressions (OTE), that
is, the particular attribute of the entity the user ex-
presses his/her sentiment about. Aspects may either
be explicitly explained in the review as in the sen-
tence “The service was really quick and I loved the
fajitas.” Here “service” and “fajita ”are explicit as-
pects. In a sentence like “Don’t go. Really horrible”,
the user didn’t use any individual term but still gives
an impression of her sentiment. In such cases, the
slot takes the value “NULL”. Our system uses a se-
quence labelling approach to tackle this problem by
the use of Conditional Random Fields. The tagger
from Mallet toolkit, is trained to identify three pos-
sible tags, namely BEG and INT for beginning and
intermediate target words and OTH for other words.
Our features are as follows:-
</bodyText>
<listItem confidence="0.99358">
• Word — The lowercase form of the word itself
• POS — Part of speech tag of the word
• Dependency — We use two kinds of depen-
dency features — the dependency label on in-
coming edge on the word, and the first depen-
dency label on outgoing edge. This proved to
be a very important feature.
• Capitalization — If the first character of the
word is in capital, mark it as capital.
• Punctuation — If the word contains any non-
alphanumeric character, we mark it as punctu-
ation.
• Seed — The word is marked as a seed if it was
present in the seed-list created by collecting all
the OTEs in the training data, splitting them by
word and removing all the stop words.
• Brown Cluster — Brown Cluster ID is obtained
by first training Brown Clustering on the same
</listItem>
<bodyText confidence="0.964046">
corpus we described for Word2Vec features in
Subtask 1. Brown clustering is a form of hier-
archical clustering of words based on the con-
text in which they occur. The intuition be-
hind the method is that a class-based language
model where probabilities of words are based
on the clusters of previous words, can over-
come the data sparsity problem inherent in lan-
guage modeling. From brown clustering, for
each word in the corpus we get the cluster ID
to which it has been assigned. We generate 100
clusters.
</bodyText>
<listItem confidence="0.803018">
• Presence in Expanded List — We curated an
expanded seed list from the original seed list
explained above. We utilized WiBi, which is a
taxonomy of Wikipedia pages and categories.
We traversed the WiBi Page graph and col-
lected the pages located next to the words (if
present) in the seed list. The new list was again
split by spaces and punctuation, and stop words
were removed. This feature is marked if the
term is present in the expanded list.
• Stop Word — This feature is marked if the cur-
rent term is a stop word in English language.
• Seed Stem — This feature contains the
stemmed form of the original word as obtained
from Porter Stemmer.
</listItem>
<sectionHeader confidence="0.9865815" genericHeader="method">
5 Subtask 3: Sentiment Polarity
Classification
</sectionHeader>
<bodyText confidence="0.998852761904762">
In this subtask, the input consists of a review sen-
tence and the set of aspect categories it belongs to.
The expected output is a polarity label for each of
the associated aspect categories. We have first ex-
tracted Bag of Words and Wordnet Synset features
from both train and test data. Then we run a vari-
ety of classifiers (like Stochastic Gradient Descent,
SVM, Adaboost) multiple times and store the con-
fidence scores obtained from decision functions of
each of these classifiers. Finally we build a linear
SVM classifier that uses the scores obtained from
the classifiers in the 1st level as features, along with
15 other hand-crafted lexicon features as explained
in Section 5.2. This is also known as stacking, a
form of ensemble learning. It is essentially stacking
of classifiers inside a classifier. Stacking typically
yields performance better than any single one of the
trained models, and this is what we wanted to lever-
age. However, since we need polarity labels per as-
pect category, we need to identify the segments in
the sentence that deals with each of the categories
</bodyText>
<page confidence="0.983383">
762
</page>
<bodyText confidence="0.99988575">
and then treat those segments as individual sam-
ples for polarity detection. For example, if there are
three aspect categories associated with a sentence,
we want to break it down into 3 {sentence,category}
</bodyText>
<equation confidence="0.9885685">
pairs: sent1, {cat1,cat2, cat3} →
{sent1, cat2}, {sent2,cat2}, {sent3, cat3}
</equation>
<bodyText confidence="0.9899865">
For each {sentence, category} pair, we find a
word in the sentence that is the best representative
of the category, which we call as centroid. Then we
take a window of n words surrounding the centroid
and consider that window to be the segment of in-
terest for that category. So in this example sentence,
we need to have three centroids and hence three seg-
ments, not necessarily disjoint:
</bodyText>
<equation confidence="0.976174">
{sent1, cat1}, {sent1,cat2}, {sent1, cat3} →
{seg1, cat1}, {seg2,cat2}, {seg3, cat3}
</equation>
<bodyText confidence="0.999366888888889">
We experimented with the window size, and de-
cided upon using a window size of 3 words to the
left and to the right of the centroid. It is interesting to
note that among sentences that have more than one
category, the average length of a review sentence is
15 words in train data and 17 words in test data.
After we get these segments, we extract the fol-
lowing features from these segments for polarity de-
tection:
</bodyText>
<listItem confidence="0.998415076923077">
• Bag of Words
• Grapheme Stretching i.e. words with re-
peated characters. For example, words like
“Tooooo goooood” indicates strong subjectiv-
ity and therefore is less likely to belong to Neu-
tral class.
• Presence of exclamation also signals subjectiv-
ity, usually positivity.
• Presence of wh-words and conditional words
like why, what,if, etc. Observation tells us that
such presence are mostly characteristic of sen-
tences with negative polarity.
• Wordnet Synsets, as explained before
</listItem>
<bodyText confidence="0.9716028">
While bag of words features include statistical in-
formation, WordNet synsets help incorporate se-
mantic information. These two complementary fea-
tures help us in making the maximal discrimination
among the target classes.
</bodyText>
<subsectionHeader confidence="0.910165">
5.1 Extracting Centroid for a {Sentence,
Category} Pair
</subsectionHeader>
<bodyText confidence="0.99999454054054">
We automatically generate a set of seed words for
each of the aspect categories by the following tech-
nique: From the train data, we consider all sentences
labelled with a single category as a single document.
As a result, for 13 possible categories in the train
data, we have 13 documents. Now for each doc-
ument (corresponding to each category), we com-
pute the TF-IDF scores of all the words and consider
words having TF-IDF greater than a certain thresh-
old as seed words for that category. We ascertain the
optimal value of the threshold to be 0.2 through ex-
perimentation. We generate a co-occurrence matrix
of words from three datasets SemEval 2015 train
data, SemEval 2014 train and test data. Typically,
it is considered that two words co-occur if they are
present as bigram in the corpus. However, we define
co-occurrence as occurring in the same review sen-
tence, rather than occurring as a bigram as it is less
likely to find repetition of co-occurring bigrams in
a smaller corpus. This co-occurrence matrix stores
the frequency of co-occurrence of two words in the
corpus. For N words in the vocabulary, we have a
N x N co-occurrence matrix. Given a {sentence,
category} pair, for each word in the review sentence,
we find the Point wise Mutual Information (P.M.I.)
between that word and each word in the seed list of
the assigned category and take their average for that
word. We do the same for all words in the sentence.
The word in the sentence having the maximum av-
erage P.M.I. score is defined as the centroid for the
{sentence, category} pair. P.M.I is defined as the
ratio of the probability of occurrence of two words
together in the corpus to the product of the probabil-
ities of occurrence of the two words independently
in the corpus. We derive the co-occurrence frequen-
cies from the co-occurrence matrix we built in the
previous step.
</bodyText>
<subsectionHeader confidence="0.991375">
5.2 Ensemble Learning – Stacking Classifiers
</subsectionHeader>
<bodyText confidence="0.9999225">
After feature extraction, we train 3 kinds of classi-
fiers — Linear Support Vector Machines, Stochastic
Gradient Descent and Adaboost, for each of the fea-
tures — Bag of words and Wordnet Synsets. We
repeat the process K times where K E Z. We
have experimentally chosen K to be 30 — it is ac-
</bodyText>
<page confidence="0.995984">
763
</page>
<bodyText confidence="0.999962272727273">
tually a trade off between the time taken to train
the model and the performance improvements. As
we increased K over 30, the improvement in perfor-
mance started to diminish. For each test sample, we
obtain 3 scores (corresponding to three classes —
positive, negative, neutral) from the decision func-
tion of each classifier. We use these confidence
scores as features along with 15 other hand-crafted
lexicon features for a linear Support Vector Ma-
chine classifier. We employ features such as num-
ber of positive tokens, number of negative tokens,
total positive sentiment score, total negative senti-
ment score, sum of sentiment scores, maximum sen-
timent score, etc. from Sentiwordnet (Baccianella et
al., 2010), Bing Liu’s opinion lexicon (Hu and Liu,
2004a), MPQA subjectivity lexicon (Wilson et al.,
2005), NRC Emotion Association lexicon (Moham-
mad and Turney, 2013), Sentiment140 lexicon (Go
et al., 2009), and NRC Hashtag Lexicon (Moham-
mad and Kiritchenko, 2014).
For the final linear SVM classifier, we experimen-
tally ascertain the optimal value of the parameter
C to be 0.024. The linear SVM classifiers, in the
first level of stacking, had a default value of 1.0 for
parameter C. We did not have enough time to tune
them, as we had many classifiers inside the main
SVM classifier. The Ada Boost Classifier uses 100
decision tree estimators and a default learning rate
of 1. We have used Scikit Learn for building all the
classifiers. Although we employ several classiers,
the time taken is negligible. This is because the dif-
ferent classifiers in the first stage of stacking can be
trained in parallel quite easily.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.99968425">
We submitted unconstrained systems for the Restau-
rants dataset. We did not run our system for other
domains mainly due to lack of time during the com-
petition. Table 2 shows our final F1-scores obtained
on SemEval official test data, for each of the three
slots. Tables 3, 4 and 5 presents the results of ab-
lation experiments carried out for slots 1, 2 and 3
respectively. We show the effect of varying the size
of the context window surrounding the centroid, on
F1-score in Figure 1. Finally Table 6 compares our
ensemble system with a baseline system trained on
a single linear SVM with only lexicon features.
</bodyText>
<table confidence="0.992002">
Subtask Our Score Best Score Rank
Slot 1 0.57 0.62 4
Slot 2 0.53 0.70 7
Slot 3 0.71 0.78 8
</table>
<tableCaption confidence="0.870597">
Table 2: Official Results for SemEval 2015.
</tableCaption>
<table confidence="0.999971666666667">
Feature Precision Recall F1
Unigrams 0.64 0.51 0.57
Unigrams+Bigrams 0.51 0.45 0.48
Unigrams+WordNet syn 0.53 0.48 0.50
Unigrams+Word2Vec 0.52 0.46 0.49
TF-IDF 0.47 0.42 0.44
</table>
<tableCaption confidence="0.999201">
Table 3: Experiment with Features for Slot 1.
</tableCaption>
<table confidence="0.999959333333333">
Feature Precision Recall F1
All 0.51 0.55 0.52
All - (Seed+Expanded Seed) 0.52 0.55 0.53 2
POS+Dep.+Punct.+Brown 0.35 0.54 0.43
POS+Dep.+Punct.+Stopwords 0.64 0.53 0.58 3
POS+Dep. 0.62 0.51 0.57
</table>
<tableCaption confidence="0.995809">
Table 4: Ablation Experiment for Slot 2.
</tableCaption>
<table confidence="0.999758">
Feature Accuracy
All (BoW + WordNet syn) 0.71
All - BoW 0.68
All - WordNet syn 0.70
</table>
<tableCaption confidence="0.98991">
Table 5: Ablation Experiment for Slot 3.
</tableCaption>
<table confidence="0.998943333333333">
System Accuracy
Linear SVM with only lexicon 0.68
Our system 0.71
</table>
<tableCaption confidence="0.939697333333333">
Table 6: Slot 3: Comparison of our ensemble learning
technique with a baseline system trained on a single Lin-
earSVM with only lexicon features.
</tableCaption>
<figureCaption confidence="0.993334">
Figure 1: Variation of F1 score with context window size.
</figureCaption>
<footnote confidence="0.997134">
2Submitted system
3This result was obtained during ablation experiment post-
competition
</footnote>
<page confidence="0.995621">
764
</page>
<sectionHeader confidence="0.9914" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99998825">
This paper describes the system submitted by team
SIEL for SemEval 2015 Task 12. For all the three
subtasks, our system performs quite well, ranking
between 4th and 8th. We experimented with Ensem-
ble Learning technique for slot 3, which we want to
explore and improve further. In future, we would
like to work on adapting our system to other do-
mains as well.
</bodyText>
<sectionHeader confidence="0.99473" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999791">
We sincerely thank Samik Datta and Mohit Ku-
mar for all the fruitful discussions and encourage-
ment, Riddhiman Dasgupta for insights on Ensem-
ble Learning and Mark Franco-Salvador for guiding
us with Wibi.
</bodyText>
<sectionHeader confidence="0.998875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999526857142857">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
in Proc. of LREC.
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining sentiments from tweets. In Proceedings
of the 3rd Workshop in Computational Approaches
to Subjectivity and Sentiment Analysis, WASSA ’12,
pages 11–18, Stroudsburg, PA, USA.
Caroline Brun, Nicoleta Diana Popa, and Claude Roux.
2014. Xrce: Hybrid classification for aspect-based
sentiment analysis. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 838–842.
Tom´aˇs Brychcin, Michal Konkol, and Josef Steinberger.
2014. Uwb: Machine learning approach to aspect-
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 817–822.
Rafael Carrascosa. 2014. An entry to kaggle’s ’sentiment
analysis on movie reviews’ competition.
Giuseppe Castellucci, Simone Filice, Danilo Croce, and
Roberto Basili. 2014. Unitor: Aspect based sentiment
analysis with structured learning. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval 2014), pages 761–767, Dublin, Ireland,
August.
Ingo Feinerer and Kurt Hornik, 2014. wordnet: WordNet
Interface. R package version 0.1-10.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In In ACL, pages 363–370.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Processing, pages 1–6.
Minqing Hu and Bing Liu. 2004a. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA.
Minqing Hu and Bing Liu. 2004b. Mining opinion
features in customer reviews. In Proceedings of the
19th National Conference on Artifical Intelligence,
AAAI’04, pages 755–760.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’10, pages 1035–1045,
Stroudsburg, PA, USA.
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and
Saif Mohammad. 2014. Nrc-canada-2014: Detecting
aspects and sentiment in customer reviews. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014), pages 437–442.
Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indrajit
Bhattacharya, and Srujana Merugu, 2011. Exploiting
Coherence for the Simultaneous Discovery of Latent
Facets and associated Sentiments, chapter 43, pages
498–509.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies.
Bing Liu. May 2012. Sentiment Analysis and Opinion
Mining (Introduction and Survey).
Edward Loper and Steven Bird. 2002. Nltk: The natural
language toolkit. In In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia: Association for Computa-
tional Linguistics.
Julian Mcauley, Jure Leskovec, and Dan Jurafsky. 2012.
Pale lagerthe pale lager model.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Saif M. Mohammad and Svetlana Kiritchenko. 2014.
Using hashtags to capture fine emotion categories from
tweets. Computational Intelligence, pages n/a–n/a.
Saif M. Mohammad and Peter D. Turney. 2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436–465.
</reference>
<page confidence="0.975777">
765
</page>
<reference confidence="0.999909057142857">
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-art
in sentiment analysis of tweets. CoRR, abs/1308.6242.
Subhabrata Mukherjee and Pushpak Bhattacharyya.
2012. Feature specific sentiment analysis for prod-
uct reviews. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
volume 7181 of Lecture Notes in Computer Science,
pages 475–487.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42Nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’04, Stroudsburg, PA, USA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–
2830.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Ion Androutsopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 27–35, Dublin, Ireland, August.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Har-
ris Papageorgiou, Suresh Manandhar, and Ion An-
droutsopoulos. 2015. Semeval-2015 task 12: Aspect
based sentiment analysis. In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval 2015), Denver, Colorado.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-
work for Topic Modelling with Large Corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta, May.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80, Dublin, Ireland, August.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
Zhiqiang Toh and Wenting Wang. 2014. Dlirec: Aspect
term extraction and term polarity classification system.
In Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval 2014), pages 235–240,
Dublin, Ireland, August.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. Dcu: Aspect-based polarity
classification for semeval task 4. In Proceedings of
the 8th International Workshop on Semantic Evalua-
tion (SemEval 2014), pages 223–229, Dublin, Ireland,
August.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the Conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT ’05, pages
347–354, Stroudsburg, PA, USA.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In Proceedings of
the 15th ACM International Conference on Informa-
tion and Knowledge Management, CIKM ’06, pages
43–50, New York, NY, USA.
</reference>
<page confidence="0.998186">
766
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.633184">
<title confidence="0.999724">SIEL: Aspect Based Sentiment Analysis in Reviews</title>
<author confidence="0.995577">Satarupa Guha</author>
<author confidence="0.995577">Aditya Joshi</author>
<author confidence="0.995577">Vasudeva</author>
<affiliation confidence="0.8657535">Search and Information Extraction International Institute of Information Technology,</affiliation>
<address confidence="0.997778">Gachibowli, Hyderabad, Telengana,</address>
<email confidence="0.967176">vv@iiit.ac.in</email>
<abstract confidence="0.991864357142857">Following the footsteps of SemEval-2014 Task 4 (Pontiki et al., 2014), SemEval-2015 too had a task dedicated to aspect-level sentiment analysis (Pontiki et al., 2015), which saw participation from over 25 teams. In Aspectbased Sentiment Analysis, the aim is to identify the aspects of entities and the sentiment expressed for each aspect. In this paper, we present a detailed description of our system, that stood 4th in Aspect Category subtask (slot 1), 7th in Opinion Target Expression subtask (slot 2) and 8th in Sentiment Polarity subtask (slot 3) on the Restaurant datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In</title>
<date>2010</date>
<booktitle>in Proc. of LREC.</booktitle>
<contexts>
<context position="20296" citStr="Baccianella et al., 2010" startWordPosition="3395" endWordPosition="3398">ovements. As we increased K over 30, the improvement in performance started to diminish. For each test sample, we obtain 3 scores (corresponding to three classes — positive, negative, neutral) from the decision function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifi</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In in Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Piyush Arora</author>
<author>Senthil Madhappan</author>
<author>Nikhil Kapre</author>
<author>Mukesh Singh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Mining sentiments from tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12,</booktitle>
<pages>11--18</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5273" citStr="Bakliwal et al., 2012" startWordPosition="830" endWordPosition="833">ystem to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm. (Mohammad et al., 2013) achieved the best performance in Aspect Category Polarity Detection task in SemEval 2014 using various innovative linguistic </context>
</contexts>
<marker>Bakliwal, Arora, Madhappan, Kapre, Singh, Varma, 2012</marker>
<rawString>Akshat Bakliwal, Piyush Arora, Senthil Madhappan, Nikhil Kapre, Mukesh Singh, and Vasudeva Varma. 2012. Mining sentiments from tweets. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, WASSA ’12, pages 11–18, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
<author>Nicoleta Diana Popa</author>
<author>Claude Roux</author>
</authors>
<title>Xrce: Hybrid classification for aspect-based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>838--842</pages>
<contexts>
<context position="6175" citStr="Brun et al., 2014" startWordPosition="969" endWordPosition="972">imilar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm. (Mohammad et al., 2013) achieved the best performance in Aspect Category Polarity Detection task in SemEval 2014 using various innovative linguistic features and publicly available sentiment lexica and two automatically comEntities RESTAURANT, FOOD, DRINKS, SERVICE, AMBIENCE, LOCATION Attributes GENERAL, PRICES, QUALITY, STYLE OPTIONS, MISCELLANEOUS Table 1: Entities and Attributes in Restaurants dataset. piled polarity lexica. (Brun et al., 2014) used information from its syntactic parser, BoW features, and an out-of-domain sentiment lexicon to train an SVM model. We have experimented with the techniques and features from these previous works and have also added some of our own. 3 Subtask 1: Aspect Category Detection The Aspect Category Detection task involves identifying every entity E and attribute A pair E#A towards which an opinion is expressed in the given review. We take a supervised classification approach where we use C one-vs-all Random Forest Classifiers, for each of the C {entity,attribute} pairs or aspect categories in the</context>
</contexts>
<marker>Brun, Popa, Roux, 2014</marker>
<rawString>Caroline Brun, Nicoleta Diana Popa, and Claude Roux. 2014. Xrce: Hybrid classification for aspect-based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 838–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Brychcin</author>
<author>Michal Konkol</author>
<author>Josef Steinberger</author>
</authors>
<title>Uwb: Machine learning approach to aspectbased sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>817--822</pages>
<marker>Brychcin, Konkol, Steinberger, 2014</marker>
<rawString>Tom´aˇs Brychcin, Michal Konkol, and Josef Steinberger. 2014. Uwb: Machine learning approach to aspectbased sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 817–822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Carrascosa</author>
</authors>
<title>An entry to kaggle’s ’sentiment analysis on movie reviews’ competition.</title>
<date>2014</date>
<contexts>
<context position="5122" citStr="Carrascosa, 2014" startWordPosition="808" endWordPosition="809"> both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm</context>
</contexts>
<marker>Carrascosa, 2014</marker>
<rawString>Rafael Carrascosa. 2014. An entry to kaggle’s ’sentiment analysis on movie reviews’ competition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Castellucci</author>
<author>Simone Filice</author>
<author>Danilo Croce</author>
<author>Roberto Basili</author>
</authors>
<title>Unitor: Aspect based sentiment analysis with structured learning.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>761--767</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3224" citStr="Castellucci et al., 2014" startWordPosition="516" endWordPosition="519">he given text. The OTE slot takes the value “NULL” when there is no explicit mention of the opinion entity or no mention at all. For Sentiment Polarity Detection task, each identified E#A pair of the given text has to be assigned a polarity - positive, negative, or neutral. 2 Related Work The Aspect Category Detection task can be thought of as similar to document classification task, which has a huge trove of excellent literature. Specifically delving into classification of reviews, (Kiritchenko et al., 2014) showed state-of-art performance, using interesting linguistic and lexicon features. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods</context>
</contexts>
<marker>Castellucci, Filice, Croce, Basili, 2014</marker>
<rawString>Giuseppe Castellucci, Simone Filice, Danilo Croce, and Roberto Basili. 2014. Unitor: Aspect based sentiment analysis with structured learning. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 761–767, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Feinerer</author>
<author>Kurt Hornik</author>
</authors>
<title>wordnet: WordNet Interface. R package version 0.1-10.</title>
<date>2014</date>
<marker>Feinerer, Hornik, 2014</marker>
<rawString>Ingo Feinerer and Kurt Hornik, 2014. wordnet: WordNet Interface. R package version 0.1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In In ACL,</booktitle>
<pages>363--370</pages>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In In ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<booktitle>Processing,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="20494" citStr="Go et al., 2009" startWordPosition="3425" endWordPosition="3428">ion function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a default learning rate of 1. We have used Scikit Learn for building all the classifiers. Although we employ several classiers, the time taken is negligible.</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Processing, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="3675" citStr="Hu and Liu, 2004" startWordPosition="580" endWordPosition="583"> classification of reviews, (Kiritchenko et al., 2014) showed state-of-art performance, using interesting linguistic and lexicon features. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that</context>
<context position="20342" citStr="Hu and Liu, 2004" startWordPosition="3403" endWordPosition="3406"> performance started to diminish. For each test sample, we obtain 3 scores (corresponding to three classes — positive, negative, neutral) from the decision function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a def</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004a. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th National Conference on Artifical Intelligence, AAAI’04,</booktitle>
<pages>755--760</pages>
<contexts>
<context position="3675" citStr="Hu and Liu, 2004" startWordPosition="580" endWordPosition="583"> classification of reviews, (Kiritchenko et al., 2014) showed state-of-art performance, using interesting linguistic and lexicon features. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that</context>
<context position="20342" citStr="Hu and Liu, 2004" startWordPosition="3403" endWordPosition="3406"> performance started to diminish. For each test sample, we obtain 3 scores (corresponding to three classes — positive, negative, neutral) from the decision function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a def</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004b. Mining opinion features in customer reviews. In Proceedings of the 19th National Conference on Artifical Intelligence, AAAI’04, pages 755–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niklas Jakob</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting opinion targets in a single- and cross-domain setting with conditional random fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>1035--1045</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3968" citStr="Jakob and Gurevych, 2010" startWordPosition="623" endWordPosition="626">n et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved sys</context>
</contexts>
<marker>Jakob, Gurevych, 2010</marker>
<rawString>Niklas Jakob and Iryna Gurevych. 2010. Extracting opinion targets in a single- and cross-domain setting with conditional random fields. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 1035–1045, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Colin Cherry</author>
<author>Saif Mohammad</author>
</authors>
<title>Nrc-canada-2014: Detecting aspects and sentiment in customer reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>437--442</pages>
<contexts>
<context position="3113" citStr="Kiritchenko et al., 2014" startWordPosition="500" endWordPosition="504"> that refers to the reviewed entity E of a pair E#A. The OTE is defined by its starting and ending offsets in the given text. The OTE slot takes the value “NULL” when there is no explicit mention of the opinion entity or no mention at all. For Sentiment Polarity Detection task, each identified E#A pair of the given text has to be assigned a polarity - positive, negative, or neutral. 2 Related Work The Aspect Category Detection task can be thought of as similar to document classification task, which has a huge trove of excellent literature. Specifically delving into classification of reviews, (Kiritchenko et al., 2014) showed state-of-art performance, using interesting linguistic and lexicon features. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identific</context>
<context position="4617" citStr="Kiritchenko et al., 2014" startWordPosition="721" endWordPosition="725">Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which</context>
</contexts>
<marker>Kiritchenko, Zhu, Cherry, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif Mohammad. 2014. Nrc-canada-2014: Detecting aspects and sentiment in customer reviews. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Himabindu Lakkaraju</author>
<author>Chiranjib Bhattacharyya</author>
<author>Indrajit Bhattacharya</author>
<author>Srujana Merugu</author>
</authors>
<title>Exploiting Coherence for the Simultaneous Discovery of Latent Facets and associated Sentiments, chapter 43,</title>
<date>2011</date>
<pages>498--509</pages>
<contexts>
<context position="5653" citStr="Lakkaraju et al., 2011" startWordPosition="896" endWordPosition="899">(Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm. (Mohammad et al., 2013) achieved the best performance in Aspect Category Polarity Detection task in SemEval 2014 using various innovative linguistic features and publicly available sentiment lexica and two automatically comEntities RESTAURANT, FOOD, DRINKS, SERVICE, AMBIENCE, LOCATION Attributes GENERAL, PRICES, QUALITY, STYLE OPTIONS, MISCELLANEOUS Table 1: Entities and Attributes in Restaurants dataset. piled polarity lexica. (Brun et al., 2014) used information from its syntactic parser, BoW features, and an out-of-domai</context>
</contexts>
<marker>Lakkaraju, Bhattacharyya, Bhattacharya, Merugu, 2011</marker>
<rawString>Himabindu Lakkaraju, Chiranjib Bhattacharyya, Indrajit Bhattacharya, and Srujana Merugu, 2011. Exploiting Coherence for the Simultaneous Discovery of Latent Facets and associated Sentiments, chapter 43, pages 498–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<contexts>
<context position="3801" citStr="Liu, 2012" startWordPosition="602" endWordPosition="603">tures. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining (Introduction and Survey).</title>
<date>2012</date>
<contexts>
<context position="3801" citStr="Liu, 2012" startWordPosition="602" endWordPosition="603">tures. (Castellucci et al., 2014) used simple bag of words based features, generalized using distributional vectors learnt from external data. (Brychc´ın et al., 2014) employed MaxEnt classifiers using addi759 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 759–766, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. May 2012. Sentiment Analysis and Opinion Mining (Introduction and Survey).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>Nltk: The natural language toolkit. In</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics.</booktitle>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. Nltk: The natural language toolkit. In In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Mcauley</author>
<author>Jure Leskovec</author>
<author>Dan Jurafsky</author>
</authors>
<date>2012</date>
<note>Pale lagerthe pale lager model.</note>
<contexts>
<context position="5624" citStr="Mcauley et al., 2012" startWordPosition="891" endWordPosition="894">h recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm. (Mohammad et al., 2013) achieved the best performance in Aspect Category Polarity Detection task in SemEval 2014 using various innovative linguistic features and publicly available sentiment lexica and two automatically comEntities RESTAURANT, FOOD, DRINKS, SERVICE, AMBIENCE, LOCATION Attributes GENERAL, PRICES, QUALITY, STYLE OPTIONS, MISCELLANEOUS Table 1: Entities and Attributes in Restaurants dataset. piled polarity lexica. (Brun et al., 2014) used information from its syntactic parser, BoW </context>
</contexts>
<marker>Mcauley, Leskovec, Jurafsky, 2012</marker>
<rawString>Julian Mcauley, Jure Leskovec, and Dan Jurafsky. 2012. Pale lagerthe pale lager model.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
</authors>
<title>Using hashtags to capture fine emotion categories from tweets. Computational Intelligence,</title>
<date>2014</date>
<pages>pages n/a–n/a.</pages>
<contexts>
<context position="20552" citStr="Mohammad and Kiritchenko, 2014" startWordPosition="3433" endWordPosition="3437">se confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a default learning rate of 1. We have used Scikit Learn for building all the classifiers. Although we employ several classiers, the time taken is negligible. This is because the different classifiers in the first st</context>
</contexts>
<marker>Mohammad, Kiritchenko, 2014</marker>
<rawString>Saif M. Mohammad and Svetlana Kiritchenko. 2014. Using hashtags to capture fine emotion categories from tweets. Computational Intelligence, pages n/a–n/a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<pages>29--3</pages>
<contexts>
<context position="20454" citStr="Mohammad and Turney, 2013" startWordPosition="3418" endWordPosition="3422">sses — positive, negative, neutral) from the decision function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a default learning rate of 1. We have used Scikit Learn for building all the classifiers. Although we employ several </context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<journal>CoRR,</journal>
<pages>1308--6242</pages>
<contexts>
<context position="5747" citStr="Mohammad et al., 2013" startWordPosition="911" endWordPosition="914">esented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 2011), both of whom mined great benefits from the topic modelling paradigm. (Mohammad et al., 2013) achieved the best performance in Aspect Category Polarity Detection task in SemEval 2014 using various innovative linguistic features and publicly available sentiment lexica and two automatically comEntities RESTAURANT, FOOD, DRINKS, SERVICE, AMBIENCE, LOCATION Attributes GENERAL, PRICES, QUALITY, STYLE OPTIONS, MISCELLANEOUS Table 1: Entities and Attributes in Restaurants dataset. piled polarity lexica. (Brun et al., 2014) used information from its syntactic parser, BoW features, and an out-of-domain sentiment lexicon to train an SVM model. We have experimented with the techniques and featur</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-the-art in sentiment analysis of tweets. CoRR, abs/1308.6242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Subhabrata Mukherjee</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Feature specific sentiment analysis for product reviews.</title>
<date>2012</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>7181</volume>
<pages>475--487</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<contexts>
<context position="4372" citStr="Mukherjee and Bhattacharyya, 2012" startWordPosition="682" endWordPosition="685">using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), </context>
</contexts>
<marker>Mukherjee, Bhattacharyya, 2012</marker>
<rawString>Subhabrata Mukherjee and Pushpak Bhattacharyya. 2012. Feature specific sentiment analysis for product reviews. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 7181 of Lecture Notes in Computer Science, pages 475–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4944" citStr="Pang and Lee, 2004" startWordPosition="779" endWordPosition="782">attacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42Nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2830</pages>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825– 2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitris Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>27--35</pages>
<location>Dublin, Ireland,</location>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitris Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Suresh Manandhar</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Semeval-2015 task 12: Aspect based sentiment analysis.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado.</location>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Manandhar, Androutsopoulos, 2015</marker>
<rawString>Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta, Malta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<location>Dublin, Ireland,</location>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<contexts>
<context position="5051" citStr="Socher et al., 2013" startWordPosition="797" endWordPosition="800">xtract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the task at hand are works done by (Mcauley et al., 2012) and (Lakkaraju et al., 201</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiqiang Toh</author>
<author>Wenting Wang</author>
</authors>
<title>Dlirec: Aspect term extraction and term polarity classification system.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>235--240</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4734" citStr="Toh and Wang, 2014" startWordPosition="744" endWordPosition="747">re compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prio</context>
</contexts>
<marker>Toh, Wang, 2014</marker>
<rawString>Zhiqiang Toh and Wenting Wang. 2014. Dlirec: Aspect term extraction and term polarity classification system. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 235–240, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joachim Wagner</author>
</authors>
<title>Piyush Arora,</title>
<location>Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and</location>
<marker>Wagner, </marker>
<rawString>Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lamia Tounsi</author>
</authors>
<title>Dcu: Aspect-based polarity classification for semeval task 4.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>223--229</pages>
<location>Dublin, Ireland,</location>
<marker>Tounsi, 2014</marker>
<rawString>Lamia Tounsi. 2014. Dcu: Aspect-based polarity classification for semeval task 4. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223–229, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4970" citStr="Wilson et al., 2005" startWordPosition="784" endWordPosition="787">d Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguistic features and extra resources like Wordnet and word clusters. The task of Sentiment Analysis has been enriched with some of the seminal works like (Pang and Lee, 2004) and (Wilson et al., 2005), and has reached new heights with recent publications from (Socher et al., 2013) which combines grammatical cues with deep learning. (Carrascosa, 2014) presented innovative techniques of ensemble learning for the task of Sentiment Analysis, which we too have adopted in concept. (Bakliwal et al., 2012) presents a simple sentiment scoring function which uses prior information to classify and weight various sentiment bearing words/phrases in tweets. However, none of these works are crafted to handle Aspect based Sentiment Analysis and it is not trivial to adapt them for this task. Similar to the</context>
<context position="20393" citStr="Wilson et al., 2005" startWordPosition="3410" endWordPosition="3413"> sample, we obtain 3 scores (corresponding to three classes — positive, negative, neutral) from the decision function of each classifier. We use these confidence scores as features along with 15 other hand-crafted lexicon features for a linear Support Vector Machine classifier. We employ features such as number of positive tokens, number of negative tokens, total positive sentiment score, total negative sentiment score, sum of sentiment scores, maximum sentiment score, etc. from Sentiwordnet (Baccianella et al., 2010), Bing Liu’s opinion lexicon (Hu and Liu, 2004a), MPQA subjectivity lexicon (Wilson et al., 2005), NRC Emotion Association lexicon (Mohammad and Turney, 2013), Sentiment140 lexicon (Go et al., 2009), and NRC Hashtag Lexicon (Mohammad and Kiritchenko, 2014). For the final linear SVM classifier, we experimentally ascertain the optimal value of the parameter C to be 0.024. The linear SVM classifiers, in the first level of stacking, had a default value of 1.0 for parameter C. We did not have enough time to tune them, as we had many classifiers inside the main SVM classifier. The Ada Boost Classifier uses 100 decision tree estimators and a default learning rate of 1. We have used Scikit Learn </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM ’06,</booktitle>
<pages>43--50</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="4176" citStr="Zhuang et al., 2006" startWordPosition="655" endWordPosition="658">r Computational Linguistics tional features like word clusters learnt using various methods like LDA. (Hu and Liu, 2004b) initiated works on aspect identification in product reviews using an association rule based system. In his book (Liu, 2012) specifies four methods for aspect extraction, namely, frequent phrases, opinion and target relations, supervised learning and topic models. (Jakob and Gurevych, 2010) highlighted the use of Conditional Random Fields to extract the aspect terms and phrases and demonstrated a significant improvement in the F-Measure compared to then state-of-the-art by (Zhuang et al., 2006), which used a supervised approach to extract feature-opinion pairs. There are some approaches that utilize NLP semantics to extract aspect terms. Bhattacharyya (Mukherjee and Bhattacharyya, 2012) created a system to discover dependency parsing rules to extract opinion expressions. Many new works use hybrid approaches combining both NLP as well as statistical methods to create improved systems. In SemEval 2014, (Kiritchenko et al., 2014) used an in-house entity tagging system to find labels for Outside Term (O) and Aspect Term (T). (Toh and Wang, 2014) used tagging approach with more linguisti</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, CIKM ’06, pages 43–50, New York, NY, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>