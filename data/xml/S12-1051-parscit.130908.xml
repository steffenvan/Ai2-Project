<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000410">
<title confidence="0.890158">
SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity
</title>
<author confidence="0.920457">
Eneko Agirre
</author>
<affiliation confidence="0.931059">
University of the Basque Country
</affiliation>
<address confidence="0.6089">
Donostia, 20018, Basque Country
</address>
<email confidence="0.961083">
e.agirre@ehu.es
</email>
<author confidence="0.990969">
Daniel Cer
</author>
<affiliation confidence="0.7434105">
Stanford University
Stanford, CA 94305, USA
</affiliation>
<email confidence="0.986194">
danielcer@stanford.edu
</email>
<author confidence="0.995861">
Mona Diab Aitor Gonzalez-Agirre
</author>
<affiliation confidence="0.9971425">
Center for Computational Learning Systems University of the Basque Country
Columbia University Donostia, 20018, Basque Country
</affiliation>
<email confidence="0.997583">
mdiab@ccls.columbia.edu agonzalez278@ikasle.ehu.es
</email>
<sectionHeader confidence="0.998539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995687826086956">
Semantic Textual Similarity (STS) measures
the degree of semantic equivalence between
two texts. This paper presents the results of
the STS pilot task in Semeval. The training
data contained 2000 sentence pairs from pre-
viously existing paraphrase datasets and ma-
chine translation evaluation resources. The
test data also comprised 2000 sentences pairs
for those datasets, plus two surprise datasets
with 400 pairs from a different machine trans-
lation evaluation corpus and 750 pairs from a
lexical resource mapping exercise. The sim-
ilarity of pairs of sentences was rated on a
0-5 scale (low to high similarity) by human
judges using Amazon Mechanical Turk, with
high Pearson correlation scores, around 90%.
35 teams participated in the task, submitting
88 runs. The best results scored a Pearson
correlation &gt;80%, well above a simple lexical
baseline that only scored a 31% correlation.
This pilot task opens an exciting way ahead,
although there are still open issues, specially
the evaluation metric.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981677444444444">
Semantic Textual Similarity (STS) measures the
degree of semantic equivalence between two sen-
tences. STS is related to both Textual Entailment
(TE) and Paraphrase (PARA). STS is more directly
applicable in a number of NLP tasks than TE and
PARA such as Machine Translation and evaluation,
Summarization, Machine Reading, Deep Question
Answering, etc. STS differs from TE in as much as
it assumes symmetric graded equivalence between
the pair of textual snippets. In the case of TE the
equivalence is directional, e.g. a car is a vehicle, but
a vehicle is not necessarily a car. Additionally, STS
differs from both TE and PARA in that, rather than
being a binary yes/no decision (e.g. a vehicle is not a
car), STS incorporates the notion of graded semantic
similarity (e.g. a vehicle and a car are more similar
than a wave and a car).
STS provides a unified framework that allows for
an extrinsic evaluation of multiple semantic compo-
nents that otherwise have tended to be evaluated in-
dependently and without broad characterization of
their impact on NLP applications. Such components
include word sense disambiguation and induction,
lexical substitution, semantic role labeling, multi-
word expression detection and handling, anaphora
and coreference resolution, time and date resolution,
named-entity handling, underspecification, hedging,
semantic scoping and discourse analysis. Though
not in the scope of the current pilot task, we plan to
explore building an open source toolkit for integrat-
ing and applying diverse linguistic analysis modules
to the STS task.
While the characterization of STS is still prelim-
inary, we observed that there was no comparable
existing dataset extensively annotated for pairwise
semantic sentence similarity. We approached the
construction of the first STS dataset with the fol-
lowing goals: (1) To set a definition of STS as a
graded notion which can be easily communicated to
non-expert annotators beyond the likert-scale; (2) To
gather a substantial amount of sentence pairs from
diverse datasets, and to annotate them with high
quality; (3) To explore evaluation measures for STS;
(4) To explore the relation of STS to PARA and Ma-
chine Translation Evaluation exercises.
</bodyText>
<page confidence="0.98482">
385
</page>
<note confidence="0.9726605">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385–393,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9995815">
In the next section we present the various sources
of the STS data and the annotation procedure used.
Section 4 investigates the evaluation of STS sys-
tems. Section 5 summarizes the resources and tools
used by participant systems. Finally, Section 6
draws the conclusions.
</bodyText>
<sectionHeader confidence="0.989577" genericHeader="method">
2 Source Datasets
</sectionHeader>
<bodyText confidence="0.991614447058823">
Datasets for STS are scarce. Existing datasets in-
clude (Li et al., 2006) and (Lee et al., 2005). The
first dataset includes 65 sentence pairs which cor-
respond to the dictionary definitions for the 65
word pairs in Similarity(Rubenstein and Goode-
nough, 1965). The authors asked human informants
to assess the meaning of the sentence pairs on a
scale from 0.0 (minimum similarity) to 4.0 (maxi-
mum similarity). While the dataset is very relevant
to STS, it is too small to train, develop and test typ-
ical machine learning based systems. The second
dataset comprises 50 documents on news, ranging
from 51 to 126 words. Subjects were asked to judge
the similarity of document pairs on a five-point scale
(with 1.0 indicating “highly unrelated” and 5.0 indi-
cating “highly related”). This second dataset com-
prises a larger number of document pairs, but it goes
beyond sentence similarity into textual similarity.
When constructing our datasets, gathering natu-
rally occurring pairs of sentences with different de-
grees of semantic equivalence was a challenge in it-
self. If we took pairs of sentences at random, the
vast majority of them would be totally unrelated, and
only a very small fragment would show some sort of
semantic equivalence. Accordingly, we investigated
reusing a collection of existing datasets from tasks
that are related to STS.
We first studied the pairs of text from the Recog-
nizing TE challenge. The first editions of the chal-
lenge included pairs of sentences as the following:
T: The Christian Science Monitor named a US
journalist kidnapped in Iraq as freelancer Jill
Carroll.
H: Jill Carroll was abducted in Iraq.
The first sentence is the text, and the second is
the hypothesis. The organizers of the challenge an-
notated several pairs with a binary tag, indicating
whether the hypothesis could be entailed from the
text. Although these pairs of text are interesting we
decided to discard them from this pilot because the
length of the hypothesis was typically much shorter
than the text, and we did not want to bias the STS
task in this respect. We may, however, explore using
TE pairs for STS in the future.
Microsoft Research (MSR) has pioneered the ac-
quisition of paraphrases with two manually anno-
tated datasets. The first, called MSR Paraphrase
(MSRpar for short) has been widely used to evaluate
text similarity algorithms. It contains 5801 pairs of
sentences gleaned over a period of 18 months from
thousands of news sources on the web (Dolan et
al., 2004). 67% of the pairs were tagged as para-
phrases. The inter annotator agreement is between
82% and 84%. Complete meaning equivalence is
not required, and the annotation guidelines allowed
for some relaxation. The pairs which were anno-
tated as not being paraphrases ranged from com-
pletely unrelated semantically, to partially overlap-
ping, to those that were almost-but-not-quite seman-
tically equivalent. In this sense our graded annota-
tions enrich the dataset with more nuanced tags, as
we will see in the following section. We followed
the original split of 70% for training and 30% for
testing. A sample pair from the dataset follows:
The Senate Select Committee on Intelligence
is preparing a blistering report on prewar
intelligence on Iraq.
American intelligence leading up to the
war on Iraq will be criticized by a powerful
US Congressional committee due to report
soon, officials said today.
In order to construct a dataset which would reflect
a uniform distribution of similarity ranges, we sam-
pled the MSRpar dataset at certain ranks of string
similarity. We used the implementation readily ac-
cessible at CPAN1 of a well-known metric (Ukko-
nen, 1985). We sampled equal numbers of pairs
from five bands of similarity in the [0.4 .. 0.8] range
separately from the paraphrase and non-paraphrase
pairs. We sampled 1500 pairs overall, which we split
50% for training and 50% for testing.
The second dataset from MSR is the MSR Video
Paraphrase Corpus (MSRvid for short). The authors
showed brief video segments to Annotators from
Amazon Mechanical Turk (AMT) and were asked
</bodyText>
<footnote confidence="0.9990165">
1http://search.cpan.org/˜mlehmann/
String-Similarity-1.04/Similarity.pm
</footnote>
<page confidence="0.997733">
386
</page>
<figureCaption confidence="0.998108">
Figure 1: Video and corresponding descriptions from
MSRvid
Figure 2: Definition and instructions for annotation
</figureCaption>
<bodyText confidence="0.99981734375">
to provide a one-sentence description of the main ac-
tion or event in the video (Chen and Dolan, 2011).
Nearly 120 thousand sentences were collected for
2000 videos. The sentences can be taken to be
roughly parallel descriptions, and they included sen-
tences for many languages. Figure 1 shows a video
and corresponding descriptions.
The sampling procedure from this dataset is sim-
ilar to that for MSRpar. We construct two bags of
data to draw samples. The first includes all possible
pairs for the same video, and the second includes
pairs taken from different videos. Note that not all
sentences from the same video were equivalent, as
some descriptions were contradictory or unrelated.
Conversely, not all sentences coming from different
videos were necessarily unrelated, as many videos
were on similar topics. We took an equal number of
samples from each of these two sets, in an attempt to
provide a balanced dataset between equivalent and
non-equivalent pairs. The sampling was also done
according to string similarity, but in four bands in the
[0.5 .. 0.8] range, as sentences from the same video
had a usually higher string similarity than those in
the MSRpar dataset. We sampled 1500 pairs overall,
which we split 50% for training and 50% for testing.
Given the strong connection between STS sys-
tems and Machine Translation evaluation metrics,
we also sampled pairs of segments that had been
part of human evaluation exercises. Those pairs in-
cluded a reference translation and a automatic Ma-
chine Translation system submission, as follows:
The only instance in which no tax is levied is
when the supplier is in a non-EU country and
the recipient is in a Member State of the EU.
The only case for which no tax is still
perceived ”is an example of supply in the
European Community from a third country.
We selected pairs from the translation shared task
of the 2007 and 2008 ACL Workshops on Statistical
Machine Translation (WMT) (Callison-Burch et al.,
2007; Callison-Burch et al., 2008). For consistency,
we only used French to English system submissions.
The training data includes all of the Europarl human
ranked fr-en system submissions from WMT 2007,
with each machine translation being paired with the
correct reference translation. This resulted in 729
unique training pairs. The test data is comprised of
all Europarl human evaluated fr-en pairs from WMT
2008 that contain 16 white space delimited tokens or
less.
In addition, we selected two other datasets that
were used as out-of-domain testing. One of them
comprised of all the human ranked fr-en system
submissions from the WMT 2007 news conversa-
tion test set, resulting in 351 unique system refer-
ence pairs.2 The second set is radically different as
it comprised 750 pairs of glosses from OntoNotes
4.0 (Hovy et al., 2006) and WordNet 3.1 (Fellbaum,
1998) senses. The mapping of the senses of both re-
sources comprised 110K sense pairs. The similarity
between the sense pairs was generated using simple
word overlap. 50% of the pairs were sampled from
senses which were deemed as equivalent senses, the
rest from senses which did not map to one another.
</bodyText>
<sectionHeader confidence="0.995037" genericHeader="method">
3 Annotation
</sectionHeader>
<bodyText confidence="0.99997225">
In this first dataset we defined a straightforward lik-
ert scale ranging from 5 to 0, but we decided to pro-
vide definitions for each value in the scale (cf. Fig-
ure 2). We first did pilot annotations of 200 pairs se-
</bodyText>
<footnote confidence="0.962905">
2At the time of the shared task, this data set contained dupli-
cates resulting in 399 sentence pairs.
</footnote>
<page confidence="0.996841">
387
</page>
<bodyText confidence="0.99997921875">
lected at random from the three main datasets in the
training set. We did the annotation, and the pairwise
Pearson ranged from 84% to 87% among ourselves.
The agreement of each annotator with the average
scores of the other was between 87% and 89%.
In the future, we would like to explore whether
the definitions improve the consistency of the tag-
ging with respect to a likert scale without defini-
tions. Note also that in the assessment of the qual-
ity and evaluation of the systems performances, we
just took the resulting SS scores and their averages.
Using the qualitative descriptions for each score in
analysis and evaluation is left for future work.
Given the good results of the pilot we decided to
deploy the task in Amazon Mechanical Turk (AMT)
in order to crowd source the annotation task. The
turkers were required to have achieved a 95% of ap-
proval rating in their previous HITs, and had to pass
a qualification task which included 6 example pairs.
Each HIT included 5 pairs of sentences, and was
paid at 0.20$ each. We collected 5 annotations per
HIT. In the latest data collection, each HIT required
114.9 second for completion.
In order to ensure the quality, we also performed
post-hoc validation. Each HIT contained one pair
from our pilot. After the tagging was completed
we checked the correlation of each individual turker
with our scores, and removed annotations of turkers
which had low correlations (below 50%). Given the
high quality of the annotations among the turkers,
we could alternatively use the correlation between
the turkers itself to detect poor quality annotators.
</bodyText>
<sectionHeader confidence="0.985046" genericHeader="method">
4 Systems Evaluation
</sectionHeader>
<bodyText confidence="0.999988466666667">
Given two sentences, s1 and s2, an STS system
would need to return a similarity score. Participants
can also provide a confidence score indicating their
confidence level for the result returned for each pair,
but this confidence is not used for the main results.
The output of the systems performance is evaluated
using the Pearson product-moment correlation co-
efficient between the system scores and the human
scores, as customary in text similarity (Rubenstein
and Goodenough, 1965). We calculated Pearson for
each evaluation dataset separately.
In order to have a single Pearson measure for each
system we concatenated the gold standard (and sys-
tem outputs) for all 5 datasets into a single gold stan-
dard file (and single system output). The first ver-
sion of the results were published using this method,
but the overall score did not correspond well to the
individual scores in the datasets, and participants
proposed two additional evaluation metrics, both of
them based on Pearson correlation. The organizers
of the task decided that it was more informative, and
on the benefit of the community, to also adopt those
evaluation metrics, and the idea of having a single
main evaluation metric was dropped. This decision
was not without controversy, but the organizers gave
more priority to openness and inclusiveness and to
the involvement of participants. The final result ta-
ble thus included three evaluation metrics. For the
future we plan to analyze the evaluation metrics, in-
cluding non-parametric metrics like Spearman.
</bodyText>
<subsectionHeader confidence="0.939302">
4.1 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999001730769231">
The first evaluation metric is the Pearson correla-
tion for the concatenation of all five datasets, as de-
scribed above. We will use overall Pearson or sim-
ply ALL to refer to this measure.
The second evaluation metric normalizes the out-
put for each dataset separately, using the linear least
squares method. We concatenated the system results
for five datasets and then computed a single Pear-
son correlation. Given Y = {yi} and X = {xi}
(the gold standard scores and the system scores,
respectively), we transform the system scores into
X&apos; = {x&apos;i} in order to minimize the squared error
�i (yi − x&apos;i)2. The linear transformation is given by
x&apos;i = xi * 01 + 02, where 01 and 02 are found an-
alytically. We refer to this measure as Normalized
Pearson or simply ALLnorm. This metric was sug-
gested by one of the participants, Sergio Jimenez.
The third evaluation metric is the weighted mean
of the Pearson correlations on individual datasets.
The Pearson returned for each dataset is weighted
according to the number of sentence pairs in that
dataset. Given ri the five Pearson scores for
each dataset, and ni the number of pairs in each
dataset, the weighted mean is given as &amp;__1..5(ri *
ni)/ &amp;__1..5 ni We refer to this measure as weighted
mean of Pearson or Mean for short.
</bodyText>
<subsectionHeader confidence="0.998938">
4.2 Using confidence scores
</subsectionHeader>
<bodyText confidence="0.999862">
Participants were allowed to include a confidence
score between 1 and 100 for each of their scores.
We used weighted Pearson to use those confidence
</bodyText>
<page confidence="0.994483">
388
</page>
<bodyText confidence="0.9997664">
scores3. Table 2 includes the list of systems which
provided a non-uniform confidence. The results
show that some systems were able to improve their
correlation, showing promise for the usefulness of
confidence in applications.
</bodyText>
<subsectionHeader confidence="0.999668">
4.3 The Baseline System
</subsectionHeader>
<bodyText confidence="0.999967363636364">
We produced scores using a simple word overlap
baseline system. We tokenized the input sentences
splitting at white spaces, and then represented each
sentence as a vector in the multidimensional to-
ken space. Each dimension had 1 if the token was
present in the sentence, 0 otherwise. Similarity of
vectors was computed using cosine similarity.
We also run a random baseline several times,
yielding close to 0 correlations in all datasets, as ex-
pected. We will refer to the random baseline again
in Section 4.5.
</bodyText>
<subsectionHeader confidence="0.990367">
4.4 Participation
</subsectionHeader>
<bodyText confidence="0.999973769230769">
Participants could send a maximum of three system
runs. After downloading the test datasets, they had
a maximum of 120 hours to upload the results. 35
teams participated, submitting 88 system runs (cf.
first column of Table 1). Due to lack of space we
can’t detail the full names of authors and institutions
that participated. The interested reader can use the
name of the runs to find the relevant paper in these
proceedings.
There were several issues in the submissions. The
submission software did not ensure that the nam-
ing conventions were appropriately used, and this
caused some submissions to be missed, and in two
cases the results were wrongly assigned. Some par-
ticipants returned Not-a-Number as a score, and the
organizers had to request whether those where to be
taken as a 0 or as a 5.
Finally, one team submitted past the 120 hour
deadline and some teams sent missing files after the
deadline. All those are explicitly marked in Table 1.
The teams that included one of the organizers are
also explicitly marked. We want to stress that in
these teams the organizers did not allow the devel-
opers of the system to access any data or informa-
tion which was not available for the rest of partic-
ipants. One exception is weiwei, as they generated
</bodyText>
<footnote confidence="0.835781666666667">
3http://en.wikipedia.org/wiki/Pearson_
product-moment_correlation_coefficient#
Calculating_a_weighted_correlation
</footnote>
<bodyText confidence="0.989639833333333">
the 110K OntoNotes-WordNet dataset from which
the other organizers sampled the surprise data set.
After the submission deadline expired, the orga-
nizers published the gold standard in the task web-
site, in order to ensure a transparent evaluation pro-
cess.
</bodyText>
<sectionHeader confidence="0.534775" genericHeader="evaluation">
4.5 Results
</sectionHeader>
<bodyText confidence="0.999962181818182">
Table 1 shows the results for each run in alphabetic
order. Each result is followed by the rank of the sys-
tem according to the given evaluation measure. To
the right, the Pearson score for each dataset is given.
In boldface, the three best results in each column.
First of all we want to stress that the large majority
of the systems are well above the simple baseline,
although the baseline would rank 70 on the Mean
measure, improving over 19 runs.
The correlation for the non-MT datasets were re-
ally high: the highest correlation was obtained was
for MSRvid (0.88 r), followed by MSRpar (0.73 r)
and On-WN (0.73 r). The results for the MT evalu-
ation data are lower, (0.57 r) for SMT-eur and (0.61
r) for SMT-News. The simple token overlap base-
line, on the contrary, obtained the highest results
for On-WN (0.59 r), with (0.43 r) on MSRpar and
(0.40 r) on MSRvid. The results for MT evaluation
data are also reversed, with (0.40 r) for SMT-eur and
(0.45 r) for SMT-News.
The ALLnorm measure yields the highest corre-
lations. This comes at no surprise, as it involves a
normalization which transforms the system outputs
using the gold standard. In fact, a random base-
line which gets Pearson correlations close to 0 in all
datasets would attain Pearson of 0.58914.
Although not included in the results table for lack
of space, we also performed an analysis of confi-
dence intervals. For instance, the best run according
to ALL (r = .8239) has a 95% confidence interval of
[.8123,.8349] and the second a confidence interval
of [.8016,.8254], meaning that the differences are
not statistically different.
</bodyText>
<sectionHeader confidence="0.972242" genericHeader="evaluation">
5 Tools and resources used
</sectionHeader>
<bodyText confidence="0.998988666666667">
The organizers asked participants to submit a de-
scription file, special emphasis on the tools and re-
sources that they used. Table 3 shows in a simpli-
</bodyText>
<footnote confidence="0.964656">
4We run the random baseline 10 times. The mean is reported
here. The standard deviation is 0.0005
</footnote>
<page confidence="0.997852">
389
</page>
<figure confidence="0.995336911111111">
Run
00-baseline/task6-baseline
aca08ls/task6-University Of Sheffield-Hybrid
aca08ls/task6-University Of Sheffield-Machine Learning
aca08ls/task6-University Of Sheffield-Vector Space
acaputo/task6-UNIBA-DEPRI
acaputo/task6-UNIBA-LSARI
acaputo/task6-UNIBA-RI
baer/task6-UKP-run1
baer/task6-UKP-run2 plus postprocessing smt twsi
baer/task6-UKP-run3 plus random
croce/task6-UNITOR-1 REGRESSION BEST FEATURES
croce/task6-UNITOR-2 REGRESSION ALL FEATURES
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS
csjxu/task6-PolyUCOMP-RUN1
danielcer/stanford fsa†
danielcer/stanford pdaAll†
danielcer/stanford rte†
davide buscaldi/task6-IRIT-pg1
davide buscaldi/task6-IRIT-pg3
davide buscaldi/task6-IRIT-wu
demetrios glinos/task6-ATA-BASE
demetrios glinos/task6-ATA-CHNK
demetrios glinos/task6-ATA-STAT
desouza/task6-FBK-run1
desouza/task6-FBK-run2
desouza/task6-FBK-run3
dvilarinoayala/task6-BUAP-RUN-1
dvilarinoayala/task6-BUAP-RUN-2
dvilarinoayala/task6-BUAP-RUN-3
enrique/task6-UNED-H34measures
enrique/task6-UNED-HallMeasures
enrique/task6-UNED-SP INIST
georgiana dinu/task6-SAARLAND-ALIGN VSSIM
georgiana dinu/task6-SAARLAND-MIXT VSSIM
jan snajder/task6-takelab-simple
jan snajder/task6-takelab-syntax
janardhan/task6-janardhan-UNL matching
jhasneha/task6-Penn-ELReg
jhasneha/task6-Penn-ERReg
jhasneha/task6-Penn-LReg
jotacastillo/task6-SAGAN-RUN1
jotacastillo/task6-SAGAN-RUN2
jotacastillo/task6-SAGAN-RUN3
Konstantin Z/task6-ABBYY-General
M Rios/task6-UOW-LEX PARA
M Rios/task6-UOW-LEX PARA SEM
M Rios/task6-UOW-SEM
mheilman/task6-ETS-PERP
mheilman/task6-ETS-PERPphrases
mheilman/task6-ETS-TERp
nitish aggarwal/task6-aggarwal-run1,t
nitish aggarwal/task6-aggarwal-run2,t
nitish aggarwal/task6-aggarwal-run3
nmalandrakis/task6-DeepPurple-DeepPurple hierarchical
nmalandrakis/task6-DeepPurple-DeepPurple sigmoid
nmalandrakis/task6-DeepPurple-DeepPurple single
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach*
rada/task6-UNT-CombinedRegression
rada/task6-UNT-IndividualDecTree
rada/task6-UNT-IndividualRegression
sbdlrhmn/task6-sbdlrhmn-Run1
sbdlrhmn/task6-sbdlrhmn-Run2
sgjimenezv/task6-SOFT-CARDINALITY
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION
siva/task6-DSS-alignheuristic
siva/task6-DSS-average
siva/task6-DSS-wordsim
skamler /task6-EHU-RUN1v2,t†
sokolov/task6-LIMSI-cosprod
sokolov/task6-LIMSI-gradtree
sokolov/task6-LIMSI-sumdiff
spirin2/task6-UIUC-MLNLP-Blend
spirin2/task6-UIUC-MLNLP-CCM
spirin2/task6-UIUC-MLNLP-Puzzle
sranjans/task6-sranjans-1
sranjans/task6-sranjans-2
sranjans/task6-sranjans-3
tiantianzhu7/task6-tiantianzhu7-1
tiantianzhu7/task6-tiantianzhu7-2
tiantianzhu7/task6-tiantianzhu7-3
weiwei/task6-weiwei-run1,t†
yeh/task6-SRIUBC-SYSTEM1†
yeh/task6-SRIUBC-SYSTEM2†
yeh/task6-SRIUBC-SYSTEM3†
ygutierrez/task6-UMCC DLSI-MultiLex
ygutierrez/task6-UMCC DLSI-MultiSem
ygutierrez/task6-UMCC DLSI-MultiSemLex
yrkakde/task6-yrkakde-DiceWordnet
yrkakde/task6-yrkakde-JaccNERPenalty
</figure>
<table confidence="0.995186955555557">
ALL Rank ALLnrm Rank Mean Rank MSRpar MSRvid SMT-eur On-WN SMT-news
.3110 87 .6732 85 .4356 70 .4334 .2996 .4542 .5864 .3908
.6485 34 .8238 15 .6100 18 .5166 .8187 .4859 .6676 .4280
.7241 17 .8169 18 .5750 38 .5166 .8187 .4859 .6390 .2089
.6054 48 .7946 44 .5943 27 .5460 .7241 .4858 .6676 .4280
.6141 46 .8027 38 .5891 31 .4542 .7673 .5126 .6593 .4636
.6221 44 .8079 30 .5728 40 .3886 .7908 .4679 .6826 .4238
.6285 41 .7951 43 .5651 45 .4128 .7612 .4531 .6306 .4887
.8117 4 .8559 4 .6708 4 .6821 .8708 .5118 .6649 .4672
.8239 1 .8579 2 .6773 1 .6830 .8739 .5280 .6641 .4937
.7790 8 .8166 19 .4320 71 .6830 .8739 .5280 -.0620 -.0520
.7474 13 .8292 12 .6316 10 .5695 .8217 .5168 .6591 .4713
.7475 12 .8297 11 .6323 9 .5763 .8217 .5102 .6591 .4713
.6289 40 .8150 21 .5939 28 .4686 .8027 .4574 .6591 .4713
.6528 31 .7642 59 .5492 51 .4728 .6593 .4835 .6196 .4290
.6354 38 .7212 70 .4848 66 .3795 .5350 .4377 .6052 .4164
.4229 77 .7160 72 .5044 62 .4409 .4698 .4558 .6468 .4769
.5589 55 .7807 55 .4674 67 .4374 .8037 .3533 .3077 .3235
.4280 76 .7379 65 .5009 63 .4295 .6125 .4952 .5387 .3614
.4813 68 .7569 61 .5202 58 .4171 .6728 .5179 .5526 .3693
.4064 81 .7287 69 .4898 65 .4326 .5833 .4856 .5317 .3480
.3454 83 .6990 81 .2772 87 .1684 .6256 .2244 .1648 .0988
.4976 64 .7160 73 .3215 86 .2312 .6595 .1504 .2735 .1426
.4165 79 .7129 75 .3312 85 .1887 .6482 .2769 .2950 .1336
.5633 54 .7127 76 .3628 82 .2494 .6117 .1495 .4212 .2439
.6438 35 .8080 29 .5888 32 .5128 .7807 .3796 .6228 .5474
.6517 32 .8106 25 .6077 20 .5169 .7773 .4419 .6298 .6085
.4997 63 .7568 62 .5260 56 .4037 .6532 .4521 .6050 .4537
-.0260 89 .5933 89 .1016 89 .1109 .0057 .0348 .1788 .1964
.6630 25 .7474 64 .5105 59 .4018 .6378 .4758 .5691 .4057
.4381 75 .7518 63 .5577 48 .5328 .5788 .4785 .6692 .4465
.2791 88 .6694 87 .4286 72 .3861 .2570 .4086 .6006 .5305
.4680 69 .7625 60 .5615 47 .5166 .6303 .4625 .6442 .4753
.4952 65 .7871 50 .5065 60 .4043 .7718 .2686 .5721 .3505
.4548 71 .8258 13 .5662 43 .6310 .8312 .1391 .5966 .3806
.8133 3 .8635 1 .6753 2 .7343 .8803 .4771 .6797 .3989
.8138 2 .8569 3 .6601 5 .6985 .8620 .3612 .7049 .4683
.3431 84 .6878 84 .3481 83 .1936 .5504 .3755 .2888 .3387
.6622 27 .8048 34 .5654 44 .5480 .7844 .3513 .6040 .3607
.6573 28 .8083 28 .5755 37 .5610 .7857 .3568 .6214 .3732
.6497 33 .8043 36 .5699 41 .5460 .7818 .3547 .5969 .4137
.5522 57 .7904 47 .5906 29 .5659 .7113 .4739 .6542 .4253
.6272 42 .8032 37 .5838 34 .5538 .7706 .4480 .6135 .3894
.6311 39 .7943 45 .5649 46 .5394 .7560 .4181 .5904 .3746
.5636 53 .8052 33 .5759 36 .4797 .7821 .4576 .6488 .3682
.6397 36 .7187 71 .3825 80 .3628 .6426 .3074 .2806 .2082
.5981 49 .6955 82 .3473 84 .3529 .5724 .3066 .2643 .1164
.5361 59 .6287 88 .2567 88 .2995 .2910 .1611 .2571 .2212
.7808 7 .8064 32 .6305 11 .6211 .7210 .4722 .7080 .5149
.7834 6 .8089 27 .6399 7 .6397 .7200 .4850 .7124 .5312
.4477 73 .7291 68 .5253 57 .5049 .5217 .4748 .6169 .4566
.5777 52 .8158 20 .5466 52 .3675 .8427 .3534 .6030 .4430
.5833 51 .8183 17 .5683 42 .3720 .8330 .4238 .6513 .4499
.4911 67 .7696 57 .5377 53 .5320 .6874 .4514 .5827 .2818
.6228 43 .8100 26 .5979 23 .5984 .7717 .4292 .6480 .3702
.5540 56 .7997 41 .5558 50 .5960 .7616 .2628 .6016 .3446
.4918 66 .7646 58 .5061 61 .4989 .7092 .4437 .4879 .2441
.3880 82 .6706 86 .4111 76 .3427 .3549 .4271 .5298 .4034
.7418 14 .8406 7 .6159 14 .5032 .8695 .4797 .6715 .4033
.7677 9 .8389 9 .5947 25 .5693 .8688 .4203 .6491 .2256
.7846 5 .8440 6 .6162 13 .5353 .8750 .4203 .6715 .4033
.6663 23 .7842 53 .5376 54 .5440 .7335 .3830 .5860 .2445
.4169 78 .7104 77 .4986 64 .4617 .4489 .4719 .6353 .4353
.7331 15 .8526 5 .6708 3 .6405 .8562 .5152 .7109 .4833
.7107 19 .8397 8 .6486 6 .6316 .8237 .4320 .7109 .4833
.5253 60 .7962 42 .6030 21 .5735 .7123 .4781 .6984 .4177
.5490 58 .8047 35 .5943 26 .5020 .7645 .4875 .6677 .4324
.5130 61 .7895 49 .5287 55 .3765 .7761 .4161 .5728 .3964
.3129 86 .6935 83 .3889 79 .3605 .5187 .2259 .4098 .3465
.6392 37 .7344 67 .3940 78 .3948 .6597 .0143 .4157 .2889
.6789 22 .7377 66 .4118 75 .4848 .6636 .0934 .3706 .2455
.6196 45 .7101 78 .4131 74 .4295 .5724 .2842 .3989 .2575
.4592 70 .7800 56 .5782 35 .6523 .6691 .3566 .6117 .4603
.7269 16 .8217 16 .6104 17 .5769 .8203 .4667 .5835 .4945
.3216 85 .7857 51 .4376 69 .5635 .8056 .0630 .2774 .2409
.6529 30 .8018 39 .6249 12 .6124 .7240 .5581 .6703 .4533
.6651 24 .8128 22 .6366 8 .6254 .7538 .5328 .6649 .5036
.5045 62 .7846 52 .5905 30 .6167 .7061 .5666 .5664 .3968
.4533 72 .7134 74 .4192 73 .4184 .5630 .2083 .4822 .2745
.4157 80 .7099 79 .3960 77 .4260 .5628 .1546 .4552 .1923
.4446 74 .7097 80 .3740 81 .3411 .5946 .1868 .4029 .1823
.6946 20 .8303 10 .6081 19 .4106 .8351 .5128 .7273 .4383
.7513 11 .8017 40 .5997 22 .6084 .7458 .4688 .6315 .3994
.7562 10 .8111 24 .5858 33 .6050 .7939 .4294 .5871 .3366
.6876 21 .7812 54 .4668 68 .4791 .7901 .2159 .3843 .2801
.6630 26 .7922 46 .5560 49 .6022 .7709 .4435 .4327 .4264
.6529 29 .8115 23 .6116 16 .5269 .7756 .4688 .6539 .5470
.7213 18 .8239 14 .6158 15 .6205 .8104 .4325 .6256 .4340
.5977 50 .7902 48 .5742 39 .5294 .7470 .5531 .5698 .3659
.6067 47 .8078 31 .5955 24 .5757 .7765 .4989 .6257 .3468
</table>
<tableCaption confidence="0.963981">
Table 1: The first row corresponds to the baseline. ALL for overall Pearson, ALLnorm for Pearson after normaliza-
tion, and Mean for mean of Pearsons. We also show the ranks for each measure. Rightmost columns show Pearson for
each individual dataset. Note: * system submitted past the 120 hour window, * post-deadline fixes, † team involving
one of the organizers.
</tableCaption>
<page confidence="0.917294">
390
</page>
<table confidence="0.999767818181818">
Run ALL ALLw MSRpar MSRparw MSRvid MSRvidw SMT-eur SMT-eurw On-WN On-WNw SMT-news SMT-newsw
davide buscaldi/task6-IRIT-pg1 .4280 .4946 .4295 .4082 .6125 .6593 .4952 .5273 .5387 .5574 .3614 .4674
davide buscaldi/task6-IRIT-pg3 .4813 .5503 .4171 .4033 .6728 .7048 .5179 .5529 .5526 .5950 .3693 .4648
davide buscaldi/task6-IRIT-wu .4064 .4682 .4326 .4035 .5833 .6253 .4856 .5138 .5317 .5189 .3480 .4482
enrique/task6-UNED-H34measures .4381 .2615 .5328 .4494 .5788 .4913 .4785 .4660 .6692 .6440 .4465 .3632
enrique/task6-UNED-HallMeasures .2791 .2002 .3861 .3802 .2570 .2343 .4086 .4212 .6006 .5947 .5305 .4858
enrique/task6-UNED-SP INIST .4680 .3754 .5166 .5082 .6303 .5588 .4625 .4801 .6442 .5761 .4753 .4143
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach .3880 .3636 .3427 .3498 .3549 .3353 .4271 .3989 .5298 .4619 .4034 .3228
tiantianzhu7/task6-tiantianzhu7-1 .4533 .5442 .4184 .4241 .5630 .5630 .2083 .4220 .4822 .5031 .2745 .3536
tiantianzhu7/task6-tiantianzhu7-2 .4157 .5249 .4260 .4340 .5628 .5758 .1546 .4776 .4552 .4926 .1923 .3362
tiantianzhu7/task6-tiantianzhu7-3 .4446 .5229 .3411 .3611 .5946 .5899 .1868 .4769 .4029 .4365 .1823 .4014
</table>
<tableCaption confidence="0.9971265">
Table 2: Results according to weighted correlation for the systems that provided non-uniform confidence alongside
their scores.
</tableCaption>
<bodyText confidence="0.9999804">
fied way the tools and resources used by those par-
ticipants that did submit a valid description file. In
the last row, the totals show that WordNet was the
most used resource, followed by monolingual cor-
pora and Wikipedia. Acronyms, dictionaries, mul-
tilingual corpora, stopword lists and tables of para-
phrases were also used.
Generic NLP tools like lemmatization and PoS
tagging were widely used, and to a lesser extent,
parsing, word sense disambiguation, semantic role
labeling and time and date resolution (in this or-
der). Knowledge-based and distributional methods
got used nearly equally, and to a lesser extent, align-
ment and/or statistical machine translation software,
lexical substitution, string similarity, textual entail-
ment and machine translation evaluation software.
Machine learning was widely used to combine and
tune components. Several less used tools were also
listed but were used by three or less systems.
The top scoring systems tended to use most of
the resources and tools listed (UKP, Takelab), with
some notable exceptions like Sgjimenez which was
based on string similarity. For a more detailed anal-
ysis, the reader is directed to the papers of the par-
ticipants in this volume.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999957425">
This paper presents the SemEval 2012 pilot eval-
uation exercise on Semantic Textual Similarity. A
simple definition of STS beyond the likert-scale was
set up, and a wealth of annotated data was pro-
duced. The similarity of pairs of sentences was
rated on a 0-5 scale (low to high similarity) by hu-
man judges using Amazon Mechanical Turk. The
dataset includes 1500 sentence pairs from MSRpar
and MSRvid (each), ca. 1500 pairs from WMT,
and 750 sentence pairs from a mapping between
OntoNotes and WordNet senses. The correlation be-
tween non-expert annotators and annotations from
the authors is very high, showing the high quality of
the dataset. The dataset was split 50% as train and
test, with the exception of the surprise test datasets:
a subset of WMT from a different domain and the
OntoNotes-WordNet mapping. All datasets are pub-
licly available.5
The exercise was very successful in participation
and results. 35 teams participated, submitting 88
runs. The best results scored a Pearson correlation
over 80%, well beyond a simple lexical baseline
with 31% of correlation. The metric for evaluation
was not completely satisfactory, and three evalua-
tion metrics were finally published. We discuss the
shortcomings of those measures.
There are several tasks ahead in order to make
STS a mature field. The first is to find a satisfac-
tory evaluation metric. The second is to analyze the
definition of the task itself, with a thorough analysis
of the definitions in the likert scale.
We would also like to analyze the relation be-
tween the STS scores and the paraphrase judgements
in MSR, as well as the human evaluations in WMT.
Finally, we would also like to set up an open frame-
work where NLP components and similarity algo-
rithms can be combined by the community. All in
all, we would like this dataset to be the focus of the
community working on algorithmic approaches for
semantic processing and inference at large.
</bodyText>
<sectionHeader confidence="0.997753" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.991671">
We would like to thank all participants, specially (in al-
phabetic order) Yoan Gutierrez, Michael Heilman, Ser-
gio Jimenez, Nitin Madnami, Diana McCarthy and Shru-
tiranjan Satpathy for their contributions on evaluation
metrics. Eneko Agirre was partially funded by the
</bodyText>
<footnote confidence="0.998493">
5http://www.cs.york.ac.uk/semeval-2012/
task6/
</footnote>
<page confidence="0.987615">
391
</page>
<table confidence="0.997429909090909">
Acronyms Monolingual corpora Stop words Wilripedia KB Similarity Lexical Substitution MT evaluation POS tagger SMT Syntax Time and date resolution
Dictionaries Multilingual corpora Tables of paraphrases WordNet Lemmatizer Machine Learning MWE Semantic Role Labeling String similarity Textual entailment Word Sense Disambiguation
Distributional thesaurus Named Entity recognition Other
aca08ls/task6-University Of Sheffield-Hybrid x x x x x x x
aca08ls/task6-University Of Sheffield-Machine Learning x x x x x x x
aca08ls/task6-University Of Sheffield-Vector Space x x x x x
baer/task6-UKP-run1 x x x x x x x x x x x x x x
baer/task6-UKP-run2 plus postprocessing smt twsi x x x x x x x x x x x x x x
baer/task6-UKP-run3 plus random x x x x x x x x x x x x x x
croce/task6-UNITOR-1 REGRESSION BEST FEATURES x x x x x x
croce/task6-UNITOR-2 REGRESSION ALL FEATURES x x x x x x
croce/task6-UNITOR-3 REGRESSION ALL FEATURES ALL DOMAINS x x x x x x
csjxu/task6-PolyUCOMP-RUN x x x x
danielcer/stanford fsa x x x x x x x
danielcer/stanford pdaAll x x x x x x x
danielcer/stanford rte x x x x x x x x
davide buscaldi/task6-IRIT-pg1 x x x x x
davide buscaldi/task6-IRIT-pg3 x x x x x
davide buscaldi/task6-IRIT-wu x x x x x
demetrios glinos/task6-ATA-BASE x x x x x x x
demetrios glinos/task6-ATA-CHNK x x x x x x x
demetrios glinos/task6-ATA-STAT x x x x x x x
</table>
<equation confidence="0.812766326086956">
desouza/task6-FBK-run1 x x x x x x x x x x x x x
desouza/task6-FBK-run2 x x x x x x x x
desouza/task6-FBK-run3 x x x x x x
dvilarinoayala/task6-BUAP-RUN-1 x x
dvilarinoayala/task6-BUAP-RUN-2 x
dvilarinoayala/task6-BUAP-RUN-3 x x
jan snajder/task6-takelab-simple x x x x x x x x x x x x x
jan snajder/task6-takelab-syntax x x x x x x x x x
janardhan/task6-janardhan-UNL matching x x x x x x
jotacastillo/task6-SAGAN-RUN1 x x x x x x x x
jotacastillo/task6-SAGAN-RUN2 x x x x x x x x
jotacastillo/task6-SAGAN-RUN3 x x x x x x x x
Konstantin Z/task6-ABBYY-General
M Rios/task6-UOW-LEX PARA x x x x x x x x
M Rios/task6-UOW-LEX PARA SEM x x x x x x x x
M Rios/task6-UOW-SEM x x x x x x x
mheilman/task6-ETS-PERP x x x x x x x
mheilman/task6-ETS-PERPphrases x x x x x x x x x
mheilman/task6-ETS-TERp x x x x x x x
parthapakray/task6-JU CSE NLP-Semantic Syntactic Approach x x x x x x x x x x
rada/task6-UNT-CombinedRegression x x x x x x x x x
rada/task6-UNT-IndividualDecTree x x x x x x x x x
rada/task6-UNT-IndividualRegression x x x x x x x x x
sgjimenezv/task6-SOFT-CARDINALITY x x x
sgjimenezv/task6-SOFT-CARDINALITY-ONE-FUNCTION x x x
skamler /task6-EHU-RUN1v2 x x x x x
sokolov/task6-LIMSI-cosprod x x x x
sokolov/task6-LIMSI-gradtree x x x x
sokolov/task6-LIMSI-sumdiff x x x x
spirin2/task6-UIUC-MLNLP-Blend x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-CCM x x x x x x x x x x x
spirin2/task6-UIUC-MLNLP-Puzzle x x x x x x x x x x x
sranjans/task6-sranjans-1 x x x x x x x x
sranjans/task6-sranjans-2 x x x x x x x x x x x
sranjans/task6-sranjans-3 x x x x x x x x x x x
tiantianzhu7/task6-tiantianzhu7-1 x x x x
tiantianzhu7/task6-tiantianzhu7-2 x x x
tiantianzhu7/task6-tiantianzhu7-3 x x x x
weiwei/task6-weiwei-run1 x x x x x x
yeh/task6-SRIUBC-SYSTEM1 x x x x x x x
yeh/task6-SRIUBC-SYSTEM2 x x x x x x x
yeh/task6-SRIUBC-SYSTEM3 x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiLex x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSem x x x x x x x
ygutierrez/task6-UMCC DLSI-MultiSemLex x x x x x x x x
yrkakde/task6-yrkakde-DiceWordnet x x x
</equation>
<table confidence="0.595544">
Total 8 6 10 33 5 5 9 20 47 7 31 37 49 13 13 4 7 12 43 9 4 13 17 10 5 15 25
</table>
<tableCaption confidence="0.939599">
Table 3: Resources and tools used by the systems that submitted a description file. Leftmost columns correspond to
the resources, and rightmost to tools, in alphabetic order.
</tableCaption>
<page confidence="0.996294">
392
</page>
<bodyText confidence="0.6998008125">
European Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 270082
(PATHS project) and the Ministry of Economy under
grant TIN2009-14715-C04-01 (KNOW2 project). Daniel
Cer gratefully acknowledges the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Labora-
tory (AFRL) prime contract no. FA8750-09-C-0181 and
the support of the DARPA Broad Operational Language
Translation (BOLT) program through IBM. The STS an-
notations were funded by an extension to DARPA GALE
subcontract to IBM # W0853748 4911021461.0 to Mona
Diab. Any opinions, findings, and conclusion or recom-
mendations expressed in this material are those of the
author(s) and do not necessarily reflect the view of the
DARPA, AFRL, or the US government.
</bodyText>
<sectionHeader confidence="0.98934" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999904048780488">
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, StatMT ’07, pages 136–158.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, StatMT ’08, pages 70–106.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meetings of the Asso-
ciation for Computational Linguistics (ACL).
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In COLING
04: Proceedings of the 20th international conference
on Computational Linguistics, page 350.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Michael D. Lee, Brandon Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text doc-
ument similarity. In Proceedings of the 27th Annual
Conference of the Cognitive Science Society, pages
1254–1259, Mahwah, NJ.
Y. Li, D. McLean, Z. A. Bandar, J. D. O’Shea, and
K. Crockett. 2006. Sentence similarity based on se-
mantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138–
1150, August.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
E. Ukkonen. 1985. Algorithms for approximate string
matching. Information and Contro, 64:110–118.
</reference>
<page confidence="0.999371">
393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321119">
<title confidence="0.978764">SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity</title>
<author confidence="0.944733">Eneko</author>
<affiliation confidence="0.998249">University of the Basque</affiliation>
<address confidence="0.997926">Donostia, 20018, Basque</address>
<email confidence="0.938486">e.agirre@ehu.es</email>
<author confidence="0.785612">Daniel</author>
<affiliation confidence="0.938916">Stanford</affiliation>
<address confidence="0.99752">Stanford, CA 94305,</address>
<email confidence="0.999364">danielcer@stanford.edu</email>
<author confidence="0.995383">Mona Diab Aitor Gonzalez-Agirre</author>
<affiliation confidence="0.9852005">Center for Computational Learning Systems University of the Basque Country Columbia University Donostia, 20018, Basque Country</affiliation>
<note confidence="0.559451">mdiab@ccls.columbia.edu agonzalez278@ikasle.ehu.es</note>
<abstract confidence="0.99005775">Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07,</booktitle>
<pages>136--158</pages>
<contexts>
<context position="10347" citStr="Callison-Burch et al., 2007" startWordPosition="1642" endWordPosition="1645">ation metrics, we also sampled pairs of segments that had been part of human evaluation exercises. Those pairs included a reference translation and a automatic Machine Translation system submission, as follows: The only instance in which no tax is levied is when the supplier is in a non-EU country and the recipient is in a Member State of the EU. The only case for which no tax is still perceived ”is an example of supply in the European Community from a third country. We selected pairs from the translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2007; Callison-Burch et al., 2008). For consistency, we only used French to English system submissions. The training data includes all of the Europarl human ranked fr-en system submissions from WMT 2007, with each machine translation being paired with the correct reference translation. This resulted in 729 unique training pairs. The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less. In addition, we selected two other datasets that were used as out-of-domain testing. One of them comprised of all the human ranked fr-</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ’07, pages 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08,</booktitle>
<pages>70--106</pages>
<contexts>
<context position="10377" citStr="Callison-Burch et al., 2008" startWordPosition="1646" endWordPosition="1649">d pairs of segments that had been part of human evaluation exercises. Those pairs included a reference translation and a automatic Machine Translation system submission, as follows: The only instance in which no tax is levied is when the supplier is in a non-EU country and the recipient is in a Member State of the EU. The only case for which no tax is still perceived ”is an example of supply in the European Community from a third country. We selected pairs from the translation shared task of the 2007 and 2008 ACL Workshops on Statistical Machine Translation (WMT) (Callison-Burch et al., 2007; Callison-Burch et al., 2008). For consistency, we only used French to English system submissions. The training data includes all of the Europarl human ranked fr-en system submissions from WMT 2007, with each machine translation being paired with the correct reference translation. This resulted in 729 unique training pairs. The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less. In addition, we selected two other datasets that were used as out-of-domain testing. One of them comprised of all the human ranked fr-en system submissions from the</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, StatMT ’08, pages 70–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meetings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="8484" citStr="Chen and Dolan, 2011" startWordPosition="1332" endWordPosition="1335">ange separately from the paraphrase and non-paraphrase pairs. We sampled 1500 pairs overall, which we split 50% for training and 50% for testing. The second dataset from MSR is the MSR Video Paraphrase Corpus (MSRvid for short). The authors showed brief video segments to Annotators from Amazon Mechanical Turk (AMT) and were asked 1http://search.cpan.org/˜mlehmann/ String-Similarity-1.04/Similarity.pm 386 Figure 1: Video and corresponding descriptions from MSRvid Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). Nearly 120 thousand sentences were collected for 2000 videos. The sentences can be taken to be roughly parallel descriptions, and they included sentences for many languages. Figure 1 shows a video and corresponding descriptions. The sampling procedure from this dataset is similar to that for MSRpar. We construct two bags of data to draw samples. The first includes all possible pairs for the same video, and the second includes pairs taken from different videos. Note that not all sentences from the same video were equivalent, as some descriptions were contradictory or unrelated. Conversely, no</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meetings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In COLING 04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<contexts>
<context position="6621" citStr="Dolan et al., 2004" startWordPosition="1040" endWordPosition="1043"> text are interesting we decided to discard them from this pilot because the length of the hypothesis was typically much shorter than the text, and we did not want to bias the STS task in this respect. We may, however, explore using TE pairs for STS in the future. Microsoft Research (MSR) has pioneered the acquisition of paraphrases with two manually annotated datasets. The first, called MSR Paraphrase (MSRpar for short) has been widely used to evaluate text similarity algorithms. It contains 5801 pairs of sentences gleaned over a period of 18 months from thousands of news sources on the web (Dolan et al., 2004). 67% of the pairs were tagged as paraphrases. The inter annotator agreement is between 82% and 84%. Complete meaning equivalence is not required, and the annotation guidelines allowed for some relaxation. The pairs which were annotated as not being paraphrases ranged from completely unrelated semantically, to partially overlapping, to those that were almost-but-not-quite semantically equivalent. In this sense our graded annotations enrich the dataset with more nuanced tags, as we will see in the following section. We followed the original split of 70% for training and 30% for testing. A sampl</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>B. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In COLING 04: Proceedings of the 20th international conference on Computational Linguistics, page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11210" citStr="Fellbaum, 1998" startWordPosition="1783" endWordPosition="1784">the correct reference translation. This resulted in 729 unique training pairs. The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less. In addition, we selected two other datasets that were used as out-of-domain testing. One of them comprised of all the human ranked fr-en system submissions from the WMT 2007 news conversation test set, resulting in 351 unique system reference pairs.2 The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.1 (Fellbaum, 1998) senses. The mapping of the senses of both resources comprised 110K sense pairs. The similarity between the sense pairs was generated using simple word overlap. 50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another. 3 Annotation In this first dataset we defined a straightforward likert scale ranging from 5 to 0, but we decided to provide definitions for each value in the scale (cf. Figure 2). We first did pilot annotations of 200 pairs se2At the time of the shared task, this data set contained duplicates resultin</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="11177" citStr="Hovy et al., 2006" startWordPosition="1776" endWordPosition="1779">chine translation being paired with the correct reference translation. This resulted in 729 unique training pairs. The test data is comprised of all Europarl human evaluated fr-en pairs from WMT 2008 that contain 16 white space delimited tokens or less. In addition, we selected two other datasets that were used as out-of-domain testing. One of them comprised of all the human ranked fr-en system submissions from the WMT 2007 news conversation test set, resulting in 351 unique system reference pairs.2 The second set is radically different as it comprised 750 pairs of glosses from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.1 (Fellbaum, 1998) senses. The mapping of the senses of both resources comprised 110K sense pairs. The similarity between the sense pairs was generated using simple word overlap. 50% of the pairs were sampled from senses which were deemed as equivalent senses, the rest from senses which did not map to one another. 3 Annotation In this first dataset we defined a straightforward likert scale ranging from 5 to 0, but we decided to provide definitions for each value in the scale (cf. Figure 2). We first did pilot annotations of 200 pairs se2At the time of the shared task, this data </context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Lee</author>
<author>Brandon Pincombe</author>
<author>Matthew Welsh</author>
</authors>
<title>An empirical evaluation of models of text document similarity.</title>
<date>2005</date>
<booktitle>In Proceedings of the 27th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1254--1259</pages>
<location>Mahwah, NJ.</location>
<contexts>
<context position="4235" citStr="Lee et al., 2005" startWordPosition="642" endWordPosition="645"> STS to PARA and Machine Translation Evaluation exercises. 385 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385–393, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics In the next section we present the various sources of the STS data and the annotation procedure used. Section 4 investigates the evaluation of STS systems. Section 5 summarizes the resources and tools used by participant systems. Finally, Section 6 draws the conclusions. 2 Source Datasets Datasets for STS are scarce. Existing datasets include (Li et al., 2006) and (Lee et al., 2005). The first dataset includes 65 sentence pairs which correspond to the dictionary definitions for the 65 word pairs in Similarity(Rubenstein and Goodenough, 1965). The authors asked human informants to assess the meaning of the sentence pairs on a scale from 0.0 (minimum similarity) to 4.0 (maximum similarity). While the dataset is very relevant to STS, it is too small to train, develop and test typical machine learning based systems. The second dataset comprises 50 documents on news, ranging from 51 to 126 words. Subjects were asked to judge the similarity of document pairs on a five-point sc</context>
</contexts>
<marker>Lee, Pincombe, Welsh, 2005</marker>
<rawString>Michael D. Lee, Brandon Pincombe, and Matthew Welsh. 2005. An empirical evaluation of models of text document similarity. In Proceedings of the 27th Annual Conference of the Cognitive Science Society, pages 1254–1259, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>D McLean</author>
<author>Z A Bandar</author>
<author>J D O’Shea</author>
<author>K Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<pages>1150</pages>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Y. Li, D. McLean, Z. A. Bandar, J. D. O’Shea, and K. Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138– 1150, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="4397" citStr="Rubenstein and Goodenough, 1965" startWordPosition="666" endWordPosition="670"> Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics In the next section we present the various sources of the STS data and the annotation procedure used. Section 4 investigates the evaluation of STS systems. Section 5 summarizes the resources and tools used by participant systems. Finally, Section 6 draws the conclusions. 2 Source Datasets Datasets for STS are scarce. Existing datasets include (Li et al., 2006) and (Lee et al., 2005). The first dataset includes 65 sentence pairs which correspond to the dictionary definitions for the 65 word pairs in Similarity(Rubenstein and Goodenough, 1965). The authors asked human informants to assess the meaning of the sentence pairs on a scale from 0.0 (minimum similarity) to 4.0 (maximum similarity). While the dataset is very relevant to STS, it is too small to train, develop and test typical machine learning based systems. The second dataset comprises 50 documents on news, ranging from 51 to 126 words. Subjects were asked to judge the similarity of document pairs on a five-point scale (with 1.0 indicating “highly unrelated” and 5.0 indicating “highly related”). This second dataset comprises a larger number of document pairs, but it goes bey</context>
<context position="13944" citStr="Rubenstein and Goodenough, 1965" startWordPosition="2244" endWordPosition="2247">ns among the turkers, we could alternatively use the correlation between the turkers itself to detect poor quality annotators. 4 Systems Evaluation Given two sentences, s1 and s2, an STS system would need to return a similarity score. Participants can also provide a confidence score indicating their confidence level for the result returned for each pair, but this confidence is not used for the main results. The output of the systems performance is evaluated using the Pearson product-moment correlation coefficient between the system scores and the human scores, as customary in text similarity (Rubenstein and Goodenough, 1965). We calculated Pearson for each evaluation dataset separately. In order to have a single Pearson measure for each system we concatenated the gold standard (and system outputs) for all 5 datasets into a single gold standard file (and single system output). The first version of the results were published using this method, but the overall score did not correspond well to the individual scores in the datasets, and participants proposed two additional evaluation metrics, both of them based on Pearson correlation. The organizers of the task decided that it was more informative, and on the benefit </context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Ukkonen</author>
</authors>
<title>Algorithms for approximate string matching.</title>
<date>1985</date>
<journal>Information and Contro,</journal>
<pages>64--110</pages>
<contexts>
<context position="7776" citStr="Ukkonen, 1985" startWordPosition="1227" endWordPosition="1229">inal split of 70% for training and 30% for testing. A sample pair from the dataset follows: The Senate Select Committee on Intelligence is preparing a blistering report on prewar intelligence on Iraq. American intelligence leading up to the war on Iraq will be criticized by a powerful US Congressional committee due to report soon, officials said today. In order to construct a dataset which would reflect a uniform distribution of similarity ranges, we sampled the MSRpar dataset at certain ranks of string similarity. We used the implementation readily accessible at CPAN1 of a well-known metric (Ukkonen, 1985). We sampled equal numbers of pairs from five bands of similarity in the [0.4 .. 0.8] range separately from the paraphrase and non-paraphrase pairs. We sampled 1500 pairs overall, which we split 50% for training and 50% for testing. The second dataset from MSR is the MSR Video Paraphrase Corpus (MSRvid for short). The authors showed brief video segments to Annotators from Amazon Mechanical Turk (AMT) and were asked 1http://search.cpan.org/˜mlehmann/ String-Similarity-1.04/Similarity.pm 386 Figure 1: Video and corresponding descriptions from MSRvid Figure 2: Definition and instructions for anno</context>
</contexts>
<marker>Ukkonen, 1985</marker>
<rawString>E. Ukkonen. 1985. Algorithms for approximate string matching. Information and Contro, 64:110–118.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>