<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044115">
<title confidence="0.9986725">
Extracting Protein-Protein Interaction based on Discriminative Training of
the Hidden Vector State Model
</title>
<author confidence="0.97885">
Deyu Zhou and Yulan He
</author>
<affiliation confidence="0.973296">
Informatics Research Centre, The University of Reading, Reading RG6 6BX, UK
</affiliation>
<email confidence="0.996512">
Email:d.zhou@reading.ac.uk, y.he@reading.ac.uk
</email>
<sectionHeader confidence="0.999633" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919851851852">
The knowledge about gene clusters and protein in-
teractions is important for biological researchers
to unveil the mechanism of life. However, large
quantity of the knowledge often hides in the liter-
ature, such as journal articles, reports, books and
so on. Many approaches focusing on extracting in-
formation from unstructured text, such as pattern
matching, shallow and deep parsing, have been pro-
posed especially for extracting protein-protein inter-
actions (Zhou and He, 2008).
A semantic parser based on the Hidden Vector
State (HVS) model for extracting protein-protein in-
teractions is presented in (Zhou et al., 2008). The
HVS model is an extension of the basic discrete
Markov model in which context is encoded as a
stack-oriented state vector. Maximum Likelihood
estimation (MLE) is used to derive the parameters
of the HVS model. In this paper, we propose a dis-
criminative approach based on parse error measure
to train the HVS model. To adjust the HVS model to
achieve minimum parse error rate, the generalized
probabilistic descent (GPD) algorithm (Kuo et al.,
2002) is used. Experiments have been conducted on
the GENIA corpus. The results demonstrate mod-
est improvements when the discriminatively trained
HVS model outperforms its MLE trained counter-
part by 2.5% in F-measure on the GENIA corpus.
</bodyText>
<sectionHeader confidence="0.992251" genericHeader="keywords">
2 Methodologies
</sectionHeader>
<bodyText confidence="0.999109318181818">
The Hidden Vector State (HVS) model (He and
Young, 2005) is a discrete Hidden Markov Model
(HMM) in which each HMM state represents the
state of a push-down automaton with a finite stack
size.
Normally, MLE is used for generative probabil-
ity model training in which only the correct model
needs to be updated during training. It is be-
lieved that improvement can be achieved by train-
ing the generative model based on a discriminative
optimization criteria (Klein and Manning, 2002) in
which the training procedure is designed to maxi-
mize the conditional probability of the parses given
the sentences in the training corpus. That is, not only
the likelihood for the correct model should be in-
creased but also the likelihood for the incorrect mod-
els should be decreased.
Assuming the most likely semantic parse tree
C = Cj and there are altogether M semantic parse
hypotheses for a particular sentence W, a parse er-
ror measure (Juang et al., 1993; Chou et al., 1993;
Chen and Soong, 1994) can be defined as
</bodyText>
<equation confidence="0.986709">
d(W) = − log P(W, Cj) + log[
</equation>
<bodyText confidence="0.999098692307692">
where q is a positive number and is used to se-
lect competing semantic parses. When q = 1,
the competing semantic parse term is the average
of all the competing semantic parse scores. When
q —* oc, the competing semantic parse term be-
P(W, Ci) which is the score for the top
competing semantic parse. By varying the value of
q, we can take all the competing semantic parses into
consideration. d(W) &gt; 0 implies classification er-
ror and d(W) &lt; 0 implies correct decision.
The sigmoid function can be used to normalize
d(W) in a smooth zero-one range and the loss func-
tion is thus defined as (Juang et al., 1993):
</bodyText>
<equation confidence="0.9405514">
C(W) = sigmoid(d(W)) (2)
1 � 1
M − 1 i�i��j P(W, Ci)&amp;quot;1 &amp;quot; (1)
comes max
i.i74j
</equation>
<page confidence="0.944702">
98
</page>
<note confidence="0.600565">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 98–99,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.991318">
where
</bodyText>
<equation confidence="0.89771">
1
sigmoid(x) = 1 + e−γx (3)
Here, -y is a constant which controls the slope of the
</equation>
<bodyText confidence="0.8847315">
sigmoid function.
The update formula is given by:
</bodyText>
<equation confidence="0.992031">
λk+1 = λk − 2kV`(Wi, λk) (4)
</equation>
<bodyText confidence="0.9898645">
where ck is the step size.
Using the definition of E(Wi, Ak) and after work-
ing out the mathematics, we get the update formu-
lae 5, 6, 7,
</bodyText>
<equation confidence="0.997305933333333">
3log P(n|c0)´∗ = log P(n|c0) − 2γ`(di)(1 − `(di))
⎛ ⎞
X P(Wi, Ci, λ)η
⎝−I(Cj, n, c0) + I(Ci, n, c0) ⎠ (5)
i,ii4j Pi,ii4j P(Wi, Ci, λ)η
(log P(c[1]|c[2..D]))∗ = log P(c[1]|c[2..D]) − 2γ`(di)(1 − `(di))
⎛ ⎞
X P(Wi, Ci, λ)η
⎝−I(Cj, c[1], c[2..D]) + I(Ci, c[1], c[2..D])
i,i j4j Pi,ii4j P(Wi, Ci, λ)η ⎠
(log P(w|c))∗ = log P(w|c) − 2γ`(di)(1 − `(di))
⎛ ⎞
X P(Wi, Ci, λ)η
⎝−I(Cj, w, c) + I(Ci, w, c) ⎠
i,i j4j Pi,ii4j P(Wi, Ci, λ)η
</equation>
<bodyText confidence="0.9999785">
where I(Ci, n, c&apos;) denotes the number of times
the operation of popping up n semantic tags at
the current vector state c&apos; in the Ci parse tree,
I(Ci, c[1], c[2..D]) denotes the number of times the
operation of pushing the semantic tag c[1] at the cur-
rent vector state c[2..D] in the Ci parse tree and
I(Ci, w, c) denotes the number of times of emitting
the word w at the state c in the parse tree Ci.
</bodyText>
<sectionHeader confidence="0.944873" genericHeader="introduction">
3 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.998228181818182">
GENIA (Kim et al., 2003) is a collection of 2000 re-
search abstracts selected from the search results of
MEDLINE database using keywords (MESH terms)
“human, blood cells and transcription factors”. All
these abstracts were then split into sentences and
those containing more than two protein names and
at least one interaction keyword were kept. Alto-
gether 3533 sentences were left and 2500 sentences
were sampled to build our data set.
The results using MLE and discriminative train-
ing are listed in Table 1. Discriminative training
</bodyText>
<note confidence="0.895714">
improves on the MLE by relatively 2.5% where N
</note>
<tableCaption confidence="0.9755345">
Table 1: Performance comparison of MLE versus Dis-
criminative training
</tableCaption>
<table confidence="0.9717502">
Measurement GENIA
MLE Discriminative
Recall 61.78% 64.59%
Precision 61.16% 61.51%
F-measure 61.47% 63.01%
</table>
<bodyText confidence="0.997383">
and I are set to 5 and 200 individually. Here N de-
notes the number of semantic parse hypotheses and
I denotes the the number of sentences in the training
data.
</bodyText>
<sectionHeader confidence="0.998854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998092702702703">
J.K. Chen and F.K. Soong. 1994. An n-best candidates-
based discriminative training for speech recognition
applications. IEEE Transactions on Speech and Audio
Processing, 2:206 – 216.
W. Chou, C.H. Lee, and B.H. Juang. 1993. Minimum
(6) error rate training based on n-best string models. In
Acoustics, Speech, and Signal Processing, IEEE Inter-
national Conference on ICASSP ’93, volume 2, pages
652 – 655.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech and
Language, 19(1):85–106.
B.H. Juang, W. Chou, and C.H. Lee. 1993. Statistical
and discriminative methods for speech recognition. In
Rubio, editor, Speech Recognition and Understanding,
NATO ASI Series, Berlin. Springer-Verlag.
JD. Kim, T. Ohta, Y. Tateisi, and J Tsujii. 2003. GE-
NIA corpus–semantically annotated corpus for bio-
textmining. Bioinformatics, 19(Suppl 1):i180–2.
D. Klein and C. D. Manning. 2002. Conditional struc-
ture versus conditional estimation in nlp models. In
Proc. the ACL-02 conference on Empirical methods in
natural language processing, pages 9–16, University
of Pennsylvania, PA.
H.-K.J. Kuo, E. Fosle-Lussier, H. Jiang, and C.H. Lee.
2002. Discriminative training of language models
for speech recognition. In Acoustics, Speech, and
Signal Processing, IEEE International Conference on
ICASSP ’02, volume 1, pages 325 – 328.
Deyu Zhou and Yulan He. 2008. Extracting Interac-
tions between Proteins from the Literature. Journal
of Biomedical Informatics, 41:393–407.
Deyu Zhou, Yulan He, and Chee Keong Kwoh. 2008.
Extracting Protein-Protein Interactions from the Liter-
ature using the Hidden Vector State Model. Interna-
tional Journal of Bioinformatics Research and Appli-
cations, 4(1):64–80.
</reference>
<figure confidence="0.423641">
(7)
</figure>
<page confidence="0.97529">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629343">
<title confidence="0.890334">Extracting Protein-Protein Interaction based on Discriminative Training the Hidden Vector State Model</title>
<author confidence="0.972492">Deyu Zhou</author>
<author confidence="0.972492">Yulan He</author>
<affiliation confidence="0.875118">Informatics Research Centre, The University of Reading, Reading RG6 6BX,</affiliation>
<email confidence="0.924239">d.zhou@reading.ac.uk,y.he@reading.ac.uk</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Chen</author>
<author>F K Soong</author>
</authors>
<title>An n-best candidatesbased discriminative training for speech recognition applications.</title>
<date>1994</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<pages>2--206</pages>
<contexts>
<context position="2574" citStr="Chen and Soong, 1994" startWordPosition="409" endWordPosition="412"> achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus. That is, not only the likelihood for the correct model should be increased but also the likelihood for the incorrect models should be decreased. Assuming the most likely semantic parse tree C = Cj and there are altogether M semantic parse hypotheses for a particular sentence W, a parse error measure (Juang et al., 1993; Chou et al., 1993; Chen and Soong, 1994) can be defined as d(W) = − log P(W, Cj) + log[ where q is a positive number and is used to select competing semantic parses. When q = 1, the competing semantic parse term is the average of all the competing semantic parse scores. When q —* oc, the competing semantic parse term beP(W, Ci) which is the score for the top competing semantic parse. By varying the value of q, we can take all the competing semantic parses into consideration. d(W) &gt; 0 implies classification error and d(W) &lt; 0 implies correct decision. The sigmoid function can be used to normalize d(W) in a smooth zero-one range and t</context>
</contexts>
<marker>Chen, Soong, 1994</marker>
<rawString>J.K. Chen and F.K. Soong. 1994. An n-best candidatesbased discriminative training for speech recognition applications. IEEE Transactions on Speech and Audio Processing, 2:206 – 216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chou</author>
<author>C H Lee</author>
<author>B H Juang</author>
</authors>
<title>Minimum (6) error rate training based on n-best string models.</title>
<date>1993</date>
<booktitle>In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’93,</booktitle>
<volume>2</volume>
<pages>652--655</pages>
<contexts>
<context position="2551" citStr="Chou et al., 1993" startWordPosition="405" endWordPosition="408"> improvement can be achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus. That is, not only the likelihood for the correct model should be increased but also the likelihood for the incorrect models should be decreased. Assuming the most likely semantic parse tree C = Cj and there are altogether M semantic parse hypotheses for a particular sentence W, a parse error measure (Juang et al., 1993; Chou et al., 1993; Chen and Soong, 1994) can be defined as d(W) = − log P(W, Cj) + log[ where q is a positive number and is used to select competing semantic parses. When q = 1, the competing semantic parse term is the average of all the competing semantic parse scores. When q —* oc, the competing semantic parse term beP(W, Ci) which is the score for the top competing semantic parse. By varying the value of q, we can take all the competing semantic parses into consideration. d(W) &gt; 0 implies classification error and d(W) &lt; 0 implies correct decision. The sigmoid function can be used to normalize d(W) in a smoo</context>
</contexts>
<marker>Chou, Lee, Juang, 1993</marker>
<rawString>W. Chou, C.H. Lee, and B.H. Juang. 1993. Minimum (6) error rate training based on n-best string models. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’93, volume 2, pages 652 – 655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>S Young</author>
</authors>
<title>Semantic processing using the hidden vector state model. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="1645" citStr="He and Young, 2005" startWordPosition="249" endWordPosition="252">elihood estimation (MLE) is used to derive the parameters of the HVS model. In this paper, we propose a discriminative approach based on parse error measure to train the HVS model. To adjust the HVS model to achieve minimum parse error rate, the generalized probabilistic descent (GPD) algorithm (Kuo et al., 2002) is used. Experiments have been conducted on the GENIA corpus. The results demonstrate modest improvements when the discriminatively trained HVS model outperforms its MLE trained counterpart by 2.5% in F-measure on the GENIA corpus. 2 Methodologies The Hidden Vector State (HVS) model (He and Young, 2005) is a discrete Hidden Markov Model (HMM) in which each HMM state represents the state of a push-down automaton with a finite stack size. Normally, MLE is used for generative probability model training in which only the correct model needs to be updated during training. It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus. That is, not only the likelihood </context>
</contexts>
<marker>He, Young, 2005</marker>
<rawString>Y. He and S. Young. 2005. Semantic processing using the hidden vector state model. Computer Speech and Language, 19(1):85–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Juang</author>
<author>W Chou</author>
<author>C H Lee</author>
</authors>
<title>Statistical and discriminative methods for speech recognition.</title>
<date>1993</date>
<booktitle>Speech Recognition and Understanding, NATO ASI Series,</booktitle>
<editor>In Rubio, editor,</editor>
<publisher>Springer-Verlag.</publisher>
<location>Berlin.</location>
<contexts>
<context position="2532" citStr="Juang et al., 1993" startWordPosition="401" endWordPosition="404"> It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus. That is, not only the likelihood for the correct model should be increased but also the likelihood for the incorrect models should be decreased. Assuming the most likely semantic parse tree C = Cj and there are altogether M semantic parse hypotheses for a particular sentence W, a parse error measure (Juang et al., 1993; Chou et al., 1993; Chen and Soong, 1994) can be defined as d(W) = − log P(W, Cj) + log[ where q is a positive number and is used to select competing semantic parses. When q = 1, the competing semantic parse term is the average of all the competing semantic parse scores. When q —* oc, the competing semantic parse term beP(W, Ci) which is the score for the top competing semantic parse. By varying the value of q, we can take all the competing semantic parses into consideration. d(W) &gt; 0 implies classification error and d(W) &lt; 0 implies correct decision. The sigmoid function can be used to norma</context>
</contexts>
<marker>Juang, Chou, Lee, 1993</marker>
<rawString>B.H. Juang, W. Chou, and C.H. Lee. 1993. Statistical and discriminative methods for speech recognition. In Rubio, editor, Speech Recognition and Understanding, NATO ASI Series, Berlin. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta Kim</author>
<author>Y Tateisi</author>
<author>J Tsujii</author>
</authors>
<title>GENIA corpus–semantically annotated corpus for biotextmining. Bioinformatics,</title>
<date>2003</date>
<contexts>
<context position="4667" citStr="Kim et al., 2003" startWordPosition="812" endWordPosition="815">,i j4j Pi,ii4j P(Wi, Ci, λ)η ⎠ (log P(w|c))∗ = log P(w|c) − 2γ`(di)(1 − `(di)) ⎛ ⎞ X P(Wi, Ci, λ)η ⎝−I(Cj, w, c) + I(Ci, w, c) ⎠ i,i j4j Pi,ii4j P(Wi, Ci, λ)η where I(Ci, n, c&apos;) denotes the number of times the operation of popping up n semantic tags at the current vector state c&apos; in the Ci parse tree, I(Ci, c[1], c[2..D]) denotes the number of times the operation of pushing the semantic tag c[1] at the current vector state c[2..D] in the Ci parse tree and I(Ci, w, c) denotes the number of times of emitting the word w at the state c in the parse tree Ci. 3 Experimental Setup and Results GENIA (Kim et al., 2003) is a collection of 2000 research abstracts selected from the search results of MEDLINE database using keywords (MESH terms) “human, blood cells and transcription factors”. All these abstracts were then split into sentences and those containing more than two protein names and at least one interaction keyword were kept. Altogether 3533 sentences were left and 2500 sentences were sampled to build our data set. The results using MLE and discriminative training are listed in Table 1. Discriminative training improves on the MLE by relatively 2.5% where N Table 1: Performance comparison of MLE versu</context>
</contexts>
<marker>Kim, Tateisi, Tsujii, 2003</marker>
<rawString>JD. Kim, T. Ohta, Y. Tateisi, and J Tsujii. 2003. GENIA corpus–semantically annotated corpus for biotextmining. Bioinformatics, 19(Suppl 1):i180–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in nlp models.</title>
<date>2002</date>
<booktitle>In Proc. the ACL-02 conference on Empirical methods in natural language processing,</booktitle>
<pages>9--16</pages>
<location>University of Pennsylvania, PA.</location>
<contexts>
<context position="2069" citStr="Klein and Manning, 2002" startWordPosition="320" endWordPosition="323">s when the discriminatively trained HVS model outperforms its MLE trained counterpart by 2.5% in F-measure on the GENIA corpus. 2 Methodologies The Hidden Vector State (HVS) model (He and Young, 2005) is a discrete Hidden Markov Model (HMM) in which each HMM state represents the state of a push-down automaton with a finite stack size. Normally, MLE is used for generative probability model training in which only the correct model needs to be updated during training. It is believed that improvement can be achieved by training the generative model based on a discriminative optimization criteria (Klein and Manning, 2002) in which the training procedure is designed to maximize the conditional probability of the parses given the sentences in the training corpus. That is, not only the likelihood for the correct model should be increased but also the likelihood for the incorrect models should be decreased. Assuming the most likely semantic parse tree C = Cj and there are altogether M semantic parse hypotheses for a particular sentence W, a parse error measure (Juang et al., 1993; Chou et al., 1993; Chen and Soong, 1994) can be defined as d(W) = − log P(W, Cj) + log[ where q is a positive number and is used to sel</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. Conditional structure versus conditional estimation in nlp models. In Proc. the ACL-02 conference on Empirical methods in natural language processing, pages 9–16, University of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-K J Kuo</author>
<author>E Fosle-Lussier</author>
<author>H Jiang</author>
<author>C H Lee</author>
</authors>
<title>Discriminative training of language models for speech recognition.</title>
<date>2002</date>
<booktitle>In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’02,</booktitle>
<volume>1</volume>
<pages>325--328</pages>
<contexts>
<context position="1340" citStr="Kuo et al., 2002" startWordPosition="201" endWordPosition="204">(Zhou and He, 2008). A semantic parser based on the Hidden Vector State (HVS) model for extracting protein-protein interactions is presented in (Zhou et al., 2008). The HVS model is an extension of the basic discrete Markov model in which context is encoded as a stack-oriented state vector. Maximum Likelihood estimation (MLE) is used to derive the parameters of the HVS model. In this paper, we propose a discriminative approach based on parse error measure to train the HVS model. To adjust the HVS model to achieve minimum parse error rate, the generalized probabilistic descent (GPD) algorithm (Kuo et al., 2002) is used. Experiments have been conducted on the GENIA corpus. The results demonstrate modest improvements when the discriminatively trained HVS model outperforms its MLE trained counterpart by 2.5% in F-measure on the GENIA corpus. 2 Methodologies The Hidden Vector State (HVS) model (He and Young, 2005) is a discrete Hidden Markov Model (HMM) in which each HMM state represents the state of a push-down automaton with a finite stack size. Normally, MLE is used for generative probability model training in which only the correct model needs to be updated during training. It is believed that impro</context>
</contexts>
<marker>Kuo, Fosle-Lussier, Jiang, Lee, 2002</marker>
<rawString>H.-K.J. Kuo, E. Fosle-Lussier, H. Jiang, and C.H. Lee. 2002. Discriminative training of language models for speech recognition. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP ’02, volume 1, pages 325 – 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyu Zhou</author>
<author>Yulan He</author>
</authors>
<title>Extracting Interactions between Proteins from the Literature.</title>
<date>2008</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>41--393</pages>
<contexts>
<context position="742" citStr="Zhou and He, 2008" startWordPosition="102" endWordPosition="105"> He Informatics Research Centre, The University of Reading, Reading RG6 6BX, UK Email:d.zhou@reading.ac.uk, y.he@reading.ac.uk 1 Introduction The knowledge about gene clusters and protein interactions is important for biological researchers to unveil the mechanism of life. However, large quantity of the knowledge often hides in the literature, such as journal articles, reports, books and so on. Many approaches focusing on extracting information from unstructured text, such as pattern matching, shallow and deep parsing, have been proposed especially for extracting protein-protein interactions (Zhou and He, 2008). A semantic parser based on the Hidden Vector State (HVS) model for extracting protein-protein interactions is presented in (Zhou et al., 2008). The HVS model is an extension of the basic discrete Markov model in which context is encoded as a stack-oriented state vector. Maximum Likelihood estimation (MLE) is used to derive the parameters of the HVS model. In this paper, we propose a discriminative approach based on parse error measure to train the HVS model. To adjust the HVS model to achieve minimum parse error rate, the generalized probabilistic descent (GPD) algorithm (Kuo et al., 2002) i</context>
</contexts>
<marker>Zhou, He, 2008</marker>
<rawString>Deyu Zhou and Yulan He. 2008. Extracting Interactions between Proteins from the Literature. Journal of Biomedical Informatics, 41:393–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyu Zhou</author>
<author>Yulan He</author>
<author>Chee Keong Kwoh</author>
</authors>
<title>Extracting Protein-Protein Interactions from the Literature using the Hidden Vector State Model.</title>
<date>2008</date>
<journal>International Journal of Bioinformatics Research and Applications,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="886" citStr="Zhou et al., 2008" startWordPosition="125" endWordPosition="128">e knowledge about gene clusters and protein interactions is important for biological researchers to unveil the mechanism of life. However, large quantity of the knowledge often hides in the literature, such as journal articles, reports, books and so on. Many approaches focusing on extracting information from unstructured text, such as pattern matching, shallow and deep parsing, have been proposed especially for extracting protein-protein interactions (Zhou and He, 2008). A semantic parser based on the Hidden Vector State (HVS) model for extracting protein-protein interactions is presented in (Zhou et al., 2008). The HVS model is an extension of the basic discrete Markov model in which context is encoded as a stack-oriented state vector. Maximum Likelihood estimation (MLE) is used to derive the parameters of the HVS model. In this paper, we propose a discriminative approach based on parse error measure to train the HVS model. To adjust the HVS model to achieve minimum parse error rate, the generalized probabilistic descent (GPD) algorithm (Kuo et al., 2002) is used. Experiments have been conducted on the GENIA corpus. The results demonstrate modest improvements when the discriminatively trained HVS m</context>
</contexts>
<marker>Zhou, He, Kwoh, 2008</marker>
<rawString>Deyu Zhou, Yulan He, and Chee Keong Kwoh. 2008. Extracting Protein-Protein Interactions from the Literature using the Hidden Vector State Model. International Journal of Bioinformatics Research and Applications, 4(1):64–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>