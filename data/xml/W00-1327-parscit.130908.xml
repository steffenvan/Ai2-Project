<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001352">
<title confidence="0.9192935">
Using Semantically Motivated Estimates to Help
Subcategorization Acquisition
</title>
<author confidence="0.993881">
Anna Korhonen
</author>
<affiliation confidence="0.99905">
Computer Laboratory, University of Cambridge
</affiliation>
<address confidence="0.723248">
Pembroke Street, Cambridge CB2 3QG, UK
</address>
<email confidence="0.997935">
alk23@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.994769" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999562117647059">
Research into the automatic acquisition of
subcategorization frames from corpora is
starting to produce large-scale computational
lexicons which include valuable frequency in-
formation. However, the accuracy of the
resulting lexicons shows room for improve-
ment.. One source of error lies in the lack
of accurate back-off estimates for subcatego-
rization frames, delimiting the performance
of statistical techniques frequently employed
in verbal acquisition. In this paper, we
propose a method of obtaining more accu-
rate, semantically motivated back-off esti-
mates, demonstrate how these estimates can
be used to improve the learning of subcatego-
rization frames, and discuss using the method
to benefit large-scale lexical acquisition.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998624">
Manual development of large subcategorised
lexicons has proved difficult because pred-
icates change behaviour between sublan-
guages, domains and over time. Yet parsers
depend crucially on such information, and
probabilistic parsers would greatly benefit
from accurate information concerning the rel-
ative frequency of different subcategorization
frames (scFs) for a given predicate.
Over the past years acquiring subcatego-
rization dictionaries from textual corpora has
become increasingly popular (e.g. Brent,
1991, 1993; Ushioda et al., 1993; Briscoe and
Carroll, 1997; Manning, 1993; Carroll and
Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar
and Zeman, 2000). The different approaches
vary according to the methods used and the
number of scFs being extracted. Regardless
of this, there is a ceiling on the performance
of these systems at around 80% token recall&apos;.
</bodyText>
<footnote confidence="0.988291333333333">
1Token recall is the percentage of SCF tokens in a
sample of manually analysed text that were correctly
acquired by the system.
</footnote>
<bodyText confidence="0.999873465116279">
One significant source of error lies in the
statistical filtering methods frequently used
to remove noise from automatically acquired
scFs. These methods are reported to be par-
ticularly unreliable for low frequency scFs
(Brent, 1991, 1993; Briscoe and Carroll, 1997;
Manning, 1993; Manning and Schiltze, 1999;
Korhonen, Gorrell and McCarthy, 2000), re-
sulting in a poor overall performance.
According to Korhonen, Gorrell and Mc-
Carthy (2000), the poor performance of sta-
tistical filtering can be largely explained by
the zipfian nature of the data, coupled with
the fact that many statistical tests are based
on the assumption of two zipfian distributions
correlating: the conditional SCF distribution
of an individual verb (p(scfilverbi)) and the
unconditional SCF distribution of all verbs in
general (p(scfi)). Contrary to this assump-
tion, however, there is no significant correla-
tion between the two distributions.
Korhonen, Gorrell and McCarthy (2000)
have showed that a simple method of filtering
scFs on the basis of their relative frequency
performs more accurately than statistical fil-
tering. This method sensitive to the sparse
data problem is best integrated with smooth-
ing. Yet the performance of the sophisticated
smoothing techniques which back-off to an un-
conditional distribution also suffer from the
lack of correlation between p(scfilverbi) and
p(scfi).
In this paper, we propose a method for ob-
taining more accurate back-off estimates for
SCF acquisition. Taking Levin&apos;s verb classifi-
cation (Levin, 1993) as a starting point, we
show that in terms of SCF distributions, in-
dividual verbs correlate better with other se-
mantically similar verbs than with all verbs
in general. On the basis of this observation,
we propose classifying verbs according to their
semantic class and using the conditional SCF
distributions of a few other members in the
</bodyText>
<page confidence="0.998069">
216
</page>
<bodyText confidence="0.998683307692308">
same class as back-off estimates of the class
(p(sc filsemantic class3)).
Adopting the SCF acquisition system of
Briscoe and Carroll (1997) we report an ex-
periment which demonstrates how these esti-
mates can be used in filtering. This is done
by acquiring the conditional SCF distributions
for selected test verbs, smoothing these dis-
tributions with the unconditional distribution
of the respective verb class, and applying a
simple method for filtering the resulting set
of scFs. Our results show that the proposed
method improves the acquisition of SCFS sig-
nificantly. We discuss how this method can be
used to benefit large-scale SCF acquisition.
We begin by reporting our findings that the
SCF distributions of semantically similar verbs
correlate well (section 2). We then introduce
the method we adopted for constructing the
back-off estimates for the data used in our ex-
periment (section 3.1), summarise the main
features of the SCF acquisition approach (sec-
tion 3.2), and describe the smoothing tech-
niques adopted (section 3.3). Finally, we re-
view the empirical evaluation (section 4) and
discuss directions for future work (section 5).
</bodyText>
<sectionHeader confidence="0.841995" genericHeader="method">
2 Examining SCF Correlation
between Semantically Similar
Verbs
</sectionHeader>
<bodyText confidence="0.999982736842106">
To examine the degree of SCF correlation
between semantically similar verbs, we took
Levin&apos;s verb classification (1993) as a start-
ing point. Levin verb classes are based on
the ability of a verb to occur in specific
diathesis alternations, i.e. specific pairs of
syntactic frames which are assumed to be
meaning preserving. The classification pro-
vides semantically-motivated sets of syntactic
frames associated with individual classes.
While Levin&apos;s shows that there is corre-
lation between the SCFS related to the verb
sense, our aim is to examine whether there is
also correlation between the scFs specific to
the verb form. Unlike Levin, we are concerned
with polysemic SCF distributions involving all
senses of verbs. In addition, we are not only
interested in the degree of correlation between
sets of SCFS, but also in comparing the rank-
ing of SCFS between distributions. Neverthe-
less, Levin classes provide us a useful starting
point.
Focusing on five broad Levin classes -
change of possession, assessment, killing, mo-
tion, and destroy verbs - we chose four test
verbs from each class and examined the de-
gree with which the SCF distribution for these
verbs correlates with the SCF distributions for
two other verbs from the same Levin class.
The latter verbs were chosen so that one of
the verbs is a synonym, and the other a hyper-
nym, of a test verb. We used WordNet (Miller
et al., 1990) for defining and recognising these
semantic relations. We defined a hypernym
as a test verb&apos;s hypernym in WordNet, and a
synonym as a verb which, in WordNet, shares
this same hypernym with a test verb. We also
examined how well the SCF distribution for
the different test verbs correlates with the SCF
distribution of all English verbs in general and
with that of a semantically different verb (i.e.
a verb belonging to a different Levin class).
We used two methods for obtaining the
SCF distributions. The first was to acquire
an unfiltered subcategorization lexicon for 20
million words of the British National Corpus
(BNc) (Leech, 1992) data using Briscoe and
Carroll&apos;s (1997) system (introduced in sec-
tion 3.2). This gives us the observed distribu-
tion of SCFS for individual verbs and that for
all verbs in the BNC data. The second method
was to manually analyse around 300 occur-
rences of each test verb in the BNC data. This
gives us an estimate of the correct SCF distri-
butions for the individual verbs. The estimate
for the correct distribution of scFs over all
English verbs was obtained by extracting the
number of verbs which are members of each
SCF class in the ANLT dictionary (Boguraev et
al., 1987).
The degree of correlation was examined by
calculating the Kullback-Leibler distance (RL)
(Cover and Thomas, 1991) and the Spearman
rank correlation coefficient (Rc) (Spearman,
1904) between the different distributions2.
The results given in tables 1 and 2 were ob-
tained by correlating the observed SCF distri-
butions from the BNC data. Table 1 shows
an example of correlating the SCF distribu-
tion of the motion verb fly against that of (i)
its hypernym move, (ii) synonym sail, (iii) all
verbs in general, and (iv) agree, which is not
related semantically. The results show that
the SCF distribution for fly clearly correlates
better with the SCF distribution for move and
sail than that for all verbs and agree. The av-
</bodyText>
<footnote confidence="0.859034">
2Note that KL &gt; 0, With ICL near to 0 denoting
strong association, and —1 &lt; RC &lt; 1, with RC near to
0 denoting a low degree of association and RC near to
-1 and 1 denoting strong association.
</footnote>
<page confidence="0.986159">
217
</page>
<table confidence="0.9949678">
KL RC
fly move 0.25 0.83
fly sail 0.62 0.61
fly all verbs 2.13 0.51
fly agree 2.27 0.12
</table>
<tableCaption confidence="0.997481">
Table 1: Correlating the SCF distribution of fly against other SCF distributions
</tableCaption>
<table confidence="0.9993884">
KL RC
hypernym 0.65 0.71
synonym 0.71 0.66
all verbs 1.59 0.41
semantically different verb 1.74 0.38
</table>
<tableCaption confidence="0.998201">
Table 2: Overall correlation results
</tableCaption>
<bodyText confidence="0.9961192">
erage results for all test verbs given in table 2
indicate that the degree of SCF correlation is
the best with semantically similar verbs. Hy-
pernym and synonym relations are nearly as
good, the majority of verbs showing slightly
better SCF correlation with hypernyms. The
SCF correlation between individual verbs, and
verbs in general, is poor, but still better than
with semantically unrelated verbs.
These findings with the observed SCF dis-
tributions hold as well with the correct SCF
distributions, as seen in table 3. The results
show that in terms of SCF distributions, verbs
in all classes examined correlate better with
their hypernym verbs than with all verbs in
general.
As one might expect, the polysemy of the
individual verbs affects the degree of SCF cor-
relation between semantically similar verbs.
The degree of SCF correlation is higher with
those verbs whose predominant&apos; sense is in-
volved with the Levin class examined. For
example, the SCF distribution for the killing
verb murder correlates better with that for
the verb kill than that for the verb execute,
whose predominant sense is not involved with
killing verbs.
These results show that the verb sense spe-
cific SCF correlation observed by Levin ex-
tends to the verb form specific SCF correlation
and applies to the ranking of scFs as well.
This suggests that we can obtain more accu-
rate back-off estimates for verbal acquisition
by basing them on a semantic verb type. To
find out whether such semantically motivated
3Predominant sense refers here to the most frequent
sense of verbs in WordNet.
estimates can be used to improve SCF acqui-
sition, we performed an experiment which we
describe below.
</bodyText>
<sectionHeader confidence="0.999448" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.999375">
3.1 Back-off Estimates for the Data
</subsectionHeader>
<bodyText confidence="0.999993148148148">
The test data consisted of a total of 60 verbs
from 12 broad Levin classes, listed in table 4.
Two of the examined Levin classes were col-
lapsed together with another similar Levin
class, making the total number of test classes
10. The verbs were chosen at random, sub-
ject to the constraint that they occurred fre-
quently enough in corpus data4 and when ap-
plicable, represented different sub-classes of
each examined Levin class. To reduce the
problem of polysemy, we required that the
predominant sense of each verb corresponds to
the Levin class in question. This was ensured
by manually verifying that the most frequent
sense of a verb in WordNet corresponds to the
sense involved in the particular Levin class.
To obtain the back-off estimates, we chose
4-5 representative verbs from each verb class
and obtained correct SCF distributions for
these verbs by manually analysing around 300
occurrences of each verb in the BNC data. We
merged the resulting set of SCF distributions
to construct the unconditional SCF distribu-
tion for the verb class. This approach was
taken to minimise the sparse data problem
and cover SCF variations within verb classes
and due to polysemy. The back-off estimates
</bodyText>
<footnote confidence="0.705348">
4 We required at least 300 occurrences for each verb.
This was merely to guarantee accurate enough testing,
as we evaluated our results against manual analysis of
corpus data (see section 4).
</footnote>
<page confidence="0.968627">
218
</page>
<table confidence="0.999909125">
Verb class KL Hypernym KL All Verbs
RC RC
change of possession 0.61 0.64 1.16 0.38
assessment 0.28 0.71 0.73 0.48
killing 0.70 0.63 1.14 0.37
destroy 0.30 0.60 1.19 0.29
motion 0.29 0.73 1.72 0.42
AVERAGE 0.44 0.66 1.19 0.39
</table>
<tableCaption confidence="0.987314">
Table 3: Correlation results for five verb classes
</tableCaption>
<table confidence="0.9990186875">
Verb class Test verbs
putting place, lay, drop, pour, load, fill
sending and carrying, send, ship, carry, bring, transport
exerting force pull, push
change of possession give, lend, contribute, donate, offer
provide, supply, acquire, buy
assessment, analyse
searching fish, explore, investigate
social interaction agree, communicate, struggle, marry, meet, visit
killing kill, murder, slaughter, strangle
destroy demolish, destroy, ruin, devastate
appearance, disappearance arise, emerge, disappear, vanish
and occurrence
motion arrive, depart, march, move, slide, swing
travel, walk, fly, sail, dance
aspectual begin, end, start, terminate, complete
</table>
<tableCaption confidence="0.999479">
Table 4: Test data
</tableCaption>
<bodyText confidence="0.99990875">
for motion verbs, for example, were obtained
by merging the SCF distributions of the verbs
march, move, fly, slide and sail. Each verb
used in obtaining the estimates was excluded
when testing the verb itself. For example,
when acquiring subcategorization for the verb
fly, estimates were obtained only using verbs
march, move, slide and sail.
</bodyText>
<subsectionHeader confidence="0.958919">
3.2 Framework for SCF Acquisition
</subsectionHeader>
<bodyText confidence="0.998656692307692">
Briscoe and Carroll&apos;s (1997) verbal acquisition
system distinguishes 163 SCFS and returns rel-
ative frequencies for each SCF found for a given
predicate. The scFs are a superset of classes
found in the ANLT and COMLEX (Grishman
et a/., 1994) dictionaries. They incorporate
information about control of predicative ar-
guments, as well as alternations such as ex-
traposition and particle movement. The sys-
tem employs a shallow parser to obtain the
subcategorization information. Potential SCF
entries are filtered before the final SCF lexi-
con is produced. While Briscoe and Carroll
(1997) used a statistical filter based on bi-
nomial hypothesis test, we adopted another
method, where the conditional SCF distribu-
tion from the system is smoothed before fil-
tering the SCFS, using the different techniques
introduced in section 3.3. After smoothing,
filtering is performed by applying a threshold
to the resulting set of probability estimates.
We used training data to find an optimal av-
erage threshold for each verb class examined.
This filtering method allows us to examine
the benefits of smoothing without introducing
problems based on the statistical filter.
</bodyText>
<subsectionHeader confidence="0.982961">
3.3 Smoothing
3.3.1 Add One Smoothing
</subsectionHeader>
<bodyText confidence="0.9999502">
Add one smoothing5 has the effect of giving
some of the probability space to the scFs un-
seen in the conditional distribution. As it as-
sumes a uniform prior on events, it provides a
baseline smoothing method against which the
</bodyText>
<footnote confidence="0.471898333333333">
s See (Manning and Schiitze, 1999) for detailed in-
formation about the smoothing techniques discussed
here.
</footnote>
<page confidence="0.996682">
219
</page>
<bodyText confidence="0.999898166666667">
more sophisticated methods can be compared.
Let c(x) be the frequency of a SCF given a
verb, N the total number of SCF tokens for
this verb in the conditional distribution, and
C the total number of SCF types. The esti-
mated probability of the SC F is:
</bodyText>
<equation confidence="0.4672985">
c(x n) + 1 (1)
P‘zn N C
</equation>
<subsectionHeader confidence="0.754686">
3.3.2 Katz Backing-off
</subsectionHeader>
<bodyText confidence="0.9998582">
In Katz backing-off (Katz, 1987), some of the
probability space is given to the scFs unseen
or of low frequency in the conditional distri-
bution. This is done by backing-off to an un-
conditional distribution. Let p(X) be a prob-
ability of a SCF in the conditional distribution,
and p(x„p) its probability in the unconditional
distribution, obtained by maximum likelihood
estimation. The estimated probability of the
SCF is calculated as follows:
</bodyText>
<equation confidence="0.9992885">
(1 — d) x p(X) i fc(x,i) &gt;
X p(x„p) otherwise
</equation>
<bodyText confidence="0.999942916666667">
The cut off frequency c1 is an empiri-
cally defined threshold determining whether
to back-off or not. When counts are lower
than c1 they are held too low to give an accu-
rate estimate, and we back-off to an uncondi-
tional distribution. In this case, we discount
p(x,) a certain amount to reserve some of the
probablity space for unseen and very low fre-
quency scFs. The discount (d) is defined em-
pirically, and a is a normalization constant
which ensures that the probabilities of the re-
sulting distribution sum to 1.
</bodyText>
<subsubsectionHeader confidence="0.912703">
3.3.3 Linear Interpolation
</subsubsectionHeader>
<bodyText confidence="0.999691428571429">
While Katz backing-off consults different es-
timates depending on their specificity, linear
interpolation makes a linear combination of
them. Linear interpolation is used here for the
simple task of combining a conditional distri-
bution with an unconditional one. The esti-
mated probability of the SCF is given by
</bodyText>
<equation confidence="0.999626">
P(r) = Ai(p(x.))+ A2(p(x.p)) (3)
</equation>
<bodyText confidence="0.999952">
where the Ai denotes weights for differ-
ent context sizes (obtained by optimising the
smoothing performance on the training data
for all xr,) and sum to 1.
</bodyText>
<sectionHeader confidence="0.998863" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995182">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.998472948717949">
To evaluate the approach, we took a sample of
20 million words of the BNC and extracted all
sentences containing an occurrence of one of
the 60 test verbs on average of 3000 citations
of each. The sentences containing these verbs
were processed by the SCF acquisition system,
and the smoothing methods were applied be-
fore filtering. We also obtained results for a
baseline without any smoothing.
The results were evaluated against a man-
ual analysis of the corpus data. This was
obtained by analysing up to a maximum of
300 occurrences for each test verb in BNC
or Los (Garside et al., 1987), Susanne and
SEC (Taylor and Knowles, 1988) corpora. We
calculated type precision (percentage of scFs
acquired which were also exemplified in the
manual analysis) and recall (percentage of the
scFs exemplified in the manual analysis which
were also acquired automatically), and com-
bined them into a single measure of overall
performance using the F measure (Manning
and Schiitze, 1999).
F2 precision recall (4
= )
precision + recall
We estimated accuracy with which the sys-
tem ranks true positive classes against the cor-
rect ranking. This was computed by calculat-
ing the percentage of pairs of scFs at posi-
tions (n, m) such that n &lt; m in the system
ranking that occur in the same order in the
ranking from the manual analysis. This gives
us an estimate of the accuracy of the relative
frequencies of SCFS output by the system. In
addition to the system results, we also calcu-
lated KL and RC between the acquired unfil-
tered SCF distributions and the distributions
obtained from the manual analysis.
</bodyText>
<subsectionHeader confidence="0.573744">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.998058181818182">
Table 5 gives average results for the 60 test
verbs using each method. The results indi-
cate that both add one smoothing and Katz
backing-off improve the baseline performance
only slightly. Linear interpolation outper-
forms these methods, achieving better results
on all measures. The improved Kr, indicates
that the method improves the overall accu-
racy of SCF distributions. The results with
RC and system accuracy show that it helps
to correct the ranking of SCFS. The fact
</bodyText>
<figure confidence="0.408565">
(2)
</figure>
<page confidence="0.978361">
220
</page>
<table confidence="0.999603833333333">
Method KL RC accuracy System results recall (%)
precision F measure
Baseline 0.63 0.72 79.2 78.5 63.3 70.1
Add one 0.64 0.74 79.0 85.3 59.7 70.2
Katz backing-off 0.61 0.75 79.0 76.4 67.6 71.7
Linear interpolation 0.51 0.82 84.4 87.8 68.7 77.1
</table>
<tableCaption confidence="0.9274955">
Table 5: Average results with different methods using semantically motivated back-off estimates
for smoothing
</tableCaption>
<table confidence="0.9998878">
Method KL RC accuracy System results (%) F measure
precision recall
Baseline 0.63 0.72 79.2 78.5 63.3 70.1
Katz backing-off 0.68 0.69 77.2 75.2 61.7 67.8
Linear interpolation 0.79 0.64 76.7 71.4 64.1 67.6
</table>
<tableCaption confidence="0.912999">
Table 6: Average results using the SCF distribution of all verbs as back-off estimates for smooth-
ing
</tableCaption>
<bodyText confidence="0.981176818181818">
that both precision and recall show clear im-
provement over the baseline results demon-
strates that linear interpolation can be suc-
cessfully combined with the filtering method
employed. These results seem to suggest that
a smoothing method which affects both the
highly ranked scFs and scFs of low frequency
is profitable for this task.
In this experiment, the semantically moti-
vated back-off estimates helped to reduce the
sparse data problem significantly. While a to-
tal of 151 correct scFs were missing in the test
data, only three were missing after smoothing
with Katz backing-off or linear interpolation.
For comparison, we re-run these experi-
ments using the general SCF distribution of
all verbs as back-off estimates for smooth-
ing6. The average results for the 60 test verbs
given in table 6 show that when using these
estimates, we obtain worse results than with
the baseline method. This demonstrates that
while such estimates provide an easy solution
to the sparse data problem, they can actually
degrade the accuracy of verbal acquisition.
Table 7 displays individual results for the
different verb classes. It lists the results ob-
tained with la and RC using the baseline
method and linear interpolation with semanti-
cally motivated estimates. Examining the re-
sults obtained with linear interpolation allows
us to consider the accuracy of the back-off es-
6These estimates were obtained by extracting the
number of verbs which are members of each SCF class
in the ANLT dictionary. See section 2 for details.
timates for each verb class. Out of ten verb
classes, eight show improvement with linear
interpolation, with both KL and RC. However,
two verb classes - aspectual verbs, and verbs
of appearance, disappearance and occurrence
- show worse results when linear interpolation
is used.
According to Levin (1993), these two verb
classes need further classification before a full
semantic account can be made. The prob-
lem with aspectual verbs is that the class
contains verbs taking sentential complements.
As Levin does not classify verbs on basis
of their sentential complement-taking proper-
ties, more classification work is required be-
fore we can obtain accurate SCF estimates for
this type of verb.
The problem with verbs of appearance is
more specific to the verb class. Levin remarks
that the definition of appearance verbs may
be too loose. In addition, there are signifi-
cant syntactic differences between the verbs
belonging to the different sub-classes.
This suggests that we should examine the
degree of SCF correlation between verbs from
different sub-classes before deciding on the fi-
nal (sub-)class for which we obtain the es-
timates. As the results with the combined
Levin classes show, estimates can also be suc-
cessfully built using verbs from different Levin
classes, provided that the classes are similar
enough.
</bodyText>
<page confidence="0.994003">
221
</page>
<table confidence="0.999888">
Verb class KL linear i. RC linear i.
baseline baseline
putting 0.70 0.66 0.68 0.70
sending and carrying, 0.64 0.50 0.72 0.96
exerting force
change of possession 0.61 0.60 0.61 0.75
assessment, 0.81 0.62 0.61 0.70
searching
social interaction 0.65 0.58 0.72 0.80
killing 0.69 0.67 0.91 0.95
destroy 0.95 0.20 0.70 0.97
appearance, disappearance 0.14 0.17 0.91 0.83
and occurrence
motion 0.66 0.58 0.56 0.66
aspectual 0.48 0.54 0.86 0.89
</table>
<tableCaption confidence="0.998203">
Table 7: Baseline and linear interpolation results for the verb classes
</tableCaption>
<sectionHeader confidence="0.99674" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999989777777778">
In this paper, we have shown that the verb
form specific SCF distributions of semantically
similar verbs correlate well. On the basis
of this observation, we have proposed using
verb class specific back-off estimates in SCF
acquisition. Employing the SCF acquisition
framework of Briscoe and Carroll (1997), we
have demonstrated that these estimates can
be used to improve SCF acquisition signifi-
cantly, when combined with smoothing and
a simple filtering method.
We have not yet explored the possibility
of using the semantically motivated estimates
with statistical filtering. In principle, this
should help to improve the performance of the
statistical methods which make use of back-off
estimates. If filtering based on relative fre-
quencies still achieves better results, it would
be worth investigating ways of handling the
low frequency data for integration with this
method. As Korhonen, Gorrell and McCarthy
(2000) discuss, any statistical filtering method
would work better at low frequences than the
one applied, since this simply disregards all
low frequency SCFS.
In addition to refining the filtering method,
our future work will focus on integrating
this approach with large-scale SCF acquisition.
This will involve (i) defining the set of seman-
tic verb classes across the lexicon, (ii) obtain-
ing back-off estimates for each verb class, and
(iii) implementing a method capable of auto-
matically classifying verbs to semantic classes.
The latter can be done by linking the Word-
Net synonym sets with semantic classes, using
a similar method to that employed by Dorr
(1997). With the research reported, verbs
were classified to semantic classes according
to their most frequent sense. While this ap-
proach proved satisfactory, our future work
will include investigating ways of addressing
the problem of polysemy better.
The manual effort needed for obtaining the
back-off estimates was quite high for this pre-
liminary experiment. However, our recent in-
vestigation shows that the total number of se-
mantic classes across the whole lexicon is un-
likely to exceed 50. This is because many of
the Levin classes have proved similar enough
in terms of SCF distributions that they can be
combined together. Therefore the additional
effort required to carry out the proposed work
seems justified, given the accuracy- enhance-
ment reported.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.948656">
I thank Ted Briscoe and Diana McCarthy for
useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.996811" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.855006777777778">
Boguraev, B., Briscoe, E., Carroll, J., Carter,
D. and Grover, C. 1987. The derivation of a
grammatically-indexed lexicon from the Long-
man Dictionary of Contemporary English. In
Proceedings of the 25th Annual Meeting of
the Association for Computational Linguis-
tics, Stanford, CA. 193-200.
Brent, M. 1991. Automatic acquisition of
subcategorization frames from untagged text.
</reference>
<page confidence="0.989505">
222
</page>
<reference confidence="0.98745318556701">
In Proceedings of the 29th Annual Meeting
of the Association for Computational Linguis-
tics, Berkeley, CA. 209-214.
Brent, M. 1993. From grammar to lexicon:
unsupervised learning of lexical syntax. Com-
putational Linguistics 19.3: 243-262.
Briscoe, E. and Carroll, J. 1993. Gener-
alised probabilistic LR parsing for unification-
based grammars. Computational Linguistics
19.1: 25-60.
Briscoe, E.J. and J. Carroll 1997. Automatic
extraction of subcategorization from corpora.
In Proceedings of the 5th ACL Conf. on Ap-
plied Natural Language Processing, Washing-
ton, DC. 356-363.
Briscoe, T., Carroll, J. and Korhonen, A.
1997. Automatic extraction of subcategoriza-
tion frames from corpora - a framework and
3 experiments. Sparkle WP5 Deliverable.
Available in http://www.ilc.pi.cnr.it/.
Carroll, G. and Rooth, M. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing,
Granada, Spain.
Cover, Thomas, M. and Thomas, J.A. 1991.
Elements of Information Theory. Wiley-
Interscience, New York.
Dorr, B. 1997. Large-scale dictionary con-
struction for foreign language tutoring and
interlingual machine translation. Machine
Translation 12.4: 271-325.
Gahl, S. 1998. Automatic extraction of sub-
corpora based on subcategorization frames
from a part-of-speech tagged corpus. In Pro-
ceedings of the COLING-ACL&apos;98, Montreal,
Canada.
Grishman, R., Macleod, C. and Meyers, A.
1994. Comlex syntax: building a computa-
tional lexicon. In Proceedings of the Interna-
tional Conference on Computational Linguis-
tics, COLING-94, Kyoto, Japan. 268-272.
Garside, R., Leech, G. and Sampson, G. 1987.
The computational analysis of English: A
corpus-based approach. Longman, London.
Katz, S. M. 1987. Estimation of probabili-
ties from sparse data for the language model
component of speech recogniser. IEEE Trans-
actions on Acoustics, Speech, and Signal Pro-
cessing 35.3: 400-401.
Korhonen, A., Gorrell, G. and McCarthy,
D. 2000. Statistical filtering and subcatego-
rization frame acquisition. In Proceedings of
the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, Hong Kong.
Leech, G. 1992. 100 million words of English:
the British National Corpus. Language Re-
search 28(1): 1-13.
Levin, B. 1993. English Verb Classes and
Alternations. Chicago University Press,
Chicago.
Manning, C. 1993. Automatic acquisition of
a large subcategorization dictionary from cor-
pora. In Proceedings of the 31st Annual Meet-
ing of the Association for Computational Lin-
guistics, Columbus, Ohio. 235-242.
Manning, C. and Schfitze, H. 1999. Founda-
tions of Statistical Natural Language Process-
ing. New York University, Ms.
Miller, G., Beckwith, R., Felbaum, C., Gross,
D. and Miller, K. 1993. Introduction to
WordNet: An On-Line Lexical Database.
ftp//clarity.princeton.edu/pub/WordNet/
5papers.ps.
Sampson, G. 1995. English for the computer.
Oxford, UK: Oxford University Press.
Sarkar, A. and Zeman, D. 2000. Auto-
matic Extraction of Subcategorization Frames
for Czech. In Proceedings of the Inter-
national Conference on Computational Lin-
guistics, COLING-00, Saarbrucken, Germany.
691-697.
Spearman, C. 1904. The proof and mea-
surement of association between two things.
American Journal of Psychology 15: 72-101.
Taylor, L. and Knowles, G. 1988. Manual
of information to accompany the SEC cor-
pus: the machine-readable corpus of spoken
English. University of Lancaster, UK, Ms.
Ushioda, A., Evans, D., Gibson, T. and
Waibel, A. 1993. The automatic acquisition of
frequencies of verb subcategorization frames
from tagged corpora. In Boguraev, B. and
Pustejovsky, J. eds. SIGLEX ACL Workshop
on the Acquisition of Lexical Knowledge from
Text. Columbus, Ohio: 95-106.
</reference>
<page confidence="0.999169">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.504107">
<title confidence="0.998608">Using Semantically Motivated Estimates to Subcategorization Acquisition</title>
<author confidence="0.97176">Anna</author>
<affiliation confidence="0.751157">Computer Laboratory, University of Pembroke Street, Cambridge CB2 3QG,</affiliation>
<email confidence="0.978293">alk23@cl.cam.ac.uk</email>
<abstract confidence="0.996689333333333">Research into the automatic acquisition of subcategorization frames from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement.. One source of error lies in the lack of accurate back-off estimates for subcategorization frames, delimiting the performance of statistical techniques frequently employed in verbal acquisition. In this paper, propose a method of obtaining more accurate, semantically motivated back-off estimates, demonstrate how these estimates can be used to improve the learning of subcategorization frames, and discuss using the method to benefit large-scale lexical acquisition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>D Carter</author>
<author>C Grover</author>
</authors>
<title>The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="7623" citStr="Boguraev et al., 1987" startWordPosition="1191" endWordPosition="1194">f the British National Corpus (BNc) (Leech, 1992) data using Briscoe and Carroll&apos;s (1997) system (introduced in section 3.2). This gives us the observed distribution of SCFS for individual verbs and that for all verbs in the BNC data. The second method was to manually analyse around 300 occurrences of each test verb in the BNC data. This gives us an estimate of the correct SCF distributions for the individual verbs. The estimate for the correct distribution of scFs over all English verbs was obtained by extracting the number of verbs which are members of each SCF class in the ANLT dictionary (Boguraev et al., 1987). The degree of correlation was examined by calculating the Kullback-Leibler distance (RL) (Cover and Thomas, 1991) and the Spearman rank correlation coefficient (Rc) (Spearman, 1904) between the different distributions2. The results given in tables 1 and 2 were obtained by correlating the observed SCF distributions from the BNC data. Table 1 shows an example of correlating the SCF distribution of the motion verb fly against that of (i) its hypernym move, (ii) synonym sail, (iii) all verbs in general, and (iv) agree, which is not related semantically. The results show that the SCF distribution</context>
</contexts>
<marker>Boguraev, Briscoe, Carroll, Carter, Grover, 1987</marker>
<rawString>Boguraev, B., Briscoe, E., Carroll, J., Carter, D. and Grover, C. 1987. The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford, CA. 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>209--214</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1461" citStr="Brent, 1991" startWordPosition="196" endWordPosition="197">ion frames, and discuss using the method to benefit large-scale lexical acquisition. 1 Introduction Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatica</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Brent, M. 1991. Automatic acquisition of subcategorization frames from untagged text. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA. 209-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>243--262</pages>
<marker>Brent, 1993</marker>
<rawString>Brent, M. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Computational Linguistics 19.3: 243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalised probabilistic LR parsing for unificationbased grammars.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>25--60</pages>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Briscoe, E. and Carroll, J. 1993. Generalised probabilistic LR parsing for unificationbased grammars. Computational Linguistics 19.1: 25-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conf. on Applied Natural Language Processing,</booktitle>
<pages>356--363</pages>
<location>Washington, DC.</location>
<contexts>
<context position="1516" citStr="Briscoe and Carroll, 1997" startWordPosition="203" endWordPosition="206">to benefit large-scale lexical acquisition. 1 Introduction Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These methods are reported to be par</context>
<context position="3946" citStr="Briscoe and Carroll (1997)" startWordPosition="582" endWordPosition="585">aper, we propose a method for obtaining more accurate back-off estimates for SCF acquisition. Taking Levin&apos;s verb classification (Levin, 1993) as a starting point, we show that in terms of SCF distributions, individual verbs correlate better with other semantically similar verbs than with all verbs in general. On the basis of this observation, we propose classifying verbs according to their semantic class and using the conditional SCF distributions of a few other members in the 216 same class as back-off estimates of the class (p(sc filsemantic class3)). Adopting the SCF acquisition system of Briscoe and Carroll (1997) we report an experiment which demonstrates how these estimates can be used in filtering. This is done by acquiring the conditional SCF distributions for selected test verbs, smoothing these distributions with the unconditional distribution of the respective verb class, and applying a simple method for filtering the resulting set of scFs. Our results show that the proposed method improves the acquisition of SCFS significantly. We discuss how this method can be used to benefit large-scale SCF acquisition. We begin by reporting our findings that the SCF distributions of semantically similar verb</context>
<context position="13867" citStr="Briscoe and Carroll (1997)" startWordPosition="2211" endWordPosition="2214">nd sail. 3.2 Framework for SCF Acquisition Briscoe and Carroll&apos;s (1997) verbal acquisition system distinguishes 163 SCFS and returns relative frequencies for each SCF found for a given predicate. The scFs are a superset of classes found in the ANLT and COMLEX (Grishman et a/., 1994) dictionaries. They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. While Briscoe and Carroll (1997) used a statistical filter based on binomial hypothesis test, we adopted another method, where the conditional SCF distribution from the system is smoothed before filtering the SCFS, using the different techniques introduced in section 3.3. After smoothing, filtering is performed by applying a threshold to the resulting set of probability estimates. We used training data to find an optimal average threshold for each verb class examined. This filtering method allows us to examine the benefits of smoothing without introducing problems based on the statistical filter. 3.3 Smoothing 3.3.1 Add One </context>
<context position="23074" citStr="Briscoe and Carroll (1997)" startWordPosition="3732" endWordPosition="3735">2 0.61 0.70 searching social interaction 0.65 0.58 0.72 0.80 killing 0.69 0.67 0.91 0.95 destroy 0.95 0.20 0.70 0.97 appearance, disappearance 0.14 0.17 0.91 0.83 and occurrence motion 0.66 0.58 0.56 0.66 aspectual 0.48 0.54 0.86 0.89 Table 7: Baseline and linear interpolation results for the verb classes 5 Conclusion In this paper, we have shown that the verb form specific SCF distributions of semantically similar verbs correlate well. On the basis of this observation, we have proposed using verb class specific back-off estimates in SCF acquisition. Employing the SCF acquisition framework of Briscoe and Carroll (1997), we have demonstrated that these estimates can be used to improve SCF acquisition significantly, when combined with smoothing and a simple filtering method. We have not yet explored the possibility of using the semantically motivated estimates with statistical filtering. In principle, this should help to improve the performance of the statistical methods which make use of back-off estimates. If filtering based on relative frequencies still achieves better results, it would be worth investigating ways of handling the low frequency data for integration with this method. As Korhonen, Gorrell and</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, E.J. and J. Carroll 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conf. on Applied Natural Language Processing, Washington, DC. 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
<author>A Korhonen</author>
</authors>
<title>Automatic extraction of subcategorization frames from corpora - a framework and 3 experiments. Sparkle WP5 Deliverable. Available</title>
<date>1997</date>
<note>in http://www.ilc.pi.cnr.it/.</note>
<marker>Briscoe, Carroll, Korhonen, 1997</marker>
<rawString>Briscoe, T., Carroll, J. and Korhonen, A. 1997. Automatic extraction of subcategorization frames from corpora - a framework and 3 experiments. Sparkle WP5 Deliverable. Available in http://www.ilc.pi.cnr.it/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1555" citStr="Carroll and Rooth 1998" startWordPosition="209" endWordPosition="212">. 1 Introduction Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These methods are reported to be particularly unreliable for low frequency </context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Carroll, G. and Rooth, M. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cover</author>
<author>M</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>WileyInterscience,</publisher>
<location>New York.</location>
<marker>Cover, M, Thomas, 1991</marker>
<rawString>Cover, Thomas, M. and Thomas, J.A. 1991. Elements of Information Theory. WileyInterscience, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
</authors>
<title>Large-scale dictionary construction for foreign language tutoring and interlingual machine translation.</title>
<date>1997</date>
<journal>Machine Translation</journal>
<volume>12</volume>
<pages>271--325</pages>
<contexts>
<context position="24351" citStr="Dorr (1997)" startWordPosition="3933" endWordPosition="3934">ork better at low frequences than the one applied, since this simply disregards all low frequency SCFS. In addition to refining the filtering method, our future work will focus on integrating this approach with large-scale SCF acquisition. This will involve (i) defining the set of semantic verb classes across the lexicon, (ii) obtaining back-off estimates for each verb class, and (iii) implementing a method capable of automatically classifying verbs to semantic classes. The latter can be done by linking the WordNet synonym sets with semantic classes, using a similar method to that employed by Dorr (1997). With the research reported, verbs were classified to semantic classes according to their most frequent sense. While this approach proved satisfactory, our future work will include investigating ways of addressing the problem of polysemy better. The manual effort needed for obtaining the back-off estimates was quite high for this preliminary experiment. However, our recent investigation shows that the total number of semantic classes across the whole lexicon is unlikely to exceed 50. This is because many of the Levin classes have proved similar enough in terms of SCF distributions that they c</context>
</contexts>
<marker>Dorr, 1997</marker>
<rawString>Dorr, B. 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation 12.4: 271-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gahl</author>
</authors>
<title>Automatic extraction of subcorpora based on subcategorization frames from a part-of-speech tagged corpus.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL&apos;98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1567" citStr="Gahl, 1998" startWordPosition="213" endWordPosition="214">development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These methods are reported to be particularly unreliable for low frequency scFs (Brent,</context>
</contexts>
<marker>Gahl, 1998</marker>
<rawString>Gahl, S. 1998. Automatic extraction of subcorpora based on subcategorization frames from a part-of-speech tagged corpus. In Proceedings of the COLING-ACL&apos;98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>Comlex syntax: building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-94,</booktitle>
<pages>268--272</pages>
<location>Kyoto,</location>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Grishman, R., Macleod, C. and Meyers, A. 1994. Comlex syntax: building a computational lexicon. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan. 268-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The computational analysis of English: A corpus-based approach.</title>
<date>1987</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="17255" citStr="Garside et al., 1987" startWordPosition="2787" endWordPosition="2790"> to 1. 4 Evaluation 4.1 Method To evaluate the approach, we took a sample of 20 million words of the BNC and extracted all sentences containing an occurrence of one of the 60 test verbs on average of 3000 citations of each. The sentences containing these verbs were processed by the SCF acquisition system, and the smoothing methods were applied before filtering. We also obtained results for a baseline without any smoothing. The results were evaluated against a manual analysis of the corpus data. This was obtained by analysing up to a maximum of 300 occurrences for each test verb in BNC or Los (Garside et al., 1987), Susanne and SEC (Taylor and Knowles, 1988) corpora. We calculated type precision (percentage of scFs acquired which were also exemplified in the manual analysis) and recall (percentage of the scFs exemplified in the manual analysis which were also acquired automatically), and combined them into a single measure of overall performance using the F measure (Manning and Schiitze, 1999). F2 precision recall (4 = ) precision + recall We estimated accuracy with which the system ranks true positive classes against the correct ranking. This was computed by calculating the percentage of pairs of scFs </context>
</contexts>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R., Leech, G. and Sampson, G. 1987. The computational analysis of English: A corpus-based approach. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of speech recogniser.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="15144" citStr="Katz, 1987" startWordPosition="2429" endWordPosition="2430">probability space to the scFs unseen in the conditional distribution. As it assumes a uniform prior on events, it provides a baseline smoothing method against which the s See (Manning and Schiitze, 1999) for detailed information about the smoothing techniques discussed here. 219 more sophisticated methods can be compared. Let c(x) be the frequency of a SCF given a verb, N the total number of SCF tokens for this verb in the conditional distribution, and C the total number of SCF types. The estimated probability of the SC F is: c(x n) + 1 (1) P‘zn N C 3.3.2 Katz Backing-off In Katz backing-off (Katz, 1987), some of the probability space is given to the scFs unseen or of low frequency in the conditional distribution. This is done by backing-off to an unconditional distribution. Let p(X) be a probability of a SCF in the conditional distribution, and p(x„p) its probability in the unconditional distribution, obtained by maximum likelihood estimation. The estimated probability of the SCF is calculated as follows: (1 — d) x p(X) i fc(x,i) &gt; X p(x„p) otherwise The cut off frequency c1 is an empirically defined threshold determining whether to back-off or not. When counts are lower than c1 they are hel</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, S. M. 1987. Estimation of probabilities from sparse data for the language model component of speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing 35.3: 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>G Gorrell</author>
<author>D McCarthy</author>
</authors>
<title>Statistical filtering and subcategorization frame acquisition.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<location>Hong Kong.</location>
<marker>Korhonen, Gorrell, McCarthy, 2000</marker>
<rawString>Korhonen, A., Gorrell, G. and McCarthy, D. 2000. Statistical filtering and subcategorization frame acquisition. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research</journal>
<volume>28</volume>
<issue>1</issue>
<pages>1--13</pages>
<contexts>
<context position="7050" citStr="Leech, 1992" startWordPosition="1091" endWordPosition="1092">g these semantic relations. We defined a hypernym as a test verb&apos;s hypernym in WordNet, and a synonym as a verb which, in WordNet, shares this same hypernym with a test verb. We also examined how well the SCF distribution for the different test verbs correlates with the SCF distribution of all English verbs in general and with that of a semantically different verb (i.e. a verb belonging to a different Levin class). We used two methods for obtaining the SCF distributions. The first was to acquire an unfiltered subcategorization lexicon for 20 million words of the British National Corpus (BNc) (Leech, 1992) data using Briscoe and Carroll&apos;s (1997) system (introduced in section 3.2). This gives us the observed distribution of SCFS for individual verbs and that for all verbs in the BNC data. The second method was to manually analyse around 300 occurrences of each test verb in the BNC data. This gives us an estimate of the correct SCF distributions for the individual verbs. The estimate for the correct distribution of scFs over all English verbs was obtained by extracting the number of verbs which are members of each SCF class in the ANLT dictionary (Boguraev et al., 1987). The degree of correlation</context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Leech, G. 1992. 100 million words of English: the British National Corpus. Language Research 28(1): 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations.</title>
<date>1993</date>
<publisher>Chicago University Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="3462" citStr="Levin, 1993" startWordPosition="505" endWordPosition="506">utions. Korhonen, Gorrell and McCarthy (2000) have showed that a simple method of filtering scFs on the basis of their relative frequency performs more accurately than statistical filtering. This method sensitive to the sparse data problem is best integrated with smoothing. Yet the performance of the sophisticated smoothing techniques which back-off to an unconditional distribution also suffer from the lack of correlation between p(scfilverbi) and p(scfi). In this paper, we propose a method for obtaining more accurate back-off estimates for SCF acquisition. Taking Levin&apos;s verb classification (Levin, 1993) as a starting point, we show that in terms of SCF distributions, individual verbs correlate better with other semantically similar verbs than with all verbs in general. On the basis of this observation, we propose classifying verbs according to their semantic class and using the conditional SCF distributions of a few other members in the 216 same class as back-off estimates of the class (p(sc filsemantic class3)). Adopting the SCF acquisition system of Briscoe and Carroll (1997) we report an experiment which demonstrates how these estimates can be used in filtering. This is done by acquiring </context>
<context position="21227" citStr="Levin (1993)" startWordPosition="3440" endWordPosition="3441">ation with semantically motivated estimates. Examining the results obtained with linear interpolation allows us to consider the accuracy of the back-off es6These estimates were obtained by extracting the number of verbs which are members of each SCF class in the ANLT dictionary. See section 2 for details. timates for each verb class. Out of ten verb classes, eight show improvement with linear interpolation, with both KL and RC. However, two verb classes - aspectual verbs, and verbs of appearance, disappearance and occurrence - show worse results when linear interpolation is used. According to Levin (1993), these two verb classes need further classification before a full semantic account can be made. The problem with aspectual verbs is that the class contains verbs taking sentential complements. As Levin does not classify verbs on basis of their sentential complement-taking properties, more classification work is required before we can obtain accurate SCF estimates for this type of verb. The problem with verbs of appearance is more specific to the verb class. Levin remarks that the definition of appearance verbs may be too loose. In addition, there are significant syntactic differences between </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alternations. Chicago University Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1531" citStr="Manning, 1993" startWordPosition="207" endWordPosition="208">cal acquisition. 1 Introduction Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These methods are reported to be particularly unrel</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Manning, C. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio. 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schfitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<location>New York University, Ms.</location>
<marker>Manning, Schfitze, 1999</marker>
<rawString>Manning, C. and Schfitze, H. 1999. Foundations of Statistical Natural Language Processing. New York University, Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Felbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An On-Line Lexical Database. ftp//clarity.princeton.edu/pub/WordNet/ 5papers.ps.</title>
<date>1993</date>
<marker>Miller, Beckwith, Felbaum, Gross, Miller, 1993</marker>
<rawString>Miller, G., Beckwith, R., Felbaum, C., Gross, D. and Miller, K. 1993. Introduction to WordNet: An On-Line Lexical Database. ftp//clarity.princeton.edu/pub/WordNet/ 5papers.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the computer.</title>
<date>1995</date>
<publisher>Oxford University Press.</publisher>
<location>Oxford, UK:</location>
<marker>Sampson, 1995</marker>
<rawString>Sampson, G. 1995. English for the computer. Oxford, UK: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>D Zeman</author>
</authors>
<title>Automatic Extraction of Subcategorization Frames for Czech.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-00,</booktitle>
<pages>691--697</pages>
<location>Saarbrucken,</location>
<contexts>
<context position="1606" citStr="Sarkar and Zeman, 2000" startWordPosition="217" endWordPosition="220">tegorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These methods are reported to be particularly unreliable for low frequency scFs (Brent, 1991, 1993; Briscoe and Carroll, 1997;</context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>Sarkar, A. and Zeman, D. 2000. Automatic Extraction of Subcategorization Frames for Czech. In Proceedings of the International Conference on Computational Linguistics, COLING-00, Saarbrucken, Germany. 691-697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Spearman</author>
</authors>
<title>The proof and measurement of association between two things.</title>
<date>1904</date>
<journal>American Journal of Psychology</journal>
<volume>15</volume>
<pages>72--101</pages>
<contexts>
<context position="7806" citStr="Spearman, 1904" startWordPosition="1218" endWordPosition="1219">erbs and that for all verbs in the BNC data. The second method was to manually analyse around 300 occurrences of each test verb in the BNC data. This gives us an estimate of the correct SCF distributions for the individual verbs. The estimate for the correct distribution of scFs over all English verbs was obtained by extracting the number of verbs which are members of each SCF class in the ANLT dictionary (Boguraev et al., 1987). The degree of correlation was examined by calculating the Kullback-Leibler distance (RL) (Cover and Thomas, 1991) and the Spearman rank correlation coefficient (Rc) (Spearman, 1904) between the different distributions2. The results given in tables 1 and 2 were obtained by correlating the observed SCF distributions from the BNC data. Table 1 shows an example of correlating the SCF distribution of the motion verb fly against that of (i) its hypernym move, (ii) synonym sail, (iii) all verbs in general, and (iv) agree, which is not related semantically. The results show that the SCF distribution for fly clearly correlates better with the SCF distribution for move and sail than that for all verbs and agree. The av2Note that KL &gt; 0, With ICL near to 0 denoting strong associati</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>Spearman, C. 1904. The proof and measurement of association between two things. American Journal of Psychology 15: 72-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Taylor</author>
<author>G Knowles</author>
</authors>
<title>Manual of information to accompany the SEC corpus: the machine-readable corpus of spoken English.</title>
<date>1988</date>
<location>University of Lancaster, UK, Ms.</location>
<contexts>
<context position="17299" citStr="Taylor and Knowles, 1988" startWordPosition="2794" endWordPosition="2797">te the approach, we took a sample of 20 million words of the BNC and extracted all sentences containing an occurrence of one of the 60 test verbs on average of 3000 citations of each. The sentences containing these verbs were processed by the SCF acquisition system, and the smoothing methods were applied before filtering. We also obtained results for a baseline without any smoothing. The results were evaluated against a manual analysis of the corpus data. This was obtained by analysing up to a maximum of 300 occurrences for each test verb in BNC or Los (Garside et al., 1987), Susanne and SEC (Taylor and Knowles, 1988) corpora. We calculated type precision (percentage of scFs acquired which were also exemplified in the manual analysis) and recall (percentage of the scFs exemplified in the manual analysis which were also acquired automatically), and combined them into a single measure of overall performance using the F measure (Manning and Schiitze, 1999). F2 precision recall (4 = ) precision + recall We estimated accuracy with which the system ranks true positive classes against the correct ranking. This was computed by calculating the percentage of pairs of scFs at positions (n, m) such that n &lt; m in the s</context>
</contexts>
<marker>Taylor, Knowles, 1988</marker>
<rawString>Taylor, L. and Knowles, G. 1988. Manual of information to accompany the SEC corpus: the machine-readable corpus of spoken English. University of Lancaster, UK, Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
<author>D Evans</author>
<author>T Gibson</author>
<author>A Waibel</author>
</authors>
<title>The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.</title>
<date>1993</date>
<booktitle>SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text.</booktitle>
<pages>95--106</pages>
<editor>In Boguraev, B. and Pustejovsky, J. eds.</editor>
<location>Columbus, Ohio:</location>
<contexts>
<context position="1489" citStr="Ushioda et al., 1993" startWordPosition="199" endWordPosition="202">cuss using the method to benefit large-scale lexical acquisition. 1 Introduction Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Yet parsers depend crucially on such information, and probabilistic parsers would greatly benefit from accurate information concerning the relative frequency of different subcategorization frames (scFs) for a given predicate. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall&apos;. 1Token recall is the percentage of SCF tokens in a sample of manually analysed text that were correctly acquired by the system. One significant source of error lies in the statistical filtering methods frequently used to remove noise from automatically acquired scFs. These met</context>
</contexts>
<marker>Ushioda, Evans, Gibson, Waibel, 1993</marker>
<rawString>Ushioda, A., Evans, D., Gibson, T. and Waibel, A. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In Boguraev, B. and Pustejovsky, J. eds. SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text. Columbus, Ohio: 95-106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>