<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011929">
<title confidence="0.975622">
UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands
</title>
<author confidence="0.997351">
Woodley Packard
</author>
<affiliation confidence="0.998052">
University of Washington
</affiliation>
<email confidence="0.987684">
sweaglesw@sweaglesw.org
</email>
<sectionHeader confidence="0.993617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999816">
This paper describes a deep-parsing ap-
proach to SemEval-2014 Task 6, a novel
context-informed supervised parsing and
semantic analysis problem in a controlled
domain. The system comprises a hand-
built rule-based solution based on a pre-
existing broad coverage deep grammar of
English, backed up by a off-the-shelf data-
driven PCFG parser, and achieves the best
score reported among the task participants.
</bodyText>
<sectionHeader confidence="0.998658" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.90651628125">
SemEval-2014 Task 6 involves automatic transla-
tion of natural language commands for a robotic
arm into structured “robot control language”
(RCL) instructions (Dukes, 2013a). Statements of
RCL are trees, with a fixed vocabulary of content
words like prism at the leaves, and markup like
action: or destination: at the nonterminals.
The yield of the tree largely aligns with the words
in the command, but there are frequently substitu-
tions, insertions, and deletions.
A unique and interesting property of this task
is the availability of highly relevant machine-
readable descriptions of the spatial context of each
command. Given a candidate RCL fragment de-
scribing an object to be manipulated, a spatial
planner provided by the task organizers can auto-
matically enumerate the set of task-world objects
that match the description. This information can
be used to resolve some of the ambiguity inherent
in natural language.
The commands come from the Robot Com-
mands Treebank (Dukes, 2013a), a crowdsourced
corpus built using a game with a purpose (von
Ahn, 2006). Style varies considerably, with miss-
ing determiners, missing or unexpected punc-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organizers. Licence details:
http://creativecommons.org/licenses/by/4.0/
tuation, and missing capitalization all common
(Dukes, 2013b). Examples (1) and (2) show typi-
cal commands from the dataset.
</bodyText>
<listItem confidence="0.991526">
(1) drop the blue cube
(2) Pick yellow cube and drop it on top of blue cube
</listItem>
<bodyText confidence="0.998969">
Although the natural language commands vary
in their degree of conformance to what might be
called standard English, the hand-built gold stan-
dard RCL annotations provided with them (e.g.
Figure 1) are commendable in their uniformity and
accuracy, in part because they have been automat-
ically verified against the formal before and after
scene descriptions using the spatial planner.
</bodyText>
<equation confidence="0.944405333333333">
(event: (action: drop)
(entity: (color: blue)
(type: cube))
</equation>
<figureCaption confidence="0.995289">
Figure 1: RCL corresponding to Example (1).
</figureCaption>
<sectionHeader confidence="0.998803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999860714285714">
Automatic interpretation of natural language is
a difficult and long-standing research problem.
Some approaches have taken a relatively shal-
low view; for instance, ELIZA (Weizenbaum,
1966) used pattern matching to somewhat con-
vincingly participate in an English conversation.
Approaches taking a deeper view tend to parse
utterances into structured representations. These
are usually abstract and general-purpose in na-
ture, e.g. the syntax trees produced by main-
stream PCFG parsers and the DRS produced by
the Boxer system (Bos, 2008). As a notable ex-
ception, Dukes (2014) presents a novel method to
produce RCL output directly.
The English Resource Grammar (ERG;
Flickinger, 2000) employed as a component in
the present work is a broad-coverage precision
hand-written unification grammar of English,
following the Head-driven Phrase Structure
Grammar theory of syntax (Pollard &amp; Sag, 1994).
The ERG produces Minimal Recursion Semantics
</bodyText>
<page confidence="0.971397">
812
</page>
<note confidence="0.731484">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.922135454545455">
(MRS; Copestake et al., 2005) analyses, which
are flat structures that explicitly encode predicate
argument relations (and other data). A simplified
MRS structure is shown in Figure 2. With minor
modifications to allow determinerless NPs and
some unexpected measure noun lexemes (as in
“two squares to the left”, etc), the ERG yields
analyses for 99% of the commands in the training
portion of the Robot Command Treebank.
(INDEX = e, {pron(x), cube n(y),
drop v cause(e, x, y), blue a( , y)})
</bodyText>
<figureCaption confidence="0.9686085">
Figure 2: Highly simplified view of the MRS pro-
duced by the ERG for Example (1).
</figureCaption>
<sectionHeader confidence="0.986541" genericHeader="method">
3 ERG-based RCL Synthesis
</sectionHeader>
<bodyText confidence="0.9999384">
This section outlines the method my sys-
tem employs to synthesize RCL outputs from
the MRS analyses produced by the ERG.
The ERG provides a ranked list of candidate
MRS analyses for each input. As a first step,
grossly inappropriate analyses are ruled out, e.g.
those proposing non-imperative main verbs or
domain-inappropriate parts of speech (“block” as
a verb). An attempt is made to convert each re-
maining analysis into a candidate RCL statement.
If conversion is successful, the result is tested for
coherence with respect to the known world state,
using the supplied spatial planner. An RCL state-
ment is incoherent if it involves picking up or mov-
ing an entity which does not exist, or if its com-
mand type (take, move, drop) is incompatible
with the current state of the robot arm, e.g. drop
is incoherent when the robot arm is not holding
anything. Processing stops as soon as a coherent
result is found.1
</bodyText>
<subsectionHeader confidence="0.995051">
3.1 From MRS to RCL
</subsectionHeader>
<bodyText confidence="0.999974285714286">
Given an individual (imperative) MRS structure,
the first step in conversion to RCL is to iden-
tify the sequence of top-level verbal predications.
The INDEX property of the MRS provides an en-
try point. In a simple command like Example (1),
the INDEX will point to a single verbal predica-
tion, whereas in a compound command such as
</bodyText>
<footnote confidence="0.989085">
1Practically speaking, conversion from MRS to RCL is
accomplished by a relatively short C program embodying
these rules and steps (about 1500 lines in the final version):
http://sweaglesw.org/svn/semeval-2014-task6/tags/dublin
</footnote>
<bodyText confidence="0.999873265306123">
Example (2), the INDEX will point to a coordina-
tion predication, which itself will have left and
right arguments which must be visited recursively.
Each verbal predication visited in this manner gen-
erates an event: RCL statement whose action:
property is determined by a looking up the ver-
bal predicate in a short hand-written table (e.g.
drop v cause maps to action: drop). If the
predicate is not found in the table, the most com-
mon action move is guessed.
Every RCL event: element must have an
entity: subelement, representing the object to
be moved by the action. Although in princi-
ple MRS makes no guarantees about the gener-
alizability of the semantic interpretation of argu-
ment roles across different predicates, in prac-
tice the third argument of every verbal predicate
relevant to this domain represents the object to
be moved; hence, synthesis of an event: pro-
ceeds by inspecting the third argument of the
MRS predicate which gave rise to it. Some types
of event: also involve a destination: subele-
ment, which encodes the location where the en-
tity should come to rest. When present, a verbal
predicate’s fourth argument almost always iden-
tifies a prepositional predication holding this in-
formation, although there are exceptions (e.g. for
move v from-to rel it is the fifth). When no such
resultative role is present, the first prepositional
modifier (if any) of the verbal event variable is
used for the destination: subelement.
Synthesis of an entity: element from a
referential index like y in Figure 2 or a
spatial-relation: element from a preposi-
tional predication proceeds in much the same way:
the RCL type: or relation: is determined by
a simple table lookup, and subelements are built
based on connections indicated in the MRS. One
salient difference is the treatment of predicates
that are not found in their respective lookup ta-
bles. Whereas unknown command predicates de-
fault to the most common action move, unknown
modifying spatial relations are simply dropped,2
and unknown entity types cause conversion to fail,
on the theory that an incorrect parse is likely. Pru-
dent rejection of suspect parses only rarely elim-
inates all available analyses, and generally helps
to find the most appropriate one. On development
data, the first analysis produced by the ERG was
</bodyText>
<footnote confidence="0.994087">
2If the spatial relation is part of a mandatory
destination: element, this can then cause conversion to fail.
</footnote>
<page confidence="0.997725">
813
</page>
<bodyText confidence="0.999692">
convertible for 87% of commands, and the first
RCL hypothesis was spatially coherent for 96% of
commands. These numbers indicate that the parse
ranking component of the ERG works quite well.
</bodyText>
<subsectionHeader confidence="0.999768">
3.2 Polishing the Rules
</subsectionHeader>
<bodyText confidence="0.999996074074074">
I split the 2500 task-supplied annotated commands
into a randomly-divided training set (2000 com-
mands) and development set (500 commands).
Throughout this work, the development set was
only used for estimating performance on unseen
data and tuning system combination settings; the
contents of the development set were never in-
spected for rule writing or error analysis pur-
poses. Although the conversion architecture out-
lined above constitutes an effective framework,
there were quite a few details to be worked
through, such as the construction of the lookup ta-
bles, identification of cases requiring special han-
dling, elimination of undesirable parses, modest
extension of the ERG, etc. An error-analysis
tool which performed a fine-grained comparison
of the synthesized RCL statements with the gold-
standard ones and agglomerated common error
types proved invaluable when writing rules. 3 Pol-
ishing the system in this manner took about two
weeks of part-time effort; I maintained a log giv-
ing a short summary of each tweak (e.g. “map
center n of rel to type: region”). These
tweaks required varying amounts of time to imple-
ment, from a few seconds up to perhaps an hour;
system accuracy as a function of the number of
such tweaks is shown in Figure 3.
</bodyText>
<subsectionHeader confidence="0.99756">
3.3 Anaphora and Ellipsis
</subsectionHeader>
<bodyText confidence="0.999980846153846">
Some commands use anaphora to evoke the iden-
tity or type of previously mentioned entities. Typ-
ically, the pronoun “it” refers to a specific entity
while the pronoun “one” refers to the type of an
entity (e.g. “Put the red cube on the blue one.”).
Empirically, the antecedent is nearly always the
first entity: element in the RCL statement, and
this heuristic works well in the system. A small
fraction of commands (&lt; 0.5% of the training
data) elide the pronoun, in commands like “Take
the blue tetrahedron and place in front left corner.”
In principle these could be detected and accommo-
dated through the addition of a simple mal-rule to
</bodyText>
<footnote confidence="0.715968">
3The error-analysis tool walks the system and gold
RCL trees in tandem, recording differences and printing the
most common mismatches. It consists of about 100 lines of
Python and shell script, and took perhaps an hour to build.
</footnote>
<figure confidence="0.964403">
10 20 30 40 50 60 70 80
Approx. Number of Tweaks
</figure>
<figureCaption confidence="0.99977325">
Figure 3: Tuning the MRS-to-RCL conversion
system by tweaking/adding rules. Development-
set accuracy was only checked occasionally during
rule-writing to avoid over-fitting.
</figureCaption>
<bodyText confidence="0.9817945">
the ERG (Bender et al., 2004), but for simplicity
my system ignores this problem, leading to errors.
</bodyText>
<sectionHeader confidence="0.992401" genericHeader="method">
4 Robustness Strategies
</sectionHeader>
<bodyText confidence="0.999955032258064">
If none of the analyses produced by the ERG result
in coherent RCL statements, the system produces
no output. On the one hand this results in quite a
high precision: on the training data, 96.75% of the
RCL statements produced are exactly correct. On
the other hand, in some scenarios a lower precision
result may be preferable to no result. The ERG-
based system fails to produce any output for 3.1%
of the training data inputs, a number that should be
expected to increase for unseen data (since conver-
sion can sometimes fail when the MRS contains
unrecognized predicates).
In order to produce a best-guess answer for
these remaining items, I employed the Berkeley
parser (Petrov et al., 2006), a state-of-the-art data-
driven system that induces a PCFG from a user-
supplied corpus of strings annotated with parse
trees. The RCL treebank is not directly suitable as
training material for the Berkeley parser, since the
yield of an RCL tree is not identical to (or even
in 1-to-1 correspondence with) the words of the
input utterance. In the interest of keeping things
simple, I produced a phrase structure translation
of the RCL treebank by simply discarding the el-
ements of the RCL trees that did not correspond
to any input, and inserting (X word) nodes for in-
put words that were not aligned to any RCL frag-
ment. The question of where in the tree to insert
these X nodes is presumably of considerable im-
portance, but again in the interest of simplicity I
simply clustered them together with the first RCL-
</bodyText>
<figure confidence="0.9467177">
Training Set
Development Set
Accuracy 100
80
60
40
20
0
814
S
</figure>
<figureCaption confidence="0.8945915">
Figure 4: Automatic phrase structure tree transla-
tion of the RCL statement shown in Figure 1.
</figureCaption>
<bodyText confidence="0.999782866666667">
aligned word appearing after them. Unaligned in-
put tokens at the end of the sentence were added
as siblings of the root node. Figure 4 shows the
phrase structure tree resulting from the translation
of the RCL statement shown in Figure 1.
Using this phrase structure treebank, the Berke-
ley parser tools make it possible to automatically
derive a similar phrase structure tree for any input
string, and indeed when the input string is a com-
mand such as the ones of interest in this work, the
resulting tree is quite close to an RCL statement.
Deletion of the X nodes yields a robust system
that frequently produces the exact correct RCL,
at least for those items where only input-aligned
RCL leaves are required. The most common type
of non-input-aligned RCL fragment is the id: el-
ement, identifying the antecedent of an anaphor.
As with the ERG-based system, a heuristic select-
ing the first entity as the antecedent whenever an
anaphor is present works quite well.
Improving the output of the statistical system
via tweaks of the type used in the ERG-based sys-
tem was much more challenging, due to the rel-
ative impoverishedness of the information made
available by the parser. Accurately detecting situ-
ations to improve without causing collateral dam-
age proved difficult. However, the base accu-
racy of the statistical system was quite good, and
when used as a back-off it improved overall sys-
tem scores considerably, as shown in Table 5.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9998295">
The combined system performs best on both por-
tions of the data. Over the development data, the
MRS-based system performs considerably better
than the statistical system, in part due to the use
of spatial planning in the MRS-based system (time
did not permit adding spatial planning to the statis-
</bodyText>
<table confidence="0.999320625">
System Dev R P Eval
P R
MRS-only (−SP) 90.7 88.0 92.1 80.3
MRS-only (+SP) 95.4 92.2 96.1 82.4
Robust-only (−SP) 88.2 88.2 81.5 81.5
Combined (−SP) 90.8 90.8 90.5 90.5
Combined (+SP) 95.0 95.0 92.5 92.5
ERG coverage 98.6 91.0
</table>
<figureCaption confidence="0.88683325">
Figure 5: Evaluation results. ±SP indicates
whether or not spatial planning was used. The ro-
bust and combined systems always returned a re-
sult, so P = R.
</figureCaption>
<bodyText confidence="0.996758533333333">
tical system). The statistical system has a slightly
higher recall than the MRS-only system without
spatial planning, but the MRS-only system has a
higher precision — markedly so on the evalua-
tion data. This is consistent with previous find-
ings combining precision grammars with statisti-
cal systems (Packard et al., 2014).
ERG coverage dropped precipitously from
roughly 99% on the development data to 91%
on the evaluation data. This is likely the major
cause of the 10% absolute drop in the recall of the
MRS-only system. The fact that the robust sta-
tistical system encounters a comparable drop on
the evaluation data suggests that the text is qual-
itatively different from the (also held-out) devel-
opment data. One possible explanation is that
whereas the development data was randomly se-
lected from the 2500 task-provided training com-
mands, the evaluation data was taken as the se-
quentially following segment of the treebank, re-
sulting in the same distribution of game-with-a-
purpose participants (and hence writing styles) be-
tween the training and development sets but a dif-
ferent distribution for the evaluation data. 4
Dukes (2014) reports an accuracy of 96.53%,
which appears to be superior to the present system;
however, that system appears to have used more
training data than was available for the shared task,
and averaged scores over the entire treebank, mak-
ing direct comparison difficult.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.886186875">
I am grateful to Dan Flickinger, Emily Bender and
Stephan Oepen for their many helpful suggestions.
4Reviewers suggested dropping sentence initial punctua-
tion and reading “cell” as “tile.” This trick boosts the MRS-
only recall to 91.1% and the combined system to 94.5%,
demonstrating both the frailty of NLP systems to unexpected
inputs and the presence of surprises in the evaluation data.
ERG coverage rose from 91.0% to 98.6%.
</bodyText>
<figure confidence="0.996345214285714">
event:
✟✟✟ ❍ ❍ ❍
action:
drop color:
✟✟ ❍
entity:
❍
✟ ❍
drop
X blue
the blue
type:
cube
cube
</figure>
<page confidence="0.994801">
815
</page>
<sectionHeader confidence="0.995456" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998817959183673">
Bender, E. M., Flickinger, D., Oepen, S., Walsh, A., &amp;
Baldwin, T. (2004). Arboretum: Using a preci-
sion grammar for grammar checking in CALL.
In Instil/icall symposium 2004.
Bos, J. (2008). Wide-coverage semantic analysis with
boxer. In J. Bos &amp; R. Delmonte (Eds.), Seman-
tics in text processing. step 2008 conference pro-
ceedings (pp. 277–286). College Publications.
Copestake, A., Flickinger, D., Pollard, C., &amp; Sag, I.
(2005). Minimal recursion semantics: An intro-
duction. Research on Language &amp; Computation,
3(2), 281–332.
Dukes, K. (2013a). Semantic annotation of robotic
spatial commands. In Language and Technology
Conference (LTC). Poznan, Poland.
Dukes, K. (2013b). Train robots: A dataset for
natural language human-robot spatial interac-
tion through verbal commands. In Interna-
tional Conference on Social Robotics (ICSR).
Embodied Communication of Goals and Inten-
tions Workshop.
Dukes, K. (2014). Contextual Semantic Parsing using
Crowdsourced Spatial Descriptions. Computa-
tion and Language. arXiv:1405.0145 [cs.CL].
Flickinger, D. (2000). On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(01), 15-28.
Packard, W., Bender, E. M., Read, J., Oepen, S., &amp; Dri-
dan, R. (2014). Simple negation scope reso-
lution through deep parsing: A semantic solu-
tion to a semantic problem. In Proceedings of
the 52nd annual meeting of the Association for
Computational Linguistics. Baltimore, USA.
Petrov, S., Barrett, L., Thibaux, R., &amp;Klein, D. (2006).
Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st Inter-
national Conference on Computational Linguis-
tics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics (pp. 433–
440).
Pollard, C., &amp; Sag, I. A. (1994). Head-Driven Phrase
Structure Grammar. Chicago, USA: The Uni-
versity of Chicago Press.
von Ahn, L. (2006). Games with a purpose. Computer,
39(6), 92–94.
Weizenbaum, J. (1966). ELIZA — a computer pro-
gram for the study of natural language commu-
nication between man and machine. Communi-
cations of the ACM, 9(1), 36–45.
</reference>
<page confidence="0.998816">
816
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978205">
<title confidence="0.999284">UW-MRS: Leveraging a Deep Grammar for Robotic Spatial Commands</title>
<author confidence="0.996152">Woodley Packard</author>
<affiliation confidence="0.999667">University of Washington</affiliation>
<email confidence="0.993449">sweaglesw@sweaglesw.org</email>
<abstract confidence="0.998982">This paper describes a deep-parsing approach to SemEval-2014 Task 6, a novel context-informed supervised parsing and semantic analysis problem in a controlled domain. The system comprises a handbuilt rule-based solution based on a preexisting broad coverage deep grammar of English, backed up by a off-the-shelf datadriven PCFG parser, and achieves the best score reported among the task participants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>A Walsh</author>
<author>T Baldwin</author>
</authors>
<title>Arboretum: Using a precision grammar for grammar checking in CALL.</title>
<date>2004</date>
<booktitle>In Instil/icall symposium</booktitle>
<contexts>
<context position="10787" citStr="Bender et al., 2004" startWordPosition="1732" endWordPosition="1735">ahedron and place in front left corner.” In principle these could be detected and accommodated through the addition of a simple mal-rule to 3The error-analysis tool walks the system and gold RCL trees in tandem, recording differences and printing the most common mismatches. It consists of about 100 lines of Python and shell script, and took perhaps an hour to build. 10 20 30 40 50 60 70 80 Approx. Number of Tweaks Figure 3: Tuning the MRS-to-RCL conversion system by tweaking/adding rules. Developmentset accuracy was only checked occasionally during rule-writing to avoid over-fitting. the ERG (Bender et al., 2004), but for simplicity my system ignores this problem, leading to errors. 4 Robustness Strategies If none of the analyses produced by the ERG result in coherent RCL statements, the system produces no output. On the one hand this results in quite a high precision: on the training data, 96.75% of the RCL statements produced are exactly correct. On the other hand, in some scenarios a lower precision result may be preferable to no result. The ERGbased system fails to produce any output for 3.1% of the training data inputs, a number that should be expected to increase for unseen data (since conversio</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Walsh, Baldwin, 2004</marker>
<rawString>Bender, E. M., Flickinger, D., Oepen, S., Walsh, A., &amp; Baldwin, T. (2004). Arboretum: Using a precision grammar for grammar checking in CALL. In Instil/icall symposium 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer. In</title>
<date>2008</date>
<booktitle>Semantics in text processing. step 2008 conference proceedings</booktitle>
<pages>277--286</pages>
<publisher>College Publications.</publisher>
<contexts>
<context position="3141" citStr="Bos, 2008" startWordPosition="476" endWordPosition="477">e) (type: cube)) Figure 1: RCL corresponding to Example (1). 2 Related Work Automatic interpretation of natural language is a difficult and long-standing research problem. Some approaches have taken a relatively shallow view; for instance, ELIZA (Weizenbaum, 1966) used pattern matching to somewhat convincingly participate in an English conversation. Approaches taking a deeper view tend to parse utterances into structured representations. These are usually abstract and general-purpose in nature, e.g. the syntax trees produced by mainstream PCFG parsers and the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar of English, following the Head-driven Phrase Structure Grammar theory of syntax (Pollard &amp; Sag, 1994). The ERG produces Minimal Recursion Semantics 812 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816, Dublin, Ireland, August 23-24, 2014. (MRS; Copestake et al., 2005) analyses, which are flat s</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Bos, J. (2008). Wide-coverage semantic analysis with boxer. In J. Bos &amp; R. Delmonte (Eds.), Semantics in text processing. step 2008 conference proceedings (pp. 277–286). College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>281--332</pages>
<contexts>
<context position="3714" citStr="Copestake et al., 2005" startWordPosition="557" endWordPosition="560">d the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar of English, following the Head-driven Phrase Structure Grammar theory of syntax (Pollard &amp; Sag, 1994). The ERG produces Minimal Recursion Semantics 812 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816, Dublin, Ireland, August 23-24, 2014. (MRS; Copestake et al., 2005) analyses, which are flat structures that explicitly encode predicate argument relations (and other data). A simplified MRS structure is shown in Figure 2. With minor modifications to allow determinerless NPs and some unexpected measure noun lexemes (as in “two squares to the left”, etc), the ERG yields analyses for 99% of the commands in the training portion of the Robot Command Treebank. (INDEX = e, {pron(x), cube n(y), drop v cause(e, x, y), blue a( , y)}) Figure 2: Highly simplified view of the MRS produced by the ERG for Example (1). 3 ERG-based RCL Synthesis This section outlines the met</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Copestake, A., Flickinger, D., Pollard, C., &amp; Sag, I. (2005). Minimal recursion semantics: An introduction. Research on Language &amp; Computation, 3(2), 281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dukes</author>
</authors>
<title>Semantic annotation of robotic spatial commands.</title>
<date>2013</date>
<booktitle>In Language and Technology Conference (LTC).</booktitle>
<location>Poznan, Poland.</location>
<contexts>
<context position="724" citStr="Dukes, 2013" startWordPosition="102" endWordPosition="103">@sweaglesw.org Abstract This paper describes a deep-parsing approach to SemEval-2014 Task 6, a novel context-informed supervised parsing and semantic analysis problem in a controlled domain. The system comprises a handbuilt rule-based solution based on a preexisting broad coverage deep grammar of English, backed up by a off-the-shelf datadriven PCFG parser, and achieves the best score reported among the task participants. 1 Introduction SemEval-2014 Task 6 involves automatic translation of natural language commands for a robotic arm into structured “robot control language” (RCL) instructions (Dukes, 2013a). Statements of RCL are trees, with a fixed vocabulary of content words like prism at the leaves, and markup like action: or destination: at the nonterminals. The yield of the tree largely aligns with the words in the command, but there are frequently substitutions, insertions, and deletions. A unique and interesting property of this task is the availability of highly relevant machinereadable descriptions of the spatial context of each command. Given a candidate RCL fragment describing an object to be manipulated, a spatial planner provided by the task organizers can automatically enumerate </context>
<context position="1964" citStr="Dukes, 2013" startWordPosition="292" endWordPosition="293"> that match the description. This information can be used to resolve some of the ambiguity inherent in natural language. The commands come from the Robot Commands Treebank (Dukes, 2013a), a crowdsourced corpus built using a game with a purpose (von Ahn, 2006). Style varies considerably, with missing determiners, missing or unexpected puncThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/ tuation, and missing capitalization all common (Dukes, 2013b). Examples (1) and (2) show typical commands from the dataset. (1) drop the blue cube (2) Pick yellow cube and drop it on top of blue cube Although the natural language commands vary in their degree of conformance to what might be called standard English, the hand-built gold standard RCL annotations provided with them (e.g. Figure 1) are commendable in their uniformity and accuracy, in part because they have been automatically verified against the formal before and after scene descriptions using the spatial planner. (event: (action: drop) (entity: (color: blue) (type: cube)) Figure 1: RCL co</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Dukes, K. (2013a). Semantic annotation of robotic spatial commands. In Language and Technology Conference (LTC). Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dukes</author>
</authors>
<title>Train robots: A dataset for natural language human-robot spatial interaction through verbal commands.</title>
<date>2013</date>
<booktitle>In International Conference on Social Robotics (ICSR). Embodied Communication of Goals and Intentions Workshop.</booktitle>
<contexts>
<context position="724" citStr="Dukes, 2013" startWordPosition="102" endWordPosition="103">@sweaglesw.org Abstract This paper describes a deep-parsing approach to SemEval-2014 Task 6, a novel context-informed supervised parsing and semantic analysis problem in a controlled domain. The system comprises a handbuilt rule-based solution based on a preexisting broad coverage deep grammar of English, backed up by a off-the-shelf datadriven PCFG parser, and achieves the best score reported among the task participants. 1 Introduction SemEval-2014 Task 6 involves automatic translation of natural language commands for a robotic arm into structured “robot control language” (RCL) instructions (Dukes, 2013a). Statements of RCL are trees, with a fixed vocabulary of content words like prism at the leaves, and markup like action: or destination: at the nonterminals. The yield of the tree largely aligns with the words in the command, but there are frequently substitutions, insertions, and deletions. A unique and interesting property of this task is the availability of highly relevant machinereadable descriptions of the spatial context of each command. Given a candidate RCL fragment describing an object to be manipulated, a spatial planner provided by the task organizers can automatically enumerate </context>
<context position="1964" citStr="Dukes, 2013" startWordPosition="292" endWordPosition="293"> that match the description. This information can be used to resolve some of the ambiguity inherent in natural language. The commands come from the Robot Commands Treebank (Dukes, 2013a), a crowdsourced corpus built using a game with a purpose (von Ahn, 2006). Style varies considerably, with missing determiners, missing or unexpected puncThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. Licence details: http://creativecommons.org/licenses/by/4.0/ tuation, and missing capitalization all common (Dukes, 2013b). Examples (1) and (2) show typical commands from the dataset. (1) drop the blue cube (2) Pick yellow cube and drop it on top of blue cube Although the natural language commands vary in their degree of conformance to what might be called standard English, the hand-built gold standard RCL annotations provided with them (e.g. Figure 1) are commendable in their uniformity and accuracy, in part because they have been automatically verified against the formal before and after scene descriptions using the spatial planner. (event: (action: drop) (entity: (color: blue) (type: cube)) Figure 1: RCL co</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Dukes, K. (2013b). Train robots: A dataset for natural language human-robot spatial interaction through verbal commands. In International Conference on Social Robotics (ICSR). Embodied Communication of Goals and Intentions Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dukes</author>
</authors>
<title>Contextual Semantic Parsing using Crowdsourced Spatial Descriptions. Computation and Language.</title>
<date>2014</date>
<note>arXiv:1405.0145 [cs.CL].</note>
<contexts>
<context position="3179" citStr="Dukes (2014)" startWordPosition="483" endWordPosition="484">sponding to Example (1). 2 Related Work Automatic interpretation of natural language is a difficult and long-standing research problem. Some approaches have taken a relatively shallow view; for instance, ELIZA (Weizenbaum, 1966) used pattern matching to somewhat convincingly participate in an English conversation. Approaches taking a deeper view tend to parse utterances into structured representations. These are usually abstract and general-purpose in nature, e.g. the syntax trees produced by mainstream PCFG parsers and the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar of English, following the Head-driven Phrase Structure Grammar theory of syntax (Pollard &amp; Sag, 1994). The ERG produces Minimal Recursion Semantics 812 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816, Dublin, Ireland, August 23-24, 2014. (MRS; Copestake et al., 2005) analyses, which are flat structures that explicitly encode predi</context>
<context position="15813" citStr="Dukes (2014)" startWordPosition="2590" endWordPosition="2591">The fact that the robust statistical system encounters a comparable drop on the evaluation data suggests that the text is qualitatively different from the (also held-out) development data. One possible explanation is that whereas the development data was randomly selected from the 2500 task-provided training commands, the evaluation data was taken as the sequentially following segment of the treebank, resulting in the same distribution of game-with-apurpose participants (and hence writing styles) between the training and development sets but a different distribution for the evaluation data. 4 Dukes (2014) reports an accuracy of 96.53%, which appears to be superior to the present system; however, that system appears to have used more training data than was available for the shared task, and averaged scores over the entire treebank, making direct comparison difficult. Acknowledgements I am grateful to Dan Flickinger, Emily Bender and Stephan Oepen for their many helpful suggestions. 4Reviewers suggested dropping sentence initial punctuation and reading “cell” as “tile.” This trick boosts the MRSonly recall to 91.1% and the combined system to 94.5%, demonstrating both the frailty of NLP systems t</context>
</contexts>
<marker>Dukes, 2014</marker>
<rawString>Dukes, K. (2014). Contextual Semantic Parsing using Crowdsourced Spatial Descriptions. Computation and Language. arXiv:1405.0145 [cs.CL].</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>01</issue>
<pages>15--28</pages>
<contexts>
<context position="3288" citStr="Flickinger, 2000" startWordPosition="499" endWordPosition="500">long-standing research problem. Some approaches have taken a relatively shallow view; for instance, ELIZA (Weizenbaum, 1966) used pattern matching to somewhat convincingly participate in an English conversation. Approaches taking a deeper view tend to parse utterances into structured representations. These are usually abstract and general-purpose in nature, e.g. the syntax trees produced by mainstream PCFG parsers and the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar of English, following the Head-driven Phrase Structure Grammar theory of syntax (Pollard &amp; Sag, 1994). The ERG produces Minimal Recursion Semantics 812 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816, Dublin, Ireland, August 23-24, 2014. (MRS; Copestake et al., 2005) analyses, which are flat structures that explicitly encode predicate argument relations (and other data). A simplified MRS structure is shown in Figure 2. With minor modific</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Flickinger, D. (2000). On building a more efficient grammar by exploiting types. Natural Language Engineering, 6(01), 15-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Packard</author>
<author>E M Bender</author>
<author>J Read</author>
<author>S Oepen</author>
<author>R Dridan</author>
</authors>
<title>Simple negation scope resolution through deep parsing: A semantic solution to a semantic problem.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.</booktitle>
<location>Baltimore, USA.</location>
<contexts>
<context position="14998" citStr="Packard et al., 2014" startWordPosition="2455" endWordPosition="2458">92.2 96.1 82.4 Robust-only (−SP) 88.2 88.2 81.5 81.5 Combined (−SP) 90.8 90.8 90.5 90.5 Combined (+SP) 95.0 95.0 92.5 92.5 ERG coverage 98.6 91.0 Figure 5: Evaluation results. ±SP indicates whether or not spatial planning was used. The robust and combined systems always returned a result, so P = R. tical system). The statistical system has a slightly higher recall than the MRS-only system without spatial planning, but the MRS-only system has a higher precision — markedly so on the evaluation data. This is consistent with previous findings combining precision grammars with statistical systems (Packard et al., 2014). ERG coverage dropped precipitously from roughly 99% on the development data to 91% on the evaluation data. This is likely the major cause of the 10% absolute drop in the recall of the MRS-only system. The fact that the robust statistical system encounters a comparable drop on the evaluation data suggests that the text is qualitatively different from the (also held-out) development data. One possible explanation is that whereas the development data was randomly selected from the 2500 task-provided training commands, the evaluation data was taken as the sequentially following segment of the tr</context>
</contexts>
<marker>Packard, Bender, Read, Oepen, Dridan, 2014</marker>
<rawString>Packard, W., Bender, E. M., Read, J., Oepen, S., &amp; Dridan, R. (2014). Simple negation scope resolution through deep parsing: A semantic solution to a semantic problem. In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics. Baltimore, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D &amp;Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics</booktitle>
<pages>433--440</pages>
<marker>Petrov, Barrett, Thibaux, &amp;Klein, 2006</marker>
<rawString>Petrov, S., Barrett, L., Thibaux, R., &amp;Klein, D. (2006). Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (pp. 433– 440).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press.</publisher>
<location>Chicago, USA:</location>
<contexts>
<context position="3497" citStr="Pollard &amp; Sag, 1994" startWordPosition="527" endWordPosition="530">tion. Approaches taking a deeper view tend to parse utterances into structured representations. These are usually abstract and general-purpose in nature, e.g. the syntax trees produced by mainstream PCFG parsers and the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar of English, following the Head-driven Phrase Structure Grammar theory of syntax (Pollard &amp; Sag, 1994). The ERG produces Minimal Recursion Semantics 812 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 812–816, Dublin, Ireland, August 23-24, 2014. (MRS; Copestake et al., 2005) analyses, which are flat structures that explicitly encode predicate argument relations (and other data). A simplified MRS structure is shown in Figure 2. With minor modifications to allow determinerless NPs and some unexpected measure noun lexemes (as in “two squares to the left”, etc), the ERG yields analyses for 99% of the commands in the training portion of the Robot Command </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C., &amp; Sag, I. A. (1994). Head-Driven Phrase Structure Grammar. Chicago, USA: The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L von Ahn</author>
</authors>
<title>Games with a purpose.</title>
<date>2006</date>
<journal>Computer,</journal>
<volume>39</volume>
<issue>6</issue>
<pages>92--94</pages>
<marker>von Ahn, 2006</marker>
<rawString>von Ahn, L. (2006). Games with a purpose. Computer, 39(6), 92–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weizenbaum</author>
</authors>
<title>ELIZA — a computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Communications of the ACM,</journal>
<volume>9</volume>
<issue>1</issue>
<pages>36--45</pages>
<contexts>
<context position="2795" citStr="Weizenbaum, 1966" startWordPosition="423" endWordPosition="424">e to what might be called standard English, the hand-built gold standard RCL annotations provided with them (e.g. Figure 1) are commendable in their uniformity and accuracy, in part because they have been automatically verified against the formal before and after scene descriptions using the spatial planner. (event: (action: drop) (entity: (color: blue) (type: cube)) Figure 1: RCL corresponding to Example (1). 2 Related Work Automatic interpretation of natural language is a difficult and long-standing research problem. Some approaches have taken a relatively shallow view; for instance, ELIZA (Weizenbaum, 1966) used pattern matching to somewhat convincingly participate in an English conversation. Approaches taking a deeper view tend to parse utterances into structured representations. These are usually abstract and general-purpose in nature, e.g. the syntax trees produced by mainstream PCFG parsers and the DRS produced by the Boxer system (Bos, 2008). As a notable exception, Dukes (2014) presents a novel method to produce RCL output directly. The English Resource Grammar (ERG; Flickinger, 2000) employed as a component in the present work is a broad-coverage precision hand-written unification grammar</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Weizenbaum, J. (1966). ELIZA — a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1), 36–45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>