<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000231">
<title confidence="0.99917">
A Cohesion Graph Based Approach for Unsupervised Recognition of
Literal and Non-literal Use of Multiword Expressions
</title>
<author confidence="0.988885">
Linlin Li and Caroline Sporleder
</author>
<affiliation confidence="0.983384">
Saarland University
</affiliation>
<address confidence="0.841592333333333">
Postfach 15 11 50
66041 Saarbr¨ucken
Germany
</address>
<email confidence="0.997573">
{linlin,csporled}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.99475" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946555555555">
We present a graph-based model for rep-
resenting the lexical cohesion of a dis-
course. In the graph structure, vertices cor-
respond to the content words of a text and
edges connecting pairs of words encode
how closely the words are related semanti-
cally. We show that such a structure can be
used to distinguish literal and non-literal
usages of multi-word expressions.
</bodyText>
<sectionHeader confidence="0.998403" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999794406779661">
Multiword expressions (MWEs) are defined as
“idiosyncratic interpretations that cross word
boundaries or spaces” (Sag et al., 2001). Such
expressions are pervasive in natural language;
they are estimated to be equivalent in number
to simplex words in mental lexicon (Jackendoff,
1997). MWEs exhibit a number of lexical, syn-
tactic, semantic, pragmatic and statistical idiosyn-
crasies: syntactic peculiarities (e.g., by and large,
ad hoc), semantic non-compositionality (e.g., as
in kick the bucket (die) and red tape (bureau-
cracy)), pragmatic idiosyncrasies (the expression
is sometimes associated with a fixed pragmatic
point, e.g., good morning, good night), variation
in syntactic flexibility (e.g., I handed in my thesis
= I handed my thesis in vs. Kim kicked the bucket
=� *the bucket was kicked by Kim), variation in
productivity (there are various levels of productiv-
ity for different MWEs, e.g., kick/*beat/*hit the
bucket, call/ring/phone/*telephone up).
These idiosyncrasies pose challenges for NLP
systems, which have to recognize that an expres-
sion is an MWE to deal with it properly. Recogniz-
ing MWEs has been shown to be useful for a num-
ber of applications such as information retrieval
(Lewis and Croft, 1990; Rila Mandala and Tanaka,
2000; Wacholder and Song, 2003) and POS tag-
ging (Piao et al., 2003). It has also been shown
that MWEs account for 8% of parsing errors with
precision grammars (Baldwin et al., 2004). Fur-
thermore, MWE detection is used in information
extraction (Lin, 1998b) and an integral component
of symbolic MT systems (Gerber and Yang, 1997;
Bond and Shirai, 1997).
However, the special properties of MWEs can
also be exploited to recognize MWEs automati-
cally. There have been many studies on MWEs:
identification (determining whether multiple sim-
plex words form a MWE in a given token context,
e.g. put the sweater on vs. put the sweater on
the table), extraction (recognizing MWEs as word
units at the type level), detecting or measuring
compositionality of MWEs, semantic interpreta-
tion (interpreting the semantic association among
components in MWEs).
To extract MWEs, various methods have been
proposed that exploit the syntactic and lexical
fixedness exhibited by MWEs, or apply various
statistical measures across all co-occurrence vec-
tors between the whole expression and its com-
ponent parts (see Section 2). These methods can
be used to automatically identify potentially id-
iomatic expressions at a type level, but they do not
say anything about the idiomaticity of an expres-
sion in a particular context. While some idioms
(e.g., ad hoc) are always used idiomatically, there
are numerous others that can be used both idiomat-
ically (see Example 1) and non-idiomatically (see
Example 2).
</bodyText>
<listItem confidence="0.545351833333333">
(1) When the members of De la Guarda aren’t
hanging around, they’re yelling and
bouncing off the wall.
(2) Blinded by the sun, Erstad leaped at the
wall, but the ball bounced off the wall well
below his glove.
</listItem>
<bodyText confidence="0.989713">
Our work aims to distinguish the literal and
non-literal usages of idiomatic expressions in a
</bodyText>
<page confidence="0.988014">
75
</page>
<note confidence="0.999713">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75–83,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999883894736842">
discourse context (so-called token based classifi-
cation). It is therefore different from type-based
approaches which aim to detect the general id-
iomaticity of an expression rather than its actual
usage in a particular context.
We utilize the cohesive structure of a discourse
(Halliday and Hasan, 1976) to distinguish literal or
non-literal usage of MWEs. The basic idea is that
the component words of an MWE contribute to the
cohesion of the discourse in the literal case, while
in the non-literal case they do not. For instance, in
the literal use of break the ice in Example 3, the
content word ice contributes to the overall seman-
tic connectivity of the whole sentence by the fact
that ice is semantically related to water. In con-
trast, in the non-literal example in 4, the word ice
does not contribute to the overall cohesion as it is
poorly connected to all the other (content) words
in this specific context (play, party, games).
</bodyText>
<listItem confidence="0.98540025">
(3) The water would break the ice into floes
with its accumulated energy.
(4) We played a couple of party games to
break the ice.
</listItem>
<bodyText confidence="0.960867852941176">
Our approach bears similarities to Hirst and St-
Onge’s (1998) method for detecting malapropisms
based on their non-participation in cohesive
chains. However, computing such chains requires
a pre-defined similarity threshold which governs
whether a word is placed in a particular chain. Set-
ting this threshold typically requires a manually la-
beled development set, which makes this method
weakly supervised. We propose an alternative,
parameter-free method in which we model the co-
hesive structure of a discourse as a graph structure
(called cohesion graph), where the vertices of the
graph correspond to the content words of the text
and the edges encode the semantic relatedness be-
tween pairs of words. To distinguish between lit-
eral and non-literal use of MWEs, we look at how
the average relatedness of the graph changes when
the component words of the MWE are excluded or
included in the graph (see Section 3).1
We first introduced the cohesion graph method
in Sporleder and Li (2009). In the present paper,
1By modeling lexical cohesion as a graph structure, we
follow earlier approaches in information retrieval, notably by
Salton and colleagues (Salton et al., 1994). The difference is
that these works aim at representing similarity between larger
text segments (e.g., paragraphs) in a so-called ’text’ or ’para-
graph relation map’, whose vertices correspond to a text seg-
ment and whose edges represent the similarity between the
segments (modeled as weighted term overlap).
we provide a formalization of the graph and ex-
periment with different vertex and edge weight-
ing schemes. We also report on experiments with
varying the size of the input context and also with
pruning the graph structure automatically.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999879463414634">
Type-based MWE classification aims to extract
multiword expression types in text from observa-
tions of the token distribution. It aims to pick
up on word combinations which occur with com-
paratively high frequencies when compared to the
frequencies of the individual words (Evert and
Krenn, 2001; Smadja, 19993). The lexical and
syntactic fixedness property can also be utilized to
automatically extract MWEs (Baldwin and Villav-
icencio, 2002).
The study of semantic compositionality of
MWEs focuses on the degree to which the seman-
tics of the parts of an MWE contribute towards the
meaning of the whole. The aim is a binary classi-
fication of the MWEs as idiosyncratically decom-
posable (e.g. spill the beans) or non-decomposable
(e.g. kick the bucket). Several approaches have
been proposed. Lin (1999) uses the substitution
test2 and mutual information (MI) to determine
the compositionality of the phrase. An obvious
change of the MI value of the phrase in the sub-
stitution test is taken as the evidence of the MWEs
being non-compositional. Bannard et al. (2003)
assume that compositional MWEs occur in sim-
ilar lexical context as their component parts. The
co-occurrence vector representations of verb parti-
cle construction (VPC) and the component words
are utilized to determine the compositionality of
the MWE.
There have also been a few token-based classi-
fication approaches, aimed at classifying individ-
ual instances of a potential idiom as literal or non-
literal. Katz and Giesbrecht (2006) make use of
latent semantic analysis (LSA) to explore the local
linguistic context that can serve to identify multi-
word expressions that have non-compositional
meaning. They measure the cosine vector similar-
ity between the vectors associated with an MWE
as a whole and the vectors associated with its con-
stituent parts and interpret it as the degree to which
the MWE is compositional. They report an av-
</bodyText>
<footnote confidence="0.999198333333333">
2The substitution test aims to replace part of the idiom’s
component words with semantically similar words, and test
how the co-occurrence frequency changes.
</footnote>
<page confidence="0.993877">
76
</page>
<bodyText confidence="0.999982375">
erage accuracy of 72%, but the data set used in
their evaluation is small. Birke and Sarkar (2006)
use literal and non-literal seed sets acquired with-
out human supervision to perform bootstrapping
learning. The new instances of potential idioms
are always labeled according to the closest set.
While their approach is unsupervised clustering,
they do rely on some resources such as databases
of idioms. Cook et al. (2007) and Fazly et al.
(2009) rely crucially on the concept of canonical
form (CForm). It is assumed that for each idiom
there is a fixed form (or a small set of those) cor-
responding to the syntactic pattern(s) in which the
idiom normally occurs. The canonical form al-
lows for inflection variation of the heard verb but
not for other variations (such as nominal inflec-
tion, choice of determiner etc.). It has been ob-
served that if an expression is used idiomatically
it typically occurs in its canonical form (Riehe-
mann, 2001). Fazly and her colleagues exploit this
behavior and propose an unsupervised method for
token-based idiom classification in which an ex-
pression is classified as idiomatic if it occurs in
canonical form and literal otherwise. The canon-
ical forms are determined automatically using a
statistical, frequency-based measure. They also
developed statistical measures to measure the lex-
ical and syntactic fixedness of a given expression,
which is used to automatically recognize expres-
sion types, as well as their token identification in
context. They report an average accuracy of 72%
for their canonical form (CForm) classifier.
</bodyText>
<sectionHeader confidence="0.959904" genericHeader="method">
3 Cohesion Graph
</sectionHeader>
<bodyText confidence="0.999917166666667">
In this section, we first give a formal definition of
the cohesion graph that is used for modeling dis-
course connectivity, then we define the discourse
connectivity. Finally, we introduced our graph-
based classifier for distinguishing literal and non-
literal use of MWEs.
</bodyText>
<subsectionHeader confidence="0.999514">
3.1 Cohesion Graph Structure
</subsectionHeader>
<bodyText confidence="0.9675803125">
A cohesion graph (CG) is an undirected complete
graph 3 G = (V, E), where
V : is a set of nodes {v1, v2, ..., vn}, where each
node vi = (ti, idi) represents a unique token in the
discourse. ti is the string form of the token, and idi
denotes the position of the token in the context.
3In the mathematical field of graph theory, a complete
graph is a simple graph in which every pair of distinct vertices
is connected by an edge. The complete graph on n vertices
has n(n − 1)/2 edges.
E: is a set of edges {e12, e13, ..., e(n)(n−1)},
such that each edge eij connects a pair of nodes
(vi, vj). n is the total number of tokens in the dis-
course that the graph models. The value of eij rep-
resents the semantic relatedness of the two tokens
ti, tj that eij connects:
</bodyText>
<equation confidence="0.940154">
eij = h(ti, tj) (5)
</equation>
<bodyText confidence="0.998443428571428">
where h is a semantic relatedness assignment
function. The explicit form of h will be discussed
in the next section.
ei is the average semantic relatedness of the to-
ken ti in the discourse. It represents the average
relatedness score of a certain token to its surround-
ing context:
</bodyText>
<equation confidence="0.964159285714286">
n
ei = λij x eij (6)
j=1,j7�i
where λij is the weight of the edge eij, with the
n
constraint, λij = 1.
j=1,j7�i
</equation>
<bodyText confidence="0.999813">
The edge weight function λij allows us to
weight the relatedness between two tokens, for ex-
ample based on their distance in the text. The mo-
tivation for this is that the closer two tokens occur
together, the more likely it is that their relatedness
is not accidental. For instance, the idiom break
the ice in Example 7 could be misclassified as lit-
eral due to there being a high relatedness score be-
tween ice and snow. The weight function is in-
troduced so that relatedness with tokens that are
closer to MWE component words counts more.
(7) The train was canceled because of the wind
and snow. All the people in the small village
train station felt upset. Suddenly, one guy
broke the ice and proposed to play a game.
The weight function λij is defined in terms of
the inverse of the distance δ between the two token
positions idi and idj:
</bodyText>
<equation confidence="0.92980175">
δ(idi, idj)
λij = (8)
δ(idi, idj)
j
</equation>
<bodyText confidence="0.9999862">
As the semantic relatedness among the MWE
component words does not contain any informa-
tion of how these component words are seman-
tically involved in the context, we do not count
the edges between the MWE component words
</bodyText>
<page confidence="0.991438">
77
</page>
<bodyText confidence="0.95953075">
(as e45 in Figure 1). We set all the weights
for connecting MWE component words to be 0,
δ(idmwe&apos;
i ,idmwe
j ) = 0.
c(G): is defined as the discourse connectivity
of the cohesion graph. It represents the semantic
relatedness score of the discourse.
</bodyText>
<equation confidence="0.983322333333333">
n
c(G) = (Qi × ei) (9)
i=1
</equation>
<bodyText confidence="0.999320636363636">
where n is the total number of tokens in the
discourse, Qi is the weight of the average seman-
tic relatedness of the token ti with the constraint
Qi = 1. It represents the importance of the
i
relatedness contribution of a specific token ti in
the discourse. For instance, the word Monday in
Example 12 should be assigned less weight than
the word bilateral as it is not part of the central
theme(s) of the discourse. This is often the case
for time expressions. Qi is defined as:
</bodyText>
<equation confidence="0.97809575">
salience(ti) (10)
Qi =
salience(tj)
j
</equation>
<bodyText confidence="0.999943285714286">
To model the salience of a token for the se-
mantic context of the text we use a tf.idf-based
weighting scheme. Since we represent word to-
kens rather than word types in the cohesion graph,
we do not need to model the term frequency tf
separately, instead we set salience to the log value
of the inverse document frequency idf:
</bodyText>
<equation confidence="0.94378">
salience(ti) = log |D ||{d : ti ∈d} |(11)
</equation>
<bodyText confidence="0.998432434782609">
where D is the total number of documents in our
data set and |{d : ti ∈ d} |is the number of docu-
ments in which ti occurs. Terms which are related
to the sub-topics of a document will typically only
occur in a few texts in the collection, hence their
idf (and often also their tf) is high and they will
thus be given more weight in the graph. Terms
which are not related to the central themes of a
text, such as temporal expressions, will be given
a lower weight. A complication arises for compo-
nent words of the MWE: these occur in all of our
examples and thus will receive a very low idf. This
is an artifact of the data and not what we want as
it means that the average connectivity of the graph
virtually always increases if the MWE is excluded,
causing the classifier to over-predict ’non-literal’.
To counteract this effect, we set |{d : ti ∈ d} |of
these words uniformly to 1.
(12) “Gujral will meet Sharif on Monday and
discuss bilateral relations,” the Press Trust
of India added. The minister said Sharif and
Gujral would be able to “break the ice” over
Kashmir.
</bodyText>
<subsectionHeader confidence="0.995184">
3.2 Graph-based Classifier
</subsectionHeader>
<bodyText confidence="0.9977735">
The cohesion graph based classifier compares the
cohesion graph connectivity of the discourse in-
cluding the MWE component words with the con-
nectivity of the discourse excluding the MWE
component words to check how well the MWE
component words are semantically connected to
the context. If the cohesion graph connectivity
increases by including MWE component words,
the MWE is thought to be semantically well re-
lated to its discourse. It is classified as literal (oth-
erwise as non-literal). In other words, the cohe-
sion graph based algorithm detects the strength of
relatedness between the MWE component words
and their context by calculating the discourse con-
nectivity gain, and classifies instances as literal or
non-literal based on this gain. This process is de-
scribed as Formula 13 (if Ac &gt; 0, it is literal;
otherwise it is non-literal):
</bodyText>
<equation confidence="0.985904">
Ac = c(G) − c(G&apos;) (13)
</equation>
<bodyText confidence="0.996012666666667">
where, c(G) is the discourse connectivity of the
context with MWE component words (as shown
with the complete graph in Figure 1 ); c(G&apos;) is
the discourse connectivity of the context without
MWE component words (as shown with the sub-
graph {v1, v2, v3} in Figure 1).
</bodyText>
<figureCaption confidence="0.9912015">
Figure 1: Cohesion Graph for identifying literal or
non-literal usage of MWEs
</figureCaption>
<page confidence="0.994457">
78
</page>
<sectionHeader confidence="0.978195" genericHeader="method">
4 Modeling Semantic Relatedness
</sectionHeader>
<bodyText confidence="0.971237884615385">
In Section 3.1, we did not define how we model
the semantic relatedness between two tokens
(h(ti, tj)). Modeling semantic relatedness be-
tween two terms is currently an area of active re-
search. There are two main approaches. Methods
based on manually built lexical knowledge bases,
such as WordNet, compute the shortest path be-
tween two concepts in the knowledge base and/or
look at word overlap in the glosses (see Budan-
itsky and Hirst (2006) for an overview). Distri-
butional approaches, on the other hand, rely on
text corpora, and model relatedness by comparing
the contexts in which two words occur, assuming
that related words occur in similar context (e.g.,
Hindle (1990), Lin (1998a), Mohammad and Hirst
(2006)). More recently, there has also been re-
search on using Wikipedia and related resources
for modeling semantic relatedness (Ponzetto and
Strube, 2007; Zesch et al., 2008).
WordNet-based approaches are unsuitable for
our purposes as they only model so-called “classi-
cal relations” like hypernymy, antonymy etc. For
our task, we need to model a wide range of re-
lations, e.g., between ice and water. Hence we
opted for a distributional approach. We experi-
mented with two different approaches, one (DV )
based on syntactic co-occurrences in a large text
corpus and the other (NGD) based on search en-
gine page counts.
Dependency Vectors (DV) is a distributional
approach which does not look simply at word co-
occurrences in a fixed-size window but takes into
account syntactic (dependency) relations between
words (Pad´o and Lapata, 2007). Each target word
is represented by a co-occurrence vector where di-
mension represents a chosen term and the vector
contains the co-occurrence information between
that word and the chosen terms in a corpus (we
used the BNC in our experiments). A variety of
distance measures can be used to compute the sim-
ilarity of two vectors; here we use the cosine sim-
ilarity which is defined as:
Xn xiyi
i=1
Normalized Google Distance (NGD) uses the
page counts returned by a search engine as prox-
ies for word co-occurrence and thereby quantifies
the strength of a relationship between two words
(see Cilibrasi and Vitanyi (2007)). The basic idea
is that the more often two terms occur together rel-
ative to their overall occurrence the more closely
they are related. NGD is defined as follows:
</bodyText>
<equation confidence="0.997726333333333">
NGD(x, y) = max{log f(x), log f(y)} − log f(x, y)
log M − min{log f(x), log f(y)}
(15)
</equation>
<bodyText confidence="0.999993151515152">
where x and y are the two words whose associ-
ation strength is computed, f(x) is the page count
returned by the search engine for the term x (and
likewise for f(y) and y), f(x, y) is the page count
returned when querying for “x AND y” (i.e., the
number of pages that contain both, x and y), and
M is the number of web pages indexed by the
search engine. When querying for a term we query
for a disjunction of all its inflected forms.4 As it
is difficult to obtain a specific and reliable number
for the number of pages indexed by a search en-
gine, we approximated it by setting it to the num-
ber of hits obtained for the word the. The assump-
tion is that the word the occurs in all English lan-
guage web pages (Lapata and Keller, 2005).
Using web counts rather than bi-gram counts
from a corpus as the basis for computing semantic
relatedness has the advantage that the web is a sig-
nificantly larger database than any compiled cor-
pus, which makes it much more likely that we can
find information about the concepts we are look-
ing for (thus alleviating data sparseness). How-
ever, search engine counts are notoriously unre-
liable (Kilgariff, 2007; Matsuo et al., 2007) and
while previous studies have shown that web counts
can be used as reliable proxies for corpus-based
counts for some applications (Zhu and Rosenfeld,
2001; Lapata and Keller, 2005) it is not clear that
this also applies when modeling semantic related-
ness. We thus carried out a number of experiments
testing the reliability of page counts (Section 4.1)
and comparing the NGD measure to a standard
distributional approach (Section 4.2).
</bodyText>
<figure confidence="0.787652454545455">
simcos(−→x , →−y ) =
x2i
Xn
i=1
tuu v
2
yi
Xn
i=1
tuu v
(14) 4The inflected forms were generated by apply-
</figure>
<footnote confidence="0.42293525">
ing the morph tools developed at the University of
Sussex (Minnen et al., 2001) which are available
at: http://www.informatics.susx.ac.uk/
research/groups/nlp/carroll/morph.html
</footnote>
<page confidence="0.997174">
79
</page>
<subsectionHeader confidence="0.988286">
4.1 Search Engine Stability
</subsectionHeader>
<bodyText confidence="0.999646653846154">
We first carried out some experiments to test the
stability of the page counts returned by two of the
most widely-used search engines, Google and Ya-
hoo. For both search engines, we found a number
of problems.5
Total number of pages indexed The total num-
ber of the web pages indexed by a search engine
varies across time and the numbers provided are
somewhat unreliable. This is a potential problem
for NGD because we need to fix the value of M in
Formula 15. As an approximative solution, we set
it to the number of hits obtained for the word the,
assuming that it will occur in all English language
pages (Lapata and Keller, 2005).
Page count variation The number of page hits
for a given term also varies across time (see exam-
ple (4.1) for two queries for Jim at different times
tl and t2). However, we found that the variance in
the number of pages tends to be relatively stable
over short time spans, hence we can address this
problem by carrying out all queries in one quick
session without much delay. However, this means
we cannot store page counts in a database and re-
use them at a later stage; for each new example
which we want to classify at a later stage, we have
to re-compute all relevant counts.
</bodyText>
<equation confidence="0.987731">
(16) Hits(Jim, t1) = 763,000,000
Hits(Jim, t2) = 757,000,000
</equation>
<bodyText confidence="0.993350875">
Problems with conjunction and disjunction
The search engines’ AND and OR operators are
problematic and can return counter-intuitive re-
sults (see Table 1). This is a potential problem
for us because we have to query for conjunctions
of terms and disjunctions of inflected forms. For
the time being we ignored this problem as it is not
straightforward to solve.
</bodyText>
<equation confidence="0.504367">
OPT = AND OPT = OR
car 3,590,000,000
car OPT car 4,670,000,000 3,550,000,000
car OPT car OPT car 3,490,000,000 3,530,000,000
</equation>
<tableCaption confidence="0.997898">
Table 1: Operator test for Yahoo
</tableCaption>
<bodyText confidence="0.99792275">
Problems with high-frequency terms We also
found that both the Google and Yahoo API seem
to have problems with high frequency words, with
the Google SOAP API throwing an exception and
</bodyText>
<footnote confidence="0.957368">
5See also the discussions in Jean V´eronis blog: http://
aixtal.blogspot.com and the comments in Kilgariff
(2007).
</footnote>
<bodyText confidence="0.998742375">
the Yahoo API returning the same 10-digit num-
ber for every high frequency word. This might be
a data overflow problem. We addressed this prob-
lem by excluding high frequency words.
When comparing Yahoo and Google we found
that Yahoo’s page counts tend to be more consis-
tent than Google’s. We therefore opted for Yahoo
in our further experiments.
</bodyText>
<subsectionHeader confidence="0.964487">
4.2 NGD vs. Co-occurrence Vectors
</subsectionHeader>
<bodyText confidence="0.999984172413793">
In principle, we believe that the web-based ap-
proach for computing relatedness is more suitable
for our task since it gives us access to more data
and allows us to also model relations based on (up-
to-date) world knowledge. However, the question
arises whether the stability problems observed in
the previous section have a negative effect on the
performance of the NGD measure. To test this, we
conducted a small study in which we compared
the relatedness scores obtained by NGD and the
semantic vector space model to the human ratings
compiled by Finkelstein et al. (2002).6
We used Spearman’s correlation test (Spear-
man, 1904) to compare the ranked human ratings
to the ranked ratings obtained by NGD and the
vector space method. The (human) inter-annotator
agreement varies a lot for different pairs of annota-
tors (between 0.41 and 0.82 by Spearman’s corre-
lation test), suggesting that deciding on the seman-
tic relatedness between arbitrary pairs of words
is not an easy task even for humans. In gen-
eral, the NGD-human agreement is comparable to
the human-human agreement. The agreement be-
tween the NGD and average human agreement is
higher than some human-human agreements. Fur-
thermore, we found that NGD actually outper-
forms the dependency vector method on this data
set.7 Hence, we decided to use NGD in the fol-
lowing experiments.
</bodyText>
<sectionHeader confidence="0.998175" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99934">
We tested our graph-based classifiers on a manu-
ally annotated data set, which we describe in Sec-
</bodyText>
<footnote confidence="0.9886189">
6The data sets are available at: http://www.cs.
technion.ac.il/˜gabr/resources/data/
wordsim353/
7There may be several reasons for this. Apart from the
fact that NGD has access to a larger data set, it may also be
that syntactic co-occurrence information is not ideal for mod-
eling this type of relatedness; co-occurrence information in a
fixed window might be more useful. Furthermore, we did not
spend much time on finding an optimal parameter setting for
the dependency vector method.
</footnote>
<page confidence="0.997746">
80
</page>
<bodyText confidence="0.990728">
tion 5.1. We report on our experiments and results
in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.989692">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.988995129032258">
Throughout the experiments we used the data set
from Sporleder and Li (2009). The data consist of
17 potentially idiomatic expressions from the En-
glish Gigaword corpus, which were extracted with
five paragraphs of context and manually annotated
as ’literal’ or ’non-literal’ (see Table 2). The inter-
annotator agreement on a doubly annotated sam-
ple of the data was 97% and the kappa score 0.7
(Cohen, 1960).
expression literal non-lit. all
back the wrong horse 0 25 25
bite off more than one can chew 2 142 144
bite one’s tongue 16 150 166
blow one’s own trumpet 0 9 9
bounce off the wall* 39 7 46
break the ice 20 521 541
drop the ball* 688 215 903
get one’s feet wet 17 140 157
pass the buck 7 255 262
play with fire 34 532 566
pull the trigger* 11 4 15
rock the boat 8 470 478
set in stone 9 272 281
spill the beans 3 172 175
sweep under the carpet 0 9 9
swim against the tide 1 125 126
tear one’s hair out 7 54 61
all 862 3102 3964
Table 2: Idiom statistics (* indicates expressions
for which the literal usage is more common than
the non-literal one)
</bodyText>
<subsectionHeader confidence="0.998445">
5.2 The Influence of Context Size and
Weighting Scheme
</subsectionHeader>
<bodyText confidence="0.999970375">
To gain some insights into the performance of the
graph-based classifier, we experimented with dif-
ferent context sizes and weighting schemes. In ad-
dition to the basic cohesion graph approach with
five paragraphs of context (CGA), we tested a
variant which only uses the current paragraph as
context (CGApara) to determine how sensitive the
classifier is to the context size. We also experi-
mented with three weighting schemes. The ba-
sic classifier (CGA) uses uniform edge and node
weights. CGAew uses edge weights based on the
inverse distance between the tokens. CGAnw uses
node weights based on idf. Finally, CGAew+nw
uses both edge and node weights.
We also carried out a pruning experiment in
which we removed nodes from the graph that are
only weakly connected to the context (called weak
cohesion nodes). We hypothesize that these do
not contribute much to the overall connectivity but
may add noise. Pruning can thus be seen as a
more gentle version of node weighting, in which
we only remove the top n outliers rather than re-
weight all nodes. For comparison we also imple-
mented a baseline (BASE), which always assigns
the majority class (’non-literal’).
Table 3 shows the results for the classifiers dis-
cussed above. In addition to accuracy, which is
not very informative as the class distribution in our
data set is quite skewed, we show the precision,
recall, and F-score for the minority class (literal).
All classifiers obtain a relatively high accuracy but
vary in the precision, recall and F-Score values.
</bodyText>
<table confidence="0.999674875">
Method LPrec. LRec. LFβ=1 Acc.
Base – – – 0.78
CGA 0.50 0.69 0.58 0.79
CGApara 0.42 0.67 0.51 0.71
CGAprun 0.49 0.72 0.58 0.78
CGAew 0.51 0.63 0.57 0.79
CGAnw 0.48 0.68 0.56 0.77
CGAew+nw 0.49 0.61 0.54 0.78
</table>
<tableCaption confidence="0.998935">
Table 3: Accuracy (Acc.), literal precision
</tableCaption>
<bodyText confidence="0.98892724">
(LPrec.), recall (LRec.), and F-Score (LFβ=1) for
the classifier
It can be seen that the basic cohesion graph
classifier (CGA) outperforms the baseline on ac-
curacy. Moreover, it is reasonably good at iden-
tifying literal usages among the majority of non-
literal occurrences, as witnessed by an F-score of
58%. To obtain a better idea of the behavior of
this classifier, we plotted the distribution of the
MWE instances in the classifier’s feature space,
where the first dimension represents the discourse
connectivity of the context with MWE component
words (c(G)) and the second represents the dis-
course connectivity of the context without MWE
component words (c(G�)). The graph-based clas-
sifier, which calculates the connectivity gain (see
Equation 13), is a simple linear classifier in which
the line y = x is chosen as the decision boundary.
Examples above that line are classified as ’literal’,
examples below as ’non-literal’. Figure 2 shows
the true distribution of literal and non-literal exam-
ples in our data set. It can be seen that most non-
literal examples are indeed below the line while
most literal ones are above it (though a certain
number of literal examples can also be found be-
</bodyText>
<page confidence="0.995155">
81
</page>
<bodyText confidence="0.9822995">
low the line). So, in general we would expect our
classifier to have a reasonable performance.
</bodyText>
<figureCaption confidence="0.639983">
Figure 2: Decision boundaries of the cohesion
graph
</figureCaption>
<bodyText confidence="0.98880078125">
Returning to the results in Table 3, we find
that a smaller context worsens the performance
of the classifier (CGApara). Pruning the 3 least
connected nodes (CGAprun) does not lead to a
significant change in performance. Edge weight-
ing (CGAew), node weighting (CGAnw) and their
combination (CGAew+nw), on the other hand,
seem to have a somewhat negative influence on
the literal recall and F-score. It seems that the
weighting scheme scales down the influence of
MWE component words. As a result, the prod-
uct of the weight and the relatedness value for the
idiom component words are lower than the aver-
age, which leads to the negative contribution of
the idiom words to the cohesion graph (over pre-
dicting non-literal usage). We need to investigate
more sophisticated weighting schemes to assign
better weights to idiom component words in the
future. The negative performance of the weight-
ing scheme may be also due to the fact that we
used a relatively small context of five paragraphs.8
Both the idf and the distance weighting should
probably be defined on larger contexts. For ex-
ample, the distance between two tokens within a
paragraph probably has not such a large effect on
whether their relatedness score is reliable or ac-
cidental. Hence it might be better to model the
edge weight as the distance in terms of paragraphs
rather than words. The idf scores, too, might be
more reliable if more context was used.
8Note that we used news texts which typically have very
short paragraphs.
</bodyText>
<sectionHeader confidence="0.997035" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998315">
In this paper, we described an approach for token-
based idiom classification. Our approach is based
on the observation that literally used expressions
typically exhibit strong cohesive ties with the sur-
rounding discourse, while idiomatic expressions
do not. Hence, idiomatic use of MWEs can be
detected by the absence of such ties.
We propose a graph-based method which ex-
ploits this behavior to classify MWEs as literal
or non-literal. The method compares how the
MWE component words contribute the overall se-
mantic connectivity of the graph. We provided a
formalization of the graph and experimented with
varying the context size and weighting scheme for
nodes and edges. We found that the method gener-
ally works better for larger contexts; the weighting
schemes proved somewhat unsuccessful, at least
for our current context size. In the future, we plan
to experiment with larger context sizes and more
sophisticated weighting schemes.
</bodyText>
<sectionHeader confidence="0.99752" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.852326">
This work was funded by the Cluster of Excellence “Multi-
modal Computing and Interaction”.
</bodyText>
<sectionHeader confidence="0.996959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999306666666667">
T. Baldwin, A. Villavicencio. 2002. Extracting the un-
extractable: a case study on verb-particles. In Proc.
of CoNLL-02.
T. Baldwin, E. M. Bender, D. Flickinger, A. Kim,
S. Oepen. 2004. Road-testing the english resource
grammar over the british national corpus. In Proc.
LREC-04, 2047–2050.
C. Bannard, T. Baldwin, A. Lascarides. 2003. A sta-
tistical approach to the semantics of verb-particles.
In Proc. ACL 2003 Workshop on Multiword Expres-
sions.
J. Birke, A. Sarkar. 2006. A clustering approach
for the nearly unsupervised recognition of nonliteral
language. In Proceedings of EACL-06.
F. Bond, S. Shirai. 1997. Practical and efficient or-
ganization of a large valency dictionary. In Work-
shop on Multilingual Information Processing Natu-
ral Language Processing Pacific Rim Symposium.
A. Budanitsky, G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computa-
tional Linguistics, 32(1):13–47.
R. L. Cilibrasi, P. M. Vitanyi. 2007. The Google sim-
ilarity distance. IEEE Trans. Knowledge and Data
Engineering, 19(3):370–383.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surements, 20:37–46.
</reference>
<page confidence="0.980487">
82
</page>
<reference confidence="0.999922572815534">
P. Cook, A. Fazly, S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the ACL-07 Workshop on A Broader
Perspective on Multiword Expressions.
S. Evert, B. Krenn. 2001. Methods for the qualitative
evaluation of lexical association measures. In Proc.
ACL-01.
A. Fazly, P. Cook, S. Stevenson. 2009. Unsupervised
type and token identification of idiomatic expres-
sions. Computational Linguistics, 35(1):61–103.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, E. Ruppin. 2002. Plac-
ing search in context: The concept revisited. ACM
Transactions on Information Systems, 20(1):116–
131.
L. Gerber, J. Yang. 1997. Systran mt dictionary devel-
opment. In Proc. Fifth Machine Translation Summit.
M. Halliday, R. Hasan. 1976. Cohesion in English.
Longman House, New York.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In Proceedings of ACL-90,
268–275.
G. Hirst, D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correc-
tion of malapropisms. In C. Fellbaum, ed., Word-
Net: An electronic lexical database, 305–332. The
MIT Press.
R. Jackendoff. 1997. The Architecture of the Language
Faculty. MIT Press.
G. Katz, E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
ACL/COLING-06 Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties.
A. Kilgariff. 2007. Googleology is bad science. Com-
putational Linguistics, 33(1):147–151.
M. Lapata, F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2:1–31.
D. D. Lewis, W. B. Croft. 1990. Term clustering of
syntactic phrase. In Proceedings of SIGIR-90, 13th
ACM International Conference on Research and De-
velopment in Information Retrieval.
D. Lin. 1998a. Automatic retrieval and clustering of
similar words. In Proceedings of ACL-98.
D. Lin. 1998b. Using collocation statistics in informa-
tion extraction. In Proc. MUC-7.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
317–324.
Y. Matsuo, H. Tomobe, T. Nishimura. 2007. Robust
estimation of google counts for social network ex-
traction. In AAAI-07.
G. Minnen, J. Carroll, D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(3):207–223.
S. Mohammad, G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. In Proceedings of EMNLP-06.
S. Pad´o, M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161–199.
S. S. L. Piao, P. Rayson, D. Archer, A. Wilson,
T. McEnery. 2003. Extracting multiword expres-
sions with a semantic tagger. In Proc. of the ACL
2003 Workshop on Multiword Expressions, 49–56.
S. P. Ponzetto, M. Strube. 2007. Knowledge derived
from Wikipedia for computing semantic relatedness.
Journal of Artificial Intelligence Research, 30:181–
212.
S. Riehemann. 2001. A Constructional Approach to
Idioms and Word Formation. Ph.D. thesis, Stanford
University.
T. T. Rila Mandala, H. Tanaka. 2000. Query expansion
using heterogeneous thesauri. Inf. Process. Man-
age., 36(3).
I. A. Sag, T. Baldwin, F. Bond, A. Copestake,
D. Flickinger. 2001. Multiword expressions: a pain
in the neck for NLP. In Lecture Notes in Computer
Science.
G. Salton, J. Allan, C. Buckley, A. Singhal. 1994.
Automatic analysis, theme generation and sum-
marization of machine-readable texts. Science,
264(3):1421–1426.
F. Smadja. 19993. Retrieving collocations from text:
Xtract. Computational Linguistics, 19(1):143–177.
C. Spearman. 1904. The proof and measurement of
association between two things. Amer. J. Psychol,
72–101.
C. Sporleder, L. Li. 2009. Unsupervised recognition of
literal and non-literal use of idiomatic expressions.
In Proceedings of EACL-09.
N. Wacholder, P. Song. 2003. Toward a task-based
gold standard for evaluation of NP chunks and tech-
nical terms. In Proc HLT-NAACL.
T. Zesch, C. M¨uller, I. Gurevych. 2008. Using wik-
tionary for computing semantic relatedness. In Pro-
ceedings of AAAI-08, 861–867.
X. Zhu, R. Rosenfeld. 2001. Improving trigram lan-
guage modeling with the world wide web. In Pro-
ceedings of ICASSP-01.
</reference>
<page confidence="0.99931">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.362611">
<title confidence="0.9978695">A Cohesion Graph Based Approach for Unsupervised Recognition Literal and Non-literal Use of Multiword Expressions</title>
<author confidence="0.99009">Li</author>
<affiliation confidence="0.983248">Saarland</affiliation>
<address confidence="0.55625">Postfach 15 11 66041</address>
<abstract confidence="0.9995879">We present a graph-based model for representing the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>A Villavicencio</author>
</authors>
<title>Extracting the unextractable: a case study on verb-particles.</title>
<date>2002</date>
<booktitle>In Proc. of CoNLL-02.</booktitle>
<contexts>
<context position="7094" citStr="Baldwin and Villavicencio, 2002" startWordPosition="1127" endWordPosition="1131">h different vertex and edge weighting schemes. We also report on experiments with varying the size of the input context and also with pruning the graph structure automatically. 2 Related Work Type-based MWE classification aims to extract multiword expression types in text from observations of the token distribution. It aims to pick up on word combinations which occur with comparatively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. </context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>T. Baldwin, A. Villavicencio. 2002. Extracting the unextractable: a case study on verb-particles. In Proc. of CoNLL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>A Kim</author>
<author>S Oepen</author>
</authors>
<title>Road-testing the english resource grammar over the british national corpus.</title>
<date>2004</date>
<booktitle>In Proc. LREC-04,</booktitle>
<pages>2047--2050</pages>
<contexts>
<context position="2073" citStr="Baldwin et al., 2004" startWordPosition="316" endWordPosition="319">variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (in</context>
</contexts>
<marker>Baldwin, Bender, Flickinger, Kim, Oepen, 2004</marker>
<rawString>T. Baldwin, E. M. Bender, D. Flickinger, A. Kim, S. Oepen. 2004. Road-testing the english resource grammar over the british national corpus. In Proc. LREC-04, 2047–2050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>T Baldwin</author>
<author>A Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verb-particles.</title>
<date>2003</date>
<booktitle>In Proc. ACL 2003 Workshop on Multiword Expressions.</booktitle>
<contexts>
<context position="7715" citStr="Bannard et al. (2003)" startWordPosition="1232" endWordPosition="1235">. The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying individual instances of a potential idiom as literal or nonliteral. Katz and Giesbrecht (2006) make use of latent semantic analysis (LSA) to explore the local linguistic context that can serve to identify multiword expressions that have non-compositional meaning</context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>C. Bannard, T. Baldwin, A. Lascarides. 2003. A statistical approach to the semantics of verb-particles. In Proc. ACL 2003 Workshop on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Birke</author>
<author>A Sarkar</author>
</authors>
<title>A clustering approach for the nearly unsupervised recognition of nonliteral language.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-06.</booktitle>
<contexts>
<context position="8809" citStr="Birke and Sarkar (2006)" startWordPosition="1407" endWordPosition="1410">LSA) to explore the local linguistic context that can serve to identify multiword expressions that have non-compositional meaning. They measure the cosine vector similarity between the vectors associated with an MWE as a whole and the vectors associated with its constituent parts and interpret it as the degree to which the MWE is compositional. They report an av2The substitution test aims to replace part of the idiom’s component words with semantically similar words, and test how the co-occurrence frequency changes. 76 erage accuracy of 72%, but the data set used in their evaluation is small. Birke and Sarkar (2006) use literal and non-literal seed sets acquired without human supervision to perform bootstrapping learning. The new instances of potential idioms are always labeled according to the closest set. While their approach is unsupervised clustering, they do rely on some resources such as databases of idioms. Cook et al. (2007) and Fazly et al. (2009) rely crucially on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs. The canonical form allows for infle</context>
</contexts>
<marker>Birke, Sarkar, 2006</marker>
<rawString>J. Birke, A. Sarkar. 2006. A clustering approach for the nearly unsupervised recognition of nonliteral language. In Proceedings of EACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Bond</author>
<author>S Shirai</author>
</authors>
<title>Practical and efficient organization of a large valency dictionary.</title>
<date>1997</date>
<booktitle>In Workshop on Multilingual Information Processing Natural Language Processing Pacific Rim Symposium.</booktitle>
<contexts>
<context position="2244" citStr="Bond and Shirai, 1997" startWordPosition="343" endWordPosition="346">sies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (interpreting the semantic association among components in MWEs). To extract MWEs, various methods have been proposed that exploit the syntactic and lexical fixedness exhibit</context>
</contexts>
<marker>Bond, Shirai, 1997</marker>
<rawString>F. Bond, S. Shirai. 1997. Practical and efficient organization of a large valency dictionary. In Workshop on Multilingual Information Processing Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="16811" citStr="Budanitsky and Hirst (2006)" startWordPosition="2826" endWordPosition="2830"> component words (as shown with the subgraph {v1, v2, v3} in Figure 1). Figure 1: Cohesion Graph for identifying literal or non-literal usage of MWEs 78 4 Modeling Semantic Relatedness In Section 3.1, we did not define how we model the semantic relatedness between two tokens (h(ti, tj)). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky, G. Hirst. 2006. Evaluating WordNetbased measures of semantic distance. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Cilibrasi</author>
<author>P M Vitanyi</author>
</authors>
<title>The Google similarity distance.</title>
<date>2007</date>
<journal>IEEE Trans. Knowledge and Data Engineering,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="18533" citStr="Cilibrasi and Vitanyi (2007)" startWordPosition="3108" endWordPosition="3111">pata, 2007). Each target word is represented by a co-occurrence vector where dimension represents a chosen term and the vector contains the co-occurrence information between that word and the chosen terms in a corpus (we used the BNC in our experiments). A variety of distance measures can be used to compute the similarity of two vectors; here we use the cosine similarity which is defined as: Xn xiyi i=1 Normalized Google Distance (NGD) uses the page counts returned by a search engine as proxies for word co-occurrence and thereby quantifies the strength of a relationship between two words (see Cilibrasi and Vitanyi (2007)). The basic idea is that the more often two terms occur together relative to their overall occurrence the more closely they are related. NGD is defined as follows: NGD(x, y) = max{log f(x), log f(y)} − log f(x, y) log M − min{log f(x), log f(y)} (15) where x and y are the two words whose association strength is computed, f(x) is the page count returned by the search engine for the term x (and likewise for f(y) and y), f(x, y) is the page count returned when querying for “x AND y” (i.e., the number of pages that contain both, x and y), and M is the number of web pages indexed by the search eng</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2007</marker>
<rawString>R. L. Cilibrasi, P. M. Vitanyi. 2007. The Google similarity distance. IEEE Trans. Knowledge and Data Engineering, 19(3):370–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurements,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="25584" citStr="Cohen, 1960" startWordPosition="4320" endWordPosition="4321"> more useful. Furthermore, we did not spend much time on finding an optimal parameter setting for the dependency vector method. 80 tion 5.1. We report on our experiments and results in Section 5.2. 5.1 Data Throughout the experiments we used the data set from Sporleder and Li (2009). The data consist of 17 potentially idiomatic expressions from the English Gigaword corpus, which were extracted with five paragraphs of context and manually annotated as ’literal’ or ’non-literal’ (see Table 2). The interannotator agreement on a doubly annotated sample of the data was 97% and the kappa score 0.7 (Cohen, 1960). expression literal non-lit. all back the wrong horse 0 25 25 bite off more than one can chew 2 142 144 bite one’s tongue 16 150 166 blow one’s own trumpet 0 9 9 bounce off the wall* 39 7 46 break the ice 20 521 541 drop the ball* 688 215 903 get one’s feet wet 17 140 157 pass the buck 7 255 262 play with fire 34 532 566 pull the trigger* 11 4 15 rock the boat 8 470 478 set in stone 9 272 281 spill the beans 3 172 175 sweep under the carpet 0 9 9 swim against the tide 1 125 126 tear one’s hair out 7 54 61 all 862 3102 3964 Table 2: Idiom statistics (* indicates expressions for which the liter</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurements, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cook</author>
<author>A Fazly</author>
<author>S Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multiword Expressions.</booktitle>
<contexts>
<context position="9132" citStr="Cook et al. (2007)" startWordPosition="1457" endWordPosition="1460"> is compositional. They report an av2The substitution test aims to replace part of the idiom’s component words with semantically similar words, and test how the co-occurrence frequency changes. 76 erage accuracy of 72%, but the data set used in their evaluation is small. Birke and Sarkar (2006) use literal and non-literal seed sets acquired without human supervision to perform bootstrapping learning. The new instances of potential idioms are always labeled according to the closest set. While their approach is unsupervised clustering, they do rely on some resources such as databases of idioms. Cook et al. (2007) and Fazly et al. (2009) rely crucially on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs. The canonical form allows for inflection variation of the heard verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically it typically occurs in its canonical form (Riehemann, 2001). Fazly and her colleagues exploit this behavior and propose an unsupervised me</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>P. Cook, A. Fazly, S. Stevenson. 2007. Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multiword Expressions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
<author>B Krenn</author>
</authors>
<title>Methods for the qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proc. ACL-01.</booktitle>
<contexts>
<context position="6947" citStr="Evert and Krenn, 2001" startWordPosition="1107" endWordPosition="1110">esent the similarity between the segments (modeled as weighted term overlap). we provide a formalization of the graph and experiment with different vertex and edge weighting schemes. We also report on experiments with varying the size of the input context and also with pruning the graph structure automatically. 2 Related Work Type-based MWE classification aims to extract multiword expression types in text from observations of the token distribution. It aims to pick up on word combinations which occur with comparatively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of </context>
</contexts>
<marker>Evert, Krenn, 2001</marker>
<rawString>S. Evert, B. Krenn. 2001. Methods for the qualitative evaluation of lexical association measures. In Proc. ACL-01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>P Cook</author>
<author>S Stevenson</author>
</authors>
<title>Unsupervised type and token identification of idiomatic expressions.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<contexts>
<context position="9156" citStr="Fazly et al. (2009)" startWordPosition="1462" endWordPosition="1465"> report an av2The substitution test aims to replace part of the idiom’s component words with semantically similar words, and test how the co-occurrence frequency changes. 76 erage accuracy of 72%, but the data set used in their evaluation is small. Birke and Sarkar (2006) use literal and non-literal seed sets acquired without human supervision to perform bootstrapping learning. The new instances of potential idioms are always labeled according to the closest set. While their approach is unsupervised clustering, they do rely on some resources such as databases of idioms. Cook et al. (2007) and Fazly et al. (2009) rely crucially on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs. The canonical form allows for inflection variation of the heard verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically it typically occurs in its canonical form (Riehemann, 2001). Fazly and her colleagues exploit this behavior and propose an unsupervised method for token-based idi</context>
</contexts>
<marker>Fazly, Cook, Stevenson, 2009</marker>
<rawString>A. Fazly, P. Cook, S. Stevenson. 2009. Unsupervised type and token identification of idiomatic expressions. Computational Linguistics, 35(1):61–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Finkelstein</author>
<author>E Gabrilovich</author>
<author>Y Matias</author>
<author>E Rivlin</author>
<author>Z Solan</author>
<author>G Wolfman</author>
<author>E Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<pages>131</pages>
<contexts>
<context position="23739" citStr="Finkelstein et al. (2002)" startWordPosition="4017" endWordPosition="4020">ther experiments. 4.2 NGD vs. Co-occurrence Vectors In principle, we believe that the web-based approach for computing relatedness is more suitable for our task since it gives us access to more data and allows us to also model relations based on (upto-date) world knowledge. However, the question arises whether the stability problems observed in the previous section have a negative effect on the performance of the NGD measure. To test this, we conducted a small study in which we compared the relatedness scores obtained by NGD and the semantic vector space model to the human ratings compiled by Finkelstein et al. (2002).6 We used Spearman’s correlation test (Spearman, 1904) to compare the ranked human ratings to the ranked ratings obtained by NGD and the vector space method. The (human) inter-annotator agreement varies a lot for different pairs of annotators (between 0.41 and 0.82 by Spearman’s correlation test), suggesting that deciding on the semantic relatedness between arbitrary pairs of words is not an easy task even for humans. In general, the NGD-human agreement is comparable to the human-human agreement. The agreement between the NGD and average human agreement is higher than some human-human agreeme</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116– 131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gerber</author>
<author>J Yang</author>
</authors>
<title>Systran mt dictionary development.</title>
<date>1997</date>
<booktitle>In Proc. Fifth Machine Translation Summit.</booktitle>
<contexts>
<context position="2220" citStr="Gerber and Yang, 1997" startWordPosition="339" endWordPosition="342">e up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (interpreting the semantic association among components in MWEs). To extract MWEs, various methods have been proposed that exploit the syntactic and l</context>
</contexts>
<marker>Gerber, Yang, 1997</marker>
<rawString>L. Gerber, J. Yang. 1997. Systran mt dictionary development. In Proc. Fifth Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman House,</publisher>
<location>New York.</location>
<contexts>
<context position="4158" citStr="Halliday and Hasan, 1976" startWordPosition="647" endWordPosition="650">the ball bounced off the wall well below his glove. Our work aims to distinguish the literal and non-literal usages of idiomatic expressions in a 75 Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 75–83, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP discourse context (so-called token based classification). It is therefore different from type-based approaches which aim to detect the general idiomaticity of an expression rather than its actual usage in a particular context. We utilize the cohesive structure of a discourse (Halliday and Hasan, 1976) to distinguish literal or non-literal usage of MWEs. The basic idea is that the component words of an MWE contribute to the cohesion of the discourse in the literal case, while in the non-literal case they do not. For instance, in the literal use of break the ice in Example 3, the content word ice contributes to the overall semantic connectivity of the whole sentence by the fact that ice is semantically related to water. In contrast, in the non-literal example in 4, the word ice does not contribute to the overall cohesion as it is poorly connected to all the other (content) words in this spec</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>M. Halliday, R. Hasan. 1976. Cohesion in English. Longman House, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In Proceedings of ACL-90,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="17045" citStr="Hindle (1990)" startWordPosition="2866" endWordPosition="2867">atedness between two tokens (h(ti, tj)). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and </context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In Proceedings of ACL-90, 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>WordNet: An electronic lexical database,</booktitle>
<pages>305--332</pages>
<editor>In C. Fellbaum, ed.,</editor>
<publisher>The MIT Press.</publisher>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>G. Hirst, D. St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In C. Fellbaum, ed., WordNet: An electronic lexical database, 305–332. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>The Architecture of the Language Faculty.</title>
<date>1997</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="928" citStr="Jackendoff, 1997" startWordPosition="135" endWordPosition="136">the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions. 1 Introduction Multiword expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries or spaces” (Sag et al., 2001). Such expressions are pervasive in natural language; they are estimated to be equivalent in number to simplex words in mental lexicon (Jackendoff, 1997). MWEs exhibit a number of lexical, syntactic, semantic, pragmatic and statistical idiosyncrasies: syntactic peculiarities (e.g., by and large, ad hoc), semantic non-compositionality (e.g., as in kick the bucket (die) and red tape (bureaucracy)), pragmatic idiosyncrasies (the expression is sometimes associated with a fixed pragmatic point, e.g., good morning, good night), variation in syntactic flexibility (e.g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket =� *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for diff</context>
</contexts>
<marker>Jackendoff, 1997</marker>
<rawString>R. Jackendoff. 1997. The Architecture of the Language Faculty. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Katz</author>
<author>E Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</booktitle>
<contexts>
<context position="8147" citStr="Katz and Giesbrecht (2006)" startWordPosition="1298" endWordPosition="1301">e the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying individual instances of a potential idiom as literal or nonliteral. Katz and Giesbrecht (2006) make use of latent semantic analysis (LSA) to explore the local linguistic context that can serve to identify multiword expressions that have non-compositional meaning. They measure the cosine vector similarity between the vectors associated with an MWE as a whole and the vectors associated with its constituent parts and interpret it as the degree to which the MWE is compositional. They report an av2The substitution test aims to replace part of the idiom’s component words with semantically similar words, and test how the co-occurrence frequency changes. 76 erage accuracy of 72%, but the data </context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>G. Katz, E. Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the ACL/COLING-06 Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgariff</author>
</authors>
<title>Googleology is bad science.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="19923" citStr="Kilgariff, 2007" startWordPosition="3371" endWordPosition="3372">ed by a search engine, we approximated it by setting it to the number of hits obtained for the word the. The assumption is that the word the occurs in all English language web pages (Lapata and Keller, 2005). Using web counts rather than bi-gram counts from a corpus as the basis for computing semantic relatedness has the advantage that the web is a significantly larger database than any compiled corpus, which makes it much more likely that we can find information about the concepts we are looking for (thus alleviating data sparseness). However, search engine counts are notoriously unreliable (Kilgariff, 2007; Matsuo et al., 2007) and while previous studies have shown that web counts can be used as reliable proxies for corpus-based counts for some applications (Zhu and Rosenfeld, 2001; Lapata and Keller, 2005) it is not clear that this also applies when modeling semantic relatedness. We thus carried out a number of experiments testing the reliability of page counts (Section 4.1) and comparing the NGD measure to a standard distributional approach (Section 4.2). simcos(−→x , →−y ) = x2i Xn i=1 tuu v 2 yi Xn i=1 tuu v (14) 4The inflected forms were generated by applying the morph tools developed at t</context>
<context position="22785" citStr="Kilgariff (2007)" startWordPosition="3858" endWordPosition="3859">e have to query for conjunctions of terms and disjunctions of inflected forms. For the time being we ignored this problem as it is not straightforward to solve. OPT = AND OPT = OR car 3,590,000,000 car OPT car 4,670,000,000 3,550,000,000 car OPT car OPT car 3,490,000,000 3,530,000,000 Table 1: Operator test for Yahoo Problems with high-frequency terms We also found that both the Google and Yahoo API seem to have problems with high frequency words, with the Google SOAP API throwing an exception and 5See also the discussions in Jean V´eronis blog: http:// aixtal.blogspot.com and the comments in Kilgariff (2007). the Yahoo API returning the same 10-digit number for every high frequency word. This might be a data overflow problem. We addressed this problem by excluding high frequency words. When comparing Yahoo and Google we found that Yahoo’s page counts tend to be more consistent than Google’s. We therefore opted for Yahoo in our further experiments. 4.2 NGD vs. Co-occurrence Vectors In principle, we believe that the web-based approach for computing relatedness is more suitable for our task since it gives us access to more data and allows us to also model relations based on (upto-date) world knowled</context>
</contexts>
<marker>Kilgariff, 2007</marker>
<rawString>A. Kilgariff. 2007. Googleology is bad science. Computational Linguistics, 33(1):147–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<pages>2--1</pages>
<contexts>
<context position="19515" citStr="Lapata and Keller, 2005" startWordPosition="3301" endWordPosition="3304"> the search engine for the term x (and likewise for f(y) and y), f(x, y) is the page count returned when querying for “x AND y” (i.e., the number of pages that contain both, x and y), and M is the number of web pages indexed by the search engine. When querying for a term we query for a disjunction of all its inflected forms.4 As it is difficult to obtain a specific and reliable number for the number of pages indexed by a search engine, we approximated it by setting it to the number of hits obtained for the word the. The assumption is that the word the occurs in all English language web pages (Lapata and Keller, 2005). Using web counts rather than bi-gram counts from a corpus as the basis for computing semantic relatedness has the advantage that the web is a significantly larger database than any compiled corpus, which makes it much more likely that we can find information about the concepts we are looking for (thus alleviating data sparseness). However, search engine counts are notoriously unreliable (Kilgariff, 2007; Matsuo et al., 2007) and while previous studies have shown that web counts can be used as reliable proxies for corpus-based counts for some applications (Zhu and Rosenfeld, 2001; Lapata and </context>
<context position="21329" citStr="Lapata and Keller, 2005" startWordPosition="3607" endWordPosition="3610">t carried out some experiments to test the stability of the page counts returned by two of the most widely-used search engines, Google and Yahoo. For both search engines, we found a number of problems.5 Total number of pages indexed The total number of the web pages indexed by a search engine varies across time and the numbers provided are somewhat unreliable. This is a potential problem for NGD because we need to fix the value of M in Formula 15. As an approximative solution, we set it to the number of hits obtained for the word the, assuming that it will occur in all English language pages (Lapata and Keller, 2005). Page count variation The number of page hits for a given term also varies across time (see example (4.1) for two queries for Jim at different times tl and t2). However, we found that the variance in the number of pages tends to be relatively stable over short time spans, hence we can address this problem by carrying out all queries in one quick session without much delay. However, this means we cannot store page counts in a database and reuse them at a later stage; for each new example which we want to classify at a later stage, we have to re-compute all relevant counts. (16) Hits(Jim, t1) =</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>M. Lapata, F. Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>W B Croft</author>
</authors>
<title>Term clustering of syntactic phrase.</title>
<date>1990</date>
<booktitle>In Proceedings of SIGIR-90, 13th ACM International Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="1865" citStr="Lewis and Croft, 1990" startWordPosition="279" endWordPosition="282"> a fixed pragmatic point, e.g., good morning, good night), variation in syntactic flexibility (e.g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket =� *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token</context>
</contexts>
<marker>Lewis, Croft, 1990</marker>
<rawString>D. D. Lewis, W. B. Croft. 1990. Term clustering of syntactic phrase. In Proceedings of SIGIR-90, 13th ACM International Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL-98.</booktitle>
<contexts>
<context position="2146" citStr="Lin, 1998" startWordPosition="329" endWordPosition="330">s, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (interpreting the semantic association among components in MWEs). To extract</context>
<context position="17056" citStr="Lin (1998" startWordPosition="2868" endWordPosition="2869">n two tokens (h(ti, tj)). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other (</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998a. Automatic retrieval and clustering of similar words. In Proceedings of ACL-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using collocation statistics in information extraction.</title>
<date>1998</date>
<booktitle>In Proc. MUC-7.</booktitle>
<contexts>
<context position="2146" citStr="Lin, 1998" startWordPosition="329" endWordPosition="330">s, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing MWEs as word units at the type level), detecting or measuring compositionality of MWEs, semantic interpretation (interpreting the semantic association among components in MWEs). To extract</context>
<context position="17056" citStr="Lin (1998" startWordPosition="2868" endWordPosition="2869">n two tokens (h(ti, tj)). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other (</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998b. Using collocation statistics in information extraction. In Proc. MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL-99,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="7453" citStr="Lin (1999)" startWordPosition="1190" endWordPosition="1191">atively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An obvious change of the MI value of the phrase in the substitution test is taken as the evidence of the MWEs being non-compositional. Bannard et al. (2003) assume that compositional MWEs occur in similar lexical context as their component parts. The co-occurrence vector representations of verb particle construction (VPC) and the component words are utilized to determine the compositionality of the MWE. There have also been a few token-based classification approaches, aimed at classifying </context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>D. Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of ACL-99, 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>H Tomobe</author>
<author>T Nishimura</author>
</authors>
<title>Robust estimation of google counts for social network extraction.</title>
<date>2007</date>
<booktitle>In AAAI-07.</booktitle>
<contexts>
<context position="19945" citStr="Matsuo et al., 2007" startWordPosition="3373" endWordPosition="3376">gine, we approximated it by setting it to the number of hits obtained for the word the. The assumption is that the word the occurs in all English language web pages (Lapata and Keller, 2005). Using web counts rather than bi-gram counts from a corpus as the basis for computing semantic relatedness has the advantage that the web is a significantly larger database than any compiled corpus, which makes it much more likely that we can find information about the concepts we are looking for (thus alleviating data sparseness). However, search engine counts are notoriously unreliable (Kilgariff, 2007; Matsuo et al., 2007) and while previous studies have shown that web counts can be used as reliable proxies for corpus-based counts for some applications (Zhu and Rosenfeld, 2001; Lapata and Keller, 2005) it is not clear that this also applies when modeling semantic relatedness. We thus carried out a number of experiments testing the reliability of page counts (Section 4.1) and comparing the NGD measure to a standard distributional approach (Section 4.2). simcos(−→x , →−y ) = x2i Xn i=1 tuu v 2 yi Xn i=1 tuu v (14) 4The inflected forms were generated by applying the morph tools developed at the University of Susse</context>
</contexts>
<marker>Matsuo, Tomobe, Nishimura, 2007</marker>
<rawString>Y. Matsuo, H. Tomobe, T. Nishimura. 2007. Robust estimation of google counts for social network extraction. In AAAI-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>J Carroll</author>
<author>D Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="20568" citStr="Minnen et al., 2001" startWordPosition="3481" endWordPosition="3484">d while previous studies have shown that web counts can be used as reliable proxies for corpus-based counts for some applications (Zhu and Rosenfeld, 2001; Lapata and Keller, 2005) it is not clear that this also applies when modeling semantic relatedness. We thus carried out a number of experiments testing the reliability of page counts (Section 4.1) and comparing the NGD measure to a standard distributional approach (Section 4.2). simcos(−→x , →−y ) = x2i Xn i=1 tuu v 2 yi Xn i=1 tuu v (14) 4The inflected forms were generated by applying the morph tools developed at the University of Sussex (Minnen et al., 2001) which are available at: http://www.informatics.susx.ac.uk/ research/groups/nlp/carroll/morph.html 79 4.1 Search Engine Stability We first carried out some experiments to test the stability of the page counts returned by two of the most widely-used search engines, Google and Yahoo. For both search engines, we found a number of problems.5 Total number of pages indexed The total number of the web pages indexed by a search engine varies across time and the numbers provided are somewhat unreliable. This is a potential problem for NGD because we need to fix the value of M in Formula 15. As an appro</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>G. Minnen, J. Carroll, D. Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>G Hirst</author>
</authors>
<title>Distributional measures of concept-distance: A task-oriented evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-06.</booktitle>
<contexts>
<context position="17085" citStr="Mohammad and Hirst (2006)" startWordPosition="2870" endWordPosition="2873">(h(ti, tj)). Modeling semantic relatedness between two terms is currently an area of active research. There are two main approaches. Methods based on manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other (NGD) based on search engine p</context>
</contexts>
<marker>Mohammad, Hirst, 2006</marker>
<rawString>S. Mohammad, G. Hirst. 2006. Distributional measures of concept-distance: A task-oriented evaluation. In Proceedings of EMNLP-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pad´o</author>
<author>M Lapata</author>
</authors>
<title>Dependency-based construction of semantic space models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>S. Pad´o, M. Lapata. 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S L Piao</author>
<author>P Rayson</author>
<author>D Archer</author>
<author>A Wilson</author>
<author>T McEnery</author>
</authors>
<title>Extracting multiword expressions with a semantic tagger.</title>
<date>2003</date>
<booktitle>In Proc. of the ACL 2003 Workshop on Multiword Expressions,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="1959" citStr="Piao et al., 2003" startWordPosition="296" endWordPosition="299">g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket =� *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on the table), extraction (recognizing M</context>
</contexts>
<marker>Piao, Rayson, Archer, Wilson, McEnery, 2003</marker>
<rawString>S. S. L. Piao, P. Rayson, D. Archer, A. Wilson, T. McEnery. 2003. Extracting multiword expressions with a semantic tagger. In Proc. of the ACL 2003 Workshop on Multiword Expressions, 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Knowledge derived from Wikipedia for computing semantic relatedness.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>30</volume>
<pages>212</pages>
<contexts>
<context position="17233" citStr="Ponzetto and Strube, 2007" startWordPosition="2892" endWordPosition="2895">n manually built lexical knowledge bases, such as WordNet, compute the shortest path between two concepts in the knowledge base and/or look at word overlap in the glosses (see Budanitsky and Hirst (2006) for an overview). Distributional approaches, on the other hand, rely on text corpora, and model relatedness by comparing the contexts in which two words occur, assuming that related words occur in similar context (e.g., Hindle (1990), Lin (1998a), Mohammad and Hirst (2006)). More recently, there has also been research on using Wikipedia and related resources for modeling semantic relatedness (Ponzetto and Strube, 2007; Zesch et al., 2008). WordNet-based approaches are unsuitable for our purposes as they only model so-called “classical relations” like hypernymy, antonymy etc. For our task, we need to model a wide range of relations, e.g., between ice and water. Hence we opted for a distributional approach. We experimented with two different approaches, one (DV ) based on syntactic co-occurrences in a large text corpus and the other (NGD) based on search engine page counts. Dependency Vectors (DV) is a distributional approach which does not look simply at word cooccurrences in a fixed-size window but takes i</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. P. Ponzetto, M. Strube. 2007. Knowledge derived from Wikipedia for computing semantic relatedness. Journal of Artificial Intelligence Research, 30:181– 212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riehemann</author>
</authors>
<title>A Constructional Approach to Idioms and Word Formation.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="9653" citStr="Riehemann, 2001" startWordPosition="1550" endWordPosition="1552">ised clustering, they do rely on some resources such as databases of idioms. Cook et al. (2007) and Fazly et al. (2009) rely crucially on the concept of canonical form (CForm). It is assumed that for each idiom there is a fixed form (or a small set of those) corresponding to the syntactic pattern(s) in which the idiom normally occurs. The canonical form allows for inflection variation of the heard verb but not for other variations (such as nominal inflection, choice of determiner etc.). It has been observed that if an expression is used idiomatically it typically occurs in its canonical form (Riehemann, 2001). Fazly and her colleagues exploit this behavior and propose an unsupervised method for token-based idiom classification in which an expression is classified as idiomatic if it occurs in canonical form and literal otherwise. The canonical forms are determined automatically using a statistical, frequency-based measure. They also developed statistical measures to measure the lexical and syntactic fixedness of a given expression, which is used to automatically recognize expression types, as well as their token identification in context. They report an average accuracy of 72% for their canonical f</context>
</contexts>
<marker>Riehemann, 2001</marker>
<rawString>S. Riehemann. 2001. A Constructional Approach to Idioms and Word Formation. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T T Rila Mandala</author>
<author>H Tanaka</author>
</authors>
<title>Query expansion using heterogeneous thesauri.</title>
<date>2000</date>
<journal>Inf. Process. Manage.,</journal>
<volume>36</volume>
<issue>3</issue>
<contexts>
<context position="1896" citStr="Mandala and Tanaka, 2000" startWordPosition="284" endWordPosition="287">g., good morning, good night), variation in syntactic flexibility (e.g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket =� *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater </context>
</contexts>
<marker>Mandala, Tanaka, 2000</marker>
<rawString>T. T. Rila Mandala, H. Tanaka. 2000. Query expansion using heterogeneous thesauri. Inf. Process. Manage., 36(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: a pain in the neck for NLP.</title>
<date>2001</date>
<booktitle>In Lecture Notes in Computer Science.</booktitle>
<contexts>
<context position="775" citStr="Sag et al., 2001" startWordPosition="110" endWordPosition="113"> University Postfach 15 11 50 66041 Saarbr¨ucken Germany {linlin,csporled}@coli.uni-saarland.de Abstract We present a graph-based model for representing the lexical cohesion of a discourse. In the graph structure, vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically. We show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions. 1 Introduction Multiword expressions (MWEs) are defined as “idiosyncratic interpretations that cross word boundaries or spaces” (Sag et al., 2001). Such expressions are pervasive in natural language; they are estimated to be equivalent in number to simplex words in mental lexicon (Jackendoff, 1997). MWEs exhibit a number of lexical, syntactic, semantic, pragmatic and statistical idiosyncrasies: syntactic peculiarities (e.g., by and large, ad hoc), semantic non-compositionality (e.g., as in kick the bucket (die) and red tape (bureaucracy)), pragmatic idiosyncrasies (the expression is sometimes associated with a fixed pragmatic point, e.g., good morning, good night), variation in syntactic flexibility (e.g., I handed in my thesis = I hand</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2001</marker>
<rawString>I. A. Sag, T. Baldwin, F. Bond, A. Copestake, D. Flickinger. 2001. Multiword expressions: a pain in the neck for NLP. In Lecture Notes in Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>J Allan</author>
<author>C Buckley</author>
<author>A Singhal</author>
</authors>
<title>Automatic analysis, theme generation and summarization of machine-readable texts.</title>
<date>1994</date>
<journal>Science,</journal>
<volume>264</volume>
<issue>3</issue>
<contexts>
<context position="6095" citStr="Salton et al., 1994" startWordPosition="972" endWordPosition="975">ph), where the vertices of the graph correspond to the content words of the text and the edges encode the semantic relatedness between pairs of words. To distinguish between literal and non-literal use of MWEs, we look at how the average relatedness of the graph changes when the component words of the MWE are excluded or included in the graph (see Section 3).1 We first introduced the cohesion graph method in Sporleder and Li (2009). In the present paper, 1By modeling lexical cohesion as a graph structure, we follow earlier approaches in information retrieval, notably by Salton and colleagues (Salton et al., 1994). The difference is that these works aim at representing similarity between larger text segments (e.g., paragraphs) in a so-called ’text’ or ’paragraph relation map’, whose vertices correspond to a text segment and whose edges represent the similarity between the segments (modeled as weighted term overlap). we provide a formalization of the graph and experiment with different vertex and edge weighting schemes. We also report on experiments with varying the size of the input context and also with pruning the graph structure automatically. 2 Related Work Type-based MWE classification aims to ext</context>
</contexts>
<marker>Salton, Allan, Buckley, Singhal, 1994</marker>
<rawString>G. Salton, J. Allan, C. Buckley, A. Singhal. 1994. Automatic analysis, theme generation and summarization of machine-readable texts. Science, 264(3):1421–1426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<date>1999</date>
<booktitle>Retrieving collocations from text: Xtract. Computational Linguistics,</booktitle>
<contexts>
<context position="6961" citStr="Smadja, 1999" startWordPosition="1111" endWordPosition="1112">tween the segments (modeled as weighted term overlap). we provide a formalization of the graph and experiment with different vertex and edge weighting schemes. We also report on experiments with varying the size of the input context and also with pruning the graph structure automatically. 2 Related Work Type-based MWE classification aims to extract multiword expression types in text from observations of the token distribution. It aims to pick up on word combinations which occur with comparatively high frequencies when compared to the frequencies of the individual words (Evert and Krenn, 2001; Smadja, 19993). The lexical and syntactic fixedness property can also be utilized to automatically extract MWEs (Baldwin and Villavicencio, 2002). The study of semantic compositionality of MWEs focuses on the degree to which the semantics of the parts of an MWE contribute towards the meaning of the whole. The aim is a binary classification of the MWEs as idiosyncratically decomposable (e.g. spill the beans) or non-decomposable (e.g. kick the bucket). Several approaches have been proposed. Lin (1999) uses the substitution test2 and mutual information (MI) to determine the compositionality of the phrase. An</context>
</contexts>
<marker>Smadja, 1999</marker>
<rawString>F. Smadja. 19993. Retrieving collocations from text: Xtract. Computational Linguistics, 19(1):143–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Spearman</author>
</authors>
<title>The proof and measurement of association between two things.</title>
<date>1904</date>
<journal>Amer. J. Psychol,</journal>
<pages>72--101</pages>
<contexts>
<context position="23794" citStr="Spearman, 1904" startWordPosition="4026" endWordPosition="4028"> we believe that the web-based approach for computing relatedness is more suitable for our task since it gives us access to more data and allows us to also model relations based on (upto-date) world knowledge. However, the question arises whether the stability problems observed in the previous section have a negative effect on the performance of the NGD measure. To test this, we conducted a small study in which we compared the relatedness scores obtained by NGD and the semantic vector space model to the human ratings compiled by Finkelstein et al. (2002).6 We used Spearman’s correlation test (Spearman, 1904) to compare the ranked human ratings to the ranked ratings obtained by NGD and the vector space method. The (human) inter-annotator agreement varies a lot for different pairs of annotators (between 0.41 and 0.82 by Spearman’s correlation test), suggesting that deciding on the semantic relatedness between arbitrary pairs of words is not an easy task even for humans. In general, the NGD-human agreement is comparable to the human-human agreement. The agreement between the NGD and average human agreement is higher than some human-human agreements. Furthermore, we found that NGD actually outperform</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>C. Spearman. 1904. The proof and measurement of association between two things. Amer. J. Psychol, 72–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>L Li</author>
</authors>
<title>Unsupervised recognition of literal and non-literal use of idiomatic expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL-09.</booktitle>
<contexts>
<context position="5910" citStr="Sporleder and Li (2009)" startWordPosition="944" endWordPosition="947">hich makes this method weakly supervised. We propose an alternative, parameter-free method in which we model the cohesive structure of a discourse as a graph structure (called cohesion graph), where the vertices of the graph correspond to the content words of the text and the edges encode the semantic relatedness between pairs of words. To distinguish between literal and non-literal use of MWEs, we look at how the average relatedness of the graph changes when the component words of the MWE are excluded or included in the graph (see Section 3).1 We first introduced the cohesion graph method in Sporleder and Li (2009). In the present paper, 1By modeling lexical cohesion as a graph structure, we follow earlier approaches in information retrieval, notably by Salton and colleagues (Salton et al., 1994). The difference is that these works aim at representing similarity between larger text segments (e.g., paragraphs) in a so-called ’text’ or ’paragraph relation map’, whose vertices correspond to a text segment and whose edges represent the similarity between the segments (modeled as weighted term overlap). we provide a formalization of the graph and experiment with different vertex and edge weighting schemes. W</context>
<context position="25255" citStr="Sporleder and Li (2009)" startWordPosition="4264" endWordPosition="4267">available at: http://www.cs. technion.ac.il/˜gabr/resources/data/ wordsim353/ 7There may be several reasons for this. Apart from the fact that NGD has access to a larger data set, it may also be that syntactic co-occurrence information is not ideal for modeling this type of relatedness; co-occurrence information in a fixed window might be more useful. Furthermore, we did not spend much time on finding an optimal parameter setting for the dependency vector method. 80 tion 5.1. We report on our experiments and results in Section 5.2. 5.1 Data Throughout the experiments we used the data set from Sporleder and Li (2009). The data consist of 17 potentially idiomatic expressions from the English Gigaword corpus, which were extracted with five paragraphs of context and manually annotated as ’literal’ or ’non-literal’ (see Table 2). The interannotator agreement on a doubly annotated sample of the data was 97% and the kappa score 0.7 (Cohen, 1960). expression literal non-lit. all back the wrong horse 0 25 25 bite off more than one can chew 2 142 144 bite one’s tongue 16 150 166 blow one’s own trumpet 0 9 9 bounce off the wall* 39 7 46 break the ice 20 521 541 drop the ball* 688 215 903 get one’s feet wet 17 140 1</context>
</contexts>
<marker>Sporleder, Li, 2009</marker>
<rawString>C. Sporleder, L. Li. 2009. Unsupervised recognition of literal and non-literal use of idiomatic expressions. In Proceedings of EACL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Wacholder</author>
<author>P Song</author>
</authors>
<title>Toward a task-based gold standard for evaluation of NP chunks and technical terms.</title>
<date>2003</date>
<booktitle>In Proc HLT-NAACL.</booktitle>
<contexts>
<context position="1923" citStr="Wacholder and Song, 2003" startWordPosition="288" endWordPosition="291">ht), variation in syntactic flexibility (e.g., I handed in my thesis = I handed my thesis in vs. Kim kicked the bucket =� *the bucket was kicked by Kim), variation in productivity (there are various levels of productivity for different MWEs, e.g., kick/*beat/*hit the bucket, call/ring/phone/*telephone up). These idiosyncrasies pose challenges for NLP systems, which have to recognize that an expression is an MWE to deal with it properly. Recognizing MWEs has been shown to be useful for a number of applications such as information retrieval (Lewis and Croft, 1990; Rila Mandala and Tanaka, 2000; Wacholder and Song, 2003) and POS tagging (Piao et al., 2003). It has also been shown that MWEs account for 8% of parsing errors with precision grammars (Baldwin et al., 2004). Furthermore, MWE detection is used in information extraction (Lin, 1998b) and an integral component of symbolic MT systems (Gerber and Yang, 1997; Bond and Shirai, 1997). However, the special properties of MWEs can also be exploited to recognize MWEs automatically. There have been many studies on MWEs: identification (determining whether multiple simplex words form a MWE in a given token context, e.g. put the sweater on vs. put the sweater on t</context>
</contexts>
<marker>Wacholder, Song, 2003</marker>
<rawString>N. Wacholder, P. Song. 2003. Toward a task-based gold standard for evaluation of NP chunks and technical terms. In Proc HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zesch</author>
<author>C M¨uller</author>
<author>I Gurevych</author>
</authors>
<title>Using wiktionary for computing semantic relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of AAAI-08,</booktitle>
<pages>861--867</pages>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>T. Zesch, C. M¨uller, I. Gurevych. 2008. Using wiktionary for computing semantic relatedness. In Proceedings of AAAI-08, 861–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>R Rosenfeld</author>
</authors>
<title>Improving trigram language modeling with the world wide web.</title>
<date>2001</date>
<booktitle>In Proceedings of ICASSP-01.</booktitle>
<contexts>
<context position="20102" citStr="Zhu and Rosenfeld, 2001" startWordPosition="3398" endWordPosition="3401"> web pages (Lapata and Keller, 2005). Using web counts rather than bi-gram counts from a corpus as the basis for computing semantic relatedness has the advantage that the web is a significantly larger database than any compiled corpus, which makes it much more likely that we can find information about the concepts we are looking for (thus alleviating data sparseness). However, search engine counts are notoriously unreliable (Kilgariff, 2007; Matsuo et al., 2007) and while previous studies have shown that web counts can be used as reliable proxies for corpus-based counts for some applications (Zhu and Rosenfeld, 2001; Lapata and Keller, 2005) it is not clear that this also applies when modeling semantic relatedness. We thus carried out a number of experiments testing the reliability of page counts (Section 4.1) and comparing the NGD measure to a standard distributional approach (Section 4.2). simcos(−→x , →−y ) = x2i Xn i=1 tuu v 2 yi Xn i=1 tuu v (14) 4The inflected forms were generated by applying the morph tools developed at the University of Sussex (Minnen et al., 2001) which are available at: http://www.informatics.susx.ac.uk/ research/groups/nlp/carroll/morph.html 79 4.1 Search Engine Stability We f</context>
</contexts>
<marker>Zhu, Rosenfeld, 2001</marker>
<rawString>X. Zhu, R. Rosenfeld. 2001. Improving trigram language modeling with the world wide web. In Proceedings of ICASSP-01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>