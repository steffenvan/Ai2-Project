<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025396">
<title confidence="0.966249">
Classical Chinese Sentence Segmentation
</title>
<author confidence="0.999457">
Hen-Hsen Huang†, Chuen-Tsai Sun‡ and Hsin-Hsi Chen†
</author>
<affiliation confidence="0.9997535">
†Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan
‡Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan
</affiliation>
<email confidence="0.9914">
hhhuang@nlg.csie.ntu.edu.tw ctsun@cis.nctu.edu.tw hhchen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.993767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969722222222">
Sentence segmentation is a fundamental
issue in Classical Chinese language
processing. To facilitate reading and
processing of the raw Classical Chinese
data, we propose a statistical method to
split unstructured Classical Chinese text
into smaller pieces such as sentences and
clauses. The segmenter based on the
conditional random field (CRF) model is
tested under different tagging schemes
and various features including n-gram,
jump, word class, and phonetic informa-
tion. We evaluated our method on four
datasets from several eras (i.e., from the
5th century BCE to the 19th century).
Our CRF segmenter achieves an F-score
of 83.34% and can be applied on a varie-
ty of data from different eras.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793081632653">
Chinese word segmentation is a well-known and
widely studied problem in Chinese language
processing. In Classical Chinese processing, sen-
tence segmentation is an even more vexing issue.
Unlike English and other western languages,
there is no delimiter marking the end of the word
in Chinese. Moreover, not only is there a lack of
delimiters between the words, almost all pre-
20th century Chinese is written without any
punctuation marks. Figure 1 shows photocopies
of printed and hand written documents from the
19th century. Within any given paragraph, the
Chinese characters are printed as evenly spaced
characters, with nothing to separate words from
words, phrases from phrases, and sentences from
sentences. Thus, inside a paragraph, explicit
boundaries of sentences and clauses are lacking.
In order to understand the structure, readers of
Classical Chinese have to manually identify
these boundaries during the reading. This
process is called Classical Chinese sentence
segmentation, or Judo (句讀).
For example, the opening lines of the Daoist
classic Zhuangzi originally lacked segmentation:
北/north 冥/ocean 有/have 魚/fish 其/it 名/name
為/is 鯤/Kun (a kind of big fish) 鯤/Kun 之/of
大/big 不/not 知/know 幾/how 千/thousand 里
/mile 也/exclamation
The meaning of the text is hard to interpret
without segmentation. Below is the identical text
as segmented by a human being. It is clearly
more readable.
北冥有魚/in the north ocean there is a fish
其名為鯤 /its name is Kun
鯤之大/the size of the Kun
不知幾千里也/I don’t know how many
thousand miles the fish is
However, sentence segmentation in Classical
Chinese is not a trivial problem. Classical Chi-
nese sentence segmentation, like Chinese word
segmentation, is inherently ambiguous. Individ-
uals generally perform sentence segmentation in
instinctive ways. To identify the boundaries of
sentences and clauses, they primarily rely on
their experience and sense of the language rather
than on a systematic procedure. It is thus diffi-
cult to construct a set of rules or practical proce-
dures to specify the segmentation of the infinite
variety of Classical Chinese sentences.
</bodyText>
<figureCaption confidence="0.999385">
Figure 1. A Printed Page (Left) and a Hand Written Manuscript (Right) from the 19th Century.
</figureCaption>
<bodyText confidence="0.998987451612903">
Because of the importance of sentence seg-
mentation, beginning in the 20th century, some
editions of the Chinese classics have been labor-
intensively segmented and marked with modern
punctuation. However, innumerable documents
in Classical Chinese from the centuries of Chi-
nese history remain to be segmented. To aid in
processing these documents, we propose an au-
tomated Classical Chinese sentence segmenta-
tion approach that enables completion of seg-
mentation tasks quickly and accurately. To con-
struct the sentence segmenter for Classical Chi-
nese, the popular sequence tagging models, con-
ditional random field (CRF) (Lafferty et al.,
2001), are adopted in this study.
The rest of this paper is organized as follows.
First, we describe the Classical Chinese sentence
segmentation problem in Section 2. In Section 3,
we review the relevant literature, including sen-
tence boundary detection (SBD) and Chinese
word segmentation. In Section 4, we introduce
the tagging schemes along with the features, and
show how the sentence segmentation problem
can be transformed into a sequence tagging
problem and decoded with CRFs. In Section 5,
the experimental setup and data are described. In
Section 6, we report the experimental results and
discuss the properties and the challenges of the
Classical Chinese sentence segmentation prob-
lem. Finally, we conclude the remarks in Section
7.
</bodyText>
<sectionHeader confidence="0.980325" genericHeader="introduction">
2 Problem Description
</sectionHeader>
<bodyText confidence="0.98424494117647">
The outcomes of Classical Chinese sentence
segmentation are not well-defined in linguistics
at present. In general, the results of segmentation
consist of sentences, clauses, and phrases. For
instance, in the segmented sentence “野馬也 /
塵埃也 / 生物之以息相吹也”, “野馬也” (“the
mists on the mountains like wild horses”) and
“塵埃也” (“the dust in the air”) are phrases, and ”
生物之以息相吹也” (“the living creatures blow
their breaths at each other”) is a clause. A
sentence such as “吾以是狂而不信也” (“I do
not believe it because it is ridiculous.”) is a short
sentence itself, and does not require any
segmentation. For a given text, there is no strict
rule to determine at which level the
segmentation should be performed. For instance,
the opening lines of the Daoist classic Daodejing
is “道可道非常道名可名非常名” (“The way
that can be spoken is not the eternal way. The
name that can be given is not the eternal name.”)
which is usually segmented as “道可道 / 非常道
/ 名可名 / 非常名”, but may also be segmented
as “道 / 可道 / 非常道 / 名 / 可名 / 非常名”.
Either segmentation is reasonable.
In this paper, we do not distinguish among the
three levels of segmentation. Instead, our system
learns directly from the human-segmented cor-
pus. After training, our system will be adapted to
perform human-like segmentation automatically.
Further, we do not distinguish the various out-
comes of Classical Chinese sentence segmenta-
tion. Instead, for the sake of convenience, every
product of the segmentation process is termed
“clause” in the following sections.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.995871488372093">
Besides Classical Chinese, sentence boundary
detection (SBD) is also an issue in English and
other western languages. SBD in written texts
and speech represents quite different problems.
For written text, the SBD task is to distinguish
periods used as the end-of-sentence indicator
(full stop) from other usages, such as parts of
abbreviations and decimal points. By contrast,
the task of SBD in speech is closely related to
the task of Classical Chinese sentence segmenta-
tion. In speech processing, the outcome of
speech recognizers is a sequence of words, in
which the punctuation marks are absence, and
the sentence boundaries are thus lacking. To re-
cover the syntactic structure of the original
speech, SBD is required.
Like Classical Chinese sentence segmentation,
the task of SBD in speech is to determine which
of the inter-word boundaries in the stream of
words should be marked as end-of-sentence, and
then to divide the entire word sequence into in-
dividual sentences. Empirical methods are com-
monly employed to deal with this problem. Such
methods involve many different sequence labe-
ling models including HMMs (Shriberg et al.,
2000), maximum entropy (Maxent) models (Liu
et al., 2004), and CRFs (Liu et al., 2005).
Among these, a CRF model used in Liu et al
(2005) offered the lowest error rate.
Chinese word segmentation is a problem
closely related to Classical Chinese sentence
segmentation. The former identifies the bounda-
ries of the words in a given text, while the latter
identifies the boundaries of the sentences, claus-
es, and phrases. In contrast to sentences and
clauses, the length of Chinese words is shorter,
and the variety of Chinese words is more limited.
Despite the minor unknown words, most of the
frequent words can be handled with a dictionary
predefined by Chinese language experts or ex-
tracted from the corpus automatically. However,
it is impossible to maintain a dictionary of the
infinite number of sentences and clauses. For
these reasons, the Classical Chinese sentence
segmentation problem is more challenging.
Methods of Chinese word segmentation can
be mainly classified into heuristic rule-based
approaches, statistical machine learning ap-
proaches, and hybrid approaches. Hybrid ap-
proaches combine the advantages of heuristic
and statistical approaches to achieve better re-
sults (Gao et al., 2003; Xue, 2003; Peng et al.,
2004).
Xue (2003) transformed the Chinese word
segmentation problem into a tagging problem.
For a given sequence of Chinese characters, the
author applies a Maxent tagger to assign each
character one of four positions-of-character
(POC) tags, and then coverts the tagged se-
quence into a segmented sequence. The four
POC tags used in Xue (2003) denote the posi-
tions of characters within a word. For example,
the first character of a word is tagged “left
boundary”, the last character of a word is tagged
“right boundary”, the middle character of a word
is tagged “middle”, and a single character that
forms a word by itself is tagged “single-
character-word”. Once the given sequence is
tagged, the boundaries of words are also re-
vealed, and the task of segmentation becomes
straightforward. However, the Maxent models
used in Xue (2003) suffer from an inherent the
label bias problem. Peng et al (2004) uses the
CRFs to address this issue. The tags used in
Peng et al (2004) are of only two types, “start”
and “non-start”, in which the “start” tag denotes
the first character of a word, and the characters
in other positions are given the “non-start” tag.
The closest previous works to Classic Chinese
sentence segmentation are Huang (2008) and
Zhang et al. (2009). Huang combined the Xue’s
tagging scheme (i.e., 4-tag set) and CRFs to ad-
dress the Classical Chinese sentence segmenta-
tion problem and reported an F-score of 80.96%
averaged over various datasets. A similar work
by Zhang et al. reported an F-score of 71.42%.
</bodyText>
<sectionHeader confidence="0.997864" genericHeader="method">
4 Methods
</sectionHeader>
<bodyText confidence="0.999748111111111">
Conditional random field is our tagging model,
and the implementation is CrfSgd 1.31 provided
by Léon Bottou. As denoted by the tool name,
the parameters in this implementation are opti-
mized using Stochastic Gradient Descent (SGD)
which convergences much faster than the com-
mon optimization algorithms such as L-BFGS
and conjugate gradient (Vishwanathan, et al.,
2006). To construct the sentence segmenter on
</bodyText>
<footnote confidence="0.824312">
1 http://leon.bottou.org/projects/sgd
</footnote>
<bodyText confidence="0.998192">
CRF, the tagging scheme and the feature func-
tions play the crucial roles.
</bodyText>
<subsectionHeader confidence="0.999346">
4.1 Tagging Schemes
</subsectionHeader>
<bodyText confidence="0.9966864">
In the previous works (Huang, 2008; Zhang et
al., 2009), POC tags used in Chinese word seg-
mentation (Xue, 2003) are converted to denote
the positions of characters within a clause. The
4-tag set is redefined as L (“the left boundary of
a clause”), R (“the right boundary of a clause”),
M (“the middle character of a clause”), and S (“a
single character forming a clause”). For example,
the sentence “北冥有魚其名為鯤鯤之大不知幾
千里也” should be tagged as follows.
</bodyText>
<equation confidence="0.9974395">
北/L 冥/M 有/M 魚/R 其/L 名/M 為/M 鯤/R 鯤
/L 之/M 大/R 不/L 知/M 幾/M 千/M 里/M 也/R
</equation>
<bodyText confidence="0.996582041666667">
We can easily split the sentence into clauses by
making a break after each character tagged R
and S and obtain the final outcome “北冥有魚 /
其名為鯤 / 鯤之大 / 不知幾千里也”.
In this work, more tagging schemes are expe-
rimented. The basic tagging scheme for segmen-
tation is 2-tag set in which only two types of tags,
“start” and “non-start”, are used to label the se-
quence. The segmented fragments (clauses) for
sentence segmentation are usually much longer
than those for word segmentation. Thus, we add
more middle states into the 4-tag set to model
the nature of long fragments. The Markov chain
of our tagging scheme is shown in Figure 2,
where L2, L3, ..., Lk are the additional states to
extend Xue’s 4-tag set. In our experiments, vari-
ous k values are tested. If the k value is 1, the
scheme is identical to the one used in the two
previous works (Zhang et al., 2009; Huang,
2008). The 2-tag set, 4-tag set, 5-tag set and their
corresponding examples are listed in Table 1.
With the tagging scheme, the Classical Chinese
sentence segmentation task is transformed into a
sequence labeling or tagging task.
</bodyText>
<sectionHeader confidence="0.5536" genericHeader="method">
4.2 Features
</sectionHeader>
<bodyText confidence="0.99993425">
Due to the flexibility of the feature function in-
terface provided by CRFs, we apply various fea-
ture conjunctions. Besides the n-gram character
patterns, the phonetic information and the part-
</bodyText>
<figureCaption confidence="0.99763">
Figure 2. Markov Chain of Our Tagging Scheme.
</figureCaption>
<table confidence="0.999947785714286">
Tag set Tags Example
2-tag S: Start 不知其幾千里也
N: Non-Start 不知其幾千里也
4-tag L1: Left-end 不知其幾千里也
(k=1)
M: Middle 不知其幾千里也
R: Right-end 不知其幾千里也
S: Single 性 / 猶鰥柳也
5-tag L1: Left-end 不知其幾千里也
(k=2)
L2: Left-2nd 不知其幾千里也
M: Middle 不知其幾千里也
R: Right-end 不知其幾千里也
S: Single 性 / 猶鰥柳也
</table>
<tableCaption confidence="0.999826">
Table 1. Examples of Tag Sets.
</tableCaption>
<bodyText confidence="0.999864">
of-speech (POS) are also included. The pronun-
ciation of each Chinese character is labeled in
three ways. The first one is Mandarin Phonetic
Symbols (MPS), also known as Bopomofo,
which is a phonetic system for Modern Chinese.
The initial/final/tone of each character can be
obtained from its MPS label.
However, Chinese pronunciation varies in the
thousands of years, and the pronunciation of
Modern Chinese is much different from the
Classical Chinese. For this reason, two Ancient
Chinese phonetic systems, Fanqie (反切) and
Guangyun (廣韻), are applied to label the cha-
racters. The pronunciation of a target character is
represented by two characters in the Fanqie sys-
tem. The first character indicates the initial of
the target character, and the second character
indicates the combination of the final and the
tone. The Guangyun system is in a similar man-
ner with a smaller phonetic symbol set. There
are 8,157 characters in our phonetic dictionary
and the statistics are shown in Table 2.
The POS information is also considered. It is
difficult to construct a Classical Chinese POS
</bodyText>
<table confidence="0.9983365">
System #Initials #Finals #Tones
MPS 21 36 5
Fanqie 403 1,054
Guangyun 43 203
</table>
<tableCaption confidence="0.957589">
Table 2. Phonetic System Statistics.
</tableCaption>
<table confidence="0.9999482">
POS # Characters Examples
Beginning 60 �, *, �
Middle 50 �, 19
End 45 �, _�91, Lh, �
Interjection 20 UT, r&amp;quot; Olt, pW1
</table>
<tableCaption confidence="0.999237">
Table 3. Four Types of POS.
</tableCaption>
<bodyText confidence="0.999591125">
tagger at this moment. Instead, we collected
three types of particles that are usually placed at
the beginning, at the middle, and at the end of
Classical Chinese clauses. In addition, the inter-
jections which are usually used at the end of
clauses are also collected. Some examples are
given in Table 3. The five feature sets and the
feature templates are shown in Table 4.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999962411764706">
There are three major sets of experiments. In the
1st set of experiments, we test different tagging
schemes for Classical Chinese sentence segmen-
tation. In the 2nd set of experiments, all kinds of
feature sets and their combinations are tested.
The performances of the first two sets of expe-
riments are evaluated by 10-fold cross-validation
on four datasets which cross both eras and con-
texts. In the 3rd set of experiments, we train the
system on one dataset, and test it on the others.
In last part of the experiments, the generality of
the datasets and the toughness of our system are
tested (Peng et al., 2004). The cut-off threshold
for the features is set to 2 for all the experiments.
In other words, the features occur only once in
the training set will be ignored. The other op-
tions of CrfSgd remain default.
</bodyText>
<subsectionHeader confidence="0.825455">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999978083333333">
The datasets used in the evaluation are collected
from the corpora of the Pre-Qin and Han Dynas-
ties (the 5th century BCE to the 1st century BCE)
and the Qing Dynasty (the 17th century CE to
the 20th century CE). Chinese in the 19th cen-
tury is fairly different from Chinese in the era
before 0 CE. In ancient Chinese, the syntax is
much simpler, the sentences are shorter, and the
words are largely composed of a single character.
Those are unlike later and more modern Chinese,
where word segmentation is a serious issue.
Given these properties, the task of segmenting
</bodyText>
<table confidence="0.8552811">
Feature Set Template Function
Character 𝐶𝑖, −2 ≤ 𝑖 ≤ 2 Unigrams
𝐶𝑖 𝐶𝑖+1, −2 ≤ 𝑖 ≤ 1 Bigrams
𝐶𝑖 𝐶𝑖+1𝐶𝑖+2, −2 ≤ 𝑖 ≤ 0 Trigrams
𝐶𝑖𝐶𝑖 +2,−2 ≤ 𝑖 ≤ 0 Jumps
POS 𝑃𝑂𝑆_𝐵(𝐶0) Current character serves as a clause-beginning particle.
𝑃𝑂𝑆_𝑀(𝐶0) Current character serves as a clause-middle particle.
𝑃𝑂𝑆_𝐸(𝐶0) Current character serves as a clause-end particle.
𝑃𝑂𝑆_𝐼 (𝐶0) Current character serves as an interjection.
MPS 𝑀_𝐼(𝐶0) The initial of current character in MPS.
𝑀_𝐹(𝐶0) The final of current character in MPS.
𝑀_𝑇(𝐶0) The tone of current character in MPS.
𝑀_𝐹(𝐶−1)𝑀_𝑇 (𝐶−1)𝑀_𝐼 (𝐶0) The connection between successive characters.
Fanqie 𝐹_𝐼(𝐶0) The initial of current character in Fanqie.
𝐹_𝐹(𝐶0) The final and the tone of current character in Fanqie.
𝐹_𝐹(𝐶−1)𝐹_𝐼 (𝐶0) The connection between successive characters.
Guangyun 𝐺_𝐼(𝐶0) The initial of the current character in Guangyun.
𝐺_𝐹(𝐶0) The final and the tone of current character in Guan-
gyun.
𝐺_𝐹(𝐶−1)𝐺_𝐼 (𝐶0) The connection between successive characters.
</table>
<tableCaption confidence="0.994897">
Table 4. Feature Templates.
</tableCaption>
<table confidence="0.998716571428571">
Corpus Author Era # of data # of Size of cha- Average # of
entries characters racter set characters/clause
Zuozhuan Zuo Qiuming 500 BCE 3,381 195,983 3,238 4.145
Zhuangzi Zhuangzi 300 BCE 1,128 65,165 2,936 5.183
Shiji Qian Sima 100 BCE 4,778 503,890 4,788 5.049
Qing Documents Qing Dynasty 19th 1,000 111,739 3,147 7.199
Officials century
</table>
<tableCaption confidence="0.997443">
Table 5. Datasets and Statistics.
</tableCaption>
<bodyText confidence="0.999868463414634">
ancient Chinese sentences is easier than that of
segmenting later Chinese ones. Thus, we col-
lected texts from the pre-Qin and Han period,
and from the late Qing Dynasty closer to the
present, to show that our system can handle
Classical Chinese as it has evolved across a span
of two thousand years.
A summary of the four datasets is listed in
Table 5. The Zuozhuan is one of earliest histori-
cal works, recording events of China in the
Spring and Autumn Period (from 722 BCE to
481 BCE). The book Zhuangzi was named after
its semi-legendary author, the Daoist philoso-
pher Zhuangzi, who lived around the 4th century
BCE. The book consists of stories and fables, in
which the philosophy of the Dao is propounded.
The Shiji, known in English as The Records of
the Grand Historian, was written by Qian Sima
in the 1st century BCE. It narrates Chinese histo-
ry from 2600 BCE to 100 BCE. The Shiji is not
only an extremely long book of more than
500,000 characters, but also the chief historical
work of ancient China, exerting an enormous
influence on subsequent Chinese literature and
historiography.
The three ancient works are the most impor-
tant classics of Chinese literature. We fetched
well-segmented electronic editions of these
works from the online database of the Institute
of History and philology of the Academia Sinica,
Taiwan.2 Each work was partitioned into para-
graphs forming a single data entry, which acted
as the basic unit of training and testing. The da-
taset of Qing documents is selected from the
Qing Palace Memorials (奏摺) related to Taiwan
written in the 19th century. These documents
were kindly provided by the Taiwan History
Digital Library and have also been human-
segmented and stored on electronic media (Chen
et al., 2007). We randomly selected 1,000 para-
graphs from them as our dataset.
</bodyText>
<footnote confidence="0.891301">
2http://hanji.sinica.edu.tw
</footnote>
<subsectionHeader confidence="0.997878">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.997676714285714">
For Classical Chinese sentence segmentation, we
define the precision P as the ratio of the bounda-
ries of clauses which are correctly segmented to
all segmented boundaries, the recall R as the ra-
tio of correctly segmented boundaries to all ref-
erence boundaries, and the score F as the har-
monic mean of precision and recall:
</bodyText>
<equation confidence="0.903226">
P × R × 2
F= P+ R
</equation>
<table confidence="0.963437">
Dataset Precision Recall F-Score
Zuozhuan 100% 32.80% 42.73%
Zhuangzi 100% 19.84% 29.83%
Shiji 100% 14.11% 20.63%
Qing Doc. 100% 33.08% 41.42%
Average 100% 24.96% 33.65%
Table 6. Performance of Majority-Class Baseline.
Tag Set Precision Recall F-Score
2-tag set 85.00% 82.16% 82.92%
4-tag set 85.11% 82.13% 82.95%
5-tag set 85.26% 82.36% 83.18%
7-tag set 84.47% 82.18% 82.74%
Baseline 100% 24.96% 33.65%
</table>
<tableCaption confidence="0.999514">
Table 7. Comparison between Tagging Schemes.
</tableCaption>
<table confidence="0.999933384615385">
Features Precision Recall F-Score
Character 85.26% 82.36% 83.18%
POS 61.04% 40.35% 43.93%
MPS 65.31% 54.00% 56.31%
Fanqie 80.96% 76.80% 77.95%
Guangyun 73.11% 69.13% 69.59%
POS + 81.07% 74.91% 76.77%
Fanqie
Character 85.43% 82.52% 83.34%
+ Fanqie
Character 85.67% 81.70% 82.98%
+ POS +
Fanqie
</table>
<tableCaption confidence="0.999728">
Table 8. Comparison between Feature Sets.
</tableCaption>
<table confidence="0.999981333333333">
Dataset Precision Recall F-Score
Zuozhuan 92.83% 91.56% 91.79%
Zhuangzi 81.02% 78.87% 79.34%
Shiji 80.79% 78.10% 78.99%
Qing Doc. 87.07% 81.53% 83.24%
Average 85.43% 82.52% 83.34%
</table>
<tableCaption confidence="0.999758">
Table 9. Performance on Four Datasets.
</tableCaption>
<sectionHeader confidence="0.997826" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999900222222223">
Our baseline is a majority-class tagger which
always regards the whole paragraph as a single
sentence (i.e., never segments). In Table 6, the
performance of the baseline is given. In the 1st
set of experiments, four tagging schemes are
tested while the feature set is Character. The re-
sults are shown in Table 7. In the table, each of
the precision, the recall, and the F-score are av-
eraged over the four datasets for each scheme.
The results show that the CRF with the 5-tag set
is superior to the 4-tag set used in previous
works. However, the performance is degraded
when the k is larger.
In the 2nd set of experiments, the tag scheme
is fixed to the 5-tag set and a number of feature
set combinations are tested. The results are
shown in Table 8. The performance of MPS is
significantly inferior to the other two phonetic
systems. As expected, the pronunciation of Clas-
sical Chinese is much different from that of
Modern Chinese, thus the Ancient Chinese pho-
netic systems are more suitable for this work.
The Fanqie has a surprisingly performance close
to the Character. However, performance of the
combination of Character and Fanqie is similar
to the performance of Character only model.
This result indicates that the phonetic informa-
tion is an important clue to Classical Chinese
sentence segmentation but such information is
mostly already covered by the characters. Be-
sides, the simple POS features do not help a lot.
The higher precision and the lower recall of the
POS features show that the particles such as Z/
乎/者/也 is indeed a clue to segmentation, but
does not catch enough cases.
The best performance comes from the com-
bination of Character and Fanqie with the 5-tag
set. We use this configuration as our final tagger.
The performances of our tagger for each dataset
are given in Table 9. The result shows that our
tagger achieves fairly good performance on the
Zuozhuan segmentation, while obtaining accept-
able performance overall. Because the 19th cen-
tury Chinese is more complex than ancient Chi-
nese, what we had assumed was that segmenta-
tion of the Qing documents would more difficult.
However, the results indicate that our assump-
tion does not seem to be true. Our tagger per-
forms the sentence segmentation on the Qing
documents well, even better than on the Zhuang-
zi and on the Shiji. The issues of longer clauses
and word segmentation described earlier in this
paper do not significantly affect the performance
of our system.
In the last experiments, our system is trained
and tested on different datasets, and the results
are presented in Table 10, where the training
datasets are in the rows and the test datasets are
in the columns, and the F-scores of the segmen-
tation performance are shown in the inner entries.
As expected, the results of segmentation tasks
across datasets are significantly poorer than the
segmentation in the first two experiments.
These results indicate that our system main-
tains its performance on a test dataset differing
from the training dataset, but the difference in
written eras between the test dataset and training
dataset cannot be very large. Among all datasets,
Shiji is the best training dataset. As training on
Shiji and testing on the two other ancient corpo-
ra Zuozhuan and Zhuangzi, the performances of
our CRF segmenter are not bad.
</bodyText>
<table confidence="0.999534">
Training Set Testing Set
Zuozhuan Zhuangzi Shiji Qing doc. Average
Zuozhuan 72.04% 59.12% 38.85% 56.67%
Zhuangzi 63.70% 52.51% 42.75% 52.99%
Shiji 76.27% 75.46% 44.11% 65.28%
Qing doc. 52.68% 53.13% 42.61% 49.47%
Average 64.22% 66.88% 51.41% 41.90%
</table>
<tableCaption confidence="0.998882">
Table 10. F-score of Segmentation cross the Datasets.
</tableCaption>
<sectionHeader confidence="0.988832" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99967476">
Our Classical Chinese sentence segmentation is
important for many applications such as text
mining, information retrieval, corpora research,
and digital archiving. To aid in processing such
kind of data, an automatic sentence segmenta-
tion system is proposed. Different tagging
schemes and various features are introduced and
tested. Our system was evaluated using three
sets of experiments. Five main results are de-
rived. First, the CRF segmenter achieves an F-
score of 91.79% in the best case and 83.34% in
overall performance. Second, a little longer tag-
ging scheme improves the performance. Third,
the phonetic information, especially sourced
from Fanqie, is an important clue for Classical
Chinese sentence segmentation and may be use-
ful in the related tasks. Fourth, our method per-
forms well on data from various eras. In the ex-
periments, texts from both 500 BCE and the
19th century were well-segmented. Last, the
CRF segmenter maintains a certain level of per-
formance in situations which the test data and
the training data differ in authors, genres, and
written styles, but eras in which they were pro-
duced are sufficiently close.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992047368421">
Chen, Szu-Pei, Jieh Hsiang, Hsieh-Chang Tu, and
Micha Wu. 2007. On Building a Full-Text Digital
Library of Historical Documents. In Proceedings
of the 10th International Conference on Asian
Digital Libraries, Lecture Notes in Computer
Science, Springer-Verlag 4822:49-60.
Gao, Jianfeng, Mu Li, and Chang-Ning Huang. 2003.
Improved Source-Channel Models for Chinese
Word Segmentation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, 272-279.
Huang, Hen-Hsen. 2008. Classical Chinese Sentence
Division by Sequence Labeling Approaches. Mas-
ter’s Thesis, National Chiao Tung University,
Hsinchu, Taiwan.
Lafferty, John, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmentation and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, 282-289.
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2004. Comparing and Combining
Generative and Posterior Probability Models:
Some Advances in Sentence Boundary Detection
in Speech. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing.
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using Conditional Random
Fields for Sentence Boundary Detection in Speech.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, 451-
458. Ann Arbor, Mich., USA.
Peng, Fuchun, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese Segmentation and New Word
Detection using Conditional Random Fields. In
Proceedings of the 20th International Conference
on Computational Linguistics, 562-568.
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-
Tür, and Gökhan Tür. 2000. Prosody-Based Au-
tomatic Segmentation of Speech into Sentences
and Topics. Speech Communication, 32(1-2):127-
154.
Vishwanathan, S. V. N., Nicol N. Schraudolph, Mark
W. Schmidt, and Kevin P. Murphy. 2006. Accele-
rated training of conditional random fields with
stochastic gradient methods. In Proceedings of the
23th International Conference on Machine Learn-
ing, 969–976. ACM Press, New York, USA.
Xue, Nianwen. 2003. Chinese Word Segmentation as
Character Tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Zhang, Hel, Wang Xiao-dong, Yang Jian-yu, and
Zhou Wei-dong. 2009. Method of Sentence Seg-
mentation and Punctuating for Ancient Chinese.
Application Research of Computers, 26(9):3326-
3329.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780932">
<title confidence="0.999031">Classical Chinese Sentence Segmentation</title>
<author confidence="0.999514">Chuen-Tsai Hsin-Hsi</author>
<affiliation confidence="0.99805">of Computer Science and Information Engineering, National Taiwan University, Taipei,</affiliation>
<address confidence="0.962597">of Computer Science, National Chiao Tung University, Hsinchu, Taiwan</address>
<email confidence="0.830908">hhhuang@nlg.csie.ntu.edu.twctsun@cis.nctu.edu.twhhchen@csie.ntu.edu.tw</email>
<abstract confidence="0.998298">Sentence segmentation is a fundamental issue in Classical Chinese language processing. To facilitate reading and processing of the raw Classical Chinese data, we propose a statistical method to split unstructured Classical Chinese text into smaller pieces such as sentences and clauses. The segmenter based on the conditional random field (CRF) model is tested under different tagging schemes and various features including n-gram, jump, word class, and phonetic information. We evaluated our method on four datasets from several eras (i.e., from the 5th century BCE to the 19th century). Our CRF segmenter achieves an F-score of 83.34% and can be applied on a variety of data from different eras.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Szu-Pei Chen</author>
<author>Jieh Hsiang</author>
<author>Hsieh-Chang Tu</author>
<author>Micha Wu</author>
</authors>
<title>On Building a Full-Text Digital Library of Historical Documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Asian Digital Libraries, Lecture Notes in Computer Science, Springer-Verlag</booktitle>
<pages>4822--49</pages>
<contexts>
<context position="19056" citStr="Chen et al., 2007" startWordPosition="3114" endWordPosition="3117">most important classics of Chinese literature. We fetched well-segmented electronic editions of these works from the online database of the Institute of History and philology of the Academia Sinica, Taiwan.2 Each work was partitioned into paragraphs forming a single data entry, which acted as the basic unit of training and testing. The dataset of Qing documents is selected from the Qing Palace Memorials (奏摺) related to Taiwan written in the 19th century. These documents were kindly provided by the Taiwan History Digital Library and have also been humansegmented and stored on electronic media (Chen et al., 2007). We randomly selected 1,000 paragraphs from them as our dataset. 2http://hanji.sinica.edu.tw 5.2 Evaluation Metrics For Classical Chinese sentence segmentation, we define the precision P as the ratio of the boundaries of clauses which are correctly segmented to all segmented boundaries, the recall R as the ratio of correctly segmented boundaries to all reference boundaries, and the score F as the harmonic mean of precision and recall: P × R × 2 F= P+ R Dataset Precision Recall F-Score Zuozhuan 100% 32.80% 42.73% Zhuangzi 100% 19.84% 29.83% Shiji 100% 14.11% 20.63% Qing Doc. 100% 33.08% 41.42%</context>
</contexts>
<marker>Chen, Hsiang, Tu, Wu, 2007</marker>
<rawString>Chen, Szu-Pei, Jieh Hsiang, Hsieh-Chang Tu, and Micha Wu. 2007. On Building a Full-Text Digital Library of Historical Documents. In Proceedings of the 10th International Conference on Asian Digital Libraries, Lecture Notes in Computer Science, Springer-Verlag 4822:49-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Chang-Ning Huang</author>
</authors>
<title>Improved Source-Channel Models for Chinese Word Segmentation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>272--279</pages>
<contexts>
<context position="8517" citStr="Gao et al., 2003" startWordPosition="1327" endWordPosition="1330"> the frequent words can be handled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, an</context>
</contexts>
<marker>Gao, Li, Huang, 2003</marker>
<rawString>Gao, Jianfeng, Mu Li, and Chang-Ning Huang. 2003. Improved Source-Channel Models for Chinese Word Segmentation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, 272-279.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hen-Hsen Huang</author>
</authors>
<title>Classical Chinese Sentence Division by Sequence Labeling Approaches. Master’s Thesis,</title>
<date>2008</date>
<institution>National Chiao Tung University,</institution>
<location>Hsinchu, Taiwan.</location>
<contexts>
<context position="9781" citStr="Huang (2008)" startWordPosition="1542" endWordPosition="1543">tagged “singlecharacter-word”. Once the given sequence is tagged, the boundaries of words are also revealed, and the task of segmentation becomes straightforward. However, the Maxent models used in Xue (2003) suffer from an inherent the label bias problem. Peng et al (2004) uses the CRFs to address this issue. The tags used in Peng et al (2004) are of only two types, “start” and “non-start”, in which the “start” tag denotes the first character of a word, and the characters in other positions are given the “non-start” tag. The closest previous works to Classic Chinese sentence segmentation are Huang (2008) and Zhang et al. (2009). Huang combined the Xue’s tagging scheme (i.e., 4-tag set) and CRFs to address the Classical Chinese sentence segmentation problem and reported an F-score of 80.96% averaged over various datasets. A similar work by Zhang et al. reported an F-score of 71.42%. 4 Methods Conditional random field is our tagging model, and the implementation is CrfSgd 1.31 provided by Léon Bottou. As denoted by the tool name, the parameters in this implementation are optimized using Stochastic Gradient Descent (SGD) which convergences much faster than the common optimization algorithms such</context>
<context position="12004" citStr="Huang, 2008" startWordPosition="1927" endWordPosition="1928">-tag set in which only two types of tags, “start” and “non-start”, are used to label the sequence. The segmented fragments (clauses) for sentence segmentation are usually much longer than those for word segmentation. Thus, we add more middle states into the 4-tag set to model the nature of long fragments. The Markov chain of our tagging scheme is shown in Figure 2, where L2, L3, ..., Lk are the additional states to extend Xue’s 4-tag set. In our experiments, various k values are tested. If the k value is 1, the scheme is identical to the one used in the two previous works (Zhang et al., 2009; Huang, 2008). The 2-tag set, 4-tag set, 5-tag set and their corresponding examples are listed in Table 1. With the tagging scheme, the Classical Chinese sentence segmentation task is transformed into a sequence labeling or tagging task. 4.2 Features Due to the flexibility of the feature function interface provided by CRFs, we apply various feature conjunctions. Besides the n-gram character patterns, the phonetic information and the partFigure 2. Markov Chain of Our Tagging Scheme. Tag set Tags Example 2-tag S: Start 不知其幾千里也 N: Non-Start 不知其幾千里也 4-tag L1: Left-end 不知其幾千里也 (k=1) M: Middle 不知其幾千里也 R: Right-e</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, Hen-Hsen. 2008. Classical Chinese Sentence Division by Sequence Labeling Approaches. Master’s Thesis, National Chiao Tung University, Hsinchu, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmentation and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="3906" citStr="Lafferty et al., 2001" startWordPosition="585" endWordPosition="588">portance of sentence segmentation, beginning in the 20th century, some editions of the Chinese classics have been laborintensively segmented and marked with modern punctuation. However, innumerable documents in Classical Chinese from the centuries of Chinese history remain to be segmented. To aid in processing these documents, we propose an automated Classical Chinese sentence segmentation approach that enables completion of segmentation tasks quickly and accurately. To construct the sentence segmenter for Classical Chinese, the popular sequence tagging models, conditional random field (CRF) (Lafferty et al., 2001), are adopted in this study. The rest of this paper is organized as follows. First, we describe the Classical Chinese sentence segmentation problem in Section 2. In Section 3, we review the relevant literature, including sentence boundary detection (SBD) and Chinese word segmentation. In Section 4, we introduce the tagging schemes along with the features, and show how the sentence segmentation problem can be transformed into a sequence tagging problem and decoded with CRFs. In Section 5, the experimental setup and data are described. In Section 6, we report the experimental results and discuss</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmentation and Labeling Sequence Data. In Proceedings of the 18th International Conference on Machine Learning, 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Comparing and Combining Generative and Posterior Probability Models: Some Advances in Sentence Boundary Detection in Speech.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7372" citStr="Liu et al., 2004" startWordPosition="1147" endWordPosition="1150">ion marks are absence, and the sentence boundaries are thus lacking. To recover the syntactic structure of the original speech, SBD is required. Like Classical Chinese sentence segmentation, the task of SBD in speech is to determine which of the inter-word boundaries in the stream of words should be marked as end-of-sentence, and then to divide the entire word sequence into individual sentences. Empirical methods are commonly employed to deal with this problem. Such methods involve many different sequence labeling models including HMMs (Shriberg et al., 2000), maximum entropy (Maxent) models (Liu et al., 2004), and CRFs (Liu et al., 2005). Among these, a CRF model used in Liu et al (2005) offered the lowest error rate. Chinese word segmentation is a problem closely related to Classical Chinese sentence segmentation. The former identifies the boundaries of the words in a given text, while the latter identifies the boundaries of the sentences, clauses, and phrases. In contrast to sentences and clauses, the length of Chinese words is shorter, and the variety of Chinese words is more limited. Despite the minor unknown words, most of the frequent words can be handled with a dictionary predefined by Chin</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2004</marker>
<rawString>Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2004. Comparing and Combining Generative and Posterior Probability Models: Some Advances in Sentence Boundary Detection in Speech. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Mary Harper</author>
</authors>
<title>Using Conditional Random Fields for Sentence Boundary Detection in Speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>451--458</pages>
<location>Ann Arbor, Mich., USA.</location>
<contexts>
<context position="7401" citStr="Liu et al., 2005" startWordPosition="1153" endWordPosition="1156">e sentence boundaries are thus lacking. To recover the syntactic structure of the original speech, SBD is required. Like Classical Chinese sentence segmentation, the task of SBD in speech is to determine which of the inter-word boundaries in the stream of words should be marked as end-of-sentence, and then to divide the entire word sequence into individual sentences. Empirical methods are commonly employed to deal with this problem. Such methods involve many different sequence labeling models including HMMs (Shriberg et al., 2000), maximum entropy (Maxent) models (Liu et al., 2004), and CRFs (Liu et al., 2005). Among these, a CRF model used in Liu et al (2005) offered the lowest error rate. Chinese word segmentation is a problem closely related to Classical Chinese sentence segmentation. The former identifies the boundaries of the words in a given text, while the latter identifies the boundaries of the sentences, clauses, and phrases. In contrast to sentences and clauses, the length of Chinese words is shorter, and the variety of Chinese words is more limited. Despite the minor unknown words, most of the frequent words can be handled with a dictionary predefined by Chinese language experts or extra</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and Mary Harper. 2005. Using Conditional Random Fields for Sentence Boundary Detection in Speech. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, 451-458. Ann Arbor, Mich., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese Segmentation and New Word Detection using Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="8548" citStr="Peng et al., 2004" startWordPosition="1333" endWordPosition="1336">ndled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single character that forms</context>
<context position="15122" citStr="Peng et al., 2004" startWordPosition="2453" endWordPosition="2456">nts There are three major sets of experiments. In the 1st set of experiments, we test different tagging schemes for Classical Chinese sentence segmentation. In the 2nd set of experiments, all kinds of feature sets and their combinations are tested. The performances of the first two sets of experiments are evaluated by 10-fold cross-validation on four datasets which cross both eras and contexts. In the 3rd set of experiments, we train the system on one dataset, and test it on the others. In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al., 2004). The cut-off threshold for the features is set to 2 for all the experiments. In other words, the features occur only once in the training set will be ignored. The other options of CrfSgd remain default. 5.1 Datasets The datasets used in the evaluation are collected from the corpora of the Pre-Qin and Han Dynasties (the 5th century BCE to the 1st century BCE) and the Qing Dynasty (the 17th century CE to the 20th century CE). Chinese in the 19th century is fairly different from Chinese in the era before 0 CE. In ancient Chinese, the syntax is much simpler, the sentences are shorter, and the wor</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Peng, Fuchun, Fangfang Feng, and Andrew McCallum. 2004. Chinese Segmentation and New Word Detection using Conditional Random Fields. In Proceedings of the 20th International Conference on Computational Linguistics, 562-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Dilek HakkaniTür</author>
<author>Gökhan Tür</author>
</authors>
<title>Prosody-Based Automatic Segmentation of Speech into Sentences and Topics. Speech Communication,</title>
<date>2000</date>
<pages>32--1</pages>
<contexts>
<context position="7320" citStr="Shriberg et al., 2000" startWordPosition="1139" endWordPosition="1142">recognizers is a sequence of words, in which the punctuation marks are absence, and the sentence boundaries are thus lacking. To recover the syntactic structure of the original speech, SBD is required. Like Classical Chinese sentence segmentation, the task of SBD in speech is to determine which of the inter-word boundaries in the stream of words should be marked as end-of-sentence, and then to divide the entire word sequence into individual sentences. Empirical methods are commonly employed to deal with this problem. Such methods involve many different sequence labeling models including HMMs (Shriberg et al., 2000), maximum entropy (Maxent) models (Liu et al., 2004), and CRFs (Liu et al., 2005). Among these, a CRF model used in Liu et al (2005) offered the lowest error rate. Chinese word segmentation is a problem closely related to Classical Chinese sentence segmentation. The former identifies the boundaries of the words in a given text, while the latter identifies the boundaries of the sentences, clauses, and phrases. In contrast to sentences and clauses, the length of Chinese words is shorter, and the variety of Chinese words is more limited. Despite the minor unknown words, most of the frequent words</context>
</contexts>
<marker>Shriberg, Stolcke, HakkaniTür, Tür, 2000</marker>
<rawString>Shriberg, Elizabeth, Andreas Stolcke, Dilek HakkaniTür, and Gökhan Tür. 2000. Prosody-Based Automatic Segmentation of Speech into Sentences and Topics. Speech Communication, 32(1-2):127-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V N Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Mark W Schmidt</author>
<author>Kevin P Murphy</author>
</authors>
<title>Accelerated training of conditional random fields with stochastic gradient methods.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23th International Conference on Machine Learning, 969–976.</booktitle>
<publisher>ACM Press,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="10443" citStr="Vishwanathan, et al., 2006" startWordPosition="1646" endWordPosition="1649">bined the Xue’s tagging scheme (i.e., 4-tag set) and CRFs to address the Classical Chinese sentence segmentation problem and reported an F-score of 80.96% averaged over various datasets. A similar work by Zhang et al. reported an F-score of 71.42%. 4 Methods Conditional random field is our tagging model, and the implementation is CrfSgd 1.31 provided by Léon Bottou. As denoted by the tool name, the parameters in this implementation are optimized using Stochastic Gradient Descent (SGD) which convergences much faster than the common optimization algorithms such as L-BFGS and conjugate gradient (Vishwanathan, et al., 2006). To construct the sentence segmenter on 1 http://leon.bottou.org/projects/sgd CRF, the tagging scheme and the feature functions play the crucial roles. 4.1 Tagging Schemes In the previous works (Huang, 2008; Zhang et al., 2009), POC tags used in Chinese word segmentation (Xue, 2003) are converted to denote the positions of characters within a clause. The 4-tag set is redefined as L (“the left boundary of a clause”), R (“the right boundary of a clause”), M (“the middle character of a clause”), and S (“a single character forming a clause”). For example, the sentence “北冥有魚其名為鯤鯤之大不知幾 千里也” should </context>
</contexts>
<marker>Vishwanathan, Schraudolph, Schmidt, Murphy, 2006</marker>
<rawString>Vishwanathan, S. V. N., Nicol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In Proceedings of the 23th International Conference on Machine Learning, 969–976. ACM Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese Word Segmentation as Character Tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="8528" citStr="Xue, 2003" startWordPosition="1331" endWordPosition="1332">s can be handled with a dictionary predefined by Chinese language experts or extracted from the corpus automatically. However, it is impossible to maintain a dictionary of the infinite number of sentences and clauses. For these reasons, the Classical Chinese sentence segmentation problem is more challenging. Methods of Chinese word segmentation can be mainly classified into heuristic rule-based approaches, statistical machine learning approaches, and hybrid approaches. Hybrid approaches combine the advantages of heuristic and statistical approaches to achieve better results (Gao et al., 2003; Xue, 2003; Peng et al., 2004). Xue (2003) transformed the Chinese word segmentation problem into a tagging problem. For a given sequence of Chinese characters, the author applies a Maxent tagger to assign each character one of four positions-of-character (POC) tags, and then coverts the tagged sequence into a segmented sequence. The four POC tags used in Xue (2003) denote the positions of characters within a word. For example, the first character of a word is tagged “left boundary”, the last character of a word is tagged “right boundary”, the middle character of a word is tagged “middle”, and a single </context>
<context position="10727" citStr="Xue, 2003" startWordPosition="1692" endWordPosition="1693">del, and the implementation is CrfSgd 1.31 provided by Léon Bottou. As denoted by the tool name, the parameters in this implementation are optimized using Stochastic Gradient Descent (SGD) which convergences much faster than the common optimization algorithms such as L-BFGS and conjugate gradient (Vishwanathan, et al., 2006). To construct the sentence segmenter on 1 http://leon.bottou.org/projects/sgd CRF, the tagging scheme and the feature functions play the crucial roles. 4.1 Tagging Schemes In the previous works (Huang, 2008; Zhang et al., 2009), POC tags used in Chinese word segmentation (Xue, 2003) are converted to denote the positions of characters within a clause. The 4-tag set is redefined as L (“the left boundary of a clause”), R (“the right boundary of a clause”), M (“the middle character of a clause”), and S (“a single character forming a clause”). For example, the sentence “北冥有魚其名為鯤鯤之大不知幾 千里也” should be tagged as follows. 北/L 冥/M 有/M 魚/R 其/L 名/M 為/M 鯤/R 鯤 /L 之/M 大/R 不/L 知/M 幾/M 千/M 里/M 也/R We can easily split the sentence into clauses by making a break after each character tagged R and S and obtain the final outcome “北冥有魚 / 其名為鯤 / 鯤之大 / 不知幾千里也”. In this work, more tagging schemes</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, Nianwen. 2003. Chinese Word Segmentation as Character Tagging. Computational Linguistics and Chinese Language Processing, 8(1):29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hel Zhang</author>
<author>Wang Xiao-dong</author>
<author>Yang Jian-yu</author>
<author>Zhou Wei-dong</author>
</authors>
<title>Method of Sentence Segmentation and Punctuating for Ancient Chinese.</title>
<date>2009</date>
<journal>Application Research of Computers,</journal>
<pages>26--9</pages>
<contexts>
<context position="9805" citStr="Zhang et al. (2009)" startWordPosition="1545" endWordPosition="1548">racter-word”. Once the given sequence is tagged, the boundaries of words are also revealed, and the task of segmentation becomes straightforward. However, the Maxent models used in Xue (2003) suffer from an inherent the label bias problem. Peng et al (2004) uses the CRFs to address this issue. The tags used in Peng et al (2004) are of only two types, “start” and “non-start”, in which the “start” tag denotes the first character of a word, and the characters in other positions are given the “non-start” tag. The closest previous works to Classic Chinese sentence segmentation are Huang (2008) and Zhang et al. (2009). Huang combined the Xue’s tagging scheme (i.e., 4-tag set) and CRFs to address the Classical Chinese sentence segmentation problem and reported an F-score of 80.96% averaged over various datasets. A similar work by Zhang et al. reported an F-score of 71.42%. 4 Methods Conditional random field is our tagging model, and the implementation is CrfSgd 1.31 provided by Léon Bottou. As denoted by the tool name, the parameters in this implementation are optimized using Stochastic Gradient Descent (SGD) which convergences much faster than the common optimization algorithms such as L-BFGS and conjugate</context>
<context position="11990" citStr="Zhang et al., 2009" startWordPosition="1923" endWordPosition="1926">or segmentation is 2-tag set in which only two types of tags, “start” and “non-start”, are used to label the sequence. The segmented fragments (clauses) for sentence segmentation are usually much longer than those for word segmentation. Thus, we add more middle states into the 4-tag set to model the nature of long fragments. The Markov chain of our tagging scheme is shown in Figure 2, where L2, L3, ..., Lk are the additional states to extend Xue’s 4-tag set. In our experiments, various k values are tested. If the k value is 1, the scheme is identical to the one used in the two previous works (Zhang et al., 2009; Huang, 2008). The 2-tag set, 4-tag set, 5-tag set and their corresponding examples are listed in Table 1. With the tagging scheme, the Classical Chinese sentence segmentation task is transformed into a sequence labeling or tagging task. 4.2 Features Due to the flexibility of the feature function interface provided by CRFs, we apply various feature conjunctions. Besides the n-gram character patterns, the phonetic information and the partFigure 2. Markov Chain of Our Tagging Scheme. Tag set Tags Example 2-tag S: Start 不知其幾千里也 N: Non-Start 不知其幾千里也 4-tag L1: Left-end 不知其幾千里也 (k=1) M: Middle 不知其幾</context>
</contexts>
<marker>Zhang, Xiao-dong, Jian-yu, Wei-dong, 2009</marker>
<rawString>Zhang, Hel, Wang Xiao-dong, Yang Jian-yu, and Zhou Wei-dong. 2009. Method of Sentence Segmentation and Punctuating for Ancient Chinese. Application Research of Computers, 26(9):3326-3329.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>