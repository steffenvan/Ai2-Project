<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048090">
<title confidence="0.996959">
A Text Categorization Based on Summarization Technique
</title>
<author confidence="0.992192">
Sue J. Ker Jen-Nan Chen
</author>
<affiliation confidence="0.904083333333333">
Department of Computer Science, Department of Management,
Soochow University Ming Chuan University
Taipei 100, Taiwan, Taipei 111, Taiwan,
</affiliation>
<email confidence="0.987846">
ksj@cis.scu.edu.tw jnchen@mcu.edu.tw
</email>
<sectionHeader confidence="0.99725" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835555555556">
We propose a new approach to text
categorization based upon the ideas of
summarization. It combines word-based
frequency and position method to get
categorization knowledge from the title field
only. Experimental results indicate that
summarization-based categorization can
achieve acceptable performance on Reuters
news corpus.
</bodyText>
<sectionHeader confidence="0.975497" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999947117647059">
With the current explosive growth of Internet
usage, the demand for fast and useful access to
online data is increasing. An efficient
categorization system should provide accurate
information quickly. There are many
applications for text categorization, including
information retrieval, text routing, text filtering
and text understanding systems.
The text categorization systems use
predefined categories to label new documents.
Many different approaches have been applied to
this task, including nearest neighbor classifiers
(Masand, Linoff and Waltz, 1992; Yang, 1994;
Lam and Ho, 1998; Yang, 1999), Bayesian
independence classifiers (Lewis and Ringuette,
1994; Baker and McCallum, 1998; McCallum
and Nigam, 1998), decision trees (Fuhr et al.,
1991; Lewis and Ringuette, 1994; Apte et al.,
1998), induction rule learning (Apte et al., 1994;
Cohen and Singer, 1996; Mouilinier et al., 1996),
neural networks (Wiener, Pedersen and Weigend,
1995; Ng, Goh and Low, 1997), and support
vector machines (Joachims, 1998). These
categorization algorithms have been applied to
many different subject domains, usually news
stories (Apte et al., 1994; Lewis and Ringuette,
1994; Wiener, Pedersen and Weigend, 1995;
Yang, 1999), but also physics abstracts (Fuhr et
al., 1991), and medical texts (Yang and Chute,
1994).
In this research to resolve the task of text
categorization we apply a method of text
summarization, that is, combining word-based
frequency and position method to get
categorization knowledge from the title field
only. Experimental results indicate that
summarization-based categorization can achieve
acceptable performance on Reuters news corpus.
Additionally, the computation time for the title
field is very short. Thus, this system is
appropriate for online document classifier.
Following is a description of the organization
of this paper. Section 2 describes the previous
work of summarization. Summarization-based
algorithms for text categorization are outlined in
Section 3. The experiments we undertook to
assess the performance of these algorithms are
the topic of Section 4. Quantitative
experimental results are also summarized.
Finally, concluding remarks and
recommendation for future work is made.
</bodyText>
<sectionHeader confidence="0.963949" genericHeader="method">
1 Text Summarization
</sectionHeader>
<bodyText confidence="0.999846176470588">
The task of summarization is to identify
informative evidence from a given document,
which are most relevant to its content and create
a shorter version of summary of the document
from this information. The informative
evidence associated with techniques used in
summarization may also provide clues for text
categorization to determine the appropriate
category of the document.
Several techniques for text summarization
have been reported in the literature, including
methods based on position (Edmundson, 1969;
Hovy and Lin, 1997; Teufel and Moens, 1997),
cue phrase (McKeown and Radev, 1995;
Mahesh, 1997), word frequency (Teufel and
Moens, 1997), and discourse segmentation
(Boguraev and Kennedy, 1997).
</bodyText>
<page confidence="0.997356">
79
</page>
<bodyText confidence="0.999953478260869">
Of the above approaches, both word
frequency and position methods are easy to
implement. In this research we combine these
two approaches to investigate the efforts for
categorization. In regard to the position
method, Hovy and Lin (1997) considered the
title is the most likely to bear topics. They
claim words in titles are positively relevant to
summarization. Teufel and Moens (1997) also
confirmed this viewpoint; they mentioned that
words in the title are good candidates for
document specific concepts. They showed
21.7% recall and precision, when the title
method is used alone, with an increased
performance of 3%, when combined with other
methods.
Furthermore, from observation of the TREC
evaluation during recent years, it has been
shown that there is no significant difference
between short and long query. It seems
reasonable to acquire informative clues from the
title, still not degrading the categorization
performance severely.
</bodyText>
<sectionHeader confidence="0.993034" genericHeader="method">
2 Methods
</sectionHeader>
<bodyText confidence="0.999826">
This section describes a series of algorithms
based on the title summarization technique for
text categorization.
</bodyText>
<subsectionHeader confidence="0.998051">
2.1 Preprocessing and Feature Selection
</subsectionHeader>
<bodyText confidence="0.9999684">
We divide the corpus texts into words, delineate
by white space and punctuation. All characters
are lower-case and stop words are removed.
After the words are stemmed, we call them
terms. These terms are then used as features.
</bodyText>
<subsectionHeader confidence="0.999235">
2.2 Term Weighting
</subsectionHeader>
<bodyText confidence="0.970642769230769">
Weights are now assigned to the surviving
features in each category. We design several
different formulas for term weighting. In each
formula, we associate a weight, W(f c), with
each surviving feature, f, in category c, in the
same way weights can be obtained in
information retrieval when assigning them to
index terms. In addition, we normalize the
value of term frequency, y, between categories.
The probability of category is also taken into
account. We define W(f c) as equations 1
through 3. .
W(f , c) 4 f (Eq. 1-a)
</bodyText>
<figure confidence="0.949933555555555">
Max,
f
W(f,c). p(c)xâ€”xidf
Max,
W(f,c)= p(c)xf.cxidf f
W(f,c)= ffCxidff
idf f = df f
N,
p(c)=
</figure>
<bodyText confidence="0.975922555555556">
where C= the frequency of the feature
appearing in the category c,
T= the number of categories,
dff= the number of categories that
contain the feature f,
Max, = the maximum frequency of any
feature in category c,
Arc= the document numbers belonging
category c in training sets.
</bodyText>
<subsectionHeader confidence="0.998718">
2.3 Category Ranking
</subsectionHeader>
<bodyText confidence="0.999980222222222">
We now have an index suitable for use in the
category ranking process. The index contains
features and a weighted value, W(f, c),
associated with each feature f in each category c.
Given a document, d, a rank can be associated
with each category with respect to d. Let F, is
the set of features, f, in category c. The ranking
of category c with respect to document d,R(c, d),
is defined as equation 4.
</bodyText>
<equation confidence="0.6040055">
R(c, d)= x W(f, ,c) (Eq. 4)
feF,nd
</equation>
<bodyText confidence="0.999986">
where Ci= the frequency of the feature f
appearing in the document d,
Fc= the set of features f in category c.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9994365">
To assess the proposed method&apos;s effectiveness,
we apply the algorithms described in the
previous section and conduct a series of
experiments. Tests are performed on the
Reuters corpus. A general description of the
materials used in these experiments follows.
Finally, the success rates are quantitatively
evaluated.
</bodyText>
<subsectionHeader confidence="0.999431">
3.1 The Reuters Corpus
</subsectionHeader>
<bodyText confidence="0.999287">
To make our effectiveness comparable to other
researchers&apos; results in text categorization, we
chose the commonly used Reuters news story
corpus for the data. This corpus has many
different versions. Yang (1999) points out
</bodyText>
<equation confidence="0.9914956">
(Eq. 1-b)
(Eq. 1-c)
(Eq. 1-d)
(Eq. 2)
(Eq. 3)
</equation>
<page confidence="0.904149">
80
</page>
<bodyText confidence="0.998652333333333">
there are at least five versions of the Reuters
corpus, depending on how the training/test sets
are divided and the scope of categories or
documents used for evaluation. In this paper,
we select the Reuters version 3 (a formatted
version is currently available at Yang&apos;s
homepage http://moscow.mt.cs.cmu.edu:
8081/reuters_21450/apte), constructed by Apte
et al., as our data set.
This version contains 7,789 training and 3,309
test documents within 93 categories. The
distribution of category number is tabulated in
Table 1. Most of these documents have only a
single category, but some documents are
multicategory. The average numbers of
categories per document are 1.23 and 1.24 on
training and test sets, respectively. The number
of training documents per category varies widely,
from 2 (dfl, fishmeal, ..., etc.) to a maximum of
2,877 (earn). Tables 2 and 3 show the top ten
most frequent categories and ten least frequent
categories on the training sets. The average
length of title field and whole document are 7.4
and 126.9 words per document, respectively.
</bodyText>
<subsectionHeader confidence="0.993038">
3.2 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999751923076923">
In this paper, we only use TITLE field as the
scope of texts. In our first experiment, the
variable is variant term weighting formulas that
are described in Section 3. We want to see the
effects on categorization performance, when
probability of category and normalized process
of term frequency are used. The first
experiment is summarized in Table 4.
A second experiment is to locate the most
preferred threshold value of minimum term
frequency. For the number of features in our
experiment, the values 10, 20, 50, 100, 150, 200,
300 and 900 are tested.
</bodyText>
<subsectionHeader confidence="0.953478">
3.3 Experimental Results and
</subsectionHeader>
<sectionHeader confidence="0.947115" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.999473111111111">
We survey the effectiveness of our algorithms
by using the conventional 11-point average
precision (Salton and McGill, 1983; Yang
1999).
We first investigate a suitable term weighted
formula by doing a set of initial categorization
from Method 1 through 4. Threshold of
minimum term frequency is fixed at 3. The
results are tabulated in Table 5. It can be seen
</bodyText>
<tableCaption confidence="0.984194">
Table 1 The distribution of category number on
corpus.
</tableCaption>
<table confidence="0.999125777777778">
Category Training sets Test sets
No.
Doc # Percentage Doc # Percentage
1 6586 84.6% 2823 85.3%
2 878 11.3% 347 10.5%
3 188 2.4% 65 2.0%
4 61 0.8% 36 1.1%
5 39 0.5% 21 0.6%
Above 5 37 0.5% 17 0.5%
</table>
<tableCaption confidence="0.9908555">
Table 2 The ten most frequent categories in the
training sets.
</tableCaption>
<table confidence="0.999823615384615">
Topic Document No.
Name
Training sets Test sets
earn 2877 1176
acq 1651 776
money-fx 538 207
grain 433 168
crude 388 197
trade 369 135
interest 347 150
wheat 212 81
ship 198 92
corn 176 64
</table>
<tableCaption confidence="0.9875625">
Table 3 The ten least frequent categories in the
traininn sets.
</tableCaption>
<table confidence="0.922198">
Topic Document No.
Name
Training sets Test sets
</table>
<figureCaption confidence="0.9798233">
comglutenfeed 2 0
dfl 2 1
fishmeal 2 0
linseed 2 0
naphtha 2 4
nzdlr 2 1
palladium 2 1
palmkemel 2 1
rand 2 1
wool 2 0
</figureCaption>
<bodyText confidence="0.946221111111111">
Table 4 The choice of term-weighting formulas
in the first experiment.
Method Id. 1 2 3 4
Formula Id. 1-a 1-b 1-c 1-d
Prob. used v v x x
Max, used v x v x
that Method 4 appears to perform well in our
measure. The average 11-point evaluation can
achieve 82.7% precision for Method 4 (tfxidf).
</bodyText>
<page confidence="0.996929">
81
</page>
<bodyText confidence="0.998161782608695">
It seems to point out that small text size (only
TITLE field is used) is not bad for text
categorization, when compared with kNN&apos;s 93%
and LLSF&apos;s 92% for full texts (Yang, 1999).
The other experimental variable is the number
of chosen features. Table 5 shows the large
feature sets earn the better result when
probability is absent.
In the next experiment, with the term
weighting formula fixed at Eq. 1-d (Method 4),
we vary the minimum number of term frequency
from 1 to 3. Table 6 indicates that there are no
significant differences among judgements, but
shows a little improvement for those small
threshold values. The data also shows that the
information contained in the title field is almost
come together and very little noise. Thus, it
seems to have no effects for the processing of
sparse data.
Table 5 The 11-point average precision scores
of the first experiment. For the
minimum term frequency, value 3 was
used.
</bodyText>
<table confidence="0.999403">
Feature Method Id.
No.
1 2 3 4
10 71.9% 69.2% 70.1% 72.1%
20 76.3% 74.1% 73.8% 77.1%
50 78.8% 77.4% 74.3% 80.2%
100 80.2% 79.2% 74.5% 81.9%
150 80.1% 79.8% 73.8% 82.4%
200 80.2% 80.2% 73.3% 82.6%
300 80.0% 80.5% 73.0% 82.6%
900 79.6% 80.9% 72.2% 82.7%
</table>
<tableCaption confidence="0.872418">
Table 6 The 11-point average precision scores
of Method 4.
</tableCaption>
<table confidence="0.999932777777778">
Feature # Tf&gt;=3 Tf&gt;=2 Tf &gt;=1
10 72.1% 72.2% 72.3% ,
20 77.1% 77.2% 77.3%
50 80.2% 80.3% 80.5%
100 81.9% 82.1% 82.3%
150 82.4% 82.7% 82.9%
200 82.6% 82.9% 83.1%
300 82.6% 82.9% 83.2%
900 82.7% 83.0% 83.2%
</table>
<sectionHeader confidence="0.818351" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999938272727273">
In this paper, we apply the most popular
methods, in text summarization, position and
word frequency, to resolve the task of text
categorization. We use a word-based term
weighted technique from the title field, which is
informative but short in length, to process
categorization. The results show short title
field will reduce execution time, and provide
acceptable performance. Thus, this system
would be appropriate for an online document
classifier.
Previous work shows the hybrid approach for
the text categorization and summarization is
more efficient than a single scheme. Thus, we
will try to combine several schemes in the future.
In addition, in the position method, we could use
hybrid structure to consider the title and some
specific position in the document, for instance,
the first sentence in the first paragraph or the
first sentence in the second paragraph. When
there is insufficient information in title field, it is
helpful to proceed to the next position.
</bodyText>
<sectionHeader confidence="0.99838" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999891">
This research is partially supported by the ROC
NSC grants 88-2213-E-031-003 and 89-2213-E-
031-004. We would like to thank the anonymous
referees for their valuable comments. Any errors
that remain are solely the responsibility of the
authors.
</bodyText>
<sectionHeader confidence="0.99915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995553086956522">
William J. Hutchins (1986) Machine Translation:
Past, Present, Future. Ellis Horwood, John Wiley
&amp; Sons, Chichester, England, 382 p.
Apte C., Damerau F. and Weiss S. (1994) Towards
language independent automated learning of text
categorization models. In 17th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR &apos;94),
pp. 23-30.
Apte C., Damerau F. and Weiss S. (1998) Text
mining with decision rules and decisions trees. In
Proceedings of the Conference on Automated
Learning and discovery, Workshop 6: Learning
from text and Web.
Baker L.D. and McCallum A.K. (1998)
Distributional clustering of words for text
categorization. In 21th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR &apos;98), pp. 96-103.
Boguraev B. and Kennedy C. (1997) Salience-
based content characterisation of text documents.
In Proceedings of ACL/EACL &apos;97 Workshop on
Intelligent Scalable Text Summarization, pp. 2-9.
</reference>
<page confidence="0.992748">
82
</page>
<reference confidence="0.997934321428572">
Cohen W.W. and Singer Y. (1996) Context â€”
sensitive learning methods for text categorization.
In 19th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR &apos;96), pp. 307-315.
Edmundson H.P. (1969) New methods in automatic
extracting. Journal of ACM, 16(2): 264-285.
Fuhr N., Hartmanna S., Lustig G., Schwantner M.
and Tzeras K. (1991) Air/X â€” a rule-based
multistage indexing systems for large subject fields.
In Proceedings of RIAO &apos;91, pp. 606-623.
Hovy E. and Lin C.Y. (1997) Automated text
summarization in SUMMARIST, In Proceedings
of ACL/EACL &apos;97 Workshop on Intelligent Scalable
Text Summarization, pp. 18-24.
Joachioms Thorsten (1998) Text categorization
with support vector machines: Learning with many
relevant features. In European Conference on
Machine Learning (ECML).
Lam W. and Ho C.Y. (1998) Using a generalized
instance set for automatic text categorization. In
21th Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR &apos;98), pp. 81-89.
Lewis D.D. and Ringuette M. (1994) Comparison
of two learning algorithms for text categorization.
In proceedings of the 3&apos; Annual Symposium on
Document Analysis and Information Retrieval
(SDAIR &apos;94), pp. 81-93.
Mahesh K. (1997) Hypertext summary extraction
for fast document browsing. In Proceedings of
AAAI Spring Symposium: NLP for WWW, pp. 95-
104 .
Masand M., Linoff G. and Waltz D. (1992)
Classifying news stories using memory based
reasoning. In 15th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval (SIGIR &apos;92), pp. 59-64.
McCallum A. and Nigam K. (1998) A comparison
of event models for Naive Bayes text
categorization. In AAAI-98 Workshop on Learning
for Text Categorization.
McKeown K. and Radev D. (1995) In 18&amp;quot; Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR &apos;95), pp. 74-82.
Mouilinier I., Raskinis G. and Ganascia J. (1996)
Text categorization: A symbolic approach. In
proceedings of the 5th Annual Symposium on
Document Analysis and Information Retrieval
(SDAIR &apos;96).
Ng, H.T., Goh W.B. and Low K.L. (1997) Feature
selection, perceptron learning, and a usability case
study for text categorization. In 20th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR &apos;97), pp. 67-73.
Salton G. and McGill M.J. (1983) Introduction to
modern information retrieval. McGraw-Hill
Computer Science Series. McGraw-Hill, New
York.
Teufel S. and Moens M. (1997) Sentence extraction
as a classification task, In Proceedings of
ACL/EACL &apos;97 Workshop on Intelligent Scalable
Text Summarization, pp. 58-65.
Wiener E., Pedersen 1.0. and Weigend A.S. (1995)
A neural network approach to topic spotting. In
proceedings of the 4&amp;quot; Annual Symposium on
Document Analysis and Information Retrieval
(SDAIR &apos;95).
Yang Y. (1994) Expert network: Effective and
efficient learning from human decision in text
categorization and retrieval. In I 7th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR &apos;94), pp. 13-22.
Yang Y. (1999) An evaluation of statistical
approaches to text categorization, Information
Retrieval. Vol. 1, pp. 69-90.
Yang Y. and Chute C.G. (1994) An application of
expert network to clinical classification and
MEDLINE indexing. In Proceedings of the 18th
Annual Symposium on Computer Applications in
Medical Care, pp. 157-161.
</reference>
<page confidence="0.999312">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924380">
<title confidence="0.999773">A Text Categorization Based on Summarization Technique</title>
<author confidence="0.999864">J Ker Chen</author>
<affiliation confidence="0.9998675">Department of Computer Science, Department of Management, Soochow University Ming Chuan University</affiliation>
<address confidence="0.995651">Taipei 100, Taiwan, Taipei 111, Taiwan,</address>
<email confidence="0.968589">ksj@cis.scu.edu.twjnchen@mcu.edu.tw</email>
<abstract confidence="0.9956589">We propose a new approach to text categorization based upon the ideas of summarization. It combines word-based frequency and position method to get categorization knowledge from the title field only. Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William J Hutchins</author>
</authors>
<title>Machine Translation: Past, Present, Future. Ellis Horwood,</title>
<date>1986</date>
<volume>382</volume>
<pages>p.</pages>
<publisher>John Wiley &amp; Sons,</publisher>
<location>Chichester, England,</location>
<marker>Hutchins, 1986</marker>
<rawString>William J. Hutchins (1986) Machine Translation: Past, Present, Future. Ellis Horwood, John Wiley &amp; Sons, Chichester, England, 382 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Apte</author>
<author>F Damerau</author>
<author>S Weiss</author>
</authors>
<title>Towards language independent automated learning of text categorization models.</title>
<date>1994</date>
<booktitle>In 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94),</booktitle>
<pages>23--30</pages>
<contexts>
<context position="1448" citStr="Apte et al., 1994" startWordPosition="194" endWordPosition="197">ons for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text summarization, that is, combining word-based fr</context>
</contexts>
<marker>Apte, Damerau, Weiss, 1994</marker>
<rawString>Apte C., Damerau F. and Weiss S. (1994) Towards language independent automated learning of text categorization models. In 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94), pp. 23-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Apte</author>
<author>F Damerau</author>
<author>S Weiss</author>
</authors>
<title>Text mining with decision rules and decisions trees.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference on Automated Learning and discovery, Workshop 6: Learning from text and Web.</booktitle>
<contexts>
<context position="1404" citStr="Apte et al., 1998" startWordPosition="187" endWordPosition="190">information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text sum</context>
</contexts>
<marker>Apte, Damerau, Weiss, 1998</marker>
<rawString>Apte C., Damerau F. and Weiss S. (1998) Text mining with decision rules and decisions trees. In Proceedings of the Conference on Automated Learning and discovery, Workshop 6: Learning from text and Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L D Baker</author>
<author>A K McCallum</author>
</authors>
<title>Distributional clustering of words for text categorization.</title>
<date>1998</date>
<booktitle>In 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98),</booktitle>
<pages>96--103</pages>
<contexts>
<context position="1295" citStr="Baker and McCallum, 1998" startWordPosition="169" endWordPosition="172">or fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yan</context>
</contexts>
<marker>Baker, McCallum, 1998</marker>
<rawString>Baker L.D. and McCallum A.K. (1998) Distributional clustering of words for text categorization. In 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98), pp. 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>C Kennedy</author>
</authors>
<title>Saliencebased content characterisation of text documents.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>2--9</pages>
<contexts>
<context position="3556" citStr="Boguraev and Kennedy, 1997" startWordPosition="500" endWordPosition="503">ment, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirmed this viewpoint; they mentioned that words in the title are good candidates for document specific concepts. They showed 21.7% recall and precision, when the title method is used alone, with an </context>
</contexts>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>Boguraev B. and Kennedy C. (1997) Saliencebased content characterisation of text documents. In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization, pp. 2-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>Y Singer</author>
</authors>
<title>Context â€” sensitive learning methods for text categorization.</title>
<date>1996</date>
<booktitle>In 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;96),</booktitle>
<pages>307--315</pages>
<contexts>
<context position="1472" citStr="Cohen and Singer, 1996" startWordPosition="198" endWordPosition="201">rization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text summarization, that is, combining word-based frequency and position met</context>
</contexts>
<marker>Cohen, Singer, 1996</marker>
<rawString>Cohen W.W. and Singer Y. (1996) Context â€” sensitive learning methods for text categorization. In 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;96), pp. 307-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>264--285</pages>
<contexts>
<context position="3361" citStr="Edmundson, 1969" startWordPosition="473" endWordPosition="474">ummarized. Finally, concluding remarks and recommendation for future work is made. 1 Text Summarization The task of summarization is to identify informative evidence from a given document, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirm</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson H.P. (1969) New methods in automatic extracting. Journal of ACM, 16(2): 264-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
<author>S Hartmanna</author>
<author>G Lustig</author>
<author>M Schwantner</author>
<author>K Tzeras</author>
</authors>
<title>Air/X â€” a rule-based multistage indexing systems for large subject fields.</title>
<date>1991</date>
<booktitle>In Proceedings of RIAO &apos;91,</booktitle>
<pages>606--623</pages>
<contexts>
<context position="1357" citStr="Fuhr et al., 1991" startWordPosition="179" endWordPosition="182">categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of te</context>
</contexts>
<marker>Fuhr, Hartmanna, Lustig, Schwantner, Tzeras, 1991</marker>
<rawString>Fuhr N., Hartmanna S., Lustig G., Schwantner M. and Tzeras K. (1991) Air/X â€” a rule-based multistage indexing systems for large subject fields. In Proceedings of RIAO &apos;91, pp. 606-623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
</authors>
<title>Automated text summarization in SUMMARIST,</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>18--24</pages>
<contexts>
<context position="3381" citStr="Hovy and Lin, 1997" startWordPosition="475" endWordPosition="478">y, concluding remarks and recommendation for future work is made. 1 Text Summarization The task of summarization is to identify informative evidence from a given document, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirmed this viewpoint; t</context>
</contexts>
<marker>Hovy, Lin, 1997</marker>
<rawString>Hovy E. and Lin C.Y. (1997) Automated text summarization in SUMMARIST, In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization, pp. 18-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachioms Thorsten</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In European Conference on Machine Learning (ECML).</booktitle>
<marker>Thorsten, 1998</marker>
<rawString>Joachioms Thorsten (1998) Text categorization with support vector machines: Learning with many relevant features. In European Conference on Machine Learning (ECML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lam</author>
<author>C Y Ho</author>
</authors>
<title>Using a generalized instance set for automatic text categorization.</title>
<date>1998</date>
<booktitle>In 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98),</booktitle>
<pages>81--89</pages>
<contexts>
<context position="1194" citStr="Lam and Ho, 1998" startWordPosition="156" endWordPosition="159">s news corpus. Introduction With the current explosive growth of Internet usage, the demand for fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen a</context>
</contexts>
<marker>Lam, Ho, 1998</marker>
<rawString>Lam W. and Ho C.Y. (1998) Using a generalized instance set for automatic text categorization. In 21th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;98), pp. 81-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>M Ringuette</author>
</authors>
<title>Comparison of two learning algorithms for text categorization.</title>
<date>1994</date>
<booktitle>In proceedings of the 3&apos; Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;94),</booktitle>
<pages>81--93</pages>
<contexts>
<context position="1269" citStr="Lewis and Ringuette, 1994" startWordPosition="165" endWordPosition="168">nternet usage, the demand for fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 199</context>
</contexts>
<marker>Lewis, Ringuette, 1994</marker>
<rawString>Lewis D.D. and Ringuette M. (1994) Comparison of two learning algorithms for text categorization. In proceedings of the 3&apos; Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;94), pp. 81-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mahesh</author>
</authors>
<title>Hypertext summary extraction for fast document browsing.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI Spring Symposium: NLP for WWW,</booktitle>
<pages>95--104</pages>
<contexts>
<context position="3458" citStr="Mahesh, 1997" startWordPosition="489" endWordPosition="490">tion The task of summarization is to identify informative evidence from a given document, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirmed this viewpoint; they mentioned that words in the title are good candidates for document specif</context>
</contexts>
<marker>Mahesh, 1997</marker>
<rawString>Mahesh K. (1997) Hypertext summary extraction for fast document browsing. In Proceedings of AAAI Spring Symposium: NLP for WWW, pp. 95-104 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Masand</author>
<author>G Linoff</author>
<author>D Waltz</author>
</authors>
<title>Classifying news stories using memory based reasoning.</title>
<date>1992</date>
<booktitle>In 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;92),</booktitle>
<pages>59--64</pages>
<marker>Masand, Linoff, Waltz, 1992</marker>
<rawString>Masand M., Linoff G. and Waltz D. (1992) Classifying news stories using memory based reasoning. In 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;92), pp. 59-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for Naive Bayes text categorization.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Learning for Text Categorization.</booktitle>
<contexts>
<context position="1322" citStr="McCallum and Nigam, 1998" startWordPosition="173" endWordPosition="176">to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>McCallum A. and Nigam K. (1998) A comparison of event models for Naive Bayes text categorization. In AAAI-98 Workshop on Learning for Text Categorization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>D Radev</author>
</authors>
<date>1995</date>
<booktitle>In 18&amp;quot; Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;95),</booktitle>
<pages>74--82</pages>
<contexts>
<context position="3443" citStr="McKeown and Radev, 1995" startWordPosition="485" endWordPosition="488">is made. 1 Text Summarization The task of summarization is to identify informative evidence from a given document, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirmed this viewpoint; they mentioned that words in the title are good candidates for </context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown K. and Radev D. (1995) In 18&amp;quot; Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;95), pp. 74-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mouilinier</author>
<author>G Raskinis</author>
<author>J Ganascia</author>
</authors>
<title>Text categorization: A symbolic approach.</title>
<date>1996</date>
<booktitle>In proceedings of the 5th Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;96).</booktitle>
<contexts>
<context position="1498" citStr="Mouilinier et al., 1996" startWordPosition="202" endWordPosition="205">rmation retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text summarization, that is, combining word-based frequency and position method to get categorization </context>
</contexts>
<marker>Mouilinier, Raskinis, Ganascia, 1996</marker>
<rawString>Mouilinier I., Raskinis G. and Ganascia J. (1996) Text categorization: A symbolic approach. In proceedings of the 5th Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>W B Goh</author>
<author>K L Low</author>
</authors>
<title>Feature selection, perceptron learning, and a usability case study for text categorization.</title>
<date>1997</date>
<booktitle>In 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;97),</booktitle>
<pages>67--73</pages>
<marker>Ng, Goh, Low, 1997</marker>
<rawString>Ng, H.T., Goh W.B. and Low K.L. (1997) Feature selection, perceptron learning, and a usability case study for text categorization. In 20th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;97), pp. 67-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<booktitle>McGraw-Hill Computer Science Series.</booktitle>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="8813" citStr="Salton and McGill, 1983" startWordPosition="1361" endWordPosition="1364">ble is variant term weighting formulas that are described in Section 3. We want to see the effects on categorization performance, when probability of category and normalized process of term frequency are used. The first experiment is summarized in Table 4. A second experiment is to locate the most preferred threshold value of minimum term frequency. For the number of features in our experiment, the values 10, 20, 50, 100, 150, 200, 300 and 900 are tested. 3.3 Experimental Results and Discussion We survey the effectiveness of our algorithms by using the conventional 11-point average precision (Salton and McGill, 1983; Yang 1999). We first investigate a suitable term weighted formula by doing a set of initial categorization from Method 1 through 4. Threshold of minimum term frequency is fixed at 3. The results are tabulated in Table 5. It can be seen Table 1 The distribution of category number on corpus. Category Training sets Test sets No. Doc # Percentage Doc # Percentage 1 6586 84.6% 2823 85.3% 2 878 11.3% 347 10.5% 3 188 2.4% 65 2.0% 4 61 0.8% 36 1.1% 5 39 0.5% 21 0.6% Above 5 37 0.5% 17 0.5% Table 2 The ten most frequent categories in the training sets. Topic Document No. Name Training sets Test sets </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton G. and McGill M.J. (1983) Introduction to modern information retrieval. McGraw-Hill Computer Science Series. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Sentence extraction as a classification task,</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="3406" citStr="Teufel and Moens, 1997" startWordPosition="479" endWordPosition="482">s and recommendation for future work is made. 1 Text Summarization The task of summarization is to identify informative evidence from a given document, which are most relevant to its content and create a shorter version of summary of the document from this information. The informative evidence associated with techniques used in summarization may also provide clues for text categorization to determine the appropriate category of the document. Several techniques for text summarization have been reported in the literature, including methods based on position (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997), cue phrase (McKeown and Radev, 1995; Mahesh, 1997), word frequency (Teufel and Moens, 1997), and discourse segmentation (Boguraev and Kennedy, 1997). 79 Of the above approaches, both word frequency and position methods are easy to implement. In this research we combine these two approaches to investigate the efforts for categorization. In regard to the position method, Hovy and Lin (1997) considered the title is the most likely to bear topics. They claim words in titles are positively relevant to summarization. Teufel and Moens (1997) also confirmed this viewpoint; they mentioned that words </context>
</contexts>
<marker>Teufel, Moens, 1997</marker>
<rawString>Teufel S. and Moens M. (1997) Sentence extraction as a classification task, In Proceedings of ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization, pp. 58-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wiener</author>
<author>Pedersen</author>
</authors>
<title>A neural network approach to topic spotting.</title>
<date>1995</date>
<booktitle>In proceedings of the 4&amp;quot; Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;95).</booktitle>
<marker>Wiener, Pedersen, 1995</marker>
<rawString>Wiener E., Pedersen 1.0. and Weigend A.S. (1995) A neural network approach to topic spotting. In proceedings of the 4&amp;quot; Annual Symposium on Document Analysis and Information Retrieval (SDAIR &apos;95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>Expert network: Effective and efficient learning from human decision in text categorization and retrieval.</title>
<date>1994</date>
<booktitle>In I 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94),</booktitle>
<pages>13--22</pages>
<contexts>
<context position="1176" citStr="Yang, 1994" startWordPosition="154" endWordPosition="155">ce on Reuters news corpus. Introduction With the current explosive growth of Internet usage, the demand for fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; </context>
</contexts>
<marker>Yang, 1994</marker>
<rawString>Yang Y. (1994) Expert network: Effective and efficient learning from human decision in text categorization and retrieval. In I 7th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR &apos;94), pp. 13-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization,</title>
<date>1999</date>
<journal>Information Retrieval.</journal>
<volume>1</volume>
<pages>69--90</pages>
<contexts>
<context position="1207" citStr="Yang, 1999" startWordPosition="160" endWordPosition="161">roduction With the current explosive growth of Internet usage, the demand for fast and useful access to online data is increasing. An efficient categorization system should provide accurate information quickly. There are many applications for text categorization, including information retrieval, text routing, text filtering and text understanding systems. The text categorization systems use predefined categories to label new documents. Many different approaches have been applied to this task, including nearest neighbor classifiers (Masand, Linoff and Waltz, 1992; Yang, 1994; Lam and Ho, 1998; Yang, 1999), Bayesian independence classifiers (Lewis and Ringuette, 1994; Baker and McCallum, 1998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1</context>
<context position="6944" citStr="Yang (1999)" startWordPosition="1061" endWordPosition="1062">ent d, Fc= the set of features f in category c. 3 Experiments To assess the proposed method&apos;s effectiveness, we apply the algorithms described in the previous section and conduct a series of experiments. Tests are performed on the Reuters corpus. A general description of the materials used in these experiments follows. Finally, the success rates are quantitatively evaluated. 3.1 The Reuters Corpus To make our effectiveness comparable to other researchers&apos; results in text categorization, we chose the commonly used Reuters news story corpus for the data. This corpus has many different versions. Yang (1999) points out (Eq. 1-b) (Eq. 1-c) (Eq. 1-d) (Eq. 2) (Eq. 3) 80 there are at least five versions of the Reuters corpus, depending on how the training/test sets are divided and the scope of categories or documents used for evaluation. In this paper, we select the Reuters version 3 (a formatted version is currently available at Yang&apos;s homepage http://moscow.mt.cs.cmu.edu: 8081/reuters_21450/apte), constructed by Apte et al., as our data set. This version contains 7,789 training and 3,309 test documents within 93 categories. The distribution of category number is tabulated in Table 1. Most of these </context>
<context position="8825" citStr="Yang 1999" startWordPosition="1365" endWordPosition="1366">ting formulas that are described in Section 3. We want to see the effects on categorization performance, when probability of category and normalized process of term frequency are used. The first experiment is summarized in Table 4. A second experiment is to locate the most preferred threshold value of minimum term frequency. For the number of features in our experiment, the values 10, 20, 50, 100, 150, 200, 300 and 900 are tested. 3.3 Experimental Results and Discussion We survey the effectiveness of our algorithms by using the conventional 11-point average precision (Salton and McGill, 1983; Yang 1999). We first investigate a suitable term weighted formula by doing a set of initial categorization from Method 1 through 4. Threshold of minimum term frequency is fixed at 3. The results are tabulated in Table 5. It can be seen Table 1 The distribution of category number on corpus. Category Training sets Test sets No. Doc # Percentage Doc # Percentage 1 6586 84.6% 2823 85.3% 2 878 11.3% 347 10.5% 3 188 2.4% 65 2.0% 4 61 0.8% 36 1.1% 5 39 0.5% 21 0.6% Above 5 37 0.5% 17 0.5% Table 2 The ten most frequent categories in the training sets. Topic Document No. Name Training sets Test sets earn 2877 11</context>
<context position="10257" citStr="Yang, 1999" startWordPosition="1646" endWordPosition="1647">ning sets Test sets comglutenfeed 2 0 dfl 2 1 fishmeal 2 0 linseed 2 0 naphtha 2 4 nzdlr 2 1 palladium 2 1 palmkemel 2 1 rand 2 1 wool 2 0 Table 4 The choice of term-weighting formulas in the first experiment. Method Id. 1 2 3 4 Formula Id. 1-a 1-b 1-c 1-d Prob. used v v x x Max, used v x v x that Method 4 appears to perform well in our measure. The average 11-point evaluation can achieve 82.7% precision for Method 4 (tfxidf). 81 It seems to point out that small text size (only TITLE field is used) is not bad for text categorization, when compared with kNN&apos;s 93% and LLSF&apos;s 92% for full texts (Yang, 1999). The other experimental variable is the number of chosen features. Table 5 shows the large feature sets earn the better result when probability is absent. In the next experiment, with the term weighting formula fixed at Eq. 1-d (Method 4), we vary the minimum number of term frequency from 1 to 3. Table 6 indicates that there are no significant differences among judgements, but shows a little improvement for those small threshold values. The data also shows that the information contained in the title field is almost come together and very little noise. Thus, it seems to have no effects for the</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yang Y. (1999) An evaluation of statistical approaches to text categorization, Information Retrieval. Vol. 1, pp. 69-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>C G Chute</author>
</authors>
<title>An application of expert network to clinical classification and MEDLINE indexing.</title>
<date>1994</date>
<booktitle>In Proceedings of the 18th Annual Symposium on Computer Applications in Medical Care,</booktitle>
<pages>157--161</pages>
<contexts>
<context position="1913" citStr="Yang and Chute, 1994" startWordPosition="264" endWordPosition="267">998; McCallum and Nigam, 1998), decision trees (Fuhr et al., 1991; Lewis and Ringuette, 1994; Apte et al., 1998), induction rule learning (Apte et al., 1994; Cohen and Singer, 1996; Mouilinier et al., 1996), neural networks (Wiener, Pedersen and Weigend, 1995; Ng, Goh and Low, 1997), and support vector machines (Joachims, 1998). These categorization algorithms have been applied to many different subject domains, usually news stories (Apte et al., 1994; Lewis and Ringuette, 1994; Wiener, Pedersen and Weigend, 1995; Yang, 1999), but also physics abstracts (Fuhr et al., 1991), and medical texts (Yang and Chute, 1994). In this research to resolve the task of text categorization we apply a method of text summarization, that is, combining word-based frequency and position method to get categorization knowledge from the title field only. Experimental results indicate that summarization-based categorization can achieve acceptable performance on Reuters news corpus. Additionally, the computation time for the title field is very short. Thus, this system is appropriate for online document classifier. Following is a description of the organization of this paper. Section 2 describes the previous work of summarizati</context>
</contexts>
<marker>Yang, Chute, 1994</marker>
<rawString>Yang Y. and Chute C.G. (1994) An application of expert network to clinical classification and MEDLINE indexing. In Proceedings of the 18th Annual Symposium on Computer Applications in Medical Care, pp. 157-161.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>