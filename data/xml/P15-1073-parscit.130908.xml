<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001216">
<title confidence="0.988327">
Demographic Factors Improve Classification Performance
</title>
<author confidence="0.998352">
Dirk Hovy
</author>
<affiliation confidence="0.9983505">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.888556">
Njalsgade 140
</address>
<email confidence="0.998126">
dirk@cst.dk
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835142857143">
Extra-linguistic factors influence language
use, and are accounted for by speakers
and listeners. Most natural language pro-
cessing (NLP) tasks to date, however,
treat language as uniform. This assump-
tion can harm performance. We investi-
gate the effect of including demographic
information on performance in a variety
of text-classification tasks. We find that
by including age or gender information,
we consistently and significantly improve
performance over demographic-agnostic
models. These results hold across three
text-classification tasks in five languages.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997619849056604">
When we use language, we take demographic
factors of the speakers into account. In other
words, we do have certain expectations as to who
uses “super cute,” “rather satisfying,” or “rad,
dude.” Sociolinguistics has long since studied the
interplay between demographic factors and lan-
guage use (Labov, 1964; Milroy and Milroy, 1992;
Holmes, 1997; Macaulay, 2001; Macaulay, 2002;
Barbieri, 2008; Wieling et al., 2011; Rickford and
Price, 2013, inter alia).1 These factors greatly in-
fluence word choice, syntax, and even semantics.
In natural language processing (NLP), however,
we have largely ignored demographic factors, and
treated language as a uniform medium. It was ir-
relevant, (and thus not modeled) whether a text
was produced by a middle-aged man, an elderly
lady, or a teenager. These three groups, how-
ever, differ along a whole host of demographic
axes, and these differences are reflected in their
language use.
1Apart from the demographic factors, other factors such
as mood, interpersonal relationship, authority, language atti-
tude, etc. contribute to our perception of language.
A model that is agnostic to demographic dif-
ferences will lose these distinctions, and perfor-
mance suffers whenever the model is applied to a
new demographic. Historically, the demograph-
ics of training and test data (newswire) were rela-
tively homogenous, language was relatively uni-
form, and information the main objective. Un-
der these uniform conditions, the impact of demo-
graphics on performance was small.
Lately, however, NLP is increasingly applied
to other domains, such as social media, where
language is less canonical, demographic informa-
tion about the author is available, and the authors’
goals are no longer purely informational. The in-
fluence of demographic factors in this medium is
thus much stronger than on the data we have tra-
ditionally used to induce models. The resulting
performance drops have often been addressed via
various domain adaptation approaches (Blitzer et
al., 2006; Daume III and Marcu, 2006; Reichart
and Rappoport, 2007; Chen et al., 2009; Daum´e et
al., 2010; Chen et al., 2011; Plank and Moschitti,
2013; Plank et al., 2014; Hovy et al., 2015b, inter
alia). However, the authors and target demograph-
ics of social media differ radically from those in
newswire text, and domain might in some case be
a secondary effect to demographics. In this paper,
we thus ask whether we also need demographic
adaptation.
Concretely, we investigate
</bodyText>
<listItem confidence="0.979151333333333">
1. how we can encode demographic factors, and
2. what effect they have on the performance of
text-classification tasks
</listItem>
<bodyText confidence="0.999615857142857">
We focus on age and gender, and similarly
to Bamman et al. (2014a), we use distributed
word representations (embeddings) conditioned
on these demographic factors (see Section 2.1) to
incorporate the information.
We evaluate the effect of demographic informa-
tion on classification performance in three NLP
</bodyText>
<page confidence="0.953803">
752
</page>
<note confidence="0.977632">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 752–762,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99878975">
tasks: sentiment analysis (Section 2.2), topic de-
tection (Section 2.3), and author attribute classifi-
cation (Section 2.4). 2
We compare Fi-performance of classifiers a)
trained with access to demographic information,
or b) under agnostic conditions. We find that
demographic-aware models consistently outper-
form their agnostic counterparts in all tasks.
</bodyText>
<subsectionHeader confidence="0.640904">
Our contributions
</subsectionHeader>
<bodyText confidence="0.999817428571429">
We investigate the effect of demographic fac-
tors on classification performance. We show that
NLP systems benefit from demographic aware-
ness, i.e., that information about age and gender
can lead to significant performance improvements
in three different NLP tasks across five different
languages.
</bodyText>
<sectionHeader confidence="0.996229" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999928379310345">
We use data from an international user review
website, Trustpilot. It contains information both
about the review (text and star rating), as well as
the reviewer, in form of a profile. The profile in-
cluded a screen name, and potentially information
about gender and birth year.
Since demographic factors are extra-linguistic,
we assume that the same effects hold irrespective
of language. To investigate this hypothesis, we use
data from several languages (Danish, French, and
German) and varieties (American English, British
English).
We use data from the countries with most users,
i.e., Great Britain, Denmark, Germany, France,
and the US. The selection was made based on the
availability of sufficient amounts of training data
(see Table 1 for more details). The high number of
users in Denmark (one tenth of the country’s pop-
ulation) might be due to the fact that Trustpilot is
a Danish company and thus existed there longer
than in other countries. Danish users also provide
(in relative terms) more information about them-
selves than users of any other country, so that even
in absolute numbers, there is oftentimes more in-
formation available than for larger countries like
France or Germany, where users are more reluc-
tant to disclose information.
While most of this profile information is vol-
untary, we have good coverage for both age and
</bodyText>
<footnote confidence="0.99567675">
2We selected these tasks to represent a range of text-
classification applications, and based on the availability of
suitable data with respect to target and demographic vari-
ables.
</footnote>
<table confidence="0.9971215">
USERS AGE GENDER PLACE ALL
UK 1,424k 7% 62% 5% 4%
France 741k 3% 53% 2% 1%
Denmark 671k 23% 87% 17% 16%
US 648k 8% 59% 7% 4%
Germany 329k 8% 47% 6% 4%
</table>
<tableCaption confidence="0.9934145">
Table 1: Number of users and % per variable per
country (after applying augmentations).
</tableCaption>
<bodyText confidence="0.996839585365854">
gender. In case of missing gender values, we base
a guess on the first name (if given), by choosing
the gender most frequently associated with that
name in the particular language. We do require
that one gender is prevalent (accounting for 95%
of all mentions), and that there is enough support
(at least 3 attributed instances), though. For age,
coverage is less dense, so the resulting data sets
are smaller, but still sufficient.
For more information on Trustpilot as a re-
source, see Hovy et al. (2015a).
We split each review into sentences, tokenize,
replace numbers with a 0, lowercase the data, and
join frequent bigrams with an underscore to form
a single token.
For each language, we collect four sub-corpora,
namely two for gender (male and female) and
two for age (under 35 and over 45). The sub-
corpora for the discrete variable gender are rela-
tively straightforward (although see (Bamman et
al., 2014b)), but the split for the continuous age
variable are less clear. While the effect of age on
language use is undisputed (Barke, 2000; Barbieri,
2008; Rickford and Price, 2013), providing a clear
cut-off is hard. We therefore use age ranges that
result in roughly equally sized data sets for both
groups, and that are not contiguous.
For each independent variable (age and gender),
we induce embeddings for the two sub-groups (see
section 2.1), as well as a “mixed” setting. We
also extract labeled data for each task (see sections
2.2, 2.3, and 2.4). Each of these data sets is ran-
domly split into training and test data, 60:40. Note
that we do not set any parameters on development
data, but instead use off-the-shelf software with
default parameters for classification. Table 2 gives
an overview of the number of training and test in-
stances for each task and both variables (gender
and age).
Note that this setup is somewhat artificial: the
vocabulary of the embeddings can subsume the
</bodyText>
<page confidence="0.998615">
753
</page>
<table confidence="0.964769388888889">
GENDER
TASK COUNTRY TRAIN TEST
Denmark 72.48k 48.32k
France 33.34k 22.23k
TOPIC Germany 18.35k 12.23k
UK 110.40k 73.60k
US 36.95k 24.63k
Denmark 150.29k 100.19k
France 40.38k 26.92k
SENTIMENT Germany 17.35k 11.57k
UK 93.98k 62.65k
US 43.36k 28.91k
Denmark 180.31k 120.20k
France 10.69k 7.12k
ATTRIBUTES Germany 11.47k 7.64k
UK 70.87k 47.25k
US 28.10k 18.73k
total 918.32k 612.20k
</table>
<figure confidence="0.985314444444445">
AGE
TRAIN TEST
26.89k 17.93k
3.67k 2.45k
4.82k 3.22k
13.26k 8.84k
7.25k 4.84k
45.18k 30.12k
3.94k 2.63k
3.52k 2.35k
15.80k 10.53k
3.90k 2.60k
180.31k 120.20k
10.69k 7.12k
11.47k 7.64k
70.87k 47.25k
28.10k 18.73k
429.66k 286.43k
</figure>
<tableCaption confidence="0.945941">
Table 2: Number of sentences per task for gender and age as independent variable
</tableCaption>
<bodyText confidence="0.999744888888889">
vocabulary of the tasks (there is some loss due
to frequency cut-offs in word2vec). The out-of-
vocabulary rate on the tasks is thus artificially low
and can inflate results. In a standard “improve-
ment over baseline”-setup, this would be problem-
atic. However, the results should not be interpreted
with respect to their absolute value on the respec-
tive tasks, but with respect to the relative differ-
ences.
</bodyText>
<subsectionHeader confidence="0.981388">
2.1 Conditional Embeddings
</subsectionHeader>
<bodyText confidence="0.817593">
total 880k 4.51m
</bodyText>
<tableCaption confidence="0.745832">
Table 3: Number of sentences used to induce em-
beddings
</tableCaption>
<bodyText confidence="0.996004931034483">
Embeddings are distributed representations of
words in a vector space, capturing syntactic and
semantic regularities among the words. We
learn our word embeddings by using word2vec3
(Mikolov et al., 2013) on unlabeled review data.
Our corpora are relatively small, compared to the
language modeling tasks the tool was developed
for (see Table 3 for the number of instances used
for each language and variable). We thus follow
the suggestions in the word2vec documentation
and use the skip-gram model and hierarchical soft-
max rather than the standard continuous-bag-of-
words model. This setting penalizes low-frequent
words less. All out-of-vocabulary (OOV) words
are replaced with an “unknown” token, which is
represented as the averaged vector over all other
words.
In this paper, we want to use embeddings to
capture group-specific differences. We therefore
train embeddings on each of the sub-corpora
(e.g., male, female, and U35, O45) separately. As
comparison, we create a mixed setting. For each
variable, we combine half of both sub-corpora
(say, men and women) to form a third corpus
with no demographic distinction. We also train
embeddings on this data. This setting assumes
that there are no demographic differences, which
is the common approach in NLP to date.
Since embeddings depend crucially on the
</bodyText>
<figure confidence="0.9474025">
3https://code.google.com/p/word2vec/
576k
70k
US
COUNTRY AGE GENDER
Denmark 495k
1.6m
France
Germany
UK
36k
47k
232k
490k
211k
1.63m
</figure>
<page confidence="0.995644">
754
</page>
<bodyText confidence="0.999992263157895">
size of the available training data, and since we
want to avoid modeling size effects, we balance
the three corpora we use to induce embeddings
such that all three contain the same number of
instances.4
Note that while we condition the embeddings on
demographic variables, they are not task-specific.
While general-purpose embeddings are widely
used in the NLP community, task-specific embed-
dings are known to lead to better results for var-
ious tasks, including sentiment analysis (Tang et
al., 2014). Inducing task-specific embeddings car-
ries the risk of overfitting to a task and data set,
though, and would make it harder to attribute per-
formance differences to demographic factors.
Since we are only interested in the relative dif-
ference between demographic-aware and unaware
systems, not in the absolute performance on the
tasks, we do not use task-specific embeddings.
</bodyText>
<subsectionHeader confidence="0.999682">
2.2 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999993294117647">
Sentiment analysis is the task of determining the
polarity of a document. In our experiments, we
use three polarity values: positive, negative, and
neutral. To collect data for the sentiment analysis
task, we select all reviews that contain the target
variable (gender or age), and a star-rating. Fol-
lowing previous work on similar data (Blitzer et
al., 2007; Hardt and Wulff, 2012; Elming et al.,
2014), we use one, three, or five star ratings, cor-
responding to negative, neutral, and positive senti-
ment, respectively.
We balance the data sets so that both training
and test set contain equal amounts of all three la-
bels. We do this in order to avoid demographic-
specific label distributions (women and people
over 45 tend to give more positive ratings than men
and people under 35, see Section 3.1).
</bodyText>
<subsectionHeader confidence="0.996887">
2.3 Topic Identification
</subsectionHeader>
<bodyText confidence="0.9974457">
Topic identification is the task of assigning a high-
level concept to a document that captures its con-
tent. In our case, the topic labels are taken from
the Trustpilot taxonomy for companies (e.g., Elec-
tronics, Pets, etc.). Again, there is a strong gender
bias: the most common topic for men is Computer
&amp; Accessories, the most common topic among
women is Pets. There is thus considerably less
overlap between the groups than for the other
4Note, however, that the vocabulary sizes still vary among
languages and between age and gender.
tasks. In order not to model gender-specific topic
bias and to eliminate topic frequency as a con-
founding factor, we restrict ourselves to the five
most frequent labels that occur in both groups. We
also ensure that we have the same number of ex-
amples for each label in both groups. However,
in the interest of data size, we do not enforce a
uniform distribution over the five labels (i.e., the
classes are not balanced).
</bodyText>
<subsectionHeader confidence="0.997416">
2.4 Author Attribute Identification
</subsectionHeader>
<bodyText confidence="0.999945615384615">
Author attribute identification is the task of infer-
ring demographic factors from linguistic features
(Alowibdi et al., 2013; Ciot et al., 2013; Liu and
Ruths, 2013). It is often used in author profiling
(Koppel et al., 2002) and stylometrics (Goswami
et al., 2009; Sarawgi et al., 2011). Rosenthal and
McKeown (2011) have shown that these attributes
are correlated.
In this paper, we restrict ourselves to using gen-
der to predict age, and age to predict gender. This
serves as an additional test case. Again, we bal-
ance the class labels to minimize the effect of any
confounding factors.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99921">
3.1 Data Analysis
</subsectionHeader>
<bodyText confidence="0.999997222222222">
Before we analyze the effect of demographic
differences on NLP performance, we investigate
whether there is an effect on the non-linguistic cor-
relates, i.e., ratings and topics. To measure the in-
fluence of demographic factors on these values, we
quantify the distributions over the three sentiment
labels and the five topic labels. We analyze both
gender and age groups separately, but in the inter-
est of space average across all languages.
</bodyText>
<figureCaption confidence="0.999605">
Figure 1: Label distribution for gender
</figureCaption>
<figure confidence="0.998869">
100
80
60
40
20
0
male
female
</figure>
<page confidence="0.778731">
755
</page>
<figureCaption confidence="0.999522">
Figure 2: Label distribution for age groups
</figureCaption>
<bodyText confidence="0.99839345">
Figures 1 and 2 show the distributions over
sentiment labels. We note that men give more
negative and fewer positive ratings than women.
The same holds for people in the younger group,
who are more skewed towards negative ratings
than people in the older group. While the differ-
ences are small, they suggest that demographics
correlate with rating behavior have a measurable
effect on model performance.
The gender distributions over categories ex-
hibit a very different tendency. Table 3 shows
that the review categories (averaged over all
languages) are highly gender-specific. With the
exception of Hotels and Fashion Accessories, the
two distributions are almost bimodal opposites.
However, they are still significantly correlated
(Spearman ρ is 0.49 at p &lt; 0.01).
The difference in the two distributions illus-
trates why we need to control for topic frequency
in our experiments.
</bodyText>
<subsectionHeader confidence="0.99785">
3.2 Models
</subsectionHeader>
<bodyText confidence="0.999978">
Classifiers For all tasks, we use logistic regres-
sion models5 with standard parameter settings. In
order to isolate the effect of demographic dif-
ferences on performance in all text classification
tasks, we need to represent variable length doc-
uments based only upon the embeddings of the
words they contain.
We follow Tang et al. (2014) in using convo-
lutional layers over word embeddings (Collobert
et al., 2011) to generate fixed-length input repre-
sentations. Figure 4 schematically shows the pro-
cedure for the minimum of a 4-dimensional toy
</bodyText>
<footnote confidence="0.762422333333333">
5http://scikit-learn.org/stable/
modules/generated/sklearn.linear_model.
LogisticRegression.html
</footnote>
<bodyText confidence="0.93369180952381">
example. For each instance, we collect five N-
dimensional statistics over the t by N input ma-
trix, where N is the dimensionality of the embed-
dings (here: 100), and t is the sentence length in
words.
From the matrix representation, we compute the
dimension-wise minimum, maximum, and mean
representation, as well as one standard deviation
above and below the mean. We then concate-
nate those five 100-dimensional vectors to a 500-
dimensional vector thats represents each instance
(i.e., review) as input to the logistic regression
classifier.
Taking the maximum and minimum across all
embedding dimensions is equivalent to represent-
ing the exterior surface of the “instance manifold”
(the volume in embedding space within which all
words in the instance reside). Adding the mean
and standard deviation summarizes the density
per-dimension within the manifold. This way, we
can represent any input sentence solely based on
the embeddings, and with the same feature vector
dimensionality.
Figure 4: Example for deriving embedding statis-
tics from sentence in 4-dimensional space. Mini-
mum shaded
The approach is the same for all three tasks, and
we did not tune any parameters to maximize per-
formance. The results are thus maximally compa-
rable to each other, albeit far from state-of-the-art.
Overall performance could be improved with task-
specific features and more sophisticated models,
but it would make the results less comparable, and
complicate identifying the source of performance
differences. We leave this for future research.
Comparison In order to compare demographic-
aware and agnostic models, we use the following
setup for each task and language:
1. In the “agnostic” setting, we train a logistic-
regression model using the joint embeddings
(i.e., embeddings induced on the corpus con-
taining both sub-groups, e.g. male and fe-
</bodyText>
<figure confidence="0.989068673469388">
90
80
70
60
50
40
30
20
10
0
O45
U35
that was cool
min()
0.1
0.8
0.2
0.4
0.4
0.5
0.6
0.3
0.9
0.6
0.7
0.2
0.1
0.5
0.2
0.2
-std()
mean()
+std()
max()
0.9 0.46 0.14 0.8
0.8 0.63 0.5 0.76
0.7 0.5 0.28 0.72
0.4 0.3 0.22 0.38
756
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
mae
female
</figure>
<figureCaption confidence="0.999996">
Figure 3: Distribution of the 30 most frequent categories per gender over all languages
</figureCaption>
<bodyText confidence="0.998098590909091">
male) and group-agnostic training data (i.e.,
data that contains an equal amount of in-
stances from either sub-group).
2. In the demographic-aware setting, we train a
logistic-regression model for each of the two
sub-groups (e.g., male and female). For each
sub-group, we use the group-specific embed-
dings (i.e., embeddings induced on, say, male
data) and group-specific training data (i.e.,
instances collected from male data).
We measure F1-performance for both settings
(agnostic and demographic-aware) on the test set.
The test data contains an equal amount of in-
stances from both sub-groups (say, male and fe-
male). We use the demographic-aware classifier
appropriate for each instance (e.g., male classi-
fier for male instances), i.e., we assume that the
model has access to this information. For many
user-generated content settings, this is realistic,
since demographic information is available. How-
ever, we only predict the target variable (senti-
ment, topic, or author attribute). We do not require
the model to predict the sub-group (age or gender
group).
We assume that demographic factors hold
irrespective of language. We thus compute a
macro-F1 over all languages. Micro-F1 would
favor languages for which there is more data
available, i.e., performance on those languages
would dominate the average performance. Since
we do not want to ascribe more importance
to any particular language, macro-F1 is more
appropriate.
Even if there is a difference in performance
between the agnostic and aware settings, this dif-
ference could still be due to the specific data set.
In order to test whether the difference is also sta-
tistically significant, we use a bootstrap-sampling
test. In a bootstrap-sampling test, we sample
subsets of the predictions of both settings (with
replacement) 10,000 times. For each sample,
we measure F1 of both systems, and compare
the winning system of the sample to the winning
system on the entire data set. The number of times
</bodyText>
<page confidence="0.996617">
757
</page>
<table confidence="0.968802">
SENTIMENT ANALYSIS TOPIC CLASSIFICATION AGE CLASSIFICATION
COUNTRY AGNOSTIC AWARE
Denmark 61.75 *62.00
France 61.21 61.09
Germany 60.50 61.36
UK 65.22 65.12
US 60.94 61.24
avg 61.92 62.16
AGNOSTIC AWARE
49.19 *50.08
38.45 *39.33
60.45 61.11
66.02 66.26
65.64 65.37
55.95 56.43
AGNOSTIC AWARE
59.94 *60.22
53.85 54.21
60.19 60.20
59.78 *60.35
61.97 62.68
59.15 59.53
</table>
<tableCaption confidence="0.996612">
Table 4: F1 for gender-aware and agnostic models on tasks. Averages are macro average. * : p &lt; 0.05
</tableCaption>
<bodyText confidence="0.999952">
the sample winner differs from the entire data
set, divided by 10, 000, is the reported p-value.
Bootstrap-sampling essentially simulates runs of
the two systems on different data sets. If one
system outperforms the other under most of these
conditions (i.e., the test returns a low p-value), we
can be reasonably sure that the difference is not
due to chance.
As discussed in Berg-Kirkpatrick et al. (2012)
and Søgaard et al. (2014), this test is the most ap-
propriate for NLP data, since it does not make any
assumptions about the underlying distributions,
and directly takes performance into account. Note
that the test still depends on data size, though,
so that small differences in performance on larger
data sets can be significant, while larger differ-
ences on small sets might not.
We test for significance with the standard cutoff
of p &lt; 0.05. However, even under a bootstrap-
sampling test, we can only limit the number of
likely false positives. If we run enough tests, we
increase the chance of reporting a type-I error. In
order to account for this effect, we use Bonferroni
corrections for each of the tasks.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999759625">
For each task, we compare the demographic-aware
setting to an agnostic setting. The latter is equiva-
lent to the currently common approach in NLP. For
each task and language, the setting with the higher
performance is marked in bold. Statistically sig-
nificant differences (at p &lt; 0.05) are marked with
a star (*). Note that for the macro-averaged scores,
we cannot perform bootstrap significance testing.
</bodyText>
<subsectionHeader confidence="0.89952">
4.1 Gender
</subsectionHeader>
<bodyText confidence="0.999963378378378">
Table 4 shows the F1 scores for the different tasks.
In the left column of each task (labeled AGNOS-
TIC), the system is trained on embeddings and data
from both genders, in the same ratios as in the test
data. This column is similar to the configuration
normally used in NLP to date, where – at least in
theory – data comes from a uniformly distributed
sample.
In the right column (labeled AWARE), the
classification is based on the classifier trained on
embeddings and data from the respective gender.
While the improvements are small, they are
consistent. We do note some variance in consis-
tency across tasks.
The largest average improvement among the
three tasks is on topic classification. This improve-
ment is interesting, since we have seen stark dif-
ferences for the topic distribution between gen-
ders. Note, however that we controlled for this
factor in our experiments (cf. Table 3). The re-
sults thus show that taking gender into account
improves topic classification performance even af-
ter controlling for prior topic distribution as a con-
founding factor.
The improvements in age classification are the
most consistent. This consistency is likely due
to the fact that author attributes are often corre-
lated. The fact that the attributes are related can
be exploited in stacking approaches, where the at-
tributes are predicted together.
Analyzing the errors, the misclassifications for
sentiment analysis (the weakest task) seem to be
system-independent. Mistakes are mainly due to
the simplicity of the system. Since we do not ex-
plicitly model negation, we incur errors such as “I
will never order anywhere else again” classified as
negative, even though it is in fact rather positive.
</bodyText>
<page confidence="0.994049">
758
</page>
<table confidence="0.997271347826087">
SENTIMENT ANALYSIS
COUNTRY AGNOSTIC AWARE
Denmark 58.74 59.12
France 53.50 53.40
Germany 51.91 52.83
UK 59.72 *60.83
US 55.57 56.00
avg 55.89 56.44
TOPIC CLASSIFICATION GENDER CLASSIFICATION
AGNOSTIC AWARE
45.11 46.00
43.54 42.64
*56.91 55.41
59.40 *60.88
61.14 61.38
53.22 53.26
AGNOSTIC AWARE
58.82 58.97
54.64 54.24
54.04 54.51
57.69 *58.25
60.05 60.97
57.05 57.59
</table>
<tableCaption confidence="0.985632">
Table 5: F1 for age-aware and agnostic models on tasks. Averages are macro average. * : p &lt; 0.05
</tableCaption>
<subsectionHeader confidence="0.997219">
4.2 Age
</subsectionHeader>
<bodyText confidence="0.99998712">
Table 5 presents the results for systems with age
as independent demographic variable. Again, we
show the difference between the agnostic and
age-aware setting in parallel columns for each
task.
The improvements are similar to the ones
for gender. The smaller magnitude across tasks
indicates that knowledge of age offers less dis-
criminative power than knowledge of gender. This
in itself is an interesting result, suggesting that the
age gap is much smaller than the gender gap when
it comes to language variation (i.e., older people’s
language is more similar to younger people than
the language of men is to women). The difference
between groups could be a domain-effect, though,
caused by the fact that all subjects are using a
form of “reviewese” when leaving their feedback.
Why this effect would be more prevalent across
ages than across genders is not obvious from the
data.
When averaged over all languages, the age-
aware setup again consistently outperforms the ag-
nostic setup, as it did for gender. While the final
numbers are lower than in the gender setting, av-
erage improvements tend to be just as decisive.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999854872340425">
Most work in NLP that has dealt with demo-
graphic factors has either a) looked at the corre-
lation of socio-economic attributes with linguis-
tic features (Eisenstein et al., 2011; Eisenstein,
2013a; Eisenstein, 2013b; Doyle, 2014; Bamman
et al., 2014a; Eisenstein, to appear), or b) used lin-
guistic features to infer socio-economic attributes
(Rosenthal and McKeown, 2011; Nguyen et al.,
2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu
and Ruths, 2013; Bergsma et al., 2013; Volkova et
al., 2015).
Our approach is related to the work by Eisen-
stein (2013a) and Doyle (2014), in that we in-
vestigate the influence of extralinguistic factors.
Both of them work on Twitter and use geocoding
information, whereas we focus on age and gen-
der. Also, rather than correlating with census-level
statistics, as in (Eisenstein et al., 2011; Eisenstein,
2013a; Eisenstein, to appear), we take individual
information of each author into account.
Volkova et al. (2013) also explore the influence
of gender and age on text-classification. They
include demographic-specific features into their
model and show improvements on sentiment anal-
ysis in three languages. Our work extends to more
languages and three different text-classification
tasks. We also use word representations trained
on corpora from the various demographic groups,
rather than incorporating the differences explicitly
as features in our model.
Recently, Bamman et al. (2014a) have shown
how regional lexical differences (i.e., situated lan-
guage) can be learned and represented via dis-
tributed word representations (embeddings). They
evaluate the conditional embeddings intrinsically,
to show that the regional representatives of sports
teams, parks, etc. are more closely associated with
the respective hypernyms than other representa-
tives. We also use embeddings conditioned on de-
mographic factors (age and gender instead of loca-
tion), but evaluate their effect on performance ex-
trinsically, when used as input to an NLP system,
rather than intrinsically (i.e., for discovering cor-
relations between language use and demographic
statistics).
Tang et al. (2014) learn embeddings for senti-
ment analysis by splitting up their data by rating.
</bodyText>
<page confidence="0.996713">
759
</page>
<bodyText confidence="0.999985">
We follow their methodology in using embeddings
to represent variable length inputs for classifica-
tion.
The experiments on author attribute identifi-
cation are inspired by a host of previous work
(Rosenthal and McKeown, 2011; Nguyen et al.,
2011; Alowibdi et al., 2013; Ciot et al., 2013;
Liu and Ruths, 2013; Volkova et al., 2015, in-
ter alia). The main difference is that we use em-
beddings trained on another demographic variable
rather than n-gram based features, and that our
goal is not to build a state-of-the-art system.
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.99997624">
The results in Section 4 have shown that incor-
porating information on age and gender improves
performance across a host of text-classification
tasks. Even though the improvements are small
and vary from task to task, they hold consistently
across three tasks and languages. The magnitude
of the improvements could be improved by using
task-specific embeddings, additional features, and
more sophisticated models. This would obscure
the influence of the individual factors, though.
The observed improvements are solely due to
the fact that different demographic groups use lan-
guage quite differently. Sociolinguistic research
suggests that younger people and women tend
to be more creative in their language use than
men and older groups. The former are thus of-
ten the drivers of language change (Holmes, 2013;
Nguyen et al., 2014). Modeling language as uni-
form loses these distinctions, and thus causes per-
formance drops.
As NLP systems are increasingly used for busi-
ness intelligence and decision making, systematic
performance differences carry the danger of dis-
advantaging minority groups whose language use
differs from the norm.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999785">
In this paper, we investigate the influence of age
and gender on topic identification, sentiment anal-
ysis, and author attribute identification. We induce
embeddings conditioned on the respective demo-
graphic variable and use those embeddings as sole
input to classifiers to build both demographic-
agnostic and aware models. We evaluate our mod-
els on five languages.
Our results show that the models using de-
mographic information perform on average better
than the agnostic models. The improvements are
small, but consistent, and in 8/30 cases, also statis-
tically significant at p &lt; 0.05, according to boot-
strap sampling tests.
The results indicate that NLP systems can im-
prove classification performance by incorporat-
ing demographic information, where available. In
most of situated texts (social media, etc.), this is
the case. While the improvements vary among
tasks, the results suggest that similar to domain
adaptation, we should start addressing the problem
of demographic adaptation in NLP.
</bodyText>
<sectionHeader confidence="0.994768" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996963">
Thanks to ˇZeljko Agi´c, David Bamman, Jacob
Eisenstein, Stephan Gouws, Anders Johannsen,
Barbara Plank, Anders Søgaard, and Svitlana
Volkova for their invaluable feedback, as well as
to the anonymous reviewers, whose comments
helped improve the paper. The author was sup-
ported under ERC Starting Grant LOWLANDS
No. 313695.
</bodyText>
<sectionHeader confidence="0.998063" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99899275">
Jalal S Alowibdi, Ugo A Buy, and Philip Yu. 2013.
Empirical evaluation of profile characteristics for
gender classification on twitter. In Machine Learn-
ing and Applications (ICMLA), 2013 12th Interna-
tional Conference on, volume 1, pages 365–369.
IEEE.
David Bamman, Chris Dyer, and Noah A. Smith.
2014a. Distributed representations of geographi-
cally situated language. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics, pages 828–834. Proceedings of
ACL.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014b. Gender identity and lexical varia-
tion in social media. Journal of Sociolinguistics,
18(2):135–160.
Federica Barbieri. 2008. Patterns of age-based lin-
guistic variation in American English. Journal of
sociolinguistics, 12(1):58–88.
Andrew J Barke. 2000. The Effect of Age on the
Style of Discourse among Japanese Women. In Pro-
ceedings of the 14th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 23–34.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of EMNLP.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
</reference>
<page confidence="0.98259">
760
</page>
<reference confidence="0.995938865384615">
Broadly improving user classification via
communication-based name and location clustering
on twitter. In HLT-NAACL, pages 1010–1019.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of EMNLP.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes and
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of ACL.
Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.
2009. Extracting discriminative concepts for do-
main adaptation in text mining. In KDD.
Minmin Chen, Killiang Weinberger, and John Blitzer.
2011. Co-training for domain adaptation. In NIPS.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18–21.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Hal Daum´e, Abhishek Kumar, and Avishek Saha.
2010. Frustratingly easy semi-supervised domain
adaptation. In ACL Workshop on Domain Adapta-
tion for NLP.
Hal Daume III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26:101–126.
Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In EACL.
Jacob Eisenstein, Noah Smith, and Eric Xing. 2011.
Discovering sociolinguistic associations with struc-
tured sparsity. In Proceedings of ACL.
Jacob Eisenstein. 2013a. Phonological factors in so-
cial media writing. In Workshop on Language Anal-
ysis in Social Media, NAACL.
Jacob Eisenstein. 2013b. What to do about bad lan-
guage on the internet. In Proceedings of NAACL.
Jacob Eisenstein. to appear. Systematic patterning
in phonologically-motivated orthographic variation.
Journal of Sociolinguistics.
Jakob Elming, Barbara Plank, and Dirk Hovy. 2014.
Robust cross-domain sentiment analysis for low-
resource languages. In Proceedings of the 5th Work-
shop on Computational Approaches to Subjectivity,
Sentiment and Social Media Analysis, pages 2–7,
Baltimore, Maryland, June. Association for Compu-
tational Linguistics.
Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi.
2009. Stylometric analysis of bloggers’ age and
gender. In Third International AAAI Conference on
Weblogs and Social Media.
Daniel Hardt and Julie Wulff. 2012. What is the mean-
ing of 5*’s? an investigation of the expression and
rating of sentiment. In Empirical Methods in Natu-
ral Language Processing, page 319.
Janet Holmes. 1997. Women, language and identity.
Journal of Sociolinguistics, 1(2):195–223.
Janet Holmes. 2013. An introduction to sociolinguis-
tics. Routledge.
Dirk Hovy, Anders Johannsen, and Anders Søgaard.
2015a. User review-sites as a source for large-scale
sociolinguistic studies. In Proceedings of WWW.
Dirk Hovy, Barbara Plank, H´ector Martinez Alonso,
and Anders Søgaard. 2015b. Mining for unambigu-
ous instances to adapt pos taggers to new domains.
In Proceedings of NAACL-HLT.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2002. Automatically categorizing writ-
ten texts by author gender. Literary and Linguistic
Computing, 17(4):401–412.
William Labov. 1964. The social stratification of En-
glish in New York City. Ph.D. thesis, Columbia uni-
versity.
Wendy Liu and Derek Ruths. 2013. What’s in a name?
using first names as features for gender inference in
twitter. In Analyzing Microtext: 2013 AAAI Spring
Symposium.
Ronald Macaulay. 2001. You’re like ‘why not?’ the
quotative expressions of glasgow adolescents. Jour-
nal of Sociolinguistics, 5(1):3–21.
Ronald Macaulay. 2002. Extremely interesting, very
interesting, or only quite interesting? adverbs and
social class. Journal of Sociolinguistics, 6(3):398–
417.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Lesley Milroy and James Milroy. 1992. Social net-
work and social class: Toward an integrated soci-
olinguistic model. Language in society, 21(01):1–
26.
Dong Nguyen, Noah A Smith, and Carolyn P Ros´e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115–123. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.965266">
761
</page>
<reference confidence="0.999666032258064">
Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru¨oz, Ri-
lana Gravel, Mariet Theune, Theo Meder, and Fran-
ciska De Jong. 2014. Predicting Author Gender
and Age from Tweets: Sociolinguistic Theories and
Crowd Wisdom. In Proceedings of COLING 2014.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of ACL.
Barbara Plank, Dirk Hovy, Ryan McDonald, and An-
ders Søgaard. 2014. Adapting taggers to twitter
with not-so-distant supervision. In Proceedings of
COLING. COLING.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL.
John Rickford and Mackenzie Price. 2013. Girlz ii
women: Age-grading, language change and stylistic
variation. Journal of Sociolinguistics, 17(2):143–
179.
Sara Rosenthal and Kathleen McKeown. 2011. Age
prediction in blogs: A study of style, content, and
online behavior in pre-and post-social media genera-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 763–
772. Association for Computational Linguistics.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evi-
dence beyond topic and genre. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning, pages 78–86. Association for
Computational Linguistics.
Anders Søgaard, Anders Johannsen, Barbara Plank,
Dirk Hovy, and H´ector Martinez Alonso. 2014.
What’s in a p-value in nlp? In Proceedings of the
Eighteenth Conference on Computational Natural
Language Learning, pages 1–10, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting
Liu, and Bing Qin. 2014. Learning sentiment-
specific word embedding for twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1555–1565.
Svitlana Volkova, Theresa Wilson, and David
Yarowsky. 2013. Exploring demographic language
variations to improve multilingual sentiment anal-
ysis in social media. In Proceedings of EMNLP,
pages 1815–1827.
Svitlana Volkova, Yoram Bachrach, Michael Arm-
strong, and Vijay Sharma. 2015. Inferring latent
user properties from texts published in social media
(demo). In Proceedings of the Twenty-Ninth Confer-
ence on Artificial Intelligence (AAAI), Austin, TX,
January.
Martijn Wieling, John Nerbonne, and R Harald
Baayen. 2011. Quantitative social dialectology: Ex-
plaining linguistic variation geographically and so-
cially. PloS one, 6(9):e23613.
</reference>
<page confidence="0.996663">
762
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482942">
<title confidence="0.999511">Demographic Factors Improve Classification Performance</title>
<author confidence="0.991322">Dirk</author>
<affiliation confidence="0.99974">Center for Language University of Copenhagen,</affiliation>
<address confidence="0.495172">Njalsgade</address>
<email confidence="0.983641">dirk@cst.dk</email>
<abstract confidence="0.999589466666667">Extra-linguistic factors influence language use, and are accounted for by speakers and listeners. Most natural language processing (NLP) tasks to date, however, treat language as uniform. This assumption can harm performance. We investigate the effect of including demographic information on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jalal S Alowibdi</author>
<author>Ugo A Buy</author>
<author>Philip Yu</author>
</authors>
<title>Empirical evaluation of profile characteristics for gender classification on twitter.</title>
<date>2013</date>
<booktitle>In Machine Learning and Applications (ICMLA), 2013 12th International Conference on,</booktitle>
<volume>1</volume>
<pages>365--369</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="13723" citStr="Alowibdi et al., 2013" startWordPosition="2176" endWordPosition="2179"> languages and between age and gender. tasks. In order not to model gender-specific topic bias and to eliminate topic frequency as a confounding factor, we restrict ourselves to the five most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether ther</context>
<context position="26202" citStr="Alowibdi et al., 2013" startWordPosition="4202" endWordPosition="4205">re setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classi</context>
<context position="28252" citStr="Alowibdi et al., 2013" startWordPosition="4514" endWordPosition="4517">c factors (age and gender instead of location), but evaluate their effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art system. 6 Discussion The results in Section 4 have shown that incorporating information on age and gender improves performance across a host of text-classification tasks. Even though the improvements are small and vary from task to task, they hold consistently across three tasks and languages. The magnitude of the improvements could be improved by</context>
</contexts>
<marker>Alowibdi, Buy, Yu, 2013</marker>
<rawString>Jalal S Alowibdi, Ugo A Buy, and Philip Yu. 2013. Empirical evaluation of profile characteristics for gender classification on twitter. In Machine Learning and Applications (ICMLA), 2013 12th International Conference on, volume 1, pages 365–369. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Distributed representations of geographically situated language.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>828--834</pages>
<contexts>
<context position="3390" citStr="Bamman et al. (2014" startWordPosition="515" endWordPosition="518"> and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Section 2.1) to incorporate the information. We evaluate the effect of demographic information on classification performance in three NLP 752 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 752–762, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tasks: sentiment analysis (Section 2.2), topic detection (Section 2.3), and author attribute c</context>
<context position="7246" citStr="Bamman et al., 2014" startWordPosition="1129" endWordPosition="1132">rt (at least 3 attributed instances), though. For age, coverage is less dense, so the resulting data sets are smaller, but still sufficient. For more information on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a clear cut-off is hard. We therefore use age ranges that result in roughly equally sized data sets for both groups, and that are not contiguous. For each independent variable (age and gender), we induce embeddings for the two sub-groups (see section 2.1), as well as a “mixed” setting. We also extract labeled data for each task (see sections 2.2, 2.3, and 2.4). Each of these data sets is randomly split into t</context>
<context position="26037" citStr="Bamman et al., 2014" startWordPosition="4177" endWordPosition="4180"> their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; </context>
</contexts>
<marker>Bamman, Dyer, Smith, 2014</marker>
<rawString>David Bamman, Chris Dyer, and Noah A. Smith. 2014a. Distributed representations of geographically situated language. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 828–834. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Jacob Eisenstein</author>
<author>Tyler Schnoebelen</author>
</authors>
<title>Gender identity and lexical variation in social media.</title>
<date>2014</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="3390" citStr="Bamman et al. (2014" startWordPosition="515" endWordPosition="518"> and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Section 2.1) to incorporate the information. We evaluate the effect of demographic information on classification performance in three NLP 752 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 752–762, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics tasks: sentiment analysis (Section 2.2), topic detection (Section 2.3), and author attribute c</context>
<context position="7246" citStr="Bamman et al., 2014" startWordPosition="1129" endWordPosition="1132">rt (at least 3 attributed instances), though. For age, coverage is less dense, so the resulting data sets are smaller, but still sufficient. For more information on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a clear cut-off is hard. We therefore use age ranges that result in roughly equally sized data sets for both groups, and that are not contiguous. For each independent variable (age and gender), we induce embeddings for the two sub-groups (see section 2.1), as well as a “mixed” setting. We also extract labeled data for each task (see sections 2.2, 2.3, and 2.4). Each of these data sets is randomly split into t</context>
<context position="26037" citStr="Bamman et al., 2014" startWordPosition="4177" endWordPosition="4180"> their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; </context>
</contexts>
<marker>Bamman, Eisenstein, Schnoebelen, 2014</marker>
<rawString>David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014b. Gender identity and lexical variation in social media. Journal of Sociolinguistics, 18(2):135–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federica Barbieri</author>
</authors>
<title>Patterns of age-based linguistic variation in American English.</title>
<date>2008</date>
<journal>Journal of sociolinguistics,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="1137" citStr="Barbieri, 2008" startWordPosition="157" endWordPosition="158">t by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demographic factors, other factors such as mood, inter</context>
<context position="7395" citStr="Barbieri, 2008" startWordPosition="1156" endWordPosition="1157">rmation on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a clear cut-off is hard. We therefore use age ranges that result in roughly equally sized data sets for both groups, and that are not contiguous. For each independent variable (age and gender), we induce embeddings for the two sub-groups (see section 2.1), as well as a “mixed” setting. We also extract labeled data for each task (see sections 2.2, 2.3, and 2.4). Each of these data sets is randomly split into training and test data, 60:40. Note that we do not set any parameters on development data, but instead use off-the-shelf software with default paramet</context>
</contexts>
<marker>Barbieri, 2008</marker>
<rawString>Federica Barbieri. 2008. Patterns of age-based linguistic variation in American English. Journal of sociolinguistics, 12(1):58–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Barke</author>
</authors>
<title>The Effect of Age on the Style of Discourse among Japanese Women.</title>
<date>2000</date>
<booktitle>In Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>23--34</pages>
<contexts>
<context position="7379" citStr="Barke, 2000" startWordPosition="1154" endWordPosition="1155">For more information on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a clear cut-off is hard. We therefore use age ranges that result in roughly equally sized data sets for both groups, and that are not contiguous. For each independent variable (age and gender), we induce embeddings for the two sub-groups (see section 2.1), as well as a “mixed” setting. We also extract labeled data for each task (see sections 2.2, 2.3, and 2.4). Each of these data sets is randomly split into training and test data, 60:40. Note that we do not set any parameters on development data, but instead use off-the-shelf software with</context>
</contexts>
<marker>Barke, 2000</marker>
<rawString>Andrew J Barke. 2000. The Effect of Age on the Style of Discourse among Japanese Women. In Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation, pages 23–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in NLP.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="21347" citStr="Berg-Kirkpatrick et al. (2012)" startWordPosition="3400" endWordPosition="3403">.02 66.26 65.64 65.37 55.95 56.43 AGNOSTIC AWARE 59.94 *60.22 53.85 54.21 60.19 60.20 59.78 *60.35 61.97 62.68 59.15 59.53 Table 4: F1 for gender-aware and agnostic models on tasks. Averages are macro average. * : p &lt; 0.05 the sample winner differs from the entire data set, divided by 10, 000, is the reported p-value. Bootstrap-sampling essentially simulates runs of the two systems on different data sets. If one system outperforms the other under most of these conditions (i.e., the test returns a low p-value), we can be reasonably sure that the difference is not due to chance. As discussed in Berg-Kirkpatrick et al. (2012) and Søgaard et al. (2014), this test is the most appropriate for NLP data, since it does not make any assumptions about the underlying distributions, and directly takes performance into account. Note that the test still depends on data size, though, so that small differences in performance on larger data sets can be significant, while larger differences on small sets might not. We test for significance with the standard cutoff of p &lt; 0.05. However, even under a bootstrapsampling test, we can only limit the number of likely false positives. If we run enough tests, we increase the chance of rep</context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in NLP. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Mark Dredze</author>
<author>Benjamin Van Durme</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<date>2013</date>
<marker>Bergsma, Dredze, Van Durme, Wilson, Yarowsky, 2013</marker>
<rawString>Shane Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, and David Yarowsky. 2013.</rawString>
</citation>
<citation valid="false">
<title>Broadly improving user classification via communication-based name and location clustering on twitter.</title>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1010--1019</pages>
<marker></marker>
<rawString>Broadly improving user classification via communication-based name and location clustering on twitter. In HLT-NAACL, pages 1010–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2734" citStr="Blitzer et al., 2006" startWordPosition="404" endWordPosition="407"> uniform, and information the main objective. Under these uniform conditions, the impact of demographics on performance was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="12137" citStr="Blitzer et al., 2007" startWordPosition="1908" endWordPosition="1911">e performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.2 Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following previous work on similar data (Blitzer et al., 2007; Hardt and Wulff, 2012; Elming et al., 2014), we use one, three, or five star ratings, corresponding to negative, neutral, and positive sentiment, respectively. We balance the data sets so that both training and test set contain equal amounts of all three labels. We do this in order to avoid demographicspecific label distributions (women and people over 45 tend to give more positive ratings than men and people under 35, see Section 3.1). 2.3 Topic Identification Topic identification is the task of assigning a highlevel concept to a document that captures its content. In our case, the topic la</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Chen</author>
<author>Wai Lam</author>
<author>Ivor Tsang</author>
<author>Tak-Lam Wong</author>
</authors>
<title>Extracting discriminative concepts for domain adaptation in text mining.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="2810" citStr="Chen et al., 2009" startWordPosition="417" endWordPosition="420">he impact of demographics on performance was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distribut</context>
</contexts>
<marker>Chen, Lam, Tsang, Wong, 2009</marker>
<rawString>Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong. 2009. Extracting discriminative concepts for domain adaptation in text mining. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Killiang Weinberger</author>
<author>John Blitzer</author>
</authors>
<title>Co-training for domain adaptation.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2850" citStr="Chen et al., 2011" startWordPosition="425" endWordPosition="428"> was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) con</context>
</contexts>
<marker>Chen, Weinberger, Blitzer, 2011</marker>
<rawString>Minmin Chen, Killiang Weinberger, and John Blitzer. 2011. Co-training for domain adaptation. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morgane Ciot</author>
<author>Morgan Sonderegger</author>
<author>Derek Ruths</author>
</authors>
<title>Gender inference of twitter users in nonenglish contexts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--21</pages>
<location>Seattle, Wash,</location>
<contexts>
<context position="13742" citStr="Ciot et al., 2013" startWordPosition="2180" endWordPosition="2183">age and gender. tasks. In order not to model gender-specific topic bias and to eliminate topic frequency as a confounding factor, we restrict ourselves to the five most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on t</context>
<context position="26221" citStr="Ciot et al., 2013" startWordPosition="4206" endWordPosition="4209">ntly outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They incl</context>
<context position="28271" citStr="Ciot et al., 2013" startWordPosition="4518" endWordPosition="4521">er instead of location), but evaluate their effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art system. 6 Discussion The results in Section 4 have shown that incorporating information on age and gender improves performance across a host of text-classification tasks. Even though the improvements are small and vary from task to task, they hold consistently across three tasks and languages. The magnitude of the improvements could be improved by using task-specifi</context>
</contexts>
<marker>Ciot, Sonderegger, Ruths, 2013</marker>
<rawString>Morgane Ciot, Morgan Sonderegger, and Derek Ruths. 2013. Gender inference of twitter users in nonenglish contexts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash, pages 18–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="16085" citStr="Collobert et al., 2011" startWordPosition="2562" endWordPosition="2565">. However, they are still significantly correlated (Spearman ρ is 0.49 at p &lt; 0.01). The difference in the two distributions illustrates why we need to control for topic frequency in our experiments. 3.2 Models Classifiers For all tasks, we use logistic regression models5 with standard parameter settings. In order to isolate the effect of demographic differences on performance in all text classification tasks, we need to represent variable length documents based only upon the embeddings of the words they contain. We follow Tang et al. (2014) in using convolutional layers over word embeddings (Collobert et al., 2011) to generate fixed-length input representations. Figure 4 schematically shows the procedure for the minimum of a 4-dimensional toy 5http://scikit-learn.org/stable/ modules/generated/sklearn.linear_model. LogisticRegression.html example. For each instance, we collect five Ndimensional statistics over the t by N input matrix, where N is the dimensionality of the embeddings (here: 100), and t is the sentence length in words. From the matrix representation, we compute the dimension-wise minimum, maximum, and mean representation, as well as one standard deviation above and below the mean. We then c</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Abhishek Kumar</author>
<author>Avishek Saha</author>
</authors>
<title>Frustratingly easy semi-supervised domain adaptation.</title>
<date>2010</date>
<booktitle>In ACL Workshop on Domain Adaptation for NLP.</booktitle>
<marker>Daum´e, Kumar, Saha, 2010</marker>
<rawString>Hal Daum´e, Abhishek Kumar, and Avishek Saha. 2010. Frustratingly easy semi-supervised domain adaptation. In ACL Workshop on Domain Adaptation for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Daniel Marcu</author>
</authors>
<title>Domain adaptation for statistical classifiers.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>26--101</pages>
<marker>Daume, Marcu, 2006</marker>
<rawString>Hal Daume III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelligence Research, 26:101–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Doyle</author>
</authors>
<title>Mapping dialectal variation by querying social media.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="26016" citStr="Doyle, 2014" startWordPosition="4175" endWordPosition="4176"> when leaving their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011</context>
</contexts>
<marker>Doyle, 2014</marker>
<rawString>Gabriel Doyle. 2014. Mapping dialectal variation by querying social media. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Noah Smith</author>
<author>Eric Xing</author>
</authors>
<title>Discovering sociolinguistic associations with structured sparsity.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="25965" citStr="Eisenstein et al., 2011" startWordPosition="4167" endWordPosition="4170">d by the fact that all subjects are using a form of “reviewese” when leaving their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with cens</context>
</contexts>
<marker>Eisenstein, Smith, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Noah Smith, and Eric Xing. 2011. Discovering sociolinguistic associations with structured sparsity. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Phonological factors in social media writing.</title>
<date>2013</date>
<booktitle>In Workshop on Language Analysis in Social Media, NAACL.</booktitle>
<contexts>
<context position="25983" citStr="Eisenstein, 2013" startWordPosition="4171" endWordPosition="4172">bjects are using a form of “reviewese” when leaving their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistic</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013a. Phonological factors in social media writing. In Workshop on Language Analysis in Social Media, NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="25983" citStr="Eisenstein, 2013" startWordPosition="4171" endWordPosition="4172">bjects are using a form of “reviewese” when leaving their feedback. Why this effect would be more prevalent across ages than across genders is not obvious from the data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistic</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013b. What to do about bad language on the internet. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>to appear. Systematic patterning in phonologically-motivated orthographic variation.</title>
<journal>Journal of Sociolinguistics.</journal>
<marker>Eisenstein, </marker>
<rawString>Jacob Eisenstein. to appear. Systematic patterning in phonologically-motivated orthographic variation. Journal of Sociolinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Elming</author>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
</authors>
<title>Robust cross-domain sentiment analysis for lowresource languages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,</booktitle>
<pages>2--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="12182" citStr="Elming et al., 2014" startWordPosition="1916" endWordPosition="1919">ors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.2 Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following previous work on similar data (Blitzer et al., 2007; Hardt and Wulff, 2012; Elming et al., 2014), we use one, three, or five star ratings, corresponding to negative, neutral, and positive sentiment, respectively. We balance the data sets so that both training and test set contain equal amounts of all three labels. We do this in order to avoid demographicspecific label distributions (women and people over 45 tend to give more positive ratings than men and people under 35, see Section 3.1). 2.3 Topic Identification Topic identification is the task of assigning a highlevel concept to a document that captures its content. In our case, the topic labels are taken from the Trustpilot taxonomy f</context>
</contexts>
<marker>Elming, Plank, Hovy, 2014</marker>
<rawString>Jakob Elming, Barbara Plank, and Dirk Hovy. 2014. Robust cross-domain sentiment analysis for lowresource languages. In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 2–7, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumit Goswami</author>
<author>Sudeshna Sarkar</author>
<author>Mayur Rustagi</author>
</authors>
<title>Stylometric analysis of bloggers’ age and gender.</title>
<date>2009</date>
<booktitle>In Third International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="13863" citStr="Goswami et al., 2009" startWordPosition="2201" endWordPosition="2204">ing factor, we restrict ourselves to the five most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the non-linguistic correlates, i.e., ratings and topics. To measure the influence of demographic factors on these values, </context>
</contexts>
<marker>Goswami, Sarkar, Rustagi, 2009</marker>
<rawString>Sumit Goswami, Sudeshna Sarkar, and Mayur Rustagi. 2009. Stylometric analysis of bloggers’ age and gender. In Third International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hardt</author>
<author>Julie Wulff</author>
</authors>
<title>What is the meaning of 5*’s? an investigation of the expression and rating of sentiment.</title>
<date>2012</date>
<booktitle>In Empirical Methods in Natural Language Processing,</booktitle>
<pages>319</pages>
<contexts>
<context position="12160" citStr="Hardt and Wulff, 2012" startWordPosition="1912" endWordPosition="1915">ces to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.2 Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis task, we select all reviews that contain the target variable (gender or age), and a star-rating. Following previous work on similar data (Blitzer et al., 2007; Hardt and Wulff, 2012; Elming et al., 2014), we use one, three, or five star ratings, corresponding to negative, neutral, and positive sentiment, respectively. We balance the data sets so that both training and test set contain equal amounts of all three labels. We do this in order to avoid demographicspecific label distributions (women and people over 45 tend to give more positive ratings than men and people under 35, see Section 3.1). 2.3 Topic Identification Topic identification is the task of assigning a highlevel concept to a document that captures its content. In our case, the topic labels are taken from the</context>
</contexts>
<marker>Hardt, Wulff, 2012</marker>
<rawString>Daniel Hardt and Julie Wulff. 2012. What is the meaning of 5*’s? an investigation of the expression and rating of sentiment. In Empirical Methods in Natural Language Processing, page 319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
</authors>
<title>Women, language and identity.</title>
<date>1997</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="1089" citStr="Holmes, 1997" startWordPosition="151" endWordPosition="152">iety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demogr</context>
</contexts>
<marker>Holmes, 1997</marker>
<rawString>Janet Holmes. 1997. Women, language and identity. Journal of Sociolinguistics, 1(2):195–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Holmes</author>
</authors>
<title>An introduction to sociolinguistics.</title>
<date>2013</date>
<publisher>Routledge.</publisher>
<contexts>
<context position="29332" citStr="Holmes, 2013" startWordPosition="4687" endWordPosition="4688">y from task to task, they hold consistently across three tasks and languages. The magnitude of the improvements could be improved by using task-specific embeddings, additional features, and more sophisticated models. This would obscure the influence of the individual factors, though. The observed improvements are solely due to the fact that different demographic groups use language quite differently. Sociolinguistic research suggests that younger people and women tend to be more creative in their language use than men and older groups. The former are thus often the drivers of language change (Holmes, 2013; Nguyen et al., 2014). Modeling language as uniform loses these distinctions, and thus causes performance drops. As NLP systems are increasingly used for business intelligence and decision making, systematic performance differences carry the danger of disadvantaging minority groups whose language use differs from the norm. 7 Conclusion In this paper, we investigate the influence of age and gender on topic identification, sentiment analysis, and author attribute identification. We induce embeddings conditioned on the respective demographic variable and use those embeddings as sole input to cla</context>
</contexts>
<marker>Holmes, 2013</marker>
<rawString>Janet Holmes. 2013. An introduction to sociolinguistics. Routledge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Anders Johannsen</author>
<author>Anders Søgaard</author>
</authors>
<title>User review-sites as a source for large-scale sociolinguistic studies.</title>
<date>2015</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="2916" citStr="Hovy et al., 2015" startWordPosition="437" endWordPosition="440">domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Section 2.1) to incorpo</context>
<context position="6839" citStr="Hovy et al. (2015" startWordPosition="1061" endWordPosition="1064">ermany 329k 8% 47% 6% 4% Table 1: Number of users and % per variable per country (after applying augmentations). gender. In case of missing gender values, we base a guess on the first name (if given), by choosing the gender most frequently associated with that name in the particular language. We do require that one gender is prevalent (accounting for 95% of all mentions), and that there is enough support (at least 3 attributed instances), though. For age, coverage is less dense, so the resulting data sets are smaller, but still sufficient. For more information on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a cle</context>
</contexts>
<marker>Hovy, Johannsen, Søgaard, 2015</marker>
<rawString>Dirk Hovy, Anders Johannsen, and Anders Søgaard. 2015a. User review-sites as a source for large-scale sociolinguistic studies. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Barbara Plank</author>
<author>H´ector Martinez Alonso</author>
<author>Anders Søgaard</author>
</authors>
<title>Mining for unambiguous instances to adapt pos taggers to new domains.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2916" citStr="Hovy et al., 2015" startWordPosition="437" endWordPosition="440">domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Section 2.1) to incorpo</context>
<context position="6839" citStr="Hovy et al. (2015" startWordPosition="1061" endWordPosition="1064">ermany 329k 8% 47% 6% 4% Table 1: Number of users and % per variable per country (after applying augmentations). gender. In case of missing gender values, we base a guess on the first name (if given), by choosing the gender most frequently associated with that name in the particular language. We do require that one gender is prevalent (accounting for 95% of all mentions), and that there is enough support (at least 3 attributed instances), though. For age, coverage is less dense, so the resulting data sets are smaller, but still sufficient. For more information on Trustpilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a cle</context>
</contexts>
<marker>Hovy, Plank, Alonso, Søgaard, 2015</marker>
<rawString>Dirk Hovy, Barbara Plank, H´ector Martinez Alonso, and Anders Søgaard. 2015b. Mining for unambiguous instances to adapt pos taggers to new domains. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Shlomo Argamon, and Anat Rachel Shimoni.</title>
<date>2002</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<marker>Koppel, 2002</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2002. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>The social stratification of English in</title>
<date>1964</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia university.</institution>
<location>New York City.</location>
<contexts>
<context position="1050" citStr="Labov, 1964" startWordPosition="145" endWordPosition="146">ic information on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in the</context>
</contexts>
<marker>Labov, 1964</marker>
<rawString>William Labov. 1964. The social stratification of English in New York City. Ph.D. thesis, Columbia university.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>What’s in a name? using first names as features for gender inference in twitter. In Analyzing Microtext:</title>
<date>2013</date>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="13764" citStr="Liu and Ruths, 2013" startWordPosition="2184" endWordPosition="2187">ks. In order not to model gender-specific topic bias and to eliminate topic frequency as a confounding factor, we restrict ourselves to the five most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the non-linguistic corr</context>
<context position="26242" citStr="Liu and Ruths, 2013" startWordPosition="4210" endWordPosition="4213">e agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-speci</context>
<context position="28292" citStr="Liu and Ruths, 2013" startWordPosition="4522" endWordPosition="4525">ion), but evaluate their effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art system. 6 Discussion The results in Section 4 have shown that incorporating information on age and gender improves performance across a host of text-classification tasks. Even though the improvements are small and vary from task to task, they hold consistently across three tasks and languages. The magnitude of the improvements could be improved by using task-specific embeddings, additio</context>
</contexts>
<marker>Liu, Ruths, 2013</marker>
<rawString>Wendy Liu and Derek Ruths. 2013. What’s in a name? using first names as features for gender inference in twitter. In Analyzing Microtext: 2013 AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Macaulay</author>
</authors>
<title>You’re like ‘why not?’ the quotative expressions of glasgow adolescents.</title>
<date>2001</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1105" citStr="Macaulay, 2001" startWordPosition="153" endWordPosition="154">lassification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demographic factors, o</context>
</contexts>
<marker>Macaulay, 2001</marker>
<rawString>Ronald Macaulay. 2001. You’re like ‘why not?’ the quotative expressions of glasgow adolescents. Journal of Sociolinguistics, 5(1):3–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Macaulay</author>
</authors>
<title>Extremely interesting, very interesting, or only quite interesting? adverbs and social class.</title>
<date>2002</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>417</pages>
<contexts>
<context position="1121" citStr="Macaulay, 2002" startWordPosition="155" endWordPosition="156">sks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demographic factors, other factors suc</context>
</contexts>
<marker>Macaulay, 2002</marker>
<rawString>Ronald Macaulay. 2002. Extremely interesting, very interesting, or only quite interesting? adverbs and social class. Journal of Sociolinguistics, 6(3):398– 417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="9635" citStr="Mikolov et al., 2013" startWordPosition="1515" endWordPosition="1518">e out-ofvocabulary rate on the tasks is thus artificially low and can inflate results. In a standard “improvement over baseline”-setup, this would be problematic. However, the results should not be interpreted with respect to their absolute value on the respective tasks, but with respect to the relative differences. 2.1 Conditional Embeddings total 880k 4.51m Table 3: Number of sentences used to induce embeddings Embeddings are distributed representations of words in a vector space, capturing syntactic and semantic regularities among the words. We learn our word embeddings by using word2vec3 (Mikolov et al., 2013) on unlabeled review data. Our corpora are relatively small, compared to the language modeling tasks the tool was developed for (see Table 3 for the number of instances used for each language and variable). We thus follow the suggestions in the word2vec documentation and use the skip-gram model and hierarchical softmax rather than the standard continuous-bag-ofwords model. This setting penalizes low-frequent words less. All out-of-vocabulary (OOV) words are replaced with an “unknown” token, which is represented as the averaged vector over all other words. In this paper, we want to use embeddin</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lesley Milroy</author>
<author>James Milroy</author>
</authors>
<title>Social network and social class: Toward an integrated sociolinguistic model. Language in society,</title>
<date>1992</date>
<pages>21--01</pages>
<contexts>
<context position="1075" citStr="Milroy and Milroy, 1992" startWordPosition="147" endWordPosition="150">n on performance in a variety of text-classification tasks. We find that by including age or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart f</context>
</contexts>
<marker>Milroy, Milroy, 1992</marker>
<rawString>Lesley Milroy and James Milroy. 1992. Social network and social class: Toward an integrated sociolinguistic model. Language in society, 21(01):1– 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Noah A Smith</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Author age prediction from text using linear regression.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<pages>115--123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Nguyen, Smith, Ros´e, 2011</marker>
<rawString>Dong Nguyen, Noah A Smith, and Carolyn P Ros´e. 2011. Author age prediction from text using linear regression. In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115–123. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Dolf Trieschnigg</author>
<author>A Seza Dogru¨oz</author>
<author>Rilana Gravel</author>
<author>Mariet Theune</author>
<author>Theo Meder</author>
<author>Franciska De Jong</author>
</authors>
<title>Predicting Author Gender and Age from Tweets: Sociolinguistic Theories and Crowd Wisdom.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Nguyen, Trieschnigg, Dogru¨oz, Gravel, Theune, Meder, De Jong, 2014</marker>
<rawString>Dong Nguyen, Dolf Trieschnigg, A. Seza Dogru¨oz, Rilana Gravel, Mariet Theune, Theo Meder, and Franciska De Jong. 2014. Predicting Author Gender and Age from Tweets: Sociolinguistic Theories and Crowd Wisdom. In Proceedings of COLING 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2877" citStr="Plank and Moschitti, 2013" startWordPosition="429" endWordPosition="432"> however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demograph</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Ryan McDonald</author>
<author>Anders Søgaard</author>
</authors>
<title>Adapting taggers to twitter with not-so-distant supervision.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING. COLING.</booktitle>
<contexts>
<context position="2897" citStr="Plank et al., 2014" startWordPosition="433" endWordPosition="436">ly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a), we use distributed word representations (embeddings) conditioned on these demographic factors (see Sect</context>
</contexts>
<marker>Plank, Hovy, McDonald, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, Ryan McDonald, and Anders Søgaard. 2014. Adapting taggers to twitter with not-so-distant supervision. In Proceedings of COLING. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2791" citStr="Reichart and Rappoport, 2007" startWordPosition="413" endWordPosition="416">er these uniform conditions, the impact of demographics on performance was small. Lately, however, NLP is increasingly applied to other domains, such as social media, where language is less canonical, demographic information about the author is available, and the authors’ goals are no longer purely informational. The influence of demographic factors in this medium is thus much stronger than on the data we have traditionally used to induce models. The resulting performance drops have often been addressed via various domain adaptation approaches (Blitzer et al., 2006; Daume III and Marcu, 2006; Reichart and Rappoport, 2007; Chen et al., 2009; Daum´e et al., 2010; Chen et al., 2011; Plank and Moschitti, 2013; Plank et al., 2014; Hovy et al., 2015b, inter alia). However, the authors and target demographics of social media differ radically from those in newswire text, and domain might in some case be a secondary effect to demographics. In this paper, we thus ask whether we also need demographic adaptation. Concretely, we investigate 1. how we can encode demographic factors, and 2. what effect they have on the performance of text-classification tasks We focus on age and gender, and similarly to Bamman et al. (2014a</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rickford</author>
<author>Mackenzie Price</author>
</authors>
<title>Girlz ii women: Age-grading, language change and stylistic variation.</title>
<date>2013</date>
<journal>Journal of Sociolinguistics,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>179</pages>
<contexts>
<context position="1185" citStr="Rickford and Price, 2013" startWordPosition="163" endWordPosition="166">on, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demographic factors, other factors such as mood, interpersonal relationship, authority, language attit</context>
<context position="7422" citStr="Rickford and Price, 2013" startWordPosition="1158" endWordPosition="1161">pilot as a resource, see Hovy et al. (2015a). We split each review into sentences, tokenize, replace numbers with a 0, lowercase the data, and join frequent bigrams with an underscore to form a single token. For each language, we collect four sub-corpora, namely two for gender (male and female) and two for age (under 35 and over 45). The subcorpora for the discrete variable gender are relatively straightforward (although see (Bamman et al., 2014b)), but the split for the continuous age variable are less clear. While the effect of age on language use is undisputed (Barke, 2000; Barbieri, 2008; Rickford and Price, 2013), providing a clear cut-off is hard. We therefore use age ranges that result in roughly equally sized data sets for both groups, and that are not contiguous. For each independent variable (age and gender), we induce embeddings for the two sub-groups (see section 2.1), as well as a “mixed” setting. We also extract labeled data for each task (see sections 2.2, 2.3, and 2.4). Each of these data sets is randomly split into training and test data, 60:40. Note that we do not set any parameters on development data, but instead use off-the-shelf software with default parameters for classification. Tab</context>
</contexts>
<marker>Rickford, Price, 2013</marker>
<rawString>John Rickford and Mackenzie Price. 2013. Girlz ii women: Age-grading, language change and stylistic variation. Journal of Sociolinguistics, 17(2):143– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Age prediction in blogs: A study of style, content, and online behavior in pre-and post-social media generations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1,</booktitle>
<pages>763--772</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13916" citStr="Rosenthal and McKeown (2011)" startWordPosition="2209" endWordPosition="2212"> most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the non-linguistic correlates, i.e., ratings and topics. To measure the influence of demographic factors on these values, we quantify the distributions over the three sentimen</context>
<context position="26158" citStr="Rosenthal and McKeown, 2011" startWordPosition="4194" endWordPosition="4197">data. When averaged over all languages, the ageaware setup again consistently outperforms the agnostic setup, as it did for gender. While the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore th</context>
<context position="28208" citStr="Rosenthal and McKeown, 2011" startWordPosition="4506" endWordPosition="4509">. We also use embeddings conditioned on demographic factors (age and gender instead of location), but evaluate their effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art system. 6 Discussion The results in Section 4 have shown that incorporating information on age and gender improves performance across a host of text-classification tasks. Even though the improvements are small and vary from task to task, they hold consistently across three tasks and languages. The magnit</context>
</contexts>
<marker>Rosenthal, McKeown, 2011</marker>
<rawString>Sara Rosenthal and Kathleen McKeown. 2011. Age prediction in blogs: A study of style, content, and online behavior in pre-and post-social media generations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 763– 772. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruchita Sarawgi</author>
<author>Kailash Gajulapalli</author>
<author>Yejin Choi</author>
</authors>
<title>Gender attribution: tracing stylometric evidence beyond topic and genre.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>78--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13886" citStr="Sarawgi et al., 2011" startWordPosition="2205" endWordPosition="2208">t ourselves to the five most frequent labels that occur in both groups. We also ensure that we have the same number of examples for each label in both groups. However, in the interest of data size, we do not enforce a uniform distribution over the five labels (i.e., the classes are not balanced). 2.4 Author Attribute Identification Author attribute identification is the task of inferring demographic factors from linguistic features (Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013). It is often used in author profiling (Koppel et al., 2002) and stylometrics (Goswami et al., 2009; Sarawgi et al., 2011). Rosenthal and McKeown (2011) have shown that these attributes are correlated. In this paper, we restrict ourselves to using gender to predict age, and age to predict gender. This serves as an additional test case. Again, we balance the class labels to minimize the effect of any confounding factors. 3 Experiments 3.1 Data Analysis Before we analyze the effect of demographic differences on NLP performance, we investigate whether there is an effect on the non-linguistic correlates, i.e., ratings and topics. To measure the influence of demographic factors on these values, we quantify the distrib</context>
</contexts>
<marker>Sarawgi, Gajulapalli, Choi, 2011</marker>
<rawString>Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi. 2011. Gender attribution: tracing stylometric evidence beyond topic and genre. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 78–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Anders Johannsen</author>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>H´ector Martinez Alonso</author>
</authors>
<title>What’s in a p-value in nlp?</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="21373" citStr="Søgaard et al. (2014)" startWordPosition="3405" endWordPosition="3408">GNOSTIC AWARE 59.94 *60.22 53.85 54.21 60.19 60.20 59.78 *60.35 61.97 62.68 59.15 59.53 Table 4: F1 for gender-aware and agnostic models on tasks. Averages are macro average. * : p &lt; 0.05 the sample winner differs from the entire data set, divided by 10, 000, is the reported p-value. Bootstrap-sampling essentially simulates runs of the two systems on different data sets. If one system outperforms the other under most of these conditions (i.e., the test returns a low p-value), we can be reasonably sure that the difference is not due to chance. As discussed in Berg-Kirkpatrick et al. (2012) and Søgaard et al. (2014), this test is the most appropriate for NLP data, since it does not make any assumptions about the underlying distributions, and directly takes performance into account. Note that the test still depends on data size, though, so that small differences in performance on larger data sets can be significant, while larger differences on small sets might not. We test for significance with the standard cutoff of p &lt; 0.05. However, even under a bootstrapsampling test, we can only limit the number of likely false positives. If we run enough tests, we increase the chance of reporting a type-I error. In </context>
</contexts>
<marker>Søgaard, Johannsen, Plank, Hovy, Alonso, 2014</marker>
<rawString>Anders Søgaard, Anders Johannsen, Barbara Plank, Dirk Hovy, and H´ector Martinez Alonso. 2014. What’s in a p-value in nlp? In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 1–10, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentimentspecific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1555--1565</pages>
<contexts>
<context position="11381" citStr="Tang et al., 2014" startWordPosition="1789" endWordPosition="1792">e.com/p/word2vec/ 576k 70k US COUNTRY AGE GENDER Denmark 495k 1.6m France Germany UK 36k 47k 232k 490k 211k 1.63m 754 size of the available training data, and since we want to avoid modeling size effects, we balance the three corpora we use to induce embeddings such that all three contain the same number of instances.4 Note that while we condition the embeddings on demographic variables, they are not task-specific. While general-purpose embeddings are widely used in the NLP community, task-specific embeddings are known to lead to better results for various tasks, including sentiment analysis (Tang et al., 2014). Inducing task-specific embeddings carries the risk of overfitting to a task and data set, though, and would make it harder to attribute performance differences to demographic factors. Since we are only interested in the relative difference between demographic-aware and unaware systems, not in the absolute performance on the tasks, we do not use task-specific embeddings. 2.2 Sentiment Analysis Sentiment analysis is the task of determining the polarity of a document. In our experiments, we use three polarity values: positive, negative, and neutral. To collect data for the sentiment analysis ta</context>
<context position="16009" citStr="Tang et al. (2014)" startWordPosition="2550" endWordPosition="2553">Fashion Accessories, the two distributions are almost bimodal opposites. However, they are still significantly correlated (Spearman ρ is 0.49 at p &lt; 0.01). The difference in the two distributions illustrates why we need to control for topic frequency in our experiments. 3.2 Models Classifiers For all tasks, we use logistic regression models5 with standard parameter settings. In order to isolate the effect of demographic differences on performance in all text classification tasks, we need to represent variable length documents based only upon the embeddings of the words they contain. We follow Tang et al. (2014) in using convolutional layers over word embeddings (Collobert et al., 2011) to generate fixed-length input representations. Figure 4 schematically shows the procedure for the minimum of a 4-dimensional toy 5http://scikit-learn.org/stable/ modules/generated/sklearn.linear_model. LogisticRegression.html example. For each instance, we collect five Ndimensional statistics over the t by N input matrix, where N is the dimensionality of the embeddings (here: 100), and t is the sentence length in words. From the matrix representation, we compute the dimension-wise minimum, maximum, and mean represent</context>
<context position="27902" citStr="Tang et al. (2014)" startWordPosition="4458" endWordPosition="4461">ge) can be learned and represented via distributed word representations (embeddings). They evaluate the conditional embeddings intrinsically, to show that the regional representatives of sports teams, parks, etc. are more closely associated with the respective hypernyms than other representatives. We also use embeddings conditioned on demographic factors (age and gender instead of location), but evaluate their effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentimentspecific word embedding for twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Theresa Wilson</author>
<author>David Yarowsky</author>
</authors>
<title>Exploring demographic language variations to improve multilingual sentiment analysis in social media.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1815--1827</pages>
<contexts>
<context position="26742" citStr="Volkova et al. (2013)" startWordPosition="4291" endWordPosition="4294">tributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improvements on sentiment analysis in three languages. Our work extends to more languages and three different text-classification tasks. We also use word representations trained on corpora from the various demographic groups, rather than incorporating the differences explicitly as features in our model. Recently, Bamman et al. (2014a) have shown how regional lexical differences (i.e., situated language) can be learned and represented via distributed word re</context>
</contexts>
<marker>Volkova, Wilson, Yarowsky, 2013</marker>
<rawString>Svitlana Volkova, Theresa Wilson, and David Yarowsky. 2013. Exploring demographic language variations to improve multilingual sentiment analysis in social media. In Proceedings of EMNLP, pages 1815–1827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svitlana Volkova</author>
<author>Yoram Bachrach</author>
<author>Michael Armstrong</author>
<author>Vijay Sharma</author>
</authors>
<title>Inferring latent user properties from texts published in social media (demo).</title>
<date>2015</date>
<booktitle>In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI),</booktitle>
<location>Austin, TX,</location>
<contexts>
<context position="26287" citStr="Volkova et al., 2015" startWordPosition="4218" endWordPosition="4221">le the final numbers are lower than in the gender setting, average improvements tend to be just as decisive. 5 Related Work Most work in NLP that has dealt with demographic factors has either a) looked at the correlation of socio-economic attributes with linguistic features (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, 2013b; Doyle, 2014; Bamman et al., 2014a; Eisenstein, to appear), or b) used linguistic features to infer socio-economic attributes (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Bergsma et al., 2013; Volkova et al., 2015). Our approach is related to the work by Eisenstein (2013a) and Doyle (2014), in that we investigate the influence of extralinguistic factors. Both of them work on Twitter and use geocoding information, whereas we focus on age and gender. Also, rather than correlating with census-level statistics, as in (Eisenstein et al., 2011; Eisenstein, 2013a; Eisenstein, to appear), we take individual information of each author into account. Volkova et al. (2013) also explore the influence of gender and age on text-classification. They include demographic-specific features into their model and show improv</context>
<context position="28314" citStr="Volkova et al., 2015" startWordPosition="4526" endWordPosition="4529">eir effect on performance extrinsically, when used as input to an NLP system, rather than intrinsically (i.e., for discovering correlations between language use and demographic statistics). Tang et al. (2014) learn embeddings for sentiment analysis by splitting up their data by rating. 759 We follow their methodology in using embeddings to represent variable length inputs for classification. The experiments on author attribute identification are inspired by a host of previous work (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Alowibdi et al., 2013; Ciot et al., 2013; Liu and Ruths, 2013; Volkova et al., 2015, inter alia). The main difference is that we use embeddings trained on another demographic variable rather than n-gram based features, and that our goal is not to build a state-of-the-art system. 6 Discussion The results in Section 4 have shown that incorporating information on age and gender improves performance across a host of text-classification tasks. Even though the improvements are small and vary from task to task, they hold consistently across three tasks and languages. The magnitude of the improvements could be improved by using task-specific embeddings, additional features, and more</context>
</contexts>
<marker>Volkova, Bachrach, Armstrong, Sharma, 2015</marker>
<rawString>Svitlana Volkova, Yoram Bachrach, Michael Armstrong, and Vijay Sharma. 2015. Inferring latent user properties from texts published in social media (demo). In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI), Austin, TX, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martijn Wieling</author>
<author>John Nerbonne</author>
<author>R Harald Baayen</author>
</authors>
<title>Quantitative social dialectology: Explaining linguistic variation geographically and socially.</title>
<date>2011</date>
<journal>PloS one,</journal>
<pages>6--9</pages>
<contexts>
<context position="1159" citStr="Wieling et al., 2011" startWordPosition="159" endWordPosition="162">ge or gender information, we consistently and significantly improve performance over demographic-agnostic models. These results hold across three text-classification tasks in five languages. 1 Introduction When we use language, we take demographic factors of the speakers into account. In other words, we do have certain expectations as to who uses “super cute,” “rather satisfying,” or “rad, dude.” Sociolinguistics has long since studied the interplay between demographic factors and language use (Labov, 1964; Milroy and Milroy, 1992; Holmes, 1997; Macaulay, 2001; Macaulay, 2002; Barbieri, 2008; Wieling et al., 2011; Rickford and Price, 2013, inter alia).1 These factors greatly influence word choice, syntax, and even semantics. In natural language processing (NLP), however, we have largely ignored demographic factors, and treated language as a uniform medium. It was irrelevant, (and thus not modeled) whether a text was produced by a middle-aged man, an elderly lady, or a teenager. These three groups, however, differ along a whole host of demographic axes, and these differences are reflected in their language use. 1Apart from the demographic factors, other factors such as mood, interpersonal relationship,</context>
</contexts>
<marker>Wieling, Nerbonne, Baayen, 2011</marker>
<rawString>Martijn Wieling, John Nerbonne, and R Harald Baayen. 2011. Quantitative social dialectology: Explaining linguistic variation geographically and socially. PloS one, 6(9):e23613.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>