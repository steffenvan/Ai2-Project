<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99858">
A Sequential Model for Multi-Class Classification
</title>
<author confidence="0.998517">
Yair Even-Zohar Dan Roth
</author>
<affiliation confidence="0.999195">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.997017">
evenzoha,danr@uiuc.edu
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999425857142857">
Many classification problems require decisions
among a large number of competing classes. These
tasks, however, are not handled well by general pur-
pose learning methods and are usually addressed in
an ad-hoc fashion. We suggest a general approach
– a sequential learning model that utilizes classi-
fiers to sequentially restrict the number of compet-
ing classes while maintaining, with high probability,
the presence of the true outcome in the candidates
set. Some theoretical and computational properties
of the model are discussed and we argue that these
are important in NLP-like domains. The advantages
of the model are illustrated in an experiment in part-
of-speech tagging.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9951405">
A large number of important natural language infer-
ences can be viewed as problems of resolving ambi-
guity, either semantic or syntactic, based on proper-
ties of the surrounding context. These, in turn, can
all be viewed as classification problems in which
the goal is to select a class label from among a
collection of candidates. Examples include part-of
speech tagging, word-sense disambiguation, accent
restoration, word choice selection in machine trans-
lation, context-sensitive spelling correction, word
selection in speech recognition and identifying dis-
course markers.
Machine learning methods have become the
most popular technique in a variety of classifi-
cation problems of these sort, and have shown
significant success. A partial list consists of
Bayesian classifiers (Gale et al., 1993), decision
lists (Yarowsky, 1994), Bayesian hybrids (Gold-
ing, 1995), HMMs (Charniak, 1993), inductive
logic methods (Zelle and Mooney, 1996), memory-
</bodyText>
<footnote confidence="0.4763455">
This research is supported by NSF grants IIS-9801638,IIS-
0085836 and SBR-987345.
</footnote>
<bodyText confidence="0.99992176923077">
based methods (Zavrel et al., 1997), linear classi-
fiers (Roth, 1998; Roth, 1999) and transformation-
based learning (Brill, 1995).
In many of these classification problems a signif-
icant source of difficulty is the fact that the number
of candidates is very large – all words in words se-
lection problems, all possible tags in tagging prob-
lems etc. Since general purpose learning algorithms
do not handle these multi-class classification prob-
lems well (see below), most of the studies do not
address the whole problem; rather, a small set of
candidates (typically two) is first selected, and the
classifier is trained to choose among these. While
this approach is important in that it allows the re-
search community to develop better learning meth-
ods and evaluate them in a range of applications,
it is important to realize that an important stage is
missing. This could be significant when the clas-
sification methods are to be embedded as part of
a higher level NLP tasks such as machine transla-
tion or information extraction, where the small set
of candidates the classifier can handle may not be
fixed and could be hard to determine.
In this work we develop a general approach to
the study of multi-class classifiers. We suggest a se-
quential learning model that utilizes (almost) gen-
eral purpose classifiers to sequentially restrict the
number of competing classes while maintaining,
with high probability, the presence of the true out-
come in the candidate set.
In our paradigm the sought after classifier has to
choose a single class label (or a small set of la-
bels) from among a large set of labels. It works
by sequentially applying simpler classifiers, each
of which outputs a probability distribution over the
candidate labels. These distributions are multiplied
and thresholded, resulting in that each classifier in
the sequence needs to deal with a (significantly)
smaller number of the candidate labels than the pre-
vious classifier. The classifiers in the sequence are
selected to be simple in the sense that they typically
work only on part of the feature space where the de-
composition of feature space is done so as to achieve
statistical independence. Simple classifier are used
since they are more likely to be accurate; they are
chosen so that, with high probability (w.h.p.), they
have one sided error, and therefore the presence of
the true label in the candidate set is maintained. The
order of the sequence is determined so as to maxi-
mize the rate of decreasing the size of the candidate
labels set.
Beyond increased accuracy on multi-class classi-
fication problems , our scheme improves the com-
putation time of these problems several orders of
magnitude, relative to other standard schemes.
In this work we describe the approach, discuss
an experiment done in the context of part-of-speech
(pos) tagging, and provide some theoretical justifi-
cations to the approach. Sec. 2 provides some back-
ground on approaches to multi-class classification
in machine learning and in NLP. In Sec. 3 we de-
scribe the sequential model proposed here and in
Sec. 4 we describe an experiment the exhibits some
of its advantages. Some theoretical justifications are
outlined in Sec. 5.
</bodyText>
<sectionHeader confidence="0.994095" genericHeader="method">
2 Multi-Class Classification
</sectionHeader>
<bodyText confidence="0.996139902912621">
Several works within the machine learning commu-
nity have attempted to develop general approaches
to multi-class classification. One of the most
promising approaches is that of error correcting out-
put codes (Dietterich and Bakiri, 1995); however,
this approach has not been able to handle well a
large number of classes (over 10 or 15, say) and its
use for most large scale NLP applications is there-
fore questionable. Statistician have studied several
schemes such as learning a single classifier for each
of the class labels (one vs. all) or learning a discrim-
inator for each pair of class labels, and discussed
their relative merits(Hastie and Tibshirani, 1998).
Although it has been argued that the latter should
provide better results than others, experimental re-
sults have been mixed (Allwein et al., 2000) and in
some cases, more involved schemes, e.g., learning a
classifier for each set of three class labels (and de-
ciding on the prediction in a tournament like fash-
ion) were shown to perform better (Teow and Loe,
2000). Moreover, none of these methods seem to be
computationally plausible for large scale problems,
since the number of classifiers one needs to train is,
at least, quadratic in the number of class labels.
Within NLP, several learning works have already
addressed the problem of multi-class classification.
In (Kudoh and Matsumoto, 2000) the methods of
“all pairs” was used to learn phrase annotations for
shallow parsing. More than different classifiers
where used in this task, making it infeasible as a
general solution. All other cases we know of, have
taken into account some properties of the domain
and, in fact, several of the works can be viewed as
instantiations of the sequential model we formalize
here, albeit done in an ad-hoc fashion.
In speech recognition, a sequential model is used
to process speech signal. Abstracting away some
details, the first classifier used is a speech signal an-
alyzer; it assigns a positive probability only to some
of the words (using Levenshtein distance (Leven-
shtein, 1966) or somewhat more sophisticated tech-
niques (Levinson et al., 1990)). These words are
then assigned probabilities using a different contex-
tual classifier e.g., a language model, and then, (as
done in most current speech recognizers) an addi-
tional sentence level classifier uses the outcome of
the word classifiers in a word lattice to choose the
most likely sentence.
Several word prediction tasks make decisions in
a sequential way as well. In spell correction con-
fusion sets are created using a classifier that takes
as input the word transcription and outputs a posi-
tive probability for potential words. In conventional
spellers, the output of this classifier is then given
to the user who selects the intended word. In con-
text sensitive spelling correction (Golding and Roth,
1999; Mangu and Brill, 1997) an additional classi-
fier is then utilized to predict among words that are
supported by the first classifier, using contextual and
lexical information of the surrounding words. In all
studies done so far, however, the first classifier – the
confusion sets – were constructed manually by the
researchers.
Other word predictions tasks have also con-
structed manually the list of confusion sets (Lee
and Pereira, 1999; Dagan et al., 1999; Lee, 1999)
and justifications where given as to why this is a
reasonable way to construct it. (Even-Zohar and
Roth, 2000) present a similar task in which the con-
fusion sets generation was automated. Their study
also quantified experimentally the advantage in us-
ing early classifiers to restrict the size of the confu-
sion set.
Many other NLP tasks, such as pos tagging,
name entity recognition and shallow parsing require
multi-class classifiers. In several of these cases the
number of classes could be very large (e.g., pos tag-
ging in some languages, pos tagging when a finer
proper noun tag is used). The sequential model sug-
gested here is a natural solution.
,
and is typically large, on the order
of . We address this problem using the
Sequential Model (SM) in which simpler classifiers
are sequentially used to filter subsets of out of
consideration.
The sequential model is formally defined as a-
tuple:
where
is the set of classifiers used by the
model, .
is a set of constant thresholds.
Given and a set of class labels,
theth classifier outputs a probability distribution1
over labels in
(where is the probability assigned to class
by), and satisfies that if then
.
The set of remaining candidates after theth clas-
sification stage is determined byand:
The sequential process can be viewed as a mul-
tiplication of distributions. (Hinton, 2000) argues
that a product of distributions (or, “experts”, PoE)
</bodyText>
<footnote confidence="0.841559666666667">
1The output of many classifiers can be viewed, after appro-
priate normalization, as a confidence measure that can be used
as our .
</footnote>
<bodyText confidence="0.997058058823529">
is an efficient way to make decisions in cases where
several different constrains play a role, and is ad-
vantageous over additive models. In fact, due to the
thresholding step, our model can be viewed as a se-
lective PoE. The thresholding ensures that the SM
has the following monotonicity property:
that is, as we evaluate the classifiers sequentially,
smaller or equal (size) confusion sets are consid-
ered. A desirable design goal for the SM is that,
w.h.p., the classifiers have one sided error (even at
the price of rejecting fewer classes). That is, if
is the true target2, then we would like to have
that . The rest of this paper presents
a concrete instantiation of the SM, and then pro-
vides a theoretical analysis of some of its properties
(Sec. 5). This work does not address the question of
acquiring SM i.e., learning .
</bodyText>
<sectionHeader confidence="0.990231" genericHeader="method">
4 Example: POS Tagging
</sectionHeader>
<bodyText confidence="0.999980518518519">
This section describes a two part experiment of pos
tagging in which we compare, under identical con-
ditions, two classification models: A SM and a sin-
gle classifier. Both are provided with the same input
features and the only difference between them is the
model structure.
In the first part, the comparison is done in the
context of assigning pos tags to unknown words –
those words which were not presented during train-
ing and therefore the learner has no baseline knowl-
edge about possible POS they may take. This ex-
periment emphasizes the advantage of using the SM
during evaluation in terms of accuracy. The second
part is done in the context of pos tagging of known
words. It compares processing time as well as accu-
racy of assigning pos tags to known words (that is,
the classifier utilizes knowledge about possible POS
tags the target word may take). This part exhibits a
large reduction in training time using the SM over
the more common one-vs-all method while the ac-
curacy of the two methods is almost identical.
Two types of features – lexical features and
contextual features may be used when learning
how to tag words for pos. Contextual features cap-
ture the information in the surrounding context and
the word lemma while the lexical features capture
the morphology of the unknown word.3 Several is-
</bodyText>
<footnote confidence="0.963457666666667">
2We use the terms class and target interchangeably.
3Lexical features are used only when tagging unknown
words.
</footnote>
<sectionHeader confidence="0.564952" genericHeader="method">
3 The Sequential Model
</sectionHeader>
<bodyText confidence="0.9995805">
We study the problem of learning a multi-class clas-
sifier, where
is the set of class labels.
is a decomposition of the do-
main (not necessarily disjoint; it could be that
).
determines the order in
which the classifiers are learned and evaluated.
For convenience we denote
sues make the pos tagging problem a natural prob-
lem to study within the SM. (i) A relatively large
number of classes (about 50). (ii) A natural decom-
position of the feature space to contextual and lexi-
cal features. (iii) Lexical knowledge (for unknown
words) and the word lemma (for known words) pro-
vide, w.h.p, one sided error (Mikheev, 1997).
</bodyText>
<subsectionHeader confidence="0.999682">
4.1 The Tagger Classifiers
</subsectionHeader>
<bodyText confidence="0.99155315">
The domain in our experiment is defined using the
following set of features, all of which are computed
relative to the target word .
Contextual Features (as in (Brill, 1995; Roth
and Zelenko,1998)):
Let be the tags of the word preceding,
(following) the target word, respectively.
.
.
.
.
.
.
.
Baseline tag for word . In case is an
unknown word, the baseline is proper singular noun
“NNP” for capitalized words and common singular
noun “NN” otherwise. (This feature is introduced
only in some of the experiments.)
9.The target word .
</bodyText>
<subsectionHeader confidence="0.742032">
Lexical Features:
</subsectionHeader>
<bodyText confidence="0.966914833333333">
Let be any three characters observed in the
examples.
10. Target word is capitalized.
.
.
.
In the following experiment, the SM used for un-
known words makes use of three different classifiers
and or , defined as follows:
The SM is compared with a single classifier – either
or . Notice that is a single classifier that
uses the same information as used by the SM. Fig 1
</bodyText>
<figureCaption confidence="0.586661">
Figure 1: POS Tagging of Unknown Word using
</figureCaption>
<bodyText confidence="0.990413296296296">
Contextual and Lexical features in a Sequential
Model. The input for capitalized classifier has 2
values and therefore 2 ways to create confusion
sets. There are at most different in-
puts for the suffix classifier (26 character + 10
digits + 5 other symbols), therefore suffix may
emit up to confusion sets.
illustrates the SM that was used in the experiments.
All the classifiers in the sequential model, as
well as the single classifier, use the SNoW learn-
ing architecture (Roth, 1998) with the Winnow up-
date rule. SNoW (Sparse Network of Winnows)
is a multi-class classifier that is specifically tai-
lored for learning in domains in which the poten-
tial number of features taking part in decisions is
very large, but in which decisions actually depend
on a small number of those features. SNoW works
by learning a sparse network of linear functions
over a pre-defined or incrementally learned feature
space. SNoW has already been used successfully on
several tasks in natural language processing (Roth,
1998; Roth and Zelenko, 1998; Golding and Roth,
1999; Punyakanok and Roth, 2001).
Specifically, for each class label SNoW learns a
function that maps a feature based
representation of the input instance to a number
which can be interpreted as the prob-
</bodyText>
<figure confidence="0.370652">
: a classifier based on the lexical feature .
: a classifier based on lexical features
: a classifier based on contextual features
.
: a classifier based on all the features, .
</figure>
<figureCaption confidence="0.994875666666667">
ends with and length(
ends with and length(
ends with and length(
</figureCaption>
<bodyText confidence="0.9800735">
ability ofbeing the class label corresponding to.
At prediction time, given , SNoW outputs
</bodyText>
<equation confidence="0.94856">
(1)
</equation>
<bodyText confidence="0.99680014893617">
All functions – in our case, target nodes are
used, one for each pos tag – reside over the same
feature space, but can be thought of as autonomous
functions (networks). That is, a given example is
treated autonomously by each target subnetwork; an
example labeledis considered as a positive exam-
ple for the function learned forand as a negative
example for the rest of the functions (target nodes).
The network is sparse in that a target node need not
be connected to all nodes in the input layer. For ex-
ample, it is not connected to input nodes (features)
that were never active with it in the same sentence.
Although SNoW is used with different targets,
the SM utilizes by determining the confusion set dy-
namically. That is, in evaluation (prediction), the
maximum in Eq. 1 is taken only over the currently
applicable confusion set. Moreover, in training, a
given example is used to train only target networks
that are in the currently applicable confusion set.
That is, an example that is positive for target, is
viewed as positive for this target (if it is in the con-
fusion set), and as negative for the other targets in
the confusion set. All other targets do not see this
example.
The case of POS tagging of known words is han-
dled in a similar way. In this case, all possible tags
are known. In training, we record, for each word ,
all pos tags with which it was tagged in the training
corpus. During evaluation, whenever word oc-
curs, it is tagged with one of these pos tags. That
is, in evaluation, the confusion set consists only of
those tags observed with the target word in train-
ing, and the maximum in Eq. 1 is taken only over
these. This is always the case when using (or ),
both in the SM and as a single classifier. In training,
though, for the sake of this experiment, we treat
( ) differently depending on whether it is trained
for the SM or as a single classifier. When trained as
a single classifier (e.g., (Roth and Zelenko, 1998)),
uses each-tagged example as a positive exam-
ple forand a negative example for all other tags.
On the other hand, the SM classifier is trained on a
-tagged example of word , by using it as a posi-
tive example forand a negative example only for
the effective confusion set. That is, those pos tags
which have been observed as tags of in the train-
ing corpus.
</bodyText>
<subsectionHeader confidence="0.976098">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999831428571429">
The data for the experiments was extracted from the
Penn Treebank WSJ and Brown corpora. The train-
ing corpus consists of words. The test
corpus consists of words of which
are unknown words (that is, they do not occur in the
training corpus. (Numbers (the pos “CD”), are not
included among the unknown words).
</bodyText>
<table confidence="0.614051">
POS Tagging of Unknown Words
+ baseline baseline
</table>
<tableCaption confidence="0.931839666666667">
Table 1: POS tagging of unknown words using
contextual features (accuracy in percent). is
a classifier that uses only contextual features, +
</tableCaption>
<bodyText confidence="0.95664365">
baseline is the same classifier with the addition of
the baseline feature (“NNP” or “NN”).
Table 1 summarizes the results of the experiments
with a single classifier that uses only contextual fea-
tures. Notice that adding the baseline POS signifi-
cantly improves the results but not much is gained
over the baseline. The reason is that the baseline
feature is almost perfect ( ) in the training
data. For that reason, in the next experiments we
do not use the baseline at all, since it could hide
the phenomenon addressed. (In practice, one might
want to use a more sophisticated baseline, as in
(Dermatas and Kokkinakis, 1995).)
SM( ) SM( )
Table 2: POS tagging of unknown words using
contextual and lexical Features (accuracy in per-
cent). is based only on contextual features, is
based on contextual and lexical features. SM( )
denotes thatfollowsin the sequential model.
Table 2 summarizes the results of the main exper-
iment in this part. It exhibits the advantage of using
the SM (columns 3,4) over a single classifier that
makes use of the same features set (column 2). In
both cases, all features are used. In , a classifier
is trained on input that consists of all these features
and chooses a label from among all class labels. In
the same features are used as input,
but different classifiers are used sequentially – using
only part of the feature space and restricting the set
of possible outcomes available to the next classifier
in the sequence –chooses only from among those
left as candidates.
It is interesting to note that further improvement
can be achieved, as shown in the right most column.
Given that the last stage in is iden-
tical to the single classifier , this shows the con-
tribution of the filtering done in the first two stages
usingand. In addition, this result shows that
the input spaces of the classifiers need not be dis-
joint.
</bodyText>
<subsectionHeader confidence="0.918146">
POS Tagging of Known Words
</subsectionHeader>
<bodyText confidence="0.9994795">
Essentially everyone who is learning a POS tagger
for known words makes use of a “sequential model”
assumption during evaluation – by restricting the
set of candidates, as discussed in Sec 4.1). The fo-
cus of this experiment is thus to investigate the ad-
vantage of the SM during training. In this case, a
single (one-vs-all) classifier trains each tag against
all other tags, while a SM classifier trains it only
against the effective confusion set (Sec 4.1).
Table 3 compares the performance of theclas-
sifier trained using in a one-vs-all method to the
same classifier trained the SM way. The results are
only for known words and the results of Brill’s tag-
ger (Brill, 1995) are presented for comparison.
</bodyText>
<table confidence="0.404945">
one-vs-all SM Brill
</table>
<tableCaption confidence="0.9337465">
Table 3: POS Tagging of known words using con-
textual features (accuracy in percent). one-vs-all
</tableCaption>
<bodyText confidence="0.994522125">
denotes training where example serves as positive
example to the true tag and as negative example to
all the other tags. SM denotes training where
example serves as positive example to the true tag
and as a negative example only to a restricted set of
tags in based on a previous classifier – here, a sim-
ple baseline restriction.
While, in principle, (see Sec 5) the SM should do
better (an never worse) than the one-vs-all classifier,
we believe that in this case SM does not have any
performance advantages since the classifiers work
in a very high dimensional feature space which al-
lows the one-vs-all classifier to find a separating hy-
perplane that separates the positive examples many
different kinds of negative examples (even irrelevant
ones).
However, the key advantage of the SM in this
case is the significant decrease in computation time,
both in training and evaluation. Table 4 shows that
in the pos tagging task, training using the SM is 6
times faster than with a one-vs-all method and 3000
faster than Brill’s learner. In addition, the evaluation
time of our tagger was about twice faster than that
of Brill’s tagger.
</bodyText>
<table confidence="0.929428666666667">
one-vs-all SM Brill
Train
Test
</table>
<tableCaption confidence="0.996964">
Table 4: Processing time for POS tagging of
known words using contextual features (In CPU
</tableCaption>
<bodyText confidence="0.9932334">
seconds). Train: training time over sentences.
Brill’s learner was interrupted after 12 days of train-
ing (default threshold was used). Test: average
number of seconds to evaluate a single sentence. All
runs were done on the same machine.
</bodyText>
<sectionHeader confidence="0.989875" genericHeader="method">
5 The Sequential model: Theoretical
Justification
</sectionHeader>
<bodyText confidence="0.990470666666667">
In this section, we discuss some of the theoretical
aspects of the SM and explain some of its advan-
tages. In particular, we discuss the following issues:
</bodyText>
<listItem confidence="0.9228115">
1. Domain Decomposition: When the input fea-
ture space can be decomposed, we show that it
is advantageous to do it and learn several clas-
sifiers, each on a smaller domain.
2. Range Decomposition: Reducing confusion
set size is advantageous both in training and
testing the classifiers.
(a) Test: Smaller confusion set is shown to
yield a smaller expected error.
(b) Training: Under the assumptions that a
small confusion set (determined dynam-
ically by previous classifiers in the se-
quence) is used when a classifier is eval-
uated, it is shown that training the classi-
fiers this way is advantageous.
3. Expressivity: SM can be viewed as a way to
</listItem>
<bodyText confidence="0.9622783">
generate an expressive classifier by building
on a number of simpler ones. We argue that
the SM way of generating an expressive clas-
sifier has advantages over other ways of doing
it, such as decision tree. (Sec 5.3).
In addition, SM has several significant computa-
tional advantages both in training and in test, since
it only needs to consider a subset of the set of can-
didate class labels. We will not discuss these issues
in detail here.
</bodyText>
<subsectionHeader confidence="0.998349">
5.1 Decomposing the Domain
</subsectionHeader>
<bodyText confidence="0.999842513513514">
Decomposing the domain is not an essential part of
the SM; it is possible that all the classifiers used ac-
tually use the same domain. As we shown below,
though, when a decomposition is possible, it is ad-
vantageous to use it.
It is shown in Eq. 2-7 that when it is possible to
decompose the domain to subsets that are condition-
ally independent given the class label, the SM with
classifiers defined on these subsets is as accurate as
the optimal single classifier. (In fact, this is shown
for a pure product of simpler classifiers; the SM uses
a selective product.)
In the following we assume that
provide a decomposition of the domain (Sec. 3)
and that . By condi-
tional independence we mean that
in Eq. 3 is identical and there-
fore can be treated as a constant. Eq. 5 is derived by
applying the independence assumption. Eq. 6 is de-
rived by using the Bayes rule for each term
separately.
We note that although the conditional indepen-
dence assumption is a strong one, it is a reasonable
assumption in many NLP applications; in particu-
lar, when cross modality information is used, this
assumption typically holds for decomposition that
is done across modalities. For example, in POS tag-
ging, lexical information is often conditionally in-
dependent of contextual information, given the true
POS. (E.g., assume that word is a gerund; then the
context is independent of the “ing” word ending.)
In addition, decomposing the domain has signif-
icant advantages from the learning theory point of
view (Roth, 1999). Learning over domains of lower
dimensionality implies better generalization bounds
or, equivalently, more accurate classifiers for a fixed
size training set.
</bodyText>
<subsectionHeader confidence="0.99976">
5.2 Decomposing the range
</subsectionHeader>
<bodyText confidence="0.999768666666667">
The SM attempts to reduce the size of the candidates
set. We justify this by considering two cases: (i)
Test: we will argue that prediction among a smaller
set of classes has advantages over predicting among
a large set of classes; (ii) Training: we will argue
that it is advantageous to ignore irrelevant examples.
</bodyText>
<subsectionHeader confidence="0.894069">
5.2.1 Decomposing the range during Test
</subsectionHeader>
<bodyText confidence="0.892538857142857">
The following discussion formalizes the intuition
that a smaller confusion set in preferred. Let
be the true target function and the
probability assigned by the final classifier to class
given example . Assuming that
the prediction is done, naturally, by choosing the
most likely class label, we see that the expected er-
ror when using a confusion set of size is:
(8)
Now we have:
Claim 1 Let
be two sets of class labels and assume
for example. Then .
Proof. Denote:
Then,
where is the input for theth classifier.
(2)
Claim 1 shows that reducing the size of the con-
fusion set can only help; this holds under the as-
sumption that the true class label is not eliminated
from consideration by down stream classifiers, that
is, under the one-sided error assumption. Moreover,
it is easy to see that the proof of Claim 1 allows us
to relax the one sided error assumption and assume
instead that the previous classifiers err with a prob-
ability which is smaller than:
, and the hypothesis produced by . Then,
for all
</bodyText>
<figure confidence="0.736005">
(9)
,
In the limit, as
</figure>
<subsectionHeader confidence="0.943442">
5.2.2 Decomposing the range during training
</subsectionHeader>
<bodyText confidence="0.997042138888889">
We will assume now, as suggested by the previous
discussion, that in the evaluation stage the small-
est possible set of candidates will be considered by
each classifier. Based on this assumption, Claim 2
shows that training this way is advantageous. That
is, that utilizing the SM in training yields a better
classifier.
Let be a learning algorithm that is trained to
minimize:
where is an example, is the true
class, is the hypothesis, is a loss function and
is the probability of seeing example when
(see (Allwein et al., 2000)). (Notice that in
this section we are using general loss function; we
could use, in particular, binary loss function used
in Sec 5.2.) We phrase and prove the next claim,
w.l.o.g, the case of vs. class labels.
Claim 2 Let be the set of class la-
bels, letbe the set ofexamples for class. Assume
a sequential model in which class does not com-
pete with class . That is, whenever the
SM filters out such that the final classifier (✟❆)
considers only and . Then, the error of the hy-
pothesis - produced by algorithm (for❆) - when
trained on examples in is no larger than
the error produced by the hypothesis it produces
when trained on examples in.
Proof. Assume that the algorithm , when
trained on a sample, produces a hypothesis that
minimizes the empirical error over.
Denote when is sampled according to
a distribution that supports only examples with label
in . Let be a sample set of size , according to
In particular this holds if is a hypothesis pro-
duced by when trained on, that is sampled ac-
cording to .
</bodyText>
<subsectionHeader confidence="0.891079">
5.3 Expressivity
</subsectionHeader>
<bodyText confidence="0.98506335">
The SM is a decision process that is conceptually
similar to a decision tree processes (Rasoul and
Landgrebe, 1991; Mitchell, 1997), especially if one
allows more general classifiers in the decision tree
nodes. In this section we show that (i) the SM can
express any DT. (ii) the SM is more compact than a
decision tree even when the DT makes used of more
expressive internal nodes (Murthy et al., 1994).
The next theorem shows that for a fixed set of
functions (queries) over the input features, any bi-
nary decision tree can be represented as a SM. Ex-
tending the proof beyond binary decision trees is
straight-forward.
Theorem 3 Let be a binary decision tree with
internal nodes. Then, there exist a sequential model
such that and have the same size, and they
produce the same predictions.
Proof (Sketch): Given a decision tree on
nodes we show how to construct a SM that produces
equivalent predictions.
</bodyText>
<listItem confidence="0.938679166666667">
1. Generate a confusion set the consists of
classes, each representing an internal node in
.
2. For each internal node in , assign a clas-
sifier: .
3. Order the classifiers such that a clas-
</listItem>
<bodyText confidence="0.809527">
sifier that is assigned to node is processed
before any classifier that was assigned to any
of the children of.
4. Define each classifierthat was assigned to
node to have an influence on the
outcome iff node lies in the path
( ) from the root to the predicted
class.
</bodyText>
<listItem confidence="0.850106">
5. Show that using steps 1-4, the predicted target
of and are identical.
</listItem>
<bodyText confidence="0.9985163">
This completes that proof and shows that the result-
ing SM is of equivalent size to the original decision
tree.
We note that given a SM, it is also relatively easy
(details omitted) to construct a decision tree that
produces the same decisions as the final classifier of
the SM. However, the simple construction results in
a decision tree that is exponentially larger than the
original SM. Theorem 4 shows that this difference
in expressivity is inherent.
</bodyText>
<construct confidence="0.8238115">
Theorem 4 Let be the number of classifiers in
a sequential model and the number of internal
nodes a in decision tree . Let be the set
of classes in the output of and also the maxi-
mum degree of the internal nodes in . Denote by
the number offunctions representable
by respectively. Then, when
is exponentially larger than .
</construct>
<bodyText confidence="0.992926555555556">
Proof (Sketch): The proof follows by counting
the number of functions that can be represented
using a decision tree with internal nodes(Wilf,
1994), and the number of functions that can be rep-
resented using a sequential model on intermedi-
ate classifier. Given the exponential gap, it follows
that one may need exponentially large decision trees
to represent an equivalent predictor to an size
SM.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985884615385">
A wide range and a large number of classifica-
tion tasks will have to be used in order to perform
any high level natural language inference such as
speech recognition, machine translation or question
answering. Although in each instantiation the real
conflict could be only to choose among a small set
of candidates, the original set of candidates could be
very large; deriving the small set of candidates that
are relevant to the task at hand may not be immedi-
ate.
This paper addressed this problem by developing
a general paradigm for multi-class classification that
sequentially restricts the set of candidate classes to
a small set, in a way that is driven by the data ob-
served. We have described the method and provided
some justifications for its advantages, especially in
NLP-like domains. Preliminary experiments also
show promise.
Several issues are still missing from this work.
In our experimental study the decomposition of the
feature space was done manually; it would be nice
to develop methods to do this automatically. Bet-
ter understanding of methods for thresholding the
probability distributions that the classifiers output,
as well as principled ways to order them are also
among the future directions of this research.
</bodyText>
<sectionHeader confidence="0.993983" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969040526785714">
L. E. Allwein, R. E. Schapire, and Y. Singer. 2000.
Reducing multiclass to binary: a unifying ap-
proach for margin classifiers. In Proceedings
of the 17th International Workshop on Machine
Learning, pages 9–16.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543–565.
E. Charniak. 1993. Statistical Language Learning.
MIT Press.
I. Dagan, L. Lee, and F. Pereira. 1999. Similarity-
based models of word cooccurrence probabilities.
Machine Learning, 34(1-3):43–69.
E. Dermatas and G. Kokkinakis. 1995. Automatic
stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137–164.
T. G. Dietterich and G. Bakiri. 1995. Solving multi-
class learning problems via error-correcting out-
put codes. Journal of Artificial Intelligence Re-
search, 2:263–286.
Y. Even-Zohar and D. Roth. 2000. A classification
approach to word prediction. In NAALP 2000,
pages 124–131.
W. A. Gale, K. W. Church, and D. Yarowsky. 1993.
A method for disambiguating word senses in a
large corpus. Computers and the Humanities,
26:415–439.
A. R. Golding and D. Roth. 1999. A Winnow based
approach to context-sensitive spelling correction.
Machine Learning, 34(1-3):107–130. Special Is-
sue on Machine Learning and Natural Language.
A. R. Golding. 1995. A Bayesian hybrid method
for context-sensitive spelling correction. In Pro-
,
ceedings of the 3rd workshop on very large cor-
pora, ACL-95.
T. Hastie and R. Tibshirani. 1998. Classifica-
tion by pairwise coupling. In Michael I. Jordan,
Michael J. Kearns, and Sara A. Solla, editors,
Advances in Neural Information Processing Sys-
tems, volume 10. The MIT Press.
G. Hinton. 2000. Training products of experts
by minimizing contrastive divergence. Technical
Report GCNU TR 2000-004, University College
London.
T. Kudoh and Y. Matsumoto. 2000. Use of sup-
port vector machines for chunk identification. In
CoNLL, pages 142–147, Lisbon, Protugal.
L. Lee and F. Pereira. 1999. Distributional similar-
ity models: Clustering vs. nearest neighbors. In
ACL 99, pages 33–40.
L. Lee. 1999. Measure of distributional similarity.
In ACL 99, pages 25–32.
V.I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. In
Sov. Phys-Dokl, volume 10, pages 707–710.
S.E. Levinson, A. Ljolje, and L.G. Miller. 1990.
Continuous speech recognition from phonetic
transcription. In Speech and Natural Language
Workshop, pages 190–199.
L. Mangu and E. Brill. 1997. Automatic rule ac-
quisition for spelling correction. In Proc. of the
International Conference on Machine Learning,
pages 734–741.
A. Mikheev. 1997. Automatic rule induction for
unknown word guessing. In Computational Lin-
guistic, volume 23(3), pages 405–423.
T. M. Mitchell. 1997. Machine Learning. Mcgraw-
Hill.
S. Murthy, S. Kasif, and S. Salzberg. 1994. A sys-
tem for induction of oblique decision trees. Jour-
nal ofArtificial Intelligence Research, 2:1:1–33.
V. Punyakanok and D. Roth. 2001. The use of clas-
sifiers in sequential inference. In NIPS-13; The
2000 Conference on Advances in Neural Infor-
mation Processing Systems.
S. S. Rasoul and D. A. Landgrebe. 1991. A sur-
vey of decision tree classifier methodology. IEEE
Transactions on Systems, Man, and Cybernetics,
21 (3):660–674.
D. Roth and D. Zelenko. 1998. Part of speech
tagging using a network of linear separators.
In COLING-ACL 98, The 17th International
Conference on Computational Linguistics, pages
1136–1142.
D. Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In Proc.
National Conference on Artificial Intelligence,
pages 806–813.
D. Roth. 1999. Learning in natural language. In
Proc. Int’l Joint Conference on Artificial Intelli-
gence, pages 898–904.
L-W. Teow and K-F. Loe. 2000. Handwritten digit
recognition with a novel vision model that ex-
tracts linearly separable features. In CVPR’00,
The IEEE Conference on Computer Vision and
Pattern Recognition, pages 76–81.
H. S. Wilf. 1994. generatingfunctionology. Aca-
demic Press Inc., Boston, MA, second edition.
D. Yarowsky. 1994. Decision lists for lexical ambi-
guity resolution: application to accent restoration
in Spanish and French. In Proc. of the Annual
Meeting of the ACL, pages 88–95.
J. Zavrel, W. Daelemans, and J. Veenstra. 1997.
Resolving pp attachment ambiguities with mem-
ory based learning. In Computational Natural
Language Learning, Madrid, Spain, July.
J. M. Zelle and R. J. Mooney. 1996. Learning to
parse database queries using inductive logic pro-
gramming. In Proc. National Conference on Ar-
tificial Intelligence, pages 1050–1055.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.969215">
<title confidence="0.999983">A Sequential Model for Multi-Class Classification</title>
<author confidence="0.999826">Yair Even-Zohar Dan Roth</author>
<affiliation confidence="0.999915">Department of Computer Science University of Illinois at Urbana-Champaign</affiliation>
<email confidence="0.998715">evenzoha,danr@uiuc.edu</email>
<abstract confidence="0.998049">Many classification problems require decisions among a large number of competing classes. These tasks, however, are not handled well by general purpose learning methods and are usually addressed in an ad-hoc fashion. We suggest a general approach – a sequential learning model that utilizes classifiers to sequentially restrict the number of competing classes while maintaining, with high probability, the presence of the true outcome in the candidates set. Some theoretical and computational properties of the model are discussed and we argue that these are important in NLP-like domains. The advantages of the model are illustrated in an experiment in partof-speech tagging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L E Allwein</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Reducing multiclass to binary: a unifying approach for margin classifiers.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th International Workshop on Machine Learning,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="5915" citStr="Allwein et al., 2000" startWordPosition="936" endWordPosition="939">ing output codes (Dietterich and Bakiri, 1995); however, this approach has not been able to handle well a large number of classes (over 10 or 15, say) and its use for most large scale NLP applications is therefore questionable. Statistician have studied several schemes such as learning a single classifier for each of the class labels (one vs. all) or learning a discriminator for each pair of class labels, and discussed their relative merits(Hastie and Tibshirani, 1998). Although it has been argued that the latter should provide better results than others, experimental results have been mixed (Allwein et al., 2000) and in some cases, more involved schemes, e.g., learning a classifier for each set of three class labels (and deciding on the prediction in a tournament like fashion) were shown to perform better (Teow and Loe, 2000). Moreover, none of these methods seem to be computationally plausible for large scale problems, since the number of classifiers one needs to train is, at least, quadratic in the number of class labels. Within NLP, several learning works have already addressed the problem of multi-class classification. In (Kudoh and Matsumoto, 2000) the methods of “all pairs” was used to learn phr</context>
<context position="27418" citStr="Allwein et al., 2000" startWordPosition="4631" endWordPosition="4634">oduced by . Then, for all (9) , In the limit, as 5.2.2 Decomposing the range during training We will assume now, as suggested by the previous discussion, that in the evaluation stage the smallest possible set of candidates will be considered by each classifier. Based on this assumption, Claim 2 shows that training this way is advantageous. That is, that utilizing the SM in training yields a better classifier. Let be a learning algorithm that is trained to minimize: where is an example, is the true class, is the hypothesis, is a loss function and is the probability of seeing example when (see (Allwein et al., 2000)). (Notice that in this section we are using general loss function; we could use, in particular, binary loss function used in Sec 5.2.) We phrase and prove the next claim, w.l.o.g, the case of vs. class labels. Claim 2 Let be the set of class labels, letbe the set ofexamples for class. Assume a sequential model in which class does not compete with class . That is, whenever the SM filters out such that the final classifier (✟❆) considers only and . Then, the error of the hypothesis - produced by algorithm (for❆) - when trained on examples in is no larger than the error produced by the hypothesi</context>
</contexts>
<marker>Allwein, Schapire, Singer, 2000</marker>
<rawString>L. E. Allwein, R. E. Schapire, and Y. Singer. 2000. Reducing multiclass to binary: a unifying approach for margin classifiers. In Proceedings of the 17th International Workshop on Machine Learning, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="2024" citStr="Brill, 1995" startWordPosition="296" endWordPosition="297">and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these. While this approach is important in that it allows the research community to develop bette</context>
<context position="13044" citStr="Brill, 1995" startWordPosition="2140" endWordPosition="2141">e classifiers are learned and evaluated. For convenience we denote sues make the pos tagging problem a natural problem to study within the SM. (i) A relatively large number of classes (about 50). (ii) A natural decomposition of the feature space to contextual and lexical features. (iii) Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997). 4.1 The Tagger Classifiers The domain in our experiment is defined using the following set of features, all of which are computed relative to the target word . Contextual Features (as in (Brill, 1995; Roth and Zelenko,1998)): Let be the tags of the word preceding, (following) the target word, respectively. . . . . . . . Baseline tag for word . In case is an unknown word, the baseline is proper singular noun “NNP” for capitalized words and common singular noun “NN” otherwise. (This feature is introduced only in some of the experiments.) 9.The target word . Lexical Features: Let be any three characters observed in the examples. 10. Target word is capitalized. . . . In the following experiment, the SM used for unknown words makes use of three different classifiers and or , defined as follows</context>
<context position="20818" citStr="Brill, 1995" startWordPosition="3506" endWordPosition="3507">ords makes use of a “sequential model” assumption during evaluation – by restricting the set of candidates, as discussed in Sec 4.1). The focus of this experiment is thus to investigate the advantage of the SM during training. In this case, a single (one-vs-all) classifier trains each tag against all other tags, while a SM classifier trains it only against the effective confusion set (Sec 4.1). Table 3 compares the performance of theclassifier trained using in a one-vs-all method to the same classifier trained the SM way. The results are only for known words and the results of Brill’s tagger (Brill, 1995) are presented for comparison. one-vs-all SM Brill Table 3: POS Tagging of known words using contextual features (accuracy in percent). one-vs-all denotes training where example serves as positive example to the true tag and as negative example to all the other tags. SM denotes training where example serves as positive example to the true tag and as a negative example only to a restricted set of tags in based on a previous classifier – here, a simple baseline restriction. While, in principle, (see Sec 5) the SM should do better (an never worse) than the one-vs-all classifier, we believe that i</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1759" citStr="Charniak, 1993" startWordPosition="258" endWordPosition="259">lect a class label from among a collection of candidates. Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), m</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>E. Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similaritybased models of word cooccurrence probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="8388" citStr="Dagan et al., 1999" startWordPosition="1339" endWordPosition="1342">spellers, the output of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar task in which the confusion sets generation was automated. Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set. Many other NLP tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers. In several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when a finer proper noun tag is us</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1999. Similaritybased models of word cooccurrence probabilities. Machine Learning, 34(1-3):43–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dermatas</author>
<author>G Kokkinakis</author>
</authors>
<title>Automatic stochastic tagging of natural language texts.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="18887" citStr="Dermatas and Kokkinakis, 1995" startWordPosition="3167" endWordPosition="3170">s, + baseline is the same classifier with the addition of the baseline feature (“NNP” or “NN”). Table 1 summarizes the results of the experiments with a single classifier that uses only contextual features. Notice that adding the baseline POS significantly improves the results but not much is gained over the baseline. The reason is that the baseline feature is almost perfect ( ) in the training data. For that reason, in the next experiments we do not use the baseline at all, since it could hide the phenomenon addressed. (In practice, one might want to use a more sophisticated baseline, as in (Dermatas and Kokkinakis, 1995).) SM( ) SM( ) Table 2: POS tagging of unknown words using contextual and lexical Features (accuracy in percent). is based only on contextual features, is based on contextual and lexical features. SM( ) denotes thatfollowsin the sequential model. Table 2 summarizes the results of the main experiment in this part. It exhibits the advantage of using the SM (columns 3,4) over a single classifier that makes use of the same features set (column 2). In both cases, all features are used. In , a classifier is trained on input that consists of all these features and chooses a label from among all class</context>
</contexts>
<marker>Dermatas, Kokkinakis, 1995</marker>
<rawString>E. Dermatas and G. Kokkinakis. 1995. Automatic stochastic tagging of natural language texts. Computational Linguistics, 21(2):137–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
<author>G Bakiri</author>
</authors>
<title>Solving multiclass learning problems via error-correcting output codes.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--263</pages>
<contexts>
<context position="5340" citStr="Dietterich and Bakiri, 1995" startWordPosition="840" endWordPosition="843">ing, and provide some theoretical justifications to the approach. Sec. 2 provides some background on approaches to multi-class classification in machine learning and in NLP. In Sec. 3 we describe the sequential model proposed here and in Sec. 4 we describe an experiment the exhibits some of its advantages. Some theoretical justifications are outlined in Sec. 5. 2 Multi-Class Classification Several works within the machine learning community have attempted to develop general approaches to multi-class classification. One of the most promising approaches is that of error correcting output codes (Dietterich and Bakiri, 1995); however, this approach has not been able to handle well a large number of classes (over 10 or 15, say) and its use for most large scale NLP applications is therefore questionable. Statistician have studied several schemes such as learning a single classifier for each of the class labels (one vs. all) or learning a discriminator for each pair of class labels, and discussed their relative merits(Hastie and Tibshirani, 1998). Although it has been argued that the latter should provide better results than others, experimental results have been mixed (Allwein et al., 2000) and in some cases, more </context>
</contexts>
<marker>Dietterich, Bakiri, 1995</marker>
<rawString>T. G. Dietterich and G. Bakiri. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Even-Zohar</author>
<author>D Roth</author>
</authors>
<title>A classification approach to word prediction.</title>
<date>2000</date>
<booktitle>In NAALP</booktitle>
<pages>124--131</pages>
<contexts>
<context position="8511" citStr="Even-Zohar and Roth, 2000" startWordPosition="1360" endWordPosition="1363">ve spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar task in which the confusion sets generation was automated. Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set. Many other NLP tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers. In several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when a finer proper noun tag is used). The sequential model suggested here is a natural solution. , and is typically large, on the order of . We address this</context>
</contexts>
<marker>Even-Zohar, Roth, 2000</marker>
<rawString>Y. Even-Zohar and D. Roth. 2000. A classification approach to word prediction. In NAALP 2000, pages 124–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1993</date>
<pages>26--415</pages>
<contexts>
<context position="1669" citStr="Gale et al., 1993" startWordPosition="244" endWordPosition="247">text. These, in turn, can all be viewed as classification problems in which the goal is to select a class label from among a collection of candidates. Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learn</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>W. A. Gale, K. W. Church, and D. Yarowsky. 1993. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning, 34(1-3):107–130. Special Issue on Machine Learning and Natural Language.</booktitle>
<contexts>
<context position="7931" citStr="Golding and Roth, 1999" startWordPosition="1265" endWordPosition="1268">language model, and then, (as done in most current speech recognizers) an additional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence. Several word prediction tasks make decisions in a sequential way as well. In spell correction confusion sets are created using a classifier that takes as input the word transcription and outputs a positive probability for potential words. In conventional spellers, the output of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar t</context>
<context position="14894" citStr="Golding and Roth, 1999" startWordPosition="2457" endWordPosition="2460">fier, use the SNoW learning architecture (Roth, 1998) with the Winnow update rule. SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features. SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully on several tasks in natural language processing (Roth, 1998; Roth and Zelenko, 1998; Golding and Roth, 1999; Punyakanok and Roth, 2001). Specifically, for each class label SNoW learns a function that maps a feature based representation of the input instance to a number which can be interpreted as the prob: a classifier based on the lexical feature . : a classifier based on lexical features : a classifier based on contextual features . : a classifier based on all the features, . ends with and length( ends with and length( ends with and length( ability ofbeing the class label corresponding to. At prediction time, given , SNoW outputs (1) All functions – in our case, target nodes are used, one for eac</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130. Special Issue on Machine Learning and Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
</authors>
<title>A Bayesian hybrid method for context-sensitive spelling correction.</title>
<date>1995</date>
<booktitle>In Pro, ceedings of the 3rd workshop on very large corpora, ACL-95.</booktitle>
<contexts>
<context position="1736" citStr="Golding, 1995" startWordPosition="254" endWordPosition="256">hich the goal is to select a class label from among a collection of candidates. Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification probl</context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>A. R. Golding. 1995. A Bayesian hybrid method for context-sensitive spelling correction. In Pro, ceedings of the 3rd workshop on very large corpora, ACL-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hastie</author>
<author>R Tibshirani</author>
</authors>
<title>Classification by pairwise coupling.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>10</volume>
<editor>In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5767" citStr="Hastie and Tibshirani, 1998" startWordPosition="912" endWordPosition="915">earning community have attempted to develop general approaches to multi-class classification. One of the most promising approaches is that of error correcting output codes (Dietterich and Bakiri, 1995); however, this approach has not been able to handle well a large number of classes (over 10 or 15, say) and its use for most large scale NLP applications is therefore questionable. Statistician have studied several schemes such as learning a single classifier for each of the class labels (one vs. all) or learning a discriminator for each pair of class labels, and discussed their relative merits(Hastie and Tibshirani, 1998). Although it has been argued that the latter should provide better results than others, experimental results have been mixed (Allwein et al., 2000) and in some cases, more involved schemes, e.g., learning a classifier for each set of three class labels (and deciding on the prediction in a tournament like fashion) were shown to perform better (Teow and Loe, 2000). Moreover, none of these methods seem to be computationally plausible for large scale problems, since the number of classifiers one needs to train is, at least, quadratic in the number of class labels. Within NLP, several learning wor</context>
</contexts>
<marker>Hastie, Tibshirani, 1998</marker>
<rawString>T. Hastie and R. Tibshirani. 1998. Classification by pairwise coupling. In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances in Neural Information Processing Systems, volume 10. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2000</date>
<tech>Technical Report GCNU TR 2000-004,</tech>
<institution>University College London.</institution>
<contexts>
<context position="9738" citStr="Hinton, 2000" startWordPosition="1569" endWordPosition="1570"> the Sequential Model (SM) in which simpler classifiers are sequentially used to filter subsets of out of consideration. The sequential model is formally defined as atuple: where is the set of classifiers used by the model, . is a set of constant thresholds. Given and a set of class labels, theth classifier outputs a probability distribution1 over labels in (where is the probability assigned to class by), and satisfies that if then . The set of remaining candidates after theth classification stage is determined byand: The sequential process can be viewed as a multiplication of distributions. (Hinton, 2000) argues that a product of distributions (or, “experts”, PoE) 1The output of many classifiers can be viewed, after appropriate normalization, as a confidence measure that can be used as our . is an efficient way to make decisions in cases where several different constrains play a role, and is advantageous over additive models. In fact, due to the thresholding step, our model can be viewed as a selective PoE. The thresholding ensures that the SM has the following monotonicity property: that is, as we evaluate the classifiers sequentially, smaller or equal (size) confusion sets are considered. A </context>
</contexts>
<marker>Hinton, 2000</marker>
<rawString>G. Hinton. 2000. Training products of experts by minimizing contrastive divergence. Technical Report GCNU TR 2000-004, University College London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudoh</author>
<author>Y Matsumoto</author>
</authors>
<title>Use of support vector machines for chunk identification.</title>
<date>2000</date>
<booktitle>In CoNLL,</booktitle>
<pages>142--147</pages>
<location>Lisbon, Protugal.</location>
<contexts>
<context position="6466" citStr="Kudoh and Matsumoto, 2000" startWordPosition="1026" endWordPosition="1029">ts than others, experimental results have been mixed (Allwein et al., 2000) and in some cases, more involved schemes, e.g., learning a classifier for each set of three class labels (and deciding on the prediction in a tournament like fashion) were shown to perform better (Teow and Loe, 2000). Moreover, none of these methods seem to be computationally plausible for large scale problems, since the number of classifiers one needs to train is, at least, quadratic in the number of class labels. Within NLP, several learning works have already addressed the problem of multi-class classification. In (Kudoh and Matsumoto, 2000) the methods of “all pairs” was used to learn phrase annotations for shallow parsing. More than different classifiers where used in this task, making it infeasible as a general solution. All other cases we know of, have taken into account some properties of the domain and, in fact, several of the works can be viewed as instantiations of the sequential model we formalize here, albeit done in an ad-hoc fashion. In speech recognition, a sequential model is used to process speech signal. Abstracting away some details, the first classifier used is a speech signal analyzer; it assigns a positive pro</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>T. Kudoh and Y. Matsumoto. 2000. Use of support vector machines for chunk identification. In CoNLL, pages 142–147, Lisbon, Protugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Distributional similarity models: Clustering vs. nearest neighbors.</title>
<date>1999</date>
<booktitle>In ACL 99,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="8368" citStr="Lee and Pereira, 1999" startWordPosition="1335" endWordPosition="1338">words. In conventional spellers, the output of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar task in which the confusion sets generation was automated. Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set. Many other NLP tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers. In several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when a finer p</context>
</contexts>
<marker>Lee, Pereira, 1999</marker>
<rawString>L. Lee and F. Pereira. 1999. Distributional similarity models: Clustering vs. nearest neighbors. In ACL 99, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lee</author>
</authors>
<title>Measure of distributional similarity.</title>
<date>1999</date>
<booktitle>In ACL 99,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="8400" citStr="Lee, 1999" startWordPosition="1343" endWordPosition="1344"> of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar task in which the confusion sets generation was automated. Their study also quantified experimentally the advantage in using early classifiers to restrict the size of the confusion set. Many other NLP tasks, such as pos tagging, name entity recognition and shallow parsing require multi-class classifiers. In several of these cases the number of classes could be very large (e.g., pos tagging in some languages, pos tagging when a finer proper noun tag is used). The seq</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>L. Lee. 1999. Measure of distributional similarity. In ACL 99, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<booktitle>In Sov. Phys-Dokl,</booktitle>
<volume>10</volume>
<pages>707--710</pages>
<contexts>
<context position="7148" citStr="Levenshtein, 1966" startWordPosition="1140" endWordPosition="1142"> for shallow parsing. More than different classifiers where used in this task, making it infeasible as a general solution. All other cases we know of, have taken into account some properties of the domain and, in fact, several of the works can be viewed as instantiations of the sequential model we formalize here, albeit done in an ad-hoc fashion. In speech recognition, a sequential model is used to process speech signal. Abstracting away some details, the first classifier used is a speech signal analyzer; it assigns a positive probability only to some of the words (using Levenshtein distance (Levenshtein, 1966) or somewhat more sophisticated techniques (Levinson et al., 1990)). These words are then assigned probabilities using a different contextual classifier e.g., a language model, and then, (as done in most current speech recognizers) an additional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence. Several word prediction tasks make decisions in a sequential way as well. In spell correction confusion sets are created using a classifier that takes as input the word transcription and outputs a positive probability for potential w</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V.I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. In Sov. Phys-Dokl, volume 10, pages 707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Levinson</author>
<author>A Ljolje</author>
<author>L G Miller</author>
</authors>
<title>Continuous speech recognition from phonetic transcription.</title>
<date>1990</date>
<booktitle>In Speech and Natural Language Workshop,</booktitle>
<pages>190--199</pages>
<contexts>
<context position="7214" citStr="Levinson et al., 1990" startWordPosition="1149" endWordPosition="1152">ed in this task, making it infeasible as a general solution. All other cases we know of, have taken into account some properties of the domain and, in fact, several of the works can be viewed as instantiations of the sequential model we formalize here, albeit done in an ad-hoc fashion. In speech recognition, a sequential model is used to process speech signal. Abstracting away some details, the first classifier used is a speech signal analyzer; it assigns a positive probability only to some of the words (using Levenshtein distance (Levenshtein, 1966) or somewhat more sophisticated techniques (Levinson et al., 1990)). These words are then assigned probabilities using a different contextual classifier e.g., a language model, and then, (as done in most current speech recognizers) an additional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence. Several word prediction tasks make decisions in a sequential way as well. In spell correction confusion sets are created using a classifier that takes as input the word transcription and outputs a positive probability for potential words. In conventional spellers, the output of this classifier is t</context>
</contexts>
<marker>Levinson, Ljolje, Miller, 1990</marker>
<rawString>S.E. Levinson, A. Ljolje, and L.G. Miller. 1990. Continuous speech recognition from phonetic transcription. In Speech and Natural Language Workshop, pages 190–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proc. of the International Conference on Machine Learning,</booktitle>
<pages>734--741</pages>
<contexts>
<context position="7955" citStr="Mangu and Brill, 1997" startWordPosition="1269" endWordPosition="1272">, (as done in most current speech recognizers) an additional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence. Several word prediction tasks make decisions in a sequential way as well. In spell correction confusion sets are created using a classifier that takes as input the word transcription and outputs a positive probability for potential words. In conventional spellers, the output of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar task in which the confusi</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>L. Mangu and E. Brill. 1997. Automatic rule acquisition for spelling correction. In Proc. of the International Conference on Machine Learning, pages 734–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>Automatic rule induction for unknown word guessing.</title>
<date>1997</date>
<booktitle>In Computational Linguistic,</booktitle>
<volume>23</volume>
<issue>3</issue>
<pages>405--423</pages>
<contexts>
<context position="12843" citStr="Mikheev, 1997" startWordPosition="2106" endWordPosition="2107">e study the problem of learning a multi-class classifier, where is the set of class labels. is a decomposition of the domain (not necessarily disjoint; it could be that ). determines the order in which the classifiers are learned and evaluated. For convenience we denote sues make the pos tagging problem a natural problem to study within the SM. (i) A relatively large number of classes (about 50). (ii) A natural decomposition of the feature space to contextual and lexical features. (iii) Lexical knowledge (for unknown words) and the word lemma (for known words) provide, w.h.p, one sided error (Mikheev, 1997). 4.1 The Tagger Classifiers The domain in our experiment is defined using the following set of features, all of which are computed relative to the target word . Contextual Features (as in (Brill, 1995; Roth and Zelenko,1998)): Let be the tags of the word preceding, (following) the target word, respectively. . . . . . . . Baseline tag for word . In case is an unknown word, the baseline is proper singular noun “NNP” for capitalized words and common singular noun “NN” otherwise. (This feature is introduced only in some of the experiments.) 9.The target word . Lexical Features: Let be any three c</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>A. Mikheev. 1997. Automatic rule induction for unknown word guessing. In Computational Linguistic, volume 23(3), pages 405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Mitchell</author>
</authors>
<date>1997</date>
<journal>Machine Learning. McgrawHill.</journal>
<contexts>
<context position="28575" citStr="Mitchell, 1997" startWordPosition="4842" endWordPosition="4843">s in is no larger than the error produced by the hypothesis it produces when trained on examples in. Proof. Assume that the algorithm , when trained on a sample, produces a hypothesis that minimizes the empirical error over. Denote when is sampled according to a distribution that supports only examples with label in . Let be a sample set of size , according to In particular this holds if is a hypothesis produced by when trained on, that is sampled according to . 5.3 Expressivity The SM is a decision process that is conceptually similar to a decision tree processes (Rasoul and Landgrebe, 1991; Mitchell, 1997), especially if one allows more general classifiers in the decision tree nodes. In this section we show that (i) the SM can express any DT. (ii) the SM is more compact than a decision tree even when the DT makes used of more expressive internal nodes (Murthy et al., 1994). The next theorem shows that for a fixed set of functions (queries) over the input features, any binary decision tree can be represented as a SM. Extending the proof beyond binary decision trees is straight-forward. Theorem 3 Let be a binary decision tree with internal nodes. Then, there exist a sequential model such that and</context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>T. M. Mitchell. 1997. Machine Learning. McgrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Murthy</author>
<author>S Kasif</author>
<author>S Salzberg</author>
</authors>
<title>A system for induction of oblique decision trees.</title>
<date>1994</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>2--1</pages>
<contexts>
<context position="28847" citStr="Murthy et al., 1994" startWordPosition="4890" endWordPosition="4893">ribution that supports only examples with label in . Let be a sample set of size , according to In particular this holds if is a hypothesis produced by when trained on, that is sampled according to . 5.3 Expressivity The SM is a decision process that is conceptually similar to a decision tree processes (Rasoul and Landgrebe, 1991; Mitchell, 1997), especially if one allows more general classifiers in the decision tree nodes. In this section we show that (i) the SM can express any DT. (ii) the SM is more compact than a decision tree even when the DT makes used of more expressive internal nodes (Murthy et al., 1994). The next theorem shows that for a fixed set of functions (queries) over the input features, any binary decision tree can be represented as a SM. Extending the proof beyond binary decision trees is straight-forward. Theorem 3 Let be a binary decision tree with internal nodes. Then, there exist a sequential model such that and have the same size, and they produce the same predictions. Proof (Sketch): Given a decision tree on nodes we show how to construct a SM that produces equivalent predictions. 1. Generate a confusion set the consists of classes, each representing an internal node in . 2. F</context>
</contexts>
<marker>Murthy, Kasif, Salzberg, 1994</marker>
<rawString>S. Murthy, S. Kasif, and S. Salzberg. 1994. A system for induction of oblique decision trees. Journal ofArtificial Intelligence Research, 2:1:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference.</title>
<date>2001</date>
<booktitle>In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="14922" citStr="Punyakanok and Roth, 2001" startWordPosition="2461" endWordPosition="2464">ing architecture (Roth, 1998) with the Winnow update rule. SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features. SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully on several tasks in natural language processing (Roth, 1998; Roth and Zelenko, 1998; Golding and Roth, 1999; Punyakanok and Roth, 2001). Specifically, for each class label SNoW learns a function that maps a feature based representation of the input instance to a number which can be interpreted as the prob: a classifier based on the lexical feature . : a classifier based on lexical features : a classifier based on contextual features . : a classifier based on all the features, . ends with and length( ends with and length( ends with and length( ability ofbeing the class label corresponding to. At prediction time, given , SNoW outputs (1) All functions – in our case, target nodes are used, one for each pos tag – reside over the </context>
</contexts>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok and D. Roth. 2001. The use of classifiers in sequential inference. In NIPS-13; The 2000 Conference on Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Rasoul</author>
<author>D A Landgrebe</author>
</authors>
<title>A survey of decision tree classifier methodology.</title>
<date>1991</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics,</journal>
<volume>21</volume>
<pages>3--660</pages>
<contexts>
<context position="28558" citStr="Rasoul and Landgrebe, 1991" startWordPosition="4838" endWordPosition="4841">❆) - when trained on examples in is no larger than the error produced by the hypothesis it produces when trained on examples in. Proof. Assume that the algorithm , when trained on a sample, produces a hypothesis that minimizes the empirical error over. Denote when is sampled according to a distribution that supports only examples with label in . Let be a sample set of size , according to In particular this holds if is a hypothesis produced by when trained on, that is sampled according to . 5.3 Expressivity The SM is a decision process that is conceptually similar to a decision tree processes (Rasoul and Landgrebe, 1991; Mitchell, 1997), especially if one allows more general classifiers in the decision tree nodes. In this section we show that (i) the SM can express any DT. (ii) the SM is more compact than a decision tree even when the DT makes used of more expressive internal nodes (Murthy et al., 1994). The next theorem shows that for a fixed set of functions (queries) over the input features, any binary decision tree can be represented as a SM. Extending the proof beyond binary decision trees is straight-forward. Theorem 3 Let be a binary decision tree with internal nodes. Then, there exist a sequential mo</context>
</contexts>
<marker>Rasoul, Landgrebe, 1991</marker>
<rawString>S. S. Rasoul and D. A. Landgrebe. 1991. A survey of decision tree classifier methodology. IEEE Transactions on Systems, Man, and Cybernetics, 21 (3):660–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>D Zelenko</author>
</authors>
<title>Part of speech tagging using a network of linear separators.</title>
<date>1998</date>
<booktitle>In COLING-ACL 98, The 17th International Conference on Computational Linguistics,</booktitle>
<pages>1136--1142</pages>
<contexts>
<context position="14870" citStr="Roth and Zelenko, 1998" startWordPosition="2453" endWordPosition="2456">ell as the single classifier, use the SNoW learning architecture (Roth, 1998) with the Winnow update rule. SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features. SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully on several tasks in natural language processing (Roth, 1998; Roth and Zelenko, 1998; Golding and Roth, 1999; Punyakanok and Roth, 2001). Specifically, for each class label SNoW learns a function that maps a feature based representation of the input instance to a number which can be interpreted as the prob: a classifier based on the lexical feature . : a classifier based on lexical features : a classifier based on contextual features . : a classifier based on all the features, . ends with and length( ends with and length( ends with and length( ability ofbeing the class label corresponding to. At prediction time, given , SNoW outputs (1) All functions – in our case, target nod</context>
<context position="17381" citStr="Roth and Zelenko, 1998" startWordPosition="2906" endWordPosition="2909">ach word , all pos tags with which it was tagged in the training corpus. During evaluation, whenever word occurs, it is tagged with one of these pos tags. That is, in evaluation, the confusion set consists only of those tags observed with the target word in training, and the maximum in Eq. 1 is taken only over these. This is always the case when using (or ), both in the SM and as a single classifier. In training, though, for the sake of this experiment, we treat ( ) differently depending on whether it is trained for the SM or as a single classifier. When trained as a single classifier (e.g., (Roth and Zelenko, 1998)), uses each-tagged example as a positive example forand a negative example for all other tags. On the other hand, the SM classifier is trained on a -tagged example of word , by using it as a positive example forand a negative example only for the effective confusion set. That is, those pos tags which have been observed as tags of in the training corpus. 4.2 Experimental Results The data for the experiments was extracted from the Penn Treebank WSJ and Brown corpora. The training corpus consists of words. The test corpus consists of words of which are unknown words (that is, they do not occur i</context>
</contexts>
<marker>Roth, Zelenko, 1998</marker>
<rawString>D. Roth and D. Zelenko. 1998. Part of speech tagging using a network of linear separators. In COLING-ACL 98, The 17th International Conference on Computational Linguistics, pages 1136–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proc. National Conference on Artificial Intelligence,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="1964" citStr="Roth, 1998" startWordPosition="288" endWordPosition="289">spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these. While this approach is importa</context>
<context position="14325" citStr="Roth, 1998" startWordPosition="2366" endWordPosition="2367">t is a single classifier that uses the same information as used by the SM. Fig 1 Figure 1: POS Tagging of Unknown Word using Contextual and Lexical features in a Sequential Model. The input for capitalized classifier has 2 values and therefore 2 ways to create confusion sets. There are at most different inputs for the suffix classifier (26 character + 10 digits + 5 other symbols), therefore suffix may emit up to confusion sets. illustrates the SM that was used in the experiments. All the classifiers in the sequential model, as well as the single classifier, use the SNoW learning architecture (Roth, 1998) with the Winnow update rule. SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features. SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully on several tasks in natural language processing (Roth, 1998; Roth and Zelenko, 1998; Golding and Roth, 1999; Punyakanok and Roth, 2001). S</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: A unified approach. In Proc. National Conference on Artificial Intelligence, pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning in natural language.</title>
<date>1999</date>
<booktitle>In Proc. Int’l Joint Conference on Artificial Intelligence,</booktitle>
<pages>898--904</pages>
<contexts>
<context position="1977" citStr="Roth, 1999" startWordPosition="290" endWordPosition="291">rection, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these. While this approach is important in that it</context>
<context position="7931" citStr="Roth, 1999" startWordPosition="1267" endWordPosition="1268">el, and then, (as done in most current speech recognizers) an additional sentence level classifier uses the outcome of the word classifiers in a word lattice to choose the most likely sentence. Several word prediction tasks make decisions in a sequential way as well. In spell correction confusion sets are created using a classifier that takes as input the word transcription and outputs a positive probability for potential words. In conventional spellers, the output of this classifier is then given to the user who selects the intended word. In context sensitive spelling correction (Golding and Roth, 1999; Mangu and Brill, 1997) an additional classifier is then utilized to predict among words that are supported by the first classifier, using contextual and lexical information of the surrounding words. In all studies done so far, however, the first classifier – the confusion sets – were constructed manually by the researchers. Other word predictions tasks have also constructed manually the list of confusion sets (Lee and Pereira, 1999; Dagan et al., 1999; Lee, 1999) and justifications where given as to why this is a reasonable way to construct it. (Even-Zohar and Roth, 2000) present a similar t</context>
<context position="14894" citStr="Roth, 1999" startWordPosition="2459" endWordPosition="2460">e SNoW learning architecture (Roth, 1998) with the Winnow update rule. SNoW (Sparse Network of Winnows) is a multi-class classifier that is specifically tailored for learning in domains in which the potential number of features taking part in decisions is very large, but in which decisions actually depend on a small number of those features. SNoW works by learning a sparse network of linear functions over a pre-defined or incrementally learned feature space. SNoW has already been used successfully on several tasks in natural language processing (Roth, 1998; Roth and Zelenko, 1998; Golding and Roth, 1999; Punyakanok and Roth, 2001). Specifically, for each class label SNoW learns a function that maps a feature based representation of the input instance to a number which can be interpreted as the prob: a classifier based on the lexical feature . : a classifier based on lexical features : a classifier based on contextual features . : a classifier based on all the features, . ends with and length( ends with and length( ends with and length( ability ofbeing the class label corresponding to. At prediction time, given , SNoW outputs (1) All functions – in our case, target nodes are used, one for eac</context>
<context position="25273" citStr="Roth, 1999" startWordPosition="4265" endWordPosition="4266">note that although the conditional independence assumption is a strong one, it is a reasonable assumption in many NLP applications; in particular, when cross modality information is used, this assumption typically holds for decomposition that is done across modalities. For example, in POS tagging, lexical information is often conditionally independent of contextual information, given the true POS. (E.g., assume that word is a gerund; then the context is independent of the “ing” word ending.) In addition, decomposing the domain has significant advantages from the learning theory point of view (Roth, 1999). Learning over domains of lower dimensionality implies better generalization bounds or, equivalently, more accurate classifiers for a fixed size training set. 5.2 Decomposing the range The SM attempts to reduce the size of the candidates set. We justify this by considering two cases: (i) Test: we will argue that prediction among a smaller set of classes has advantages over predicting among a large set of classes; (ii) Training: we will argue that it is advantageous to ignore irrelevant examples. 5.2.1 Decomposing the range during Test The following discussion formalizes the intuition that a s</context>
</contexts>
<marker>Roth, 1999</marker>
<rawString>D. Roth. 1999. Learning in natural language. In Proc. Int’l Joint Conference on Artificial Intelligence, pages 898–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-W Teow</author>
<author>K-F Loe</author>
</authors>
<title>Handwritten digit recognition with a novel vision model that extracts linearly separable features.</title>
<date>2000</date>
<booktitle>In CVPR’00, The IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>76--81</pages>
<contexts>
<context position="6132" citStr="Teow and Loe, 2000" startWordPosition="975" endWordPosition="978">ionable. Statistician have studied several schemes such as learning a single classifier for each of the class labels (one vs. all) or learning a discriminator for each pair of class labels, and discussed their relative merits(Hastie and Tibshirani, 1998). Although it has been argued that the latter should provide better results than others, experimental results have been mixed (Allwein et al., 2000) and in some cases, more involved schemes, e.g., learning a classifier for each set of three class labels (and deciding on the prediction in a tournament like fashion) were shown to perform better (Teow and Loe, 2000). Moreover, none of these methods seem to be computationally plausible for large scale problems, since the number of classifiers one needs to train is, at least, quadratic in the number of class labels. Within NLP, several learning works have already addressed the problem of multi-class classification. In (Kudoh and Matsumoto, 2000) the methods of “all pairs” was used to learn phrase annotations for shallow parsing. More than different classifiers where used in this task, making it infeasible as a general solution. All other cases we know of, have taken into account some properties of the doma</context>
</contexts>
<marker>Teow, Loe, 2000</marker>
<rawString>L-W. Teow and K-F. Loe. 2000. Handwritten digit recognition with a novel vision model that extracts linearly separable features. In CVPR’00, The IEEE Conference on Computer Vision and Pattern Recognition, pages 76–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Wilf</author>
</authors>
<date>1994</date>
<pages>generatingfunctionology.</pages>
<publisher>Academic Press Inc.,</publisher>
<location>Boston, MA,</location>
<note>second edition.</note>
<contexts>
<context position="30803" citStr="Wilf, 1994" startWordPosition="5237" endWordPosition="5238">ults in a decision tree that is exponentially larger than the original SM. Theorem 4 shows that this difference in expressivity is inherent. Theorem 4 Let be the number of classifiers in a sequential model and the number of internal nodes a in decision tree . Let be the set of classes in the output of and also the maximum degree of the internal nodes in . Denote by the number offunctions representable by respectively. Then, when is exponentially larger than . Proof (Sketch): The proof follows by counting the number of functions that can be represented using a decision tree with internal nodes(Wilf, 1994), and the number of functions that can be represented using a sequential model on intermediate classifier. Given the exponential gap, it follows that one may need exponentially large decision trees to represent an equivalent predictor to an size SM. 6 Conclusion A wide range and a large number of classification tasks will have to be used in order to perform any high level natural language inference such as speech recognition, machine translation or question answering. Although in each instantiation the real conflict could be only to choose among a small set of candidates, the original set of c</context>
</contexts>
<marker>Wilf, 1994</marker>
<rawString>H. S. Wilf. 1994. generatingfunctionology. Academic Press Inc., Boston, MA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proc. of the Annual Meeting of the ACL,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1702" citStr="Yarowsky, 1994" startWordPosition="250" endWordPosition="251">wed as classification problems in which the goal is to select a class label from among a collection of candidates. Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle thes</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French. In Proc. of the Annual Meeting of the ACL, pages 88–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>W Daelemans</author>
<author>J Veenstra</author>
</authors>
<title>Resolving pp attachment ambiguities with memory based learning.</title>
<date>1997</date>
<booktitle>In Computational Natural Language Learning,</booktitle>
<location>Madrid, Spain,</location>
<contexts>
<context position="1932" citStr="Zavrel et al., 1997" startWordPosition="281" endWordPosition="284">in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these</context>
</contexts>
<marker>Zavrel, Daelemans, Veenstra, 1997</marker>
<rawString>J. Zavrel, W. Daelemans, and J. Veenstra. 1997. Resolving pp attachment ambiguities with memory based learning. In Computational Natural Language Learning, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. National Conference on Artificial Intelligence,</booktitle>
<pages>1050--1055</pages>
<contexts>
<context position="1809" citStr="Zelle and Mooney, 1996" startWordPosition="263" endWordPosition="266"> of candidates. Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers. Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success. A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), HMMs (Charniak, 1993), inductive logic methods (Zelle and Mooney, 1996), memoryThis research is supported by NSF grants IIS-9801638,IIS0085836 and SBR-987345. based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformationbased learning (Brill, 1995). In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole proble</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>J. M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. National Conference on Artificial Intelligence, pages 1050–1055.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>