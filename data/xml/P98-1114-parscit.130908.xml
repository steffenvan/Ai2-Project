<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<note confidence="0.660021">
Large Scale Collocation Data and Their Application
to Japanese Word Processor Technology
Yasuo Koyama, Masako Yasutake, Kenji Yoshimura and Kosho Shudo
Institute for Infonnation and Control Systems, Fukuoka University
Nanalcuma, Fukuoka, 814-0180 Japan
</note>
<email confidence="0.8121315">
koyama@aisoftco.jp, yasutake@heliatfulcuolca-u.acjp, yosimura@dsuatifulcuolca-u.aajp,
shudo@dsun.dfulcuoka-u.ac.jp
</email>
<sectionHeader confidence="0.354385" genericHeader="abstract">
abstract
</sectionHeader>
<bodyText confidence="0.9997391875">
Word processors or computers used in Japan
employ Japanese input method through key-
board stroke combined with Kana (phonetic)
character to Kanji (ideographic, Chinese) char-
acter conversion technology. The key factor of
Kana-to-Kanji conversion technology is how
to raise the accuracy of the conversion through
the homophone processing, since we have so
many homophonic Kanjis. In this paper, we
report the results of our Kana-to-Kanji conver-
sion experiments which embody the homo-
phone processing based on large scale colloca-
tion data. It is shown that approximately
135,000 collocations yield 9.1 % raise of the
conversion accuracy compared with the pro-
totype system which has no collocation data.
</bodyText>
<sectionHeader confidence="0.998215" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999980222222222">
Word processors or computers used in Japan ordi-
narily employ Japanese input method through key-
board stroke combined with Kana (phonetic) to
Kanji (ideographic, Chinese) character conversion
technology. The Kana-to-Kanji conversion is per-
formed by the morphological analysis on the input
Kana string with no space between words. Word- or
phrase-segmentation is carried out by the analysis to
identify the substring of the input which has to be
converted from Kana to Kanji. Kana-Kanji mixed
string, which is the ordinary form of Japanese writ-
ten text, is obtained as the final result. The major
issue of this technology lies in raising the accuracy
of the segmentation and the homophone processing
to select the correct Kanji among many homophonic
candidates.
The conventional methodology for processing ho-
mophones have used the function that gives the pri-
ority to the word which was used lastly or to the
high frequency word. In fact, however, this method
sometimes tends to cause inadequate conversion due
to the lack of consideration of the semantic consis-
tency of the word concurrence. While it is difficult
to employ the syntactic or semantic processing in
earnest for the word processor from the cost vs.
performance viewpoints, for example, the following
trials to improve the conversion accuracy have been
reported: Employing the case-frame to check the
semantic consistency of combination of words
[Oshima, Y. et al.,1986]. Employing the neural net-
work to describe the consistency of the concurrence
of words [Kobayashi,T. et al.,1992], Making a con-
currence dictionary for the specific topic or field,
and giving the priority to the word which is in the
dictionary when the topic is identified [Yamamoto,
K. et al., 1992]. In any of these studies, however,
many problems are left unsolved in realizing its
practical system.
Besides these semantic or quasi-semantic gadgets,
we think it much more practical and effective to use
surface level resources, namely, to use extensively
the collocation. But how many collocations contrib-
ute to the accuracy of Kana-to-Kanji conversion is
not known yet.
In this paper, we present some results of our ex-
periments of Kana-to-Kanji conversion, focusing on
the usage of large scale collocation data. In chapter
2, descriptions of the collocations used in our sys-
tem and their classification are given. In chapter 3,
the technological framework of our Kana-to-Kanji
conversion systems is outlined. In chapter 4, the
method and the results of the experiments are given
along with some discussions. In chapter 5, con-
cluding remarks are given.
</bodyText>
<sectionHeader confidence="0.919917" genericHeader="method">
2. Collocation Data
</sectionHeader>
<bodyText confidence="0.999850833333333">
Unlike the recent works on the automatic extraction
of collocations from corpus [Church, K. W, et al,
1990, lkehara, S. et al, 1996, etc.], our data have
been collected manually through the intensive in-
vestigation of various texts, spending years on it.
This is because no stochastic framework assures the
</bodyText>
<page confidence="0.998211">
694
</page>
<bodyText confidence="0.999990909090909">
accuracy of the extraction, namely the necessity and
sufficiency of the data set. The collocations which
are used in our Kana-to-Kanji conversion system
consist of two kinds: (1) idiomatic expressions,
whose meanings seem to be difficult to compose
from the typical meaning of the individual compo-
nent words [Shudo, K. et al., 1988]. (2) stereotypical
expressions in which the concurrence of component
words is seen in the texts with high frequency. The
collocations are also classified into two classes by a
grammatical criterion: one is a class of functional
collocations, which work as functional words such
as particles (postpositionals) or auxiliary verbs, the
other is a class of conceptual collocations which
work as nouns, verbs, adjectives, adverbs, etc. The
latter is further classified into two kinds: uninter-
ruptible collocations, whose concurrence relation-
ship of words are so strong that they can be dealt
with as single words, and interruptible collocations,
which are occasionally used separately.
In the following, the parenthesized number is the
number of expressions adopted in the system.
</bodyText>
<subsectionHeader confidence="0.987085">
2.1 Functional Collocations (2,174)
</subsectionHeader>
<bodyText confidence="0.9895715">
We call expressions which work like a particle rela-
tional collocation and expressions which work like
an auxiliary verb at the end of the predicate auxili-
ary predicative collocation [Shudo, K. et al. ,1980].
relational collocations (760)
ex. IZ/&amp;quot;DIXC
ni/tuite (about)
auxiliary predicative collocations (1,414)
ex.4.1fttitt:T.
nakereba/naranai (must)
</bodyText>
<subsectionHeader confidence="0.990712">
2.2 Uninterruptible Conceptual Col-
locations (54,290)
</subsectionHeader>
<bodyText confidence="0.866674666666667">
four-Kanji-compound (2,231)
ex. it 1313 3.1* .
gaaerunsut
(every miller draws water to his own mill)
adverb + particle type (3,089)
ex. t- Z•t:.
</bodyText>
<equation confidence="0.691288333333333">
ata.tato (disconcertedly)
adverb + suru type (1,043)
ex.&lt;
</equation>
<bodyText confidence="0.8496412">
ctusZut (toil and moil)
noun type (21,128)
ex. *0)/1pl
akana tanin (perfect stranger)
verb type (13,225)
ex. 9 hi/*
otsuriga/kuru
(be enough to make the change)
adjective type (2,394)
ex. sr,. ir
</bodyText>
<figure confidence="0.542744545454545">
uraganashii (mournful)
adjective verb type (397)
ex. VENE/#446
gokigen/naname (in a bad mood)
adverb and other type (8,185)
ex.
meni/nuete (remarkably)
proverb type (2,598)
ex. t &amp;quot;C
otteha/koni/shitagae
(when old, obey your children)
</figure>
<subsectionHeader confidence="0.6395655">
2.3 Interruptible Conceptual Colloca-
tions (78,251)
</subsectionHeader>
<bodyText confidence="0.9203919375">
noun type (7,627)
ex. El:10AL%
akugyouno/rnukui (fruit of an evil deed)
verb type (64,087)
ex. gat 6 /31 hxti.
ushirogamtwo/hikareru
(feel as if one&apos;s heart were left behind)
adjective type (3,617)
ex. In ht/
taictoga 00kil ( act in a lordly manner)
adjective verb type (2,018)
ex. ag
yakushaga/ue (be more able)
others (902)
ex. titl::/31(t tY3
atom/iukenu (can not give up)
</bodyText>
<sectionHeader confidence="0.949618" genericHeader="method">
3. Kana-to-Kanji Conversion Systems
</sectionHeader>
<bodyText confidence="0.999155272727273">
We developed four different Kana-to-Kanji conver-
sion systems, phasing in the collocation data de-
scribed in 2. The technological framework of the
system is based on extended bunsetsu (e-
bunsetsu) model [Shudo, K. et al., 1980] for the
unit of the segmentation of the input Kana string,
and on minimum cost method [Yoshimura,K. et
al., 1987] combined with Viterbi&apos;s algorithm
[Viterbi, A„ J., 1967] for the reduction of the ambi-
guity of the segmentation.
A bunsetsu is the basic postpositional or predicative
</bodyText>
<page confidence="0.996118">
695
</page>
<bodyText confidence="0.995637304347826">
phrase which composes Japanese sentences, and an
e-bunsetsu, which is a natural extension of the bun-
setsu, is defmed roughly as follows:
&lt;e-bunsetsu&gt;::= &lt;prefix&gt;* &lt;conceptual word I
uninterruptible conceptual collocation&gt;
&lt;suffix&gt;* &lt;functional word I
functional collocation&gt;*
The e-bunsetsu which includes no collocation is the
bunsetsu. More refined rules are used in the actual
segmentation process. The interruptible conceptual
collocation is not treated as a single unit but as a
string of bunsetsus in the segmentation process.
Each collocation in the dictionary which is com-
posed of multiple number of bunsetsus is marked
with the boundary between bunsetsus. The system
first tries to segment the input Kana string into e-
bunsetsus. Every possible segmentation is evaluated
by its cost. A segmentation which is assigned the
least cost is chosen as the solution.
The boundary between e-bunsetsus in examples in
this paper is denoted by &amp;quot;I&amp;quot;.
ex. two results of e-bunsetsu-segmentation:
Artfttpim (zo t; VA* 9 /v
</bodyText>
<equation confidence="0.71660625">
hitoha/tagatriKunikositatrotonaartmasen
(there is nothing like being watchful)
1Vtbigi &lt; 17-41 1.-/Vt/*1.) *t_iv
rlitoha/ *g lcuni/kosita/fotoha/arimasen
</equation>
<bodyText confidence="0.994921305555556">
In the above examples, /I, 1i/1J &lt; Idgaliku: is
uninterruptible conceptual collocation and 1:-VM
it/iFft/*9 1±AJ naositalcotoha/arimasen: is
a functional collocation. In the first example, these
collocations are dealt with a single words. The
second example shows the conventional bunsetsu-
segmentation.
The cost for the segmentation candidate is the sum
of three partial costs: b-cost, c-cost and d-cost
shown below.
(1)a segment cost is assigned to each segment. Sum
of segment costs of all segments is the basic cost
(b-cost) of a segmentation candidate. By this, the
collocation tends to have priority over the ordi-
nary word. The standard and initial value of each
segment cost is 2, and it is increased by 1 for each
occurrence of the prefix, suffix, etc. in the seg-
ment.
(2)a concatenation cost (c-cost) is assigned to speci-
fic e-bunsetsu boundaries to revise the b-cost.
The concatenation, such as adnominal-noun, ad-
verb-verb, noun-noun, etc. is paid a bonus ,
namely a negative cost, -1.
(3)a dependency cost (d-cost), which has a negative
value, is assigned to the strong dependency rela-
tionship between conceptual words in the candi-
date, representing the consistency of concurrence
of conceptual words. By this, the segmentation
containing the interrupted conceptual collocation
tends to have priority. The value of ad-cost varies
from —3 to —1, depending on the strength of the
concurrence. The interruptible conceptual collo-
cation is given the biggest bonus i.e.-3.
The reduction of the homophonic ambiguity, which
limits Kanji candidates, is carried out in the course
of the segmentation and its evaluation by the cost.
</bodyText>
<subsectionHeader confidence="0.998842">
3.1 Prototype System A
</subsectionHeader>
<bodyText confidence="0.999922142857143">
We first developed a prototype Kana-to-Kanji con-
version system which we. call System A, revising
Kana-to-Kanji conversion software on the market,
WXG Ver2.05 for PC.
System A has no collocation data but conventional
lexical resources, namely functional words (1,010)
and conceptual words (131,661).
</bodyText>
<subsectionHeader confidence="0.998178">
3.2 System B, C and D
</subsectionHeader>
<bodyText confidence="0.999937363636364">
We reinforced System A to obtain System B, C and
D by phasing in the following collocational re-
sources. System B is System A equipped addition-
ally with functional collocations (2,174) and unin-
terruptible conceptual collocations except for four-
Kanji-compound and proverb type collocations
(49,461). System C is System B equipped addition-
ally with four-Kanji-compound (2,231) and proverb
type collocations (2,598). Further, System D is
System C equipped additionally with interruptible
conceptual collocations (78,251).
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="evaluation">
4. Experiments
</sectionHeader>
<subsectionHeader confidence="0.994767">
4.1 Text Data for Evaluation
</subsectionHeader>
<bodyText confidence="0.972915">
Prior to the experiments of Kana-to-Kanji conver-
sion, we prepared a large volume of text data by
hand which is formally a set of triples whose first
component a is a Kana string (a sentence) with no
space, The second component b is the correct seg-
mentation result of a, indicating each boundary
between bunsetsus with &amp;quot;I&amp;quot; or &amp;quot;.&amp;quot;. &amp;quot;I&amp;quot; and &amp;quot;.&amp;quot;
means obligatory and optional boundary, respec-
tively. The third component c is the correct conver-
sion result of a, which is a Kana-Kanji mixed string.
ex. a: 1:_12(zIgi;btLyCl.N6
</bodyText>
<subsubsectionHeader confidence="0.233608">
niwambaragasaiteiru
</subsubsectionHeader>
<page confidence="0.811907">
696
</page>
<equation confidence="0.8899225">
(roses are in bloom in a garden)
b: 1.2471Z/IgGti/O-C.LN
niwant/baraga/saite.tru
c: glIZI/i-5btIOALvt.0 }
</equation>
<bodyText confidence="0.999793333333333">
The introduction of the optional boundary assures
the flexible evaluation. For example, each of PAL&apos;
saite/iru (be in bloom) and PALIT
saiteiru is accepted as a correct result. The data file
is divided into two sub-files, fl and f2, depending
on the number of bunsetsus in the Kana string a. fl
has 10,733 triples, whose a has less than five
bunsetsus and f2 has 12,192 triples, whose a has
more than four bunsetsus.
</bodyText>
<subsectionHeader confidence="0.999921">
4.2 Method of Evaluation
</subsectionHeader>
<bodyText confidence="0.976182833333333">
Each a in the text data is fed to the conversion sys-
tem. The system outputs two forms of the least cost
result: b&apos;, Kana string segmented to bunsetsus by
&amp;quot;I&amp;quot;, and c&apos;, Kana-Kanji mixed string, corresponding
to b and c of the correct data, respectively. Each of
the following three cases is counted for the evalua-
tion.
SS (Segmentation Success): b&apos;= b
CS (Complete Success): b&apos;= b and c&apos;= c
TS (Tolerative Success): b&apos;= b and c
There are many kinds of notational fluctuation in
Japanese. For example, the conjugational suffix of
some kind of Japanese verb is not always necessi-
tated, therefore,V)±(iM±Ii and ± are all
acceptable results for input 59.Z uriage (sales).
Besides, a single word has sometimes more than
one Kanji notations, e.g. 5 hama (beach) and 31
hama (beach) are both acceptable, and so on. c&apos;— c
in the case of TS means that c&apos; coincides with c
completely or excepting the part which is hetero-
morphic in the above sense. For this, each of our
conversion system has a dictionary which contains
approximately 35,000 fluctuated notations of con-
ceptual words.
</bodyText>
<subsectionHeader confidence="0.994706">
4.3 Results of Experiments
</subsectionHeader>
<bodyText confidence="0.656151615384616">
Results of the experiments are given in Table 1 and
Table 2 for input file fl and f2, respectively.
Comparing the statistics of system A with D, we can
conclude that the introduction of approximately
135,000 collocation data causes 8.1 % and 10.5 %
raise of CS and TS rate, respectively, in case of re-
latively short input strings (fl). The raise of SS rate
for fl is 2.7%. In case of the longer input strings (f2)
whose average number of bunsetsus is approxi-
mately 12.6, the raise of CS, TS and SS rate is 2.4 %,
5.2 % and 5.7 %, respectively. As a consequence,
the raise of CS, TS and SS rate is 6.2 %, 9.1 % and
3.8 % on the average, respectively.
</bodyText>
<table confidence="0.996207">
System A System B System C
Sys ii D
SS(Segmentation Success) 9,656(90.0%) 9,912(92.4%) 9,927(92.5%) 9,954(92.7%)
CS(Complete Success) 5,085(47.4%) 5,638(52.5%) 5,677(52.9%) 5,953(55.5%)
TS(Tolerative Success) 6,226(58.0%) 6,971(64.9%) 7,024(65.4%) 7,355(68.5%)
</table>
<tableCaption confidence="0.9891265">
Table 1:Result of the experiments for 10,733 short input strings data, fl.
(average number of Kana characters per input is 13.7)
</tableCaption>
<table confidence="0.9996214">
System A System C System D
Sys ii B
SS 8,345(68.4%) 8,978(73.6%) 8,988(73.7%) 9,037(74.1%)
CS 2,422(19.9%) 2,660(21.8%) 2,673(21.9%) 2,717(22.3%)
TS 3,965(32.5%) _ 4,555(37.4%) 4,568(37.5%) 4,601(37.7%)
</table>
<tableCaption confidence="0.990505">
Table 2: Result of the experiments for 12,192 long input strings data f2.
(average number of Kana characters per input is 42.7)
</tableCaption>
<table confidence="0.9984055">
System D&apos; WXG
SS 9,949(92.7%) 9,804(91.3%)
CS 6,180(57.6%) 5,877(54.8%)
TS 7,646(71.2%) 7,290(67.9%)
</table>
<tableCaption confidence="0.999696">
Table 3:Comparison of system D&apos; with WXG for
</tableCaption>
<table confidence="0.997851">
System D&apos; WXG
SS 8,928(73.2%) 8,815(72.3%)
CS 2,738(22.5%) 2,694(22.1%)
TS 4,649(38.1%) 4,543(37.3%)
</table>
<tableCaption confidence="0.999954">
Table 4: Comparison of system D&apos; with WXG for f2.
</tableCaption>
<page confidence="0.993453">
697
</page>
<subsectionHeader confidence="0.986116">
4.4 Comparison with a Software on the
Market
</subsectionHeader>
<bodyText confidence="0.999821388888889">
We compared System D with a Kana-to-Kanji conver-
sion software for PC on the market, WXG Ver2.05 wider
the same condition except for the amount of installed
collocation data For this system D was reinforced and
renamed D&apos;, by equipping with WXG&apos;s 10,000 items of
word dependency description Both systems were dis-
abled for the learning function. WXG has approximately
60,000 collocations (3,000 uninterruptible and 57,000
interruptible collocations), whereas System D&apos; has ap-
proximately 135,000 collocations. The statistical results
are given in Table 3 and Table 4 for the corpus fl and f2,
respectively.
The tables show that the raise of CS, TS and SS rate,
which was obtained by System D&apos; is 2.5 %, 4.5 % and
3.9 % on the average, respectively. No further compari-
son with the commercial products has been done, since
we judge the performance of WXG Ver.2.05 to be aver-
age among them.
</bodyText>
<subsectionHeader confidence="0.93415">
4.5 Discussions
</subsectionHeader>
<bodyText confidence="0.999549333333333">
Table 1 ^-• 4 show that the longer input the system is
given, the more difficult for the system to make the cor-
rect solution and the difference between accuracy rate of
WXG and system D&apos; is less for f2 than for fl. Further
investigation clarified that the error of System D is
mainly caused by missing words or expressions in the
machine dictionary. Specifically, it was clarified that the
dictionary does not have the sufficient number of Kata-
Kana words and people&apos;s names. In addition, the number
of fluctuational variants installed in the dictionary men-
tioned in 42 turned out to be insufficient These problems
should be remedied in future.
</bodyText>
<sectionHeader confidence="0.972449" genericHeader="conclusions">
5. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999961714285714">
In this paper, the effectiveness of the large scale colloca-
tion data for the improvement of the conversion accuracy
of Kana-to-Kanj conversion process used in Japanese
word processors was clarified, by relatively large scale
experiment.
The extensive collection of the collocations has been
carried out manually these ten years by the authors in
order to realize not only high precision word processor
but also more general Japanese language processing in
future. A lot of resources, school textbooks newspapers
novels journals dictionaries, etc. have been investigated
by workers for the collection The candidates for the col-
location have been judged one after another by them.
Among collocations described in this paper, the idiomatic
expressions are quite burdensome in the development of
NLP, since they do not follow the principle of composi-
tionality of the meaning Generally speaking, the more
extensive collocational data it deals with, the less the
&amp;quot;rule system&amp;quot; of the rule based NLP system is burdened
This means the great importance of the enrichment of
collocational data Whereas it is inevitable that the arbi-
trariness lies in the human judgment and selection of
collocations, we believe that our collocation data is far
more refined than the automatically extracted one from
corpora which has been recently reported [Church, K W.
eta], 1990, Ikehara, S. et al, 1996, etc.].
We believe that the approach described here is important
for the evolution ofNLP product in general as well
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999559555555556">
Shudo, K et al., 1980. Morphological Aspect of Japanese
Language Processing in Proc. of 8 th InternatConf. on
Computational Linguistics(COLING80)
Oshima, Y. et al., 1986. A Disambiguation Method in
Kana-to-Kanji Conversion Using Case Frame Gram-
mar. in Trans. of IPSJ, 27-7. (in Japanese)
Kobayashi,T. et al. ,1986. Realization of Kana-to-Kanji
Conversion Using Neural Networks. in Toshiba
Review, 47-11. (in Japanese)
Yoshimura, K et al.,1987. Morphological Analysis ofJa-
panese Sentences using the Least Cost Method. in IPSJ
SIG NL-60. (in Japanese)
Shudo, K. et al. ,1988. On the Idiomatic Expressions in
Japanese Language in IPSJ SIG NL-66. (in Japanese)
Church, K.W. et a1,1990. Word Association Norms,
Mutual Information, and Lexicography. in Comput-
ational Linguistics 16.
Yamamoto, K et al. ,1992. Kana-to-Kanji Conversion
Using Co-occurrence Groups. in Proc. of 44th Conf of
IPSJ. (in Japanese)
Ikehara,S. et al. ,1996. A Statistical Method for
Extracting Uninterrupted and Interrupted Collocations
from Very Large Corpora in Proc. of 16th Internat.
Conf. on Computational Linguistics (COLING 96)
Viterbi,A.,J.,1967,Error Bounds for Convolutional Codes
and an Asymptotically Optimal Decoding Algorithm.
hi JEFF Trans. on Information Theory 13.
</reference>
<page confidence="0.99727">
698
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.275336">
<title confidence="0.99764">Large Scale Collocation Data and Their Application</title>
<author confidence="0.7804205">to Japanese Word Processor Technology Yasuo Koyama</author>
<author confidence="0.7804205">Masako Yasutake</author>
<author confidence="0.7804205">Kenji Yoshimura</author>
<author confidence="0.7804205">Kosho Shudo</author>
<affiliation confidence="0.999988">Institute for Infonnation and Control Systems, Fukuoka University</affiliation>
<address confidence="0.99826">Nanalcuma, Fukuoka, 814-0180 Japan</address>
<email confidence="0.6385015">koyama@aisoftco.jp,yasutake@heliatfulcuolca-u.acjp,yosimura@dsuatifulcuolca-u.aajp,shudo@dsun.dfulcuoka-u.ac.jp</email>
<abstract confidence="0.999262235294118">Word processors or computers used in Japan employ Japanese input method through keyboard stroke combined with Kana (phonetic) character to Kanji (ideographic, Chinese) character conversion technology. The key factor of Kana-to-Kanji conversion technology is how to raise the accuracy of the conversion through the homophone processing, since we have so many homophonic Kanjis. In this paper, we report the results of our Kana-to-Kanji conversion experiments which embody the homophone processing based on large scale collocation data. It is shown that approximately 135,000 collocations yield 9.1 % raise of the conversion accuracy compared with the prototype system which has no collocation data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Shudo</author>
</authors>
<title>Morphological Aspect of Japanese Language Processing in</title>
<date>1980</date>
<booktitle>Proc. of 8 th InternatConf. on Computational Linguistics(COLING80)</booktitle>
<marker>Shudo, 1980</marker>
<rawString>Shudo, K et al., 1980. Morphological Aspect of Japanese Language Processing in Proc. of 8 th InternatConf. on Computational Linguistics(COLING80)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Oshima</author>
</authors>
<title>A Disambiguation Method in Kana-to-Kanji Conversion Using Case Frame Grammar. in Trans. of IPSJ,</title>
<date>1986</date>
<booktitle>in Toshiba Review,</booktitle>
<pages>27--7</pages>
<note>(in Japanese)</note>
<marker>Oshima, 1986</marker>
<rawString>Oshima, Y. et al., 1986. A Disambiguation Method in Kana-to-Kanji Conversion Using Case Frame Grammar. in Trans. of IPSJ, 27-7. (in Japanese) Kobayashi,T. et al. ,1986. Realization of Kana-to-Kanji Conversion Using Neural Networks. in Toshiba Review, 47-11. (in Japanese)</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Yoshimura</author>
</authors>
<title>et al.,1987. Morphological Analysis ofJapanese Sentences using the Least Cost Method.</title>
<note>in IPSJ SIG NL-60. (in Japanese)</note>
<marker>Yoshimura, </marker>
<rawString>Yoshimura, K et al.,1987. Morphological Analysis ofJapanese Sentences using the Least Cost Method. in IPSJ SIG NL-60. (in Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shudo</author>
</authors>
<title>On the Idiomatic Expressions in Japanese Language</title>
<date>1988</date>
<booktitle>in IPSJ SIG NL-66. (in Japanese) Church, K.W. et a1,1990. Word Association Norms, Mutual Information, and Lexicography. in Computational Linguistics 16.</booktitle>
<marker>Shudo, 1988</marker>
<rawString>Shudo, K. et al. ,1988. On the Idiomatic Expressions in Japanese Language in IPSJ SIG NL-66. (in Japanese) Church, K.W. et a1,1990. Word Association Norms, Mutual Information, and Lexicography. in Computational Linguistics 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamamoto</author>
</authors>
<title>Kana-to-Kanji Conversion Using Co-occurrence Groups.</title>
<date>1992</date>
<booktitle>in Proc. of 44th Conf of IPSJ. (in Japanese)</booktitle>
<marker>Yamamoto, 1992</marker>
<rawString>Yamamoto, K et al. ,1992. Kana-to-Kanji Conversion Using Co-occurrence Groups. in Proc. of 44th Conf of IPSJ. (in Japanese)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ikehara</author>
</authors>
<title>A Statistical Method for Extracting Uninterrupted and Interrupted Collocations from Very Large Corpora in</title>
<date>1996</date>
<booktitle>Proc. of 16th Internat. Conf. on Computational Linguistics (COLING 96)</booktitle>
<marker>Ikehara, 1996</marker>
<rawString>Ikehara,S. et al. ,1996. A Statistical Method for Extracting Uninterrupted and Interrupted Collocations from Very Large Corpora in Proc. of 16th Internat. Conf. on Computational Linguistics (COLING 96)</rawString>
</citation>
<citation valid="false">
<title>Viterbi,A.,J.,1967,Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm.</title>
<journal>hi JEFF Trans. on Information Theory</journal>
<volume>13</volume>
<marker></marker>
<rawString>Viterbi,A.,J.,1967,Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm. hi JEFF Trans. on Information Theory 13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>