<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011439">
<title confidence="0.901814">
MPOWERS: a Multi Points Of VieW Evaluation Refinement Studio
</title>
<author confidence="0.89814">
Marianne Laurent, Philippe Bretier
</author>
<affiliation confidence="0.7488925">
Orange Labs
Lannion, France
</affiliation>
<email confidence="0.742519">
{marianne.laurent, philippe.bretier}@orange-ftgroup.com
</email>
<sectionHeader confidence="0.988245" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998565">
We present our Multi Point Of vieW Eval-
uation Refinement Studio (MPOWERS),
an application framework for Spoken Di-
alogue System evaluation that implements
design conventions in a user-friendly in-
terface. It ensures that all evaluator-users
manipulate a unique shared corpus of data
with a shared set of parameters to de-
sign and retrieve their evaluations. It
therefore answers both the need for con-
vergence among the evaluation practices
and the consideration of several analyti-
cal points of view addressed by the evalu-
ators involved in Spoken Dialogue System
projects. After introducing the system ar-
chitecture, we argue the solution’s added
value in supporting a both data-driven and
goal-driven process. We conclude with fu-
ture works and perspectives of improve-
ment upheld by human processes.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996508886363636">
The evaluation of Spoken Dialogue Systems
(SDS) is a twofold issue. On the one hand, the lack
of convention on evaluation criteria and the many
different evaluation needs and situations along
with SDS projects lead to nomadic evaluation set-
ups and interpretations. We inventoried seven job
families contributing to these projects: the market-
ing people, the business managers, the technical
and ergonomics experts, the hosting providers, the
contracting owners as well as the actual human op-
erators which integrate SDS in their activity (Lau-
rent et al., 2010). Various experimental proto-
cols for data collection and analytical data pro-
cessing flourish in the domain. On the other hand,
however they may not share evaluation needs and
methods, the various potential evaluators need to
cooperate inside and across projects. This claims
for a convergence of evaluation practices toward
standardized methodologies. The domain has put
a lot of efforts toward the definition of commensu-
rable metrics (Paek, 2007) for comparative evalu-
ations and improved transparency over communi-
cations on systems’ performances.
Nonetheless, we believe that no one-size-fits-all
solution may cover all evaluation needs (Laurent
and Bretier, 2010). We therefore work onto the
rationalization - not the standardization - of eval-
uation practices. By rationalization, we refer to the
definition of common norms to describe the eval-
uation protocols; common thinking models and
vocabulary, for evaluators to make their proce-
dures explicit. Our Multi Points Of VieW Evalu-
ation Refinement Studio (MPOWERS) facilitates
the design, from a unique corpus of parameters, of
personalized evaluations adapted to the particular
contexts. It does not compete with workbenches
like MeMo (M¨oller et al., 2006) or WITcHCRafT
(Schmitt et al., 2010) for which the overall evalu-
ation process is predefined within the tool.
The following section details the solution archi-
tecture. Then, we present the MPOWERS’s pur-
poses, emphasizing on its added value for evalua-
tors. Last, we explain the technical and process-
related aspects that must support the system.
</bodyText>
<subsectionHeader confidence="0.415125">
2 Architecture of the system
</subsectionHeader>
<bodyText confidence="0.99996075">
The application is built on a classical Business In-
telligence (BI) solution that aims to provide de-
cision makers with personalized information (See
Fig. 1). We store, in a single datamart, param-
eters retrieved from heterogeneous sources: inter-
action logs, user questionnaires and third-party an-
notations relative to the evaluation campaigns ar-
ranged on the evaluated system(s). Then, data are
cleaned, transformed and aggregated into Key Per-
formance Indicators (KPIs). It guarantees that the
indicators used across teams and projects are de-
fined, calculated and maintained in the same place.
</bodyText>
<subsubsectionHeader confidence="0.813012">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 265–268,
</subsubsectionHeader>
<affiliation confidence="0.880766">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.998904">
265
</page>
<figureCaption confidence="0.999953">
Figure 1: The MPOWERS architecture
</figureCaption>
<bodyText confidence="0.990371928571428">
On the upper layer, evaluators define and retrieve
personalized reports and dashboards.
We use the Let’s Go! System corpus shared by
the Carnegie Mellon University. It contains log
files generated since from 2003 from the Pitts-
burgh’s telephone-based bus information system
log files, one per module composing the system,
and a summary HTML file. At our stage of the
project the html summary allows the calculation
of a satisfying number of parameters to support
the system development and refinement. We com-
pute the dialogue duration, the number of system
and user turns, the number of barge-ins, the ratio
between user and system turn number, the number
of help requests and of no-matches per call and the
ratio of successful interactions.
The application relies on the SpagoBI 2.6 open
source solution1. Once parametrized, it enables
non-technical stakeholders to retrieve personal-
ized KPIs reports based on shared resources. For
now, it delivers basic dashboards for two user
profiles. One focuses on the service monitoring
for marketing people and business managers and
the other one provides the development team with
usability-related performance figures (see fig. 4).
The unique datamart guarantees all users to work
from similar data. Its population requires parsing
routines to identify and extract the relevant data.
</bodyText>
<sectionHeader confidence="0.973286" genericHeader="introduction">
3 Evaluation process and added value
</sectionHeader>
<bodyText confidence="0.999968625">
By automating tractable tasks, MPOWERS sup-
ports the evaluator-users in their evaluation pro-
cess driven by decision-making objectives. As
sketched in figure 2, our application-supported
process is slightly modified from the one defined
by Stufflebeam (1980): a process through which
one defines, obtains and delivers useful pieces of
information that enable to settle between the alter-
</bodyText>
<footnote confidence="0.6709935">
1http://www.spagoworld.org/
native possible decisions.
</footnote>
<figureCaption confidence="0.9751425">
Figure 2: Evaluation process with MPOWERS
(grey-tinted stages are supported by the system)
</figureCaption>
<bodyText confidence="0.997857095238095">
Custom-made Python2 routines enable to ex-
tract relevant data from the log files. They provide
CSV3 formatted files to be converted into SQL
scripts. The datamart is designed to be gradually
populated from successive evaluation campaigns
on one or several SDS. As data may originates
from diverse sources, it arrays in different formats
and often displays different parameters. Adapted
ad hoc routines permit the manipulation into con-
sistent format. We anticipate the use of separate ta-
bles in the datamart from comparative evaluations
ons distinct systems.
The retrieval of KPIs in SpagoBI requires
datasets pre-parametrized over SQL-Queries.
They describe the SDS’s performance and be-
haviour. We defined the parameters relative to
the system performance according to the ITU-T
Rec. P.Sup24 (2005). Yet, unless input corpora are
defined accordingly not all the recommendation’s
parameters can be implemented. Three modes to
display these datasets are proposed to evaluators:
</bodyText>
<listItem confidence="0.688481">
• A summary of high-level KPIs provides a
general view on the evaluated system with
”red-light indicators” (see fig. 3). Links to
more detailed charts or analysis tools are dis-
played next to each of them.
</listItem>
<footnote confidence="0.9998625">
2http://www.python.org/
3Comma-Separated Value
</footnote>
<page confidence="0.996539">
266
</page>
<figureCaption confidence="0.999722333333333">
Figure 3: High-level KPIs with link to more detailed documents. Please note that the success ratio is
calculated via an ad-hoc query and does not necessarily corresponds to the user being or not satisfied.
Figure 4: Dashboard dedicated to a high-level view on usability performance.
</figureCaption>
<page confidence="0.967627">
267
</page>
<listItem confidence="0.9991465">
• Visual dashboards display pre-processed data
according to pre-defined evaluation profiles
(see fig. 4).
• Tools for in-depth individual analysis Fil-
</listItem>
<bodyText confidence="0.955278875">
tered queries permit evaluators to individu-
ally adjust their analysis according to local
evaluation objectives. Queries can be stored
for later use or saved in PDF documents for
distribution to non-MPOWERS users.
End-users, i.e. the evaluators, are limited to dis-
play the results and proceed to in-depth queries.
An administrator access allows for prior data pro-
cessing and the configuration of datasets, KPIs and
dashboards. With collaborative enhancement pur-
poses, the application supports communication be-
tween users with built-in discussion threads infor-
mation feeds and shared to-do-lists to suggest and
negotiate future configurations.
These distinct outlooks on the corpus are
complementary. They combine a high-level
view on the service’s behaviour and performance
with detailed personalised analysis. Whatever
their layouts, every information displayed to the
evaluators-users is retrieved from a unique corpus
and from the same SQL-queries. Therefore, even
if all evaluators consider distinct features on the
evaluated service, our framework brings consis-
tency to their evaluation practices.
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="method">
4 Future work
</sectionHeader>
<bodyText confidence="0.999873090909091">
MPOWERS is on its first development stages.
Several perspectives of enhancement are planned.
First, it requires to be augmented with more KPIs
and in-depth analytical features. Second, as it
only manipulates automated log files, user ques-
tionnaires and third-party annotations are expected
to enrich its evaluation possibilities. Third, we in-
tent MPOWERS to perform comparative evalua-
tions between distinct services in the future. And
last, the framework would benefit from being em-
ployed within real evaluators’ daily activity.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999708125">
The paper presents a platform that supports the
SDS project stakeholders in their evaluation task.
While advocating for a rationalization of evalua-
tion practices among project teams and across or-
ganizations, it promotes the existence of different
cohabiting points of view instead of disregarding
them. When most evaluation contributions cover
the overall evaluation process, from experimental
data collection set-ups to guidance for interpreta-
tion, we limit to a user-centric framework, where
evaluators remain in charge of the evaluation de-
sign. We actually provide them with an opera-
tional framework and unified tools to design and
process their evaluations. This may help initiate
individual, as well as community-wide, gradual
refinements of methodologies.
</bodyText>
<sectionHeader confidence="0.997384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999954">
The demo makes use of the Let’s Go! log files
provided by the Carnegie Mellon University. We
thank Telecom Bretagne, Q. Jin, X. Chen, S.
Zarrad, F. Agez and A. Bolze for their contribu-
tion in the platform deployment.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606966666667">
M. Eskenazi, A. W. Black, A. Raux, and B. Langner.
2008. Let’s Go Lab: a platform for evaluation of
spoken dialog systems with real world users. In In-
terspeech 2008, Brisbane, Australia.
M. Laurent and P. Bretier. 2010. Ad-hoc evaluations
along the lifecycle of industrial spoken dialogue sys-
tems: heading to harmonisation? In LREC 2010,
Malta.
M. Laurent, I. Kanellos, and P. Bretier. 2010. Con-
sidering the subjectivity to rationalise evaluation ap-
proaches: the example of Spoken Dialogue Systems.
In QoMEx’10, Trondheim, Norway.
S. M¨oller, R. Englert, K.-P. Engelbrecht, V. Hafner,
A. Jameson, A. Oulasvirta, A. Raake, and N. Rei-
thinger. 2006. MeMo: towards automatic usability
evaluation of spoken dialogue services by user error.
9th International Conference on Spoken Language.
T. Paek. 2007. Toward evaluation that leads to best
practices: reconciling dialog evaluation in research
and industry. In Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Tech-
nologies, pages 40–47, New York. ACL, Rochester.
ITU-T Rec. P.Sup24. 2005. Parameters describing the
interaction with spoken dialogue systems.
A. Schmitt, G. Bertrand, T. Heinroth, W. Minker, and
J. Liscombe. 2010. Witchcraft: A workbench for
intelligent exploration of human computer conver-
sations. In LREC 2010, Malta.
D. L. Stufflebeam. 1980. L’´evaluation en ´education et
la prise de d´ecision. Ottawa.
</reference>
<page confidence="0.996843">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765874">
<title confidence="0.999581">MPOWERS: a Multi Points Of VieW Evaluation Refinement Studio</title>
<author confidence="0.999269">Marianne Laurent</author>
<author confidence="0.999269">Philippe</author>
<affiliation confidence="0.8825605">Orange Lannion,</affiliation>
<abstract confidence="0.999398380952381">We present our Multi Point Of vieW Evaluation Refinement Studio (MPOWERS), an application framework for Spoken Dialogue System evaluation that implements design conventions in a user-friendly interface. It ensures that all evaluator-users manipulate a unique shared corpus of data with a shared set of parameters to design and retrieve their evaluations. It therefore answers both the need for convergence among the evaluation practices and the consideration of several analytical points of view addressed by the evaluators involved in Spoken Dialogue System projects. After introducing the system architecture, we argue the solution’s added value in supporting a both data-driven and goal-driven process. We conclude with future works and perspectives of improvement upheld by human processes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Eskenazi</author>
<author>A W Black</author>
<author>A Raux</author>
<author>B Langner</author>
</authors>
<title>Let’s Go Lab: a platform for evaluation of spoken dialog systems with real world users.</title>
<date>2008</date>
<booktitle>In Interspeech</booktitle>
<location>Brisbane, Australia.</location>
<marker>Eskenazi, Black, Raux, Langner, 2008</marker>
<rawString>M. Eskenazi, A. W. Black, A. Raux, and B. Langner. 2008. Let’s Go Lab: a platform for evaluation of spoken dialog systems with real world users. In Interspeech 2008, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Laurent</author>
<author>P Bretier</author>
</authors>
<title>Ad-hoc evaluations along the lifecycle of industrial spoken dialogue systems: heading to harmonisation? In LREC</title>
<date>2010</date>
<contexts>
<context position="2222" citStr="Laurent and Bretier, 2010" startWordPosition="327" endWordPosition="330">ta collection and analytical data processing flourish in the domain. On the other hand, however they may not share evaluation needs and methods, the various potential evaluators need to cooperate inside and across projects. This claims for a convergence of evaluation practices toward standardized methodologies. The domain has put a lot of efforts toward the definition of commensurable metrics (Paek, 2007) for comparative evaluations and improved transparency over communications on systems’ performances. Nonetheless, we believe that no one-size-fits-all solution may cover all evaluation needs (Laurent and Bretier, 2010). We therefore work onto the rationalization - not the standardization - of evaluation practices. By rationalization, we refer to the definition of common norms to describe the evaluation protocols; common thinking models and vocabulary, for evaluators to make their procedures explicit. Our Multi Points Of VieW Evaluation Refinement Studio (MPOWERS) facilitates the design, from a unique corpus of parameters, of personalized evaluations adapted to the particular contexts. It does not compete with workbenches like MeMo (M¨oller et al., 2006) or WITcHCRafT (Schmitt et al., 2010) for which the ove</context>
</contexts>
<marker>Laurent, Bretier, 2010</marker>
<rawString>M. Laurent and P. Bretier. 2010. Ad-hoc evaluations along the lifecycle of industrial spoken dialogue systems: heading to harmonisation? In LREC 2010, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Laurent</author>
<author>I Kanellos</author>
<author>P Bretier</author>
</authors>
<title>Considering the subjectivity to rationalise evaluation approaches: the example of Spoken Dialogue Systems.</title>
<date>2010</date>
<booktitle>In QoMEx’10,</booktitle>
<location>Trondheim, Norway.</location>
<contexts>
<context position="1557" citStr="Laurent et al., 2010" startWordPosition="229" endWordPosition="233">tives of improvement upheld by human processes. 1 Introduction The evaluation of Spoken Dialogue Systems (SDS) is a twofold issue. On the one hand, the lack of convention on evaluation criteria and the many different evaluation needs and situations along with SDS projects lead to nomadic evaluation setups and interpretations. We inventoried seven job families contributing to these projects: the marketing people, the business managers, the technical and ergonomics experts, the hosting providers, the contracting owners as well as the actual human operators which integrate SDS in their activity (Laurent et al., 2010). Various experimental protocols for data collection and analytical data processing flourish in the domain. On the other hand, however they may not share evaluation needs and methods, the various potential evaluators need to cooperate inside and across projects. This claims for a convergence of evaluation practices toward standardized methodologies. The domain has put a lot of efforts toward the definition of commensurable metrics (Paek, 2007) for comparative evaluations and improved transparency over communications on systems’ performances. Nonetheless, we believe that no one-size-fits-all so</context>
</contexts>
<marker>Laurent, Kanellos, Bretier, 2010</marker>
<rawString>M. Laurent, I. Kanellos, and P. Bretier. 2010. Considering the subjectivity to rationalise evaluation approaches: the example of Spoken Dialogue Systems. In QoMEx’10, Trondheim, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨oller</author>
<author>R Englert</author>
<author>K-P Engelbrecht</author>
<author>V Hafner</author>
<author>A Jameson</author>
<author>A Oulasvirta</author>
<author>A Raake</author>
<author>N Reithinger</author>
</authors>
<title>MeMo: towards automatic usability evaluation of spoken dialogue services by user error.</title>
<date>2006</date>
<booktitle>9th International Conference on Spoken Language.</booktitle>
<marker>M¨oller, Englert, Engelbrecht, Hafner, Jameson, Oulasvirta, Raake, Reithinger, 2006</marker>
<rawString>S. M¨oller, R. Englert, K.-P. Engelbrecht, V. Hafner, A. Jameson, A. Oulasvirta, A. Raake, and N. Reithinger. 2006. MeMo: towards automatic usability evaluation of spoken dialogue services by user error. 9th International Conference on Spoken Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
</authors>
<title>Toward evaluation that leads to best practices: reconciling dialog evaluation in research and industry.</title>
<date>2007</date>
<booktitle>In Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,</booktitle>
<pages>40--47</pages>
<location>New York. ACL, Rochester.</location>
<contexts>
<context position="2004" citStr="Paek, 2007" startWordPosition="300" endWordPosition="301">ergonomics experts, the hosting providers, the contracting owners as well as the actual human operators which integrate SDS in their activity (Laurent et al., 2010). Various experimental protocols for data collection and analytical data processing flourish in the domain. On the other hand, however they may not share evaluation needs and methods, the various potential evaluators need to cooperate inside and across projects. This claims for a convergence of evaluation practices toward standardized methodologies. The domain has put a lot of efforts toward the definition of commensurable metrics (Paek, 2007) for comparative evaluations and improved transparency over communications on systems’ performances. Nonetheless, we believe that no one-size-fits-all solution may cover all evaluation needs (Laurent and Bretier, 2010). We therefore work onto the rationalization - not the standardization - of evaluation practices. By rationalization, we refer to the definition of common norms to describe the evaluation protocols; common thinking models and vocabulary, for evaluators to make their procedures explicit. Our Multi Points Of VieW Evaluation Refinement Studio (MPOWERS) facilitates the design, from a</context>
</contexts>
<marker>Paek, 2007</marker>
<rawString>T. Paek. 2007. Toward evaluation that leads to best practices: reconciling dialog evaluation in research and industry. In Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies, pages 40–47, New York. ACL, Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ITU-T Rec</author>
</authors>
<title>Parameters describing the interaction with spoken dialogue systems.</title>
<date>2005</date>
<marker>Rec, 2005</marker>
<rawString>ITU-T Rec. P.Sup24. 2005. Parameters describing the interaction with spoken dialogue systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Schmitt</author>
<author>G Bertrand</author>
<author>T Heinroth</author>
<author>W Minker</author>
<author>J Liscombe</author>
</authors>
<title>Witchcraft: A workbench for intelligent exploration of human computer conversations.</title>
<date>2010</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="2804" citStr="Schmitt et al., 2010" startWordPosition="416" endWordPosition="419">ation needs (Laurent and Bretier, 2010). We therefore work onto the rationalization - not the standardization - of evaluation practices. By rationalization, we refer to the definition of common norms to describe the evaluation protocols; common thinking models and vocabulary, for evaluators to make their procedures explicit. Our Multi Points Of VieW Evaluation Refinement Studio (MPOWERS) facilitates the design, from a unique corpus of parameters, of personalized evaluations adapted to the particular contexts. It does not compete with workbenches like MeMo (M¨oller et al., 2006) or WITcHCRafT (Schmitt et al., 2010) for which the overall evaluation process is predefined within the tool. The following section details the solution architecture. Then, we present the MPOWERS’s purposes, emphasizing on its added value for evaluators. Last, we explain the technical and processrelated aspects that must support the system. 2 Architecture of the system The application is built on a classical Business Intelligence (BI) solution that aims to provide decision makers with personalized information (See Fig. 1). We store, in a single datamart, parameters retrieved from heterogeneous sources: interaction logs, user ques</context>
</contexts>
<marker>Schmitt, Bertrand, Heinroth, Minker, Liscombe, 2010</marker>
<rawString>A. Schmitt, G. Bertrand, T. Heinroth, W. Minker, and J. Liscombe. 2010. Witchcraft: A workbench for intelligent exploration of human computer conversations. In LREC 2010, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Stufflebeam</author>
</authors>
<title>L’´evaluation en ´education et la prise de d´ecision.</title>
<date>1980</date>
<location>Ottawa.</location>
<contexts>
<context position="5613" citStr="Stufflebeam (1980)" startWordPosition="847" endWordPosition="848">n the service monitoring for marketing people and business managers and the other one provides the development team with usability-related performance figures (see fig. 4). The unique datamart guarantees all users to work from similar data. Its population requires parsing routines to identify and extract the relevant data. 3 Evaluation process and added value By automating tractable tasks, MPOWERS supports the evaluator-users in their evaluation process driven by decision-making objectives. As sketched in figure 2, our application-supported process is slightly modified from the one defined by Stufflebeam (1980): a process through which one defines, obtains and delivers useful pieces of information that enable to settle between the alter1http://www.spagoworld.org/ native possible decisions. Figure 2: Evaluation process with MPOWERS (grey-tinted stages are supported by the system) Custom-made Python2 routines enable to extract relevant data from the log files. They provide CSV3 formatted files to be converted into SQL scripts. The datamart is designed to be gradually populated from successive evaluation campaigns on one or several SDS. As data may originates from diverse sources, it arrays in differen</context>
</contexts>
<marker>Stufflebeam, 1980</marker>
<rawString>D. L. Stufflebeam. 1980. L’´evaluation en ´education et la prise de d´ecision. Ottawa.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>