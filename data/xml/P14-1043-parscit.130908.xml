<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9986195">
Ambiguity-aware Ensemble Training for Semi-supervised
Dependency Parsing
</title>
<author confidence="0.999164">
Zhenghua Li , Min Zhang∗, Wenliang Chen
</author>
<affiliation confidence="0.9731815">
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University
</affiliation>
<email confidence="0.998">
{zhli13,minzhang,wlchen}@suda.edu.cn
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884">
This paper proposes a simple yet
effective framework for semi-supervised
dependency parsing at entire tree level,
referred to as ambiguity-aware ensemble
training. Instead of only using 1-
best parse trees in previous work, our
core idea is to utilize parse forest
(ambiguous labelings) to combine
multiple 1-best parse trees generated
from diverse parsers on unlabeled data.
With a conditional random field based
probabilistic dependency parser, our
training objective is to maximize mixed
likelihood of labeled data and auto-parsed
unlabeled data with ambiguous labelings.
This framework offers two promising
advantages. 1) ambiguity encoded in
parse forests compromises noise in 1-best
parse trees. During training, the parser is
aware of these ambiguous structures, and
has the flexibility to distribute probability
mass to its preferred parse trees as long
as the likelihood improves. 2) diverse
syntactic structures produced by different
parsers can be naturally compiled into
forest, offering complementary strength
to our single-view parser. Experimental
results on benchmark data show that
our method significantly outperforms
the baseline supervised parser and
other entire-tree based semi-supervised
methods, such as self-training, co-training
and tri-training.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9864554">
Supervised dependency parsing has made great
progress during the past decade. However, it
is very difficult to further improve performance
∗Correspondence author
of supervised parsers. For example, Koo and
Collins (2010) and Zhang and McDonald (2012)
show that incorporating higher-order features into
a graph-based parser only leads to modest increase
in parsing accuracy. In contrast, semi-supervised
approaches, which can make use of large-scale
unlabeled data, have attracted more and more
interest. Previously, unlabeled data is explored to
derive useful local-context features such as word
clusters (Koo et al., 2008), subtree frequencies
(Chen et al., 2009; Chen et al., 2013), and word
co-occurrence counts (Zhou et al., 2011; Bansal
and Klein, 2011). A few effective learning meth-
ods are also proposed for dependency parsing to
implicitly utilize distributions on unlabeled data
(Smith and Eisner, 2007; Wang et al., 2008;
Suzuki et al., 2009). All above work leads to
significant improvement on parsing accuracy.
Another line of research is to pick up some
high-quality auto-parsed training instances from
unlabeled data using bootstrapping methods, such
as self-training (Yarowsky, 1995), co-training
(Blum and Mitchell, 1998), and tri-training (Zhou
and Li, 2005). However, these methods gain
limited success in dependency parsing. Although
working well on constituent parsing (McClosky et
al., 2006; Huang and Harper, 2009), self-training
is shown unsuccessful for dependency parsing
(Spreyer and Kuhn, 2009). The reason may be that
dependency parsing models are prone to amplify
previous mistakes during training on self-parsed
unlabeled data. Sagae and Tsujii (2007) apply
a variant of co-training to dependency parsing
and report positive results on out-of-domain text.
Søgaard and Rishøj (2010) combine tri-training
and parser ensemble to boost parsing accuracy.
Both work employs two parsers to process the
unlabeled data, and only select as extra training
data sentences on which the 1-best parse trees of
the two parsers are identical. In this way, the auto-
parsed unlabeled data becomes more reliable.
</bodyText>
<page confidence="0.975494">
457
</page>
<note confidence="0.732089666666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457–467,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
w0 He1 saw2 a3 deer4 riding5 a6 bicycle7 in8 they park10 .11
</note>
<figureCaption confidence="0.999474">
Figure 1: An example sentence with an ambiguous parse forest.
</figureCaption>
<bodyText confidence="0.998568101449276">
However, one obvious drawback of these methods
is that they are unable to exploit unlabeled data
with divergent outputs from different parsers.
Our experiments show that unlabeled data with
identical outputs from different parsers tends to be
short (18.25 words per sentence on average), and
only has a small proportion of 40% (see Table 6).
More importantly, we believe that unlabeled data
with divergent outputs is equally (if not more)
useful. Intuitively, an unlabeled sentence with
divergent outputs should contain some ambiguous
syntactic structures (such as preposition phrase
attachment) that are very hard to resolve and
lead to the disagreement of different parsers.
Such sentences can provide more discriminative
instances for training which may be unavailable
in labeled data.
To solve above issues, this paper proposes
a more general and effective framework for
semi-supervised dependency parsing, referred to
as ambiguity-aware ensemble training. Different
from traditional self/co/tri-training which only use
1-best parse trees on unlabeled data, our approach
adopts ambiguous labelings, represented by parse
forest, as gold-standard for unlabeled sentences.
Figure 1 shows an example sentence with an
ambiguous parse forest. The forest is formed by
two parse trees, respectively shown at the upper
and lower sides of the sentence. The differences
between the two parse trees are highlighted
using dashed arcs. The upper tree take “deer”
as the subject of “riding”, whereas the lower
one indicates that “he” rides the bicycle. The
other difference is where the preposition phrase
(PP) “in the park” should be attached, which
is also known as the PP attachment problem, a
notorious challenge for parsing. Reserving such
uncertainty has three potential advantages. First,
noise in unlabeled data is largely alleviated, since
parse forest encodes only a few highly possible
parse trees with high oracle score. Please note
that the parse forest in Figure 1 contains four
parse trees after combination of the two different
choices. Second, the parser is able to learn useful
features from the unambiguous parts of the parse
forest. Finally, with sufficient unlabeled data, it is
possible that the parser can learn to resolve such
uncertainty by biasing to more reasonable parse
trees.
To construct parse forest on unlabeled data, we
employ three supervised parsers based on different
paradigms, including our baseline graph-based
dependency parser, a transition-based dependency
parser (Zhang and Nivre, 2011), and a generative
constituent parser (Petrov and Klein, 2007). The
1-best parse trees of these three parsers are aggre-
gated in different ways. Evaluation on labeled data
shows the oracle accuracy of parse forest is much
higher than that of 1-best outputs of single parsers
(see Table 3). Finally, using a conditional random
field (CRF) based probabilistic parser, we train
a better model by maximizing mixed likelihood
of labeled data and auto-parsed unlabeled data
with ambiguous labelings. Experimental results
on both English and Chinese datasets demon-
strate that the proposed ambiguity-aware ensem-
ble training outperforms other entire-tree based
methods such as self/co/tri-training. In summary,
we make following contributions.
</bodyText>
<footnote confidence="0.970718333333333">
1. We propose a generalized ambiguity-aware
ensemble training framework for semi-
supervised dependency parsing, which can
</footnote>
<page confidence="0.993626">
458
</page>
<figure confidence="0.844107">
h s In
(b) adjacent sibling
</figure>
<listItem confidence="0.892610333333333">
make better use of unlabeled data, especially
when parsers from different views produce
divergent syntactic structures.
2. We first employ a generative constituent pars-
er for semi-supervised dependency parsing.
Experiments show that the constituent parser
is very helpful since it produces more diver-
gent structures for our semi-supervised parser
than discriminative dependency parsers.
3. We build the first state-of-the-art CRF-based
dependency parser. Using the probabilistic
parser, we benchmark and conduct systemat-
ic comparisons among ours and all previous
bootstrapping methods, including self/co/tri-
training.
</listItem>
<sectionHeader confidence="0.945897" genericHeader="method">
2 Supervised Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999327526315789">
Given an input sentence x = w0w1...wn, the goal
of dependency parsing is to build a dependency
tree as depicted in Figure 1, denoted by d =
{(h, m) : 0 &lt; h &lt; n, 0 &lt; m &lt; n}, where (h, m)
indicates a directed arc from the head word wh
to the modifier wm, and w0 is an artificial node
linking to the root of the sentence.
In parsing community, two mainstream meth-
ods tackle the dependency parsing problem from
different perspectives but achieve comparable ac-
curacy on a variety of languages. The graph-
based method views the problem as finding an
optimal tree from a fully-connected directed graph
(McDonald et al., 2005; McDonald and Pereira,
2006; Carreras, 2007; Koo and Collins, 2010),
while the transition-based method tries to find a
highest-scoring transition sequence that leads to
a legal dependency tree (Yamada and Matsumoto,
2003; Nivre, 2003; Zhang and Nivre, 2011).
</bodyText>
<subsectionHeader confidence="0.881052">
2.1 Graph-based Dependency Parser
</subsectionHeader>
<bodyText confidence="0.93214975">
(GParser)
In this work, we adopt the graph-based paradigm
because it allows us to naturally derive conditional
probability of a dependency tree d given a sen-
tence x, which is required to compute likelihood
of both labeled and unlabeled data. Under the
graph-based model, the score of a dependency tree
is factored into the scores of small subtrees p.
</bodyText>
<equation confidence="0.991276">
Score(x, d; w) = w &apos; f(x, d)
�= Score(x, p; w)
</equation>
<page confidence="0.764292">
p⊆d
</page>
<figure confidence="0.968795">
h In
(a) single dependency
</figure>
<figureCaption confidence="0.9267755">
Figure 2: Two types of scoring subtrees in our
second-order graph-based parsers.
</figureCaption>
<construct confidence="0.5212462">
Dependency features fdep(x, h, m):
wh, wm, th, tm, th±1, tm±1, tb, dir(h, m), dist(h, m)
Sibling features fsib(x, h, m, s):
wh, ws, wm, th, tm, ts, th±1, tm±1, ts±1
dir(h, m), dist(h, m)
</construct>
<tableCaption confidence="0.96663">
Table 1: Brief illustration of the syntactic features.
</tableCaption>
<bodyText confidence="0.97820175">
ti denotes the POS tag of wi. b is an index
between h and m. dir(i, j) and dist(i, j) denote
the direction and distance of the dependency (i, j).
We adopt the second-order graph-based depen-
dency parsing model of McDonald and Pereira
(2006) as our core parser, which incorporates
features from the two kinds of subtrees in Fig. 2.1
Then the score of a dependency tree is:
</bodyText>
<equation confidence="0.99554825">
�Score(x, d; w) = wdep &apos; fdep(x, h, m)
{(h,m)}⊆d
+ � wsib &apos; fsib(x, h, s, m)
{(h,s),(h,m)}⊆d
</equation>
<bodyText confidence="0.999958153846154">
where fdep(x, h, m) and fsib(x, h, s, m) are the
feature vectors of the two subtree in Fig. 2;
wdep/sib are feature weight vectors; the dot prod-
uct gives scores contributed by corresponding sub-
trees.
For syntactic features, we adopt those of Bohnet
(2010) which include two categories correspond-
ing to the two types of scoring subtrees in Fig. 2.
We summarize the atomic features used in each
feature category in Table 1. These atomic features
are concatenated in different combinations to com-
pose rich feature sets. Please refer to Table 4 of
Bohnet (2010) for the complete feature list.
</bodyText>
<subsectionHeader confidence="0.998634">
2.2 CRF-based GParser
</subsectionHeader>
<bodyText confidence="0.9922856">
Previous work on graph-based dependency pars-
ing mostly adopts linear models and perceptron
based training procedures, which lack probabilis-
tic explanations of dependency trees and do not
need to compute likelihood of labeled training
</bodyText>
<footnote confidence="0.99840325">
1Higher-order models of Carreras (2007) and Koo and
Collins (2010) can achieve higher accuracy, but has much
higher time cost (O(n4)). Our approach is applicable to these
higher-order models, which we leave for future work.
</footnote>
<page confidence="0.998504">
459
</page>
<bodyText confidence="0.9969195">
data. Instead, we build a log-linear CRF-based
dependency parser, which is similar to the CRF-
based constituent parser of Finkel et al. (2008).
Assuming the feature weights w are known, the
probability of a dependency tree d given an input
sentence x is defined as:
</bodyText>
<equation confidence="0.989309333333333">
exp{5core(x, d; w)}
p(d|x;w) =
Z(x; w) (1)
</equation>
<bodyText confidence="0.9761586">
where Z(x) is the normalization factor and Y(x)
is the set of all legal dependency trees for x.
Suppose the labeled training data is
D = {(xi, di)}Ni=1. Then the log likelihood
of D is:
</bodyText>
<equation confidence="0.994386">
L(D; w) = XN log p(di|xi; w)
i=1
</equation>
<bodyText confidence="0.999835481481482">
The training objective is to maximize the log
likelihood of the training data L(D). The partial
derivative with respect to the feature weights w is:
not sufficiently covered in manually labeled
data. Therefore, exploiting such unlabeled data
may introduce more discriminative syntactic
knowledge, largely compensating labeled training
data.
To address above issues, we propose ambiguity-
aware ensemble training, which can be interpreted
as a generalized tri-training framework. The key
idea is the use of ambiguous labelings for the
purpose of aggregating multiple 1-best parse trees
produced by several diverse parsers. Here, “am-
biguous labelings” mean an unlabeled sentence
may have multiple parse trees as gold-standard
reference, represented by parse forest (see Figure
1). The training procedure aims to maximize
mixed likelihood of both manually labeled and
auto-parsed unlabeled data with ambiguous label-
ings. For an unlabeled instance, the model is
updated to maximize the probability of its parse
forest, instead of a single parse tree in traditional
tri-training. In other words, the model is free to
distribute probability mass among the trees in the
parse forest to its liking, as long as the likelihood
improves (T¨ackstr¨om et al., 2013).
</bodyText>
<equation confidence="0.9989206">
X
Z(x; w) = exp{5core(x, d′; w)}
d′∈Y(x)
aL(D; w) = XN  X f(xi,di) − 
aw i=1   d′∈Y(xi) p(d′|xi; w)f(xi, d′)  
</equation>
<bodyText confidence="0.996299888888889">
(2)
where the first term is the empirical counts and
the second term is the model expectations. Since
Y(xi) contains exponentially many dependency
trees, direct calculation of the second term is
prohibitive. Instead, we can use the classic inside-
outside algorithm to efficiently compute the model
expectations within O(n3) time complexity, where
n is the input sentence length.
</bodyText>
<sectionHeader confidence="0.969269" genericHeader="method">
3 Ambiguity-aware Ensemble Training
</sectionHeader>
<bodyText confidence="0.999995153846154">
In standard entire-tree based semi-supervised
methods such as self/co/tri-training, automatically
parsed unlabeled sentences are used as additional
training data, and noisy 1-best parse trees are
considered as gold-standard. To alleviate the
noise, the tri-training method only uses unlabeled
data on which multiple parsers from different
views produce identical parse trees. However,
unlabeled data with divergent syntactic structures
should be more useful. Intuitively, if several
parsers disagree on an unlabeled sentence, it
implies that the unlabeled sentence contains
some difficult syntactic phenomena which are
</bodyText>
<subsectionHeader confidence="0.999302">
3.1 Likelihood of the Unlabeled Data
</subsectionHeader>
<bodyText confidence="0.980942733333333">
The auto-parsed unlabeled data with ambiguous
labelings is denoted as D′ = {(ui, Vi)}Mi=1, where
ui is an unlabeled sentence, and Vi is the corre-
sponding parse forest. Then the log likelihood of
D′ is:
where
w) is the conditional probability of
given
as defined in Eq. (1). For an unlabeled
sentence
the probability of its parse forest
is
the summation of the probabilities of all the parse
trees contained in the forest.
Then we can derive the partial deri
</bodyText>
<equation confidence="0.970035">
p(d′|ui;
d′
ui,
ui,
Vi
</equation>
<bodyText confidence="0.744504">
vative of the
</bodyText>
<equation confidence="0.9064998">
log likelihood with respect to w:
d′∈Vi
�−
d′∈Y(ui)
(3)
</equation>
<bodyText confidence="0.882727">
where
</bodyText>
<equation confidence="0.996075111111111">
˜p(d′|ui, Vi; w) is the probability of d′ un-

 
=
XM
i=1


˜p(d′|ui,Viiw)f(ui, d′)
p(d′|uiiw)f(ui, d′)
aL(D′; w)
aw
 
 X
log p(d′|ui; w) 
d′∈Vi
L(D′; w) = XM
i=1
</equation>
<page confidence="0.965768">
460
</page>
<bodyText confidence="0.959098833333333">
der the space constrained by the parse forest Vi.
The second term in Eq. (3) is the same with the
second term in Eq. (2). The first term in Eq. (3)
can be efficiently computed by running the inside-
outside algorithm in the constrained search space
Vi.
</bodyText>
<subsectionHeader confidence="0.975295">
3.2 Stochastic Gradient Descent (SGD)
Training
</subsectionHeader>
<bodyText confidence="0.9993659375">
We apply L2-norm regularized SGD training to
iteratively learn feature weights w for our CRF-
based baseline and semi-supervised parsers. We
follow the implementation in CRFsuite.2 At each
step, the algorithm approximates a gradient with
a small subset of the training examples, and then
updates the feature weights. Finkel et al. (2008)
show that SGD achieves optimal test performance
with far fewer iterations than other optimization
routines such as L-BFGS. Moreover, it is very
convenient to parallel SGD since computations
among examples in the same batch is mutually
independent.
Training with the combined labeled and unla-
beled data, the objective is to maximize the mixed
likelihood:
</bodyText>
<equation confidence="0.826239">
L(D; D′) = L(D) + L(D′)
</equation>
<bodyText confidence="0.981687933333333">
Since D′ contains much more instances than D
(1.7M vs. 40K for English, and 4M vs. 16K for
Chinese), it is likely that the unlabeled data may
overwhelm the labeled data during SGD training.
Therefore, we propose a simple corpus-weighting
strategy, as shown in Algorithm 1, where Dbi,k
is the subset of training data used in kth update
and b is the batch size; ηk is the update step,
which is adjusted following the simulated anneal-
ing procedure (Finkel et al., 2008). The idea is
to use a fraction of training data (Di) at each
iteration, and do corpus weighting by randomly
sampling labeled and unlabeled instances in a
certain proportion (N1 vs. M1).
Once the feature weights w are learnt, we can
</bodyText>
<footnote confidence="0.957363">
2http://www.chokkan.org/software/crfsuite/
</footnote>
<construct confidence="0.382121">
Algorithm 1 SGD training with mixed labeled and
unlabeled data.
</construct>
<listItem confidence="0.981898166666667">
1: Input: Labeled data D = {(xi, di)}Ni=1, and unlabeled
data D′ = {(ui, Vi)}Mj=1; Parameters: I, N1, M1, b
2: Output: w
3: Initialization: w(0) = 0, k = 0;
4: for i = 1 to I do {iterations}
5: Randomly select N1 instances from D and M1
instances from D′ to compose a new dataset Di, and
shuffle it.
6: Traverse Di: a small batch Dbi,k C Di at one step.
7: wk+1 = wk + ηk1b ∇L(Dbi,k; wk)
8: k = k + 1
9: end for
</listItem>
<bodyText confidence="0.448913">
parse the test data to find the optimal parse tree.
</bodyText>
<equation confidence="0.99629325">
d* = arg max p(d′|x; w)
d′EY(x)
= arg max 5core(x, d′; w)
d′EY(x)
</equation>
<bodyText confidence="0.98088">
This can be done with the Viterbi decoding algo-
rithm described in McDonald and Pereira (2006)
in O(n3) parsing time.
</bodyText>
<subsectionHeader confidence="0.981797">
3.3 Forest Construction with Diverse Parsers
</subsectionHeader>
<bodyText confidence="0.9996027">
To construct parse forests for unlabeled data, we
employ three diverse parsers, i.e., our baseline
GParser, a transition-based parser (ZPar3) (Zhang
and Nivre, 2011), and a generative constituen-
t parser (Berkeley Parser4) (Petrov and Klein,
2007). These three parsers are trained on labeled
data and then used to parse each unlabeled sen-
tence. We aggregate the three parsers’ outputs on
unlabeled data in different ways and evaluate the
effectiveness through experiments.
</bodyText>
<sectionHeader confidence="0.998534" genericHeader="method">
4 Experiments and Analysis
</sectionHeader>
<bodyText confidence="0.999589769230769">
To verify the effectiveness of our proposed ap-
proach, we conduct experiments on Penn Tree-
bank (PTB) and Penn Chinese Treebank 5.1 (CT-
B5). For English, we follow the popular practice
to split data into training (sections 2-21), devel-
opment (section 22), and test (section 23). For
CTB5, we adopt the data split of (Duan et al.,
2007). We convert original bracketed structures
into dependency structures using Penn2Malt with
its default head-finding rules.
For unlabeled data, we follow Chen et al. (2013)
and use the BLLIP WSJ corpus (Charniak et al.,
2000) for English and Xinhua portion of Chinese
</bodyText>
<footnote confidence="0.996889">
3http://people.sutd.edu.sg/˜yue_zhang/doc/
4https://code.google.com/p/berkeleyparser/
</footnote>
<equation confidence="0.95677725">
˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)}
Z(ui, Vi; w)
Z(ui, Vi; w) = � exp{5core(ui, d′; w)}
d′EVi
</equation>
<page confidence="0.967991">
461
</page>
<table confidence="0.996851333333333">
Train Dev Test Unlabeled
PTB 39,832 1,700 2,416 1.7M
CTB5 16,091 803 1,910 4M
</table>
<tableCaption confidence="0.994549">
Table 2: Data sets (in sentence number).
</tableCaption>
<bodyText confidence="0.993054615384615">
Gigaword Version 2.0 (LDC2009T14) (Huang,
2009) for Chinese. We build a CRF-based bigram
part-of-speech (POS) tagger with the features de-
scribed in (Li et al., 2012), and produce POS tags
for all train/development/test/unlabeled sets (10-
way jackknifing for training sets). The tagging ac-
curacy on test sets is 97.3% on English and 94.0%
on Chinese. Table 2 shows the data statistics.
We measure parsing performance using the s-
tandard unlabeled attachment score (UAS), ex-
cluding punctuation marks. For significance test,
we adopt Dan Bikel’s randomized parsing evalua-
tion comparator (Noreen, 1989).5
</bodyText>
<subsectionHeader confidence="0.978438">
4.1 Parameter Setting
</subsectionHeader>
<bodyText confidence="0.999974043478261">
When training our CRF-based parsers with SGD,
we use the batch size b = 100 for all experiments.
We run SGD for I = 100 iterations and choose
the model that performs best on development
data. For the semi-supervised parsers trained with
Algorithm 1, we use N1 = 20K and M1 = 50K
for English, and N1 = 15K and M1 = 50K for
Chinese, based on a few preliminary experiments.
To accelerate the training, we adopt parallelized
implementation of SGD and employ 20 threads for
each run. For semi-supervised cases, one iteration
takes about 2 hours on an IBM server having 2.0
GHz Intel Xeon CPUs and 72G memory.
Default parameter settings are used for training
ZPar and Berkeley Parser. We run ZPar for 50
iterations, and choose the model that achieves
highest accuracy on the development data. For
Berkeley Parser, we use the model after 5 split-
merge iterations to avoid over-fitting the train-
ing data according to the manual. The phrase-
structure outputs of Berkeley Parser are converted
into dependency structures using the same head-
finding rules.
</bodyText>
<subsectionHeader confidence="0.994504">
4.2 Methodology Study on Development Data
</subsectionHeader>
<bodyText confidence="0.9999802">
Using three supervised parsers, we have many
options to construct parse forest on unlabeled data.
To examine the effect of different ways for forest
construction, we conduct extensive methodology
study on development data. Table 3 presents the
</bodyText>
<footnote confidence="0.70761">
5http://www.cis.upenn.edu/˜dbikel/software.html
</footnote>
<bodyText confidence="0.99990531372549">
results. We divide the systems into three types: 1)
supervised single parsers; 2) CRF-based GParser
with conventional self/co/tri-training; 3) CRF-
based GParser with our approach. For the latter
two cases, we also present the oracle accuracy and
averaged head number per word (“Head/Word”)
of parse forest when applying different ways to
construct forests on development datasets.
The first major row presents performance of
the three supervised parsers. We can see that the
three parsers achieve comparable performance on
English, but the performance of ZPar is largely
inferior on Chinese.
The second major row shows the results when
we use single 1-best parse trees on unlabeled
data. When using the outputs of GParser itself
(“Unlabeled ← G”), the experiment reproduces
traditional self-training. The results on both En-
glish and Chinese re-confirm that self-training
may not work for dependency parsing, which
is consistent with previous studies (Spreyer and
Kuhn, 2009). The reason may be that dependency
parsers are prone to amplify previous mistakes on
unlabeled data during training.
The next two experiments in the second ma-
jor row reimplement co-training, where another
parser’s 1-best results are projected into unlabeled
data to help the core parser. Using unlabeled
data with the results of ZPar (“Unlabeled ← Z”)
significantly outperforms the baseline GParser by
0.30% (93.15-82.85) on English. However, the
improvement on Chinese is not significant. Using
unlabeled data with the results of Berkeley Parser
(“Unlabeled ← B”) significantly improves parsing
accuracy by 0.55% (93.40-92.85) on English and
1.06% (83.34-82.28) on Chinese. We believe the
reason is that being a generative model designed
for constituent parsing, Berkeley Parser is more
different from discriminative dependency parsers,
and therefore can provide more divergent syntactic
structures. This kind of syntactic divergence is
helpful because it can provide complementary
knowledge from a different perspective. Surdeanu
and Manning (2010) also show that the diversity of
parsers is important for performance improvement
when integrating different parsers in the super-
vised track. Therefore, we can conclude that
co-training helps dependency parsing, especially
when using a more divergent parser.
The last experiment in the second major row
is known as tri-training, which only uses unla-
</bodyText>
<page confidence="0.998424">
462
</page>
<table confidence="0.999441142857143">
English Chinese
UAS Oracle Head/Word UAS Oracle Head/Word
GParser 92.85 82.28
Supervised ZPar 92.50 — — 81.04 — —
Berkeley 92.70 82.46
Unlabeled G (self-train) 92.88 92.85 82.14 82.28
Semi-supervised GParser Unlabeled Z (co-train) 93.15 † 92.50 1.000 82.54 81.04 1.000
with Single 1-best Trees Unlabeled B (co-train) 93.40 † 92.70 83.34 † 82.46
Unlabeled B=Z (tri-train) 93.50 † 97.52 83.10 † 95.05
Unlabeled Z+G 93.18 † 94.97 1.053 82.78 86.66 1.136
Unlabeled B+G 93.35 † 96.37 1.080 83.24 † 89.72 1.188
Semi-supervised GParser Unlabeled B+Z 93.78 †t 96.18 1.082 83.86 †t 89.54 1.199
Ambiguity-aware Ensemble Unlabeled B+(ZnG) 93.77 †t 95.60 1.050 84.26 †t 87.76 1.106
Unlabeled B+Z+G 93.50 † 96.95 1.112 83.30 † 91.50 1.281
</table>
<tableCaption confidence="0.957404666666667">
Table 3: Main results on development data. G is short for GParser, Z for ZPar, and B for Berkeley Parser.
t means the corresponding parser significantly outperforms supervised parsers, and t means the result
significantly outperforms co/tri-training at confidence level of p &lt; 0.01.
</tableCaption>
<bodyText confidence="0.997297069444445">
beled sentences on which Berkeley Parser and
ZPar produce identical outputs (“Unlabeled &lt;--
B=Z”). We can see that with the verification of
two views, the oracle accuracy is much higher
than using single parsers (97.52% vs. 92.85% on
English, and 95.06% vs. 82.46% on Chinese).
Although using less unlabeled sentences (0.7M
for English and 1.2M for Chinese), tri-training
achieves comparable performance to co-training
(slightly better on English and slightly worse on
Chinese).
The third major row shows the results of
the semi-supervised GParser with our proposed
approach. We experiment with different com-
binations of the 1-best parse trees of the three
supervised parsers. The first three experiments
combine 1-best outputs of two parsers to compose
parse forest on unlabeled data. “Unlabeled &lt;--
B+(Zf1G)” means that the parse forest is initialized
with the Berkeley parse and augmented with the
intersection of dependencies of the 1-best outputs
of ZPar and GParser. In the last setting, the parse
forest contains all three 1-best results.
When the parse forests of the unlabeled data
are the union of the outputs of GParser and ZPar,
denoted as “Unlabeled &lt;-- Z+G”, each word has
1.053 candidate heads on English and 1.136 on
Chinese, and the oracle accuracy is higher than
using 1-best outputs of single parsers (94.97%
vs. 92.85% on English, 86.66% vs. 82.46%
on Chinese). However, we find that although
the parser significantly outperforms the supervised
GParser on English, it does not gain significant im-
provement over co-training with ZPar (“Unlabeled
&lt;-- Z”) on both English and Chinese.
Combining the outputs of Berkeley Parser and
GParser (“Unlabeled &lt;-- B+G”), we get higher
oracle score (96.37% on English and 89.72% on
Chinese) and higher syntactic divergence (1.085
candidate heads per word on English, and 1.188
on Chinese) than “Unlabeled &lt;-- Z+G”, which
verifies our earlier discussion that Berkeley Pars-
er produces more different structures than ZPar.
However, it leads to slightly worse accuracy than
co-training with Berkeley Parser (“Unlabeled &lt;--
B”). This indicates that adding the outputs of
GParser itself does not help the model.
Combining the outputs of Berkeley Parser and
ZPar (“Unlabeled &lt;-- B+Z”), we get the best per-
formance on English, which is also significantly
better than both co-training (“Unlabeled &lt;-- B”)
and tri-training (“Unlabeled &lt;-- B=Z”) on both
English and Chinese. This demonstrates that our
proposed approach can better exploit unlabeled
data than traditional self/co/tri-training. More
analysis and discussions are in Section 4.4.
During experimental trials, we find that “Unla-
beled &lt;-- B+(Zf1G)” can further boost performance
on Chinese. A possible explanation is that by
using the intersection of the outputs of GParser
and ZPar, the size of the parse forest is better
controlled, which is helpful considering that ZPar
performs worse on this data than both Berkeley
Parser and GParser.
Adding the output of GParser itself (“Unlabeled
&lt;-- B+Z+G”) leads to accuracy drop, although the
oracle score is higher (96.95% on English and
91.50% on Chinese) than “Unlabeled &lt;-- B+Z”.
We suspect the reason is that the model is likely to
distribute the probability mass to these parse trees
produced by itself instead of those by Berkeley
Parser or ZPar under this setting.
</bodyText>
<page confidence="0.999335">
463
</page>
<table confidence="0.493258181818182">
Sup Semi
McDonald and Pereira (2006) 91.5
Koo and Collins (2010) [higher-order] 93.04 —
Zhang and McDonald (2012) [higher-order] 93.06
Zhang and Nivre (2011) [higher-order] 92.9
Koo et al. (2008) [higher-order] 92.02 93.16
Chen et al. (2009) [higher-order] 92.40 93.16
Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79
Zhou et al. (2011) [higher-order] 91.98 92.64
Chen et al. (2013) [higher-order] 92.76 93.77
This work 92.34 93.19
</table>
<tableCaption confidence="0.999293">
Table 4: UAS comparison on English test data.
</tableCaption>
<bodyText confidence="0.999889">
In summary, we can conclude that our proposed
ambiguity-aware ensemble training is significant-
ly better than both the supervised approaches and
the semi-supervised approaches that use 1-best
parse trees. Appropriately composing the forest
parse, our approach outperforms the best results of
co-training or tri-training by 0.28% (93.78-93.50)
on English and 0.92% (84.26-83.34) on Chinese.
</bodyText>
<subsectionHeader confidence="0.99987">
4.3 Comparison with Previous Work
</subsectionHeader>
<bodyText confidence="0.9869827">
We adopt the best settings on development data
for semi-supervised GParser with our proposed
approach, and make comparison with previous
results on test data. Table 4 shows the results.
The first major row lists several state-of-the-
art supervised methods. McDonald and Pereira
(2006) propose a second-order graph-based parser,
but use a smaller feature set than our work. Koo
and Collins (2010) propose a third-order graph-
based parser. Zhang and McDonald (2012) ex-
plore higher-order features for graph-based de-
pendency parsing, and adopt beam search for
fast decoding. Zhang and Nivre (2011) propose
a feature-rich transition-based parser. All work
in the second major row adopts semi-supervised
methods. The results show that our approach
achieves comparable accuracy with most previous
semi-supervised methods. Both Suzuki et al.
(2009) and Chen et al. (2013) adopt the higher-
order parsing model of Carreras (2007), and Suzu-
ki et al. (2009) also incorporate word cluster
features proposed by Koo et al. (2008) in their sys-
tem. We expect our approach may achieve higher
performance with such enhancements, which we
leave for future work. Moreover, our method
may be combined with other semi-supervised ap-
proaches, since they are orthogonal in method-
ology and utilize unlabeled data from different
perspectives.
Table 5 make comparisons with previous results
</bodyText>
<note confidence="0.937253833333333">
Li et al. (2012) [joint]
Bohnet and Nivre (2012) [joint]
Chen et al. (2013) [higher-order]
This work
Chen et al. (2013) [higher-order]
This work
</note>
<tableCaption confidence="0.995785">
Table 5: UAS comparison on Chinese test data.
</tableCaption>
<table confidence="0.999820666666667">
Unlabeled data UAS #Sent Len Head/Word Oracle
NULL 92.34 0 — — —
Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65
Low divergence 92.94 0.5M 28.19 1.062 96.53
High divergence 93.03 0.5M 27.85 1.211 94.28
ALL 93.19 1.7M 24.15 1.087 96.09
</table>
<tableCaption confidence="0.94759175">
Table 6: Performance of our semi-supervised
GParser with different sets of “Unlabeled ←
B+Z” on English test set. “Len” means averaged
sentence length.
</tableCaption>
<bodyText confidence="0.994507636363636">
on Chinese test data. Li et al. (2012) and Bohnet
and Nivre (2012) use joint models for POS tagging
and dependency parsing, significantly outperform-
ing their pipeline counterparts. Our approach can
be combined with their work to utilize unlabeled
data to improve both POS tagging and parsing
simultaneously. Our work achieves comparable
accuracy with Chen et al. (2013), although they
adopt the higher-order model of Carreras (2007).
Again, our method may be combined with their
work to achieve higher performance.
</bodyText>
<subsectionHeader confidence="0.998844">
4.4 Analysis
</subsectionHeader>
<bodyText confidence="0.984971">
To better understand the effectiveness of our pro-
posed approach, we make detailed analysis using
the semi-supervised GParser with “Unlabeled ←
B+Z” on English datasets.
Contribution of unlabeled data with regard
to syntactic divergence: We divide the unlabeled
data into three sets according to the divergence of
the 1-best outputs of Berkeley Parser and ZPar.
The first set contains those sentences that the two
parsers produce identical parse trees, denoted by
“consistent”, which corresponds to the setting for
tri-training. Other sentences are split into two sets
according to averaged number of heads per word
in parse forests, denoted by “low divergence” and
“high divergence” respectively. Then we train
semi-supervised GParser using the three sets of
unlabeled data. Table 6 illustrates the results and
statistics. We can see that unlabeled data with
identical outputs from Berkeley Parser and ZPar
tends to be short sentences (18.25 words per sen-
</bodyText>
<figure confidence="0.630587111111111">
Supervised
Semi
UAS
82.37
81.42
81.01
81.14
83.08
82.89
</figure>
<page confidence="0.999743">
464
</page>
<bodyText confidence="0.982256411764706">
tence on average). Results show all the three sets
of unlabeled data can help the parser. Especially,
the unlabeled data with highly divergent struc-
tures leads to slightly higher improvement. This
demonstrates that our approach can better exploit
unlabeled data on which parsers of different views
produce divergent structures.
Impact of unlabeled data size: To under-
stand how our approach performs with regards to
the unlabeled data size, we train semi-supervised
GParser with different sizes of unlabeled data. Fig.
3 shows the accuracy curve on the test set. We
can see that the parser consistently achieves higher
accuracy with more unlabeled data, demonstrating
the effectiveness of our approach. We expect
that our approach has potential to achieve higher
accuracy with more additional data.
</bodyText>
<figure confidence="0.9244145">
0 50K 100K 200K 500K 1M 1.7M
Unlabeled Data Size
</figure>
<figureCaption confidence="0.988375">
Figure 3: Performance of GParser with different
sizes of “Unlabeled ← B+Z” on English test set.
</figureCaption>
<sectionHeader confidence="0.99995" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999852875">
Our work is originally inspired by the work of
T¨ackstr¨om et al. (2013). They first apply the
idea of ambiguous labelings to multilingual parser
transfer in the unsupervised parsing field, which
aims to build a dependency parser for a resource-
poor target language by making use of source-
language treebanks. Different from their work, we
explore the idea for semi-supervised dependency
parsing where a certain amount of labeled training
data is available. Moreover, we for the first
time build a state-of-the-art CRF-based depen-
dency parser and conduct in-depth comparisons
with previous methods. Similar ideas of learning
with ambiguous labelings are previously explored
for classification (Jin and Ghahramani, 2002) and
sequence labeling problems (Dredze et al., 2009).
Our work is also related with the parser ensem-
ble approaches such as stacked learning and re-
parsing in the supervised track. Stacked learning
uses one parser’s outputs as guide features for
another parser, leading to improved performance
(Nivre and McDonald, 2008; Torres Martins et
al., 2008). Re-parsing merges the outputs of
several parsers into a dependency graph, and then
apply Viterbi decoding to find a better tree (Sagae
and Lavie, 2006; Surdeanu and Manning, 2010).
One possible drawback of parser ensemble is that
several parsers are required to parse the same
sentence during the test phase. Moreover, our
approach can benefit from these methods in that
we can get parse forests of higher quality on
unlabeled data (Zhou, 2009).
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999866607142857">
This paper proposes a generalized training
framework of semi-supervised dependency
parsing based on ambiguous labelings. For
each unlabeled sentence, we combine the 1-best
parse trees of several diverse parsers to compose
ambiguous labelings, represented by a parse
forest. The training objective is to maximize the
mixed likelihood of both the labeled data and
the auto-parsed unlabeled data with ambiguous
labelings. Experiments show that our framework
can make better use of the unlabeled data,
especially those with divergent outputs from
different parsers, than traditional tri-training.
Detailed analysis demonstrates the effectiveness
of our approach. Specifically, we find that our
approach is very effective when using divergent
parsers such as the generative parser, and it is also
helpful to properly balance the size and oracle
accuracy of the parse forest of the unlabeled data.
For future work, among other possible
extensions, we would like to see how our
approach performs when employing more diverse
parsers to compose the parse forest of higher
quality for the unlabeled data, such as the easy-
first non-directional dependency parser (Goldberg
and Elhadad, 2010) and other constituent parsers
(Collins and Koo, 2005; Charniak and Johnson,
2005; Finkel et al., 2008).
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995624">
The authors would like to thank the critical
and insightful comments from our anonymous
reviewers. This work was supported by National
Natural Science Foundation of China (Grant No.
61373095, 61333018).
</bodyText>
<figure confidence="0.997556916666667">
UAS
93.2
93.1
92.9
92.8
92.7
92.6
92.5
92.4
92.3
93
B+Z Parser
</figure>
<page confidence="0.999403">
465
</page>
<sectionHeader confidence="0.995752" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997404245098039">
Mohit Bansal and Dan Klein. 2011. Web-scale
features for full-scale parsing. In Proceedings of
ACL, pages 693–702.
Avrim Blum and Tom Mitchell. 1998. Combining
labeled and unlabeled data with co-training. In
Proceedings of the 11th Annual Conference on
Computational Learning Theory, pages 92–100.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and
labeled non-projective dependency parsing. In
Proceedings of EMILP 2012, pages 1455–1465.
Bernd Bohnet. 2010. Top accuracy and fast
dependency parsing is not a contradiction. In
Proceedings of COLIIG, pages 89–97.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMILP/CoILL, pages 141–150.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings ofACL, pages 173–180.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP
1987-89 WSJ Corpus Release 1, LDC2000T43.
Linguistic Data Consortium.
Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving
dependency parsing with subtrees from auto-parsed
data. In Proceedings ofEMILP, pages 570–579.
Wenliang Chen, Min Zhang, and Yue Zhang. 2013.
Semi-supervised feature transformation for depen-
dency parsing. In Proceedings of EMILP, pages
1303–1313.
Michael J. Collins and Terry Koo. 2005. Dis-
criminative reranking for natural language parsing.
Computational Linguistics, pages 25–70.
Mark Dredze, Partha Pratim Talukdar, and Koby
Crammer. 2009. Sequence learning from data
with multiple labels. In ECML/PKDD Workshop on
Learning from Multi-Label Data.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic models for action-based Chinese dependency
parsing. In Proceedings of ECML/ECPPKDD,
pages 559–566.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condition-
al random field parsing. In Proceedings of ACL,
pages 959–967.
Yoav Goldberg and Michael Elhadad. 2010. An
efficient algorithm for easy-first non-directional
dependency parsing. In Proceedings ofIAACL.
Zhongqiang Huang and Mary Harper. 2009. Self-
training PCFG grammars with latent annotations
across languages. In Proceedings of EMILP 2009,
pages 832–841.
Chu-Ren Huang. 2009. Tagged Chinese Gigaword
Version 2.0, LDC2009T14. Linguistic Data
Consortium.
Rong Jin and Zoubin Ghahramani. 2002. Learning
with multiple labels. In Proceedings ofIIPS.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In ACL, pages 1–11.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings ofACL, pages 595–603.
Zhenghua Li, Min Zhang, Wanxiang Che, and Ting
Liu. 2012. A separately passive-aggressive training
algorithm for joint POS tagging and dependency
parsing. In COLIIG 2012, pages 1681–1698.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the IAACL, pages 152–159.
Ryan McDonald and Fernando Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL, pages 81–88.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of ACL, pages
91–98.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings ofACL, pages 950–958.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT, pages 149–160.
Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses: An introduction. John Wiley
&amp; Sons, Inc., New York.
Slav Petrov and Dan Klein. 2007. Improved
inference for unlexicalized parsing. In Proceedings
ofIAACL.
Kenji Sagae and Alon Lavie. 2006. Parser
combination by reparsing. In Proceedings of
IAACL, pages 129–132.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models
and parser ensembles. In Proceedings of the
CoILL Shared Task Session of EMILP-CoILL,
pages 1044–1050.
David A. Smith and Jason Eisner. 2007. Bootstrap-
ping feature-rich dependency parsers with entropic
priors. In Proceedings ofEMILP, pages 667–677.
</reference>
<page confidence="0.994956">
466
</page>
<reference confidence="0.999828529411764">
Anders Søgaard and Christian Rishøj. 2010. Semi-
supervised dependency parsing using generalized
tri-training. In Proceedings of ACL, pages 1065–
1073.
Kathrin Spreyer and Jonas Kuhn. 2009. Data-
driven dependency parsing of new languages using
incomplete and noisy training data. In CoNLL,
pages 12–20.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Proceedings of NAACL, pages 649–
652.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of
semi-supervised structured conditional models for
dependency parsing. In Proceedings of EMNLP,
pages 551–560.
Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discriminative
transfer parsers. In Proceedings of NAACL, pages
1061–1071.
Andr´e Filipe Torres Martins, Dipanjan Das, Noah A.
Smith, and Eric P. Xing. 2008. Stacking
dependency parsers. In Proceedings of EMNLP,
pages 157–166.
Qin Iris Wang, Dale Schuurmans, and Dekang
Lin. 2008. Semi-supervised convex training for
dependency parsing. In Proceedings ofACL, pages
532–540.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support vector
machines. In Proceedings ofIWPT, pages 195–206.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings ofACL, pages 189–196.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proceedings of EMNLP-CoNLL, pages 320–331.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings ofACL, pages 188–193.
Zhi-Hua Zhou and Ming Li. 2005. Tri-training:
Exploiting unlabeled data using three classifiers.
In IEEE Transactions on Knowledge and Data
Engineering, pages 1529–1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011. Exploiting web-derived selectional prefer-
ence to improve statistical dependency parsing. In
Proceedings ofACL, pages 1556–1565.
Zhi-Hua Zhou. 2009. When semi-supervised learning
meets ensemble learning. In MCS.
</reference>
<page confidence="0.999296">
467
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.213636">
<title confidence="0.7045846">Ambiguity-aware Ensemble Training for Dependency Parsing Li , Min Wenliang Provincial Key Laboratory for Computer Information Processing Soochow</title>
<abstract confidence="0.999490205882353">This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, to as ensemble Instead of only using best parse trees in previous work, our core idea is to utilize parse forest to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Dan Klein</author>
</authors>
<title>Web-scale features for full-scale parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>693--702</pages>
<contexts>
<context position="2297" citStr="Bansal and Klein, 2011" startWordPosition="313" endWordPosition="316">nce ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although worki</context>
</contexts>
<marker>Bansal, Klein, 2011</marker>
<rawString>Mohit Bansal and Dan Klein. 2011. Web-scale features for full-scale parsing. In Proceedings of ACL, pages 693–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="2776" citStr="Blum and Mitchell, 1998" startWordPosition="383" endWordPosition="386">al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of EMILP 2012,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="29879" citStr="Bohnet and Nivre (2012)" startWordPosition="4674" endWordPosition="4677">ous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [higher-order] This work Chen et al. (2013) [higher-order] This work Table 5: UAS comparison on Chinese test data. Unlabeled data UAS #Sent Len Head/Word Oracle NULL 92.34 0 — — — Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65 Low divergence 92.94 0.5M 28.19 1.062 96.53 High divergence 93.03 0.5M 27.85 1.211 94.28 ALL 93.19 1.7M 24.15 1.087 96.09 Table 6: Performance of our semi-supervised GParser with different sets of “Unlabeled ← B+Z” on English test set. “Len” means averaged sentence length. on Chinese test data. Li et al. (2012) and Bohnet and Nivre (2012)</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of EMILP 2012, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLIIG,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="10420" citStr="Bohnet (2010)" startWordPosition="1586" endWordPosition="1587"> and distance of the dependency (i, j). We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. 2.1 Then the score of a dependency tree is: �Score(x, d; w) = wdep &apos; fdep(x, h, m) {(h,m)}⊆d + � wsib &apos; fsib(x, h, s, m) {(h,s),(h,m)}⊆d where fdep(x, h, m) and fsib(x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations to compose rich feature sets. Please refer to Table 4 of Bohnet (2010) for the complete feature list. 2.2 CRF-based GParser Previous work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-or</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COLIIG, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of EMILP/CoILL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="8699" citStr="Carreras, 2007" startWordPosition="1286" endWordPosition="1287">l of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 &lt; h &lt; n, 0 &lt; m &lt; n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p.</context>
<context position="11049" citStr="Carreras (2007)" startWordPosition="1684" endWordPosition="1685"> two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations to compose rich feature sets. Please refer to Table 4 of Bohnet (2010) for the complete feature list. 2.2 CRF-based GParser Previous work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O(n4)). Our approach is applicable to these higher-order models, which we leave for future work. 459 data. Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRFbased constituent parser of Finkel et al. (2008). Assuming the feature weights w are known, the probability of a dependency tree d given an input sentence x is defined as: exp{5core(x, d; w)} p(d|x;w) = Z(x; w) (1) where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. Suppose</context>
<context position="29388" citStr="Carreras (2007)" startWordPosition="4597" endWordPosition="4598">order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [higher-order] This work Chen et al. (2013) [higher-order] This work Table 5: UAS</context>
<context position="30845" citStr="Carreras (2007)" startWordPosition="4829" endWordPosition="4830">L 93.19 1.7M 24.15 1.087 96.09 Table 6: Performance of our semi-supervised GParser with different sets of “Unlabeled ← B+Z” on English test set. “Len” means averaged sentence length. on Chinese test data. Li et al. (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts. Our approach can be combined with their work to utilize unlabeled data to improve both POS tagging and parsing simultaneously. Our work achieves comparable accuracy with Chen et al. (2013), although they adopt the higher-order model of Carreras (2007). Again, our method may be combined with their work to achieve higher performance. 4.4 Analysis To better understand the effectiveness of our proposed approach, we make detailed analysis using the semi-supervised GParser with “Unlabeled ← B+Z” on English datasets. Contribution of unlabeled data with regard to syntactic divergence: We divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar. The first set contains those sentences that the two parsers produce identical parse trees, denoted by “consistent”, which corresponds to the se</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of EMILP/CoILL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>173--180</pages>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and maxent discriminative reranking. In Proceedings ofACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Don Blaheta</author>
<author>Niyu Ge</author>
<author>Keith Hall</author>
<author>John Hale</author>
<author>Mark Johnson</author>
</authors>
<date>2000</date>
<booktitle>BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="18501" citStr="Charniak et al., 2000" startWordPosition="2920" endWordPosition="2923">the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (section 23). For CTB5, we adopt the data split of (Duan et al., 2007). We convert original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules. For unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese 3http://people.sutd.edu.sg/˜yue_zhang/doc/ 4https://code.google.com/p/berkeleyparser/ ˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)} Z(ui, Vi; w) Z(ui, Vi; w) = � exp{5core(ui, d′; w)} d′EVi 461 Train Dev Test Unlabeled PTB 39,832 1,700 2,416 1.7M CTB5 16,091 803 1,910 4M Table 2: Data sets (in sentence number). Gigaword Version 2.0 (LDC2009T14) (Huang, 2009) for Chinese. We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012), and produce POS tags for all train/development/test/unlabeled sets (10- way jackkn</context>
</contexts>
<marker>Charniak, Blaheta, Ge, Hall, Hale, Johnson, 2000</marker>
<rawString>Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall, John Hale, and Mark Johnson. 2000. BLLIP 1987-89 WSJ Corpus Release 1, LDC2000T43. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Jun’ichi Kazama</author>
<author>Kiyotaka Uchimoto</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving dependency parsing with subtrees from auto-parsed data.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMILP,</booktitle>
<pages>570--579</pages>
<contexts>
<context position="2202" citStr="Chen et al., 2009" startWordPosition="297" endWordPosition="300">rogress during the past decade. However, it is very difficult to further improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou a</context>
<context position="27805" citStr="Chen et al. (2009)" startWordPosition="4361" endWordPosition="4364">ng the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and</context>
</contexts>
<marker>Chen, Kazama, Uchimoto, Torisawa, 2009</marker>
<rawString>Wenliang Chen, Jun’ichi Kazama, Kiyotaka Uchimoto, and Kentaro Torisawa. 2009. Improving dependency parsing with subtrees from auto-parsed data. In Proceedings ofEMILP, pages 570–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Yue Zhang</author>
</authors>
<title>Semi-supervised feature transformation for dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of EMILP,</booktitle>
<pages>1303--1313</pages>
<contexts>
<context position="2222" citStr="Chen et al., 2013" startWordPosition="301" endWordPosition="304">past decade. However, it is very difficult to further improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). Howeve</context>
<context position="18448" citStr="Chen et al. (2013)" startWordPosition="2910" endWordPosition="2913">on unlabeled data in different ways and evaluate the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (section 23). For CTB5, we adopt the data split of (Duan et al., 2007). We convert original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules. For unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese 3http://people.sutd.edu.sg/˜yue_zhang/doc/ 4https://code.google.com/p/berkeleyparser/ ˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)} Z(ui, Vi; w) Z(ui, Vi; w) = � exp{5core(ui, d′; w)} d′EVi 461 Train Dev Test Unlabeled PTB 39,832 1,700 2,416 1.7M CTB5 16,091 803 1,910 4M Table 2: Data sets (in sentence number). Gigaword Version 2.0 (LDC2009T14) (Huang, 2009) for Chinese. We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012), and produce POS tags for all </context>
<context position="27953" citStr="Chen et al. (2013)" startWordPosition="4382" endWordPosition="4385"> Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 4.3 Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with o</context>
<context position="29333" citStr="Chen et al. (2013)" startWordPosition="4586" endWordPosition="4589">sed methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [higher-order] This work C</context>
<context position="30782" citStr="Chen et al. (2013)" startWordPosition="4818" endWordPosition="4821"> 28.19 1.062 96.53 High divergence 93.03 0.5M 27.85 1.211 94.28 ALL 93.19 1.7M 24.15 1.087 96.09 Table 6: Performance of our semi-supervised GParser with different sets of “Unlabeled ← B+Z” on English test set. “Len” means averaged sentence length. on Chinese test data. Li et al. (2012) and Bohnet and Nivre (2012) use joint models for POS tagging and dependency parsing, significantly outperforming their pipeline counterparts. Our approach can be combined with their work to utilize unlabeled data to improve both POS tagging and parsing simultaneously. Our work achieves comparable accuracy with Chen et al. (2013), although they adopt the higher-order model of Carreras (2007). Again, our method may be combined with their work to achieve higher performance. 4.4 Analysis To better understand the effectiveness of our proposed approach, we make detailed analysis using the semi-supervised GParser with “Unlabeled ← B+Z” on English datasets. Contribution of unlabeled data with regard to syntactic divergence: We divide the unlabeled data into three sets according to the divergence of the 1-best outputs of Berkeley Parser and ZPar. The first set contains those sentences that the two parsers produce identical pa</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2013</marker>
<rawString>Wenliang Chen, Min Zhang, and Yue Zhang. 2013. Semi-supervised feature transformation for dependency parsing. In Proceedings of EMILP, pages 1303–1313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing. Computational Linguistics,</title>
<date>2005</date>
<pages>25--70</pages>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael J. Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, pages 25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
</authors>
<title>Partha Pratim Talukdar, and Koby Crammer.</title>
<date>2009</date>
<booktitle>In ECML/PKDD Workshop on Learning from Multi-Label Data.</booktitle>
<marker>Dredze, 2009</marker>
<rawString>Mark Dredze, Partha Pratim Talukdar, and Koby Crammer. 2009. Sequence learning from data with multiple labels. In ECML/PKDD Workshop on Learning from Multi-Label Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangyu Duan</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
</authors>
<title>Probabilistic models for action-based Chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ECML/ECPPKDD,</booktitle>
<pages>559--566</pages>
<contexts>
<context position="18277" citStr="Duan et al., 2007" startWordPosition="2886" endWordPosition="2889">Parser4) (Petrov and Klein, 2007). These three parsers are trained on labeled data and then used to parse each unlabeled sentence. We aggregate the three parsers’ outputs on unlabeled data in different ways and evaluate the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (section 23). For CTB5, we adopt the data split of (Duan et al., 2007). We convert original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules. For unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese 3http://people.sutd.edu.sg/˜yue_zhang/doc/ 4https://code.google.com/p/berkeleyparser/ ˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)} Z(ui, Vi; w) Z(ui, Vi; w) = � exp{5core(ui, d′; w)} d′EVi 461 Train Dev Test Unlabeled PTB 39,832 1,700 2,416 1.7M CTB5 16,091 803 1,910 4M Table 2: Data sets (in sentence number). Gigaword Version 2.0 (LDC</context>
</contexts>
<marker>Duan, Zhao, Xu, 2007</marker>
<rawString>Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic models for action-based Chinese dependency parsing. In Proceedings of ECML/ECPPKDD, pages 559–566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Alex Kleeman</author>
<author>Christopher D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="11379" citStr="Finkel et al. (2008)" startWordPosition="1736" endWordPosition="1739">RF-based GParser Previous work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O(n4)). Our approach is applicable to these higher-order models, which we leave for future work. 459 data. Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRFbased constituent parser of Finkel et al. (2008). Assuming the feature weights w are known, the probability of a dependency tree d given an input sentence x is defined as: exp{5core(x, d; w)} p(d|x;w) = Z(x; w) (1) where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. Suppose the labeled training data is D = {(xi, di)}Ni=1. Then the log likelihood of D is: L(D; w) = XN log p(di|xi; w) i=1 The training objective is to maximize the log likelihood of the training data L(D). The partial derivative with respect to the feature weights w is: not sufficiently covered in manually labeled data. Therefore, exp</context>
<context position="15569" citStr="Finkel et al. (2008)" startWordPosition="2419" endWordPosition="2422"> constrained by the parse forest Vi. The second term in Eq. (3) is the same with the second term in Eq. (2). The first term in Eq. (3) can be efficiently computed by running the insideoutside algorithm in the constrained search space Vi. 3.2 Stochastic Gradient Descent (SGD) Training We apply L2-norm regularized SGD training to iteratively learn feature weights w for our CRFbased baseline and semi-supervised parsers. We follow the implementation in CRFsuite.2 At each step, the algorithm approximates a gradient with a small subset of the training examples, and then updates the feature weights. Finkel et al. (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. Moreover, it is very convenient to parallel SGD since computations among examples in the same batch is mutually independent. Training with the combined labeled and unlabeled data, the objective is to maximize the mixed likelihood: L(D; D′) = L(D) + L(D′) Since D′ contains much more instances than D (1.7M vs. 40K for English, and 4M vs. 16K for Chinese), it is likely that the unlabeled data may overwhelm the labeled data during SGD training. Therefore, we propose a simple </context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. 2008. Efficient, feature-based, conditional random field parsing. In Proceedings of ACL, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings ofIAACL.</booktitle>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Proceedings ofIAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Selftraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of EMILP</booktitle>
<pages>832--841</pages>
<contexts>
<context position="2975" citStr="Huang and Harper, 2009" startWordPosition="412" endWordPosition="415">r dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two par</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. Selftraining PCFG grammars with latent annotations across languages. In Proceedings of EMILP 2009, pages 832–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
</authors>
<title>Tagged Chinese Gigaword Version 2.0, LDC2009T14. Linguistic Data Consortium.</title>
<date>2009</date>
<contexts>
<context position="18899" citStr="Huang, 2009" startWordPosition="2978" endWordPosition="2979">rt original bracketed structures into dependency structures using Penn2Malt with its default head-finding rules. For unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese 3http://people.sutd.edu.sg/˜yue_zhang/doc/ 4https://code.google.com/p/berkeleyparser/ ˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)} Z(ui, Vi; w) Z(ui, Vi; w) = � exp{5core(ui, d′; w)} d′EVi 461 Train Dev Test Unlabeled PTB 39,832 1,700 2,416 1.7M CTB5 16,091 803 1,910 4M Table 2: Data sets (in sentence number). Gigaword Version 2.0 (LDC2009T14) (Huang, 2009) for Chinese. We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012), and produce POS tags for all train/development/test/unlabeled sets (10- way jackknifing for training sets). The tagging accuracy on test sets is 97.3% on English and 94.0% on Chinese. Table 2 shows the data statistics. We measure parsing performance using the standard unlabeled attachment score (UAS), excluding punctuation marks. For significance test, we adopt Dan Bikel’s randomized parsing evaluation comparator (Noreen, 1989).5 4.1 Parameter Setting When training our CRF-ba</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Chu-Ren Huang. 2009. Tagged Chinese Gigaword Version 2.0, LDC2009T14. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong Jin</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning with multiple labels.</title>
<date>2002</date>
<booktitle>In Proceedings ofIIPS.</booktitle>
<contexts>
<context position="33631" citStr="Jin and Ghahramani, 2002" startWordPosition="5259" endWordPosition="5262">he idea of ambiguous labelings to multilingual parser transfer in the unsupervised parsing field, which aims to build a dependency parser for a resourcepoor target language by making use of sourcelanguage treebanks. Different from their work, we explore the idea for semi-supervised dependency parsing where a certain amount of labeled training data is available. Moreover, we for the first time build a state-of-the-art CRF-based dependency parser and conduct in-depth comparisons with previous methods. Similar ideas of learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and sequence labeling problems (Dredze et al., 2009). Our work is also related with the parser ensemble approaches such as stacked learning and reparsing in the supervised track. Stacked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are </context>
</contexts>
<marker>Jin, Ghahramani, 2002</marker>
<rawString>Rong Jin and Zoubin Ghahramani. 2002. Learning with multiple labels. In Proceedings ofIIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1759" citStr="Koo and Collins (2010)" startWordPosition="233" endWordPosition="236">es. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for depen</context>
<context position="8723" citStr="Koo and Collins, 2010" startWordPosition="1288" endWordPosition="1291">parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 &lt; h &lt; n, 0 &lt; m &lt; n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p. Score(x, d; w) = w &apos; f(</context>
<context position="11076" citStr="Koo and Collins (2010)" startWordPosition="1687" endWordPosition="1690">esponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic features used in each feature category in Table 1. These atomic features are concatenated in different combinations to compose rich feature sets. Please refer to Table 4 of Bohnet (2010) for the complete feature list. 2.2 CRF-based GParser Previous work on graph-based dependency parsing mostly adopts linear models and perceptron based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O(n4)). Our approach is applicable to these higher-order models, which we leave for future work. 459 data. Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRFbased constituent parser of Finkel et al. (2008). Assuming the feature weights w are known, the probability of a dependency tree d given an input sentence x is defined as: exp{5core(x, d; w)} p(d|x;w) = Z(x; w) (1) where Z(x) is the normalization factor and Y(x) is the set of all legal dependency trees for x. Suppose the labeled training data </context>
<context position="27628" citStr="Koo and Collins (2010)" startWordPosition="4335" endWordPosition="4338">f GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser. Adding the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that</context>
<context position="28866" citStr="Koo and Collins (2010)" startWordPosition="4517" endWordPosition="4520">e trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 4.3 Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. Table 4 shows the results. The first major row lists several state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In ACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2162" citStr="Koo et al., 2008" startWordPosition="291" endWordPosition="294">ised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and M</context>
<context position="27759" citStr="Koo et al. (2008)" startWordPosition="4354" endWordPosition="4357">a than both Berkeley Parser and GParser. Adding the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-</context>
<context position="29483" citStr="Koo et al. (2008)" startWordPosition="4612" endWordPosition="4615"> propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [higher-order] This work Chen et al. (2013) [higher-order] This work Table 5: UAS comparison on Chinese test data. Unlabeled data UAS #Sent Len Head/Word Oracle NULL 92.34 0 — </context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing.</title>
<date>2012</date>
<booktitle>In COLIIG 2012,</booktitle>
<pages>1681--1698</pages>
<contexts>
<context position="19017" citStr="Li et al., 2012" startWordPosition="2996" endWordPosition="2999">or unlabeled data, we follow Chen et al. (2013) and use the BLLIP WSJ corpus (Charniak et al., 2000) for English and Xinhua portion of Chinese 3http://people.sutd.edu.sg/˜yue_zhang/doc/ 4https://code.google.com/p/berkeleyparser/ ˜p(d′|ui, Vi; w) = exp{5core(ui, d′; w)} Z(ui, Vi; w) Z(ui, Vi; w) = � exp{5core(ui, d′; w)} d′EVi 461 Train Dev Test Unlabeled PTB 39,832 1,700 2,416 1.7M CTB5 16,091 803 1,910 4M Table 2: Data sets (in sentence number). Gigaword Version 2.0 (LDC2009T14) (Huang, 2009) for Chinese. We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012), and produce POS tags for all train/development/test/unlabeled sets (10- way jackknifing for training sets). The tagging accuracy on test sets is 97.3% on English and 94.0% on Chinese. Table 2 shows the data statistics. We measure parsing performance using the standard unlabeled attachment score (UAS), excluding punctuation marks. For significance test, we adopt Dan Bikel’s randomized parsing evaluation comparator (Noreen, 1989).5 4.1 Parameter Setting When training our CRF-based parsers with SGD, we use the batch size b = 100 for all experiments. We run SGD for I = 100 iterations and choose </context>
<context position="29847" citStr="Li et al. (2012)" startWordPosition="4669" endWordPosition="4672"> accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [higher-order] This work Chen et al. (2013) [higher-order] This work Table 5: UAS comparison on Chinese test data. Unlabeled data UAS #Sent Len Head/Word Oracle NULL 92.34 0 — — — Consistent (tri-train) 92.94 0.7M 18.25 1.000 97.65 Low divergence 92.94 0.5M 28.19 1.062 96.53 High divergence 93.03 0.5M 27.85 1.211 94.28 ALL 93.19 1.7M 24.15 1.087 96.09 Table 6: Performance of our semi-supervised GParser with different sets of “Unlabeled ← B+Z” on English test set. “Len” means averaged sentence length. on Chinese test data. Li et al. (2</context>
</contexts>
<marker>Li, Zhang, Che, Liu, 2012</marker>
<rawString>Zhenghua Li, Min Zhang, Wanxiang Che, and Ting Liu. 2012. A separately passive-aggressive training algorithm for joint POS tagging and dependency parsing. In COLIIG 2012, pages 1681–1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the IAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="2950" citStr="McClosky et al., 2006" startWordPosition="408" endWordPosition="411">ds are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best p</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the IAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="8683" citStr="McDonald and Pereira, 2006" startWordPosition="1282" endWordPosition="1285">tence x = w0w1...wn, the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 &lt; h &lt; n, 0 &lt; m &lt; n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of s</context>
<context position="9940" citStr="McDonald and Pereira (2006)" startWordPosition="1496" endWordPosition="1499">) = w &apos; f(x, d) �= Score(x, p; w) p⊆d h In (a) single dependency Figure 2: Two types of scoring subtrees in our second-order graph-based parsers. Dependency features fdep(x, h, m): wh, wm, th, tm, th±1, tm±1, tb, dir(h, m), dist(h, m) Sibling features fsib(x, h, m, s): wh, ws, wm, th, tm, ts, th±1, tm±1, ts±1 dir(h, m), dist(h, m) Table 1: Brief illustration of the syntactic features. ti denotes the POS tag of wi. b is an index between h and m. dir(i, j) and dist(i, j) denote the direction and distance of the dependency (i, j). We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. 2.1 Then the score of a dependency tree is: �Score(x, d; w) = wdep &apos; fdep(x, h, m) {(h,m)}⊆d + � wsib &apos; fsib(x, h, s, m) {(h,s),(h,m)}⊆d where fdep(x, h, m) and fsib(x, h, s, m) are the feature vectors of the two subtree in Fig. 2; wdep/sib are feature weight vectors; the dot product gives scores contributed by corresponding subtrees. For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. 2. We summarize the atomic feat</context>
<context position="17377" citStr="McDonald and Pereira (2006)" startWordPosition="2743" endWordPosition="2746">ta. 1: Input: Labeled data D = {(xi, di)}Ni=1, and unlabeled data D′ = {(ui, Vi)}Mj=1; Parameters: I, N1, M1, b 2: Output: w 3: Initialization: w(0) = 0, k = 0; 4: for i = 1 to I do {iterations} 5: Randomly select N1 instances from D and M1 instances from D′ to compose a new dataset Di, and shuffle it. 6: Traverse Di: a small batch Dbi,k C Di at one step. 7: wk+1 = wk + ηk1b ∇L(Dbi,k; wk) 8: k = k + 1 9: end for parse the test data to find the optimal parse tree. d* = arg max p(d′|x; w) d′EY(x) = arg max 5core(x, d′; w) d′EY(x) This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O(n3) parsing time. 3.3 Forest Construction with Diverse Parsers To construct parse forests for unlabeled data, we employ three diverse parsers, i.e., our baseline GParser, a transition-based parser (ZPar3) (Zhang and Nivre, 2011), and a generative constituent parser (Berkeley Parser4) (Petrov and Klein, 2007). These three parsers are trained on labeled data and then used to parse each unlabeled sentence. We aggregate the three parsers’ outputs on unlabeled data in different ways and evaluate the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of o</context>
<context position="27600" citStr="McDonald and Pereira (2006)" startWordPosition="4330" endWordPosition="4333">the intersection of the outputs of GParser and ZPar, the size of the parse forest is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser. Adding the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the sem</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="8655" citStr="McDonald et al., 2005" startWordPosition="1278" endWordPosition="1281">sing Given an input sentence x = w0w1...wn, the goal of dependency parsing is to build a dependency tree as depicted in Figure 1, denoted by d = {(h, m) : 0 &lt; h &lt; n, 0 &lt; m &lt; n}, where (h, m) indicates a directed arc from the head word wh to the modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is f</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>950--958</pages>
<contexts>
<context position="33949" citStr="Nivre and McDonald, 2008" startWordPosition="5309" endWordPosition="5312">mount of labeled training data is available. Moreover, we for the first time build a state-of-the-art CRF-based dependency parser and conduct in-depth comparisons with previous methods. Similar ideas of learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and sequence labeling problems (Dredze et al., 2009). Our work is also related with the parser ensemble approaches such as stacked learning and reparsing in the supervised track. Stacked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguou</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings ofACL, pages 950–958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>149--160</pages>
<contexts>
<context position="8889" citStr="Nivre, 2003" startWordPosition="1314" endWordPosition="1315"> modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p. Score(x, d; w) = w &apos; f(x, d) �= Score(x, p; w) p⊆d h In (a) single dependency Figure 2: Two types of scoring subtrees in our second-order graph-based parsers. Dependency features fdep(x, h,</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages 149–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses: An introduction.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="19450" citStr="Noreen, 1989" startWordPosition="3063" endWordPosition="3064">tence number). Gigaword Version 2.0 (LDC2009T14) (Huang, 2009) for Chinese. We build a CRF-based bigram part-of-speech (POS) tagger with the features described in (Li et al., 2012), and produce POS tags for all train/development/test/unlabeled sets (10- way jackknifing for training sets). The tagging accuracy on test sets is 97.3% on English and 94.0% on Chinese. Table 2 shows the data statistics. We measure parsing performance using the standard unlabeled attachment score (UAS), excluding punctuation marks. For significance test, we adopt Dan Bikel’s randomized parsing evaluation comparator (Noreen, 1989).5 4.1 Parameter Setting When training our CRF-based parsers with SGD, we use the batch size b = 100 for all experiments. We run SGD for I = 100 iterations and choose the model that performs best on development data. For the semi-supervised parsers trained with Algorithm 1, we use N1 = 20K and M1 = 50K for English, and N1 = 15K and M1 = 50K for Chinese, based on a few preliminary experiments. To accelerate the training, we adopt parallelized implementation of SGD and employ 20 threads for each run. For semi-supervised cases, one iteration takes about 2 hours on an IBM server having 2.0 GHz Int</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: An introduction. John Wiley &amp; Sons, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofIAACL.</booktitle>
<contexts>
<context position="6565" citStr="Petrov and Klein, 2007" startWordPosition="954" endWordPosition="957"> Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ensemble training outperforms other entire-tree based methods suc</context>
<context position="17692" citStr="Petrov and Klein, 2007" startWordPosition="2789" endWordPosition="2792"> a small batch Dbi,k C Di at one step. 7: wk+1 = wk + ηk1b ∇L(Dbi,k; wk) 8: k = k + 1 9: end for parse the test data to find the optimal parse tree. d* = arg max p(d′|x; w) d′EY(x) = arg max 5core(x, d′; w) d′EY(x) This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O(n3) parsing time. 3.3 Forest Construction with Diverse Parsers To construct parse forests for unlabeled data, we employ three diverse parsers, i.e., our baseline GParser, a transition-based parser (ZPar3) (Zhang and Nivre, 2011), and a generative constituent parser (Berkeley Parser4) (Petrov and Klein, 2007). These three parsers are trained on labeled data and then used to parse each unlabeled sentence. We aggregate the three parsers’ outputs on unlabeled data in different ways and evaluate the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (section 23). For CTB5, we adopt the data split of (Duan et al., 2007). We convert or</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings ofIAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of IAACL,</booktitle>
<pages>129--132</pages>
<contexts>
<context position="34131" citStr="Sagae and Lavie, 2006" startWordPosition="5339" endWordPosition="5342"> Similar ideas of learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and sequence labeling problems (Dredze et al., 2009). Our work is also related with the parser ensemble approaches such as stacked learning and reparsing in the supervised track. Stacked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training obj</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of IAACL, pages 129–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoILL Shared Task Session of EMILP-CoILL,</booktitle>
<pages>1044--1050</pages>
<contexts>
<context position="3220" citStr="Sagae and Tsujii (2007)" startWordPosition="447" endWordPosition="450"> up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 457 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457–467, Baltimore, Maryland, USA, June 23-25 2014. c�2014 A</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proceedings of the CoILL Shared Task Session of EMILP-CoILL, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Bootstrapping feature-rich dependency parsers with entropic priors.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMILP,</booktitle>
<pages>667--677</pages>
<contexts>
<context position="2450" citStr="Smith and Eisner, 2007" startWordPosition="336" endWordPosition="339">eatures into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and K</context>
</contexts>
<marker>Smith, Eisner, 2007</marker>
<rawString>David A. Smith and Jason Eisner. 2007. Bootstrapping feature-rich dependency parsers with entropic priors. In Proceedings ofEMILP, pages 667–677.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Christian Rishøj</author>
</authors>
<title>Semisupervised dependency parsing using generalized tri-training.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1065--1073</pages>
<contexts>
<context position="3350" citStr="Søgaard and Rishøj (2010)" startWordPosition="466" endWordPosition="469">owsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable. 457 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 457–467, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics w0 He1 saw2 a3 deer4 riding5 a6 bicycle7 in8 they park10 .11 Figure 1: An example sentenc</context>
</contexts>
<marker>Søgaard, Rishøj, 2010</marker>
<rawString>Anders Søgaard and Christian Rishøj. 2010. Semisupervised dependency parsing using generalized tri-training. In Proceedings of ACL, pages 1065– 1073.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Spreyer</author>
<author>Jonas Kuhn</author>
</authors>
<title>Datadriven dependency parsing of new languages using incomplete and noisy training data. In CoNLL,</title>
<date>2009</date>
<pages>12--20</pages>
<contexts>
<context position="3060" citStr="Spreyer and Kuhn, 2009" startWordPosition="423" endWordPosition="426"> Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accuracy. Both work employs two parsers to process the unlabeled data, and only select as extra training data sentences on which the 1-best parse trees of the two parsers are identical. In this way, the autoparsed unlabeled data becomes more reliable.</context>
<context position="21824" citStr="Spreyer and Kuhn, 2009" startWordPosition="3436" endWordPosition="3439">ests on development datasets. The first major row presents performance of the three supervised parsers. We can see that the three parsers achieve comparable performance on English, but the performance of ZPar is largely inferior on Chinese. The second major row shows the results when we use single 1-best parse trees on unlabeled data. When using the outputs of GParser itself (“Unlabeled ← G”), the experiment reproduces traditional self-training. The results on both English and Chinese re-confirm that self-training may not work for dependency parsing, which is consistent with previous studies (Spreyer and Kuhn, 2009). The reason may be that dependency parsers are prone to amplify previous mistakes on unlabeled data during training. The next two experiments in the second major row reimplement co-training, where another parser’s 1-best results are projected into unlabeled data to help the core parser. Using unlabeled data with the results of ZPar (“Unlabeled ← Z”) significantly outperforms the baseline GParser by 0.30% (93.15-82.85) on English. However, the improvement on Chinese is not significant. Using unlabeled data with the results of Berkeley Parser (“Unlabeled ← B”) significantly improves parsing acc</context>
</contexts>
<marker>Spreyer, Kuhn, 2009</marker>
<rawString>Kathrin Spreyer and Jonas Kuhn. 2009. Datadriven dependency parsing of new languages using incomplete and noisy training data. In CoNLL, pages 12–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>649--652</pages>
<contexts>
<context position="22875" citStr="Surdeanu and Manning (2010)" startWordPosition="3588" endWordPosition="3591">n English. However, the improvement on Chinese is not significant. Using unlabeled data with the results of Berkeley Parser (“Unlabeled ← B”) significantly improves parsing accuracy by 0.55% (93.40-92.85) on English and 1.06% (83.34-82.28) on Chinese. We believe the reason is that being a generative model designed for constituent parsing, Berkeley Parser is more different from discriminative dependency parsers, and therefore can provide more divergent syntactic structures. This kind of syntactic divergence is helpful because it can provide complementary knowledge from a different perspective. Surdeanu and Manning (2010) also show that the diversity of parsers is important for performance improvement when integrating different parsers in the supervised track. Therefore, we can conclude that co-training helps dependency parsing, especially when using a more divergent parser. The last experiment in the second major row is known as tri-training, which only uses unla462 English Chinese UAS Oracle Head/Word UAS Oracle Head/Word GParser 92.85 82.28 Supervised ZPar 92.50 — — 81.04 — — Berkeley 92.70 82.46 Unlabeled G (self-train) 92.88 92.85 82.14 82.28 Semi-supervised GParser Unlabeled Z (co-train) 93.15 † 92.50 1.</context>
<context position="34160" citStr="Surdeanu and Manning, 2010" startWordPosition="5343" endWordPosition="5346">ing with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and sequence labeling problems (Dredze et al., 2009). Our work is also related with the parser ensemble approaches such as stacked learning and reparsing in the supervised track. Stacked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training objective is to maximize the mix</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Proceedings of NAACL, pages 649– 652.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semi-supervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="2491" citStr="Suzuki et al., 2009" startWordPosition="344" endWordPosition="347">s to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that depend</context>
<context position="27853" citStr="Suzuki et al. (2009)" startWordPosition="4368" endWordPosition="4371">- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 4.3 Comparison </context>
<context position="29310" citStr="Suzuki et al. (2009)" startWordPosition="4581" endWordPosition="4584">l state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi-supervised approaches, since they are orthogonal in methodology and utilize unlabeled data from different perspectives. Table 5 make comparisons with previous results Li et al. (2012) [joint] Bohnet and Nivre (2012) [joint] Chen et al. (2013) [hi</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semi-supervised structured conditional models for dependency parsing. In Proceedings of EMNLP, pages 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Target language adaptation of discriminative transfer parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>1061--1071</pages>
<marker>T¨ackstr¨om, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer parsers. In Proceedings of NAACL, pages 1061–1071.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e Filipe Torres Martins</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>157--166</pages>
<contexts>
<context position="33979" citStr="Martins et al., 2008" startWordPosition="5314" endWordPosition="5317"> available. Moreover, we for the first time build a state-of-the-art CRF-based dependency parser and conduct in-depth comparisons with previous methods. Similar ideas of learning with ambiguous labelings are previously explored for classification (Jin and Ghahramani, 2002) and sequence labeling problems (Dredze et al., 2009). Our work is also related with the parser ensemble approaches such as stacked learning and reparsing in the supervised track. Stacked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabele</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>Andr´e Filipe Torres Martins, Dipanjan Das, Noah A. Smith, and Eric P. Xing. 2008. Stacking dependency parsers. In Proceedings of EMNLP, pages 157–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
<author>Dekang Lin</author>
</authors>
<title>Semi-supervised convex training for dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>532--540</pages>
<contexts>
<context position="2469" citStr="Wang et al., 2008" startWordPosition="340" endWordPosition="343">ed parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The rea</context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2008</marker>
<rawString>Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2008. Semi-supervised convex training for dependency parsing. In Proceedings ofACL, pages 532–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT,</booktitle>
<pages>195--206</pages>
<contexts>
<context position="8876" citStr="Yamada and Matsumoto, 2003" startWordPosition="1310" endWordPosition="1313">from the head word wh to the modifier wm, and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p. Score(x, d; w) = w &apos; f(x, d) �= Score(x, p; w) p⊆d h In (a) single dependency Figure 2: Two types of scoring subtrees in our second-order graph-based parsers. Dependency featur</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT, pages 195–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2737" citStr="Yarowsky, 1995" startWordPosition="380" endWordPosition="381">such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings ofACL, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>320--331</pages>
<contexts>
<context position="1789" citStr="Zhang and McDonald (2012)" startWordPosition="238" endWordPosition="241">ructures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training. 1 Introduction Supervised dependency parsing has made great progress during the past decade. However, it is very difficult to further improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly ut</context>
<context position="27677" citStr="Zhang and McDonald (2012)" startWordPosition="4342" endWordPosition="4345">st is better controlled, which is helpful considering that ZPar performs worse on this data than both Berkeley Parser and GParser. Adding the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing </context>
<context position="28933" citStr="Zhang and McDonald (2012)" startWordPosition="4527" endWordPosition="4530">outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 4.3 Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. Table 4 shows the results. The first major row lists several state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achie</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In Proceedings of EMNLP-CoNLL, pages 320–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="6503" citStr="Zhang and Nivre, 2011" startWordPosition="945" endWordPosition="948"> with high oracle score. Please note that the parse forest in Figure 1 contains four parse trees after combination of the two different choices. Second, the parser is able to learn useful features from the unambiguous parts of the parse forest. Finally, with sufficient unlabeled data, it is possible that the parser can learn to resolve such uncertainty by biasing to more reasonable parse trees. To construct parse forest on unlabeled data, we employ three supervised parsers based on different paradigms, including our baseline graph-based dependency parser, a transition-based dependency parser (Zhang and Nivre, 2011), and a generative constituent parser (Petrov and Klein, 2007). The 1-best parse trees of these three parsers are aggregated in different ways. Evaluation on labeled data shows the oracle accuracy of parse forest is much higher than that of 1-best outputs of single parsers (see Table 3). Finally, using a conditional random field (CRF) based probabilistic parser, we train a better model by maximizing mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. Experimental results on both English and Chinese datasets demonstrate that the proposed ambiguity-aware ens</context>
<context position="8913" citStr="Zhang and Nivre, 2011" startWordPosition="1316" endWordPosition="1319"> and w0 is an artificial node linking to the root of the sentence. In parsing community, two mainstream methods tackle the dependency parsing problem from different perspectives but achieve comparable accuracy on a variety of languages. The graphbased method views the problem as finding an optimal tree from a fully-connected directed graph (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), while the transition-based method tries to find a highest-scoring transition sequence that leads to a legal dependency tree (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011). 2.1 Graph-based Dependency Parser (GParser) In this work, we adopt the graph-based paradigm because it allows us to naturally derive conditional probability of a dependency tree d given a sentence x, which is required to compute likelihood of both labeled and unlabeled data. Under the graph-based model, the score of a dependency tree is factored into the scores of small subtrees p. Score(x, d; w) = w &apos; f(x, d) �= Score(x, p; w) p⊆d h In (a) single dependency Figure 2: Two types of scoring subtrees in our second-order graph-based parsers. Dependency features fdep(x, h, m): wh, wm, th, tm, th±</context>
<context position="17611" citStr="Zhang and Nivre, 2011" startWordPosition="2777" endWordPosition="2780">1 instances from D′ to compose a new dataset Di, and shuffle it. 6: Traverse Di: a small batch Dbi,k C Di at one step. 7: wk+1 = wk + ηk1b ∇L(Dbi,k; wk) 8: k = k + 1 9: end for parse the test data to find the optimal parse tree. d* = arg max p(d′|x; w) d′EY(x) = arg max 5core(x, d′; w) d′EY(x) This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O(n3) parsing time. 3.3 Forest Construction with Diverse Parsers To construct parse forests for unlabeled data, we employ three diverse parsers, i.e., our baseline GParser, a transition-based parser (ZPar3) (Zhang and Nivre, 2011), and a generative constituent parser (Berkeley Parser4) (Petrov and Klein, 2007). These three parsers are trained on labeled data and then used to parse each unlabeled sentence. We aggregate the three parsers’ outputs on unlabeled data in different ways and evaluate the effectiveness through experiments. 4 Experiments and Analysis To verify the effectiveness of our proposed approach, we conduct experiments on Penn Treebank (PTB) and Penn Chinese Treebank 5.1 (CTB5). For English, we follow the popular practice to split data into training (sections 2-21), development (section 22), and test (sec</context>
<context position="27721" citStr="Zhang and Nivre (2011)" startWordPosition="4348" endWordPosition="4351">dering that ZPar performs worse on this data than both Berkeley Parser and GParser. Adding the output of GParser itself (“Unlabeled &lt;-- B+Z+G”) leads to accuracy drop, although the oracle score is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms t</context>
<context position="29063" citStr="Zhang and Nivre (2011)" startWordPosition="4547" endWordPosition="4550"> Comparison with Previous Work We adopt the best settings on development data for semi-supervised GParser with our proposed approach, and make comparison with previous results on test data. Table 4 shows the results. The first major row lists several state-of-theart supervised methods. McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. Koo and Collins (2010) propose a third-order graphbased parser. Zhang and McDonald (2012) explore higher-order features for graph-based dependency parsing, and adopt beam search for fast decoding. Zhang and Nivre (2011) propose a feature-rich transition-based parser. All work in the second major row adopts semi-supervised methods. The results show that our approach achieves comparable accuracy with most previous semi-supervised methods. Both Suzuki et al. (2009) and Chen et al. (2013) adopt the higherorder parsing model of Carreras (2007), and Suzuki et al. (2009) also incorporate word cluster features proposed by Koo et al. (2008) in their system. We expect our approach may achieve higher performance with such enhancements, which we leave for future work. Moreover, our method may be combined with other semi</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings ofACL, pages 188–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Ming Li</author>
</authors>
<title>Tri-training: Exploiting unlabeled data using three classifiers.</title>
<date>2005</date>
<booktitle>In IEEE Transactions on Knowledge and Data Engineering,</booktitle>
<pages>1529--1541</pages>
<contexts>
<context position="2814" citStr="Zhou and Li, 2005" startWordPosition="389" endWordPosition="392">, 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependency parsing. Although working well on constituent parsing (McClosky et al., 2006; Huang and Harper, 2009), self-training is shown unsuccessful for dependency parsing (Spreyer and Kuhn, 2009). The reason may be that dependency parsing models are prone to amplify previous mistakes during training on self-parsed unlabeled data. Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. Søgaard and Rishøj (2010) combine tri-training and parser ensemble to boost parsing accur</context>
</contexts>
<marker>Zhou, Li, 2005</marker>
<rawString>Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. In IEEE Transactions on Knowledge and Data Engineering, pages 1529–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>1556--1565</pages>
<contexts>
<context position="2272" citStr="Zhou et al., 2011" startWordPosition="309" endWordPosition="312">er improve performance ∗Correspondence author of supervised parsers. For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest. Previously, unlabeled data is explored to derive useful local-context features such as word clusters (Koo et al., 2008), subtree frequencies (Chen et al., 2009; Chen et al., 2013), and word co-occurrence counts (Zhou et al., 2011; Bansal and Klein, 2011). A few effective learning methods are also proposed for dependency parsing to implicitly utilize distributions on unlabeled data (Smith and Eisner, 2007; Wang et al., 2008; Suzuki et al., 2009). All above work leads to significant improvement on parsing accuracy. Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). However, these methods gain limited success in dependenc</context>
<context position="27907" citStr="Zhou et al. (2011)" startWordPosition="4375" endWordPosition="4378">ore is higher (96.95% on English and 91.50% on Chinese) than “Unlabeled &lt;-- B+Z”. We suspect the reason is that the model is likely to distribute the probability mass to these parse trees produced by itself instead of those by Berkeley Parser or ZPar under this setting. 463 Sup Semi McDonald and Pereira (2006) 91.5 Koo and Collins (2010) [higher-order] 93.04 — Zhang and McDonald (2012) [higher-order] 93.06 Zhang and Nivre (2011) [higher-order] 92.9 Koo et al. (2008) [higher-order] 92.02 93.16 Chen et al. (2009) [higher-order] 92.40 93.16 Suzuki et al. (2009) [higher-order,cluster] 92.70 93.79 Zhou et al. (2011) [higher-order] 91.98 92.64 Chen et al. (2013) [higher-order] 92.76 93.77 This work 92.34 93.19 Table 4: UAS comparison on English test data. In summary, we can conclude that our proposed ambiguity-aware ensemble training is significantly better than both the supervised approaches and the semi-supervised approaches that use 1-best parse trees. Appropriately composing the forest parse, our approach outperforms the best results of co-training or tri-training by 0.28% (93.78-93.50) on English and 0.92% (84.26-83.34) on Chinese. 4.3 Comparison with Previous Work We adopt the best settings on devel</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings ofACL, pages 1556–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
</authors>
<title>When semi-supervised learning meets ensemble learning.</title>
<date>2009</date>
<booktitle>In MCS.</booktitle>
<contexts>
<context position="34425" citStr="Zhou, 2009" startWordPosition="5389" endWordPosition="5390">cked learning uses one parser’s outputs as guide features for another parser, leading to improved performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Re-parsing merges the outputs of several parsers into a dependency graph, and then apply Viterbi decoding to find a better tree (Sagae and Lavie, 2006; Surdeanu and Manning, 2010). One possible drawback of parser ensemble is that several parsers are required to parse the same sentence during the test phase. Moreover, our approach can benefit from these methods in that we can get parse forests of higher quality on unlabeled data (Zhou, 2009). 6 Conclusions This paper proposes a generalized training framework of semi-supervised dependency parsing based on ambiguous labelings. For each unlabeled sentence, we combine the 1-best parse trees of several diverse parsers to compose ambiguous labelings, represented by a parse forest. The training objective is to maximize the mixed likelihood of both the labeled data and the auto-parsed unlabeled data with ambiguous labelings. Experiments show that our framework can make better use of the unlabeled data, especially those with divergent outputs from different parsers, than traditional tri-t</context>
</contexts>
<marker>Zhou, 2009</marker>
<rawString>Zhi-Hua Zhou. 2009. When semi-supervised learning meets ensemble learning. In MCS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>