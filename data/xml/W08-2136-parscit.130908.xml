<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001307">
<title confidence="0.982465">
Mixing and Blending Syntactic and Semantic Dependencies
</title>
<author confidence="0.994718">
Yvonne Samuelsson
</author>
<affiliation confidence="0.9950435">
Dept. of Linguistics
Stockholm University
</affiliation>
<email confidence="0.899511">
yvonne.samuelsson@ling.su.se
</email>
<author confidence="0.976374">
Johan Eklund
</author>
<affiliation confidence="0.8899945">
SSLIS
University College of Bor˚as
</affiliation>
<email confidence="0.967612">
johan.eklund@hb.se
</email>
<note confidence="0.555241333333333">
Oscar T¨ackstr¨om
Dept. of Linguistics and Philology
SICS / Uppsala University
</note>
<email confidence="0.746005">
oscar@sics.se
</email>
<author confidence="0.993724">
Mark Fiˇsel
</author>
<affiliation confidence="0.9993915">
Dept. of Computer Science
University of Tartu
</affiliation>
<email confidence="0.954">
fishel@ut.ee
</email>
<author confidence="0.983731">
Sumithra Velupillai
</author>
<affiliation confidence="0.9817005">
Dept. of Computer and Systems Sciences
Stockholm University / KTH
</affiliation>
<email confidence="0.904558">
sumithra@dsv.su.se
</email>
<author confidence="0.991652">
Markus Saers
</author>
<affiliation confidence="0.9992095">
Dept. of Linguistics and Philology
Uppsala University
</affiliation>
<email confidence="0.994053">
markus.saers@lingfil.uu.se
</email>
<sectionHeader confidence="0.995567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977">
Our system for the CoNLL 2008 shared
task uses a set of individual parsers, a set of
stand-alone semantic role labellers, and a
joint system for parsing and semantic role
labelling, all blended together. The system
achieved a macro averaged labelled Fi-
score of 79.79 (WSJ 80.92, Brown 70.49)
for the overall task. The labelled attach-
ment score for syntactic dependencies was
86.63 (WSJ 87.36, Brown 80.77) and the
labelled Fl-score for semantic dependen-
cies was 72.94 (WSJ 74.47, Brown 60.18).
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999359375">
This paper presents a system for the CoNLL 2008
shared task on joint learning of syntactic and se-
mantic dependencies (Surdeanu et al., 2008), com-
bining a two-step pipelined approach with a joint
approach.
In the pipelined system, eight different syntac-
tic parses were blended, yielding the input for two
variants of a semantic role labelling (SRL) system.
Furthermore, one of the syntactic parses was used
with an early version of the SRL system, to pro-
vide predicate predictions for a joint syntactic and
semantic parser. For the final submission, all nine
syntactic parses and all three semantic parses were
blended.
The system is outlined in Figure 1; the dashed
arrow indicates the potential for using the predi-
</bodyText>
<footnote confidence="0.943402">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<figureCaption confidence="0.999832">
Figure 1: Overview of the submitted system.
</figureCaption>
<bodyText confidence="0.9778425">
cate prediction to improve the joint syntactic and
semantic system.
</bodyText>
<sectionHeader confidence="0.980364" genericHeader="method">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999078833333333">
The initial parsing system was created using Malt-
Parser (Nivre et al., 2007) by blending eight dif-
ferent parsers. To further advance the syntactic ac-
curacy, we added the syntactic structure predicted
by a joint system for syntactic and semantic depen-
dencies (see Section 3.4) in the blending process.
</bodyText>
<subsectionHeader confidence="0.984547">
2.1 Parsers
</subsectionHeader>
<bodyText confidence="0.998887333333333">
The MaltParser is a dependency parser genera-
tor, with three parsing algorithms: Nivre’s arc
standard, Nivre’s arc eager (see Nivre (2004)
for a comparison between the two Nivre algo-
rithms), and Covington’s (Covington, 2001). Both
of Nivre’s algorithms assume projectivity, but
the MaltParser supports pseudo-projective parsing
(Nilsson et al., 2007), for projectivization and de-
projectivization.
</bodyText>
<figure confidence="0.9834961">
2 Pipelined SRLs
P
8 MaltParsers
P
Parser Blender
SRL Blender
Parse + SRL
Joint Parser/SRL
Possible
Iteration
</figure>
<page confidence="0.860455">
248
</page>
<reference confidence="0.2035435">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248–252
Manchester, August 2008
</reference>
<table confidence="0.999148">
WSJ Brown
Best single parse 85.22% 78.37%
LAS weights 87.00% 80.60%
Learned weights 87.36% 80.77%
</table>
<tableCaption confidence="0.999385">
Table 1: Labelled attachment score on the two test
</tableCaption>
<bodyText confidence="0.968867083333333">
sets of the best single parse, blended with weights
set to PoS labelled attachment score (LAS) and
blended with learned weights.
Four parsing algorithms (the two Nivre al-
gorithms, and Covington’s projective and non-
projective version) were used, creating eight
parsers by varying the parsing direction, left-to-
right and right-to-left. The latter was achieved by
reversing the word order in a pre-processing step
and then restoring it in post-processing. For the fi-
nal system, feature models and training parameters
were adapted from Hall et al. (2007).
</bodyText>
<subsectionHeader confidence="0.995418">
2.2 Blender
</subsectionHeader>
<bodyText confidence="0.99999735">
The single parses were blended following the pro-
cedure of Hall et al. (2007). The parses of each
sentence were combined into a weighted directed
graph. The Chu-Liu-Edmonds algorithm (Chu and
Liu, 1965; Edmonds, 1967) was then used to find
the maximum spanning tree (MST) of the graph,
which was considered the final parse of the sen-
tence. The weight of each graph edge was calcu-
lated as the sum of the weights of the correspond-
ing edges in each single parse tree.
We used a simple iterative weight updating algo-
rithm to learn the individual weights of each single
parser output and part-of-speech (PoS) using the
development set. To construct an initial MST, the
labelled attachment score was used. Each single
weight, corresponding to an edge of the hypoth-
esis tree, was then iteratively updated by slightly
increasing or decreasing the weight, depending on
whether it belonged to a correct or incorrect edge
as compared to the reference tree.
</bodyText>
<subsectionHeader confidence="0.858681">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.9999935">
The results are summarized in Table 1; the parse
with LAS weights and the best single parse
(Nivre’s arc eager algorithm with left-to-right pars-
ing direction) are also included for comparison.
</bodyText>
<sectionHeader confidence="0.980274" genericHeader="method">
3 Semantic Role Labelling
</sectionHeader>
<bodyText confidence="0.9999767">
The SRL system is a pipeline with three chained
stages: predicate identification, argument identifi-
cation, and argument classification. Predicate and
argument identification are treated as binary clas-
sification problems. In a simple post-processing
predicate classification step, a predicted predicate
is assigned the most frequent sense from the train-
ing data. Argument classification is treated as a
multi-class learning problem, where the classes
correspond to the argument types.
</bodyText>
<subsectionHeader confidence="0.999922">
3.1 Learning and Parameter Optimization
</subsectionHeader>
<bodyText confidence="0.999991375">
For learning and prediction we used the freely
available support vector machine (SVM) imple-
mentation LIBSVM (version 2.86) (Chang and
Lin, 2001). The choice of cost and kernel parame-
ter values will often significantly influence the per-
formance of the SVM classifier. We therefore im-
plemented a parameter optimizer based on the DI-
RECT optimization algorithm (Gablonsky, 2001).
It iteratively divides the search space into smaller
hyperrectangles, sampling the objective function
in the centroid of each hyperrectangle, and select-
ing those hyperrectangles that are potentially opti-
mal for further processing. The search space con-
sisted of the SVM parameters to optimize and the
objective function was the cross-validation accu-
racy reported by LIBSVM.
Tests performed during training for predicate
identification showed that the use of runtime opti-
mization of the SVM parameters for nonlinear ker-
nels yielded a higher average Fi-score effective-
ness. Surprisingly, the best nonlinear kernels were
always outperformed by the linear kernel with de-
fault settings, which indicates that the data is ap-
proximately linearly separable.
</bodyText>
<subsectionHeader confidence="0.999944">
3.2 Filtering and Data Set Splitting
</subsectionHeader>
<bodyText confidence="0.99997105882353">
To decrease the number of instances during train-
ing, all predicate and argument candidates with
PoS-tags that occur very infrequently in the
training set were filtered out. Some PoS-tags
were filtered out for all three stages, e.g. non-
alphanumerics, HYPH, SYM, and LS. This ap-
proach was effective, e.g. removing more than half
of the total number of instances for predicate pre-
diction.
To speed up the SVM training and allow for
parallelization, each data set was split into several
bins. However, there is a trade-off between speed
and accuracy. Performance consistently deterio-
rated when splitting into smaller bins. The final
system contained two variants, one with more bins
based on a combination of PoS-tags and lemma
frequency information, and one with fewer bins
</bodyText>
<page confidence="0.993269">
249
</page>
<bodyText confidence="0.9999755">
based only on PoS-tag information. The three
learning tasks used different splits. In general, the
argument identification step was the most difficult
and therefore required a larger number of bins.
</bodyText>
<subsectionHeader confidence="0.953574">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.998870333333333">
We implemented a large number of features (over
50)1 for the SRL system. Many of them can be
found in the literature, starting from Gildea and
Jurafsky (2002) and onward. All features, except
bag-of-words, take nominal values, which are bi-
narized for the vectors used as input to the SVM
classifier. Low-frequency feature values (except
for Voice, Initial Letter, Number of Words, Rela-
tive Position, and the Distance features), below a
threshold of 20 occurrences, were given a default
value.
We distinguish between single node and node
pair features. The following single node features
were used for all three learning tasks and for both
the predicate and argument node:2
</bodyText>
<listItem confidence="0.966878153846154">
• Lemma, PoS, and Dependency relation (DepRel) for
the node itself, the parent, and the left and right sibling
• Initial Letter (upper-case/lower-case), Number of
Words, and Voice (based on simple heuristics, only for
the predicate node during argument classification)
• PoS Sequence and PoS bag-of-words (BoW) for the
node itself with children and for the parent with chil-
dren
• Lemma and PoS for the first and last child of the node
• Sequence and BoW of Lemma and PoS for content
words
• Sequence and BoW of PoS for the immediate children’s
content words
• Sequence and BoW of PoS for the parent’s content
words and for the parent’s immediate children
• Sequence and BoW of DepRels for the node itself, for
the immediate children, and for the parent’s immediate
children
All extractors of node pair features, where the pair
consists of the predicate and the argument node,
can be used both for argument identification and
argument classification. We used the following
node pair features:
• Relative Position (the argument is before/after the pred-
icate), Distance in Words, Middle Distance in DepRels
• PoS Full Path, PoS Middle Path, PoS Short Path
</listItem>
<footnote confidence="0.9978646">
1Some features were discarded for the final system based
on Information Gain, calculated using Weka (Witten and
Frank, 2005).
2For all features using lemma or PoS the (predicted) split
value is used.
</footnote>
<bodyText confidence="0.999904">
The full path feature contains the PoS-tag of the ar-
gument node, all dependency relations between the
argument node and the predicate node and finally
the PoS-tag of the predicate node. The middle path
goes to the lowest common ancestor for argument
and predicate (this is also the distance calculated
by Middle Distance in DepRels) and the short path
only contains the dependency relation of the argu-
ment and predicate nodes.
</bodyText>
<subsectionHeader confidence="0.999679">
3.4 Joint Syntactic and Semantic Parsing
</subsectionHeader>
<bodyText confidence="0.9999931">
When considering one predicate at a time, SRL be-
comes a regular labelling problem. Given a pre-
dicted predicate, joint learning of syntactic and se-
mantic dependencies can be carried out by simulta-
neously assigning an argument label and a depen-
dency relation. This is possible because we know
a priori where to attach the argument, since there
is only one predicate candidate3. The MaltParser
system for English described in Hall et al. (2007)
was used as a baseline, and then optimized for this
new task, focusing on feature selection.
A large feature model was constructed, and
backward selection was carried out until no fur-
ther gain could be observed. The feature model of
MaltParser consists of a number of feature types,
each describing a starting point, a path through the
structure so far, and a column of the node arrived
at. The number of feature types was reduced from
37 to 35 based on the labelled Fl-score.
As parsing is done at the same time as argu-
ment labelling, different syntactic structures risk
being assigned to the same sentence, depending
on which predicate is currently processed. This
means that several, possibly different, parses have
to be combined into one. In this experiment, the
head and the dependency label were concatenated,
and the most frequent one was used. In case of
a tie, the first one to appear was used. The like-
lihood of the chosen labelling was also used as a
confidence measure for the syntactic blender.
</bodyText>
<subsectionHeader confidence="0.998232">
3.5 Blending and Post-Processing
</subsectionHeader>
<bodyText confidence="0.9989774">
Combining the output from several different sys-
tems has been shown to be beneficial (Koomen
et al., 2005). For the final submission, we com-
bined the output of two variants of the pipelined
SRL system, each using different data splits, with
</bodyText>
<footnote confidence="0.988221">
3The version of the joint system used in the submission
was based on an early predicate prediction. More accurate
predicates would give a major improvement for the results.
</footnote>
<page confidence="0.985742">
250
</page>
<table confidence="0.999627142857143">
Test set Pred PoS Labelled Fl Unlabelled Fl
WSJ All 82.90 90.90
NN* 81.12 86.39
VB* 85.52 96.49
Brown All 67.48 85.49
NN* 58.34 75.35
VB* 73.24 91.97
</table>
<tableCaption confidence="0.998812">
Table 2: Semantic predicate results on the test sets.
</tableCaption>
<bodyText confidence="0.9997634">
the SRL output of the joint system. A simple uni-
form weight majority vote heuristic was used, with
no combinatorial constraints on the selected argu-
ments. For each sentence, all predicates that were
identified by a majority of the systems were se-
lected. Then, for each selected predicate, its ar-
guments were picked by majority vote (ignoring
the systems not voting for the predicate). The best
single SRL system achieved a labelled F1-score
of 71.34 on the WSJ test set and 57.73 on the
Brown test set, compared to 74.47 and 60.18 for
the blended system.
As a final step, we filtered out all verbal and
nominal predicates not in PropBank or NomBank,
respectively, based on the predicted PoS-tag and
lemma. Each lexicon was expanded with lemmas
from the training set, due to predicted lemma er-
rors in the training data. This turned out to be a
successful strategy for the individual systems, but
slightly detrimental for the blended system.
</bodyText>
<subsectionHeader confidence="0.693644">
3.6 Results
</subsectionHeader>
<bodyText confidence="0.999824">
Semantic predicate results for WSJ and Brown can
be found in Table 2. Table 4 shows the results for
identification and classification of arguments.
</bodyText>
<sectionHeader confidence="0.909427" genericHeader="evaluation">
4 Analysis and Conclusions
</sectionHeader>
<bodyText confidence="0.99719775">
In general, the mixed and blended system performs
well on all tasks, rendering a sixth place in the
CoNLL 2008 shared task. The overall scores for
the submitted system can be seen in Table 3.
</bodyText>
<subsectionHeader confidence="0.999055">
4.1 Parsing
</subsectionHeader>
<bodyText confidence="0.9995323">
For the blended parsing system, the labelled at-
tachment score drops from 87.36 for the WSJ test
set to 80.77 for the Brown test set, while the unla-
belled attachment score only drops from 89.88 to
86.28. This shows that the system is robust with
regards to the overall syntactic structure, even if
picking the correct label is more difficult for the
out-of-domain text.
The parser has difficulties finding the right head
for punctuation and symbols. Apart from errors re-
</bodyText>
<table confidence="0.99589525">
WSJ + Brown WSJ Brown
Syn + Sem 79.79 80.92 70.49
Syn 86.63 87.36 80.77
Sem 72.94 74.47 60.18
</table>
<tableCaption confidence="0.994738">
Table 3: Syntactic and semantic scores on the test
</tableCaption>
<bodyText confidence="0.9636148">
sets for the submitted system. The scores, from top
to bottom, are labelled macro F1, labelled attach-
ment score and labelled F1.
garding punctuation, most errors occur for IN and
TO. A majority of these problems are related to as-
signing the correct dependency. This is not surpris-
ing, since these are categories that focus on form
rather than function.
There is no significant difference in score for left
and right dependencies, presumably because of the
bi-directional parsing. However, the system over-
predicts dependencies to the root. This is mainly
due to the way MaltParser handles tokens not be-
ing attached anywhere during parsing. These to-
kens are by default assigned to the root.
</bodyText>
<subsectionHeader confidence="0.979805">
4.2 SRL
</subsectionHeader>
<bodyText confidence="0.99993148275862">
Similarly to the parsing results, the blended SRL
system is less robust with respect to labelled F1-
score, dropping from 74.47 on the WSJ test set to
60.18 on the Brown test set. The corresponding
drop in unlabelled F1-score is from 82.90 to 75.49.
The simple method of picking the most com-
mon sense from the training data works quite well,
but the difference in domain makes it more diffi-
cult to find the correct sense for the Brown corpus.
In the future, a predicate classification module is
needed. For the WSJ corpus, assigning the most
common predicate sense works better with nomi-
nal than with verbal predicates, while verbal pred-
icates are handled better for the Brown corpus.
In general, verbal predicate-argument structures
are handled better than nominal ones, for both
test sets. This is not surprising, since nominal
predicate-argument structures tend to vary more in
their composition.
Since we do not use global constraints for the
argument labelling (looking at the whole argument
structure for each predicate), the system can out-
put the same argument label for a predicate several
times. For the WSJ test set, for instance, the ra-
tio of repeated argument labels is 5.4% in the sys-
tem output, compared to 0.3% in the gold standard.
However, since there are no confidence scores for
predictions it is difficult to handle this in the cur-
rent system.
</bodyText>
<page confidence="0.991358">
251
</page>
<table confidence="0.999604911392406">
PPOSS(pred) + ARG WSJ Fl Brown Fl 39th Annual Association for Computing Machinery
Southeast Conference, Athens, Georgia.
Edmonds, Jack. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233–240.
Gablonsky, J¨org M. 2001. Modifications of the DI-
RECT algorithm. Ph.D. thesis, North Carolina State
University, Raleigh, North Carolina.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245–288.
Hall, Johan, Jens Nilsson, Joakim Nivre, G¨uls¸en
Eryiˇgit, Be´ata Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, Prague, Czech Republic.
Koomen, Peter, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Proceed-
ings of the Ninth Conference on Computational Nat-
ural Language Learning (CoNLL-2005), Ann Arbor,
Michigan.
Nilsson, Jens, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics, Prague, Czech Republic.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas
Chanev, G¨uls¸en Eryiˇgit, Sandra K¨ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
2(13):95–135.
Nivre, Joakim. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the ACL’04
Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together, Barcelona, Spain.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008), Manchester, Great
Britain.
Witten, Ian H. and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, Amsterdam, 2nd edition.
NN* + A0 61.42 of Language
NN* + A1 67.07 38.99
NN* + A2 57.02 53.10
NN* + A3 63.08 26.19
NN* + AM-ADV 4.65 (16.67)
NN* + AM-EXT 44.78 (-)
NN* + AM-LOC 49.45 (40.00)
NN* + AM-MNR 53.51 (-)
NN* + AM-NEG 79.37 21.82
NN* + AM-TMP 67.23 (46.15)
VB* + A0 81.72 (25.00)
VB* + A1 81.77 73.58
VB* + A2 60.91 67.99
VB* + A3 61.49 50.67
VB* + A4 77.84 (14.28)
VB* + AM-ADV 47.49 (40.00)
VB* + AM-CAU 55.12 30.33
VB* + AM-DIR 41.86 (35.29)
VB* + AM-DIS 71.91 37.14
VB* + AM-EXT 60.38 37.04
VB* + AM-LOC 55.69 (-)
VB* + AM-MNR 49.54 37.50
VB* + AM-MOD 94.85 36.25
VB* + AM-NEG 93.45 82.42
VB* + AM-PNC 50.00 77.08
VB* + AM-TMP 69.59 (62.50)
VB* + C-A1 70.76 49.07
VB* + R-A0 83.68 55.32
VB* + R-A1 68.87 70.83
VB* + R-AM-LOC 38.46 51.43
VB* + R-AM-TMP 56.82 (25.00)
</table>
<tableCaption confidence="0.825236">
Table 4: Semantic argument results (58.82)
</tableCaption>
<bodyText confidence="0.9815396">
test sets, showing arguments with on the two
instances in the gold test set (fewer more than 20
Brown are given in parentheses). out within instances for
Acknowledgements by the course Ma-
This project was carried support GSLT (Swedish
chine Learning 2, organized of Language Tech-
National Graduate School Nugues from NGSLT
nology), with additional were performed Technol-
(Nordic Graduate School (projects Joakim Nivre,
ogy). We thank our supervisors thank Tore for advice and
</bodyText>
<reference confidence="0.724153615384615">
Bj¨orn Gamb¨ack and Pierre -Jen Lin, on the
support. Computations machines. p2005008 and
BalticGrid and UPPMAX On the Sundqvist at
p2005028) resources. We Science 2001. LIBSVM:
UPPMAX for technical assistance. shortest arbores-
References Sinica, 14:1396–
Chang, Chih-Chung and Chih
A library for support vector
Chu, Y. J. and T. H. Liu. 1965.
cence of a directed graph.
1400.
Covington, Michael A. 2001. A fundamental algo-
rithm for dependency parsing. In Proceedings of the
</reference>
<page confidence="0.997425">
252
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.049184">
<title confidence="0.99997">Mixing and Blending Syntactic and Semantic Dependencies</title>
<author confidence="0.995958">Yvonne Samuelsson</author>
<affiliation confidence="0.994202">Dept. of</affiliation>
<address confidence="0.728156">Stockholm</address>
<email confidence="0.961169">yvonne.samuelsson@ling.su.se</email>
<author confidence="0.999705">Johan Eklund</author>
<affiliation confidence="0.919931">SSLIS University College of</affiliation>
<email confidence="0.632921">johan.eklund@hb.se</email>
<author confidence="0.863865">Oscar T¨ackstr¨om</author>
<affiliation confidence="0.873184">Dept. of Linguistics and SICS / Uppsala</affiliation>
<email confidence="0.899129">oscar@sics.se</email>
<author confidence="0.999714">Mark Fiˇsel</author>
<affiliation confidence="0.999463">Dept. of Computer University of</affiliation>
<email confidence="0.81057">fishel@ut.ee</email>
<author confidence="0.961111">Sumithra Velupillai</author>
<affiliation confidence="0.9987405">Dept. of Computer and Systems Stockholm University /</affiliation>
<email confidence="0.634568">sumithra@dsv.su.se</email>
<author confidence="0.998323">Markus Saers</author>
<affiliation confidence="0.999953">Dept. of Linguistics and Uppsala University</affiliation>
<email confidence="0.889865">markus.saers@lingfil.uu.se</email>
<abstract confidence="0.996225583333334">Our system for the CoNLL 2008 shared task uses a set of individual parsers, a set of stand-alone semantic role labellers, and a joint system for parsing and semantic role labelling, all blended together. The system a macro averaged labelled score of 79.79 (WSJ 80.92, Brown 70.49) for the overall task. The labelled attachment score for syntactic dependencies was 86.63 (WSJ 87.36, Brown 80.77) and the for semantic dependen-</abstract>
<note confidence="0.489174">cies was 72.94 (WSJ 74.47, Brown 60.18).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>248--252</pages>
<contexts>
<context position="613" citStr="CoNLL 2008" startWordPosition="73" endWordPosition="74">ing and Blending Syntactic and Semantic Dependencies Yvonne Samuelsson Dept. of Linguistics Stockholm University yvonne.samuelsson@ling.su.se Johan Eklund SSLIS University College of Bor˚as johan.eklund@hb.se Oscar T¨ackstr¨om Dept. of Linguistics and Philology SICS / Uppsala University oscar@sics.se Mark Fiˇsel Dept. of Computer Science University of Tartu fishel@ut.ee Sumithra Velupillai Dept. of Computer and Systems Sciences Stockholm University / KTH sumithra@dsv.su.se Markus Saers Dept. of Linguistics and Philology Uppsala University markus.saers@lingfil.uu.se Abstract Our system for the CoNLL 2008 shared task uses a set of individual parsers, a set of stand-alone semantic role labellers, and a joint system for parsing and semantic role labelling, all blended together. The system achieved a macro averaged labelled Fiscore of 79.79 (WSJ 80.92, Brown 70.49) for the overall task. The labelled attachment score for syntactic dependencies was 86.63 (WSJ 87.36, Brown 80.77) and the labelled Fl-score for semantic dependencies was 72.94 (WSJ 74.47, Brown 60.18). 1 Introduction This paper presents a system for the CoNLL 2008 shared task on joint learning of syntactic and semantic dependencies (Su</context>
</contexts>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 248–252</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manchester</author>
</authors>
<date>2008</date>
<marker>Manchester, 2008</marker>
<rawString>Manchester, August 2008</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>