<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005238">
<title confidence="0.9924535">
The Unknown Word Problem: a Morphological Analysis of
Japanese Using Maximum Entropy Aided by a Dictionary
</title>
<author confidence="0.991446">
Kiyotaka Uchimotot, Satoshi Sekine$ and Hitoshi Isaharat
</author>
<affiliation confidence="0.987562">
tCommunications Research Laboratory $New York University
</affiliation>
<address confidence="0.944576">
2-2-2, Hikari-dai, Seika-cho, Soraku-gun, 715 Broadway, 7th floor
Kyoto, 619-0289 Japan New York, NY 10003, USA
</address>
<email confidence="0.98211">
Cuchimoto, isahara]@crl.go.jp sekine@cs.nyu.edu
</email>
<sectionHeader confidence="0.937822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99938875">
In this paper we describe a morphological analy-
sis method based on a maximum entropy model.
This method uses a model that can not only
consult a dictionary with a large amount of lex-
ical information but can also identify unknown
words by learning certain characteristics. The
model has the potential to overcome the un-
known word problem.
</bodyText>
<sectionHeader confidence="0.986064" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949745098039">
Morphological analysis is one of the basic tech-
niques used in Japanese sentence analysis. A
morpheme is a minimal grammatical unit, such
as a word or a suffix, and morphological analysis
is the process segmenting a given sentence into
a row of morphemes and assigning to each mor-
pheme grammatical attributes such as a part-
of-speech (POS) and an inflection type. One of
the most important problems in morphological
analysis is that posed by unknown words, which
are words found in neither a dictionary nor a
training corpus, and there have been two sta-
tistical approaches to this problem. One is to
acquire unknown words from corpora and put
them into a dictionary (e.g., (Mori and Nagao,
1996)), and the other is to estimate a model
that can identify unknown words correctly (e.g.,
(Kashioka et al., 1997; Nagata, 1999)). We
would like to be able to make good use of both
approaches. If words acquired by the former
method could be added to a dictionary and a
model developed by the latter method could
consult the amended dictionary, then the model
could be the best statistical model which has
the potential to overcome the unknown word
problem. Mori and Nagao proposed a statisti-
cal model that can consult a dictionary (Mori
and Nagao, 1998). In their model the proba-
bility that a string of letters or characters is
a morpheme is augmented when the string is
found in a dictionary. The improvement of the
accuracy was slight, however, so we think that
it is difficult to efficiently integrate the mecha-
nism for consulting a dictionary into an n-gram
model. In this paper we therefore describe a
morphological analysis method based on a max-
imum entropy (M.E.) model. This method uses
a model that can not only consult a dictionary
but can also identify unknown words by learn-
ing certain characteristics. To learn these char-
acteristics, we focused on such information as
whether or not a string is found in a dictio-
nary and what types of characters are used in a
string. The model estimates how likely a string
is to be a morpheme according to the informa-
tion on hand. When our method was used to
identify morpheme segments in sentences in the
Kyoto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%.
</bodyText>
<sectionHeader confidence="0.966641" genericHeader="introduction">
2 A Morpheme Model
</sectionHeader>
<bodyText confidence="0.99992">
This section describes a model which estimates
how likely a string is to be a morpheme. We
implemented this model within an M.E. frame-
work.
Given a tokenized test corpus, the problem
of Japanese morphological analysis can be re-
duced to the problem of assigning one of two
tags to each string in a sentence. A string is
tagged with a 1 or a 0 to indicate whether or
not it is a morpheme. When a string is a mor-
pheme, a grammatical attribute is assigned to
it. The 1 tag is thus divided into the num-
ber, n, of grammatical attributes assigned to
morphemes, and the problem is to assign an at-
tribute (from 0 to n) to every string in a given
sentence. The (n+1) tags form the space of &amp;quot;fu-
tures&amp;quot; in the M.E. formulation of our problem
of morphological analysis. The M.E. model, as
well as other similar models, enables the com-
putation of P(fjh) for any future f from the
space of possible futures, F, and for every his-
tory, h, from the space of possible histories, H.
A &amp;quot;history&amp;quot; in M.E. is all of the conditioning
data that enable us to make a decision in the
space of futures. In the problem of morphologi-
cal analysis, we can reformulate this in terms of
finding the probability of f associated with the
relationship at index t in the test corpus:
</bodyText>
<equation confidence="0.959165">
P(fjht) = P(fjInformation derivable
</equation>
<bodyText confidence="0.9712058">
from the test corpus
related to relationship t)
The computation of P(fjh) in any M.E. models
is dependent on a set of &amp;quot;features&amp;quot; which would
be helpful in making a prediction about the fu-
ture. Like most current M.E. models in com-
putational linguistics, our model is restricted to
those features which are binary functions of the
history and future. For instance, one of our fea-
tures is
</bodyText>
<equation confidence="0.999746">
1 : if has(h, x) = true,
x = &amp;quot;POS(-1)(Major) : verb,��
(1)
&amp; f = 1
0 : otherwise:
</equation>
<bodyText confidence="0.984494692307692">
Here &amp;quot;has(h,x)&amp;quot; is a binary function that re-
turns true if the history h has feature x. In our
experiments, we focused on such information as
whether or not a string is found in a dictionary,
the length of the string, what types of characters
are used in the string, and the part-of-speech of
the adjacent morpheme.
Given a set of features and some training
data, the M.E. estimation process produces a
model in which every feature gi has an associ-
ated parameter ai. This enables us to compute
the conditional probability as follows (Berger et
al., 1996):
</bodyText>
<equation confidence="0.9979186">
Qi agi(h&gt;f)
P(f jh) = Zai(h)(2)
gi(h�f)
a� (3)
i
</equation>
<bodyText confidence="0.994589">
The M.E. estimation process guarantees that for
every feature gi, the expected value of gi accord-
ing to the M.E. model will equal the empirical
expectation of gi in the training corpus. In other
words,
</bodyText>
<equation confidence="0.99347925">
X P (h, f) - gi(h, f)
h,f
= X P(h) � X PM.E.(fjh) -gi(h,f)• (4)
h f
</equation>
<bodyText confidence="0.999751555555556">
Here P is an empirical probability and PM.E. is
the probability assigned by the model.
We define part-of-speech and bunsetsu
boundaries as grammatical attributes. Here a
bunsetsu is a phrasal unit consisting of one or
more morphemes. When there are m types
of parts-of-speech, and the left-hand side of
each morpheme may or may not be a bunsetsu
boundary, the number, n, of grammatical at-
tributes assigned to morphemes is 2 x m. 1 We
propose a model which estimates the likelihood
that a given string is a morpheme and has the
grammatical attribute i(1 &lt; i &lt; n). We call it
a morpheme model. This model is represented
by Eq. (2), in which f can be one of (n + 1)
tags from 0 to n.
A given sentence is divided into morphemes,
and a grammatical attribute is assigned to each
morpheme so as to maximize the sentence prob-
ability estimated by our morpheme model. Sen-
tence probability is defined as the product of the
probabilities estimated for a particular division
of morphemes in a sentence. We use the Viterbi
algorithm to find the optimal set of morphemes
in a sentence and we use the method proposed
by Nagata (Nagata, 1994) to search for the N-
best sets.
</bodyText>
<sectionHeader confidence="0.986807" genericHeader="background">
3 Experiments and Discussion
</sectionHeader>
<subsectionHeader confidence="0.981501">
3.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.986298666666667">
The part-of-speech categories that we used fol-
low those of JUMAN (Kurohashi and Nagao,
1999). There are 53 categories covering all pos-
sible combinations of major and minor cate-
gories as defined in JUMAN. The number of
grammatical attributes is 106 if we include the
detection of whether or not the left side of a
morpheme is a bunsetsu boundary. We do not
identify inflection types probabilistically since
&apos;Not only morphemes but also bunsetsus can be iden-
tified by considering the information related to their bun-
setsu boundaries.
</bodyText>
<page confidence="0.745852">
&lt;&gt;8
</page>
<equation confidence="0.95757925">
&gt;:
g(h, f) =
Za(h) = X Y
f i
</equation>
<bodyText confidence="0.997165650000001">
they can be almost perfectly identified by check-
ing the spelling of the current morpheme after
a part-of-speech has been assigned to it. There-
fore, f in Eq. (2) can be one of 107 tags from 0
to 106.
We used the Kyoto University text corpus
(Version 2) (Kurohashi and Nagao, 1997), a
tagged corpus of the Mainichi newspaper. For
training, we used 7,958 sentences from newspa-
per articles appearing from January 1 to Jan-
uary 8, 1995, and for testing, we used 1,246
sentences from articles appearing on January 9,
1995.
Given a sentence, for every string consisting
of five or less characters and every string ap-
pearing in the JUMAN dictionary (Kurohashi
and Nagao, 1999), whether or not the string is
a morpheme was determined and then the gram-
matical attribute of each string determined to
be a morpheme was identified and assigned to
that string. The maximum length was set at five
because morphemes consisting of six or more
characters are mostly compound words or words
consisting of katakana characters. The stipula-
tion that strings consisting of six or more char-
acters appear in the JUMAN dictionary was set
because long strings not present in the JUMAN
dictionary were rarely found to be morphemes
in our training corpus. Here we assume that
compound words that do not appear in the JU-
MAN dictionary can be divided into strings con-
sisting of five or less characters because com-
pound words tend not to appear in dictionar-
ies, and in fact, compound words which con-
sist of six or more characters and do not ap-
pear in the dictionary were not found in our
training corpus. Katakana strings that are not
found in the JUMAN dictionary were assumed
to be included in the dictionary as an entry
having the part-of-speech &amp;quot;Unknown(Major),
Katakana(Minor).&amp;quot; An optimal set of mor-
phemes in a sentence is searched for by em-
ploying the Viterbi algorithm under the con-
dition that connectivity rules defined between
parts-of-speech in JUMAN must be met. The
assigned part-of-speech in the optimal set is
not always selected from the parts-of-speech at-
tached to entries in the JUMAN dictionary, but
may also be selected from the 53 categories of
the M.E. model. It is difficult to select an appro-
priate category from the 53 when there is little
training data, so we assume that every entry in
the JUMAN dictionary has all possible parts-of-
speech, and the part-of-speech assigned to each
morpheme is selected from those attached to the
entry corresponding to the morpheme string.
The features used in our experiments are
listed in Table 1. Each row in Table 1 contains a
feature type, feature values, and an experimen-
tal result that will be explained later. Each fea-
ture consists of a type and a value. The features
are basically some attributes of the morpheme
itself or those of the morpheme to the left of
it. We used the 31,717 features that were found
three or more times in the training corpus. The
notations &amp;quot;(0)&amp;quot; and &amp;quot;(-1)&amp;quot; used in the feature
type column in Table 1 respectively indicate a
target string and the morpheme on the left of
it.
The terms used in the table are the following:
String: Strings which appeared as a morpheme
five or more times in the training corpus
Length: Length of a string
POS: Part-of-speech. &amp;quot;Major&amp;quot; and &amp;quot;Minor&amp;quot;
respectively indicate major and minor part-
of-speech categories as defined in JUMAN.
Inf: Inflection type as defined in JUMAN
Dic: We use the JUMAN dictionary, which has
about 200,000 entries (Kurohashi and Na-
gao, 1999). &amp;quot;Major&amp;Minor&amp;quot; indicates pos-
sible combinations between major and mi-
nor part-of-speech categories. When the
target string is in the dictionary, the part-
of-speech attached to the entry correspond-
ing to the string is used as a feature value.
If an entry has two or more parts-of-speech,
the part-of-speech which leads to the high-
est probability in a sentence estimated from
our model is selected as a feature value.
JUMAN has another type of dictionary,
which is called a phrase dictionary. Each
entry in the phrase dictionary consists of
one or more morphemes such as &amp;quot; (to,
case marker), (wa, topic marker),
(ie, say).&amp;quot; JUMAN uses this dictionary to
detect morphemes which need a longer con-
text to be identified correctly. When the
target string corresponds to the string of
the left most morpheme in the phrase dic-
tionary in JUMAN, the part-of-speech at-
</bodyText>
<tableCaption confidence="0.994893">
Table 1: Features.
</tableCaption>
<table confidence="0.999684617647059">
Feature Feature type Feature value (Number of value) Accuracy without
number each feature set
Recall Precision F-measure
1 String(0) (4,331) 93.66% 93.81% 93.73
2 String(-1) (4,331) (-2.14%) (-1.28%) (-1.71)
3 Dic(0)(Major) Verb, Verb&amp;Phrase, Adj, Adj&amp;Phrase, 94.64% 92.87% 93.75
. . . (28)
4 Dic(0)(Minor) Common noun, Common noun&amp;Phrase, (-1.16%) (-2.22%) (-1.69)
Topic marker, . . . (90)
5 Dic(0)(Major&amp;Minor) Noun&amp;Common noun,
Noun&amp;Common noun&amp;Phrase, . . . (103)
6 Length(0) 1, 2, 3, 4, 5, 6 or more (6) 95.52% 94.11% 94.81
7 Length(-1) 1, 2, 3, 4, 5, 6 or more (6) (-0.28%) (-0.98%) (-0.63)
8 TOC(0)(Beginning) Kanji, Hiragana, Symbol, Number, 95.17% 93.89% 94.52
Katakana, Alphabet (6)
9 TOC(0)(End) Kanji, Hiragana, Symbol, Number, (-0.63%) (-1.20%) (-0.92)
Katakana, Alphabet (6)
10 TOC(0)(Transition) Kanji-+Hiragana, Number-+Kanji,
Katakana-+Kanji, . . . (30)
11 TOC(-1)(End) Kanji, Hiragana, Symbol, Number,
Katakana, Alphabet (6)
12 TOC(-1)(Transition) Kanji-+Hiragana, Number-+Kanji,
Katakana-+Kanji, . . . (30)
13 POS(-1)(Major) Verb, Adj, Noun, Unknown, .. . (15) 95.60% 95.31% 95.45
14 POS(-1)(Minor) Common noun, Sahen noun, Numeral, (-0.20%) (+0.22%) (+0.01)
. . . (45)
15 POS(-1)(Major&amp;Minor) [nil], Noun&amp;Common noun,
Noun&amp;Common noun&amp;Phrase, . . . (54)
16 Inf(-1)(Major) Vowel verb, .. . (33) 95.66% 95.00% 95.33
17 Inf(-1)(Minor) Stem, Basic form, Imperative form, .. . (60) (-0.14%) (-0.09%) (-0.11)
18 BB(-1) [nil], [exist] (2) 95.82% 95.25% 95.53
19 BB(-1) &amp; Noun&amp;Common, noun&amp;Bunsetsu boundary, (+0.02%) (+0.16%) (+0.09)
POS(-1)(Major&amp;Minor) Noun&amp;Common, noun&amp;Within a bunsetsu,
. . . (106)
</table>
<bodyText confidence="0.992202227272727">
tached to the entry plus the information
that it is in the phrase dictionary (such as
&amp;quot;Verb&amp;Phrase&amp;quot;) is used as a feature value.
TOC: Types of characters used in a string.
&amp;quot;(Beginning)&amp;quot; and &amp;quot;(End)&amp;quot; respectively
represent the leftmost and rightmost char-
acters of a string. When a string con-
sists of only one character, the &amp;quot;(Begin-
ning)&amp;quot; and &amp;quot;(End)&amp;quot; are the same charac-
ter. &amp;quot;TOC(0)(Transition)&amp;quot; represents the
transition from the leftmost character to
the rightmost one in a string. &amp;quot;TOC(-
1)(Transition)&amp;quot; represents the transition
from the rightmost character in the adja-
cent morpheme on the left to the leftmost
one in the target string. For example, when
the adjacent morpheme on the left is &amp;quot;
(sensei, teacher)&amp;quot; and the target string
is &amp;quot; (ni, case marker),&amp;quot; the feature value
&amp;quot;Kanji—�Hiragana&amp;quot; is selected.
BB: Indicates whether or not the left side of a
morpheme is a bunsetsu boundary.
</bodyText>
<subsectionHeader confidence="0.835557">
3.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999981875">
Some results of the morphological analysis are
listed in Table 2. Recall is the percentage of
morphemes in the test corpus whose segmen-
tation and major POS tag are identified cor-
rectly. Precision is the percentage of all mor-
phemes identified by the system that are iden-
tified correctly. F represents the F-measure and
is defined by the following equation.
</bodyText>
<equation confidence="0.9965825">
2 x Recall x Precision
Recall + Precision
</equation>
<bodyText confidence="0.9997">
Table 2 shows results obtained by using our
method, by using JUMAN, and by using JU-
MAN plus KNP (Kurohashi, 1998). We show
the result obtained using JUMAN plus KNP
because JUMAN alone assigns an &amp;quot;Unknown&amp;quot;
tag to katakana strings when they are not in
the dictionary. All katakana strings not found
</bodyText>
<equation confidence="0.942299">
F — measure =
</equation>
<tableCaption confidence="0.995727">
Table 2: Results of Experiments (Segmentation and major POS tagging).
</tableCaption>
<table confidence="0.99907575">
Recall Precision F-measure
Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467) 95.44
JUMAN 95.25% (29,814/31,302) 94.90% (29,814/31,417) 95.07
JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417) 98.31
</table>
<bodyText confidence="0.973680830985915">
in the dictionary are therefore evaluated as er-
rors. KNP improves on JUMAN by replacing
the &amp;quot;Unknown&amp;quot; tag with a &amp;quot;Noun&amp;quot; tag and dis-
ambiguating part-of-speech ambiguities which
arise during the process of parsing when there is
more than one JUMAN analysis with the same
score.
The accuracy in segmentation and major
POS tagging obtained with our method and
that obtained with JUMAN were about 3%
worse than that obtained with JUMAN plus
KNP. We think the main reason for this was
an insufficient amount of training data and fea-
ture sets and the inconsistency of the corpus.
The number of sentences in the training cor-
pus was only about 8,000, and we did not use
as many combined features as were proposed in
Ref. (Uchimoto et al., 1999). We were unable to
use more training data or more feature sets be-
cause every string consisting of five or less char-
acters in our training corpus was used to train
our model, so the amount of tokenized train-
ing data would have become too large and the
training would not have been completed on the
available machine if we had used more training
data or more feature sets. The inconsistency
of the corpus was due to the way the corpus
was made. The Kyoto University corpus was
made by manually correcting the output of JU-
MAN plus KNP, and it is difficult to manually
correct all of the inconsistencies in the output.
The use of JUMAN plus KNP thus has an ad-
vantage over the use of our method when we
evaluate a system&apos;s accuracy by using the Ky-
oto University corpus. For example, the num-
ber of morphemes whose rightmost character is
&amp;quot; &amp;quot; was 153 in the test corpus, and they were
all the same as those in the output of JUMAN
plus KNP. There were three errors (about 2%)
in the output of our system. There were several
inconsistencies in the test corpus such as &amp;quot;
(seisan, Noun), (sha, Suffix)(producer),&amp;quot; and
&amp;quot; (shouhi-sha, Noun)(consumer).&amp;quot; They
should have been corrected in the corpus-
making process to &amp;quot; (seisan, Noun), (sha,
Suffix)(producer),&amp;quot; and &amp;quot; (shouhi, Noun),
(sha, Suffix)(consumer).&amp;quot; It is difficult for
our model to discriminate among these with-
out over-training when there are such incon-
sistencies in the corpus. Other similar incon-
sistencies were, for example, &amp;quot; (geijutsu-
ka, Noun)(artist)&amp;quot; and &amp;quot; (kougei, Noun),
(ka, Suffix)(craftsman),&amp;quot; &amp;quot; (keishi-cho,
Noun)(the Metropolitan Police Board)&amp;quot; and &amp;quot;
(kensatsu, Noun), (cho, Noun)(the Pub-
lic Prosecutor&apos;s Office),&amp;quot; and &amp;quot; (genjitsu-
teki, Adjective)(realistic)&amp;quot; and &amp;quot; (risou,
Noun), (teki, Suffix)(ideal).&amp;quot;. If these had
been corrected consistently when making the
corpus, the accuracy obtained by our method
could have been better than that shown in Ta-
ble 2. A study on corpus revision should be un-
dertaken to resolve this issue. We believe it can
be resolved by using our trained model. There
is a high possibility that a morpheme lacks con-
sistency in the training corpus when its proba-
bility, re-estimated by our model, is low. Thus
a method which detects morphemes having a
low probability can identify those lacking con-
sistency in the training corpus. We intend to
try this in the future.
</bodyText>
<subsectionHeader confidence="0.967186">
3.3 Features and Accuracy
</subsectionHeader>
<bodyText confidence="0.998365571428571">
In our model, dictionary information and cer-
tain characteristics of unknown words are re-
flected as features, as shown in Table 1.
&amp;quot;String&amp;quot; and &amp;quot;Dic&amp;quot; reflect the dictionary in-
formation, 2 and &amp;quot;Length&amp;quot; and &amp;quot;TOC&amp;quot;(types
of characters) reflect the characteristics of un-
known words. Therefore, our model can not
only consult a dictionary but can also detect un-
known words. Table 1 shows the results of an
2&amp;quot;String&amp;quot; indicates strings that make up a morpheme
and were found five or more times in the training corpus.
Using this information as features in our M.E. model
corresponds to consulting a dictionary constructed from
the training corpus.
</bodyText>
<figure confidence="0.966599916666667">
97
96.5
96
95.5
F-measure
95
94.5
94
93.5
93
0 1000 2000 3000 4000 5000 6000 7000 8000
Number of Sentences
</figure>
<figureCaption confidence="0.984401">
Figure 1: Relation between accuracy and the number of training sentences.
</figureCaption>
<bodyText confidence="0.947321514285714">
&amp;quot;training&amp;quot;
&amp;quot;testing&amp;quot;
analysis without the complete feature set. Al-
most all of the feature sets improved accuracy.
The contribution of the dictionary information
was especially significant.
There were cases, however, in which the use
of dictionary information led to a decrease in
the accuracy. For example, we found these er-
roneous segmentations:
&amp;quot; (umi, sea) (ni, case marker)
(kaketa, bet) (romanha, the Ro-
mantic school) &amp;quot; and &amp;quot; (aranami, rag-
ing waves) (ni, case marker) (make,
lose) (naishin, one&apos;s inmost heart)
(to, case marker) &amp;quot; (Underlined strings
were errors.) when the correct segmentations
were:
&amp;quot; (umi, sea) (ni, case marker)
(kaketa, bet) (roman, romance)
(wa, topic marker) &amp;quot; and &amp;quot; (aranami,
raging waves) (ni, case marker)
(makenai, not to lose) (kokoro, heart)
(to, case marker) &amp;quot; (&amp;quot; &amp;quot; indicates a morpho-
logical boundary.).
These errors were caused by nonstandard en-
tries in the JUMAN dictionary. The dictio-
nary had not only the usual notation using kanji
characters, &amp;quot; &amp;quot; and &amp;quot; ,&amp;quot; but also the
uncommon notation using hiragana strings, &amp;quot;
&amp;quot; and &amp;quot; &amp;quot;. To prevent this type of
error, it is necessary to remove nonstandard en-
tries from the dictionary or to investigate the
frequency of such entries in large corpora and
to use it as a feature.
</bodyText>
<subsectionHeader confidence="0.988635">
3.4 Accuracy and the Amount of
Training Data
</subsectionHeader>
<bodyText confidence="0.999976833333333">
The accuracies (F-measures) for the training
corpus and the test corpus are shown in Figure 1
plotted against the number of sentences used
for training. The learning curve shows that we
can expect improvement if we use more training
data.
</bodyText>
<subsectionHeader confidence="0.996397">
3.5 Unknown Words and Accuracy
</subsectionHeader>
<bodyText confidence="0.9124938">
The strength of our method is that it can iden-
tify morphemes when they are unknown words
and can assign appropriate parts-of-speech to
them. For example, the nouns &amp;quot; (Souseki)&amp;quot;
and &amp;quot; (Rohan)&amp;quot; are not found in the JU-
</bodyText>
<tableCaption confidence="0.988564">
Table 3: Accuracy for unknown words (Recall).
</tableCaption>
<bodyText confidence="0.973487083333334">
Segmentation and Segmentation and
major POS tagging minor POS tagging
For words not found in the dictionary
nor in our training corpus
Our method 69.90% (432/618) 27.51% (170/618)
JUMAN+KNP 79.29% (490/618) 20.55% (127/618)
For words not found in the dictionary
nor in our features
Our method 76.17% (719/944) 32.20% (304/944)
JUMAN+KNP 85.70% (809/944) 27.22% (257/944)
For words not found in the dictionary
Our method 82.40% (1,1381,381) 49.24% (6801,381)
JUMAN+KNP 89.79% (1,2401,381) 38.60% (5331,381)
MAN dictionary. JUMAN plus KNP analyzes
them simply as &amp;quot; (Noun) (Noun)&amp;quot; and &amp;quot;
(Adverb) (Noun),&amp;quot; whereas our system ana-
lyzes both of them correctly. Our system cor-
rectly identified them as names of people even
though they were not in the dictionary and did
not appear as features in our M.E. model. Since
these names, or proper nouns, are newly coined
and can be represented by a variety of expres-
sions, no proper nouns can be included in a dic-
tionary, nor can they appear in a training cor-
pus; this means that proper nouns could easily
be unknown words. We investigated the accu-
racy of our method in identifying morphemes
when they are unknown words, and the re-
sults are listed in Table 3. The first row in
each section shows the recall for the morphemes
that were unknown words. The second row in
each section shows the percentage of morphemes
whose segmentation and &amp;quot;minor&amp;quot; POS tag were
identified correctly. The difference between the
first and second lines, the third and fourth lines,
and fifth and sixth lines is the definition of un-
known words. Unknown words were defined re-
spectively as words not found in the dictionary
nor in our training corpus, as words not found
in the dictionary nor in our features, and as
words not found in the dictionary. Our accu-
racy, shown as the second rows in Table 3 was
more than 5% better than that of JUMAN plus
KNP for each definition. These results show
that our model can efficiently learn the char-
acteristics of unknown words, especially those
of proper nouns such as the names of people,
organizations, and locations.
</bodyText>
<sectionHeader confidence="0.997938" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999984238636364">
Several methods based on statistical models
have been proposed for the morphological anal-
ysis of Japanese sentences. An F-measure of
about 96% was achieved by a method based
on a hidden Markov model (HMM) (Takeuchi
and Matsumoto, 1997) and by one based on
a variable-memory Markov model (Haruno and
Matsumoto, 1997; Kitauchi et al., 1999). Al-
though the accuracy obtained with these meth-
ods was better than that obtained with ours,
their accuracy cannot be compared directly
with that of our method because their part-
of-speech categories differ from ours. And an
advantage of our model is that it can handle
unknown words, whereas their models do not
handle unknown words well. In their models,
unknown words are divided into a combination
of a word consisting of one character and known
words. Haruno and Matsumoto (Haruno and
Matsumoto, 1997) achieved a recall of about
96% when using trigram or greater information,
but achieved a recall of only 94% when using bi-
gram information. This leads us to believe that
we could obtain better accuracy if we use tri-
gram or greater information. We plan to do so
in future work.
Two approaches have been used to deal with
unknown words: acquiring unknown words from
corpora and putting them into a dictionary
(e.g., (Mori and Nagao, 1996)) and develop-
ing a model that can identify unknown words
correctly (e.g., (Kashioka et al., 1997; Nagata,
1999)). Nagata reported a recall of about 40%
for unknown words (Nagata, 1999). As shown
in Table 3, our method achieved a recall of
69.90% for unknown words. Our accuracy was
about 30% better than his. It is difficult to
compare his method with ours directly because
he used a different corpus (the EDR corpus),
but the part-of-speech categories and the def-
inition of morphemes he used were similar to
ours. Thus, this comparison is helpful in evalu-
ating our method. There are no spaces between
morphemes in Japanese. In general, therefore,
detecting whether a given string is an unknown
word or is not a morpheme is difficult when it
is not found in the dictionary, nor in the train-
ing corpus. However, our model learns whether
or not a given string is a morpheme and has a
huge amount of data for learning what in a cor-
pus is not a morpheme. Therefore, we believe
that the characteristics of our model led to its
good results for identifying unknown words.
Mori and Nagao proposed a model that can
consult a dictionary (Mori and Nagao, 1998);
they reported an F-measure of about 92 when
using the EDR corpus and of about 95 when
using the Kyoto University corpus. Their slight
improvement in accuracy by using dictionary in-
formation resulted in an F-measure of about 0.2,
while our improvement was about 1.7. Their
accuracy of 95% when using the Kyoto Univer-
sity corpus is similar to ours, but they added
to their dictionary all of the words appearing
in the training corpus. Therefore, their exper-
iment had to deal with fewer unknown words
than ours did.
With regard to the morphological analy-
sis of English sentences, methods for part-of-
speech tagging based on an HMM (Cutting et
al., 1992), a variable-memory Markov model
(Sch�utze and Singer, 1994), a decision tree
model (Daelemans et al., 1996), an M.E. model
(Ratnaparkhi, 1996), a neural network model
(Schmid, 1994), and a transformation-based
error-driven learning model (Brill, 1995) have
been proposed, as well as a combined method
(Marquez and Padro, 1997; van Halteren et al.,
1998). On available machines, however, these
models cannot handle a large amount of lex-
ical information. We think that our model,
which can not only consult a dictionary with
a large amount of lexical information, but can
also identify unknown words by learning cer-
tain characteristics, has the potential to achieve
good accuracy for part-of-speech tagging in En-
glish. We plan to apply our model to English
sentences.
</bodyText>
<sectionHeader confidence="0.992114" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999988916666667">
This paper described a method for morpho-
logical analysis based on a maximum entropy
(M.E.) model. This method uses a model
that can not only consult a dictionary but can
also identify unknown words by learning cer-
tain characteristics. To learn these characteris-
tics, we focused on such information as whether
or not a string is found in a dictionary and
what types of characters are used in a string.
The model estimates how likely a string is to
be a morpheme according to the information
on hand. When our method was used to iden-
tify morpheme segments in sentences in the Ky-
oto University corpus and to identify the ma-
jor parts-of-speech of these morphemes, the re-
call and precision were respectively 95.80% and
95.09%. In our experiments without each fea-
ture set shown in Tables 1, we found that dic-
tionary information significantly contributes to
improving accuracy. We also found that our
model can efficiently learn the characteristics of
unknown words, especially proper nouns such
as the names of people, organizations, and lo-
cations.
</bodyText>
<sectionHeader confidence="0.978761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996638499999999">
Adam L. Berger, Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A Max-
imum Entropy Approach to Natural Lan-
guage Processing. Computational Linguis-
tics, 22(1):3971.
Eric Brill. 1995. Transformation-Based Error-
Driven Learning and Natural Language Pro-
cessing: A Case Study in Part-of-Speech Tag-
ging. Computational Linguistics, 21(4):543-
565.
Doung Cutting, Julian Kupiec, Jan Peder-
sen, and Penelope Sibun. 1992. A Practical
Part-of-Speech Tagger. In Proceedings of the
Third Conference on Applied Natural Lan-
guage Processing, pages 133{140.
Walter Daelemans, Jakub Zavrel, Peter Berck,
and Steven Gills. 1996. MBT: A Memory-
Based Part-of-Speech Tagger-Generator. In
Proceedings of the 4th Workshop on Very
Large Corpora, pages 1-14.
Masahiko Haruno and Yuji Matsumoto. 1997.
Mistake-Driven Mixture of Hierarchical-Tag
Context Trees. In Proceedings of the 35th An-
nual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 230-237.
Hideki Kashioka, Stephen G. Eubank, and
Ezra W. Black. 1997. Decision-Tree Mor-
phological Analysis without a Dictionary for
Japanese. In Proceedings of the Natural Lan-
guage Processing Pacific Rim Symposium,
pages 541-544.
Akira Kitauchi, Takehito Utsuro, and Yuji Mat-
sumoto. 1999. Probabilistic Model Learn-
ing for Japanese Morphological Analysis by
Error-driven Feature Selection. Transactions
of Information Processing Society of Japan,
40(5):2325-2337. (in Japanese).
Sadao Kurohashi and Makoto Nagao. 1997.
Building a Japanese Parsed Corpus while Im-
proving the Parsing System. In Proceedings
of the Natural Language Processing Pacific
Rim Symposium, pages 451-456.
Sadao Kurohashi and Makoto Nagao, 1999.
Japanese Morphological Analysis System JU-
MAN Version 3.61. Department of Informat-
ics, Kyoto University.
Sadao Kurohashi, 1998. Japanese Depen-
dency/Case Structure Analyzer KNP Ver-
sion 2.0b6. Department of Informatics, Ky-
oto University.
Lluis Marquez and Lluis Padro. 1997. A Flexi-
ble POS Tagger Using an Automatically Ac-
quired Language Model. In Proceedings of
the 35th Annual Meeting of the Association
for Computational Linguistics (ACL), pages
238-252.
Shinsuke Mori and Makoto Nagao. 1996.
Word Extraction from Corpora and Its Part-
of-Speech Estimation Using Distributional
Analysis. In Proceedings of the 16th Interna-
tional Conference on Computational Linguis-
tics (COLING96), pages 1119-1122.
Shinsuke Mori and Makoto Nagao. 1998. An
Improvement of a Morphological Analysis by
a Morpheme Clustering. Journal of Nat-
ural Language Processing, 5(2):75-103. (in
Japanese).
Masaaki Nagata. 1994. A Stochastic Japanese
Morphological Analyzer Using a Forward-DP
Backward-A* N-Best Search Algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics (COL-
ING94), pages 201-207.
Masaaki Nagata. 1999. A Part of Speech Esti-
mation Method for Japanese Unknown Words
using a Statistical Model of Morphology and
Context. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 277-284.
Adwait Ratnaparkhi. 1996. A Maximum En-
tropy Model for Part-Of-Speech Tagging. In
Conference on Empirical Methods in Natural
Language Processing, pages 133-142.
Helmut Schmid. 1994. Part-Of-Speech Tagging
with Neural Networks. In Proceedings of the
15th International Conference on Computa-
tional Linguistics (COLING94), pages 172-
176.
Hinrich Schutze and Yoram Singer. 1994. Part-
of-Speech Tagging Using a Variable Memory
Markov Model. In Proceedings of the 32nd
Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 181-187.
Koichi Takeuchi and Yuji Matsumoto. 1997.
HMM Parameter Learning for Japanese
Morphological Analyzer. Transactions of
Information Processing Society of Japan,
83(3):500-509. (in Japanese).
Kiyotaka Uchimoto, Satoshi Sekine, and Hi-
toshi Isahara. 1999. Japanese Dependency
Structure Analysis Based on Maximum En-
tropy Models. In Proceedings of the Ninth
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL&apos;99), pages 196-203.
Hans van Halteren, Jakub Zavrel, and Walter
Daelemans. 1998. Improving Data Driven
Wordclass Tagging by System Combination.
In Proceedings of the COLING-ACL &apos;98,
pages 491-497.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.869561">
<title confidence="0.985521">The Unknown Word Problem: a Morphological Analysis of Japanese Using Maximum Entropy Aided by a Dictionary</title>
<author confidence="0.970041">Satoshi</author>
<affiliation confidence="0.999979">Research Laboratory York University</affiliation>
<address confidence="0.9939525">2-2-2, Hikari-dai, Seika-cho, Soraku-gun, 715 Broadway, 7th floor Kyoto, 619-0289 Japan New York, NY 10003, USA</address>
<email confidence="0.972805">Cuchimoto,isahara]@crl.go.jpsekine@cs.nyu.edu</email>
<abstract confidence="0.994497777777778">In this paper we describe a morphological analysis method based on a maximum entropy model. This method uses a model that can not only consult a dictionary with a large amount of lexical information but can also identify unknown words by learning certain characteristics. The model has the potential to overcome the unknown word problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="5356" citStr="Berger et al., 1996" startWordPosition="931" endWordPosition="934">) = true, x = &amp;quot;POS(-1)(Major) : verb,�� (1) &amp; f = 1 0 : otherwise: Here &amp;quot;has(h,x)&amp;quot; is a binary function that returns true if the history h has feature x. In our experiments, we focused on such information as whether or not a string is found in a dictionary, the length of the string, what types of characters are used in the string, and the part-of-speech of the adjacent morpheme. Given a set of features and some training data, the M.E. estimation process produces a model in which every feature gi has an associated parameter ai. This enables us to compute the conditional probability as follows (Berger et al., 1996): Qi agi(h&gt;f) P(f jh) = Zai(h)(2) gi(h�f) a� (3) i The M.E. estimation process guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words, X P (h, f) - gi(h, f) h,f = X P(h) � X PM.E.(fjh) -gi(h,f)• (4) h f Here P is an empirical probability and PM.E. is the probability assigned by the model. We define part-of-speech and bunsetsu boundaries as grammatical attributes. Here a bunsetsu is a phrasal unit consisting of one or more morphemes. When there are m types of parts-of-speech, an</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):3971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="26490" citStr="Brill, 1995" startWordPosition="4494" endWordPosition="4495">hen using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in English. We plan to apply our model to English sentences. 5 Conclusion This paper described a method for morphological analysis based on a maxi</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-Based ErrorDriven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doung Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A Practical Part-of-Speech Tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>133--140</pages>
<contexts>
<context position="26238" citStr="Cutting et al., 1992" startWordPosition="4457" endWordPosition="4460">f about 92 when using the EDR corpus and of about 95 when using the Kyoto University corpus. Their slight improvement in accuracy by using dictionary information resulted in an F-measure of about 0.2, while our improvement was about 1.7. Their accuracy of 95% when using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words b</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doung Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A Practical Part-of-Speech Tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 133{140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gills</author>
</authors>
<title>MBT: A MemoryBased Part-of-Speech Tagger-Generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th Workshop on Very Large Corpora,</booktitle>
<pages>1--14</pages>
<contexts>
<context position="26346" citStr="Daelemans et al., 1996" startWordPosition="4473" endWordPosition="4476">improvement in accuracy by using dictionary information resulted in an F-measure of about 0.2, while our improvement was about 1.7. Their accuracy of 95% when using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gills, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gills. 1996. MBT: A MemoryBased Part-of-Speech Tagger-Generator. In Proceedings of the 4th Workshop on Very Large Corpora, pages 1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiko Haruno</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mistake-Driven Mixture of Hierarchical-Tag Context Trees.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>230--237</pages>
<contexts>
<context position="23472" citStr="Haruno and Matsumoto, 1997" startWordPosition="3977" endWordPosition="3980">, shown as the second rows in Table 3 was more than 5% better than that of JUMAN plus KNP for each definition. These results show that our model can efficiently learn the characteristics of unknown words, especially those of proper nouns such as the names of people, organizations, and locations. 4 Related Work Several methods based on statistical models have been proposed for the morphological analysis of Japanese sentences. An F-measure of about 96% was achieved by a method based on a hidden Markov model (HMM) (Takeuchi and Matsumoto, 1997) and by one based on a variable-memory Markov model (Haruno and Matsumoto, 1997; Kitauchi et al., 1999). Although the accuracy obtained with these methods was better than that obtained with ours, their accuracy cannot be compared directly with that of our method because their partof-speech categories differ from ours. And an advantage of our model is that it can handle unknown words, whereas their models do not handle unknown words well. In their models, unknown words are divided into a combination of a word consisting of one character and known words. Haruno and Matsumoto (Haruno and Matsumoto, 1997) achieved a recall of about 96% when using trigram or greater informati</context>
</contexts>
<marker>Haruno, Matsumoto, 1997</marker>
<rawString>Masahiko Haruno and Yuji Matsumoto. 1997. Mistake-Driven Mixture of Hierarchical-Tag Context Trees. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 230-237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kashioka</author>
<author>Stephen G Eubank</author>
<author>Ezra W Black</author>
</authors>
<title>Decision-Tree Morphological Analysis without a Dictionary for Japanese.</title>
<date>1997</date>
<booktitle>In Proceedings of the Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>541--544</pages>
<contexts>
<context position="1547" citStr="Kashioka et al., 1997" startWordPosition="242" endWordPosition="245">sis is the process segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as a partof-speech (POS) and an inflection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two statistical approaches to this problem. One is to acquire unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of letters or characters is a morpheme is augmented when the string is found in a dictionary. The improvement of </context>
<context position="24533" citStr="Kashioka et al., 1997" startWordPosition="4158" endWordPosition="4161">sting of one character and known words. Haruno and Matsumoto (Haruno and Matsumoto, 1997) achieved a recall of about 96% when using trigram or greater information, but achieved a recall of only 94% when using bigram information. This leads us to believe that we could obtain better accuracy if we use trigram or greater information. We plan to do so in future work. Two approaches have been used to deal with unknown words: acquiring unknown words from corpora and putting them into a dictionary (e.g., (Mori and Nagao, 1996)) and developing a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Nagata reported a recall of about 40% for unknown words (Nagata, 1999). As shown in Table 3, our method achieved a recall of 69.90% for unknown words. Our accuracy was about 30% better than his. It is difficult to compare his method with ours directly because he used a different corpus (the EDR corpus), but the part-of-speech categories and the definition of morphemes he used were similar to ours. Thus, this comparison is helpful in evaluating our method. There are no spaces between morphemes in Japanese. In general, therefore, detecting whether a given string is an unknown w</context>
</contexts>
<marker>Kashioka, Eubank, Black, 1997</marker>
<rawString>Hideki Kashioka, Stephen G. Eubank, and Ezra W. Black. 1997. Decision-Tree Morphological Analysis without a Dictionary for Japanese. In Proceedings of the Natural Language Processing Pacific Rim Symposium, pages 541-544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akira Kitauchi</author>
<author>Takehito Utsuro</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Probabilistic Model Learning for Japanese Morphological Analysis by Error-driven Feature Selection.</title>
<date>1999</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>40--5</pages>
<note>(in Japanese).</note>
<contexts>
<context position="23496" citStr="Kitauchi et al., 1999" startWordPosition="3981" endWordPosition="3984">n Table 3 was more than 5% better than that of JUMAN plus KNP for each definition. These results show that our model can efficiently learn the characteristics of unknown words, especially those of proper nouns such as the names of people, organizations, and locations. 4 Related Work Several methods based on statistical models have been proposed for the morphological analysis of Japanese sentences. An F-measure of about 96% was achieved by a method based on a hidden Markov model (HMM) (Takeuchi and Matsumoto, 1997) and by one based on a variable-memory Markov model (Haruno and Matsumoto, 1997; Kitauchi et al., 1999). Although the accuracy obtained with these methods was better than that obtained with ours, their accuracy cannot be compared directly with that of our method because their partof-speech categories differ from ours. And an advantage of our model is that it can handle unknown words, whereas their models do not handle unknown words well. In their models, unknown words are divided into a combination of a word consisting of one character and known words. Haruno and Matsumoto (Haruno and Matsumoto, 1997) achieved a recall of about 96% when using trigram or greater information, but achieved a recal</context>
</contexts>
<marker>Kitauchi, Utsuro, Matsumoto, 1999</marker>
<rawString>Akira Kitauchi, Takehito Utsuro, and Yuji Matsumoto. 1999. Probabilistic Model Learning for Japanese Morphological Analysis by Error-driven Feature Selection. Transactions of Information Processing Society of Japan, 40(5):2325-2337. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Building a Japanese Parsed Corpus while Improving the Parsing System.</title>
<date>1997</date>
<booktitle>In Proceedings of the Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>451--456</pages>
<contexts>
<context position="7734" citStr="Kurohashi and Nagao, 1997" startWordPosition="1361" endWordPosition="1364">grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary. We do not identify inflection types probabilistically since &apos;Not only morphemes but also bunsetsus can be identified by considering the information related to their bunsetsu boundaries. &lt;&gt;8 &gt;: g(h, f) = Za(h) = X Y f i they can be almost perfectly identified by checking the spelling of the current morpheme after a part-of-speech has been assigned to it. Therefore, f in Eq. (2) can be one of 107 tags from 0 to 106. We used the Kyoto University text corpus (Version 2) (Kurohashi and Nagao, 1997), a tagged corpus of the Mainichi newspaper. For training, we used 7,958 sentences from newspaper articles appearing from January 1 to January 8, 1995, and for testing, we used 1,246 sentences from articles appearing on January 9, 1995. Given a sentence, for every string consisting of five or less characters and every string appearing in the JUMAN dictionary (Kurohashi and Nagao, 1999), whether or not the string is a morpheme was determined and then the grammatical attribute of each string determined to be a morpheme was identified and assigned to that string. The maximum length was set at fiv</context>
</contexts>
<marker>Kurohashi, Nagao, 1997</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1997. Building a Japanese Parsed Corpus while Improving the Parsing System. In Proceedings of the Natural Language Processing Pacific Rim Symposium, pages 451-456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1999</date>
<booktitle>Japanese Morphological Analysis System JUMAN Version 3.61.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="6982" citStr="Kurohashi and Nagao, 1999" startWordPosition="1225" endWordPosition="1228">. A given sentence is divided into morphemes, and a grammatical attribute is assigned to each morpheme so as to maximize the sentence probability estimated by our morpheme model. Sentence probability is defined as the product of the probabilities estimated for a particular division of morphemes in a sentence. We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the Nbest sets. 3 Experiments and Discussion 3.1 Experimental Conditions The part-of-speech categories that we used follow those of JUMAN (Kurohashi and Nagao, 1999). There are 53 categories covering all possible combinations of major and minor categories as defined in JUMAN. The number of grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary. We do not identify inflection types probabilistically since &apos;Not only morphemes but also bunsetsus can be identified by considering the information related to their bunsetsu boundaries. &lt;&gt;8 &gt;: g(h, f) = Za(h) = X Y f i they can be almost perfectly identified by checking the spelling of the current morpheme after a part-of-speech has been assign</context>
<context position="10910" citStr="Kurohashi and Nagao, 1999" startWordPosition="1906" endWordPosition="1910">at were found three or more times in the training corpus. The notations &amp;quot;(0)&amp;quot; and &amp;quot;(-1)&amp;quot; used in the feature type column in Table 1 respectively indicate a target string and the morpheme on the left of it. The terms used in the table are the following: String: Strings which appeared as a morpheme five or more times in the training corpus Length: Length of a string POS: Part-of-speech. &amp;quot;Major&amp;quot; and &amp;quot;Minor&amp;quot; respectively indicate major and minor partof-speech categories as defined in JUMAN. Inf: Inflection type as defined in JUMAN Dic: We use the JUMAN dictionary, which has about 200,000 entries (Kurohashi and Nagao, 1999). &amp;quot;Major&amp;Minor&amp;quot; indicates possible combinations between major and minor part-of-speech categories. When the target string is in the dictionary, the partof-speech attached to the entry corresponding to the string is used as a feature value. If an entry has two or more parts-of-speech, the part-of-speech which leads to the highest probability in a sentence estimated from our model is selected as a feature value. JUMAN has another type of dictionary, which is called a phrase dictionary. Each entry in the phrase dictionary consists of one or more morphemes such as &amp;quot; (to, case marker), (wa, topic m</context>
</contexts>
<marker>Kurohashi, Nagao, 1999</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1999. Japanese Morphological Analysis System JUMAN Version 3.61. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
</authors>
<date>1998</date>
<booktitle>Japanese Dependency/Case Structure Analyzer KNP Version 2.0b6.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<contexts>
<context position="14829" citStr="Kurohashi, 1998" startWordPosition="2527" endWordPosition="2528"> Indicates whether or not the left side of a morpheme is a bunsetsu boundary. 3.2 Results and Discussion Some results of the morphological analysis are listed in Table 2. Recall is the percentage of morphemes in the test corpus whose segmentation and major POS tag are identified correctly. Precision is the percentage of all morphemes identified by the system that are identified correctly. F represents the F-measure and is defined by the following equation. 2 x Recall x Precision Recall + Precision Table 2 shows results obtained by using our method, by using JUMAN, and by using JUMAN plus KNP (Kurohashi, 1998). We show the result obtained using JUMAN plus KNP because JUMAN alone assigns an &amp;quot;Unknown&amp;quot; tag to katakana strings when they are not in the dictionary. All katakana strings not found F — measure = Table 2: Results of Experiments (Segmentation and major POS tagging). Recall Precision F-measure Our method 95.80% (29,986/31,302) 95.09% (29,986/31,467) 95.44 JUMAN 95.25% (29,814/31,302) 94.90% (29,814/31,417) 95.07 JUMAN+KNP 98.49% (30,830/31,302) 98.13% (30,830/31,417) 98.31 in the dictionary are therefore evaluated as errors. KNP improves on JUMAN by replacing the &amp;quot;Unknown&amp;quot; tag with a &amp;quot;Noun&amp;quot; ta</context>
</contexts>
<marker>Kurohashi, 1998</marker>
<rawString>Sadao Kurohashi, 1998. Japanese Dependency/Case Structure Analyzer KNP Version 2.0b6. Department of Informatics, Kyoto University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluis Marquez</author>
<author>Lluis Padro</author>
</authors>
<title>A Flexible POS Tagger Using an Automatically Acquired Language Model.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>238--252</pages>
<contexts>
<context position="26564" citStr="Marquez and Padro, 1997" startWordPosition="4505" endWordPosition="4508">hey added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in English. We plan to apply our model to English sentences. 5 Conclusion This paper described a method for morphological analysis based on a maximum entropy (M.E.) model. This method uses a model that can not only consu</context>
</contexts>
<marker>Marquez, Padro, 1997</marker>
<rawString>Lluis Marquez and Lluis Padro. 1997. A Flexible POS Tagger Using an Automatically Acquired Language Model. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL), pages 238-252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Word Extraction from Corpora and Its Partof-Speech Estimation Using Distributional Analysis.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING96),</booktitle>
<pages>1119--1122</pages>
<contexts>
<context position="1436" citStr="Mori and Nagao, 1996" startWordPosition="223" endWordPosition="226">entence analysis. A morpheme is a minimal grammatical unit, such as a word or a suffix, and morphological analysis is the process segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as a partof-speech (POS) and an inflection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two statistical approaches to this problem. One is to acquire unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of</context>
<context position="24437" citStr="Mori and Nagao, 1996" startWordPosition="4142" endWordPosition="4145">nknown words well. In their models, unknown words are divided into a combination of a word consisting of one character and known words. Haruno and Matsumoto (Haruno and Matsumoto, 1997) achieved a recall of about 96% when using trigram or greater information, but achieved a recall of only 94% when using bigram information. This leads us to believe that we could obtain better accuracy if we use trigram or greater information. We plan to do so in future work. Two approaches have been used to deal with unknown words: acquiring unknown words from corpora and putting them into a dictionary (e.g., (Mori and Nagao, 1996)) and developing a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Nagata reported a recall of about 40% for unknown words (Nagata, 1999). As shown in Table 3, our method achieved a recall of 69.90% for unknown words. Our accuracy was about 30% better than his. It is difficult to compare his method with ours directly because he used a different corpus (the EDR corpus), but the part-of-speech categories and the definition of morphemes he used were similar to ours. Thus, this comparison is helpful in evaluating our method. There are no spaces betwee</context>
</contexts>
<marker>Mori, Nagao, 1996</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1996. Word Extraction from Corpora and Its Partof-Speech Estimation Using Distributional Analysis. In Proceedings of the 16th International Conference on Computational Linguistics (COLING96), pages 1119-1122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>An Improvement of a Morphological Analysis by a Morpheme Clustering.</title>
<date>1998</date>
<journal>Journal of Natural Language Processing,</journal>
<pages>5--2</pages>
<note>(in Japanese).</note>
<contexts>
<context position="1987" citStr="Mori and Nagao, 1998" startWordPosition="319" endWordPosition="322">m corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of letters or characters is a morpheme is augmented when the string is found in a dictionary. The improvement of the accuracy was slight, however, so we think that it is difficult to efficiently integrate the mechanism for consulting a dictionary into an n-gram model. In this paper we therefore describe a morphological analysis method based on a maximum entropy (M.E.) model. This method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we foc</context>
<context position="25587" citStr="Mori and Nagao, 1998" startWordPosition="4345" endWordPosition="4348">parison is helpful in evaluating our method. There are no spaces between morphemes in Japanese. In general, therefore, detecting whether a given string is an unknown word or is not a morpheme is difficult when it is not found in the dictionary, nor in the training corpus. However, our model learns whether or not a given string is a morpheme and has a huge amount of data for learning what in a corpus is not a morpheme. Therefore, we believe that the characteristics of our model led to its good results for identifying unknown words. Mori and Nagao proposed a model that can consult a dictionary (Mori and Nagao, 1998); they reported an F-measure of about 92 when using the EDR corpus and of about 95 when using the Kyoto University corpus. Their slight improvement in accuracy by using dictionary information resulted in an F-measure of about 0.2, while our improvement was about 1.7. Their accuracy of 95% when using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofsp</context>
</contexts>
<marker>Mori, Nagao, 1998</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1998. An Improvement of a Morphological Analysis by a Morpheme Clustering. Journal of Natural Language Processing, 5(2):75-103. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING94),</booktitle>
<pages>201--207</pages>
<contexts>
<context position="6802" citStr="Nagata, 1994" startWordPosition="1198" endWordPosition="1199">me and has the grammatical attribute i(1 &lt; i &lt; n). We call it a morpheme model. This model is represented by Eq. (2), in which f can be one of (n + 1) tags from 0 to n. A given sentence is divided into morphemes, and a grammatical attribute is assigned to each morpheme so as to maximize the sentence probability estimated by our morpheme model. Sentence probability is defined as the product of the probabilities estimated for a particular division of morphemes in a sentence. We use the Viterbi algorithm to find the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the Nbest sets. 3 Experiments and Discussion 3.1 Experimental Conditions The part-of-speech categories that we used follow those of JUMAN (Kurohashi and Nagao, 1999). There are 53 categories covering all possible combinations of major and minor categories as defined in JUMAN. The number of grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary. We do not identify inflection types probabilistically since &apos;Not only morphemes but also bunsetsus can be identified by considering the information related to their b</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A Stochastic Japanese Morphological Analyzer Using a Forward-DP Backward-A* N-Best Search Algorithm. In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A Part of Speech Estimation Method for Japanese Unknown Words using a Statistical Model of Morphology and Context.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>277--284</pages>
<contexts>
<context position="1562" citStr="Nagata, 1999" startWordPosition="246" endWordPosition="247">enting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as a partof-speech (POS) and an inflection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two statistical approaches to this problem. One is to acquire unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of letters or characters is a morpheme is augmented when the string is found in a dictionary. The improvement of the accuracy wa</context>
<context position="24548" citStr="Nagata, 1999" startWordPosition="4162" endWordPosition="4163">and known words. Haruno and Matsumoto (Haruno and Matsumoto, 1997) achieved a recall of about 96% when using trigram or greater information, but achieved a recall of only 94% when using bigram information. This leads us to believe that we could obtain better accuracy if we use trigram or greater information. We plan to do so in future work. Two approaches have been used to deal with unknown words: acquiring unknown words from corpora and putting them into a dictionary (e.g., (Mori and Nagao, 1996)) and developing a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Nagata reported a recall of about 40% for unknown words (Nagata, 1999). As shown in Table 3, our method achieved a recall of 69.90% for unknown words. Our accuracy was about 30% better than his. It is difficult to compare his method with ours directly because he used a different corpus (the EDR corpus), but the part-of-speech categories and the definition of morphemes he used were similar to ours. Thus, this comparison is helpful in evaluating our method. There are no spaces between morphemes in Japanese. In general, therefore, detecting whether a given string is an unknown word or is not a</context>
</contexts>
<marker>Nagata, 1999</marker>
<rawString>Masaaki Nagata. 1999. A Part of Speech Estimation Method for Japanese Unknown Words using a Statistical Model of Morphology and Context. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 277-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="26381" citStr="Ratnaparkhi, 1996" startWordPosition="4480" endWordPosition="4481">ary information resulted in an F-measure of about 0.2, while our improvement was about 1.7. Their accuracy of 95% when using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in English. We plan to apply our mode</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Part-Of-Speech Tagging with Neural Networks.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING94),</booktitle>
<pages>172--176</pages>
<contexts>
<context position="26420" citStr="Schmid, 1994" startWordPosition="4486" endWordPosition="4487">about 0.2, while our improvement was about 1.7. Their accuracy of 95% when using the Kyoto University corpus is similar to ours, but they added to their dictionary all of the words appearing in the training corpus. Therefore, their experiment had to deal with fewer unknown words than ours did. With regard to the morphological analysis of English sentences, methods for part-ofspeech tagging based on an HMM (Cutting et al., 1992), a variable-memory Markov model (Sch�utze and Singer, 1994), a decision tree model (Daelemans et al., 1996), an M.E. model (Ratnaparkhi, 1996), a neural network model (Schmid, 1994), and a transformation-based error-driven learning model (Brill, 1995) have been proposed, as well as a combined method (Marquez and Padro, 1997; van Halteren et al., 1998). On available machines, however, these models cannot handle a large amount of lexical information. We think that our model, which can not only consult a dictionary with a large amount of lexical information, but can also identify unknown words by learning certain characteristics, has the potential to achieve good accuracy for part-of-speech tagging in English. We plan to apply our model to English sentences. 5 Conclusion Th</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Part-Of-Speech Tagging with Neural Networks. In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), pages 172-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
<author>Yoram Singer</author>
</authors>
<title>Partof-Speech Tagging Using a Variable Memory Markov Model.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>181--187</pages>
<marker>Schutze, Singer, 1994</marker>
<rawString>Hinrich Schutze and Yoram Singer. 1994. Partof-Speech Tagging Using a Variable Memory Markov Model. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 181-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeuchi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>HMM Parameter Learning for Japanese Morphological Analyzer.</title>
<date>1997</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>83--3</pages>
<note>(in Japanese).</note>
<contexts>
<context position="23393" citStr="Takeuchi and Matsumoto, 1997" startWordPosition="3964" endWordPosition="3967">ionary nor in our features, and as words not found in the dictionary. Our accuracy, shown as the second rows in Table 3 was more than 5% better than that of JUMAN plus KNP for each definition. These results show that our model can efficiently learn the characteristics of unknown words, especially those of proper nouns such as the names of people, organizations, and locations. 4 Related Work Several methods based on statistical models have been proposed for the morphological analysis of Japanese sentences. An F-measure of about 96% was achieved by a method based on a hidden Markov model (HMM) (Takeuchi and Matsumoto, 1997) and by one based on a variable-memory Markov model (Haruno and Matsumoto, 1997; Kitauchi et al., 1999). Although the accuracy obtained with these methods was better than that obtained with ours, their accuracy cannot be compared directly with that of our method because their partof-speech categories differ from ours. And an advantage of our model is that it can handle unknown words, whereas their models do not handle unknown words well. In their models, unknown words are divided into a combination of a word consisting of one character and known words. Haruno and Matsumoto (Haruno and Matsumot</context>
</contexts>
<marker>Takeuchi, Matsumoto, 1997</marker>
<rawString>Koichi Takeuchi and Yuji Matsumoto. 1997. HMM Parameter Learning for Japanese Morphological Analyzer. Transactions of Information Processing Society of Japan, 83(3):500-509. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese Dependency Structure Analysis Based on Maximum Entropy Models.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;99),</booktitle>
<pages>196--203</pages>
<contexts>
<context position="16042" citStr="Uchimoto et al., 1999" startWordPosition="2721" endWordPosition="2724">&amp;quot;Noun&amp;quot; tag and disambiguating part-of-speech ambiguities which arise during the process of parsing when there is more than one JUMAN analysis with the same score. The accuracy in segmentation and major POS tagging obtained with our method and that obtained with JUMAN were about 3% worse than that obtained with JUMAN plus KNP. We think the main reason for this was an insufficient amount of training data and feature sets and the inconsistency of the corpus. The number of sentences in the training corpus was only about 8,000, and we did not use as many combined features as were proposed in Ref. (Uchimoto et al., 1999). We were unable to use more training data or more feature sets because every string consisting of five or less characters in our training corpus was used to train our model, so the amount of tokenized training data would have become too large and the training would not have been completed on the available machine if we had used more training data or more feature sets. The inconsistency of the corpus was due to the way the corpus was made. The Kyoto University corpus was made by manually correcting the output of JUMAN plus KNP, and it is difficult to manually correct all of the inconsistencies</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese Dependency Structure Analysis Based on Maximum Entropy Models. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;99), pages 196-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving Data Driven Wordclass Tagging by System Combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL &apos;98,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Hans van Halteren, Jakub Zavrel, and Walter Daelemans. 1998. Improving Data Driven Wordclass Tagging by System Combination. In Proceedings of the COLING-ACL &apos;98, pages 491-497.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>