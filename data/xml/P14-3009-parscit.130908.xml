<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000167">
<title confidence="0.982041">
Multi-Document Summarization Using Distortion-Rate Ratio
</title>
<author confidence="0.991893">
Ulukbek Attokurov
</author>
<affiliation confidence="0.9948315">
Department of Computer Engineering
Istanbul Technical University
</affiliation>
<email confidence="0.977822">
attokurov@itu.edu.tr
</email>
<author confidence="0.991298">
Ulug Bayazit
</author>
<affiliation confidence="0.9949215">
Department of Computer Engineering
Istanbul Technical University
</affiliation>
<email confidence="0.988175">
ulugbayazit@itu.edu.tr
</email>
<sectionHeader confidence="0.993635" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959038461539">
The current work adapts the optimal tree
pruning algorithm(BFOS) introduced by
Breiman et al.(1984) and extended by
Chou et al.(1989) to the multi-document
summarization task. BFOS algorithm is
used to eliminate redundancy which is one
of the main issues in multi-document sum-
marization. Hierarchical Agglomerative
Clustering algorithm(HAC) is employed
to detect the redundancy. The tree de-
signed by HAC algorithm is successively
pruned with the optimal tree pruning al-
gorithm to optimize the distortion vs. rate
cost of the resultant tree. Rate parameter is
defined to be the number of the sentences
in the leaves of the tree. Distortion is the
sum of the distances between the represen-
tative sentence of the cluster at each node
and the other sentences in the same clus-
ter. The sentences assigned to the leaves of
the resultant tree are included in the sum-
mary. The performance of the proposed
system assessed with the Rouge-1 metric
is seen to be better than the performance
of the DUC-2002 winners on DUC-2002
data set.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960094339623">
Nowadays, the massive amount of information
available in the form of digital media over the in-
ternet makes us seek effective ways of accessing
this information. Textual documents, audio and
video materials are uploaded every second. For in-
stance, the number of Google’s indexed web pages
has exceeded 30 billion web pages in the last two
years. Extraction of the needed information from
a massive information pool is a challenging task.
The task of skimming all the documents in their
entirety before deciding which information is rel-
evant is very time consuming.
One of the well known and extensively studied
methods for solving this problem is summariza-
tion. Text summarization produces a short ver-
sion of a document that covers the main topics in
it (Mani and Hahn, 2000). It enables the reader
to determine in a timely manner whether a given
document satisfies his/her needs or not.
A single document summarization system pro-
duces a summary of only one document whereas
a multi-document summarization system produces
a summary based on multiple documents on the
same topic. Summarization systems can also be
categorized as generic or query-based . A generic
summary contains general information about par-
ticular documents. It includes any information
supposed to be important and somehow linked to
the topics of the document set. In contrast, a query
based summary comprises information relevant to
the given query. In this case, query is a rule ac-
cording to which a summary is to be generated.
Summarization systems can be also classified
as extractive or abstractive. In extractive systems,
a summary is created by selecting important sen-
tences from a document. Here, only sentences
containing information related to the main topics
of the document are considered to be important.
These sentences are added to the summary with-
out any modification. On the other hand, abstrac-
tive systems can modify the existing sentences or
even generate new sentences to be included in the
summary. Therefore, abstractive summarization
is typically more complex than extractive summa-
rization.
The main goal in multi-document summariza-
tion is redundancy elimination. Since the docu-
ments are related to the same topics, similar text
units(passages, sentences etc.) are encountered
frequently in different documents. Such text units
that indicate the importance of the topics discussed
within them should be detected in order to re-
duce the redundancy. Some of the well-known ap-
</bodyText>
<page confidence="0.991527">
64
</page>
<subsubsectionHeader confidence="0.27199">
Proceedings of the ACL 2014 Student Research Workshop, pages 64–70,
</subsubsectionHeader>
<bodyText confidence="0.986351459459459">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
proaches that address this problem are briefly ex-
plained in the following section.
Although much work has been done to elim-
inate the redundancy in multi-document summa-
rization, the problem is still actual and addressed
in the current work as well. The current work
proposes to integrate the generalized BFOS algo-
rithm (Breiman et al., 1984) adopted by Chou et.al
(1989) for pruned tree structured quantizer design
with the HAC (Hierarchical Agglomerative Clus-
tering) algorithm. The two main parameters (dis-
tortion and rate) in the latter work are adopted to
the multi-document summarization task. Distor-
tion can be succinctly defined as the information
loss in the meaning of the sentences due to their
representation with other sentences. More specif-
ically, in the current context, distortion contribu-
tion of a cluster is taken to be the sum of the dis-
tances between the vector representations of the
sentences in the cluster and representative sen-
tence of that cluster. Rate of a summary is de-
fined to be the number of sentences in the sum-
mary, but more precise definitions involving word
or character counts are also possible. BFOS based
tree pruning algorithm is applied to the tree built
with the HAC algorithm. HAC algorithm is used
for clustering purposes since BFOS algorithm gets
tree structured data as an input. It is found that
the suggested approach yields better results in
terms of the ROUGE-1 Recall measure (Lin et
al., 2003) when compared to 400 word extractive
summaries(400E) included in DUC-2002 data set.
Also, the results with the proposed method are
higher than the ones obtained with the best sys-
tems of DUC-2002 in terms of sentence recall and
precision(Harabagiu, 2002; Halteren, 2002).
</bodyText>
<sectionHeader confidence="0.999149" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99940525">
Term frequency (Luhn, 1958), lexical chains
(Barzilay and Elhadad, 1997), location of the sen-
tences (Edmundson, 1969) and the cue phrases
(Teufel et al., 1997) are used to determine the im-
portant lexical units. Goldstein et al. (2000) pro-
posed a measure named Maximal Marginal Rel-
evance which assigns a high priority to the pas-
sages relevant to the query and has minimal sim-
ilarity to the sentences in the summary. Radev
et al. (2001) developed a system called MEAD
based on the centroid of the cluster. The words
that are most relevant to the main topics are in-
cluded in the centroid. Lin et al. designed a
statistic-based summarization system (Summarist)
which incorporated NLP(Natural Language Pro-
cessing) and IR(Information Retrieval) methods.
LSA(Latent Semantic Analysis) (Landauer et al.,
1998) has also been used extensively in recent
years for multi-document summarization. By ap-
plying SVD(Singular Value Decomposition) to the
term-document matrix, it determines the most im-
portant topics and represents the term and docu-
ments in the reduced space (Murray et al., 2005;
Steinberger and Jezek , 2004; Geiss, 2011). Rachit
Arora et al. (2008) combined LDA(Latent Dirich-
let Allocation) and SVD. In this approach, LDA is
used to detect topics and SVD is applied to select
the sentences representing these topics.
Clustering of the sentences has also been used
to determine the redundant information. In this
approach, the sentences are first clustered. The
sentences in each cluster share common informa-
tion about the main topics of the documents to be
summarized. Then a sentence is selected (Radev
et al., 2004) or generated (McKeown et al., 1999)
from each cluster that represents the sentences in
the cluster. Finally, selected sentences are added
to the summary until a predetermined length is ex-
ceeded (Aliguliyev, 2006; Hatzivassiloglou et al.,
1999; Hatzivassiloglou et al., 2001).
</bodyText>
<sectionHeader confidence="0.999029" genericHeader="method">
3 Background
</sectionHeader>
<subsectionHeader confidence="0.998877">
3.1 Generalized BFOS Algorithm
</subsectionHeader>
<bodyText confidence="0.999989714285714">
Let us assume that we have a tree T with the set of
leaves T˜. Also let us denote a sub-tree of T rooted
at any node of T as S. The leaves of the sub-trees
may happen to be the inner nodes of T. If the root
node of the sub-tree S is not identical to the root
node of T and the set of leaves S˜ is a sub-set of
T˜ then S is called a branch. But if the sub-tree S
is rooted at the root node of T then S is named a
pruned sub-tree of T. Function defined on the tree
T and on any sub-tree S is called a tree functional.
Monotonic tree functional is a class of functional
where it increases or decreases depending on the
tree size. In our case, tree size is the number of the
nodes of T.
Two main tree functionals(u1 and u2) need to
be defined in the generalized BFOS algorithm.
They are adapted to the problem under considera-
tion. In regression trees, u1 is the number of the
leaves and u2 is the mean squared distortion er-
ror. In TSVQ(Tree Structured Vector Quantiza-
tion), u1 and u2 are the length of the code and
</bodyText>
<page confidence="0.99789">
65
</page>
<bodyText confidence="0.997435454545454">
the expected distortion, respectively. In the cur-
rent context, distortion(D) and rate(R) defined in
the next section are used as the tree functionals u1
and u2.
As shown in Chou et al., the set of distortion and
rate points of the pruned sub-trees of T generate a
convex hull if distortion is an increasing and rate
is a decreasing function. Also it is stated that if
the tree Tis pruned off until the root node remains,
then it is possible to generate the sub-trees which
correspond to the vertices on the lower boundary
of the convex hull. Thus it is sufficient to consider
the sub-trees corresponding to the vertices of the
boundary to trade off between rate and distortion.
A parameter λ = −ΔD
ΔR may be used to locate
the vertices on the lower boundary of the convex
hull. AD and AR indicate the amount of distor-
tion increase and rate decrease when branch sub-
tree S is pruned off. It can be shown that a step
on the lower boundary can be taken by pruning off
at least one branch sub-tree rooted at a particular
inner node. The λ value of this sub-tree is mini-
mal among all the other branch sub-trees rooted at
various inner nodes of T, because it is a slope of
the lower boundary. At each pruning iteration, the
algorithm seeks the branch sub-tree rooted at an
inner node with the minimal lambda and prunes
it off the tree. After each pruning step, the in-
ner node at which the pruned branch sub-tree is
rooted becomes a leaf node. The pruning itera-
tions continue until the root node remains or the
pruned sub-tree meets a certain stopping criterion.
</bodyText>
<sectionHeader confidence="0.98699" genericHeader="method">
4 The Proposed Summarization System
</sectionHeader>
<bodyText confidence="0.998043757575757">
In the current work, BFOS and HAC algorithm
were incorporated to the multi-document sum-
marization system. Generalized version of the
BFOS algorithm discussed in the work of Chou
et al. (1989) with previous applications to TSVQ,
speech recognition etc. was adapted for the pur-
pose of pruning the large tree designed by the
HAC algorithm. Generalized BFOS algorithm
was preferred in the current context because it is
believed that the generated optimal trees yield the
best trade-off between the semantic distortion and
rate (the summary length in terms of number of
sentences).
The proposed system consists of the following
stages: preprocessing, redundancy detection, re-
dundancy elimination and the summary genera-
tion.
In preprocessing stage, the source documents
are represented in the vector space. Towards this
end, the sentences are parsed, stemmed and a fea-
ture set is created (terms (stems or words, n-grams
etc.) that occur in more than one document are
extracted). The sentences of the document set are
then represented by a sentence X term matrix with
n columns and m rows, where n is the number of
the sentences and m is the number of the terms in
the feature set. TF-IDF is used to determine the
values of the matrix elements. TF-IDF assigns a
value according to the importance of the terms in
the collection of the sentences. If the term t occurs
frequently in the current document but the oppo-
site is true for other documents then tf-idf value of
t is high.
</bodyText>
<equation confidence="0.830031">
TF − IDF = TF ∗ log e (1)
</equation>
<bodyText confidence="0.99938648">
where TF is the term frequency, DF is the docu-
ment frequency and N is the number of sentences.
Term frequency is the number of the occurrences
of the term in the sentence. Document frequency
is the number of the sentences in which the term is
found.
Redundancy detection is facilitated by applying
the Hierarchical Agglomerative Clustering(HAC)
algorithm. Initially, individual sentences are con-
sidered to be singletons in the HAC algorithm.
The most similar clusters are then successively
merged to form a new cluster that contains the
union of the sentences in the merged clusters. At
each step, a new (inner) node is created in the tree
as the new cluster appears and contains all the sen-
tences in the union of the merged clusters. HAC
merge operations continue until a single cluster re-
mains. The tree built after HAC operation is re-
ferred to as the HAC tree.
The third stage is the redundancy elimination.
To this end, generalized BFOS algorithm dis-
cussed previously is applied to the HAC tree. In
order to adapt the generalized BFOS algorithm to
the current context, distortion contribution of each
cluster (node) is defined as follows:
</bodyText>
<equation confidence="0.944293">
�D = d(rs, s) (2)
s∈cduster
</equation>
<bodyText confidence="0.99943575">
where d is the distance between the representative
sentence(rs) and a sentence(s) in the cluster.
By definition, the distortion contribution of each
leaf node of the HAC tree is zero.
</bodyText>
<page confidence="0.956044">
66
</page>
<bodyText confidence="0.999919621621622">
Rate is defined to be the number of sentences
in the leaves of the tree. A branch sub-tree is
removed at each pruning step of the generalized
BFOS algorithm. Correspondingly, the sentences
at the leaf nodes of the pruned branch subtree are
eliminated. As a result, the rate decreases to the
number of leaf nodes remaining after pruning.
The centroid of the cluster can be used as the
representative sentence of the cluster. Centroid
can be constituted of the important (with TF-IDF
values exceeding a threshold) words of the cluster
(Radev et al., 2004) or can be generated using Nat-
ural language processing techniques (McKeown et
al., 1999). In the current work, the simpler ap-
proach of selecting the sentence from the cluster
yielding the minimal distortion as the representa-
tive sentence is employed.
A parameter is used to determine the branch
sub-trees that are successively pruned. In each
pruning step, the branch sub-tree with minimum
A is identified to minimize the increase in total
distortion(OD) per discarded sentence(OR).
In accordance with the definition of rate given
above, OR is the change in the number of sen-
tences in the summary before and after the prun-
ing of the branch sub-tree. It also equals to the
number of pruned leaf nodes, because rate equals
to the number of the sentences stored in the leaf
nodes of the current tree. For instance, let us as-
sume that the number of sentences before pruning
is 10 and a sub-tree A is cut off. If A has 4 leaf
nodes, than 3 of them is eliminated and one is left
to represent the cluster of sentences corresponding
to the sub-tree A. Since 3 leaf nodes are removed
and each leaf node is matched to the certain sen-
tence, the current rate equals to 7. The increase in
total distortion is written as
</bodyText>
<equation confidence="0.977298">
OD = Dpost − Dprev (3)
</equation>
<bodyText confidence="0.914793304347826">
where Dprev is set equal to the sum of distortions
in the leaves of the tree before pruning and Dpost
is set equal to the sum of distortions in the leaves
of the tree after pruning.
The application of the generalized BFOS algo-
rithm to the HAC tree can be recapped as follows.
At the initial step, a representative sentence is se-
lected for each inner node and A is determined for
each inner node. At each generic pruning step, the
node with the minimum lambda value is identified,
the sub-tree rooted at that node is pruned, the root
node of the sub-tree is converted to a leaf node.
After each pruning step, the A values of the ances-
tor nodes of this new leaf node are updated. We
summarize the generalized BFOS algorithm with
a pseudocode in Algorithm 1.
Algorithm 1: PRUNING THE TREE. Prunes a
tree T created by using Hierarchical Agglom-
erative Clustering Algorithm
Input: A tree T produced by using
Hierarchical Clustering Algorithm
Output: Optimal sub-tree O obtained by
pruning T
</bodyText>
<sectionHeader confidence="0.400374" genericHeader="method">
1 For each leaf node,
</sectionHeader>
<bodyText confidence="0.937316729729729">
A ← oc,distortion(D) ← 0
2 For each inner node calculate A = ΔD
ΔR ,
where OD and OR are change in
distortion(D) and rate(R) respectively
3 rate(R) ← the number of the leaves of T
4 while the number of the nodes &gt; 1 do
find a node A with minimum A value
among the inner nodes
prune the sub-tree S rooted at the node A
convert the pruned inner node A to the
leaf node containing the representative
sentence of the sub-tree S
update the ancestor nodes of the node A:
update OD, OR and A
update rate(R)
10 return O
A summary of desired length can be created by
selecting a threshold based on rate (the number of
remaining sentences after pruning, the number of
leaf nodes of the pruned tree). Another possibil-
ity for the choice of the stopping criterion may
be based on the A parameter which monotonically
increases with pruning iterations. When a large
enough A value is reached, it may be assumed that
shortening the summary further eliminates infor-
mative sentences.
The proposed method of summarization has a
few drawbacks. The main problem is that the
pruning algorithm is highly dependent on the dis-
tortion measure. If the distortion measure is not
defined appropriately, the representative sentence
can be selected incorrectly. Another issue is the
inclusion of the irrelevant sentences into the sum-
mary. This problem may occur if the sentences
remaining after pruning operation are included in
the summary without filtering.
</bodyText>
<figure confidence="0.981043">
5
6
7
8
9
</figure>
<page confidence="0.99626">
67
</page>
<sectionHeader confidence="0.996938" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.973157978723405">
The testing of the system performed on DUC-2002
data set (Document Understanding Conference,
2002) since the proposed system is designed to
produce a generic summary without specified in-
formation need of users or predefined user profile.
This data set contains 59 document sets. For each
document set extraction based summaries with the
length 200 and 400 words are provided. Document
sets related to the single event are used for testing
purposes.
Evaluation of the system is carried out using
ROUGE package (Lin C, 2004). Rouge is a sum-
mary evaluation approach based on n-gram co-
occurrence , longest common subsequence and
skip bigram statistics (Lin et al., 2003). The per-
formance of the summarizing system is measured
with Rouge-1 Recall, Rouge-1 Precision and F1
measure(Table 1). 400E stood for the extrac-
tive 400 word summary provided by DUC-2002
data set. It was created manually as an extrac-
tive summary for evaluation purposes. Candidate
summary(CS) was produced by the proposed sys-
tem. Both summaries were compared against a
200 word abstractive summary included in DUC-
2002 data set. 200 word abstractive summary
was considered as the model summary in ROUGE
package. As shown, the summary of the proposed
system gives better results in Rouge-1 recall mea-
sure. However, the highest precision is achieved
in the 400E summary. Generally, the proposed
system outperforms the 400E summary, since F1-
score which takes into account precision and recall
is higher.
In addition, the performance of the system was
compared with the best systems(BEST) of DUC-
2002(Halteren, 2002; Harabagiu, 2002)(Table 2).
The results of the best systems(BEST) in terms
of sentence recall and sentence precision are pro-
vided by DUC-2002. Sentence recall and sentence
precision of the candidate summary(produced by
the proposed system) were calculated by using 400
word extract based summary(provided by DUC-
2002) and a candidate summary. Sentence recall
and sentence precision are defined as follows:
sentence recall = B (4)
sentence precision = C (5)
where M is the number of the sentences included
</bodyText>
<table confidence="0.998118">
summary P R F1
400E 0.313 0.553 0.382
candidate 0.3 0.573 0.394
</table>
<tableCaption confidence="0.998275">
Table 1: ROUGE-1 Results. Candidate sum-
</tableCaption>
<bodyText confidence="0.9893245">
mary(produced by the proposed system) and 400E
summary provided by DUC 2002 are compared
with 200 word abstract created manually.
in both of the summaries(a candidate and 400
word summary provided by DUC-2002(400E)),
C,B are the number of the sentences in the can-
didate summary and in a 400E summary, respec-
tively.
</bodyText>
<table confidence="0.77256425">
summary Sentence Sentence
Precision Recall
BEST 0.271 0.272
candidate 0.273 0.305
</table>
<tableCaption confidence="0.883691">
Table 2: Results. The best systems of DUC-2002
</tableCaption>
<bodyText confidence="0.924560142857143">
results and the results of the proposed system. Pro-
posed system is compared with 400 word extracts
provided by DUC-2002.
As shown, the proposed system performs bet-
ter than the best systems of DUC-2002 in terms
of sentence recall. We are more interested in sen-
tence recall because it states the ratio of the impor-
tant sentences contained in the candidate summary
if the sentences included in the 400E summary are
supposed to be important ones. Furthermore, sen-
tence precision is affected from the length of the
candidate summary.
Figure 1: The relationship between distortion and
rate. While rate is decreasing distortion is increas-
ing.
Summarizing the text can be considered as the
compression of the text. Thus it is possible to de-
pict the graph of dependence of distortion on rate
(Figure 1). The graph shows that as rate decreases
distortion increases monotonically. Therefore, if
distortion is assumed to be the information loss oc-
</bodyText>
<page confidence="0.998568">
68
</page>
<bodyText confidence="0.997496857142857">
curred when the original text is summarized then
the summaries of different quality can be produced
by restricting rate (the number of sentences).
Another graph shows the change of the lambda
value(Figure 2). The iteration number of the prun-
ing is on X axis and lambda value is on Y one. If A
value of the pruned points are sorted in ascending
order and then the graph of ordered A values is de-
picted according to their order then the graph iden-
tical to the one shown below is obtained(Figure 2).
This indicates that the node with minimal lambda
value is selected in each iteration. Consequently,
the sentences are eliminated so that increase in dis-
tortion is minimal for decrease in rate.
</bodyText>
<figureCaption confidence="0.743873">
Figure 2: A value of the pruned node. The change
of A value has upward tendency.
</figureCaption>
<bodyText confidence="0.9994914">
All in all, the quantitative analyses show that the
proposed system can be used as one of the redun-
dancy reduction methods. However, in order to
achieve the good results, the parameters of BFOS
algorithm have to be set appropriately.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999985565217391">
In this paper, the combination of tree pruning and
clustering is explored for the purpose of multi-
document summarization. Redundancy in the text
detected by the HAC algorithm is eliminated by
the generalized BFOS algorithm. It is shown that
if the parameters(distortion and rate) are set prop-
erly, generalized BFOS algorithm can be used to
reduce the redundancy in the text. The depicted
graph (Figure 1) shows that the proposed defi-
nitions of distortion and rate are eligible for the
multi-document summarization purpose.
The performance evaluation results in terms of
ROUGE-1 metric suggest that the proposed sys-
tem can perform better with additional improve-
ments (combining with LSI). Also it is stated that
distance measure selection and noisy sentence in-
clusion have significance impact on the summa-
rization procedure.
Future research will deal with the abstraction. A
new sentence will be created(not extracted) when
two clusters are merged. It will represent the clus-
ter of sentences as well as summarize the other
sentences in the same cluster.
</bodyText>
<sectionHeader confidence="0.995512" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9868865">
We thank Google for travel and conference sup-
port for this paper.
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999303488372093">
Aliguliyev R. 2006. A Novel Partitioning-Based Clus-
tering Method and Generic Document Summariza-
tion. In WI-IATW 06: Proceedings of the 2006
IEEE/WIC/ACM international conference on Web
Intelligence and Intelligent Agent Technology,pages
626–629,Washington,DC, USA.
Arora R. and Ravindran B. 2008. Latent Dirichlet Al-
location Based Multi-Document Summarization. In
Proceedings of the Second Workshop on Analytics
for Noisy Unstructured Text Data (AND 2008),91-
97.
Barzilay R. and Elhadad M. 1997. Using Lexical
Chains for Text Summarization. In Proceedings of
the ACL/EACL’97 Workshop on Intelligent Scalable
Text Summarization,pages 10-17.
Barzilay R. 2003. Information fusion for multi-
document summarization: Paraphrasing and gener-
ation, PhD thesis, DigitalCommons@Columbia.
Breiman L., Friedman J.H., Olshen R.A., and
Stone C.J. 1984. Classification and Regression
Trees. The Wadsworth Statistics/Probability Se-
ries,Belmont, CA: Wadsworth.
Chou A. Philip, Tom Lookabaugh, and Gray M.
Robert. 1989. Optimal Pruning with Applica-
tions to Tree-Structured Source Coding and Model-
ing. IEEE transactions on information theory, vol-
ume 35, no 2.
DUC–2002. 2002. Document Understanding Confer-
ence.
Edmundson H. P. 1969. New methods in automatic
extracting. Journal of the ACM,16:264-285.
Goldstein J., Mittal V., Carbonell J., and Kantrowitz M.
2000. Multi-document summarization by sentence
extraction. In Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization,pages 40-
48.
H. van Halteren. 2002. Writing style recognition and
sentence extraction. In Proceedings of the workshop
on automatic summarization,pages 66–70.
Harabagiu S.M. and Lacatusu F. 2002. Generating
single and multi-document summaries with gistex-
ter. In Proceedings of the workshop on automatic
summarization,pages 30–38.
</reference>
<page confidence="0.983701">
69
</page>
<reference confidence="0.999876333333333">
Hatzivassiloglou V., Klavans J. L., Holcombe M.
L., Barzilay R., Kan M.-Y., and McKeown K. R.
1999. Detecting text similarity over short pas-
sages: Exploring Linguistic Feature Combinations
via Machine Learning. In Proceedings of the 1999
Joint SIGDAT Conference on empirical Methods in
Natural Language Processing and very large cor-
pora,pages 203-212. College Park, MD, USA.
Hatzivassiloglou V., Klavans J. L., Holcombe M. L.,
Barzilay R., Kan M.-Y., and McKeown K. R. 2001.
SIMFINDER: A Flexible Clustering Tool for Sum-
marization. In NAACL Workshop on Automatic
Summarization,pages 41-49. Pittsburgh, PA, USA.
Hahn U. and Mani I. 2000. Computer. The
challenges of automatic summarization. IEEE
Computer,33(11),29–36.
Hovy E. and Lin C.Y. 1999. Automated Text Sum-
marization in SUMMARIST. Mani I and May-
bury M (eds.), Advances in Automatic Text Summa-
rization,pages 81–94. The MIT Press.
Johanna Geiss. 2011. Latent semantic sentence clus-
tering for multi-document summarization, PhD the-
sis. Cambridge University.
“Towards Multidocument Summarization by Refor-
mulation: Progress and Prospects”,
Kathleen McKeown, Judith Klavans, Vasilis Hatzivas-
siloglou, Regina Barzilay, Eleazar Eskin. 1999. To-
wards Multidocument Summarization by Reformu-
lation: Progress and Prospects. In Proceedings of
AAAI,Orlando, Florida.
Landauer T.K., Foltz P.W., and Laham D. 1998. In-
troduction to Latent Semantic Analysis. Discourse
Processes,25,pages 259–284.
Lin C.Y. and Hovy E. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statis-
tics. In North American Chapter of the Association
for Computational Linguistics on Human Language
Technology (HLTNAACL- 2003),pages 71-78.
Lin C–Y. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of Work-
shop on Text Summarization Branches Out, Post-
Conference Workshop of ACL 2004.
Luhn H.P. 1958. The Automatic Creation of
Literature Abstracts. IBM Journal of Research
Development,2(2):159-165.
Murray G., Renals S., and Carletta J. 2005. Extrac-
tive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology.
Radev D. R., Jing H., and Budzikowska M. 2000.
Centroid-based summarization of multiple docu-
ments: sentence extraction, utility-based evaluation,
and user studies, pages 21-29. In ANLP/NAACL
Workshop on Summarization, Morristown, NJ, USA.
Radev R., Blair–goldensohn S,Zhang Z. 2001. Exper-
iments in Single and Multi-Docuemtn Summariza-
tion using MEAD. InFirst Document Under- stand-
ing Conference,New Orleans,LA.
Radev D. R., Jing H., Stys M., and Tam D.
2004. Centroid-based summarization of mul-
tiple documents. Information Processing and
Management,40:919-938.
Scott Deerwester, Dumais T. Susan, Furnas W George,
Landauer Thomas K., and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society of Information Science,41(6):391-
407.
Steinberger J. and Jezek K. 2004. Using Latent Se-
mantic Analysis in Text Summarization and Sum-
mary Evaluation. Proceedings of ISIM ’04, pages
93-100.
Teufel, Simone, and Marc Moens. 1997. Sentence
extraction as a classification task. ACL/EACL
workshop on Intelligent and scalable Text
summarization,58-65.
</reference>
<page confidence="0.998451">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.145346">
<title confidence="0.994796">Multi-Document Summarization Using Distortion-Rate Ratio</title>
<author confidence="0.868006">Ulukbek</author>
<affiliation confidence="0.838154">Department of Computer Istanbul Technical</affiliation>
<email confidence="0.6689365">attokurov@itu.edu.trUlug</email>
<affiliation confidence="0.841623">Department of Computer Istanbul Technical</affiliation>
<email confidence="0.96623">ulugbayazit@itu.edu.tr</email>
<abstract confidence="0.997019037037037">The current work adapts the optimal tree pruning algorithm(BFOS) introduced by Breiman et al.(1984) and extended by Chou et al.(1989) to the multi-document summarization task. BFOS algorithm is used to eliminate redundancy which is one of the main issues in multi-document summarization. Hierarchical Agglomerative Clustering algorithm(HAC) is employed to detect the redundancy. The tree designed by HAC algorithm is successively pruned with the optimal tree pruning algorithm to optimize the distortion vs. rate cost of the resultant tree. Rate parameter is defined to be the number of the sentences in the leaves of the tree. Distortion is the sum of the distances between the representative sentence of the cluster at each node and the other sentences in the same cluster. The sentences assigned to the leaves of the resultant tree are included in the summary. The performance of the proposed system assessed with the Rouge-1 metric is seen to be better than the performance of the DUC-2002 winners on DUC-2002 data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Aliguliyev</author>
</authors>
<title>A Novel Partitioning-Based Clustering Method and Generic Document Summarization.</title>
<date>2006</date>
<booktitle>In WI-IATW 06: Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology,pages 626–629,Washington,DC,</booktitle>
<location>USA.</location>
<contexts>
<context position="7496" citStr="Aliguliyev, 2006" startWordPosition="1186" endWordPosition="1187">ach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that represents the sentences in the cluster. Finally, selected sentences are added to the summary until a predetermined length is exceeded (Aliguliyev, 2006; Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001). 3 Background 3.1 Generalized BFOS Algorithm Let us assume that we have a tree T with the set of leaves T˜. Also let us denote a sub-tree of T rooted at any node of T as S. The leaves of the sub-trees may happen to be the inner nodes of T. If the root node of the sub-tree S is not identical to the root node of T and the set of leaves S˜ is a sub-set of T˜ then S is called a branch. But if the sub-tree S is rooted at the root node of T then S is named a pruned sub-tree of T. Function defined on the tree T and on any sub-tree S is c</context>
</contexts>
<marker>Aliguliyev, 2006</marker>
<rawString>Aliguliyev R. 2006. A Novel Partitioning-Based Clustering Method and Generic Document Summarization. In WI-IATW 06: Proceedings of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology,pages 626–629,Washington,DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Arora</author>
<author>B Ravindran</author>
</authors>
<title>Latent Dirichlet Allocation Based Multi-Document Summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data (AND</booktitle>
<pages>2008--91</pages>
<marker>Arora, Ravindran, 2008</marker>
<rawString>Arora R. and Ravindran B. 2008. Latent Dirichlet Allocation Based Multi-Document Summarization. In Proceedings of the Second Workshop on Analytics for Noisy Unstructured Text Data (AND 2008),91-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization,pages</booktitle>
<pages>10--17</pages>
<contexts>
<context position="5738" citStr="Barzilay and Elhadad, 1997" startWordPosition="901" endWordPosition="904">to the tree built with the HAC algorithm. HAC algorithm is used for clustering purposes since BFOS algorithm gets tree structured data as an input. It is found that the suggested approach yields better results in terms of the ROUGE-1 Recall measure (Lin et al., 2003) when compared to 400 word extractive summaries(400E) included in DUC-2002 data set. Also, the results with the proposed method are higher than the ones obtained with the best systems of DUC-2002 in terms of sentence recall and precision(Harabagiu, 2002; Halteren, 2002). 2 Related Works Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al., 1997) are used to determine the important lexical units. Goldstein et al. (2000) proposed a measure named Maximal Marginal Relevance which assigns a high priority to the passages relevant to the query and has minimal similarity to the sentences in the summary. Radev et al. (2001) developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorpora</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Barzilay R. and Elhadad M. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization,pages 10-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
</authors>
<title>Information fusion for multidocument summarization: Paraphrasing and generation,</title>
<date>2003</date>
<tech>PhD thesis, DigitalCommons@Columbia.</tech>
<marker>Barzilay, 2003</marker>
<rawString>Barzilay R. 2003. Information fusion for multidocument summarization: Paraphrasing and generation, PhD thesis, DigitalCommons@Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees. The Wadsworth Statistics/Probability Series,Belmont,</title>
<date>1984</date>
<publisher>Wadsworth.</publisher>
<location>CA:</location>
<contexts>
<context position="4276" citStr="Breiman et al., 1984" startWordPosition="663" endWordPosition="666">nce of the topics discussed within them should be detected in order to reduce the redundancy. Some of the well-known ap64 Proceedings of the ACL 2014 Student Research Workshop, pages 64–70, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics proaches that address this problem are briefly explained in the following section. Although much work has been done to eliminate the redundancy in multi-document summarization, the problem is still actual and addressed in the current work as well. The current work proposes to integrate the generalized BFOS algorithm (Breiman et al., 1984) adopted by Chou et.al (1989) for pruned tree structured quantizer design with the HAC (Hierarchical Agglomerative Clustering) algorithm. The two main parameters (distortion and rate) in the latter work are adopted to the multi-document summarization task. Distortion can be succinctly defined as the information loss in the meaning of the sentences due to their representation with other sentences. More specifically, in the current context, distortion contribution of a cluster is taken to be the sum of the distances between the vector representations of the sentences in the cluster and represent</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>Breiman L., Friedman J.H., Olshen R.A., and Stone C.J. 1984. Classification and Regression Trees. The Wadsworth Statistics/Probability Series,Belmont, CA: Wadsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chou A Philip</author>
<author>Tom Lookabaugh</author>
<author>Gray M Robert</author>
</authors>
<title>Optimal Pruning with Applications to Tree-Structured Source Coding and Modeling.</title>
<date>1989</date>
<booktitle>IEEE transactions on information theory,</booktitle>
<volume>35</volume>
<marker>Philip, Lookabaugh, Robert, 1989</marker>
<rawString>Chou A. Philip, Tom Lookabaugh, and Gray M. Robert. 1989. Optimal Pruning with Applications to Tree-Structured Source Coding and Modeling. IEEE transactions on information theory, volume 35, no 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC–2002</author>
</authors>
<date>2002</date>
<institution>Document Understanding Conference.</institution>
<marker>DUC–2002, 2002</marker>
<rawString>DUC–2002. 2002. Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the</journal>
<pages>16--264</pages>
<contexts>
<context position="5783" citStr="Edmundson, 1969" startWordPosition="910" endWordPosition="911">is used for clustering purposes since BFOS algorithm gets tree structured data as an input. It is found that the suggested approach yields better results in terms of the ROUGE-1 Recall measure (Lin et al., 2003) when compared to 400 word extractive summaries(400E) included in DUC-2002 data set. Also, the results with the proposed method are higher than the ones obtained with the best systems of DUC-2002 in terms of sentence recall and precision(Harabagiu, 2002; Halteren, 2002). 2 Related Works Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al., 1997) are used to determine the important lexical units. Goldstein et al. (2000) proposed a measure named Maximal Marginal Relevance which assigns a high priority to the passages relevant to the query and has minimal similarity to the sentences in the summary. Radev et al. (2001) developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Processing) and IR(I</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson H. P. 1969. New methods in automatic extracting. Journal of the ACM,16:264-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>M Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL Workshop on Automatic Summarization,pages</booktitle>
<pages>40--48</pages>
<contexts>
<context position="5900" citStr="Goldstein et al. (2000)" startWordPosition="929" endWordPosition="932">he suggested approach yields better results in terms of the ROUGE-1 Recall measure (Lin et al., 2003) when compared to 400 word extractive summaries(400E) included in DUC-2002 data set. Also, the results with the proposed method are higher than the ones obtained with the best systems of DUC-2002 in terms of sentence recall and precision(Harabagiu, 2002; Halteren, 2002). 2 Related Works Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al., 1997) are used to determine the important lexical units. Goldstein et al. (2000) proposed a measure named Maximal Marginal Relevance which assigns a high priority to the passages relevant to the query and has minimal similarity to the sentences in the summary. Radev et al. (2001) developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Processing) and IR(Information Retrieval) methods. LSA(Latent Semantic Analysis) (Landauer et al., 1998) has also been used extensively i</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Goldstein J., Mittal V., Carbonell J., and Kantrowitz M. 2000. Multi-document summarization by sentence extraction. In Proceedings of the ANLP/NAACL Workshop on Automatic Summarization,pages 40-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>Writing style recognition and sentence extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on automatic</booktitle>
<pages>66--70</pages>
<marker>van Halteren, 2002</marker>
<rawString>H. van Halteren. 2002. Writing style recognition and sentence extraction. In Proceedings of the workshop on automatic summarization,pages 66–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Harabagiu</author>
<author>F Lacatusu</author>
</authors>
<title>Generating single and multi-document summaries with gistexter.</title>
<date>2002</date>
<booktitle>In Proceedings of the workshop on automatic</booktitle>
<pages>30--38</pages>
<marker>Harabagiu, Lacatusu, 2002</marker>
<rawString>Harabagiu S.M. and Lacatusu F. 2002. Generating single and multi-document summaries with gistexter. In Proceedings of the workshop on automatic summarization,pages 30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>M L Holcombe</author>
<author>R Barzilay</author>
<author>M-Y Kan</author>
<author>K R McKeown</author>
</authors>
<title>Detecting text similarity over short passages: Exploring Linguistic Feature Combinations via Machine Learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on empirical Methods in Natural Language Processing and very large</booktitle>
<pages>203--212</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="7527" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1188" endWordPosition="1191">o detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that represents the sentences in the cluster. Finally, selected sentences are added to the summary until a predetermined length is exceeded (Aliguliyev, 2006; Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001). 3 Background 3.1 Generalized BFOS Algorithm Let us assume that we have a tree T with the set of leaves T˜. Also let us denote a sub-tree of T rooted at any node of T as S. The leaves of the sub-trees may happen to be the inner nodes of T. If the root node of the sub-tree S is not identical to the root node of T and the set of leaves S˜ is a sub-set of T˜ then S is called a branch. But if the sub-tree S is rooted at the root node of T then S is named a pruned sub-tree of T. Function defined on the tree T and on any sub-tree S is called a tree functional. Monoto</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 1999</marker>
<rawString>Hatzivassiloglou V., Klavans J. L., Holcombe M. L., Barzilay R., Kan M.-Y., and McKeown K. R. 1999. Detecting text similarity over short passages: Exploring Linguistic Feature Combinations via Machine Learning. In Proceedings of the 1999 Joint SIGDAT Conference on empirical Methods in Natural Language Processing and very large corpora,pages 203-212. College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>M L Holcombe</author>
<author>R Barzilay</author>
<author>M-Y Kan</author>
<author>K R McKeown</author>
</authors>
<title>SIMFINDER: A Flexible Clustering Tool for Summarization.</title>
<date>2001</date>
<booktitle>In NAACL Workshop on Automatic Summarization,pages 41-49.</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="7559" citStr="Hatzivassiloglou et al., 2001" startWordPosition="1192" endWordPosition="1195">ied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that represents the sentences in the cluster. Finally, selected sentences are added to the summary until a predetermined length is exceeded (Aliguliyev, 2006; Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001). 3 Background 3.1 Generalized BFOS Algorithm Let us assume that we have a tree T with the set of leaves T˜. Also let us denote a sub-tree of T rooted at any node of T as S. The leaves of the sub-trees may happen to be the inner nodes of T. If the root node of the sub-tree S is not identical to the root node of T and the set of leaves S˜ is a sub-set of T˜ then S is called a branch. But if the sub-tree S is rooted at the root node of T then S is named a pruned sub-tree of T. Function defined on the tree T and on any sub-tree S is called a tree functional. Monotonic tree functional is a class o</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Holcombe, Barzilay, Kan, McKeown, 2001</marker>
<rawString>Hatzivassiloglou V., Klavans J. L., Holcombe M. L., Barzilay R., Kan M.-Y., and McKeown K. R. 2001. SIMFINDER: A Flexible Clustering Tool for Summarization. In NAACL Workshop on Automatic Summarization,pages 41-49. Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hahn</author>
<author>I Mani</author>
</authors>
<title>Computer. The challenges of automatic summarization.</title>
<date>2000</date>
<journal>IEEE Computer,33(11),29–36.</journal>
<marker>Hahn, Mani, 2000</marker>
<rawString>Hahn U. and Mani I. 2000. Computer. The challenges of automatic summarization. IEEE Computer,33(11),29–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
</authors>
<title>Automated Text Summarization</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization,pages 81–94. The</booktitle>
<editor>in SUMMARIST. Mani I and Maybury M (eds.),</editor>
<publisher>MIT Press.</publisher>
<marker>Hovy, Lin, 1999</marker>
<rawString>Hovy E. and Lin C.Y. 1999. Automated Text Summarization in SUMMARIST. Mani I and Maybury M (eds.), Advances in Automatic Text Summarization,pages 81–94. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Geiss</author>
</authors>
<title>Latent semantic sentence clustering for multi-document summarization, PhD thesis.</title>
<date>2011</date>
<institution>Cambridge University.</institution>
<contexts>
<context position="6787" citStr="Geiss, 2011" startWordPosition="1072" endWordPosition="1073">ords that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Processing) and IR(Information Retrieval) methods. LSA(Latent Semantic Analysis) (Landauer et al., 1998) has also been used extensively in recent years for multi-document summarization. By applying SVD(Singular Value Decomposition) to the term-document matrix, it determines the most important topics and represents the term and documents in the reduced space (Murray et al., 2005; Steinberger and Jezek , 2004; Geiss, 2011). Rachit Arora et al. (2008) combined LDA(Latent Dirichlet Allocation) and SVD. In this approach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that represents the sentences in the cluster. Fi</context>
</contexts>
<marker>Geiss, 2011</marker>
<rawString>Johanna Geiss. 2011. Latent semantic sentence clustering for multi-document summarization, PhD thesis. Cambridge University.</rawString>
</citation>
<citation valid="false">
<title>Towards Multidocument Summarization by Reformulation: Progress and Prospects”,</title>
<marker></marker>
<rawString>“Towards Multidocument Summarization by Reformulation: Progress and Prospects”,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Judith Klavans</author>
</authors>
<title>Vasilis Hatzivassiloglou, Regina Barzilay, Eleazar Eskin.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI,Orlando,</booktitle>
<location>Florida.</location>
<marker>McKeown, Klavans, 1999</marker>
<rawString>Kathleen McKeown, Judith Klavans, Vasilis Hatzivassiloglou, Regina Barzilay, Eleazar Eskin. 1999. Towards Multidocument Summarization by Reformulation: Progress and Prospects. In Proceedings of AAAI,Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,25,pages</booktitle>
<pages>259--284</pages>
<contexts>
<context position="6467" citStr="Landauer et al., 1998" startWordPosition="1020" endWordPosition="1023">ne the important lexical units. Goldstein et al. (2000) proposed a measure named Maximal Marginal Relevance which assigns a high priority to the passages relevant to the query and has minimal similarity to the sentences in the summary. Radev et al. (2001) developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Processing) and IR(Information Retrieval) methods. LSA(Latent Semantic Analysis) (Landauer et al., 1998) has also been used extensively in recent years for multi-document summarization. By applying SVD(Singular Value Decomposition) to the term-document matrix, it determines the most important topics and represents the term and documents in the reduced space (Murray et al., 2005; Steinberger and Jezek , 2004; Geiss, 2011). Rachit Arora et al. (2008) combined LDA(Latent Dirichlet Allocation) and SVD. In this approach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant informati</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer T.K., Foltz P.W., and Laham D. 1998. Introduction to Latent Semantic Analysis. Discourse Processes,25,pages 259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics.</title>
<date>2003</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL- 2003),pages</booktitle>
<pages>71--78</pages>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin C.Y. and Hovy E. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL- 2003),pages 71-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin C–Y</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out, PostConference Workshop of ACL</booktitle>
<marker>C–Y, 2004</marker>
<rawString>Lin C–Y. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of Workshop on Text Summarization Branches Out, PostConference Workshop of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research</journal>
<pages>2--2</pages>
<contexts>
<context position="5693" citStr="Luhn, 1958" startWordPosition="897" endWordPosition="898">pruning algorithm is applied to the tree built with the HAC algorithm. HAC algorithm is used for clustering purposes since BFOS algorithm gets tree structured data as an input. It is found that the suggested approach yields better results in terms of the ROUGE-1 Recall measure (Lin et al., 2003) when compared to 400 word extractive summaries(400E) included in DUC-2002 data set. Also, the results with the proposed method are higher than the ones obtained with the best systems of DUC-2002 in terms of sentence recall and precision(Harabagiu, 2002; Halteren, 2002). 2 Related Works Term frequency (Luhn, 1958), lexical chains (Barzilay and Elhadad, 1997), location of the sentences (Edmundson, 1969) and the cue phrases (Teufel et al., 1997) are used to determine the important lexical units. Goldstein et al. (2000) proposed a measure named Maximal Marginal Relevance which assigns a high priority to the passages relevant to the query and has minimal similarity to the sentences in the summary. Radev et al. (2001) developed a system called MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based sum</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn H.P. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research Development,2(2):159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Murray</author>
<author>S Renals</author>
<author>J Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="6743" citStr="Murray et al., 2005" startWordPosition="1063" endWordPosition="1066">ed MEAD based on the centroid of the cluster. The words that are most relevant to the main topics are included in the centroid. Lin et al. designed a statistic-based summarization system (Summarist) which incorporated NLP(Natural Language Processing) and IR(Information Retrieval) methods. LSA(Latent Semantic Analysis) (Landauer et al., 1998) has also been used extensively in recent years for multi-document summarization. By applying SVD(Singular Value Decomposition) to the term-document matrix, it determines the most important topics and represents the term and documents in the reduced space (Murray et al., 2005; Steinberger and Jezek , 2004; Geiss, 2011). Rachit Arora et al. (2008) combined LDA(Latent Dirichlet Allocation) and SVD. In this approach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Murray G., Renals S., and Carletta J. 2005. Extractive summarization of meeting recordings. In Proceedings of the 9th European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies,</title>
<date>2000</date>
<booktitle>In ANLP/NAACL Workshop on Summarization,</booktitle>
<pages>21--29</pages>
<location>Morristown, NJ, USA.</location>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Radev D. R., Jing H., and Budzikowska M. 2000. Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies, pages 21-29. In ANLP/NAACL Workshop on Summarization, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Radev</author>
<author>S Blair–goldensohn</author>
<author>Z Zhang</author>
</authors>
<title>Experiments in Single and Multi-Docuemtn Summarization using MEAD. InFirst Document Under- standing Conference,New Orleans,LA.</title>
<date>2001</date>
<marker>Radev, Blair–goldensohn, Zhang, 2001</marker>
<rawString>Radev R., Blair–goldensohn S,Zhang Z. 2001. Experiments in Single and Multi-Docuemtn Summarization using MEAD. InFirst Document Under- standing Conference,New Orleans,LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents. Information Processing and Management,40:919-938.</title>
<date>2004</date>
<contexts>
<context position="7284" citStr="Radev et al., 2004" startWordPosition="1151" endWordPosition="1154">nd represents the term and documents in the reduced space (Murray et al., 2005; Steinberger and Jezek , 2004; Geiss, 2011). Rachit Arora et al. (2008) combined LDA(Latent Dirichlet Allocation) and SVD. In this approach, LDA is used to detect topics and SVD is applied to select the sentences representing these topics. Clustering of the sentences has also been used to determine the redundant information. In this approach, the sentences are first clustered. The sentences in each cluster share common information about the main topics of the documents to be summarized. Then a sentence is selected (Radev et al., 2004) or generated (McKeown et al., 1999) from each cluster that represents the sentences in the cluster. Finally, selected sentences are added to the summary until a predetermined length is exceeded (Aliguliyev, 2006; Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001). 3 Background 3.1 Generalized BFOS Algorithm Let us assume that we have a tree T with the set of leaves T˜. Also let us denote a sub-tree of T rooted at any node of T as S. The leaves of the sub-trees may happen to be the inner nodes of T. If the root node of the sub-tree S is not identical to the root node of T and the se</context>
<context position="13618" citStr="Radev et al., 2004" startWordPosition="2276" endWordPosition="2279">ortion contribution of each leaf node of the HAC tree is zero. 66 Rate is defined to be the number of sentences in the leaves of the tree. A branch sub-tree is removed at each pruning step of the generalized BFOS algorithm. Correspondingly, the sentences at the leaf nodes of the pruned branch subtree are eliminated. As a result, the rate decreases to the number of leaf nodes remaining after pruning. The centroid of the cluster can be used as the representative sentence of the cluster. Centroid can be constituted of the important (with TF-IDF values exceeding a threshold) words of the cluster (Radev et al., 2004) or can be generated using Natural language processing techniques (McKeown et al., 1999). In the current work, the simpler approach of selecting the sentence from the cluster yielding the minimal distortion as the representative sentence is employed. A parameter is used to determine the branch sub-trees that are successively pruned. In each pruning step, the branch sub-tree with minimum A is identified to minimize the increase in total distortion(OD) per discarded sentence(OR). In accordance with the definition of rate given above, OR is the change in the number of sentences in the summary bef</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>Radev D. R., Jing H., Stys M., and Tam D. 2004. Centroid-based summarization of multiple documents. Information Processing and Management,40:919-938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Dumais T Susan</author>
<author>Furnas W George</author>
<author>Landauer Thomas K</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,41(6):391-407.</journal>
<marker>Deerwester, Susan, George, K, Harshman, 1990</marker>
<rawString>Scott Deerwester, Dumais T. Susan, Furnas W George, Landauer Thomas K., and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society of Information Science,41(6):391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Steinberger</author>
<author>K Jezek</author>
</authors>
<title>Using Latent Semantic Analysis in Text Summarization and Summary Evaluation.</title>
<date>2004</date>
<booktitle>Proceedings of ISIM ’04,</booktitle>
<pages>93--100</pages>
<marker>Steinberger, Jezek, 2004</marker>
<rawString>Steinberger J. and Jezek K. 2004. Using Latent Semantic Analysis in Text Summarization and Summary Evaluation. Proceedings of ISIM ’04, pages 93-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>ACL/EACL workshop on Intelligent and scalable Text</booktitle>
<pages>58--65</pages>
<marker>Teufel, Moens, 1997</marker>
<rawString>Teufel, Simone, and Marc Moens. 1997. Sentence extraction as a classification task. ACL/EACL workshop on Intelligent and scalable Text summarization,58-65.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>