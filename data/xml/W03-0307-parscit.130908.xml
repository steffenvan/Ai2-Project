<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.803715">
Phrase-based Evaluation of Word-to-Word Alignments
</title>
<author confidence="0.556637">
Michael Carl and Sisay Fissaha
</author>
<affiliation confidence="0.446668">
Institut für Angewandte Informationsforschung
</affiliation>
<address confidence="0.507651">
66111 Saarbrücken, Germany
</address>
<email confidence="0.976885">
{carl;sisay}@iai.uni-sb.de
</email>
<sectionHeader confidence="0.911608" genericHeader="abstract">
Abstract
2 Types of Alignment
</sectionHeader>
<bodyText confidence="0.999568111111111">
We evaluate the English—French word align-
ment data of the shared tasks from a phrase
alignment perspective. We discuss pe-
culiarities of the submitted data and the test
data. We show that phrase-based evaluation is
closely related to word-based evaluation. We
show examples of phrases which are easy to
align and also phrases which are difficult to
align.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998485">
We describe a phrase-based evaluation of the 16 Eng-
lish-French alignment submissions for the shared task
on Parallel Texts. The task was to indicate which word
token in an English alignment sample corresponds to
which word token in the French alignment sample. Two
types of submission were permitted: for restricted sub-
missions were allowed a “sentence” aligned segment of
the Canadian Hansards to train the systems while unre-
stricted submission would be allowed to use additional
resources. The performance of the systems was com-
pared for a set of 447 English—French hand-aligned
test samples which were also taken from the Canadian
Hansards.
Five institutes participated in the English—French
alignment task, submitting a total of 16 sets of align-
ment data. To evaluate the submitted data, we extracted
bilingual phrase dictionaries from the word-alignment
data. The extracted dictionaries of the submitted data
were compared with the extracted dictionary of the test
data.
We first discuss word-to-word and phrase-to-phrase
alignment format. We present two different methods for
extracting bilingual dictionaries from the word align-
ment data: a minimal dictionary contains the least num-
ber of unambiguous phrase-to-phrase translations while
an exhaustive dictionary contains all possible unambi-
guous translations. We examine the test data (i.e. the
“golden standard”) and the submitted alignment data.
We discuss their peculiarities and give examples of
phrases easy and difficult to align.
The test set consists of 447 alignment samples from the
Canadian Hansards which were pre-tokenized. A three-
tuple containing the alignment number, an English word
offset and a French word offset would indicate an exact
word-to-word translation1. The submitted data was sup-
posed to comply with this word-to-word alignment for-
mat. In example 1 the English sentence has 15 tokens
while the French sentence has 16 tokens. Example 1
shows the word-to-word alignment data of sample 91
for submission 12 and a plot of the data.
</bodyText>
<equation confidence="0.2199405">
Example 1: Alignment sample 91:
English (vertical):
</equation>
<bodyText confidence="0.9625405">
i was not asking for a detailed explana-
tion as to what he was doing .
</bodyText>
<subsectionHeader confidence="0.613878">
French (horizontal):
</subsectionHeader>
<bodyText confidence="0.835294666666667">
je ne lui ai pas demandé de me fournir de
telles explications sur ces activités .
Plot and word alignment data for submission 12:
</bodyText>
<table confidence="0.993647588235294">
Sample En Fr
15 x 91 15 16
14 x 91 14 14
13 x 91 13 8
12 x 91 12 8
11 x 91 11 12
10 x 91 10 9
09 x 91 9 11
08 x 91 8 12
07 x 91 7 9
06 x 91 6 9
05 x 91 5 12
04 x 91 4 3
03 x 91 3 2
02 x 91 2 8
01 x 91 1 1
00 xxxx x x x
</table>
<page confidence="0.846174">
01234567890123456
</page>
<bodyText confidence="0.834903333333333">
1 There was also an optional slot to indicate whether this
alignment would be [S]ure or [P]robable. We ignore this in-
formation in our evaluation.
</bodyText>
<subsectionHeader confidence="0.986728">
2.1 Word-to-word alignment
</subsectionHeader>
<bodyText confidence="0.781969809523809">
There are two underlying assumptions in word-to-word
alignment:
(i) each word token on the English side can have
any number of word correspondences -- includ-
ing zero -- on the French side and vice versa.
Word alignments may have crossing and am-
biguous branches. For instance in example 1,
the French word “me” on position 8 has the
translations “was”, and “he”, while “ai” has no
connection to the English side.
(ii) words (English or French) for which no align-
ments are given in the submitted data are as-
signed a null-alignment.
Example 1 has 22 word alignment points, where the
(ii) an English phrase may only be unambiguously
linked to exactly one French phrase and vice
versa.
Phrase-to-phrase alignments can be nested. For instance,
the shorter English—French phrase translation 9-9 &lt;-
&gt; 11-11 is included in the longer phrase translation
5-11 &lt;-&gt; 9-12:
</bodyText>
<listItem confidence="0.37651425">
5-11 9-12: for a detailed explanation
as to what
&lt;-&gt; fournir de telles explications
9-9 11-11: as &lt;-&gt; telles
</listItem>
<bodyText confidence="0.998047666666667">
In this way structural information can be stored. On the
other hand, we do not allow ambiguous phrase align-
ments as e.g.:
</bodyText>
<figure confidence="0.985917529411765">
8-8 12-12 explanation &lt;-&gt; explications
11-11 12-12 what &lt;-&gt; explications
35000
30000
25000
20000
15000
10000
5000
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
Figure1: Number of word alignment points and size of extracted dictionaries
null-alignment
submitted
text-dic1
text-dic2
align-dic1
</figure>
<bodyText confidence="0.979365363636364">
evaluators inserted 7 null-alignments. In some cases (i.e.
submission 11) this insertion accounts for almost 50%
of the alignment data. In figure 1, “null-alignment” plots
the union of the submitted alignment data and the in-
serted null-alignments. Null-alignments were not added
to submission 16 as it provides alignment information
for every word. The last data point on the x-axis (i.e. 17)
represents the test data.
As outlined in Melamed (1998), a sequence of words
which translates in a non-compositional fashion into a
target sequence is exhaustively linked (see example 2).
</bodyText>
<subsectionHeader confidence="0.997711">
2.2 Phrase-to-phrase alignment
</subsectionHeader>
<bodyText confidence="0.999995">
When extracting phrase-to-phrase translations from the
word-to-word alignment data we include a sufficient
context which disambiguates the phrases. Given the
word alignment data in example 1, the minimum con-
text required to disambiguate the French word “explica-
tions” is the phrase 5-11 &lt;-&gt; 9-12.
From the word alignment data we generate bilingual
dictionaries in two different ways: a minimal dictionary
contains only the shortest unambiguous phrase-to-
phrase translations. For instance, from the alignment
data in example 1, the following 8 entries are generated
as a minimal dictionary:3
</bodyText>
<subsectionHeader confidence="0.855926">
En Fr
</subsectionHeader>
<bodyText confidence="0.838036388888889">
1-1 1-1
Phrase-to-phrase alignment is represented by intervals 2-13 2-12
indicating the starting and ending words of the phrases. 3-3 2-2
In phrase-to-phrase alignment: 4-4 3-3
(i) a sequence of English word tokens (i.e. a 5-11 9-12
phrase) are mutually linked with sequences of 9-9 11-11
French word tokens (i.e. a French phrase)2. 14-14 14-14
15-15 16-16
2 We do not use the term “phrase” here in its linguistic sense: a
phrase in this paper can be any sequence of words, even if
they are not a linguistic constituent.
3 As shorthand notation we use here the offset numbers. In the
generated dictionary, we have extracted the sequences of
words instead of the offset numbers.
In an exhaustive dictionary all possible unambiguous
phrase translations are extracted. An exhaustive diction-
ary is a superset of the minimal dictionary. For example
1, seven additional entries are generated:
</bodyText>
<table confidence="0.932878125">
En Fr
1-13 1-12
1-14 1-14
1-15 1-16
2-14 2-14
2-15 2-16
3-4 2-3
14-15 14-16
</table>
<bodyText confidence="0.9984266">
Note that these additional phrase translations can be
compositionally generated with the minimal dictionary.
To evaluate the word alignment data through phrasal
alignments, we generated three types of dictionaries for
all 16 submissions and the test data:
</bodyText>
<listItem confidence="0.507153333333333">
(i) an alignment-based minimal dictionary,
word alignments could have been possible here, for in-
stance:
</listItem>
<equation confidence="0.623654">
i &lt;-&gt; je
asking &lt;-&gt; demandé
</equation>
<bodyText confidence="0.935640319148936">
explanation &lt;-&gt; explications
not &lt;-&gt; ne , pas
Despite the existence of some fine-grained word-to-
word correspondences in the test data, human aligners
tend to mark phrasal translations. In contrast to the
phrasal nature of the test alignments, most submissions
show a more compositional alignment structure. For
instance, alignment data of sample 91 for submission 16
has 18 word-to-word alignment points (example 3).
When the alignments are more compositional, more
coherent phrasal translations can be extracted. Thus, the
minimal phrase dictionary extracted from example 3
align-dic1; actually 447 small dictionar- Example 2: sample 91 Example 3: sample 91
ies for each sample alignment. of test set: of submission 16
(ii) a text-based minimal dictionary (text-
dic1)which is the union of the align-dic1. 15 x 15 x
(iii) an exhaustive text-based dictionary (text- 14 xxx 14 x
dic2) which is the union of exhaustive 13 xxx 13 xx
alignment dictionaries. 12 x xxx 12 x
As can be seen from figure 1, the size of the ex- 11 xxx 11 x
haustive dictionary (text-dic2) is in most cases 10 xxx 10 x
much bigger than those of the minimal dictionar- 09 xxx 09 x
ies align-dic1 and text-dic1. The reason is due to 08 xxxxxx 08 x
the way the data has been aligned. 07 xxxxxx 07 x
3 The word alignment data 06 xxxxxx 06 x
In this section we show that the test alignment 05 xxxxxx 05 x
data is structurally different from the submitted 04 xxxxxx 04 x
data. The hand aligned test data reflects the 03 xxxxxx 03 x
02 xxxxxx 02 xxx
01 xxxxxx 01 x
00 01234567890123456 00 01234567890123456
phrasal nature of the alignments, while the sub-
missions are to a greater extent compositional.
The test data (see set 17 in figure 1) has about twice to
three times as many word-alignment points than the
submissions. While this often leads to high precision
and lower recall for word alignment, the reverse is true
for the extracted phrasal dictionaries (also figure 3). The
test alignment data of sample 91 contains 68 word-to-
word alignment points shown in example 2; about four
times the average number of word alignment points for
this sample. Comparing example 2 with the submitted
data of submission 16 (example 3) brings to light the
phrasal nature of the test set.
Extracting a minimal phrase dictionary from test set
word alignment data in example 2 produces the follow-
ing three entries
</bodyText>
<footnote confidence="0.971877333333333">
1-14 1-15
5-8 7-12
15-15 16-16
</footnote>
<bodyText confidence="0.99971725">
The following additional entry is generated in the ex-
haustive dictionary:1-15 &lt;-&gt; 1-16. Note that more
contains 13 entries, while the exhaustive dictionary con-
tains 54 entries. Therefore, we expect that mapping the
phrase dictionaries on the test dictionaries would result
in high recall and low precision while mapping the sub-
mitted word alignment data on the test data would result
in high precision and low recall.
</bodyText>
<sectionHeader confidence="0.998909" genericHeader="method">
4 Phrase-based Evaluation
</sectionHeader>
<bodyText confidence="0.999698454545454">
Figure 2 shows the correlation of the f-score of the three
extracted dictionaries and the word alignment data (both
sure and probable). For each submission the f-score was
calculated as 2*precision*recall/precision+recall. The
curves for the alignment-based dictionaries (align-dic1)
show the mean f-score computed over all 447 samples.
Roughly all submissions show a similar pattern for
word-to-word alignment and phrase dictionaries.
We wanted to see what factors influence precision and
recall. A correlation between the length of the sample
alignment and its average recall and precision is shown
</bodyText>
<figure confidence="0.978768636363636">
40
90
80
70
60
50
30
20
10
0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
</figure>
<figureCaption confidence="0.889509">
Figure 2: f-score of word alignments and dictionaries
</figureCaption>
<figure confidence="0.839951">
sure
prob
align-dic1
text-dic1
text dic2
</figure>
<bodyText confidence="0.9678998">
in figure 3. As one would have expected, the graph
shows a tendency that shorter samples are easier to align
(higher precision and recall) than longer samples. How-
ever, there is higher variation among shorter alignments
than among longer sample alignments which indicates
unpredictability of shorter samples. As an example con-
sider alignment sample 7 (length 3):
hear, hear ! &lt;-&gt; bravo !
The extracted minimal test dictionary contains the two
entries:
</bodyText>
<equation confidence="0.966609">
hear,hear &lt;-&gt; bravo
! &lt;-&gt;!
</equation>
<bodyText confidence="0.9984955">
While most of the minimal dictionaries extracted from
the submitted data contain the entries:
</bodyText>
<equation confidence="0.9140255">
hear &lt;-&gt; bravo
! &lt;-&gt; !
</equation>
<bodyText confidence="0.979212090909091">
This leads to a value of 50 for recall and precision for
both word and phrase alignments. The average recall
and precision of sample 7 (length 3) is 53,1 and 57,8.
Figure 3 also shows that PD-recall (phrasal dictionary)
is higher than PD-precision as the samples become
longer. For example, sample 91 (length 15,5) has PD-
recall and PD-precision values of 65, and 50,2 respec-
tively. For word-to-word evaluation, however, WD-
precision is higher than WD-recall. For sample 91
(length 15,5) the WD-recall and WD-precision values
are 18,03 and 53,86 respectively.
Next we wanted to see which parts in the sample align-
ments would be easy and which parts would be difficult
to align. We assume that correct translations which ap-
pear in all submissions would be easy to find while
translations which occur only in the test set but in none
of the submissions would be difficult to find. Finally,
the same noisy translations produced by all submissions
would indicate mistakes in the test data. The cardinality
of these sets is shown in the table below.
Intersection of text-dic1 text-dic2
correct 150 434
missing 837 1949
noise 11 22
There were 150 one-word entries in
the intersection of the correct transla-
tions contained in all 16 dictionaries
text-dic1. These translations include
transfer rules which are easy to dis-
cover such as numbers, function
words, pronouns, frequent content
words and also domain specific trans-
lations:
</bodyText>
<footnote confidence="0.501976818181818">
1) pronouns
he &lt;-&gt; il
it &lt;-&gt; il
there &lt;-&gt; il
2) frequent content words
women &lt;-&gt; femmes
work &lt;-&gt; travaillent
compulsory &lt;--&gt; obligatoire
say &lt;-&gt; dire
says &lt;-&gt; dit
3) function words
</footnote>
<page confidence="0.6102412">
100
40
90
80
70
60
50
30
20
10
</page>
<figure confidence="0.952764416666667">
0
Figure3: length of alignments vs. Recall and Precision
Length
PD_Recall
PD_Precision
WD_Recall
WD_Precision
such &lt;-&gt; tel
to &lt;-&gt; de
to &lt;-&gt; pour
5) Text typical translations:
House &lt;-&gt; Chambre
</figure>
<bodyText confidence="0.979395">
The set of translation equivalences missing in all sub-
missions was much larger. There were only the follow-
ing five one-word equivalences:
</bodyText>
<figure confidence="0.894007730769231">
1) on-word translations:
and &lt;-&gt; puisque
balance &lt;-&gt; niveau
do &lt;-&gt; fait
per &lt;-&gt; le
very &lt;-&gt; fondamentalement
Most of the missing entries were multi-word transla-
tions, such as idioms, compound words etc.
1) idiomatic expressions
A buck is a buck is a buck &lt;-&gt; une
piastre est toujours une piastre
thank you very much &lt;-&gt; je vous re-
mercie
2) compound:
Canadian Wheat Board &lt;-&gt;
Commission canadienne de le bl6
3) complex prepositions
as for &lt;-&gt; en ce qui concerne
4) complex verbs and negation
does not like &lt;-&gt; ne aime pas
will be &lt;-&gt; feront
5) adverbs and adjective phrases
previous &lt;-&gt; qui me a pr6c6d6
a good thing &lt;-&gt; int6ressant
6) unresolved pronouns
the government &lt;-&gt; il
</figure>
<bodyText confidence="0.999731833333333">
There were also 11 noisy entries which occurred in all
generated submissions dictionaries but not in the test
data dictionary. The obvious explanation for this is,
again, the phrasal nature of the test data: single word
translations would be hidden in phrase translations and
not extracted as separated word translations:
</bodyText>
<figure confidence="0.541472545454545">
before &lt;-&gt; avant
believe &lt;-&gt; crois
days &lt;-&gt; jours
every &lt;-&gt; chacune
facilities &lt;-&gt; installations
jobs &lt;-&gt; emploi
positive &lt;-&gt; positifs
public &lt;-&gt; public
representations &lt;-&gt; instances
why &lt;-&gt; comment
will &lt;-&gt; servira
</figure>
<sectionHeader confidence="0.988036" genericHeader="method">
5 Submissions
</sectionHeader>
<bodyText confidence="0.969417">
This section lists the origin of the submitted data. A
more detailed description can be found in the system
description contained in these proceedings.
</bodyText>
<reference confidence="0.466771744186046">
1 BiBr.EF.7
Limited Resources 7. intersection of 1 &amp; 3
2 BiBr.EF.1
Limited Resources 1. Baseline of Bi-lingual Bracketing
3 BiBr.EF.2
Unlimited Resources 2. Baseline of Bi-lingual Bracket-
ing + POS (Brill&apos;s POS tagger for English only)
4 BiBr.EF.8
Unlimited Resources 8. intersection of 3 &amp; 6
5 BiBr.EF.3
Unlimited Resources 3. Baseline of Bi-lingual Bracket-
ing + POS (Brill&apos;s POS tagger for English only) + Eng-
lish_Chunker.
6 BiBr.EF.4
Limited Resources 4. reverse direction of (1)
7 BiBr.EF.5
Unlimited Resources 4. reverse direction of (2)
8 BiBr.EF.6
Unlimited Resources 4. reverse direction of (3)
9 data withdrawn
10 UMD.EF.
Limited Resources Trained on House and Senate Data
11 ProAlign.EF.1
Unlimited Resources ProAlign uses the cohesion be-
tween the source and target languages to constrain the
search for the most probable alignment (based on a
novel probability model). The extra resources include:
An English parser A distributional similarity database
for English words.
12 data withdrawn
13 XRCE.Base.EF.1
Limited Resources GIZA++ with English and French
lemmatizer (no trinity lexicon)
14 XRCE.Nolem.EF.2
Limited Resources GIZA++ only (no lemmatizer, no
trinity lexicon), Corpus used: Quarter
15 XRCE.Nolem.EF.3
Limited Resources GIZA++ only (no lemmatizer, no
trinity lexicon), Corpus used: Half
16 ralign.EF.1
Limited Resources Recursive parallel segmentation of
texts; scoring based on IBM-2
17 test data (golden standard)
</reference>
<sectionHeader confidence="0.960461" genericHeader="method">
6 References
</sectionHeader>
<reference confidence="0.998267363636364">
Ulrich Germann, editor (2001). Aligned Hansards of the
36th Parliamentof Canada. http://www.isi.edu/natural-
language/download/hansard/index.html
I. Dan Melamed (1998). Annotation Style Guide for the
Blinker Project, IRCS Technical Report #98-06,
http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide.
ps.gz
Franz Josef Och, Hermann Ney (2000). A Comparison
of Alignment Models for Statistical Machine Transla-
tion.. COLING 2000..http://www-i6.informatik.rwth-
aachen.de/Colleagues/och/COLING00.ps
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888105">
<title confidence="0.999742">Phrase-based Evaluation of Word-to-Word Alignments</title>
<author confidence="0.997775">Michael Carl</author>
<author confidence="0.997775">Sisay</author>
<affiliation confidence="0.99873">Institut für Angewandte</affiliation>
<address confidence="0.999692">66111 Saarbrücken, Germany</address>
<email confidence="0.999288">{carl;sisay}@iai.uni-sb.de</email>
<abstract confidence="0.990101090909091">2 Types of Alignment We evaluate the English—French word alignment data of the shared tasks from a phrase alignment perspective. We discuss peculiarities of the submitted data and the test data. We show that phrase-based evaluation is closely related to word-based evaluation. We show examples of phrases which are easy to align and also phrases which are difficult to align.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<tech>1 BiBr.EF.7</tech>
<marker></marker>
<rawString>1 BiBr.EF.7</rawString>
</citation>
<citation valid="false">
<title>Limited Resources 7. intersection of 1</title>
<journal></journal>
<booktitle>BiBr.EF.1 Limited Resources 1. Baseline of Bi-lingual Bracketing 3 BiBr.EF.2 Unlimited Resources 2. Baseline of Bi-lingual Bracketing + POS (Brill&apos;s POS tagger for English only) 4 BiBr.EF.8</booktitle>
<volume>3</volume>
<marker></marker>
<rawString>Limited Resources 7. intersection of 1 &amp; 3 2 BiBr.EF.1 Limited Resources 1. Baseline of Bi-lingual Bracketing 3 BiBr.EF.2 Unlimited Resources 2. Baseline of Bi-lingual Bracketing + POS (Brill&apos;s POS tagger for English only) 4 BiBr.EF.8</rawString>
</citation>
<citation valid="false">
<title>Unlimited Resources 8. intersection of 3</title>
<journal></journal>
<booktitle>BiBr.EF.3 Unlimited Resources 3. Baseline of Bi-lingual Bracketing + POS (Brill&apos;s POS tagger for English only) + English_Chunker.</booktitle>
<volume>6</volume>
<marker></marker>
<rawString>Unlimited Resources 8. intersection of 3 &amp; 6 5 BiBr.EF.3 Unlimited Resources 3. Baseline of Bi-lingual Bracketing + POS (Brill&apos;s POS tagger for English only) + English_Chunker.</rawString>
</citation>
<citation valid="false">
<authors>
<author>BiBr EF 4</author>
</authors>
<title>Limited Resources 4. reverse direction of (1) 7 BiBr.EF.5 Unlimited Resources 4. reverse direction of (2) 8 BiBr.EF.6 Unlimited Resources 4. reverse direction of (3) 9 data withdrawn 10 UMD.EF.</title>
<marker>4, </marker>
<rawString>6 BiBr.EF.4 Limited Resources 4. reverse direction of (1) 7 BiBr.EF.5 Unlimited Resources 4. reverse direction of (2) 8 BiBr.EF.6 Unlimited Resources 4. reverse direction of (3) 9 data withdrawn 10 UMD.EF.</rawString>
</citation>
<citation valid="false">
<title>Limited Resources Trained on House and Senate Data 11 ProAlign.EF.1 Unlimited Resources ProAlign uses the cohesion between the source and target languages to constrain the search for the most probable alignment (based on a novel probability model). The extra resources include: An English parser A distributional similarity database for English words.</title>
<marker></marker>
<rawString>Limited Resources Trained on House and Senate Data 11 ProAlign.EF.1 Unlimited Resources ProAlign uses the cohesion between the source and target languages to constrain the search for the most probable alignment (based on a novel probability model). The extra resources include: An English parser A distributional similarity database for English words.</rawString>
</citation>
<citation valid="false">
<title>12 data withdrawn 13 XRCE.Base.EF.1 Limited Resources GIZA++ with English and French lemmatizer (no trinity lexicon) 14 XRCE.Nolem.EF.2 Limited Resources GIZA++ only (no lemmatizer, no trinity lexicon), Corpus used:</title>
<booktitle>Quarter 15 XRCE.Nolem.EF.3 Limited Resources GIZA++ only (no lemmatizer, no trinity lexicon), Corpus used: Half 16 ralign.EF.1 Limited Resources</booktitle>
<marker></marker>
<rawString>12 data withdrawn 13 XRCE.Base.EF.1 Limited Resources GIZA++ with English and French lemmatizer (no trinity lexicon) 14 XRCE.Nolem.EF.2 Limited Resources GIZA++ only (no lemmatizer, no trinity lexicon), Corpus used: Quarter 15 XRCE.Nolem.EF.3 Limited Resources GIZA++ only (no lemmatizer, no trinity lexicon), Corpus used: Half 16 ralign.EF.1 Limited Resources Recursive parallel segmentation of texts; scoring based on IBM-2 17 test data (golden standard)</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Aligned Hansards of the 36th Parliamentof Canada. http://www.isi.edu/naturallanguage/download/hansard/index.html</booktitle>
<editor>Ulrich Germann, editor</editor>
<marker>2001</marker>
<rawString>Ulrich Germann, editor (2001). Aligned Hansards of the 36th Parliamentof Canada. http://www.isi.edu/naturallanguage/download/hansard/index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Annotation Style Guide for the Blinker Project, IRCS</title>
<date>1998</date>
<tech>Technical Report #98-06, http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide. ps.gz</tech>
<contexts>
<context position="5107" citStr="Melamed (1998)" startWordPosition="858" endWordPosition="859">5 6 7 8 9 10 11 12 13 14 15 16 17 Figure1: Number of word alignment points and size of extracted dictionaries null-alignment submitted text-dic1 text-dic2 align-dic1 evaluators inserted 7 null-alignments. In some cases (i.e. submission 11) this insertion accounts for almost 50% of the alignment data. In figure 1, “null-alignment” plots the union of the submitted alignment data and the inserted null-alignments. Null-alignments were not added to submission 16 as it provides alignment information for every word. The last data point on the x-axis (i.e. 17) represents the test data. As outlined in Melamed (1998), a sequence of words which translates in a non-compositional fashion into a target sequence is exhaustively linked (see example 2). 2.2 Phrase-to-phrase alignment When extracting phrase-to-phrase translations from the word-to-word alignment data we include a sufficient context which disambiguates the phrases. Given the word alignment data in example 1, the minimum context required to disambiguate the French word “explications” is the phrase 5-11 &lt;-&gt; 9-12. From the word alignment data we generate bilingual dictionaries in two different ways: a minimal dictionary contains only the shortest unam</context>
</contexts>
<marker>Melamed, 1998</marker>
<rawString>I. Dan Melamed (1998). Annotation Style Guide for the Blinker Project, IRCS Technical Report #98-06, http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide. ps.gz</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Comparison of Alignment Models for Statistical Machine Translation..</title>
<date>2000</date>
<booktitle>COLING 2000..http://www-i6.informatik.rwthaachen.de/Colleagues/och/COLING00.ps</booktitle>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och, Hermann Ney (2000). A Comparison of Alignment Models for Statistical Machine Translation.. COLING 2000..http://www-i6.informatik.rwthaachen.de/Colleagues/och/COLING00.ps</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>