<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.080529">
<title confidence="0.9987025">
Rapidly Deploying Grammar-Based Speech Applications
with Active Learning and Back-off Grammars
</title>
<author confidence="0.999319">
Tim Paek1, Sudeep Gandhe2, David Maxwel Chickering1
</author>
<affiliation confidence="0.9646465">
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052
2 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292
</affiliation>
<email confidence="0.997631">
{timpaek|dmax}@microsoft.com, gandhe@usc.edu
</email>
<sectionHeader confidence="0.997392" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998754863636364">
Grammar-based approaches to spoken lan-
guage understanding are utilized to a great ex-
tent in industry, particularly when developers
are confronted with data sparsity. In order to
ensure wide grammar coverage, developers
typically modify their grammars in an itera-
tive process of deploying the application, col-
lecting and transcribing user utterances, and
adjusting the grammar. In this paper, we ex-
plore enhancing this iterative process by leve-
raging active learning with back-off
grammars. Because the back-off grammars
expand coverage of user utterances, develop-
ers have a safety net for deploying applica-
tions earlier. Furthermore, the statistics related
to the back-off can be used for active learning,
thus reducing the effort and cost of data tran-
scription. In experiments conducted on a
commercially deployed application, the ap-
proach achieved levels of semantic accuracy
comparable to transcribing all failed utter-
ances with 87% less transcriptions.
</bodyText>
<sectionHeader confidence="0.999468" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901258064516">
Although research in spoken language understand-
ing is typically pursued from a statistical perspec-
tive, grammar-based approaches are utilized to a
great extent in industry (Knight et al., 2001).
Speech recognition grammars are often manually
authored and iteratively modified as follows: Typi-
cally, context-free grammars (CFG) are written in
a format such as Speech Recognition Grammar
Specification (SRGS) (W3C, 2004) and deployed.
Once user utterances are collected and transcribed,
the grammars are then adjusted to improve their
coverage. This process continues until minimal
OOG utterances are observed. In this paper, we
explore enhancing this iterative process of gram-
mar modification by combining back-off gram-
mars, which expand coverage of user utterances,
with active learning, which reduces “the number of
training examples to be labeled by automatically
processing unlabeled examples, and then selecting
the most informative ones with respect to a speci-
fied cost function for a human to label” (Hakkani-
Tur et al., 2002). This paper comprises three sec-
tions. In Section 2, we describe our overall ap-
proach to rapid application development (RAD). In
Section 3, we explain how data transcription can
be reduced by leveraging active learning based on
statistics related to the usage of back-off gram-
mars. Finally, in Section 4, we evaluate the active
learning approach with simulation experiments
conducted on data collected from a commercial
grammar-based speech application.
</bodyText>
<sectionHeader confidence="0.977406" genericHeader="method">
2 RAD Approach &amp; Related Work
</sectionHeader>
<bodyText confidence="0.99980875">
Working under the assumption that developers in
industry will continue to use CFGs for rapid appli-
cation development, our approach to grammar
modification is as follows:
</bodyText>
<listItem confidence="0.989262153846154">
1. Create a CFG (either manually or automatically).
1.1 Generate a back-off grammar from the CFG.
2. Deploy the application.
2.1 Use the back-off grammar for OOG utterances.
3. Gather data from users.
4. Selectively transcribe data by using statistics re-
lated to the back-off for active learning; i.e., transcribe
only those utterances that satisfy the active learning
criterion.
5. Modify CFG either manually or automatically and
go to step 1.1.
To begin with, developers start with a CFG in Step
1. If they had access to a grammatical platform
</listItem>
<footnote confidence="0.737987">
2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opi-
nions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred.
</footnote>
<page confidence="0.9928">
64
</page>
<bodyText confidence="0.993127057142857">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64–67,
Columbus, June 2008. c�2008 Association for Computational Linguistics
such as Regulus (Rayner et al., 2006), they could
in principle construct a CFG automatically for any
new domain, though most developers will probably
manually author the grammar. Two steps are added
to the typical iterative process. In Step 1.1, we
generate a back-off grammar from the CFG. One
way to accomplish this is by constructing a back-
off CFG using filler models (Paek et al., 2007),
which when applied to the same command-and-
control task in Section 4 can result in a 35% rela-
tive reduction in semantic error rate for OOG ut-
terances. However, the back-off grammar could
also be a SLM trained on artificial data created
from the CFG (Galescu et al., 1998). Whatever
back-off mechanism is employed, its coverage
should be wider than the original CFG so that ut-
terances that fail to be recognized by the CFG, or
fall below an acceptable confidence threshold, can
be handled by the back-off in a second or simulta-
neous pass. That is the gist of Step 2.1, the second
additional step. It is not only important to generate
a back-off grammar, but it must be utilized for
handling possible OOG utterances.
Our approach attempts to reduce the usual cost
associated with grammar modification after the
application has been deployed and data collected in
Step 4. The idea is simple: Exploit the fast and ac-
curate CFG recognition of in-grammar (ING) ut-
terances by making OOG utterances handled by
the back-off grammar ING. In other words, expand
CFG coverage to include whatever gets handled by
the back-off grammar. This idea is very comple-
mentary with a two-pass recognition approach
where the goal is to get utterances correctly recog-
nized by a CFG on the first pass so as to minimize
computational expenses (Paek et al., 2007).
All of this can be accomplished with reduced
transcription effort by keeping track of and leve-
raging back-off statistics for active learning. If the
back-off is a CFG, we keep track of statistics re-
lated to which CFG rules were utilized the most,
whether they allowed the task to be successfully
completed, etc. If the back-off is a SLM, we keep
track of similar statistics related to the semantic
alignment and mapping in spoken language under-
standing. Given an active learning criterion, these
statistics can be used to selectively transcribe ut-
terances which can then be used to modify the
CFG in Step 5 so that OOG utterances become
ING. Section 3 covers this in more detail.
Finally, in Step 5, the CFG grammar is mod-
ified using the selectively transcribed utterances.
Although developers will probably want to do this
manually, it is possible to automate much of this
step by making grammar changes with minimal
edit distance or Levenshtein distance.
Leveraging a wider coverage back-off grammar
is of course not new. For grammar-based applica-
tions, several researchers have investigated using a
CFG along with a back-off grammar either simul-
taneously via a domain-trained SLM (Gorrell et
a1., 2002), or in two-pass recognition using either
an SLM trained on CFG data (Gorrell, 2003) or a
dictation n-gram (Dusan &amp; Flanagan, 2002). To
our knowledge however, no prior research has con-
sidered leveraging statistics related to the back-off
grammar for active learning, especially as part of a
RAD approach.
</bodyText>
<sectionHeader confidence="0.997867" genericHeader="method">
3 Active Learning
</sectionHeader>
<bodyText confidence="0.9995124">
Our overall approach utilizes back-off grammars to
provide developers with a safety net for deploying
applications earlier, and active learning to reduce
transcription effort and cost. We now elaborate on
active learning, demonstrate the concept with re-
spect to a CFG back-off.
Active learning aims at reducing transcription
of training examples by selecting utterances that
are most likely to be informative according to a
specified cost function (Hakkani-Tur et al., 2002).
In the speech community, active learning has been
successfully applied to reducing the transcription
effort for ASR (Hakkani-Tur et al., 2002), SLU
(Tur et al., 2003b), as well as finding labeling er-
rors (Tur et al., 2003). In our case, the examples
are user utterances that need to be transcribed, and
the learning involves modifying a CFG to achieve
wider coverage of user expressions. Instead of pas-
sively transcribing everything and modifying the
CFG as such, the grammar can “actively” partici-
pate in which utterances are transcribed.
The usual procedure for selecting utterances for
grammar modification is to transcribe at least all
failed utterances, such as those that fall below a
rejection threshold. By leveraging a back-off
grammar, developers have more information with
which to select utterances for transcription. For a
CFG back-off, how frequently a back-off rule fired
can serve as an active learning criterion because
that is where OOG utterances are handled. Given
</bodyText>
<page confidence="0.998234">
65
</page>
<bodyText confidence="0.99978525">
this active learning criterion, the algorithm would
proceed as follows (where i denotes iteration, St
denotes the set of transcribed utterances, and Su
denotes the set of all utterances):
</bodyText>
<listItem confidence="0.988892285714286">
[1] Modify CFGi using St and generate corresponding
back-offi from the CFGi.
[2] Recognize utterances in set Su using CFGi + back-
offi.
[3] Compute statistics on what back-off rules fired
when and how frequently.
[4] Select the k utterances that were handled by the
</listItem>
<bodyText confidence="0.976333478260869">
most frequently occurring back-off rule and tran-
scribe them. Call the new transcribed set as Si.
[5]S,=S,US;;S„=S„—S;
[6] Stop when CFGi achieves a desired level of seman-
tic accuracy, or alternatively when back-off rules
only handle a desired percentage of Su, otherwise
go to Step 1.
Note that the set Su grows with each iteration and
follows as a result of deploying an application with
a CFGi + back-offi. Step [1] corresponds to Step 5,
1.1, and 2.1 of our approach. Steps [2-4] above
constitute the active learning criterion and can be
adjusted depending on what developers want to
optimize. This algorithm currently assumes that
runtime efficiency is the main objective (e.g., on a
mobile device); hence, it is critical to move utter-
ances recognized in the second pass to the first
pass. If developers are more interested in learning
new semantics, in Step [4] above they could tran-
scribe utterances that failed in the back-off. With
an active learning criterion in place, Step [6] pro-
vides a stopping criterion. This too can be adjusted,
and may even target budgetary objectives.
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999900586956522">
For evaluation, we used utterances collected from
204 users of Microsoft Voice Command, a gram-
mar-based command-and-control (C&amp;C) applica-
tion for high-end mobile devices (see Paek et al.,
2007 for details). We partitioned 5061 transcribed
utterances into five sets, one of which was used
exclusively for testing. The remaining four were
used for iterative CFG modification. For the first
iteration, we started with a CFG which was a de-
graded version of the grammar currently shipped
with the Voice Command product. It was obtained
by using the mode, or the most frequent user utter-
ance, for each CFG rule. We compared two ap-
proaches: CFG_Full, where each iterative CFG
was modified using the full set of transcribed utter-
ances that resulted in a failure state (i.e., when a
false recognition event occurred or the phrase con-
fidence score fell below 45%, which was set by a
proprietary tuning procedure for optimizing word-
error rate), and CFG_Active, where each iterative
CFG was modified using only those transcribed
utterances corresponding to the most frequently
occurring CFG back-off rules. For both CFG_Full
and CFG_Active, CFGi was modified using the
same set of heuristics akin to minimal edit dis-
tance. In order to assess the value of using the
back-off grammar as a safety net, we also com-
pared CFG_Full+Back-off, where a derived CFG
back-off was utilized whenever a failure state oc-
curred with CFG_Full, and CFG_Active+Back-off,
where again a CFG back-off was utilized, this time
with the back-off derived from the CFG trained on
selective utterances.
As our metric, we evaluated semantic accuracy
since that is what matters most in C&amp;C settings.
Furthermore, because recognition of part of an ut-
terance can increase the odds of ultimately achiev-
ing task completion (Paek et al., 2007), we carried
out separate evaluations for the functional consti-
tuents of a C&amp;C utterance (i.e., keyword and slot)
as well as the complete phrase (keyword + slot).
We computed accuracy as follows: For any single
utterance, the recognizer can either accept or reject
it. If it is accepted, then the semantics of the utter-
ance can either be correct (i.e., it matches what the
user intended) or incorrect, hence:
</bodyText>
<equation confidence="0.684399">
accuracy = CA / (CA + IA + R) (1)
</equation>
<bodyText confidence="0.999869235294118">
where CA denotes accepted commands that are
correct, IA denotes accepted commands that are
incorrect, and R denotes the number of rejections.
Table 2 displays semantic accuracies for both
CFG_Full and CFG_Active. Standard errors about
the mean were computed using the jacknife proce-
dure with 10 re-samples. Notice that both
CFG_Full and CFG_Active initially have the same
accuracy levels because they start off with the
same degraded CFG. The highest accuracies ob-
tained almost always occurred in the second itera-
tion after modifying the CFG with the first batch of
transcriptions. Thereafter, all accuracies seem to
decrease. In order to understand why this would be
case, we computed the coverage of the ith CFG on
the holdout set. This is reported in the „OOG%‟
column. Comparing CFG_Full to CFG_Active on
</bodyText>
<page confidence="0.96725">
66
</page>
<table confidence="0.99992815">
Approach i Utterances Keyword Slot Keyword + Slot Processing OOG%
Transcribed Accuracy Accuracy Accuracy Time (ms)
CFG_Full 1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10%
2 590 66.20% (0.12%) 71.02% (0.23%) 70.59% (0.23%) 401 (4.0586) 31.92%
3 1000 65.80% (0.15%) 69.72% (0.19%) 69.06% (0.19%) 422 (4.5804) 31.30%
4 1393 66.10% (0.13%) 67.54% (0.22%) 66.88% (0.21%) 433 (4.7061) 30.95%
CFG_Full + 1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10%
Back-off
2 590 73.32% (0.11%) 72.11% (0.22%) 71.68% (0.23%) 562 (10.4696) 31.92%
3 1000 72.52% (0.12%) 72.11% (0.21%) 71.46% (0.22%) 584 (10.4985) 31.30%
4 1393 73.02% (0.10%) 71.02% (0.23%) 70.37% (0.23%) 592 (10.6805) 30.95%
CFG_Active 1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10%
2 87 64.09% (0.13%) 74.29% (0.21%) 74.07% (0.22%) 395 (4.1469) 42.09%
3 138 64.29% (0.15%) 70.15% (0.22%) 69.50% (0.24%) 409 (4.3375) 38.02%
4 193 64.09% (0.15%) 69.72% (0.23%) 69.06% (0.24%) 413 (4.4015) 37.93%
CFG_Active 1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10%
+ Back-off
2 87 72.52% (0.10%) 76.91% (0.19%) 76.47% (0.21%) 568 (10.3494) 42.09%
3 138 71.72% (0.14%) 71.90% (0.24%) 71.24% (0.27%) 581 (10.6330) 38.02%
4 193 71.21% (0.15%) 71.90% (0.25%) 71.24% (0.26%) 580 (10.5266) 37.93%
</table>
<tableCaption confidence="0.653873">
Table 2. Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either
“Full” or “Active” transcriptions (i.e., selective transcriptions based on active learning). Parentheses indicate standard error about the mean.
The `i&apos; column represents iteration. The `Utterances Transcribed&apos; column is cumulative. The `OOG%&apos; column represents coverage of the
ith CFG on the hold-out set. Rows containing “Back-off” evaluate 2-pass recognition using both the CFG and a derived CFG back-off.
</tableCaption>
<bodyText confidence="0.999912730769231">
keyword + slot accuracy, CFG_Full decreases in
accuracy after the second iteration as does
CFG_Active. However, the OOG% of CFG_Full is
much lower than CFG_Active. In fact, it seems to
level off after the second iteration, suggesting that
perhaps the decrease in accuracies reflects the in-
crease in grammar perplexity; that is, as the gram-
mar covers more of the utterances, it has more
hypotheses to consider, and as a result, performs
slightly worse. Interestingly, after the last iteration,
CFG_Active for keyword + slot and slot accuracies
was slightly higher (69.06%) than CFG_Full
(66.88%) (p = .05). Furthermore, this was done
with 193 utterances as opposed to 1393, or 87%
less transcriptions. For keyword accuracy,
CFG_Active (64.09%) was slightly worse than
CFG_Full (66.10%) (p &lt; .05).
With respect to the value of having a back-off
grammar as a safety net, we found that both
CFG_Full and CFG_Active achieved much higher
accuracies with the back-off for keyword, slot, and
keyword + slot accuracies. Notice also that the dif-
ferences between CFG_Full and CFG_Active after
the last iteration were much closer to each other
than without the back-off, suggesting applications
should always be deployed with a back-off.
</bodyText>
<sectionHeader confidence="0.999767" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999941230769231">
In this paper, we explored enhancing the usual
iterative process of grammar modification by leve-
raging active learning with back-off grammars.
Because the back-off grammars expand coverage
of user utterances to handle OOG occurrences, de-
velopers have a safety net for deploying applica-
tions earlier. Furthermore, because statistics related
to the back-off can be used for active learning, de-
velopers can reduce the effort and cost of data
transcription. In our simulation experiments, leve-
raging active learning achieved levels of semantic
accuracy comparable to transcribing all failed ut-
terances with 87% less transcriptions.
</bodyText>
<sectionHeader confidence="0.999238" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994132576923077">
S. Dusan &amp; J. Flanagan. 2002. Adaptive dialog based upon multimod-
al language acquisition. In Proc. of ICMI.
L. Galescu, E. Ringger, &amp; J. Allen. 1998. Rapid language model de-
velopment for new task domains. In Proc. of LREC.
G. Gorrell, I. Lewin, &amp; M. Rayner. 2002. Adding intelligent help to
mixed initiative spoken dialogue systems. In Proc. of ICSLP.
G.. Gorrell. 2003. Using statistical language modeling to identify new
vocabulary in a grammar-based speech recognition system. In
Proc. of Eurospeech.
D. Hakkani-Tur, G. Riccardi &amp; A. Gorin. 2002. Active learning for
automatic speech recognition. In Proc. of ICASSP.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, &amp; I. Le-
win. 2001. Comparing grammar-based and robust approaches to
speech understanding: A case study. In Proc. of Eurospeech.
T. Paek, S. Gandhe, D. Chickering &amp; Y. Ju. 2007. Handling out-of-
grammar commands in mobile speech interaction using back-off
filler models. In Proc. of ACL Workshop on Grammar-Based Ap-
proaches to Spoken Language Processing (SPEECHGRAM).
M. Rayner, B.A. Hockey, &amp; P. Bouillon. 2006. Putting Linguistics
into Speech Recognition: The Regulus Grammar Compiler. CSLI.
G. Tur, M. Rahim &amp; D. Hakkani-Tur. 2003. Active labeling for spo-
ken language understanding. In Proc. of Eurospeech.
G. Tur, R. Schapire, &amp; D. Hakkani-Tur. 2003b. Active learning for
spoken language understanding. In Proc. of ICASSP.
W3C. 2004. Speech Recognition Grammar Specification Version 1.0.
http://www.w3.org/TR/speech-grammar
</reference>
<page confidence="0.999448">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469580">
<title confidence="0.998836">Rapidly Deploying Grammar-Based Speech with Active Learning and Back-off Grammars</title>
<author confidence="0.967938">Sudeep David Maxwel</author>
<affiliation confidence="0.679497">Research, One Microsoft Way, Redmond, WA</affiliation>
<address confidence="0.528346">Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA</address>
<email confidence="0.992359">{timpaek|dmax}@microsoft.com, gandhe@usc.edu</email>
<abstract confidence="0.997668304347826">Grammar-based approaches to spoken language understanding are utilized to a great extent in industry, particularly when developers are confronted with data sparsity. In order to ensure wide grammar coverage, developers typically modify their grammars in an iterative process of deploying the application, collecting and transcribing user utterances, and adjusting the grammar. In this paper, we explore enhancing this iterative process by leveraging active learning with back-off grammars. Because the back-off grammars expand coverage of user utterances, developers have a safety net for deploying applications earlier. Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription. In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Dusan</author>
<author>J Flanagan</author>
</authors>
<title>Adaptive dialog based upon multimodal language acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of ICMI.</booktitle>
<contexts>
<context position="7035" citStr="Dusan &amp; Flanagan, 2002" startWordPosition="1110" endWordPosition="1113">ar is modified using the selectively transcribed utterances. Although developers will probably want to do this manually, it is possible to automate much of this step by making grammar changes with minimal edit distance or Levenshtein distance. Leveraging a wider coverage back-off grammar is of course not new. For grammar-based applications, several researchers have investigated using a CFG along with a back-off grammar either simultaneously via a domain-trained SLM (Gorrell et a1., 2002), or in two-pass recognition using either an SLM trained on CFG data (Gorrell, 2003) or a dictation n-gram (Dusan &amp; Flanagan, 2002). To our knowledge however, no prior research has considered leveraging statistics related to the back-off grammar for active learning, especially as part of a RAD approach. 3 Active Learning Our overall approach utilizes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost. We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off. Active learning aims at reducing transcription of training examples by selecting utterances that are most likely to be informative</context>
</contexts>
<marker>Dusan, Flanagan, 2002</marker>
<rawString>S. Dusan &amp; J. Flanagan. 2002. Adaptive dialog based upon multimodal language acquisition. In Proc. of ICMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Galescu</author>
<author>E Ringger</author>
<author>J Allen</author>
</authors>
<title>Rapid language model development for new task domains.</title>
<date>1998</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="4633" citStr="Galescu et al., 1998" startWordPosition="708" endWordPosition="711">ey could in principle construct a CFG automatically for any new domain, though most developers will probably manually author the grammar. Two steps are added to the typical iterative process. In Step 1.1, we generate a back-off grammar from the CFG. One way to accomplish this is by constructing a backoff CFG using filler models (Paek et al., 2007), which when applied to the same command-andcontrol task in Section 4 can result in a 35% relative reduction in semantic error rate for OOG utterances. However, the back-off grammar could also be a SLM trained on artificial data created from the CFG (Galescu et al., 1998). Whatever back-off mechanism is employed, its coverage should be wider than the original CFG so that utterances that fail to be recognized by the CFG, or fall below an acceptable confidence threshold, can be handled by the back-off in a second or simultaneous pass. That is the gist of Step 2.1, the second additional step. It is not only important to generate a back-off grammar, but it must be utilized for handling possible OOG utterances. Our approach attempts to reduce the usual cost associated with grammar modification after the application has been deployed and data collected in Step 4. Th</context>
</contexts>
<marker>Galescu, Ringger, Allen, 1998</marker>
<rawString>L. Galescu, E. Ringger, &amp; J. Allen. 1998. Rapid language model development for new task domains. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gorrell</author>
<author>I Lewin</author>
<author>M Rayner</author>
</authors>
<title>Adding intelligent help to mixed initiative spoken dialogue systems.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<marker>Gorrell, Lewin, Rayner, 2002</marker>
<rawString>G. Gorrell, I. Lewin, &amp; M. Rayner. 2002. Adding intelligent help to mixed initiative spoken dialogue systems. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gorrell</author>
</authors>
<title>Using statistical language modeling to identify new vocabulary in a grammar-based speech recognition system.</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="6988" citStr="Gorrell, 2003" startWordPosition="1104" endWordPosition="1105">ail. Finally, in Step 5, the CFG grammar is modified using the selectively transcribed utterances. Although developers will probably want to do this manually, it is possible to automate much of this step by making grammar changes with minimal edit distance or Levenshtein distance. Leveraging a wider coverage back-off grammar is of course not new. For grammar-based applications, several researchers have investigated using a CFG along with a back-off grammar either simultaneously via a domain-trained SLM (Gorrell et a1., 2002), or in two-pass recognition using either an SLM trained on CFG data (Gorrell, 2003) or a dictation n-gram (Dusan &amp; Flanagan, 2002). To our knowledge however, no prior research has considered leveraging statistics related to the back-off grammar for active learning, especially as part of a RAD approach. 3 Active Learning Our overall approach utilizes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost. We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off. Active learning aims at reducing transcription of training examples by selecting ut</context>
</contexts>
<marker>Gorrell, 2003</marker>
<rawString>G.. Gorrell. 2003. Using statistical language modeling to identify new vocabulary in a grammar-based speech recognition system. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hakkani-Tur</author>
<author>G Riccardi</author>
<author>A Gorin</author>
</authors>
<title>Active learning for automatic speech recognition.</title>
<date>2002</date>
<booktitle>In Proc. of ICASSP.</booktitle>
<contexts>
<context position="7701" citStr="Hakkani-Tur et al., 2002" startWordPosition="1211" endWordPosition="1214">arch has considered leveraging statistics related to the back-off grammar for active learning, especially as part of a RAD approach. 3 Active Learning Our overall approach utilizes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost. We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off. Active learning aims at reducing transcription of training examples by selecting utterances that are most likely to be informative according to a specified cost function (Hakkani-Tur et al., 2002). In the speech community, active learning has been successfully applied to reducing the transcription effort for ASR (Hakkani-Tur et al., 2002), SLU (Tur et al., 2003b), as well as finding labeling errors (Tur et al., 2003). In our case, the examples are user utterances that need to be transcribed, and the learning involves modifying a CFG to achieve wider coverage of user expressions. Instead of passively transcribing everything and modifying the CFG as such, the grammar can “actively” participate in which utterances are transcribed. The usual procedure for selecting utterances for grammar m</context>
</contexts>
<marker>Hakkani-Tur, Riccardi, Gorin, 2002</marker>
<rawString>D. Hakkani-Tur, G. Riccardi &amp; A. Gorin. 2002. Active learning for automatic speech recognition. In Proc. of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Knight</author>
<author>G Gorrell</author>
<author>M Rayner</author>
<author>D Milward</author>
<author>R Koel-ing</author>
<author>I Lewin</author>
</authors>
<title>Comparing grammar-based and robust approaches to speech understanding: A case study.</title>
<date>2001</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="1509" citStr="Knight et al., 2001" startWordPosition="214" endWordPosition="217">tterances, developers have a safety net for deploying applications earlier. Furthermore, the statistics related to the back-off can be used for active learning, thus reducing the effort and cost of data transcription. In experiments conducted on a commercially deployed application, the approach achieved levels of semantic accuracy comparable to transcribing all failed utterances with 87% less transcriptions. 1 Introduction Although research in spoken language understanding is typically pursued from a statistical perspective, grammar-based approaches are utilized to a great extent in industry (Knight et al., 2001). Speech recognition grammars are often manually authored and iteratively modified as follows: Typically, context-free grammars (CFG) are written in a format such as Speech Recognition Grammar Specification (SRGS) (W3C, 2004) and deployed. Once user utterances are collected and transcribed, the grammars are then adjusted to improve their coverage. This process continues until minimal OOG utterances are observed. In this paper, we explore enhancing this iterative process of grammar modification by combining back-off grammars, which expand coverage of user utterances, with active learning, which</context>
</contexts>
<marker>Knight, Gorrell, Rayner, Milward, Koel-ing, Lewin, 2001</marker>
<rawString>S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, &amp; I. Lewin. 2001. Comparing grammar-based and robust approaches to speech understanding: A case study. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>S Gandhe</author>
<author>D Chickering</author>
<author>Y Ju</author>
</authors>
<title>Handling out-ofgrammar commands in mobile speech interaction using back-off filler models.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Workshop on Grammar-Based Approaches to Spoken Language Processing (SPEECHGRAM).</booktitle>
<contexts>
<context position="4361" citStr="Paek et al., 2007" startWordPosition="659" endWordPosition="662">cy of the U.S. Government, and no official endorsement should be inferred. 64 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64–67, Columbus, June 2008. c�2008 Association for Computational Linguistics such as Regulus (Rayner et al., 2006), they could in principle construct a CFG automatically for any new domain, though most developers will probably manually author the grammar. Two steps are added to the typical iterative process. In Step 1.1, we generate a back-off grammar from the CFG. One way to accomplish this is by constructing a backoff CFG using filler models (Paek et al., 2007), which when applied to the same command-andcontrol task in Section 4 can result in a 35% relative reduction in semantic error rate for OOG utterances. However, the back-off grammar could also be a SLM trained on artificial data created from the CFG (Galescu et al., 1998). Whatever back-off mechanism is employed, its coverage should be wider than the original CFG so that utterances that fail to be recognized by the CFG, or fall below an acceptable confidence threshold, can be handled by the back-off in a second or simultaneous pass. That is the gist of Step 2.1, the second additional step. It </context>
<context position="5697" citStr="Paek et al., 2007" startWordPosition="889" endWordPosition="892">r approach attempts to reduce the usual cost associated with grammar modification after the application has been deployed and data collected in Step 4. The idea is simple: Exploit the fast and accurate CFG recognition of in-grammar (ING) utterances by making OOG utterances handled by the back-off grammar ING. In other words, expand CFG coverage to include whatever gets handled by the back-off grammar. This idea is very complementary with a two-pass recognition approach where the goal is to get utterances correctly recognized by a CFG on the first pass so as to minimize computational expenses (Paek et al., 2007). All of this can be accomplished with reduced transcription effort by keeping track of and leveraging back-off statistics for active learning. If the back-off is a CFG, we keep track of statistics related to which CFG rules were utilized the most, whether they allowed the task to be successfully completed, etc. If the back-off is a SLM, we keep track of similar statistics related to the semantic alignment and mapping in spoken language understanding. Given an active learning criterion, these statistics can be used to selectively transcribe utterances which can then be used to modify the CFG i</context>
<context position="10440" citStr="Paek et al., 2007" startWordPosition="1653" endWordPosition="1656"> (e.g., on a mobile device); hence, it is critical to move utterances recognized in the second pass to the first pass. If developers are more interested in learning new semantics, in Step [4] above they could transcribe utterances that failed in the back-off. With an active learning criterion in place, Step [6] provides a stopping criterion. This too can be adjusted, and may even target budgetary objectives. 4 Evaluation For evaluation, we used utterances collected from 204 users of Microsoft Voice Command, a grammar-based command-and-control (C&amp;C) application for high-end mobile devices (see Paek et al., 2007 for details). We partitioned 5061 transcribed utterances into five sets, one of which was used exclusively for testing. The remaining four were used for iterative CFG modification. For the first iteration, we started with a CFG which was a degraded version of the grammar currently shipped with the Voice Command product. It was obtained by using the mode, or the most frequent user utterance, for each CFG rule. We compared two approaches: CFG_Full, where each iterative CFG was modified using the full set of transcribed utterances that resulted in a failure state (i.e., when a false recognition </context>
<context position="12043" citStr="Paek et al., 2007" startWordPosition="1916" endWordPosition="1919">cs akin to minimal edit distance. In order to assess the value of using the back-off grammar as a safety net, we also compared CFG_Full+Back-off, where a derived CFG back-off was utilized whenever a failure state occurred with CFG_Full, and CFG_Active+Back-off, where again a CFG back-off was utilized, this time with the back-off derived from the CFG trained on selective utterances. As our metric, we evaluated semantic accuracy since that is what matters most in C&amp;C settings. Furthermore, because recognition of part of an utterance can increase the odds of ultimately achieving task completion (Paek et al., 2007), we carried out separate evaluations for the functional constituents of a C&amp;C utterance (i.e., keyword and slot) as well as the complete phrase (keyword + slot). We computed accuracy as follows: For any single utterance, the recognizer can either accept or reject it. If it is accepted, then the semantics of the utterance can either be correct (i.e., it matches what the user intended) or incorrect, hence: accuracy = CA / (CA + IA + R) (1) where CA denotes accepted commands that are correct, IA denotes accepted commands that are incorrect, and R denotes the number of rejections. Table 2 display</context>
</contexts>
<marker>Paek, Gandhe, Chickering, Ju, 2007</marker>
<rawString>T. Paek, S. Gandhe, D. Chickering &amp; Y. Ju. 2007. Handling out-ofgrammar commands in mobile speech interaction using back-off filler models. In Proc. of ACL Workshop on Grammar-Based Approaches to Spoken Language Processing (SPEECHGRAM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>P Bouillon</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<date>2006</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="4008" citStr="Rayner et al., 2006" startWordPosition="599" endWordPosition="602">fy CFG either manually or automatically and go to step 1.1. To begin with, developers start with a CFG in Step 1. If they had access to a grammatical platform 2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opinions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred. 64 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64–67, Columbus, June 2008. c�2008 Association for Computational Linguistics such as Regulus (Rayner et al., 2006), they could in principle construct a CFG automatically for any new domain, though most developers will probably manually author the grammar. Two steps are added to the typical iterative process. In Step 1.1, we generate a back-off grammar from the CFG. One way to accomplish this is by constructing a backoff CFG using filler models (Paek et al., 2007), which when applied to the same command-andcontrol task in Section 4 can result in a 35% relative reduction in semantic error rate for OOG utterances. However, the back-off grammar could also be a SLM trained on artificial data created from the C</context>
</contexts>
<marker>Rayner, Hockey, Bouillon, 2006</marker>
<rawString>M. Rayner, B.A. Hockey, &amp; P. Bouillon. 2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>M Rahim</author>
<author>D Hakkani-Tur</author>
</authors>
<title>Active labeling for spoken language understanding.</title>
<date>2003</date>
<booktitle>In Proc. of Eurospeech.</booktitle>
<contexts>
<context position="7868" citStr="Tur et al., 2003" startWordPosition="1237" endWordPosition="1240">izes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost. We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off. Active learning aims at reducing transcription of training examples by selecting utterances that are most likely to be informative according to a specified cost function (Hakkani-Tur et al., 2002). In the speech community, active learning has been successfully applied to reducing the transcription effort for ASR (Hakkani-Tur et al., 2002), SLU (Tur et al., 2003b), as well as finding labeling errors (Tur et al., 2003). In our case, the examples are user utterances that need to be transcribed, and the learning involves modifying a CFG to achieve wider coverage of user expressions. Instead of passively transcribing everything and modifying the CFG as such, the grammar can “actively” participate in which utterances are transcribed. The usual procedure for selecting utterances for grammar modification is to transcribe at least all failed utterances, such as those that fall below a rejection threshold. By leveraging a back-off grammar, developers have mor</context>
</contexts>
<marker>Tur, Rahim, Hakkani-Tur, 2003</marker>
<rawString>G. Tur, M. Rahim &amp; D. Hakkani-Tur. 2003. Active labeling for spoken language understanding. In Proc. of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>R Schapire</author>
<author>D Hakkani-Tur</author>
</authors>
<title>Active learning for spoken language understanding.</title>
<date>2003</date>
<booktitle>In Proc. of ICASSP. W3C.</booktitle>
<contexts>
<context position="7868" citStr="Tur et al., 2003" startWordPosition="1237" endWordPosition="1240">izes back-off grammars to provide developers with a safety net for deploying applications earlier, and active learning to reduce transcription effort and cost. We now elaborate on active learning, demonstrate the concept with respect to a CFG back-off. Active learning aims at reducing transcription of training examples by selecting utterances that are most likely to be informative according to a specified cost function (Hakkani-Tur et al., 2002). In the speech community, active learning has been successfully applied to reducing the transcription effort for ASR (Hakkani-Tur et al., 2002), SLU (Tur et al., 2003b), as well as finding labeling errors (Tur et al., 2003). In our case, the examples are user utterances that need to be transcribed, and the learning involves modifying a CFG to achieve wider coverage of user expressions. Instead of passively transcribing everything and modifying the CFG as such, the grammar can “actively” participate in which utterances are transcribed. The usual procedure for selecting utterances for grammar modification is to transcribe at least all failed utterances, such as those that fall below a rejection threshold. By leveraging a back-off grammar, developers have mor</context>
</contexts>
<marker>Tur, Schapire, Hakkani-Tur, 2003</marker>
<rawString>G. Tur, R. Schapire, &amp; D. Hakkani-Tur. 2003b. Active learning for spoken language understanding. In Proc. of ICASSP. W3C. 2004. Speech Recognition Grammar Specification Version 1.0. http://www.w3.org/TR/speech-grammar</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>