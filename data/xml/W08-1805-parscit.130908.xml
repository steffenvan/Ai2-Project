<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008891">
<title confidence="0.998354">
A Data Driven Approach to Query Expansion in Question Answering
</title>
<author confidence="0.999915">
Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood
</author>
<affiliation confidence="0.997615">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.936767">
Regent Court, 211 Portobello
Sheffield S1 4DP UK
</address>
<email confidence="0.947033">
{aca00lad, acp07jw}@shef.ac.uk
{r.gaizauskas, m.greenwood}@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994732">
Automated answering of natural language
questions is an interesting and useful prob-
lem to solve. Question answering (QA)
systems often perform information re-
trieval at an initial stage. Information re-
trieval (IR) performance, provided by en-
gines such as Lucene, places a bound on
overall system performance. For example,
no answer bearing documents are retrieved
at low ranks for almost 40% of questions.
In this paper, answer texts from previous
QA evaluations held as part of the Text
REtrieval Conferences (TREC) are paired
with queries and analysed in an attempt
to identify performance-enhancing words.
These words are then used to evaluate the
performance of a query expansion method.
Data driven extension words were found
to help in over 70% of difficult questions.
These words can be used to improve and
evaluate query expansion methods. Sim-
ple blind relevance feedback (RF) was cor-
rectly predicted as unlikely to help overall
performance, and an possible explanation
is provided for its low value in IR for QA.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980475">
The task of supplying an answer to a question,
given some background knowledge, is often con-
sidered fairly trivial from a human point of view,
as long as the question is clear and the answer is
</bodyText>
<note confidence="0.723283">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967209">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.998696185185185">
known. The aim of an automated question answer-
ing system is to provide a single, unambiguous re-
sponse to a natural language question, given a text
collection as a knowledge source, within a certain
amount of time. Since 1999, the Text Retrieval
Conferences have included a task to evaluate such
systems, based on a large pre-defined corpus (such
as AQUAINT, containing around a million news
articles in English) and a set of unseen questions.
Many information retrieval systems perform
document retrieval, giving a list of potentially rel-
evant documents when queried – Google’s and Ya-
hoo!’s search products are examples of this type of
application. Users formulate a query using a few
keywords that represent the task they are trying to
perform; for example, one might search for “eif-
fel tower height” to determine how tall the Eiffel
tower is. IR engines then return a set of references
to potentially relevant documents.
In contrast, QA systems must return an exact an-
swer to the question. They should be confident
that the answer has been correctly selected; it is
no longer down to the user to research a set of doc-
ument references in order to discover the informa-
tion themselves. Further, the system takes a natural
language question as input, instead of a few user-
selected key terms.
</bodyText>
<listItem confidence="0.9740658">
Once a QA system has been provided with a
question, its processing steps can be described in
three parts - Question Pre-Processing, Text Re-
trieval and Answer Extraction:
1. Question Pre-Processing TREC questions
</listItem>
<bodyText confidence="0.946152">
are grouped into series which relate to a given
target. For example, the target may be “Hinden-
burg disaster” with questions such as “What type
of craft was the Hindenburg?” or “How fast could
it travel?”. Questions may include pronouns ref-
</bodyText>
<page confidence="0.982602">
34
</page>
<note confidence="0.9945405">
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 34–41
Manchester, UK. August 2008
</note>
<bodyText confidence="0.998725333333333">
erencing the target or even previous answers, and
as such require processing before they are suitable
for use.
</bodyText>
<listItem confidence="0.863052928571429">
2. Text Retrieval An IR component will return
a ranked set of texts, based on query terms. At-
tempting to understand and extract data from an
entire corpus is too resource intensive, and so an IR
engine defines a limited subset of the corpus that
is likely to contain answers. The question should
have been pre-processed correctly for a useful set
of texts to be retrieved – including anaphora reso-
lution.
3. Answer Extraction (AE) Given knowledge
about the question and a set of texts, the AE sys-
tem attempts to identify answers. It should be clear
that only answers within texts returned by the IR
component have any chance of being found.
</listItem>
<bodyText confidence="0.9879259">
Reduced performance at any stage will have a
knock-on effect, capping the performance of later
stages. If questions are left unprocessed and full
of pronouns (e.g.,“When did it sink?”) the IR com-
ponent has very little chance of working correctly
– in this case, the desired action is to retrieve
documents related to the Kursk submarine, which
would be impossible.
IR performance with a search engine such as
Lucene returns no useful documents for at least
35% of all questions – when looking at the top
20 returned texts. This caps the AE component
at 65% question “coverage”. We will measure the
performance of different IR component configura-
tions, to rule out problems with a default Lucene
setup.
For each question, answers are provided in the
form of regular expressions that match answer text,
and a list of documents containing these answers
in a correct context. As references to correct doc-
uments are available, it is possible to explore a
data-driven approach to query analysis. We deter-
mine which questions are hardest then concentrate
on identifying helpful terms found in correct doc-
uments, with a view to building a system than can
automatically extract these helpful terms from un-
seen questions and supporting corpus. The avail-
ability and usefulness of these terms will provide
an estimate of performance for query expansion
techniques.
There are at least two approaches which could
make use of these term sets to perform query ex-
pansion. They may occur in terms selected for
blind RF (non-blind RF is not applicable to the
TREC QA task). It is also possible to build a cata-
logue of terms known to be useful according to cer-
tain question types, thus leading to a dictionary of
(known useful) expansions that can be applied to
previously unseen questions. We will evaluate and
also test blind relevance feedback in IR for QA.
</bodyText>
<sectionHeader confidence="0.839914" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999515">
The performance of an IR system can be quanti-
fied in many ways. We choose and define mea-
sures pertinent to IR for QA. Work has been done
on relevance feedback specific to IR for QA, where
it is has usually be found to be unhelpful. We out-
line the methods used in the past, extend them, and
provide and test means of validating QA relevance
feedback.
</bodyText>
<subsectionHeader confidence="0.996812">
2.1 Measuring QA Performance
</subsectionHeader>
<bodyText confidence="0.998293375">
This paper uses two principle measures to describe
the performance of the IR component. Coverage
is defined as the proportion of questions where at
least one answer bearing text appears in the re-
trieved set. Redundancy is the average number
of answer bearing texts retrieved for each ques-
tion (Roberts and Gaizauskas, 2004).
Both these measures have a fixed limit n on the
number of texts retrieved by a search engine for a
query. As redundancy counts the number of texts
containing correct answers, and not instances of
the answer itself, it can never be greater than the
number of texts retrieved.
The TREC reference answers provide two ways
of finding a correct text, with both a regular expres-
sion and a document ID. Lenient hits (retrievals of
answer bearing documents) are those where the re-
trieved text matches the regular expression; strict
hits occur when the document ID of the retrieved
text matches that declared by TREC as correct and
the text matches the regular expression. Some doc-
uments will match the regular expression but not
be deemed as containing a correct answer (this
is common with numbers and dates (Baeza-Yates
and Ribeiro-Neto, 1999)), in which case a lenient
match is found, but not a strict one.
The answer lists as defined by TREC do not in-
clude every answer-bearing document – only those
returned by previous systems and marked as cor-
rect. Thus, false negatives are a risk, and strict
measures place an approximate lower bound on
the system’s actual performance. Similarly, lenient
</bodyText>
<page confidence="0.998959">
35
</page>
<bodyText confidence="0.5882455">
matches can occur out of context, without a sup-
porting document; performance based on lenient
matches can be viewed as an approximate upper
bound (Lin and Katz, 2005).
</bodyText>
<subsectionHeader confidence="0.998241">
2.2 Relevance Feedback
</subsectionHeader>
<bodyText confidence="0.999951125">
Relevance feedback is a widely explored technique
for query expansion. It is often done using a spe-
cific measure to select terms using a limited set of
ranked documents of size r; using a larger set will
bring term distribution closer to values over the
whole corpus, and away from ones in documents
relevant to query terms. Techniques are used to
identify phrases relevant to a query topic, in or-
der to reduce noise (such as terms with a low cor-
pus frequency that relate to only a single article)
and query drift (Roussinov and Fan, 2005; Allan,
1996).
In the context of QA, Pizzato (2006) employs
blind RF using the AQUAINT corpus in an attempt
to improve performance when answering factoid
questions on personal names. This is a similar ap-
proach to some content in this paper, though lim-
ited to the study of named entities, and does not
attempt to examine extensions from the existing
answer data.
Monz (2003) finds a negative result when apply-
ing blind feedback for QA in TREC 9, 10 and 11,
and a neutral result for TREC 7 and 8’s ad hoc re-
trieval tasks. Monz’s experiment, using r = 10
and standard Rocchio term weighting, also found
a further reduction in performance when r was
reduced (from 10 to 5). This is an isolated ex-
periment using just one measure on a limited set
of questions, with no use of the available answer
texts.
Robertson (1992) notes that there are issues
when using a whole document for feedback, as
opposed to just a single relevant passage; as men-
tioned in Section 3.1, passage- and document-level
retrieval sets must also be compared for their per-
formance at providing feedback. Critically, we
will survey the intersection between words known
to be helpful and blind RF terms based on initial
retrieval, thus showing exactly how likely an RF
method is to succeed.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999973714285714">
We first investigated the possibility of an IR-
component specific failure leading to impaired
coverage by testing a variety of IR engines and
configurations. Then, difficult questions were
identified, using various performance thresholds.
Next, answer bearing texts for these harder ques-
tions were checked for words that yielded a per-
formance increase when used for query expansion.
After this, we evaluated how likely a RF-based ap-
proach was to succeed. Finally, blind RF was ap-
plied to the whole question set. IR performance
was measured, and terms used for RF compared to
those which had proven to be helpful as extension
words.
</bodyText>
<subsectionHeader confidence="0.993543">
3.1 IR Engines
</subsectionHeader>
<bodyText confidence="0.99942252631579">
A QA framework (Greenwood, 2004a) was origi-
nally used to construct a QA system based on run-
ning a default Lucene installation. As this only
covers one IR engine in one configuration, it is
prudent to examine alternatives. Other IR engines
should be tested, using different configurations.
The chosen additional engines were: Indri, based
on the mature INQUERY engine and the Lemur
toolkit (Allan et al., 2003); and Terrier, a newer en-
gine designed to deal with corpora in the terabyte
range and to back applications entered into TREC
conferences (Ounis et al., 2005).
We also looked at both passage-level and
document-level retrieval. Passages can be de-
fined in a number of ways, such as a sentence,
a sliding window of k terms centred on the tar-
get term(s), parts of a document of fixed (and
equal) lengths, or a paragraph. In this case,
the documents in the AQUAINT corpus contain
paragraph markers which were used as passage-
level boundaries, thus making “passage-level”
and “paragraph-level” equivalent in this paper.
Passage-level retrieval may be preferable for AE,
as the number of potential distracters is some-
what reduced when compared to document-level
retrieval (Roberts and Gaizauskas, 2004).
The initial IR component configuration was with
Lucene indexing the AQUAINT corpus at passage-
level, with a Porter stemmer (Porter, 1980) and an
augmented version of the CACM (Jones and van
Rijsbergen, 1976) stopword list.
Indri natively supports document-level indexing
of TREC format corpora. Passage-level retrieval
was done using the paragraph tags defined in the
corpus as delimiters; this allows both passage- and
document-level retrieval from the same index, ac-
cording to the query.
All the IR engines were unified to use the Porter
</bodyText>
<page confidence="0.996216">
36
</page>
<table confidence="0.999556454545454">
Coverage Redundancy
Year Len. Strict Len. Strict
Lucene 2004 0.686 0.636 2.884 1.624
2005 0.703 0.566 2.780 1.155
2006 0.665 0.568 2.417 1.181
Indri 2004 0.690 0.554 3.849 1.527
2005 0.694 0.512 3.908 1.056
2006 0.691 0.552 3.373 1.152
Terrier 2004 - - - -
2005 - - - -
2006 0.638 0.493 2.520 1.000
</table>
<tableCaption confidence="0.990352">
Table 1: Performance of Lucene, Indri and Terrier at para-
graph level, over top 20 documents. This clearly shows the
limitations of the engines.
</tableCaption>
<bodyText confidence="0.999921931034482">
stemmer and the same CACM-derived stopword
list.
The top n documents for each question in the
TREC2004, TREC2005 and TREC2006 sets were
retrieved using every combination of engine, and
configuration1. The questions and targets were
processed to produce IR queries as per the default
configuration for the QA framework. Examining
the top 200 documents gave a good compromise
between the time taken to run experiments (be-
tween 30 and 240 minutes each) and the amount
one can mine into the data. Tabulated results are
shown in Table 1 and Table 2. Queries have had
anaphora resolution performed in the context of
their series by the QA framework. AE compo-
nents begin to fail due to excess noise when pre-
sented with over 20 texts, so this value is enough to
encompass typical operating parameters and leave
space for discovery (Greenwood et al., 2006).
A failure analysis (FA) tool, an early version
of which is described by (Sanka, 2005), provided
reporting and analysis of IR component perfor-
mance. In this experiment, it provided high level
comparison of all engines, measuring coverage
and redundancy as the number of documents re-
trieved, n, varies. This is measured because a per-
fect engine will return the most useful documents
first, followed by others; thus, coverage will be
higher for that engine with low values of n.
</bodyText>
<subsectionHeader confidence="0.99947">
3.2 Identification of Difficult Questions
</subsectionHeader>
<bodyText confidence="0.999379">
Once the performance of an IR configuration over
a question set is known, it’s possible to produce
a simple report listing redundancy for each ques-
tion. A performance reporting script accesses the
</bodyText>
<footnote confidence="0.80745525">
1Save Terrier / TREC2004 / passage-level retrieval;
passage-level retrieval with Terrier was very slow using our
configuration, and could not be reliably performed using the
same Terrier instance as document-level retrieval.
</footnote>
<table confidence="0.999773875">
Coverage Redundancy
Year Len. Strict Len. Strict
Indri 2004 0.926 0.837 7.841 2.663
2005 0.935 0.735 7.573 1.969
2006 0.882 0.741 6.872 1.958
Terrier 2004 0.919 0.806 7.186 2.380
2005 0.928 0.766 7.620 2.130
2006 0.983 0.783 6.339 2.067
</table>
<tableCaption confidence="0.9903055">
Table 2: Performance of Indri and Terrier at document level
IR over the AQUAINT corpus, with n = 20
</tableCaption>
<bodyText confidence="0.997761048780488">
FA tool’s database and lists all the questions in
a particular set with the strict and lenient redun-
dancy for selected engines and configurations. En-
gines may use passage- or document-level config-
urations.
Data on the performance of the three engines is
described in Table 2. As can be seen, the cover-
age with passage-level retrieval (which was often
favoured, as the AE component performs best with
reduced amounts of text) languishes between 51%
and 71%, depending on the measurement method.
Failed anaphora resolution may contribute to this
figure, though no deficiencies were found upon vi-
sual inspection.
Not all documents containing answers are noted,
only those checked by the NIST judges (Bilotti
et al., 2004). Match judgements are incomplete,
leading to the potential generation of false nega-
tives, where a correct answer is found with com-
plete supporting information, but as the informa-
tion has not been manually flagged, the system will
mark this as a failure. Assessment methods are
fully detailed in Dang et al. (2006). Factoid per-
formance is still relatively poor, although as only
1.95 documents match per question, this may be an
effect of such false negatives (Voorhees and Buck-
land, 2003). Work has been done into creating
synthetic corpora that include exhaustive answer
sets (Bilotti, 2004; Tellex et al., 2003; Lin and
Katz, 2005), but for the sake of consistency, and
easy comparison with both parallel work and prior
local results, the TREC judgements will be used to
evaluate systems in this paper.
Mean redundancy is also calculated for a num-
ber of IR engines. Difficult questions were those
for which no answer bearing texts were found by
either strict or lenient matches in any of the top n
documents, using a variety of engines. As soon as
one answer bearing document was found by an en-
gine using any measure, that question was deemed
non-difficult. Questions with mean redundancy of
</bodyText>
<page confidence="0.997181">
37
</page>
<bodyText confidence="0.99907525">
zero are marked difficult, and subjected to further
analysis. Reducing the question set to just diffi-
cult questions produces a TREC-format file for re-
testing the IR component.
</bodyText>
<subsectionHeader confidence="0.998722">
3.3 Extension of Difficult Questions
</subsectionHeader>
<bodyText confidence="0.999987701754386">
The documents deemed relevant by TREC must
contain some useful text that can help IR engine
performance. Such words should be revealed by
a gain in redundancy when used to extend an ini-
tially difficult query, usually signified by a change
from zero to a non-zero value (signifying that rele-
vant documents have been found where none were
before). In an attempt to identify where the use-
ful text is, the relevant documents for each difficult
question were retrieved, and passages matching the
answer regular expression identified. A script is
then used to build a list of terms from each passage,
removing words in the question or its target, words
that occur in the answer, and stopwords (based on
both the indexing stopword list, and a set of stems
common within the corpus). In later runs, num-
bers are also stripped out of the term list, as their
value is just as often confusing as useful (Baeza-
Yates and Ribeiro-Neto, 1999). Of course, answer
terms provide an obvious advantage that would not
be reproducible for questions where the answer is
unknown, and one of our goals is to help query ex-
pansion for unseen questions. This approach may
provide insights that will enable appropriate query
expansion where answers are not known.
Performance has been measured with both the
question followed by an extension (Q+E), as well
as the question followed by the target and then
extension candidates (Q+T+E). Runs were also
executed with just Q and Q+T, to provide non-
extended reference performance data points. Ad-
dition of the target often leads to gains in perfor-
mance (Roussinov et al., 2005), and may also aid
in cases where anaphora resolution has failed.
Some words are retained, such as titles, as in-
cluding these can be inferred from question or tar-
get terms and they will not unfairly boost redun-
dancy scores; for example, when searching for a
“Who” question containing the word “military”,
one may want to preserve appellations such as
“Lt.” or “Col.”, even if this term appears in the an-
swer.
This filtered list of extensions is then used to cre-
ate a revised query file, containing the base ques-
tion (with and without the target suffixed) as well
as new questions created by appending a candidate
extension word.
Results of retrievals with these new question are
loaded into the FA database and a report describ-
ing any performance changes is generated. The
extension generation process also creates custom
answer specifications, which replicate the informa-
tion found in the answers defined by TREC.
This whole process can be repeated with vary-
ing question difficulty thresholds, as well as alter-
native n values (typically from 5 to 100), different
engines, and various question sets.
</bodyText>
<subsectionHeader confidence="0.943078">
3.4 Relevance Feedback Performance
</subsectionHeader>
<bodyText confidence="0.999995882352941">
Now that we can find the helpful extension words
(HEWs) described earlier, we’re equipped to eval-
uate query expansion methods. One simplistic ap-
proach could use blind RF to determine candidate
extensions, and be considered potentially success-
ful should these words be found in the set of HEWs
for a query. For this, term frequencies can be
measured given the top r documents retrieved us-
ing anaphora-resolved query Q. After stopword
and question word removal, frequent terms are ap-
pended to Q, which is then re-evaluated. This
has been previously attempted for factoid ques-
tions (Roussinov et al., 2005) and with a limited
range of r values (Monz, 2003) but not validated
using a set of data-driven terms.
We investigated how likely term frequency (TF)
based RF is to discover HEWs. To do this, the
proportion of HEWs that occurred in initially re-
trieved texts was measured, as well as the propor-
tion of these texts containing at least one HEW.
Also, to see how effective an expansion method is,
suggested expansion terms can be checked against
the HEW list.
We used both the top 5 and the top 50 documents
in formulation of extension terms, with TF as a
ranking measure; 50 is significantly larger than the
optimal number of documents for AE (20), without
overly diluting term frequencies.
Problems have been found with using entire
documents for RF, as the topic may not be the
same throughout the entire discourse (Robertson
et al., 1992). Limiting the texts used for RF to
paragraphs may reduce noise; both document- and
paragraph-level terms should be checked.
</bodyText>
<page confidence="0.9986">
38
</page>
<table confidence="0.999149">
Engine
Year Lucene Indri Indri Terrier
Para Para Doc Doc
2004 76 72 37 42
2005 87 98 37 35
2006 108 118 59 53
</table>
<tableCaption confidence="0.819169666666667">
Table 3: Number of difficult questions, as defined by those
which have zero redundancy over both strict and lenient mea-
sures, at n = 20. Questions seem to get harder each year.
</tableCaption>
<table confidence="0.910271">
Document retrieval yields fewer difficult questions, as more
text is returned for potential matching.
Engine
Lucene Indri Terrier
Paragraph 226 221 -
Document - 121 109
</table>
<tableCaption confidence="0.986707">
Table 4: Number of difficult questions in the 2006 task, as de-
fined above, this time with n = 5. Questions become harder
as fewer chances are given to provide relevant documents.
</tableCaption>
<table confidence="0.9999175">
Difficult questions used 118
Variations tested 6683
Questions that benefited 87 (74.4%)
Helpful extension words (strict) 4973
Mean helpful words per question 42.144
Mean redundancy increase 3.958
</table>
<tableCaption confidence="0.90706275">
Table 6: Using Terrier Passage / strict matching, retrieving 20
docs, with TREC2006 questions / AQUAINT. Difficult ques-
tions are those where no strict matches are found in the top 20
IRT from just one engine.
</tableCaption>
<table confidence="0.99975525">
2004 2005 2006
HEW found in IRT 4.17% 18.58% 8.94%
IRT containing HEW 10.00% 33.33% 34.29%
RF words in HEW 1.25% 1.67% 5.71%
</table>
<tableCaption confidence="0.715342">
Table 7: “Helpful extension words”: the set of extensions that,
when added to the query, move redundancy above zero. r =
5, n = 20, using Indri at passage level.
</tableCaption>
<sectionHeader confidence="0.999687" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999991833333333">
Once we have HEWs, we can determine if these
are going to be of significant help when chosen as
query extensions. We can also determine if a query
expansion method is likely to be fruitful. Blind RF
was applied, and assessed using the helpful words
list, as well as RF’s effect on coverage.
</bodyText>
<subsectionHeader confidence="0.997294">
4.1 Difficult Question Analysis
</subsectionHeader>
<bodyText confidence="0.99978175">
The number of difficult questions found at n =
20 is shown in Table 3. Document-level retrieval
gave many fewer difficult questions, as the amount
of text retrieved gave a higher chance of finding
lenient matches. A comparison of strict and lenient
matching is in Table 5.
Extensions were then applied to difficult ques-
tions, with or without the target. The performance
of these extensions is shown in Table 6. Results
show a significant proportion (74.4%) of difficult
questions can benefit from being extended with
non-answer words found in answer bearing texts.
</bodyText>
<subsectionHeader confidence="0.999465">
4.2 Applying Relevance Feedback
</subsectionHeader>
<bodyText confidence="0.9978035">
Identifying HEWs provides a set of words that
are useful for evaluating potential expansion terms.
</bodyText>
<table confidence="0.9982148">
Match type
Strict Lenient
2004 39 49
Year 2005 56 66
2006 53 49
</table>
<tableCaption confidence="0.991337">
Table 5: Common difficult questions (over all three engines
mentioned above) by year and match type; n = 20.
</tableCaption>
<bodyText confidence="0.9998192">
Using simple TF based feedback (see Section 3.4),
5 terms were chosen per query. These words had
some intersection (see Table 7) with the exten-
sion words set, indicating that this RF may lead to
performance increases for previously unseen ques-
tions. Only a small number of the HEWs occur in
the initially retrieved texts (IRTs), although a no-
ticeable proportion of IRTs (up to 34.29%) contain
at least one HEW. However, these terms are prob-
ably not very frequent in the documents and un-
likely to be selected with TF-based blind RF. The
mean proportion of RF selected terms that were
HEWs was only 2.88%. Blind RF for question an-
swering fails here due to this low proportion. Strict
measures are used for evaluation as we are inter-
ested in finding documents which were not pre-
viously being retrieved rather than changes in the
distribution of keywords in IRT.
Document and passage based RF term selection
is used, to explore the effect of noise on terms, and
document based term selection proved marginally
superior. Choosing RF terms from a small set of
documents (r = 5) was found to be marginally
better than choosing from a larger set (r = 50).
In support of the suggestion that RF would be un-
</bodyText>
<table confidence="0.998493571428572">
r Baseline
5 50
Rank Doc Para Doc Para
5 0.253 0.251 0.240 0.179 0.312
10 0.331 0.347 0.331 0.284 0.434
20 0.438 0.444 0.438 0.398 0.553
50 0.583 0.577 0.577 0.552 0.634
</table>
<tableCaption confidence="0.993513">
Table 8: Coverage (strict) using blind RF. Both document-
and paragraph-level retrieval used to determine RF terms.
</tableCaption>
<page confidence="0.989116">
39
</page>
<table confidence="0.998755315789474">
Question:
Who was the nominal leader after the overthrow?
Target: Pakistani government overthrown in 1999
Extension word Redundancy
Kashmir 4
Pakistan 4
Islamabad 2.5
Question: Where did he play in college?
Target: Warren Moon
Extension word Redundancy
NFL 2.5
football 1
Question: Who have commanded the division?
Target: 82nd Airborne division
Extension word Redundancy
Gen 3
Col 2
decimated 2
officer 1
</table>
<tableCaption confidence="0.992037333333333">
Table 9: Queries with extensions, and their mean redundancy
using Indri at document level with n = 20. Without exten-
sions, redundancy is zero.
</tableCaption>
<bodyText confidence="0.9886735">
likely to locate HEWs, applying blind RF consis-
tently hampered overall coverage (Table 8).
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.998580403225807">
HEWs are often found in answer bearing texts,
though these are hard to identify through sim-
ple TF-based RF. A majority of difficult questions
can be made accessible through addition of HEWs
present in answer bearing texts, and work to deter-
mine a relationship between words found in initial
retrieval and these HEWs can lead to coverage in-
creases. HEWs also provide an effective means
of evaluating other RF methods, which can be de-
veloped into a generic rapid testing tool for query
expansion techniques. TF-based RF, while finding
some HEWs, is not effective at discovering exten-
sions, and reduces overall IR performance.
There was not a large performance change
between engines and configurations. Strict
paragraph-level coverage never topped 65%, leav-
ing a significant number of questions where no
useful information could be provided for AE.
The original sets of difficult questions for in-
dividual engines were small – often less than the
35% suggested when looking at the coverage fig-
ures. Possible causes could include:
Difficult questions being defined as those for
which average redundancy is zero: This limit
may be too low. To remedy this, we could increase
the redundancy limit to specify an arbitrary num-
ber of difficult questions out of the whole set.
The use of both strict and lenient measures: It
is possible to get a lenient match (thus marking a
question as non-difficult) when the answer text oc-
curs out of context.
Reducing n from 20 to 5 (Table 4) increased
the number of difficult questions produced. From
this we can hypothesise that although many search
engines are succeeding in returning useful docu-
ments (where available), the distribution of these
documents over the available ranks is not one that
bunches high ranking documents up as those im-
mediately retrieved (unlike a perfect engine; see
Section 3.1), but rather suggests a more even dis-
tribution of such documents over the returned set.
The number of candidate extension words for
queries (even after filtering) is often in the range
of hundreds to thousands. Each of these words
creates a separate query, and there are two varia-
tions, depending on whether the target is included
in the search terms or not. Thus, a large number
of extended queries need to be executed for each
question run. Passage-level retrieval returns less
text, which has two advantages: firstly, it reduces
the scope for false positives in lenient matching;
secondly, it is easier to scan result by eye and de-
termine why the engine selected a result.
Proper nouns are often helpful as extensions.
We noticed that these cropped up fairly regularly
for some kinds of question (e.g. “Who”). Espe-
cially useful were proper nouns associated with
locations - for example, adding “Pakistani” to
a query containing the word Pakistan lifted re-
dundancy above zero for a question on President
Musharraf, as in Table 9. This reconfirms work
done by Greenwood (2004b).
</bodyText>
<sectionHeader confidence="0.996993" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.997504533333333">
IR engines find some questions very difficult and
consistently fail to retrieve useful texts even with
high values of n. This behaviour is common over
many engines. Paragraph level retrieval seems to
give a better idea of which questions are hard-
est, although the possibility of false negatives is
present from answer lists and anaphora resolution.
Relationships exist between query words and
helpful words from answer documents (e.g. with
a military leadership themes in a query, adding the
term “general” or “gen” helps). Identification of
HEWs has potential use in query expansion. They
could be used to evaluate RF approaches, or asso-
ciated with question words and used as extensions.
Previous work has ruled out relevance feedback
</bodyText>
<page confidence="0.993477">
40
</page>
<bodyText confidence="0.999990041666667">
in particular circumstances using a single ranking
measure, though this has not been based on analy-
sis of answer bearing texts. The presence of HEWs
in IRT for difficult questions shows that guided RF
may work, but this will be difficult to pursue. Blind
RF based on term frequencies does not increase IR
performance. However, there is an intersection be-
tween words in initially retrieved texts and words
data driven analysis defines as helpful, showing
promise for alternative RF methods (e.g. based on
TFIDF). These extension words form a basis for
indicating the usefulness of RF and query expan-
sion techniques.
In this paper, we have chosen to explore only
one branch of query expansion. An alternative data
driven approach would be to build associations be-
tween recurrently useful terms given question con-
tent. Question texts could be stripped of stopwords
and proper nouns, and a list of HEWs associated
with each remaining term. To reduce noise, the
number of times a particular extension has helped
a word would be counted. Given sufficient sample
data, this would provide a reference body of HEWs
to be used as an aid to query expansion.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852614285714">
Allan, J., J. Callan, K. Collins-Thompson, B. Croft,
F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong,
P. Ogilvie, et al. 2003. The Lemur Toolkit for Lan-
guage Modeling and Information Retrieval.
Allan, J. 1996. Incremental Relevance Feedback for
Information Filtering. In Research and Development
in IR, pages 270–278.
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern
Information Retrieval. Addison Wesley.
Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works
Better for Question Answering: Stemming or Mor-
phological Query Expansion. Proc. IR for QA Work-
shop at SIGIR 2004.
Bilotti, M.W. 2004. Query Expansion Techniques for
Question Answering. Master’s thesis, Massachusetts
Institute of Technology.
Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of
the TREC 2006 QA track. Proc. 15th Text REtrieval
Conf..
Greenwood, M.A., M. Stevenson, and R. Gaizauskas.
2006. The University of Sheffield’s TREC 2006
Q&amp;A Experiments. In Proc. 15th Text REtrieval
Conference
Greenwood, M.A. 2004a. AnswerFinder: Question
Answering from your Desktop. In Proc. 7th Annual
Colloquium for the UK SIG for Computational Lin-
guistics (CLUK ’04).
Greenwood, M.A. 2004b. Using Pertainyms to Im-
prove Passage Retrieval for Questions Requesting
Information about a Location. In Proc. Workshop
on IR for QA (SIGIR 2004).
Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test
Collections. J. ofDocumentation, 32(1):59–75.
Lin, J. and B. Katz. 2005. Building a Reusable Test
Collection for Question Answering. J. American So-
ciety for Information Science and Technology.
Monz, C. 2003. From Document Retrieval to Question
Answering. ILLC Dissertation Series 2003, 4.
Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdon-
ald, and D. Johnson. 2005. Terrier IR Platform.
Proc. 27th European Conf. on IR (ECIR 05), San-
tiago de Compostela, Spain, pages 517–519.
Pizzato, L.A., D. Molla, and C. Paris. 2006. Pseudo-
Relevance Feedback using Named Entities for Ques-
tion Answering. Australasian Language Technology
Workshop (ALTW2006), pages 83–90.
Porter, M. 1980. An Algorithm for Suffix Stripping
Program. Program, 14(3):130–137.
Roberts, I and R Gaizauskas. 2004. Evaluating Passage
Retrieval Approaches for Question Answering. In
Proc. 26th European Conf. on IR.
Robertson, S.E., S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC. In
Text REtrieval Conf., pages 21–30.
Roussinov, D. and W. Fan. 2005. Discretization Based
Learning Approach to Information Retrieval. In
Proc. 2005 Conf. on Human Language Technologies.
Roussinov, D., M. Chau, E. Filatova, and J.A. Robles-
Flores. 2005. Building on Redundancy: Fac-
toid Question Answering, Robust Retrieval and the
‘Other’. In Proc. 14th Text REtrieval Conf.
Sanka, Atheesh. 2005. Passage Retrieval for Question
Answering. Master’s thesis, University of Sheffield.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative Evaluation of Passage Retrieval
Algorithms for Question Answering. Proc. 26th An-
nual Int’l ACM SIGIR Conf. on R&amp;D in IR, pages
41–47.
Voorhees, E. and L. P. Buckland, editors. 2003. Proc.
12th Text REtrieval Conference.
</reference>
<page confidence="0.999444">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250816">
<title confidence="0.999642">A Data Driven Approach to Query Expansion in Question Answering</title>
<author confidence="0.991087">Leon Derczynski</author>
<author confidence="0.991087">Jun Wang</author>
<author confidence="0.991087">Robert Gaizauskas</author>
<author confidence="0.991087">A Mark</author>
<affiliation confidence="0.9925875">Department of Computer University of</affiliation>
<address confidence="0.521364">Regent Court, 211 Sheffield S1 4DP</address>
<abstract confidence="0.996080230769231">Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions. In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method. Data driven extension words were found to help in over 70% of difficult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Callan</author>
<author>K Collins-Thompson</author>
<author>B Croft</author>
<author>F Feng</author>
<author>D Fisher</author>
<author>J Lafferty</author>
<author>L Larkey</author>
<author>TN Truong</author>
<author>P Ogilvie</author>
</authors>
<title>The Lemur Toolkit for Language Modeling and Information Retrieval.</title>
<date>2003</date>
<contexts>
<context position="11186" citStr="Allan et al., 2003" startWordPosition="1864" endWordPosition="1867">h was to succeed. Finally, blind RF was applied to the whole question set. IR performance was measured, and terms used for RF compared to those which had proven to be helpful as extension words. 3.1 IR Engines A QA framework (Greenwood, 2004a) was originally used to construct a QA system based on running a default Lucene installation. As this only covers one IR engine in one configuration, it is prudent to examine alternatives. Other IR engines should be tested, using different configurations. The chosen additional engines were: Indri, based on the mature INQUERY engine and the Lemur toolkit (Allan et al., 2003); and Terrier, a newer engine designed to deal with corpora in the terabyte range and to back applications entered into TREC conferences (Ounis et al., 2005). We also looked at both passage-level and document-level retrieval. Passages can be defined in a number of ways, such as a sentence, a sliding window of k terms centred on the target term(s), parts of a document of fixed (and equal) lengths, or a paragraph. In this case, the documents in the AQUAINT corpus contain paragraph markers which were used as passagelevel boundaries, thus making “passage-level” and “paragraph-level” equivalent in </context>
</contexts>
<marker>Allan, Callan, Collins-Thompson, Croft, Feng, Fisher, Lafferty, Larkey, Truong, Ogilvie, 2003</marker>
<rawString>Allan, J., J. Callan, K. Collins-Thompson, B. Croft, F. Feng, D. Fisher, J. Lafferty, L. Larkey, TN Truong, P. Ogilvie, et al. 2003. The Lemur Toolkit for Language Modeling and Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
</authors>
<title>Incremental Relevance Feedback for Information Filtering.</title>
<date>1996</date>
<booktitle>In Research and Development in IR,</booktitle>
<pages>270--278</pages>
<contexts>
<context position="8875" citStr="Allan, 1996" startWordPosition="1474" endWordPosition="1475">te upper bound (Lin and Katz, 2005). 2.2 Relevance Feedback Relevance feedback is a widely explored technique for query expansion. It is often done using a specific measure to select terms using a limited set of ranked documents of size r; using a larger set will bring term distribution closer to values over the whole corpus, and away from ones in documents relevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found a further red</context>
</contexts>
<marker>Allan, 1996</marker>
<rawString>Allan, J. 1996. Incremental Relevance Feedback for Information Filtering. In Research and Development in IR, pages 270–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Baeza-Yates</author>
<author>B Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="7776" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="1281" endWordPosition="1284">swer itself, it can never be greater than the number of texts retrieved. The TREC reference answers provide two ways of finding a correct text, with both a regular expression and a document ID. Lenient hits (retrievals of answer bearing documents) are those where the retrieved text matches the regular expression; strict hits occur when the document ID of the retrieved text matches that declared by TREC as correct and the text matches the regular expression. Some documents will match the regular expression but not be deemed as containing a correct answer (this is common with numbers and dates (Baeza-Yates and Ribeiro-Neto, 1999)), in which case a lenient match is found, but not a strict one. The answer lists as defined by TREC do not include every answer-bearing document – only those returned by previous systems and marked as correct. Thus, false negatives are a risk, and strict measures place an approximate lower bound on the system’s actual performance. Similarly, lenient 35 matches can occur out of context, without a supporting document; performance based on lenient matches can be viewed as an approximate upper bound (Lin and Katz, 2005). 2.2 Relevance Feedback Relevance feedback is a widely explored technique for</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
<author>B Katz</author>
<author>J Lin</author>
</authors>
<title>What Works Better for Question Answering: Stemming or Morphological Query Expansion.</title>
<date>2004</date>
<booktitle>Proc. IR for QA Workshop at SIGIR</booktitle>
<contexts>
<context position="15810" citStr="Bilotti et al., 2004" startWordPosition="2620" endWordPosition="2623">dundancy for selected engines and configurations. Engines may use passage- or document-level configurations. Data on the performance of the three engines is described in Table 2. As can be seen, the coverage with passage-level retrieval (which was often favoured, as the AE component performs best with reduced amounts of text) languishes between 51% and 71%, depending on the measurement method. Failed anaphora resolution may contribute to this figure, though no deficiencies were found upon visual inspection. Not all documents containing answers are noted, only those checked by the NIST judges (Bilotti et al., 2004). Match judgements are incomplete, leading to the potential generation of false negatives, where a correct answer is found with complete supporting information, but as the information has not been manually flagged, the system will mark this as a failure. Assessment methods are fully detailed in Dang et al. (2006). Factoid performance is still relatively poor, although as only 1.95 documents match per question, this may be an effect of such false negatives (Voorhees and Buckland, 2003). Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex</context>
</contexts>
<marker>Bilotti, Katz, Lin, 2004</marker>
<rawString>Bilotti, M.W., B. Katz, and J. Lin. 2004. What Works Better for Question Answering: Stemming or Morphological Query Expansion. Proc. IR for QA Workshop at SIGIR 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M W Bilotti</author>
</authors>
<title>Query Expansion Techniques for Question Answering. Master’s thesis,</title>
<date>2004</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="16402" citStr="Bilotti, 2004" startWordPosition="2718" endWordPosition="2719">Bilotti et al., 2004). Match judgements are incomplete, leading to the potential generation of false negatives, where a correct answer is found with complete supporting information, but as the information has not been manually flagged, the system will mark this as a failure. Assessment methods are fully detailed in Dang et al. (2006). Factoid performance is still relatively poor, although as only 1.95 documents match per question, this may be an effect of such false negatives (Voorhees and Buckland, 2003). Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex et al., 2003; Lin and Katz, 2005), but for the sake of consistency, and easy comparison with both parallel work and prior local results, the TREC judgements will be used to evaluate systems in this paper. Mean redundancy is also calculated for a number of IR engines. Difficult questions were those for which no answer bearing texts were found by either strict or lenient matches in any of the top n documents, using a variety of engines. As soon as one answer bearing document was found by an engine using any measure, that question was deemed non-difficult. Questions with mean redundancy </context>
</contexts>
<marker>Bilotti, 2004</marker>
<rawString>Bilotti, M.W. 2004. Query Expansion Techniques for Question Answering. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Dang</author>
<author>J Lin</author>
<author>D Kelly</author>
</authors>
<date>2006</date>
<booktitle>Overview of the TREC 2006 QA track. Proc. 15th Text REtrieval Conf..</booktitle>
<contexts>
<context position="16124" citStr="Dang et al. (2006)" startWordPosition="2672" endWordPosition="2675">s of text) languishes between 51% and 71%, depending on the measurement method. Failed anaphora resolution may contribute to this figure, though no deficiencies were found upon visual inspection. Not all documents containing answers are noted, only those checked by the NIST judges (Bilotti et al., 2004). Match judgements are incomplete, leading to the potential generation of false negatives, where a correct answer is found with complete supporting information, but as the information has not been manually flagged, the system will mark this as a failure. Assessment methods are fully detailed in Dang et al. (2006). Factoid performance is still relatively poor, although as only 1.95 documents match per question, this may be an effect of such false negatives (Voorhees and Buckland, 2003). Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex et al., 2003; Lin and Katz, 2005), but for the sake of consistency, and easy comparison with both parallel work and prior local results, the TREC judgements will be used to evaluate systems in this paper. Mean redundancy is also calculated for a number of IR engines. Difficult questions were those for which no a</context>
</contexts>
<marker>Dang, Lin, Kelly, 2006</marker>
<rawString>Dang, H.T., J. Lin, and D. Kelly. 2006. Overview of the TREC 2006 QA track. Proc. 15th Text REtrieval Conf..</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Greenwood</author>
<author>M Stevenson</author>
<author>R Gaizauskas</author>
</authors>
<title>The University of Sheffield’s TREC</title>
<date>2006</date>
<booktitle>In Proc. 15th Text REtrieval Conference</booktitle>
<contexts>
<context position="13813" citStr="Greenwood et al., 2006" startWordPosition="2299" endWordPosition="2302">ts were processed to produce IR queries as per the default configuration for the QA framework. Examining the top 200 documents gave a good compromise between the time taken to run experiments (between 30 and 240 minutes each) and the amount one can mine into the data. Tabulated results are shown in Table 1 and Table 2. Queries have had anaphora resolution performed in the context of their series by the QA framework. AE components begin to fail due to excess noise when presented with over 20 texts, so this value is enough to encompass typical operating parameters and leave space for discovery (Greenwood et al., 2006). A failure analysis (FA) tool, an early version of which is described by (Sanka, 2005), provided reporting and analysis of IR component performance. In this experiment, it provided high level comparison of all engines, measuring coverage and redundancy as the number of documents retrieved, n, varies. This is measured because a perfect engine will return the most useful documents first, followed by others; thus, coverage will be higher for that engine with low values of n. 3.2 Identification of Difficult Questions Once the performance of an IR configuration over a question set is known, it’s p</context>
</contexts>
<marker>Greenwood, Stevenson, Gaizauskas, 2006</marker>
<rawString>Greenwood, M.A., M. Stevenson, and R. Gaizauskas. 2006. The University of Sheffield’s TREC 2006 Q&amp;A Experiments. In Proc. 15th Text REtrieval Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Greenwood</author>
</authors>
<title>AnswerFinder: Question Answering from your Desktop.</title>
<date>2004</date>
<booktitle>In Proc. 7th Annual Colloquium for the UK SIG for Computational Linguistics (CLUK ’04).</booktitle>
<contexts>
<context position="10808" citStr="Greenwood, 2004" startWordPosition="1804" endWordPosition="1805">ic failure leading to impaired coverage by testing a variety of IR engines and configurations. Then, difficult questions were identified, using various performance thresholds. Next, answer bearing texts for these harder questions were checked for words that yielded a performance increase when used for query expansion. After this, we evaluated how likely a RF-based approach was to succeed. Finally, blind RF was applied to the whole question set. IR performance was measured, and terms used for RF compared to those which had proven to be helpful as extension words. 3.1 IR Engines A QA framework (Greenwood, 2004a) was originally used to construct a QA system based on running a default Lucene installation. As this only covers one IR engine in one configuration, it is prudent to examine alternatives. Other IR engines should be tested, using different configurations. The chosen additional engines were: Indri, based on the mature INQUERY engine and the Lemur toolkit (Allan et al., 2003); and Terrier, a newer engine designed to deal with corpora in the terabyte range and to back applications entered into TREC conferences (Ounis et al., 2005). We also looked at both passage-level and document-level retriev</context>
<context position="29092" citStr="Greenwood (2004" startWordPosition="4863" endWordPosition="4864">ieval returns less text, which has two advantages: firstly, it reduces the scope for false positives in lenient matching; secondly, it is easier to scan result by eye and determine why the engine selected a result. Proper nouns are often helpful as extensions. We noticed that these cropped up fairly regularly for some kinds of question (e.g. “Who”). Especially useful were proper nouns associated with locations - for example, adding “Pakistani” to a query containing the word Pakistan lifted redundancy above zero for a question on President Musharraf, as in Table 9. This reconfirms work done by Greenwood (2004b). 6 Conclusion and Future Work IR engines find some questions very difficult and consistently fail to retrieve useful texts even with high values of n. This behaviour is common over many engines. Paragraph level retrieval seems to give a better idea of which questions are hardest, although the possibility of false negatives is present from answer lists and anaphora resolution. Relationships exist between query words and helpful words from answer documents (e.g. with a military leadership themes in a query, adding the term “general” or “gen” helps). Identification of HEWs has potential use in</context>
</contexts>
<marker>Greenwood, 2004</marker>
<rawString>Greenwood, M.A. 2004a. AnswerFinder: Question Answering from your Desktop. In Proc. 7th Annual Colloquium for the UK SIG for Computational Linguistics (CLUK ’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Greenwood</author>
</authors>
<title>Using Pertainyms to Improve Passage Retrieval for Questions Requesting Information about a Location. In</title>
<date>2004</date>
<booktitle>Proc. Workshop on IR for QA (SIGIR</booktitle>
<contexts>
<context position="10808" citStr="Greenwood, 2004" startWordPosition="1804" endWordPosition="1805">ic failure leading to impaired coverage by testing a variety of IR engines and configurations. Then, difficult questions were identified, using various performance thresholds. Next, answer bearing texts for these harder questions were checked for words that yielded a performance increase when used for query expansion. After this, we evaluated how likely a RF-based approach was to succeed. Finally, blind RF was applied to the whole question set. IR performance was measured, and terms used for RF compared to those which had proven to be helpful as extension words. 3.1 IR Engines A QA framework (Greenwood, 2004a) was originally used to construct a QA system based on running a default Lucene installation. As this only covers one IR engine in one configuration, it is prudent to examine alternatives. Other IR engines should be tested, using different configurations. The chosen additional engines were: Indri, based on the mature INQUERY engine and the Lemur toolkit (Allan et al., 2003); and Terrier, a newer engine designed to deal with corpora in the terabyte range and to back applications entered into TREC conferences (Ounis et al., 2005). We also looked at both passage-level and document-level retriev</context>
<context position="29092" citStr="Greenwood (2004" startWordPosition="4863" endWordPosition="4864">ieval returns less text, which has two advantages: firstly, it reduces the scope for false positives in lenient matching; secondly, it is easier to scan result by eye and determine why the engine selected a result. Proper nouns are often helpful as extensions. We noticed that these cropped up fairly regularly for some kinds of question (e.g. “Who”). Especially useful were proper nouns associated with locations - for example, adding “Pakistani” to a query containing the word Pakistan lifted redundancy above zero for a question on President Musharraf, as in Table 9. This reconfirms work done by Greenwood (2004b). 6 Conclusion and Future Work IR engines find some questions very difficult and consistently fail to retrieve useful texts even with high values of n. This behaviour is common over many engines. Paragraph level retrieval seems to give a better idea of which questions are hardest, although the possibility of false negatives is present from answer lists and anaphora resolution. Relationships exist between query words and helpful words from answer documents (e.g. with a military leadership themes in a query, adding the term “general” or “gen” helps). Identification of HEWs has potential use in</context>
</contexts>
<marker>Greenwood, 2004</marker>
<rawString>Greenwood, M.A. 2004b. Using Pertainyms to Improve Passage Retrieval for Questions Requesting Information about a Location. In Proc. Workshop on IR for QA (SIGIR 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jones</author>
<author>C J van Rijsbergen</author>
</authors>
<date>1976</date>
<journal>IR Test Collections. J. ofDocumentation,</journal>
<pages>32--1</pages>
<marker>Jones, van Rijsbergen, 1976</marker>
<rawString>Jones, K.S. and C.J. van Rijsbergen. 1976. IR Test Collections. J. ofDocumentation, 32(1):59–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>B Katz</author>
</authors>
<title>Building a Reusable Test Collection for Question Answering.</title>
<date>2005</date>
<journal>J. American Society for Information Science and Technology.</journal>
<contexts>
<context position="8298" citStr="Lin and Katz, 2005" startWordPosition="1370" endWordPosition="1373">ning a correct answer (this is common with numbers and dates (Baeza-Yates and Ribeiro-Neto, 1999)), in which case a lenient match is found, but not a strict one. The answer lists as defined by TREC do not include every answer-bearing document – only those returned by previous systems and marked as correct. Thus, false negatives are a risk, and strict measures place an approximate lower bound on the system’s actual performance. Similarly, lenient 35 matches can occur out of context, without a supporting document; performance based on lenient matches can be viewed as an approximate upper bound (Lin and Katz, 2005). 2.2 Relevance Feedback Relevance feedback is a widely explored technique for query expansion. It is often done using a specific measure to select terms using a limited set of ranked documents of size r; using a larger set will bring term distribution closer to values over the whole corpus, and away from ones in documents relevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA,</context>
<context position="16444" citStr="Lin and Katz, 2005" startWordPosition="2724" endWordPosition="2727">nts are incomplete, leading to the potential generation of false negatives, where a correct answer is found with complete supporting information, but as the information has not been manually flagged, the system will mark this as a failure. Assessment methods are fully detailed in Dang et al. (2006). Factoid performance is still relatively poor, although as only 1.95 documents match per question, this may be an effect of such false negatives (Voorhees and Buckland, 2003). Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex et al., 2003; Lin and Katz, 2005), but for the sake of consistency, and easy comparison with both parallel work and prior local results, the TREC judgements will be used to evaluate systems in this paper. Mean redundancy is also calculated for a number of IR engines. Difficult questions were those for which no answer bearing texts were found by either strict or lenient matches in any of the top n documents, using a variety of engines. As soon as one answer bearing document was found by an engine using any measure, that question was deemed non-difficult. Questions with mean redundancy of 37 zero are marked difficult, and subje</context>
</contexts>
<marker>Lin, Katz, 2005</marker>
<rawString>Lin, J. and B. Katz. 2005. Building a Reusable Test Collection for Question Answering. J. American Society for Information Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
</authors>
<title>From Document Retrieval to Question Answering.</title>
<date>2003</date>
<booktitle>ILLC Dissertation Series</booktitle>
<pages>4</pages>
<contexts>
<context position="9235" citStr="Monz (2003)" startWordPosition="1536" endWordPosition="1537">elevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found a further reduction in performance when r was reduced (from 10 to 5). This is an isolated experiment using just one measure on a limited set of questions, with no use of the available answer texts. Robertson (1992) notes that there are issues when using a whole document for feedback, as opposed to just a single relevant passage; as mentioned in Section 3.1, passage- and </context>
<context position="20600" citStr="Monz, 2003" startWordPosition="3425" endWordPosition="3426"> words (HEWs) described earlier, we’re equipped to evaluate query expansion methods. One simplistic approach could use blind RF to determine candidate extensions, and be considered potentially successful should these words be found in the set of HEWs for a query. For this, term frequencies can be measured given the top r documents retrieved using anaphora-resolved query Q. After stopword and question word removal, frequent terms are appended to Q, which is then re-evaluated. This has been previously attempted for factoid questions (Roussinov et al., 2005) and with a limited range of r values (Monz, 2003) but not validated using a set of data-driven terms. We investigated how likely term frequency (TF) based RF is to discover HEWs. To do this, the proportion of HEWs that occurred in initially retrieved texts was measured, as well as the proportion of these texts containing at least one HEW. Also, to see how effective an expansion method is, suggested expansion terms can be checked against the HEW list. We used both the top 5 and the top 50 documents in formulation of extension terms, with TF as a ranking measure; 50 is significantly larger than the optimal number of documents for AE (20), with</context>
</contexts>
<marker>Monz, 2003</marker>
<rawString>Monz, C. 2003. From Document Retrieval to Question Answering. ILLC Dissertation Series 2003, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Ounis</author>
<author>G Amati</author>
<author>V Plachouras</author>
<author>B He</author>
<author>C Macdonald</author>
<author>D Johnson</author>
</authors>
<date>2005</date>
<booktitle>Terrier IR Platform. Proc. 27th European Conf. on IR (ECIR 05),</booktitle>
<pages>517--519</pages>
<location>Santiago de Compostela, Spain,</location>
<contexts>
<context position="11343" citStr="Ounis et al., 2005" startWordPosition="1891" endWordPosition="1894">ven to be helpful as extension words. 3.1 IR Engines A QA framework (Greenwood, 2004a) was originally used to construct a QA system based on running a default Lucene installation. As this only covers one IR engine in one configuration, it is prudent to examine alternatives. Other IR engines should be tested, using different configurations. The chosen additional engines were: Indri, based on the mature INQUERY engine and the Lemur toolkit (Allan et al., 2003); and Terrier, a newer engine designed to deal with corpora in the terabyte range and to back applications entered into TREC conferences (Ounis et al., 2005). We also looked at both passage-level and document-level retrieval. Passages can be defined in a number of ways, such as a sentence, a sliding window of k terms centred on the target term(s), parts of a document of fixed (and equal) lengths, or a paragraph. In this case, the documents in the AQUAINT corpus contain paragraph markers which were used as passagelevel boundaries, thus making “passage-level” and “paragraph-level” equivalent in this paper. Passage-level retrieval may be preferable for AE, as the number of potential distracters is somewhat reduced when compared to document-level retr</context>
</contexts>
<marker>Ounis, Amati, Plachouras, He, Macdonald, Johnson, 2005</marker>
<rawString>Ounis, I., G. Amati, V. Plachouras, B. He, C. Macdonald, and D. Johnson. 2005. Terrier IR Platform. Proc. 27th European Conf. on IR (ECIR 05), Santiago de Compostela, Spain, pages 517–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Pizzato</author>
<author>D Molla</author>
<author>C Paris</author>
</authors>
<title>PseudoRelevance Feedback using Named Entities for Question Answering. Australasian Language Technology Workshop</title>
<date>2006</date>
<pages>83--90</pages>
<marker>Pizzato, Molla, Paris, 2006</marker>
<rawString>Pizzato, L.A., D. Molla, and C. Paris. 2006. PseudoRelevance Feedback using Named Entities for Question Answering. Australasian Language Technology Workshop (ALTW2006), pages 83–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An Algorithm for Suffix Stripping Program.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="12117" citStr="Porter, 1980" startWordPosition="2015" endWordPosition="2016"> centred on the target term(s), parts of a document of fixed (and equal) lengths, or a paragraph. In this case, the documents in the AQUAINT corpus contain paragraph markers which were used as passagelevel boundaries, thus making “passage-level” and “paragraph-level” equivalent in this paper. Passage-level retrieval may be preferable for AE, as the number of potential distracters is somewhat reduced when compared to document-level retrieval (Roberts and Gaizauskas, 2004). The initial IR component configuration was with Lucene indexing the AQUAINT corpus at passagelevel, with a Porter stemmer (Porter, 1980) and an augmented version of the CACM (Jones and van Rijsbergen, 1976) stopword list. Indri natively supports document-level indexing of TREC format corpora. Passage-level retrieval was done using the paragraph tags defined in the corpus as delimiters; this allows both passage- and document-level retrieval from the same index, according to the query. All the IR engines were unified to use the Porter 36 Coverage Redundancy Year Len. Strict Len. Strict Lucene 2004 0.686 0.636 2.884 1.624 2005 0.703 0.566 2.780 1.155 2006 0.665 0.568 2.417 1.181 Indri 2004 0.690 0.554 3.849 1.527 2005 0.694 0.512</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. 1980. An Algorithm for Suffix Stripping Program. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Roberts</author>
<author>R Gaizauskas</author>
</authors>
<title>Evaluating Passage Retrieval Approaches for Question Answering. In</title>
<date>2004</date>
<booktitle>Proc. 26th European Conf. on IR.</booktitle>
<contexts>
<context position="6937" citStr="Roberts and Gaizauskas, 2004" startWordPosition="1139" endWordPosition="1142"> and define measures pertinent to IR for QA. Work has been done on relevance feedback specific to IR for QA, where it is has usually be found to be unhelpful. We outline the methods used in the past, extend them, and provide and test means of validating QA relevance feedback. 2.1 Measuring QA Performance This paper uses two principle measures to describe the performance of the IR component. Coverage is defined as the proportion of questions where at least one answer bearing text appears in the retrieved set. Redundancy is the average number of answer bearing texts retrieved for each question (Roberts and Gaizauskas, 2004). Both these measures have a fixed limit n on the number of texts retrieved by a search engine for a query. As redundancy counts the number of texts containing correct answers, and not instances of the answer itself, it can never be greater than the number of texts retrieved. The TREC reference answers provide two ways of finding a correct text, with both a regular expression and a document ID. Lenient hits (retrievals of answer bearing documents) are those where the retrieved text matches the regular expression; strict hits occur when the document ID of the retrieved text matches that declare</context>
<context position="11979" citStr="Roberts and Gaizauskas, 2004" startWordPosition="1992" endWordPosition="1995">so looked at both passage-level and document-level retrieval. Passages can be defined in a number of ways, such as a sentence, a sliding window of k terms centred on the target term(s), parts of a document of fixed (and equal) lengths, or a paragraph. In this case, the documents in the AQUAINT corpus contain paragraph markers which were used as passagelevel boundaries, thus making “passage-level” and “paragraph-level” equivalent in this paper. Passage-level retrieval may be preferable for AE, as the number of potential distracters is somewhat reduced when compared to document-level retrieval (Roberts and Gaizauskas, 2004). The initial IR component configuration was with Lucene indexing the AQUAINT corpus at passagelevel, with a Porter stemmer (Porter, 1980) and an augmented version of the CACM (Jones and van Rijsbergen, 1976) stopword list. Indri natively supports document-level indexing of TREC format corpora. Passage-level retrieval was done using the paragraph tags defined in the corpus as delimiters; this allows both passage- and document-level retrieval from the same index, according to the query. All the IR engines were unified to use the Porter 36 Coverage Redundancy Year Len. Strict Len. Strict Lucene </context>
</contexts>
<marker>Roberts, Gaizauskas, 2004</marker>
<rawString>Roberts, I and R Gaizauskas. 2004. Evaluating Passage Retrieval Approaches for Question Answering. In Proc. 26th European Conf. on IR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
<author>M Hancock-Beaulieu</author>
<author>A Gull</author>
<author>M Lau</author>
</authors>
<title>Okapi at TREC.</title>
<date>1992</date>
<booktitle>In Text REtrieval Conf.,</booktitle>
<pages>21--30</pages>
<contexts>
<context position="21388" citStr="Robertson et al., 1992" startWordPosition="3560" endWordPosition="3563">that occurred in initially retrieved texts was measured, as well as the proportion of these texts containing at least one HEW. Also, to see how effective an expansion method is, suggested expansion terms can be checked against the HEW list. We used both the top 5 and the top 50 documents in formulation of extension terms, with TF as a ranking measure; 50 is significantly larger than the optimal number of documents for AE (20), without overly diluting term frequencies. Problems have been found with using entire documents for RF, as the topic may not be the same throughout the entire discourse (Robertson et al., 1992). Limiting the texts used for RF to paragraphs may reduce noise; both document- and paragraph-level terms should be checked. 38 Engine Year Lucene Indri Indri Terrier Para Para Doc Doc 2004 76 72 37 42 2005 87 98 37 35 2006 108 118 59 53 Table 3: Number of difficult questions, as defined by those which have zero redundancy over both strict and lenient measures, at n = 20. Questions seem to get harder each year. Document retrieval yields fewer difficult questions, as more text is returned for potential matching. Engine Lucene Indri Terrier Paragraph 226 221 - Document - 121 109 Table 4: Number </context>
</contexts>
<marker>Robertson, Walker, Hancock-Beaulieu, Gull, Lau, 1992</marker>
<rawString>Robertson, S.E., S. Walker, M. Hancock-Beaulieu, A. Gull, and M. Lau. 1992. Okapi at TREC. In Text REtrieval Conf., pages 21–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roussinov</author>
<author>W Fan</author>
</authors>
<title>Discretization Based Learning Approach to Information Retrieval.</title>
<date>2005</date>
<booktitle>In Proc. 2005 Conf. on Human Language Technologies.</booktitle>
<contexts>
<context position="8861" citStr="Roussinov and Fan, 2005" startWordPosition="1470" endWordPosition="1473">be viewed as an approximate upper bound (Lin and Katz, 2005). 2.2 Relevance Feedback Relevance feedback is a widely explored technique for query expansion. It is often done using a specific measure to select terms using a limited set of ranked documents of size r; using a larger set will bring term distribution closer to values over the whole corpus, and away from ones in documents relevant to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found</context>
</contexts>
<marker>Roussinov, Fan, 2005</marker>
<rawString>Roussinov, D. and W. Fan. 2005. Discretization Based Learning Approach to Information Retrieval. In Proc. 2005 Conf. on Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roussinov</author>
<author>M Chau</author>
<author>E Filatova</author>
<author>J A RoblesFlores</author>
</authors>
<title>Building on Redundancy: Factoid Question Answering, Robust Retrieval and the ‘Other’. In</title>
<date>2005</date>
<booktitle>Proc. 14th Text REtrieval Conf.</booktitle>
<contexts>
<context position="18812" citStr="Roussinov et al., 2005" startWordPosition="3126" endWordPosition="3129">us advantage that would not be reproducible for questions where the answer is unknown, and one of our goals is to help query expansion for unseen questions. This approach may provide insights that will enable appropriate query expansion where answers are not known. Performance has been measured with both the question followed by an extension (Q+E), as well as the question followed by the target and then extension candidates (Q+T+E). Runs were also executed with just Q and Q+T, to provide nonextended reference performance data points. Addition of the target often leads to gains in performance (Roussinov et al., 2005), and may also aid in cases where anaphora resolution has failed. Some words are retained, such as titles, as including these can be inferred from question or target terms and they will not unfairly boost redundancy scores; for example, when searching for a “Who” question containing the word “military”, one may want to preserve appellations such as “Lt.” or “Col.”, even if this term appears in the answer. This filtered list of extensions is then used to create a revised query file, containing the base question (with and without the target suffixed) as well as new questions created by appending</context>
<context position="20550" citStr="Roussinov et al., 2005" startWordPosition="3413" endWordPosition="3416">eedback Performance Now that we can find the helpful extension words (HEWs) described earlier, we’re equipped to evaluate query expansion methods. One simplistic approach could use blind RF to determine candidate extensions, and be considered potentially successful should these words be found in the set of HEWs for a query. For this, term frequencies can be measured given the top r documents retrieved using anaphora-resolved query Q. After stopword and question word removal, frequent terms are appended to Q, which is then re-evaluated. This has been previously attempted for factoid questions (Roussinov et al., 2005) and with a limited range of r values (Monz, 2003) but not validated using a set of data-driven terms. We investigated how likely term frequency (TF) based RF is to discover HEWs. To do this, the proportion of HEWs that occurred in initially retrieved texts was measured, as well as the proportion of these texts containing at least one HEW. Also, to see how effective an expansion method is, suggested expansion terms can be checked against the HEW list. We used both the top 5 and the top 50 documents in formulation of extension terms, with TF as a ranking measure; 50 is significantly larger than</context>
</contexts>
<marker>Roussinov, Chau, Filatova, RoblesFlores, 2005</marker>
<rawString>Roussinov, D., M. Chau, E. Filatova, and J.A. RoblesFlores. 2005. Building on Redundancy: Factoid Question Answering, Robust Retrieval and the ‘Other’. In Proc. 14th Text REtrieval Conf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atheesh Sanka</author>
</authors>
<title>Passage Retrieval for Question Answering. Master’s thesis,</title>
<date>2005</date>
<institution>University of Sheffield.</institution>
<contexts>
<context position="13900" citStr="Sanka, 2005" startWordPosition="2316" endWordPosition="2317">mining the top 200 documents gave a good compromise between the time taken to run experiments (between 30 and 240 minutes each) and the amount one can mine into the data. Tabulated results are shown in Table 1 and Table 2. Queries have had anaphora resolution performed in the context of their series by the QA framework. AE components begin to fail due to excess noise when presented with over 20 texts, so this value is enough to encompass typical operating parameters and leave space for discovery (Greenwood et al., 2006). A failure analysis (FA) tool, an early version of which is described by (Sanka, 2005), provided reporting and analysis of IR component performance. In this experiment, it provided high level comparison of all engines, measuring coverage and redundancy as the number of documents retrieved, n, varies. This is measured because a perfect engine will return the most useful documents first, followed by others; thus, coverage will be higher for that engine with low values of n. 3.2 Identification of Difficult Questions Once the performance of an IR configuration over a question set is known, it’s possible to produce a simple report listing redundancy for each question. A performance </context>
</contexts>
<marker>Sanka, 2005</marker>
<rawString>Sanka, Atheesh. 2005. Passage Retrieval for Question Answering. Master’s thesis, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>B Katz</author>
<author>J Lin</author>
<author>A Fernandes</author>
<author>G Marton</author>
</authors>
<title>Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering.</title>
<date>2003</date>
<booktitle>Proc. 26th Annual Int’l ACM SIGIR Conf. on R&amp;D in IR,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="16423" citStr="Tellex et al., 2003" startWordPosition="2720" endWordPosition="2723"> 2004). Match judgements are incomplete, leading to the potential generation of false negatives, where a correct answer is found with complete supporting information, but as the information has not been manually flagged, the system will mark this as a failure. Assessment methods are fully detailed in Dang et al. (2006). Factoid performance is still relatively poor, although as only 1.95 documents match per question, this may be an effect of such false negatives (Voorhees and Buckland, 2003). Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex et al., 2003; Lin and Katz, 2005), but for the sake of consistency, and easy comparison with both parallel work and prior local results, the TREC judgements will be used to evaluate systems in this paper. Mean redundancy is also calculated for a number of IR engines. Difficult questions were those for which no answer bearing texts were found by either strict or lenient matches in any of the top n documents, using a variety of engines. As soon as one answer bearing document was found by an engine using any measure, that question was deemed non-difficult. Questions with mean redundancy of 37 zero are marked</context>
</contexts>
<marker>Tellex, Katz, Lin, Fernandes, Marton, 2003</marker>
<rawString>Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton. 2003. Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering. Proc. 26th Annual Int’l ACM SIGIR Conf. on R&amp;D in IR, pages 41–47.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>Proc. 12th Text REtrieval Conference.</booktitle>
<editor>Voorhees, E. and L. P. Buckland, editors.</editor>
<contexts>
<context position="9235" citStr="(2003)" startWordPosition="1537" endWordPosition="1537">nt to query terms. Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) employs blind RF using the AQUAINT corpus in an attempt to improve performance when answering factoid questions on personal names. This is a similar approach to some content in this paper, though limited to the study of named entities, and does not attempt to examine extensions from the existing answer data. Monz (2003) finds a negative result when applying blind feedback for QA in TREC 9, 10 and 11, and a neutral result for TREC 7 and 8’s ad hoc retrieval tasks. Monz’s experiment, using r = 10 and standard Rocchio term weighting, also found a further reduction in performance when r was reduced (from 10 to 5). This is an isolated experiment using just one measure on a limited set of questions, with no use of the available answer texts. Robertson (1992) notes that there are issues when using a whole document for feedback, as opposed to just a single relevant passage; as mentioned in Section 3.1, passage- and </context>
</contexts>
<marker>2003</marker>
<rawString>Voorhees, E. and L. P. Buckland, editors. 2003. Proc. 12th Text REtrieval Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>