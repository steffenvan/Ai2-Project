<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001638">
<note confidence="0.818643">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 209-218, Lisbon, Portugal, 2000.
</note>
<title confidence="0.990512">
The Acquisition of Word Order by
a Computational Learning System
</title>
<author confidence="0.997625">
Aline Villavicencio
</author>
<affiliation confidence="0.999402">
Computer Laboratory, University of Cambridge
</affiliation>
<address confidence="0.792668">
New Museums Site, Cambridge, CB2 3QG, England, UK
</address>
<email confidence="0.833642">
Aline.Villavicencioacl.cam.ac.uk
</email>
<sectionHeader confidence="0.974616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994173913043">
The purpose of this work is to investigate the
process of grammatical acquisition from data.
We are using a computational learning sys-
tem that is composed of a Universal Grammar
with associated parameters, and a learning al-
gorithm, following the Principles and Parame-
ters Theory. The Universal Grammar is imple-
mented as a Unification-Based Generalised Cat-
egorial Grammar, embedded in a default inher-
itance network of lexical types. The learning al-
gorithm receives input from a corpus annotated
with logical forms and sets the parameters based
on this input. This framework is used as basis
to investigate several aspects of language acqui-
sition. In this paper we are concentrating on the
acquisition of word order for different learners.
The results obtained show the different learners
having a similar performance and converging to-
wards the target grammar given the input data
available, regardless of their starting points. It
also shows how the amount of noise present in
the input data affects the speed of convergence
of the learners towards the target.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999932333333334">
In trying to solve the question of how to get a
machine to automatically learn linguistic infor-
mation from data, we can look at the way people
do it. Gold (1967) when investigating language
identification in the limit, obtained results that
implied that natural languages could not be
learned only on the basis of positive evidence.
These results were used as a confirmation for the
proposal that children must have some innate
knowledge about language, the Universal Gram-
mar (UG), to help them overcome the prob-
lem of the poverty of the stimulus and acquire
a grammar on the basis of positive evidence
only. According to Chomsky&apos;s Principles and
Parameters Theory (Chomsky 1981), the UG
is composed of principles and parameters, and
the process of learning a language is regarded
as the setting of values of a number of parame-
ters, given exposure to this particular language.
We employ this idea in the learning framework
implemented.
In this work we are interested in investigating
the acquisition of grammatical knowledge from
data, focusing on the acquisition of word or-
der, that reflects the underlying order in which
constituents occur in different languages (e.g.
SVO and SOV languages). The learning sys-
tem is equipped with a UG and associated pa-
rameters, encoded as a Unification-Based Gen-
eralised Categorial Grammar, and a learning al-
gorithm that fixes the values of the parameters
to a particular language. The learning algo-
rithm follows the Bayesian Incremental Param-
eter Setting (BIPS) algorithm (Briscoe 1999),
and when setting the parameters it uses a Mini-
mum Description Length (MDL) style bias to
choose the most probable grammar that de-
scribes the data well, given the goal of converg-
ing to the target grammar. In section 2 we de-
scribe the components of the learning system.
In section 3, we investigate the acquisition of
word order within this framework and discuss
the results obtained by different learners. Fi-
nally we present some conclusions and future
work.
</bodyText>
<sectionHeader confidence="0.969481" genericHeader="method">
2 The Learning System
</sectionHeader>
<bodyText confidence="0.9998368">
The learning system is composed of a language
learner equipped with a UG and a learning al-
gorithm that updates the initial parameter set-
tings, based on exposure to a corpus of utter-
ances. Each of these components is discussed in
</bodyText>
<page confidence="0.997501">
209
</page>
<bodyText confidence="0.96728">
more detail in the following sections.
</bodyText>
<subsectionHeader confidence="0.879662">
2.1 The Universal Grammar
</subsectionHeader>
<bodyText confidence="0.99994246">
The UG consists of principles and parame-
ters, and the latter are set according to the
linguistic environment (Chomsky 1981). This
proposal suggests that human languages follow
a common set of principles and differ among
one another only in finitely many respects, rep-
resented by a finite number of parameters that
can vary according to a finite number of val-
ues (which makes them learnable in Gold&apos;s
paradigm). In this section, we discuss the
UG and associated parameters, which are for-
malised in terms of a Unification-Based Gen-
eralised Categorial Grammar (UB-GCG), em-
bedded in a default inheritance network of lex-
ical types. We concentrate on the description
of word order parameters, which reflect the ba-
sic order in which constituents occur in different
languages.
UB-GCGs extend the basic Categorial Gram-
mars ((Bar Hillel, 1964)) by including the use of
attribute-value pairs associated with each cate-
gory and by using a larger set of rules and op-
erators. Words, categories and rules are repre-
sented in terms of typed default feature struc-
tures (TDFss), that encode orthographic, syn-
tactic and semantic information. There are
two types of categories: atomic categories (s
- sentence-, np - noun phrase-, and n -
noun), that are saturated, and complex cat-
egories, that are unsaturated. Complex cate-
gories have a functor category (defined in RE-
SULT), and a list of subcategorised elements (de-
fined in ACTIVE), with each element in the list
defined in terms of two features: SIGN, encoding
the category, and DIRECTION, encoding the di-
rection in which the category is to be combined
(where VALUE can be either forward or back-
ward). As an example, in English an intransi-
tive verb (s \ np) is encoded as shown in figure
1, where only the relevant attributes are shown.
In this work, we employ the rules of (forward
and backward) application, (forward and back-
ward) composition and generalised weak permu-
tation. A more detailed description of the UB-
GCG used can be found in (Villavicencio 2000).
The UG is implemented as a UB-GCG, em-
bedded in a default inheritance network of lex-
ical types (Villavicencio 1999), implemented in
the YADU framework (Lascarides and Copes-
take 1999). The categories and rules in the
grammar are defined as types in the hierarchy,
represented in terms of TDFSS and the feature-
structures associated with any given category
or rule are defined by the inheritance chain.
With different sub-networks used to encode dif-
ferent kinds of linguistic knowledge, linguistic
regularities are encoded near the top of a net-
work, while types further down the network are
used to represent sub-regularities or exceptions.
Thus, types are concisely defined, with only
specific information being described, since more
general information is inherited from the super-
types. The resulting UB-GCG is compact, since
it avoids redundant specifications and the infor-
mation is structured in a clear and concise way
through the specification of linguistic regulari-
ties and sub-regularities and exceptions.
Regarding the categories of the UB-GCG,
word order parameters are those that specify
the direction of each element in the subcate-
gorisation list of a complex category. In figure
1, subjdir is a parameter specifying that the
np subject is to be combined backwards. As the
categories are defined in terms of an inheritance
hierarchy, the parameters (and their values) in
these categories are propagated throughout the
hierarchy, from supertypes to subtypes, which
inherit this information by default. There are 28
parameters defined, and they are also in a hier-
archical relationship, with the supertype being
gendir, which specifies, by default, the general
direction for a language, and from which all the
other parameters inherit. Among the subtypes,
we have subjdir, which specifies the direction
of the subject, vargdir, which specifies the di-
rection of the other verbal arguments and ndir,
which specifies the direction of nominal cate-
gories. A fragment of the parameters hierarchy
can be seen in figure 2. With these 28 binary-
valued parameters the UG defines a space of
almost 800 grammars.
The parameters are set based on exposure to
a particular language, and while they are un-
set, they inherit their value by default, from
their supertypes. Then, when they are set, they
can either continue to inherit by default, in case
they have the same value as the supertype, or
they can override this default and specify their
own value, breaking the inheritance chain. For
instance, in the case of English, the value of
</bodyText>
<page confidence="0.997801">
210
</page>
<figure confidence="0.811891333333333">
intransitive
RESULT SIGN s
ACTIVE SIGN np
[DIRECTION:
VALUE
VALUE . backward
</figure>
<figureCaption confidence="0.995637">
Figure 1: Intransitive Verb type
</figureCaption>
<figure confidence="0.969031">
gendir
subjdir vargdir ndir
nmdir detdir
</figure>
<figureCaption confidence="0.9214805">
Figure 2: Fragment of The Parameters Hierar-
chy
</figureCaption>
<bodyText confidence="0.99997515">
gendir is defined, by default, as forward, cap-
turing the fact that it is a predominantly right-
branching language, and all its subtypes, like
subjdir and vargdir inherit this default in-
formation. Then an intransitive verb, which
has the direction of the subject specified by
subjdir, will be defined as s/NP, with sub-
jdir having default value forward. However,
as in English, the subject NP occurs to the left
of the verb, utterances with the subject to the
left will trigger a change in subjdir to back-
ward, which overrides the default value, break-
ing the inheritance chain, figure 3. As a re-
sult, intransitive verbs are defined as s \NP, fig-
ure 1, for the grammar to account for these
sentences. In the syntactic dimension of this
network, intransitive verbs can be considered
the general case of verbs, and the information
defined in this node is propagated through the
hierarchy to its subtypes, such as the transitive
verbs, figure 3. For the learner, the information
about subjects (subjdir = backward) has al-
ready been acquired while learning intransitive
verbs, and the learner does not need to learn
it again for transitive verbs, which not only in-
herit this information, but also have the direc-
tion for the object defined by vargdir (vargdir
= forward), as shown in figure 3. The use of
a default inheritance schema reduces the pieces
of information to be acquired by the learner,
since the information is structured and what it
learns is not a single isolated category, but a
structure that represents this information in a
general manner. This is a clear and concise way
of defining the UG with the parameters being
straightforwardly defined in the categories, in
a way that takes advantage of the default in-
heritance mechanism, to propagate information
about parameters, throughout the lexical inher-
itance network.
</bodyText>
<subsectionHeader confidence="0.990225">
2.2 The Corpus
</subsectionHeader>
<bodyText confidence="0.999996444444444">
The UG has to be general enough to capture
the grammar for any language, and the param-
eters have to be set to account for a particular
language, based on exposure to that language.
This can be obtained by means of a corpus of
utterances, annotated with logical forms, which
is described in this section. Among these sen-
tences, some will be triggers for certain param-
eters, in the sense that, to parse that sentence,
some of the parameters will have to be set to
a given value. We are using the Sachs cor-
pus (Sachs 1983) from the CHILDES project
(MacWhinney 1995), that contains interactions
between only one child and her parents, from
the age of 1 year and 1 month to 5 years and 1
month. From the resulting corpus, we extracted
material for generating two different corpora:
one containing only the child&apos;s sentences and
the other containing the caretakers&apos; sentences.
The caretakers&apos; corpus is given as input to the
learner to mirror the input to which a child
learning a language is exposed. And the child&apos;s
corpus is used for comparative purposes.
In order to annotate the caretakers&apos; corpus
with the associated logical forms, a UB-GCG
for English was built, that covers all the con-
structions in the corpus: several verbal con-
</bodyText>
<page confidence="0.988566">
211
</page>
<bodyText confidence="0.9672809">
top
complex
intransitive
.....................
oblique intransitive-control
(s\np)/pp (s\np)/(s\np)
ndir vargdir subjdir = \
... . . . ......................
nmdir detdir. ... .................
rs\np)/np
</bodyText>
<figure confidence="0.481495">
ditransitive oblique-transitive transitive-control
((s\np)/np)/np ((s\np)/np)/pp ((s\np)/np)/(s\np)
</figure>
<figureCaption confidence="0.999652">
Figure 3: A Fragment of the Network of Types
</figureCaption>
<bodyText confidence="0.99991396">
structions (intransitives, transitives, ditransi-
tives, obliques, control verbs, verbs with senten-
tial complements, etc), declarative, imperative
and interrogative sentences, and unbounded de-
pendencies (wh-questions and relative clauses),
among others. Thus the caretakers&apos; corpus con-
tains sentences annotated with logical forms,
and an example can be seen in figure 4, for the
sentence / will take him, where a simplified ver-
sion of the relevant attributes is shown, for rea-
sons of clarity. Each predicate in the semantics
list is associated with a word in the sentence,
and, among other things, it contains informa-
tion about the identifier of the predicate (SIT),
the required arguments (e.g. ACTOR and UN-
DERGOER for the verb take), as well as about
the interaction with other predicates, specified
by the boxed indices (e.g. take:AcToR = o =
i:siT). This grammar is not only used for anno-
tating the corpus, but is also the target to which
the learner has to converge. At the moment
around 1,300 utterances were annotated with
corresponding logical forms, with data ranging
from when the child is 14 months old to 20
months old.
</bodyText>
<subsectionHeader confidence="0.999856">
2.3 The Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.9999925">
The learning algorithm implements the
Bayesian Incremental Parameter Setting
(BIPS) algorithm defined by Briscoe (1999).
The parameters are binary-valued, where each
possible value in a parameter is associated with
a prior and a posterior probability. The value
with highest posterior probability is used as
the current value. Initially, in the learning
process, the posterior probability associated
with each parameter is initialised to the prior
probability, and these values are going to define
the parameter settings used. Then, as trigger
sentences are successfully parsed, the posterior
probabilities of the parameter settings that al-
lowed the sentence to be parsed are reinforced.
Otherwise, when a sentence cannot be parsed
(with the correct logical form) the learning
algorithm checks if a successful parse can be
achieved by changing the values of some of the
parameters, in constrained ways. If that is the
case, the posterior probability of the values
used are reinforced in each of the parameters,
and if they achieve a certain threshold, they
are retained as the current values, otherwise
the previous values are kept. This constraint
on the setting of the parameters ensures that a
trigger does not cause an immediate change to
a different grammar. The learner, instead, has
to wait for enough evidence in the data before
it can change the value of any parameter. As
a consequence, the learner behaves in a more
conservative way, being robust to noise present
in the input data.
Following Briscoe (1999) the probabilities as-
sociated with the parameter values correspond
to weights represented in terms of fractions,
with the denominator storing the total evidence
for a parameter and the numerator storing the
evidence for a particular value of that param-
eter. For instance, if the value backward of
the subjdir parameter has a weight of 9/10,
it means that from 10 times that evidence was
provided for subjdir, 9 times it was for the
value backward, and only once for the other
value, forward. Table 1 shows a possible ini-
tialisation for the subjdir parameter, where the
prior has a weight of 1/10 for forward, corre-
sponding to a probability of 0.1, and a weight of
</bodyText>
<page confidence="0.989728">
212
</page>
<figure confidence="0.641194666666667">
sign
ORTH &lt;i, will, take, him&gt;
CAT
</figure>
<figureCaption confidence="0.982013">
Figure 4: Sentence: I will take him
</figureCaption>
<figure confidence="0.993411083333333">
SEM
El
El
take
I&apos; SAIF i0 I .
El
UNDERGOER
will
SIT po
ARGUMENT
i him
SIT ig &apos; [ SIT
</figure>
<bodyText confidence="0.99445555">
9/10 for backward, corresponding to a proba-
bility of 0.9. The posterior is initialised with the
same values as the prior, and as backward has
a higher posterior probability it is used as the
current value for the parameter. These initial
parameter values determine the initial gram-
mar for the learner. As triggers are processed,
they provide evidence for certain parameters
and these are represented as additions to the
denominator and/or numerator of each of the
posterior weights of the parameter values. Ta-
ble 2 shows the status of the parameter after
5 triggers that provided evidence for the value
backward. Initially, the learner uses the evi-
dence provided by the triggers to choose certain
parameter values, in order to be able to parse
these triggers successfully while generating the
appropriate logical form. After that, the trig-
gers are used to reinforce these values, or to
negate them.
</bodyText>
<tableCaption confidence="0.999544">
Table 1: Initialisation of a Parameter
</tableCaption>
<table confidence="0.998207">
Value Prior Posterior
Prob. Weight Prob. Weight
Forward 0.1 1 0.1 1
113 10
Backward 0.9 9 0.9 9
10 10
</table>
<bodyText confidence="0.999857777777778">
and specifies its own value, breaking the inheri-
tance chain. For instance, in figure 3, sub jdir
overrides the default value specified by gendir,
breaking the inheritance chain. Unset subtype
parameters inherit, by default, the current value
of their supertypes, and while they are unset
they do not influence the values of their super-
types.
As the parameters are defined in a default
inheritance hierarchy, each time the posterior
probability of a given parameter is updated, it
is necessary to update the posterior probabili-
ties of its supertypes and examine the current
parameter settings to determine what the most
appropriate hierarchy for these settings is, given
the goal of converging to the target. The learner
has a preference for grammars (and thus hi-
erarchies) that not only model the data (rep-
resented by the current settings) well, but are
also compact, following the Minimum Descrip-
tion Length (MDL) Principle. In this case, the
most probable grammar in the grammar space,
among the ones consistent with the parameter
settings, is the one where the default inheritance
hierarchy is the more concise, having the min-
imum number of non-default parameter values
specified, as described in (Villavicencio 2000).
</bodyText>
<tableCaption confidence="0.966244">
Table 2: Status of the Parameter
</tableCaption>
<bodyText confidence="0.999769444444444">
The 28 word order parameters are defined in
a hierarchical relation, with the supertype pa-
rameters being set in accordance with the sub-
types, to reflect the value of the majority of the
subtypes. In this way, as the values of the sub-
types are being set, they influence the value of
the supertypes. If the value of a given sub-
type differs from the value of the supertype,
the subtype overrides the inherited default value
</bodyText>
<table confidence="0.914648333333333">
Value Prior Posterior
Prob. Weight Prob. Weight
Forward 0.1 Po 0.07 1
15
Backward 0.9 I 0.93 14-
15
</table>
<page confidence="0.999099">
213
</page>
<sectionHeader confidence="0.943821" genericHeader="method">
3 The Acquisition of Word Order
</sectionHeader>
<bodyText confidence="0.999987968253968">
We are investigating the acquisition of word
order, which reflects the underlying order in
which constituents occur in different languages.
In this section we describe one experiment,
where we compare the performance of differ-
ent learners under four conditions. Each learner
is given as input the annotated corpus of sen-
tences paired with logical forms, and they have
to change the values of the parameters corre-
sponding to the relevant constituents to account
for the order in which these constituents ap-
pear in the input sentences. We defined five
different learners corresponding to five differ-
ent initialisations of the parameter settings of
the UG, to investigate how the initialisations,
or starting points, of the learners influence con-
vergence to the target grammar. The first one,
the unset learner, is initialised with all param-
eters unset, and the others, the default learn-
ers, are each initialised with default parameter
values corresponding to one of four basic word
orders, defined in terms of the canonical order
of the verb (V), subject (S) and objects (0):
SVO, SOV, VSO and OVS. We initialised the
parameters sub jdir, vargdir and gendir of the
default learners according to each of the basic
orders, with gendir having the same direction
as vargdir, and all the other parameters hav-
ing unset values. These parameters have the
prior and posterior probabilities initialised with
0.1 for one value and 0.9 for the other. In this
way, an SVO learner, for example, is initialised
with subjdir having as current value backward
(0.9), vargdir forward (0.9) and gendir for-
ward (0.9).
The sentences in the input corpus are pre-
sented to a learner only once, sequentially, in
the original order. The input to a learner is
pre-processed by a system [Waldron, 2000] that
assigns categories to each word in a sentence.
The sentences with their putative category as-
signments are given as input to the learner. The
learner then evaluates the category assignments
for each sentence and only uses those that are
valid according to the UG to set the parame-
ters; the others are discarded. The corpus con-
tains 1,041 English sentences (which follow the
SVO order), but from these only a small propor-
tion are triggers for the parameters, in the sense
that, for the learner to process them, it has to
select certain parameter values. As each trigger-
ing sentence is processed, the learner changes or
reinforces its parameter values to reflect the or-
der of constituents in these sentences.
We wanted to check how the different learners
performed in a normal noisy environment, with
a limited corpus as input, and also to check if
there is an interaction between the different ith-
tialisations and the noise in the input data. To
do that we tested how the learners performed
under four conditions. Each condition was run
10 times for each learner, and we report here
the average results obtained.
</bodyText>
<subsectionHeader confidence="0.9975275">
3.1 Condition 1: Learners-10 in a
Noisy Environment
</subsectionHeader>
<bodyText confidence="0.999465333333333">
In the first condition, we initialised the param-
eters subjdir, vargdir and gendir of the de-
fault learners with the prior and posterior prob-
abilities of 0.1 corresponding to a weight of
1/10, and probabilities of 0.9 to a weight of
9/10. Results from the first experiment can be
seen in table 3, where the learners are specified
in the first column, the number of input triggers
in the second, the number of correct parameters
in relation to the target is in the third, and the
number of parameters that are set with these
triggers is in the fourth column.
</bodyText>
<tableCaption confidence="0.9428225">
Table 3: Convergence of the different learners -
Learners-10
</tableCaption>
<table confidence="0.999347">
Learners Triggers Parameters Parameters
Correct Set
Unset 179 22.3 10.5
SVO-10 211.4 22.5 11
SOV-10 205.4 22.2 10.2
OVS-10 271.5 22.5 11
VSO-10 198.7 22.1 10.2
</table>
<bodyText confidence="0.999901">
The results show no significant variation in
the performance of the different Learners. This
is the case with the number of parameters that
are correct in relation to the target, with an av-
erage of 22.3 parameters out of 28, and also with
the number of parameters that are set given the
triggers available, with an average of 10.5 pa-
rameters out of 28.
The only difference between the learners was
</bodyText>
<page confidence="0.998626">
214
</page>
<figureCaption confidence="0.999837">
Figure 5: Convergence of Subjdir - Learners-10 - Noisy Environment
</figureCaption>
<figure confidence="0.987434736842105">
Posterior Probability
0.9
0.8
0.7
0.6
0.5
Unset
SVO-10
SOV-10
OVS-10
- - - VSO-10
CO
CO
7.7)
Subjdir - Noisy Environment
CO 1- CO 1- CO 1-
CO r&amp;quot;-- 0) 0)N
Triggers
CO
</figure>
<bodyText confidence="0.9995903">
the time needed for each learner to converge:
the closer the starting point of the learner was
to the target, the faster it converged, as can
be seen in figure 5, for the subjdir parame-
ter. This figure shows all the learners converg-
ing to the target value, with high probability,
and with a convergence pattern very similar to
the one presented by the unset learner. Even
those default learners that were initialised with
values incompatible with the target soon over-
came this initial bias and converged to the tar-
get. The same thing happens for vargdir and
gendir. This figure also shows some sharp falls
in the convergence to the target value, for these
learners. For example, the unset learner had a
sharp drop in probability, which fell from 0.94
to 0.85, around trigger 16. These declines were
caused by noise in the category assignments of
the input triggers, which provided incorrect ev-
idence for the parameter values.
</bodyText>
<subsectionHeader confidence="0.9991975">
3.2 Condition 2: Learners-10 in a
Noise-free Environment
</subsectionHeader>
<bodyText confidence="0.9984221">
In order to test if and how much of the learn-
ers&apos; performance was affected by the presence
of noisy triggers, using the same initialisations
as the ones in condition 1, we tested how the
learners performed in a noise-free environment.
To obtain such an environment, as each trigger
was processed, a module was used for correcting
the category assignment, if noise was detected.
The results are shown in table 4.
These learners have performances similar to
</bodyText>
<tableCaption confidence="0.8930365">
Table 4: Convergence of the different learners -
Learners-10 - Noise-free
</tableCaption>
<table confidence="0.998372142857143">
Learners Triggers Parameters Parameters
Correct Set
Unset 235.1 22.3 10.6
SVO-10 227.9 22.3 10.6
SOV-10 213.9 22.6 11.2
OVS-10 212.2 22.3 10.6
VSO-10 172.4 22 10
</table>
<bodyText confidence="0.999078818181818">
those in condition 1 (section 3.1), with an av-
erage of 22.3 of the 28 parameters correct in
relation to the target, and an average of 10.6 pa-
rameters that can be set with the triggers avail-
able. But, in this condition the convergence was
slightly faster for all learners, as can be seen in
figure 6. These results show that, indeed, the
presence of noise slows down the convergence of
the learners, because they need more triggers to
compensate for the effect produced by the noisy
triggers.
</bodyText>
<subsectionHeader confidence="0.998937">
3.3 Condition 3: Learners-50 in a
Noisy Environment
</subsectionHeader>
<bodyText confidence="0.9999078">
We then tested if the use of stronger weights
to initialise the learners would affect the learn-
ers performance. The parameters subjdir,
vargdir and gendir were initialised with a
weight of 5/50 for the probability of 0.1 and a
</bodyText>
<page confidence="0.998986">
215
</page>
<figureCaption confidence="0.999145">
Figure 6: Convergence of Subjdir - Learners-10 - Noise-free Environment
</figureCaption>
<figure confidence="0.67158925">
Subjdir - Noise-free Environment
LO 0) CO N II) 0) CO N. 1— cr) 0) CO
CV LO r&apos;s &amp;quot;-- CM .7 1.0 CO CO
Triggers
</figure>
<bodyText confidence="0.982692590909091">
weight of 45/50 for the probability of 0.9. These
weights provide an extreme bias for each of the
learners. In this condition, the learners were
tested again in a normal noisy environment.
Figure 7 shows the convergence patterns pre-
sented by these learners for the subjdir param-
eter. The effect produced by the noise was in-
creased with these stronger weights, such that
all the learners had a slower convergence to the
target. Even those default learners initialised
with values compatible with the target had a
slightly slower convergence when compared to
those in condition 1, with weaker weights, be-
cause they had to overcome the stronger initial
bias before converging to the target values. But,
in spite of that, the performance of the learners
is only slightly affected by the stronger weights,
as shown in table 5. They had a performance
similar to the ones obtained by the learners in
the previous conditions, as shown in figure 8,
comparing these learners with those in condi-
tion 1.
</bodyText>
<subsectionHeader confidence="0.9365865">
3.4 Condition 4: Learners-50 in a
Noise-free Environment
</subsectionHeader>
<bodyText confidence="0.999919">
When the noise-free environment was used with
these stronger weights, the convergence pattern
was slightly faster for all learners, when com-
pared to condition 3 (which used a noisy envi-
ronment), but still slower than conditions 1 and
2, as shown in figure 9. These learners had a
similar performance to those obtained in all the
previous conditions, as can be seen in table 6,
</bodyText>
<figureCaption confidence="0.994681">
Figure 8: Learners in Noisy Environment
</figureCaption>
<bodyText confidence="0.987795">
and in figure 10, which also shows the results
obtained by the learners in condition 2, which
</bodyText>
<tableCaption confidence="0.8853925">
Table 5: Convergence of the different learners -
Learners-50 - Noise
</tableCaption>
<table confidence="0.756674666666667">
Learners Triggers Parameters Parameters
Correct Set
SVO-50 230.3 22.9 11.8
SOV-50 168.1 22.4 10.4
OVS-50 221.4 22.1 10.1
VSO-50 154.6 21.9 9.7
</table>
<figure confidence="0.991241833333333">
Noisy Environment
OZ) C C \ C
4.5.•
,‘O ,40&apos; (54 Jo-
o co co co ,
G 0 c
Learners
III Set
0 Correct
Parameters 28
24
20
16
12
8
4
0
•
</figure>
<page confidence="0.952591">
216
</page>
<figureCaption confidence="0.9999985">
Figure 7: Convergence of Subjdir - Learners-50 - Noisy Environment
Figure 9: Convergence of Subjdir - Learners-50 - Noise-free Environment
</figureCaption>
<figure confidence="0.985241771428572">
used weaker weights. 3.5 Discussion
Subjdir - Noisy Environment
1
0.9
SVO-50
0.8 - SOV-50
0.7 - OVS-50
- - VSO-50
0.6 -
0.5
tO 0) CO r...• • 0) • • •
LO CO &amp;quot;Ttr a) co
N If r O 0) N CO CO CO
Triggers
Posterior Probability
Subjdir - Noise-free Environment
.a 0.9
ea 0.8 -
.a 0.7 -
I- 0.6 -
2 0.5
0.
o
*E
00
Q.
SVO-50
SOV-50
OVS
-50
- - VSO-50
10 Ca CO • to • CO • to 0) CO
N N/- CO 0) N CO 03
U) N. 0)
Triggers
</figure>
<tableCaption confidence="0.846767">
Table 6: Convergence of the different learners
Learners-50 - Noise Free
</tableCaption>
<table confidence="0.996615166666667">
Learners Triggers Parameters Parameters
Correct Set
SVO-50 221.7 23.2 11.5
SOV-50 195.4 23.2 11.8
OVS-50 223.2 22.1 9.9
VSO-50 223.4 21.8 9.8
</table>
<bodyText confidence="0.999869384615385">
As confirmed by these results, there is a strong
interaction between the different starting points
and the presence of noise. The noise has a
strong influence on the convergence of the learn-
ers, slowing down the learning process, since the
learners need more triggers to compensate for
the effect caused by the noisy ones. The dif-
ferent initialisations caused little impact in the
learners&apos; performance, in spite of noticeably de-
laying the convergence to the target of those
learners that have values incompatible with the
target. Thus, when combining the presence of
noise with the use of stronger weights, there was
</bodyText>
<page confidence="0.98957">
217
</page>
<bodyText confidence="0.999963117647059">
a significant delay in convergence, where the fi-
nal posterior probability was up to 10% lower
than in the noise-free case (e.g. for the OVS
learner), as can be seen in figures 7 and 9.
Nonetheless, these learners were robust to the
presence of noise in the input data, only select-
ing or changing a value for a given parameter
when there was enough evidence for that. As
a consequence, all the learners were converging
towards the target, even with the small amount
of available triggers, regardless of the initialisa-
tions and the presence of noise. This is the case
even with an extreme bias in the initial values.
Moreover, the learners make effective use of the
inheritance mechanism to propagate default val-
ues, with an average of around 4.2 non-default
specifications for these learners.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997496">
The purpose of this work is to investigate the
process of grammatical acquisition from a com-
putational perspective, focusing on the acqui-
sition of word order from data. Five different
learners were implemented in this framework
and we investigated how the starting point for
the learners affects their performance in con-
verging to the target and its interaction with
noise. The learners were all converging towards
the target grammar, where the different start-
ing points and the presence of noise affected
only convergence times, with learners more far
away from the target having a slower conver-
gence pattern. Future works include annotat-
ing more data to have a bigger corpus, and run-
ning more experiments with this corpus, testing
how much data is required for all the triggers
to converge, with high probability to the tar-
get grammar. After that, we will concentrate
on investigating the acquisition of subcategori-
sation frames and argument structure, using the
same framework for learning. Although this is
primarily a cognitive computational model, it is
potentially relevant to the development of more
adaptive NLP technology.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999940166666667">
I would like to thank Ted Briscoe for his com-
ments and advice on this paper, and Fabio
Nemetz for his support. Thanks also to the
anonymous reviewers for their comments. The
research reported on this paper is supported by
doctoral studentship from CAPES/Brazil.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994027642857143">
Bar Hillel, Y. Language and Information. Wesley,
Reading, Mass. 1964.
Briscoe, T. The Acquisition of Grammar in an
Evolving Population of Language Agents. Linkop-
ing Electronic Articles in Computer and Informa-
tion Science, http://www.ep.liu.se/ea/cis/1999.
Chomsky, N. Lectures on Government and Binding.
Foris Publications, 1981.
Gold, E.M. Language Identification in the Limit. In-
formation and Control, v.10, p.44&apos;7-4&apos;74, 1967.
Lascarides, A. and Copestake, A. Default Represen-
tation in Constraint-based Frameworks. Compu-
tational Linguistics, v.25 n.1, p.55-105, 1999.
MacWhinney, B. The CHILDES Project: Tools for
Analyzing Talk. Second Edition, 1995.
Sachs, J. Talking about the there and then: the emer-
gence of displaced reference in parent-child dis-
course. In K. E. Nelson editor, Children&apos;s lan-
guage, v.4, 1983.
Villavicencio, A. Representing a System of Lexical
Types Using Default Unification. Proceedings of
EACL, 1999.
Villavicencio, A. The Acquisition of a Unification-
Based Generalised Categorial Grammar. Proceed-
ings of the Third CLUK Colloquium, 2000.
Waldron, B. Learning Natural Language within the
framework of categorial grammar. Proceedings of
the Third CLUK Colloquium, 2000.
</reference>
<figure confidence="0.999020733333333">
Noise-Free Environment
28 • Set
e 24 D Correct
B 20
W 16
12
8
co
EL 4
0
e`•
4) NC) (00 0 40
.40 , c5-1&apos; (54, co. c;\
0 d/ ,f°
Learners
</figure>
<figureCaption confidence="0.992968">
Figure 10: Learners in Noise Free Environment
</figureCaption>
<page confidence="0.994279">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.533075">
<note confidence="0.96605">of CoNLL-2000 and LLL-2000, 209-218, Lisbon, Portugal, 2000.</note>
<title confidence="0.981767">The Acquisition of Word Order a Computational Learning System</title>
<author confidence="0.975218">Aline</author>
<affiliation confidence="0.938667">Computer Laboratory, University of</affiliation>
<address confidence="0.602512">New Museums Site, Cambridge, CB2 3QG, England,</address>
<email confidence="0.959142">Aline.Villavicencioacl.cam.ac.uk</email>
<abstract confidence="0.999222625">The purpose of this work is to investigate the process of grammatical acquisition from data. We are using a computational learning system that is composed of a Universal Grammar with associated parameters, and a learning algorithm, following the Principles and Parameters Theory. The Universal Grammar is implemented as a Unification-Based Generalised Categorial Grammar, embedded in a default inheritance network of lexical types. The learning algorithm receives input from a corpus annotated with logical forms and sets the parameters based on this input. This framework is used as basis to investigate several aspects of language acquisition. In this paper we are concentrating on the acquisition of word order for different learners. The results obtained show the different learners having a similar performance and converging towards the target grammar given the input data available, regardless of their starting points. It also shows how the amount of noise present in the input data affects the speed of convergence of the learners towards the target.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bar Hillel</author>
<author>Y</author>
</authors>
<title>Language and Information.</title>
<date>1964</date>
<publisher>Wesley,</publisher>
<location>Reading, Mass.</location>
<marker>Hillel, Y, 1964</marker>
<rawString>Bar Hillel, Y. Language and Information. Wesley, Reading, Mass. 1964.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Briscoe</author>
</authors>
<title>The Acquisition of Grammar in an Evolving Population of Language Agents.</title>
<booktitle>Linkoping Electronic Articles in Computer and Information Science,</booktitle>
<location>http://www.ep.liu.se/ea/cis/1999.</location>
<marker>Briscoe, </marker>
<rawString>Briscoe, T. The Acquisition of Grammar in an Evolving Population of Language Agents. Linkoping Electronic Articles in Computer and Information Science, http://www.ep.liu.se/ea/cis/1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Lectures on Government and Binding.</title>
<date>1981</date>
<publisher>Foris Publications,</publisher>
<contexts>
<context position="2059" citStr="Chomsky 1981" startWordPosition="321" endWordPosition="322">ically learn linguistic information from data, we can look at the way people do it. Gold (1967) when investigating language identification in the limit, obtained results that implied that natural languages could not be learned only on the basis of positive evidence. These results were used as a confirmation for the proposal that children must have some innate knowledge about language, the Universal Grammar (UG), to help them overcome the problem of the poverty of the stimulus and acquire a grammar on the basis of positive evidence only. According to Chomsky&apos;s Principles and Parameters Theory (Chomsky 1981), the UG is composed of principles and parameters, and the process of learning a language is regarded as the setting of values of a number of parameters, given exposure to this particular language. We employ this idea in the learning framework implemented. In this work we are interested in investigating the acquisition of grammatical knowledge from data, focusing on the acquisition of word order, that reflects the underlying order in which constituents occur in different languages (e.g. SVO and SOV languages). The learning system is equipped with a UG and associated parameters, encoded as a Un</context>
<context position="3806" citStr="Chomsky 1981" startWordPosition="611" endWordPosition="612">estigate the acquisition of word order within this framework and discuss the results obtained by different learners. Finally we present some conclusions and future work. 2 The Learning System The learning system is composed of a language learner equipped with a UG and a learning algorithm that updates the initial parameter settings, based on exposure to a corpus of utterances. Each of these components is discussed in 209 more detail in the following sections. 2.1 The Universal Grammar The UG consists of principles and parameters, and the latter are set according to the linguistic environment (Chomsky 1981). This proposal suggests that human languages follow a common set of principles and differ among one another only in finitely many respects, represented by a finite number of parameters that can vary according to a finite number of values (which makes them learnable in Gold&apos;s paradigm). In this section, we discuss the UG and associated parameters, which are formalised in terms of a Unification-Based Generalised Categorial Grammar (UB-GCG), embedded in a default inheritance network of lexical types. We concentrate on the description of word order parameters, which reflect the basic order in whi</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. Lectures on Government and Binding. Foris Publications, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language Identification in the Limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<pages>44--7</pages>
<contexts>
<context position="1541" citStr="Gold (1967)" startWordPosition="239" endWordPosition="240">anguage acquisition. In this paper we are concentrating on the acquisition of word order for different learners. The results obtained show the different learners having a similar performance and converging towards the target grammar given the input data available, regardless of their starting points. It also shows how the amount of noise present in the input data affects the speed of convergence of the learners towards the target. 1 Introduction In trying to solve the question of how to get a machine to automatically learn linguistic information from data, we can look at the way people do it. Gold (1967) when investigating language identification in the limit, obtained results that implied that natural languages could not be learned only on the basis of positive evidence. These results were used as a confirmation for the proposal that children must have some innate knowledge about language, the Universal Grammar (UG), to help them overcome the problem of the poverty of the stimulus and acquire a grammar on the basis of positive evidence only. According to Chomsky&apos;s Principles and Parameters Theory (Chomsky 1981), the UG is composed of principles and parameters, and the process of learning a l</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>Gold, E.M. Language Identification in the Limit. Information and Control, v.10, p.44&apos;7-4&apos;74, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lascarides</author>
<author>A Copestake</author>
</authors>
<title>Default Representation in Constraint-based Frameworks.</title>
<date>1999</date>
<booktitle>Computational Linguistics, v.25 n.1,</booktitle>
<pages>55--105</pages>
<contexts>
<context position="5858" citStr="Lascarides and Copestake 1999" startWordPosition="952" endWordPosition="956">n which the category is to be combined (where VALUE can be either forward or backward). As an example, in English an intransitive verb (s \ np) is encoded as shown in figure 1, where only the relevant attributes are shown. In this work, we employ the rules of (forward and backward) application, (forward and backward) composition and generalised weak permutation. A more detailed description of the UBGCG used can be found in (Villavicencio 2000). The UG is implemented as a UB-GCG, embedded in a default inheritance network of lexical types (Villavicencio 1999), implemented in the YADU framework (Lascarides and Copestake 1999). The categories and rules in the grammar are defined as types in the hierarchy, represented in terms of TDFSS and the featurestructures associated with any given category or rule are defined by the inheritance chain. With different sub-networks used to encode different kinds of linguistic knowledge, linguistic regularities are encoded near the top of a network, while types further down the network are used to represent sub-regularities or exceptions. Thus, types are concisely defined, with only specific information being described, since more general information is inherited from the supertyp</context>
</contexts>
<marker>Lascarides, Copestake, 1999</marker>
<rawString>Lascarides, A. and Copestake, A. Default Representation in Constraint-based Frameworks. Computational Linguistics, v.25 n.1, p.55-105, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Second Edition,</title>
<date>1995</date>
<contexts>
<context position="10811" citStr="MacWhinney 1995" startWordPosition="1776" endWordPosition="1777">ghout the lexical inheritance network. 2.2 The Corpus The UG has to be general enough to capture the grammar for any language, and the parameters have to be set to account for a particular language, based on exposure to that language. This can be obtained by means of a corpus of utterances, annotated with logical forms, which is described in this section. Among these sentences, some will be triggers for certain parameters, in the sense that, to parse that sentence, some of the parameters will have to be set to a given value. We are using the Sachs corpus (Sachs 1983) from the CHILDES project (MacWhinney 1995), that contains interactions between only one child and her parents, from the age of 1 year and 1 month to 5 years and 1 month. From the resulting corpus, we extracted material for generating two different corpora: one containing only the child&apos;s sentences and the other containing the caretakers&apos; sentences. The caretakers&apos; corpus is given as input to the learner to mirror the input to which a child learning a language is exposed. And the child&apos;s corpus is used for comparative purposes. In order to annotate the caretakers&apos; corpus with the associated logical forms, a UB-GCG for English was built</context>
</contexts>
<marker>MacWhinney, 1995</marker>
<rawString>MacWhinney, B. The CHILDES Project: Tools for Analyzing Talk. Second Edition, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sachs</author>
</authors>
<title>Talking about the there and then: the emergence of displaced reference in parent-child discourse. In</title>
<date>1983</date>
<booktitle>Children&apos;s language, v.4,</booktitle>
<editor>K. E. Nelson editor,</editor>
<contexts>
<context position="10768" citStr="Sachs 1983" startWordPosition="1770" endWordPosition="1771">te information about parameters, throughout the lexical inheritance network. 2.2 The Corpus The UG has to be general enough to capture the grammar for any language, and the parameters have to be set to account for a particular language, based on exposure to that language. This can be obtained by means of a corpus of utterances, annotated with logical forms, which is described in this section. Among these sentences, some will be triggers for certain parameters, in the sense that, to parse that sentence, some of the parameters will have to be set to a given value. We are using the Sachs corpus (Sachs 1983) from the CHILDES project (MacWhinney 1995), that contains interactions between only one child and her parents, from the age of 1 year and 1 month to 5 years and 1 month. From the resulting corpus, we extracted material for generating two different corpora: one containing only the child&apos;s sentences and the other containing the caretakers&apos; sentences. The caretakers&apos; corpus is given as input to the learner to mirror the input to which a child learning a language is exposed. And the child&apos;s corpus is used for comparative purposes. In order to annotate the caretakers&apos; corpus with the associated lo</context>
</contexts>
<marker>Sachs, 1983</marker>
<rawString>Sachs, J. Talking about the there and then: the emergence of displaced reference in parent-child discourse. In K. E. Nelson editor, Children&apos;s language, v.4, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>Representing a System of Lexical Types Using Default Unification.</title>
<date>1999</date>
<booktitle>Proceedings of EACL,</booktitle>
<contexts>
<context position="5791" citStr="Villavicencio 1999" startWordPosition="945" endWordPosition="946">ng the category, and DIRECTION, encoding the direction in which the category is to be combined (where VALUE can be either forward or backward). As an example, in English an intransitive verb (s \ np) is encoded as shown in figure 1, where only the relevant attributes are shown. In this work, we employ the rules of (forward and backward) application, (forward and backward) composition and generalised weak permutation. A more detailed description of the UBGCG used can be found in (Villavicencio 2000). The UG is implemented as a UB-GCG, embedded in a default inheritance network of lexical types (Villavicencio 1999), implemented in the YADU framework (Lascarides and Copestake 1999). The categories and rules in the grammar are defined as types in the hierarchy, represented in terms of TDFSS and the featurestructures associated with any given category or rule are defined by the inheritance chain. With different sub-networks used to encode different kinds of linguistic knowledge, linguistic regularities are encoded near the top of a network, while types further down the network are used to represent sub-regularities or exceptions. Thus, types are concisely defined, with only specific information being descr</context>
</contexts>
<marker>Villavicencio, 1999</marker>
<rawString>Villavicencio, A. Representing a System of Lexical Types Using Default Unification. Proceedings of EACL, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The Acquisition of a UnificationBased Generalised Categorial Grammar.</title>
<date>2000</date>
<booktitle>Proceedings of the Third CLUK Colloquium,</booktitle>
<contexts>
<context position="5675" citStr="Villavicencio 2000" startWordPosition="925" endWordPosition="926">tegorised elements (defined in ACTIVE), with each element in the list defined in terms of two features: SIGN, encoding the category, and DIRECTION, encoding the direction in which the category is to be combined (where VALUE can be either forward or backward). As an example, in English an intransitive verb (s \ np) is encoded as shown in figure 1, where only the relevant attributes are shown. In this work, we employ the rules of (forward and backward) application, (forward and backward) composition and generalised weak permutation. A more detailed description of the UBGCG used can be found in (Villavicencio 2000). The UG is implemented as a UB-GCG, embedded in a default inheritance network of lexical types (Villavicencio 1999), implemented in the YADU framework (Lascarides and Copestake 1999). The categories and rules in the grammar are defined as types in the hierarchy, represented in terms of TDFSS and the featurestructures associated with any given category or rule are defined by the inheritance chain. With different sub-networks used to encode different kinds of linguistic knowledge, linguistic regularities are encoded near the top of a network, while types further down the network are used to rep</context>
<context position="17592" citStr="Villavicencio 2000" startWordPosition="2876" endWordPosition="2877">ine what the most appropriate hierarchy for these settings is, given the goal of converging to the target. The learner has a preference for grammars (and thus hierarchies) that not only model the data (represented by the current settings) well, but are also compact, following the Minimum Description Length (MDL) Principle. In this case, the most probable grammar in the grammar space, among the ones consistent with the parameter settings, is the one where the default inheritance hierarchy is the more concise, having the minimum number of non-default parameter values specified, as described in (Villavicencio 2000). Table 2: Status of the Parameter The 28 word order parameters are defined in a hierarchical relation, with the supertype parameters being set in accordance with the subtypes, to reflect the value of the majority of the subtypes. In this way, as the values of the subtypes are being set, they influence the value of the supertypes. If the value of a given subtype differs from the value of the supertype, the subtype overrides the inherited default value Value Prior Posterior Prob. Weight Prob. Weight Forward 0.1 Po 0.07 1 15 Backward 0.9 I 0.93 14- 15 213 3 The Acquisition of Word Order We are i</context>
</contexts>
<marker>Villavicencio, 2000</marker>
<rawString>Villavicencio, A. The Acquisition of a UnificationBased Generalised Categorial Grammar. Proceedings of the Third CLUK Colloquium, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Waldron</author>
</authors>
<title>Learning Natural Language within the framework of categorial grammar.</title>
<date>2000</date>
<booktitle>Proceedings of the Third CLUK Colloquium,</booktitle>
<contexts>
<context position="19954" citStr="Waldron, 2000" startWordPosition="3275" endWordPosition="3276">f the default learners according to each of the basic orders, with gendir having the same direction as vargdir, and all the other parameters having unset values. These parameters have the prior and posterior probabilities initialised with 0.1 for one value and 0.9 for the other. In this way, an SVO learner, for example, is initialised with subjdir having as current value backward (0.9), vargdir forward (0.9) and gendir forward (0.9). The sentences in the input corpus are presented to a learner only once, sequentially, in the original order. The input to a learner is pre-processed by a system [Waldron, 2000] that assigns categories to each word in a sentence. The sentences with their putative category assignments are given as input to the learner. The learner then evaluates the category assignments for each sentence and only uses those that are valid according to the UG to set the parameters; the others are discarded. The corpus contains 1,041 English sentences (which follow the SVO order), but from these only a small proportion are triggers for the parameters, in the sense that, for the learner to process them, it has to select certain parameter values. As each triggering sentence is processed,</context>
</contexts>
<marker>Waldron, 2000</marker>
<rawString>Waldron, B. Learning Natural Language within the framework of categorial grammar. Proceedings of the Third CLUK Colloquium, 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>