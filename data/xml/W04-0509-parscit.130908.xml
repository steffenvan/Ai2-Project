<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000159">
<title confidence="0.987382">
Analysis of Semantic Classes in Medical Text for Question Answering
</title>
<author confidence="0.995951">
Yun Niu and Graeme Hirst
</author>
<affiliation confidence="0.9986785">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.8173745">
Toronto, Ontario M5S 3G4
Canada
</address>
<email confidence="0.998328">
yun@cs.toronto.edu, gh@cs.toronto.edu
</email>
<sectionHeader confidence="0.993889" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996">
To answer questions from clinical-evidence texts,
we identify occurrences of the semantic classes —
disease, medication, patient outcome — that are
candidate elements of the answer, and the relations
among them. Additionally, we determine whether
an outcome is positive or negative.
</bodyText>
<sectionHeader confidence="0.993178" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.965296628571429">
The published medical literature is an important
source to help clinicians make decisions in patient
treatment (Sackett and Straus, 1998; Straus and
Sackett, 1999). Clinicians often need to consult lit-
erature on the latest information in patient care, such
as side effects of a medication, symptoms of a dis-
ease, or time constraints in the use of a medication.
For example:1
Q: In a patient with a suspected MI does throm-
bolysis decrease the risk of death if it is ad-
ministered 10 hours after the onset of chest
pain?
The answer to the question can be found in Clini-
cal Evidence (CE) (Barton, 2002), a regularly up-
dated publication that reviews and consolidates ex-
perimental results for clinical problems:
A: Systematic reviews of RCTs have found
that prompt thrombolytic treatment (within 6
hours and perhaps up to 12 hours and longer
after the onset of symptoms) reduces mor-
tality in people with AMI and ST elevation
or bundle branch block on their presenting
ECG.
The goal of the EpoCare project (“Evidence at
Point of Care”) at the University of Toronto is to
develop methods for answering questions automati-
cally with CE as the source text. (We do not look at
1All the examples in this paper are taken from a collection
of questions that arose over a two-week period in August 2001
in a clinical teaching unit at the University of Toronto.
primary medical research text.) Currently, the sys-
tem accepts keyword queries in PICO format (Sack-
ett et al., 2000). In this format, a clinical question is
represented by a set of four fields that correspond to
the basic elements of the question:
</bodyText>
<listItem confidence="0.654265">
P: a description of the patient (or the problem);
I: an intervention;
C: a comparison or control intervention (may
be omitted);
</listItem>
<bodyText confidence="0.975750175">
O: the clinical outcome.
For example, the question shown above can be rep-
resented in PICO format as follows:
P: myocardial infarction
I: thrombolysis
C: —
O: mortality
Our work in the project is to extend the keyword
retrieval to a system that can answer questions ex-
pressed in natural language.
In our earlier work (Niu et al., 2003), we showed
that current technologies for factoid question an-
swering (QA) are not adequate for clinical ques-
tions, whose answers must often be obtained by
synthesizing relevant context. To adapt to this new
characteristic of QA in the medical domain, we ex-
ploit semantic classes and relations between them in
medical text. Semantic classes are important for our
task because the information contained in them is
often a good candidate for answering clinical ques-
tions. In the example above, PICO elements cor-
respond to three semantic classes: DISEASE (med-
ical problem of the patient), INTERVENTION (med-
ication applied to the disease) and the CLINICAL
OUTCOME. They together constitute a SCENARIO
of treatment. Similarly, a diagnosis scenario often
includes SYMPTOMS, TESTING PROCEDURE, and
HYPOTHESIZED DISEASES. To understand the se-
mantics of medical text and find answers to clinical
questions, we need to know how these classes relate
to each other in a specific scenario. For example, is
this medication a special type of another one; is this
medication applied to this disease? These are the
kind of relations that we are interested in. In this
work, we use a cue-word–based approach to iden-
tify semantic classes in the treatment scenario and
analyze the relations between them. We also apply
an automatic classification process to determine the
polarity of an outcome, as it is important in answer-
ing clinical questions.
</bodyText>
<sectionHeader confidence="0.975094" genericHeader="introduction">
2 Identifying Semantic Classes in Medical
Text
</sectionHeader>
<subsectionHeader confidence="0.998119">
2.1 Diseases and Medications
</subsectionHeader>
<bodyText confidence="0.997665066666667">
The identification of named entities (NEs) in the
biomedical area, such as PROTEINS and CELLS, has
been extensively explored; e.g., Lee et al. (2003),
Shen et al. (2003). However, we are not aware of
any satisfactory solution that focuses on the recog-
nition of semantic classes such as MEDICATION and
DISEASE. To straightforwardly identify DISEASE
and MEDICATION in the text, we use the knowledge
base Unified Medical Language System (UMLS)
(Lindberg et al., 1993) and the software MetaMap
(Aronson, 2001).
UMLS contains three knowledge sources: the
Metathesaurus, the Semantic Network, and the Spe-
cialist Lexicon. Given an input sentence, MetaMap
separates it into phrases, identifies the medical con-
cepts embedded in the phrases, and assigns proper
semantic categories to them according to the knowl-
edge in UMLS. For example, for the phrase imme-
diate systemic anticoagulants, MetaMap identifies
immediate as a TEMPORAL CONCEPT, systemic as
a FUNCTIONAL CONCEPT, and anticoagulants as a
PHARMACOLOGIC SUBSTANCE. More than one se-
mantic category in UMLS may correspond to MED-
ICATION or DISEASE. For example, either a PHAR-
MACOLOGIC SUBSTANCE or a THERAPEUTIC OR
PREVENTIVE PROCEDURE can be a MEDICATION;
either a DISEASE OR SYNDROME or a PATHOLOGIC
FUNCTION can be a DISEASE.
We use some training text to find the mapping
between UMLS categories and the two semantic
classes in the treatment scenario. The training
text was tagged for us by a clinician to mark DIS-
EASE and MEDICATION. It was also processed by
MetaMap. After that, the annotated text was com-
pared with the output of MetaMap to find the corre-
sponding UMLS categories. Medical text contain-
ing these categories can then be identified as either
MEDICATION or DISEASE. In the example above,
anticoagulants will be taken as a MEDICATION. The
problem of identification of medical terminology is
still a big challenge in this area. MetaMap does not
provide a full solution to it. For cases in which the
output of MetaMap is not consistent with the judg-
ment of the clinician who annotated our text, our
decisions rely on the latter.
</bodyText>
<subsectionHeader confidence="0.999522">
2.2 Clinical Outcome
</subsectionHeader>
<bodyText confidence="0.9223094">
The task of identifying clinical outcomes is more
complicated. Outcomes are often not just noun
phrases; instead, they usually are expressed in com-
plex syntactic structures. The following are some
examples:
</bodyText>
<listItem confidence="0.932246388888889">
(1) Thrombolysis reduces the risk of depen-
dency, but increases the risk of death.
(2) The median proportion ofsymptom free days
improved more with salmeterol than with
placebo.
In our analysis of the text, we found another type
of outcome which is also very important: the out-
come of clinical trials:
(3) Several small comparative RCTs [random-
ized clinical trials] have found sodium cro-
moglicate to be less effective than inhaled
corticosteroids in improving symptoms and
lung function.
(4) In the systematic review of calcium chan-
nel antagonists, indirect and limited compar-
isons of intravenous versus oral administra-
tion found no significant difference in ad-
verse events.
</listItem>
<bodyText confidence="0.9995445">
We treat these as a special type of clinical outcome.
For convenience, we refer to them as “results” in the
following description when necessary. A “result”
might contain a clinical outcome within it, as results
often involve a comparison of the effects of two (or
more) interventions on a disease.
In medical text, the appearance of some words is
found often to be a signal of the occurrence of an
outcome, and usually several words signal the oc-
currence of one single outcome. The combination
approach that we applied for identifying outcomes
is based on this observation. Our approach does
not extract the whole outcome at once. Instead, it
tries to identify the different parts of an outcome
that may be scattered in the sentence, and then com-
bines them to form the complete outcome.
</bodyText>
<sectionHeader confidence="0.878898" genericHeader="method">
2.2.1 Related work
</sectionHeader>
<bodyText confidence="0.999585344827586">
Rule-based methods and machine-learning ap-
proaches have been used for similar problems.
Gildea and Jurafsky (2002) used a supervised learn-
ing method to learn both the identifier of the seman-
tic roles defined in FrameNet such as theme, target,
goal, and the boundaries of the roles (Baker et al.,
2003). A set of features were learned from a large
training set, and then applied to the unseen data to
detect the roles. The performance of the system was
quite good. However, it requires a large training
set for related roles, which is not available in many
tasks, including tasks in the medical area.
Rule-based methods are explored in information
extraction (IE) to identify roles to fill slots in some
pre-defined templates (Catal`a et al., 2003). The
rules are represented by a set of patterns, and tem-
plate role identification is usually conducted by pat-
tern matching. Slots indicating roles are embed-
ded in these patterns. Text that satisfies the con-
straints of a pattern will be identified, and the con-
tents corresponding to the slots are extracted. This
approach has been proved to be effective in many IE
tasks. However, pattern construction is very time-
consuming, especially for complicated phrasings.
In order to select the roles and only the roles, their
expression has to be customized specifically in pat-
terns. This results in increasing difficulties in pat-
tern construction, and reduces the coverage of the
patterns.
</bodyText>
<subsectionHeader confidence="0.731333">
2.2.2 A combination approach
</subsectionHeader>
<bodyText confidence="0.999326933333333">
Different pieces of an outcome are identified by var-
ious cue words. Each occurrence of a cue word sug-
gests a portion of the expression of the outcome.
Detecting all of them will increase the chance of
obtaining the complete outcome. Also, different oc-
currences of cue words provide more evidence of
the existence of an outcome.
The first step of the combination approach is to
collect the cue words. Two sections of CE (stroke
management, asthma in children) were analyzed for
detection of outcome. The text was annotated by a
clinician in the EpoCare project. About two-thirds
of each section (267 sentences in total) was taken as
the analysis examples for collecting the cue words,
and the rest (156 sentences) as the test set. Some
words we found in the analysis are the following:
Nouns: death, benefit, dependency, outcome, evi-
dence, harm, difference.
Verbs: improve, reduce, prevent, produce, in-
crease.
Adjectives: beneficial, harmful, negative, adverse,
superior.
After the cue words are identified, the next ques-
tion is what portion of text each cue word suggests
as the outcome, which determines the boundary of
the outcome. The text was pre-processed by the
Apple Pie parser (Sekine, 1997) to obtain the part-
of-speech and phrase information. We found that
for the noun cues, the noun phrase that contains the
noun will be part of the outcome. For the verb cue
words, the verb and its object together constitute
one portion of the outcome. For the adjective cue
words, often the corresponding adjective phrase or
the noun phrase belongs to the outcome. Cue words
for the results of clinical trials are processed in a
slightly different way. For example, for difference
and superior, any immediately following preposi-
tional phrase is also included in the results of the
trial.
Our approach does not rely on specific patterns,
it is more flexible than pattern-matching techniques
in IE systems, and it does not need a large training
set. A limitation of this approach is that some con-
nections between different portions of an outcome
may be missing.
</bodyText>
<subsubsectionHeader confidence="0.705925">
2.2.3 Evaluation and analysis of results
</subsubsectionHeader>
<bodyText confidence="0.999739352941177">
We evaluated the cue word method of detecting the
outcome on the remaining one-third of the sections
of CE. (The test set is rather small because of the
difficulty in obtaining the annotations.) The out-
come detection task was broken into two sub-tasks,
each evaluated separately: to identify the outcome
itself and to determine its textual boundary. The re-
sult of identification is shown in Table 1. Eighty-one
sentences in the test set contain either an outcome or
result, which is 52% of all the test sentences. This
was taken as the baseline of the evaluation: taking
all sentences in the test set as positive (i.e., contain-
ing an outcome or result). By contrast, the accuracy
of the combination approach is 83%.
There are two main reasons why some outcomes
were not identified. One is that some outcomes do
not have any cue word:
</bodyText>
<listItem confidence="0.932997666666667">
(5) Gastrointestinal symptoms and headaches
have been reported with both montelukast
and zafirlukast.
</listItem>
<bodyText confidence="0.999922571428571">
The other reason is that although some outcomes
contained words that might be regarded as cue
words, we did not include them in our set; for ex-
ample, fewer and higher. Adjectives were found to
have the most irregular usages. It is normal for them
to modify both medications and outcomes, as shown
in the following examples:
</bodyText>
<listItem confidence="0.9576955">
(6) ... children receiving higher dose inhaled
corticosteroids ...
</listItem>
<tableCaption confidence="0.999622">
Table 1: Results of identifying outcomes in CE
</tableCaption>
<table confidence="0.99993375">
False False
Method Correct Positives Negatives Precision% Recall% Accuracy%
Baseline 81 75 0 52 (81/156) 100 52
Combination approach 67 14 14 83 (67/81) 83 82
</table>
<tableCaption confidence="0.827037666666667">
Table 2: Results of boundary detection of correctly
identified outcomes in CE. A: Identified fragments;
B: true boundary.
</tableCaption>
<table confidence="0.999816714285714">
Type of Overlap Number Percentage
Exact match 26 39
A entirely within B 19 28
B entirely within A 13 19
Each partially 8 12
within the other
No match 1 1
</table>
<listItem confidence="0.8872835">
(7) ... mean morning PEFR was 4% higher in
the salmeterol group.
</listItem>
<bodyText confidence="0.999023625">
Other adjectives such as less, more, lower, shorter,
longer, and different have similar problems. If they
are taken as identifiers of outcomes then some false
positives are very likely to be generated. However,
if they are excluded, some true outcomes will be
missed. There were 14 samples of false positives.
The main cause was sentences containing cue words
that did not have any useful information:
</bodyText>
<listItem confidence="0.939165333333333">
(8) We found that the balance between bene-
fits and harms has not been clearly estab-
lished for the evacuation of supratentorial
haematomas.
(9) The third systematic review did not evaluate
these adverse outcomes.
</listItem>
<bodyText confidence="0.978508121212121">
Table 2 shows the result of boundary detection
for those outcomes that were correctly identified.
The true boundary is the boundary of an outcome
that was annotated manually. The no match case
means that there is a true outcome in the sentence
but the program missed the correct portions of text
and marked some other portions as the outcome.
The program identified 39% of the boundaries ex-
actly the same as the true boundaries. In 19% of the
samples, the true boundaries were entirely within
the identified fragments. The spurious text in them
(the text that was not in the true boundary) was
found to be small in many cases, both in terms of
number of words and in terms of the importance of
the content. The average number of words correctly
identified was 7 for each outcome and the number
of spurious words was 3.4. The most frequent con-
tent in the spurious text was the medication applied
to obtain the outcome. In the following examples,
text in “()” is the outcome (result) identified auto-
matically, and text in “{ I” is spurious.
(10) The RCTs found (no significant adverse ef-
fects {associated with salmeterol I).
(11) The second RCT ... also found (no sig-
nificant difference in mortality at 12 weeks
{with lubeluzole versus placebo I) ...
Again, adjectives are most problematic. Even
when a true adjective identifier is found, the bound-
ary of the outcome is hard to determine by an un-
supervised approach because of the variations in
the expression. In the following examples, the true
boundaries of outcomes are indicated by “[ ]”, ad-
jectives are highlighted.
</bodyText>
<listItem confidence="0.99682225">
(12) Nebulised ... , but [(serious adverse
effects) are rare].
(13) Small RCTs ... found that [... was
(effective), with ... ].
</listItem>
<bodyText confidence="0.999665">
The correctness of the output of the parser also
had an important impact on the performance, as
shown in the following example:
</bodyText>
<listItem confidence="0.7996854">
(14) RCTs found no evidence that lubeluzole
improved clinical outcomes in people with
acute ischaemic stroke.
(S ... (NPL (DT that) (JJ lubeluzole) (JJ im-
proved) (JJ clinical) (NNS outcomes)) ... )
</listItem>
<bodyText confidence="0.999924916666667">
In this parse, the verb improve was incorrectly as-
signed to be an adjective in a noun phrase. Thus im-
prove as a verb cue word was missed in identifying
the outcome. However, another cue word outcomes
was matched, so the whole noun phrase of outcomes
was identified as the outcome. On the one hand,
the example shows that the wrong parsing output
directly affects the identification process. On the
other hand, it also shows that missing one cue word
in identifying the outcome can be corrected by the
occurrence of other cue words in the combination
approach.
</bodyText>
<sectionHeader confidence="0.68001" genericHeader="method">
3 Analysis of Relations
</sectionHeader>
<bodyText confidence="0.999011333333333">
Recognition of individual semantic classes is not
enough for text understanding; we also need to
know how different entities in the same semantic
class are connected, as well as what relations hold
between different classes. Currently, all these rela-
tions are considered at the sentence level.
</bodyText>
<subsectionHeader confidence="0.99208">
3.1 Relations within the same semantic class
</subsectionHeader>
<bodyText confidence="0.999767176470588">
Relations between different medications are the fo-
cus of this sub-section, as a sentence often men-
tioned more than one medication. Relations be-
tween diseases can be analyzed in a similar way, al-
though they occur much less often than medications.
Text from CE was analyzed manually to understand
what relations are often involved and how they are
represented. The text for the analysis is the same
as in the class-identification task discussed above.
As with classes themselves, it was found that these
relations can be identified by a group of cue words
or symbols. For example, the word plus refers to
the COMBINATION of two or more medications, the
word or, as well as a comma, often suggests the AL-
TERNATIVE relation, and the word versus (or v) usu-
ally implies a COMPARISON relation, as shown in
the following examples:
</bodyText>
<listItem confidence="0.8961003125">
(15) The combination of aspirin plus streptoki-
nase significantly increased mortality at 3
months.
(16) RCTs found no evidence that calcium chan-
nel antagonists, lubeluzole, aminobutyric
acid (GABA) agonists, glycine antagonists,
or N-methyl-D-aspartate (NMDA) antago-
nists improve clinical outcomes in people
with acute ischaemic stroke.
(17) One systematic review found no short or
long term improvement in acute ischaemic
stroke with immediate systemic anticoagu-
lants (unfractionated heparin, low molecu-
lar weight heparin, heparinoids, or specific
thrombin inhibitors) versus usual care with-
out systemic anticoagulants.
</listItem>
<bodyText confidence="0.976287666666667">
It is worth noting that in CE, the experimental con-
ditions are often explained in the description of the
outcomes, for example:
</bodyText>
<listItem confidence="0.874112583333333">
(18) ... receiving higher dose inhaled corticos-
teroids (3.6cm, 95% CI 3.0 to 4.2 with dou-
ble dose beclometasone v 5.1cm, 95% CI 4.5
to 5.7 with salmeterol v 4.5cm, 95% CI 3.8
to 5.2 with placebo).
(19) It found that ... oral theophylline ... ver-
sus placebo increased the mean number of
symptom free days (63% with theophylline v
42% with placebo; P=0.02).
(20) Studies of ... inhaled steroid (see salme-
terol v high dose inhaled corticosteroids un-
der adult asthma).
</listItem>
<bodyText confidence="0.9994974">
These descriptions are usually in parentheses. They
are often phrases and even just fragments of strings
that are not represented in a manner that is uniform
with the other parts of the sentence. Their behavior
is more difficult to capture and therefore the rela-
tions among the concepts in these descriptions are
more difficult to identify. Because they usually are
examples and data, omission of them will not af-
fect the understanding of the whole sentence in most
cases.
Six common relations and their cue words were
found in the text which are shown in Table 3. Cue
words and symbols between medical concepts were
first collected from the training text. Then the re-
lations they signal were analyzed. Some cue words
are ambiguous, for example, or, and, and with. Or
could also suggest a comparison relation although
most of the time it means alternative, and could rep-
resent an alternative relation, and with could be a
specification relation. It is interesting to find that
and in the text when it connects two medications
often suggests an alternative relation rather than a
combination relation (e.g., the second and in exam-
ple 5). Also, compared with versus, plus, etc., and
and with are weak cues as most of their appearances
in the text do not suggest a relation between two
medications.
On the basis of this analysis, an automatic re-
lation analysis process was applied to the test set,
which was the same as in outcome identification.
The test process was divided into two parts: one
took parenthetical descriptions into account (case 1)
and the other one did not (case 2). In the evaluation,
for sentences that contain at least two medications,
“correct” means that the relation that holds between
the medications is correctly identified. We do not
evaluate the relation between any two medications
in a sentence; instead, we only considered two med-
ications that are related to each other by a cue word
or symbol (including those connected by cue words
</bodyText>
<tableCaption confidence="0.982416">
Table 3: Cue words/symbols for relations between
medications
</tableCaption>
<table confidence="0.773962111111111">
Relation(s) Cue Words/Symbols
comparison superior to, more than, versus, or,
compare with, between ... and ...
alternative or, “,”, and
combination plus, add to, addition of ... to ...,
combined use of, and, with, “(”
specification with, “(”
substitute substitute, substituted for
preference rather than
</table>
<tableCaption confidence="0.998362">
Table 4: Results of relation analysis
</tableCaption>
<table confidence="0.682227333333333">
Correct Wrong Missing False Positive
Case 1 49 7 10 9
Case 2 48 7 3 6
</table>
<bodyText confidence="0.99984355">
other than the set collected from the training text).
The results of the two cases are shown in Table 4.
Most errors are because of the weak indicators
with and and. As in the outcome identification task,
both the training and test sets are rather small, as no
standard annotated text is available.
Some of the surface relationships in Table 3 re-
flect deeper relationships of the semantic classes.
For example, COMPARISON, ALTERNATIVE, and
PREFERENCE imply that the two (or more) medi-
cations have some common effects on the disease(s)
they are applied to. The SPECIFICATION relation, on
the other hand, suggests a hierarchical relation be-
tween the first medication and the following ones, in
which the first medication is a higher-level concept
and the following medications are at a lower level.
For example, in example 17 above, systemic anti-
coagulants is a higher-level concept, unfractionated
heparin, low molecular weight heparin, etc., are ex-
amples of it that lie at a lower level.
</bodyText>
<subsectionHeader confidence="0.986581">
3.2 Relations between different semantic
classes
</subsectionHeader>
<bodyText confidence="0.9999765">
In a specific domain such as medicine, some default
relations often hold between semantic classes. For
example, a CAUSE–EFFECT relation is strongly em-
bedded in the three semantic classes appearing in
a sentence of the form: “medication ... disease
... outcome”, even if not in this exact order. This
default relation helps the relation analysis because
in most cases we do not need to depend on the text
between the classes to understand the whole sen-
tence. For instance, the CAUSE–EFFECT relation
is very likely to express the idea that applying the
intervention on the disease will have the outcome.
This is another reason that semantic classes are im-
portant, especially in a specific domain.
</bodyText>
<sectionHeader confidence="0.872303" genericHeader="method">
4 The polarity of outcomes
</sectionHeader>
<bodyText confidence="0.9865155">
Most clinical outcomes and the results of clinical
trials are either positive or negative:
</bodyText>
<listItem confidence="0.8003185">
(21) Positive: Thrombolysis reduced the risk of
death or dependency at the end of the stud-
ies.
(22) Negative: In the systematic review, throm-
bolysis increased fatal intracranial haemor-
rhage compared with placebo.
</listItem>
<bodyText confidence="0.999743739130435">
Polarity information is useful for several reasons.
First of all, it can filter out positive outcomes if the
question is about the negative aspects of a medica-
tion. Secondly, negative outcomes may be crucial
even if the question does not explicitly ask about
them. Finally, from the number of positive or neg-
ative descriptions of the outcome of a medication
applying to a disease, clinicians can form a general
idea about how “good” the medication is. As a first
step in understanding opposing relations between
scenarios in medical text, the polarity of outcomes
was determined by an automatic classification pro-
cess.
We use support vector machines (SVMs) to dis-
tinguish positive outcomes from negative ones.
SVMs have been shown to be efficient in text clas-
sification tasks (Joachims, 1998). Given a training
sample, the SVM finds a hyperplane with the max-
imal margin of separation between the two classes.
The classification is then just to determine which
side of the hyperplane the test sample lies in. We
used the SVMlight package (Joachims, 2002) in our
experiment.
</bodyText>
<subsectionHeader confidence="0.997813">
4.1 Training and test examples
</subsectionHeader>
<bodyText confidence="0.9999228">
The training and test sets were built by collecting
sentences from different sections in CE; 772 sen-
tences were used, 500 for training (300 positive, 200
negative), and 272 for testing (95 positive, 177 neg-
ative). All examples were labeled manually.
</bodyText>
<subsectionHeader confidence="0.952707">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999902929824561">
The classification used four different sets of fea-
tures. The first feature set includes every unigram
that appears at least three times in the whole train-
ing set. To improve the performance by attenuating
the sparse data problem, in the second feature set,
all names of diseases were replaced by the same tag
disease. This was done by pre-processing the text
using MetaMap to identify all diseases in both the
training and the test examples. Then the identified
diseases were replaced by the disease tag automat-
ically. As medications often are not mentioned in
outcomes, they were not generalized in this manner.
The third feature set represents changes described
in outcomes. Our observation is that outcomes of-
ten involve the change in a clinical value. For ex-
ample, after a medication was applied to a disease,
something was increased (enhanced, more, ...) or
decreased (reduced, less, ... ). Thus the polarity
of an outcome is often determined by how change
happens: if a bad thing (e.g., mortality) is reduced
then it is a positive outcome; if the bad thing is in-
creased, then the outcome is negative. We try to
capture this observation by adding context features
to the feature set. The way they were added is sim-
ilar to incorporating the negation effect described
by Pang et al. (2002). But instead of just finding a
“negation word” (not, isn’t, didn’t, etc.), we need to
find two groups of words: those indicating more and
those indicating less. In the training text, we found
9 words in the first group and 7 words in the second
group. When pre-processing text for classification,
following the method of Pang et al., we attached the
tag MORE to all words between the more-words
and the following punctuation mark, and the tag
LESS to the words after the less-words.
The fourth feature set is the combination of the
effects of feature set two and three. In representing
each sentence by a feature vector, we tested both
presence (feature appears or not) and frequency
(count the number of occurrences of the feature in
the sentence).
The accuracy of the classification is shown in Ta-
ble 5. The baseline is to assign a random class (here
we use negative, as they are more frequent in the test
set) to all test samples.
The presence of features performs better than fre-
quency of features in general. Using a more gen-
eral category instead of specific diseases has a pos-
itive effect on the presence-based classification. We
speculate that the effect of this generalization will
be bigger if a larger test set were used. Pang et al.
(2002) did not compare the result of using and not
using the negation context effect, so it is not clear
how much it improved their result. In our task, it
is clear that the MORE/ LESS feature has a signif-
icant effect on the performance, especially for the
frequency features.
</bodyText>
<tableCaption confidence="0.997995">
Table 5: Results of outcome polarity classification
</tableCaption>
<table confidence="0.997986777777778">
Features Presence Frequency
(%) (%)
Baseline 65.07 65.07
Original unigrams 88.97 87.87
Unigrams with disease 90.07 88.24
Unigrams with
MORE/ LESS tag 91.54 91.91
Unigrams with disease 92.65 92.28
and MORE/ LESS tag
</table>
<sectionHeader confidence="0.998228" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995027027027">
We have described our work in medical text anal-
ysis by identifying semantic classes and the rela-
tions between them. Our work suggests that seman-
tic classes in medical scenarios play an important
role in understanding medical text. The scenario
view may be extended to a framework that acts as
a guideline for further semantic analysis.
Semantic classes and their relations have di-
rect applications in medical question answering and
query refinement in information retrieval. In ques-
tion answering, the question and answer candidates
will contain some semantic classes. After identify-
ing them on both sides, the question can be com-
pared with the answer to find whether there is a
match. In information retrieval, relations between
semantic classes can be added to the index. If the
query posed by the user is too general, the system
will ask the user to refine the query by adding more
concepts and even relations so that it will be more
pertinent according to the content of the source. For
example, a user may search for a document describ-
ing the comparison of aspirin and placebo. Instead
of just using aspirin and placebo as the query terms,
the user can specify the comparison relation as well
in the query.
We will continue working on the second level of
the semantic analysis, to explore the relations on
the scenario level. A complete scenario contains all
three semantic classes. One scenario may be the ex-
planation or justification of the previous scenario(s),
or contradictory to the previous scenario(s). De-
tecting these relationships will be of great help for
understanding-based tasks, such as context-related
question answering, topic-related summarization,
etc. As different scenarios might not be adjacent to
each other in the texts, classical rhetorical analysis
cannot provide a complete solution for this problem.
</bodyText>
<sectionHeader confidence="0.992142" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999837">
The EpoCare project is supported by grants from
Bell University Laboratories at the University of
Toronto. Our work is also supported by a grant
from the Natural Sciences and Engineering Re-
search Council of Canada and an Ontario Graduate
Scholarship. We are grateful to Sharon Straus, MD,
and other members of the EpoCare project for dis-
cussion and assistance.
</bodyText>
<sectionHeader confidence="0.99915" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992538513888889">
Alan R. Aronson. 2001. Effective mapping of
biomedical text to the UMLS metathesaurus: The
MetaMap program. In Proceedings of Ameri-
can Medical Informatics Association Symposium,
pages 17–21.
Collin F. Baker, Charles J. Fillmore, and Beau
Cronin. 2003. The structure of the Framenet
database. International Journal ofLexicography,
16(3):281–296.
Stuart Barton. 2002. Clinical Evidence. BMJ Pub-
lishing Group, London.
Neus Catal`a, N´uria Castell, and Mario Mart´in.
2003. A portable method for acquiring infor-
mation extraction patterns without annotated cor-
pora. Natural Language Engineering, 9(2):151–
179.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, 28(3):245–288.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the European
Conference on Machine Learning (ECML), pages
137–142.
Thorsten Joachims. 2002. SVMlight homepage. In
http://svmlight.joachims.org/.
Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang
Rim. 2003. Two-phase biomedical NE recog-
nition based on SVMs. In Proceedings of 41st
annual meeting of the Association for Compu-
tational Linguistics, Workshop on Natural Lan-
guage Processing in Biomedicine, pages 33–40.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa. T. McCray. 1993. The Unified Medical
Language System. Methods of Information in
Medicine, 32(4):281–291.
Yun Niu, Graeme Hirst, Gregory McArthur, and
Patricia Rodriguez-Gianolli. 2003. Answering
clincal questions with role identification. In Pro-
ceedings of 41st annual meeting of the Associ-
ation for Computational Linguistics, Workshop
on Natural Language Processing in Biomedicine,
pages 73–80.
Bo Pang, Lillian Le, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In Proceedings of 2002 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 79–86.
David L. Sackett and Sharon E. Straus. 1998. Find-
ing and applying evidence during clinical rounds:
The “evidence cart”. Journal of the American
Medical Association, 280(15):1336–1338.
David L. Sackett, Sharon E. Straus, W. Scott
Richardson, William Rosenberg, and R. Brian
Haynes. 2000. Evidence-Based Medicine: How
to Practice and Teach EBM. Harcourt Publishers
Limited, Edinburgh.
Satoshi Sekine. 1997. Apple pie parser homepage.
In http://nlp.cs.nyu.edu/app/.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and
Chew-Lim Tan. 2003. Effective adaptation of
hidden Markov model–based named entity rec-
ognizer for biomedical domain. In Proceedings
of 41st annual meeting of the Association for
Computational Linguistics, Workshop on Natural
Language Processing in Biomedicine, pages 49–
56.
Sharon E. Straus and David L. Sackett. 1999.
Bringing evidence to the point of care. Journal
of the American Medical Association, 281:1171–
1172.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.996864">Analysis of Semantic Classes in Medical Text for Question Answering</title>
<author confidence="0.983378">Niu</author>
<affiliation confidence="0.9999">Department of Computer University of</affiliation>
<address confidence="0.772839">Ontario</address>
<email confidence="0.999939">yun@cs.toronto.edu,gh@cs.toronto.edu</email>
<abstract confidence="0.998052841328413">To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes — disease, medication, patient outcome — that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative. 1 Motivation The published medical literature is an important source to help clinicians make decisions in patient treatment (Sackett and Straus, 1998; Straus and Sackett, 1999). Clinicians often need to consult literature on the latest information in patient care, such as side effects of a medication, symptoms of a disease, or time constraints in the use of a medication. a patient with a suspected MI does thrombolysis decrease the risk of death if it is administered 10 hours after the onset of chest pain? The answer to the question can be found in Clinical Evidence (CE) (Barton, 2002), a regularly updated publication that reviews and consolidates experimental results for clinical problems: reviews of RCTs have found that prompt thrombolytic treatment (within 6 hours and perhaps up to 12 hours and longer after the onset of symptoms) reduces mortality in people with AMI and ST elevation or bundle branch block on their presenting ECG. The goal of the EpoCare project (“Evidence at Point of Care”) at the University of Toronto is to develop methods for answering questions automatically with CE as the source text. (We do not look at the examples in this paper are taken from a collection of questions that arose over a two-week period in August 2001 in a clinical teaching unit at the University of Toronto. primary medical research text.) Currently, the system accepts keyword queries in PICO format (Sackett et al., 2000). In this format, a clinical question is represented by a set of four fields that correspond to the basic elements of the question: description of the patient (or the comparison or control intervention (may the clinical For example, the question shown above can be represented in PICO format as follows: P: myocardial infarction Our work in the project is to extend the keyword retrieval to a system that can answer questions expressed in natural language. In our earlier work (Niu et al., 2003), we showed that current technologies for factoid question answering (QA) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context. To adapt to this new characteristic of QA in the medical domain, we exclasses them in medical text. Semantic classes are important for our task because the information contained in them is often a good candidate for answering clinical questions. In the example above, PICO elements corto three semantic classes: (medproblem of the (medapplied to the disease) the They together constitute a of treatment. Similarly, a diagnosis scenario often and To understand the semantics of medical text and find answers to clinical questions, we need to know how these classes relate to each other in a specific scenario. For example, is this medication a special type of another one; is this medication applied to this disease? These are the kind of relations that we are interested in. In this work, we use a cue-word–based approach to identify semantic classes in the treatment scenario and analyze the relations between them. We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the area, such as has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recogof semantic classes such as To straightforwardly identify the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowlin UMLS. For example, for the phrase immesystemic MetaMap identifies a and a More than one secategory in UMLS may correspond to For example, either a SUBSTANCE a OR PROCEDURE be a a OR SYNDROME a be a We use some training text to find the mapping between UMLS categories and the two semantic classes in the treatment scenario. The training was tagged for us by a clinician to mark It was also processed by MetaMap. After that, the annotated text was compared with the output of MetaMap to find the corresponding UMLS categories. Medical text containing these categories can then be identified as either In the example above, be taken as a The problem of identification of medical terminology is still a big challenge in this area. MetaMap does not provide a full solution to it. For cases in which the output of MetaMap is not consistent with the judgment of the clinician who annotated our text, our decisions rely on the latter. 2.2 Clinical Outcome The task of identifying clinical outcomes is more complicated. Outcomes are often not just noun phrases; instead, they usually are expressed in complex syntactic structures. The following are some examples: Thrombolysis the risk of depenbut the risk of (2) The median proportion ofsymptom free days more salmeterol than with placebo. In our analysis of the text, we found another type of outcome which is also very important: the outcome of clinical trials: (3) Several small comparative RCTs [randomized clinical trials] have found sodium croto be effective inhaled improving symptoms and (4) In the systematic review of calcium channel antagonists, indirect and limited comparisons of intravenous versus oral administrafound significant difference in ad- We treat these as a special type of clinical outcome. For convenience, we refer to them as “results” in the following description when necessary. A “result” might contain a clinical outcome within it, as results often involve a comparison of the effects of two (or more) interventions on a disease. In medical text, the appearance of some words is found often to be a signal of the occurrence of an outcome, and usually several words signal the occurrence of one single outcome. The combination approach that we applied for identifying outcomes is based on this observation. Our approach does not extract the whole outcome at once. Instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome. 2.2.1 Related work Rule-based methods and machine-learning approaches have been used for similar problems. Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al., 2003). A set of features were learned from a large training set, and then applied to the unseen data to detect the roles. The performance of the system was quite good. However, it requires a large training set for related roles, which is not available in many tasks, including tasks in the medical area. Rule-based methods are explored in information extraction (IE) to identify roles to fill slots in some pre-defined templates (Catal`a et al., 2003). The rules are represented by a set of patterns, and template role identification is usually conducted by pattern matching. Slots indicating roles are embedded in these patterns. Text that satisfies the constraints of a pattern will be identified, and the contents corresponding to the slots are extracted. This approach has been proved to be effective in many IE tasks. However, pattern construction is very timeconsuming, especially for complicated phrasings. In order to select the roles and only the roles, their expression has to be customized specifically in patterns. This results in increasing difficulties in pattern construction, and reduces the coverage of the patterns. 2.2.2 A combination approach Different pieces of an outcome are identified by various cue words. Each occurrence of a cue word suggests a portion of the expression of the outcome. Detecting all of them will increase the chance of obtaining the complete outcome. Also, different occurrences of cue words provide more evidence of the existence of an outcome. The first step of the combination approach is to collect the cue words. Two sections of CE (stroke management, asthma in children) were analyzed for detection of outcome. The text was annotated by a clinician in the EpoCare project. About two-thirds of each section (267 sentences in total) was taken as the analysis examples for collecting the cue words, and the rest (156 sentences) as the test set. Some words we found in the analysis are the following: benefit, dependency, outcome, eviharm, reduce, prevent, produce, inharmful, negative, adverse, After the cue words are identified, the next question is what portion of text each cue word suggests as the outcome, which determines the boundary of the outcome. The text was pre-processed by the Apple Pie parser (Sekine, 1997) to obtain the partof-speech and phrase information. We found that for the noun cues, the noun phrase that contains the noun will be part of the outcome. For the verb cue words, the verb and its object together constitute one portion of the outcome. For the adjective cue words, often the corresponding adjective phrase or the noun phrase belongs to the outcome. Cue words for the results of clinical trials are processed in a different way. For example, for any immediately following prepositional phrase is also included in the results of the trial. Our approach does not rely on specific patterns, it is more flexible than pattern-matching techniques in IE systems, and it does not need a large training set. A limitation of this approach is that some connections between different portions of an outcome may be missing. 2.2.3 Evaluation and analysis of results We evaluated the cue word method of detecting the outcome on the remaining one-third of the sections of CE. (The test set is rather small because of the difficulty in obtaining the annotations.) The outcome detection task was broken into two sub-tasks, each evaluated separately: to identify the outcome itself and to determine its textual boundary. The result of identification is shown in Table 1. Eighty-one sentences in the test set contain either an outcome or result, which is 52% of all the test sentences. This was taken as the baseline of the evaluation: taking all sentences in the test set as positive (i.e., containing an outcome or result). By contrast, the accuracy of the combination approach is 83%. There are two main reasons why some outcomes were not identified. One is that some outcomes do not have any cue word: Gastrointestinal symptoms have been reported with both montelukast and zafirlukast. The other reason is that although some outcomes contained words that might be regarded as cue words, we did not include them in our set; for ex- Adjectives were found to have the most irregular usages. It is normal for them to modify both medications and outcomes, as shown in the following examples: ... children receiving inhaled corticosteroids ...</abstract>
<note confidence="0.493276">Table 1: Results of identifying outcomes in CE</note>
<title confidence="0.654037">False False Method Correct Positives Negatives Precision% Recall% Accuracy%</title>
<phone confidence="0.446191">Baseline 81 75 0 52 (81/156) 100 52</phone>
<note confidence="0.8401649">Combination approach 67 14 14 83 (67/81) 83 82 Table 2: Results of boundary detection of correctly identified outcomes in CE. A: Identified fragments; B: true boundary. Type of Overlap Number Percentage Exact match 26 39 A entirely within B 19 28 B entirely within A 13 19 Each partially within the other 8 12 No match 1 1</note>
<abstract confidence="0.992141022222223">mean morning PEFR was 4% the salmeterol group. adjectives such as more, lower, shorter, similar problems. If they are taken as identifiers of outcomes then some false positives are very likely to be generated. However, if they are excluded, some true outcomes will be missed. There were 14 samples of false positives. The main cause was sentences containing cue words that did not have any useful information: We found that the balance between benenot been clearly established for the evacuation of supratentorial haematomas. (9) The third systematic review did not evaluate Table 2 shows the result of boundary detection for those outcomes that were correctly identified. The true boundary is the boundary of an outcome was annotated manually. The match means that there is a true outcome in the sentence but the program missed the correct portions of text and marked some other portions as the outcome. The program identified 39% of the boundaries exactly the same as the true boundaries. In 19% of the samples, the true boundaries were entirely within the identified fragments. The spurious text in them (the text that was not in the true boundary) was found to be small in many cases, both in terms of number of words and in terms of the importance of the content. The average number of words correctly identified was 7 for each outcome and the number of spurious words was 3.4. The most frequent content in the spurious text was the medication applied to obtain the outcome. In the following examples, in is the outcome (result) identified autoand text in is spurious. The RCTs found significant adverse efwith salmeterol The second RCT ... also found significant difference in mortality at 12 weeks lubeluzole versus placebo Again, adjectives are most problematic. Even when a true adjective identifier is found, the boundary of the outcome is hard to determine by an unsupervised approach because of the variations in the expression. In the following examples, the true boundaries of outcomes are indicated by “[ ]”, adjectives are highlighted. Nebulised ... , but rare]. (13) Small RCTs ... found that [... was with ... ]. The correctness of the output of the parser also had an important impact on the performance, as shown in the following example: (14) RCTs found no evidence that lubeluzole improved clinical outcomes in people with acute ischaemic stroke. (S ... (NPL (DT that) (JJ lubeluzole) (JJ improved) (JJ clinical) (NNS outcomes)) ... ) this parse, the verb incorrectly asto be an adjective in a noun phrase. Thus ima verb cue word was missed in identifying outcome. However, another cue word matched, so the whole noun phrase of was identified as the outcome. On the one hand, the example shows that the wrong parsing output directly affects the identification process. On the other hand, it also shows that missing one cue word in identifying the outcome can be corrected by the occurrence of other cue words in the combination approach. 3 Analysis of Relations Recognition of individual semantic classes is not enough for text understanding; we also need to know how different entities in the same semantic class are connected, as well as what relations hold between different classes. Currently, all these relations are considered at the sentence level. 3.1 Relations within the same semantic class Relations between different medications are the focus of this sub-section, as a sentence often mentioned more than one medication. Relations between diseases can be analyzed in a similar way, although they occur much less often than medications. Text from CE was analyzed manually to understand what relations are often involved and how they are represented. The text for the analysis is the same as in the class-identification task discussed above. As with classes themselves, it was found that these relations can be identified by a group of cue words symbols. For example, the word to two or more medications, the as well as a comma, often suggests the and the word usuimplies a as shown in the following examples: The combination of aspirin streptokinase significantly increased mortality at 3 months. (16) RCTs found no evidence that calcium chan- (GABA) (NMDA) antagonists improve clinical outcomes in people with acute ischaemic stroke. (17) One systematic review found no short or long term improvement in acute ischaemic stroke with immediate systemic anticoagu- (unfractionated molecuweight or inhibitors) care without systemic anticoagulants. It is worth noting that in CE, the experimental conditions are often explained in the description of the outcomes, for example: (18) ... receiving higher dose inhaled corticos- 95% CI 3.0 to 4.2 with double dose beclometasone v 5.1cm, 95% CI 4.5 to 5.7 with salmeterol v 4.5cm, 95% CI 3.8 5.2 with (19) It found that ... oral theophylline ... versus placebo increased the mean number of free days with theophylline v with placebo; Studies of ... inhaled steroid salmeterol v high dose inhaled corticosteroids unadult These descriptions are usually in parentheses. They are often phrases and even just fragments of strings that are not represented in a manner that is uniform with the other parts of the sentence. Their behavior is more difficult to capture and therefore the relations among the concepts in these descriptions are more difficult to identify. Because they usually are examples and data, omission of them will not affect the understanding of the whole sentence in most cases. Six common relations and their cue words were found in the text which are shown in Table 3. Cue words and symbols between medical concepts were first collected from the training text. Then the relations they signal were analyzed. Some cue words ambiguous, for example, and could also suggest a comparison relation although of the time it means alternative, repan alternative relation, and be a specification relation. It is interesting to find that the text when it connects two medications often suggests an alternative relation rather than a relation (e.g., the second exam- 5). Also, compared with etc., weak cues as most of their appearances in the text do not suggest a relation between two medications. On the basis of this analysis, an automatic relation analysis process was applied to the test set, which was the same as in outcome identification. The test process was divided into two parts: one took parenthetical descriptions into account (case 1) and the other one did not (case 2). In the evaluation, for sentences that contain at least two medications, “correct” means that the relation that holds between the medications is correctly identified. We do not evaluate the relation between any two medications in a sentence; instead, we only considered two medications that are related to each other by a cue word or symbol (including those connected by cue words Table 3: Cue words/symbols for relations between medications Relation(s) Cue Words/Symbols comparison superior to, more than, versus, or, compare with, between ... and ... alternative or, “,”, and combination plus, add to, addition of ... to ..., combined use of, and, with, “(” specification with, “(” substitute substitute, substituted for preference rather than Table 4: Results of relation analysis Correct Wrong Missing False Positive Case 1 49 7 10 9 Case 2 48 7 3 6 other than the set collected from the training text). The results of the two cases are shown in Table 4. Most errors are because of the weak indicators As in the outcome identification task, both the training and test sets are rather small, as no standard annotated text is available. Some of the surface relationships in Table 3 reflect deeper relationships of the semantic classes. example, and that the two (or more) medications have some common effects on the disease(s) are applied to. The on the other hand, suggests a hierarchical relation between the first medication and the following ones, in which the first medication is a higher-level concept and the following medications are at a lower level. example, in example 17 above, antia higher-level concept, low molecular weight etc., are examples of it that lie at a lower level. 3.2 Relations between different semantic classes In a specific domain such as medicine, some default relations often hold between semantic classes. For a is strongly embedded in the three semantic classes appearing in sentence of the form: even if not in this exact order. This default relation helps the relation analysis because in most cases we do not need to depend on the text between the classes to understand the whole sen- For instance, the is very likely to express the idea that applying the intervention on the disease will have the outcome. This is another reason that semantic classes are important, especially in a specific domain. 4 The polarity of outcomes Most clinical outcomes and the results of clinical trials are either positive or negative: (21) Thrombolysis reduced the risk of death or dependency at the end of the studies. (22) In the systematic review, thrombolysis increased fatal intracranial haemorrhage compared with placebo. Polarity information is useful for several reasons. First of all, it can filter out positive outcomes if the question is about the negative aspects of a medication. Secondly, negative outcomes may be crucial even if the question does not explicitly ask about them. Finally, from the number of positive or negative descriptions of the outcome of a medication applying to a disease, clinicians can form a general idea about how “good” the medication is. As a first step in understanding opposing relations between scenarios in medical text, the polarity of outcomes was determined by an automatic classification process. We use support vector machines (SVMs) to distinguish positive outcomes from negative ones. SVMs have been shown to be efficient in text classification tasks (Joachims, 1998). Given a training sample, the SVM finds a hyperplane with the maximal margin of separation between the two classes. The classification is then just to determine which side of the hyperplane the test sample lies in. We the package (Joachims, 2002) in our experiment. 4.1 Training and test examples The training and test sets were built by collecting sentences from different sections in CE; 772 sentences were used, 500 for training (300 positive, 200 negative), and 272 for testing (95 positive, 177 negative). All examples were labeled manually. 4.2 Evaluation The classification used four different sets of features. The first feature set includes every unigram that appears at least three times in the whole training set. To improve the performance by attenuating the sparse data problem, in the second feature set, all names of diseases were replaced by the same tag This was done by pre-processing the text using MetaMap to identify all diseases in both the training and the test examples. Then the identified were replaced by the automatically. As medications often are not mentioned in outcomes, they were not generalized in this manner. The third feature set represents changes described in outcomes. Our observation is that outcomes often involve the change in a clinical value. For example, after a medication was applied to a disease, was more, or less, ). Thus the polarity of an outcome is often determined by how change happens: if a bad thing (e.g., mortality) is reduced then it is a positive outcome; if the bad thing is increased, then the outcome is negative. We try to capture this observation by adding context features to the feature set. The way they were added is similar to incorporating the negation effect described by Pang et al. (2002). But instead of just finding a word” isn’t, etc.), we need to two groups of words: those indicating indicating In the training text, we found 9 words in the first group and 7 words in the second group. When pre-processing text for classification, following the method of Pang et al., we attached the all words between the and the following punctuation mark, and the tag the words after the The fourth feature set is the combination of the effects of feature set two and three. In representing each sentence by a feature vector, we tested both presence (feature appears or not) and frequency (count the number of occurrences of the feature in the sentence). The accuracy of the classification is shown in Table 5. The baseline is to assign a random class (here we use negative, as they are more frequent in the test set) to all test samples. features performs better than frefeatures in general. Using a more general category instead of specific diseases has a positive effect on the presence-based classification. We speculate that the effect of this generalization will be bigger if a larger test set were used. Pang et al. (2002) did not compare the result of using and not using the negation context effect, so it is not clear how much it improved their result. In our task, it clear that the has a significant effect on the performance, especially for the Table 5: Results of outcome polarity classification Features Presence Frequency (%) (%) Baseline 65.07 65.07 Original unigrams 88.97 87.87 with 90.07 88.24 Unigrams with LESS 91.54 91.91 with LESS 92.65 92.28 5 Conclusion We have described our work in medical text analysis by identifying semantic classes and the relations between them. Our work suggests that semantic classes in medical scenarios play an important role in understanding medical text. The scenario view may be extended to a framework that acts as a guideline for further semantic analysis. Semantic classes and their relations have direct applications in medical question answering and query refinement in information retrieval. In question answering, the question and answer candidates will contain some semantic classes. After identifying them on both sides, the question can be compared with the answer to find whether there is a match. In information retrieval, relations between semantic classes can be added to the index. If the query posed by the user is too general, the system will ask the user to refine the query by adding more concepts and even relations so that it will be more pertinent according to the content of the source. For example, a user may search for a document describing the comparison of aspirin and placebo. Instead just using the query terms, the user can specify the comparison relation as well in the query. We will continue working on the second level of the semantic analysis, to explore the relations on the scenario level. A complete scenario contains all three semantic classes. One scenario may be the explanation or justification of the previous scenario(s), or contradictory to the previous scenario(s). Detecting these relationships will be of great help for understanding-based tasks, such as context-related question answering, topic-related summarization, etc. As different scenarios might not be adjacent to each other in the texts, classical rhetorical analysis cannot provide a complete solution for this problem.</abstract>
<note confidence="0.421038">Acknowledgements The EpoCare project is supported by grants from</note>
<affiliation confidence="0.775582">Bell University Laboratories at the University of</affiliation>
<abstract confidence="0.906807833333333">Toronto. Our work is also supported by a grant from the Natural Sciences and Engineering Research Council of Canada and an Ontario Graduate Scholarship. We are grateful to Sharon Straus, MD, and other members of the EpoCare project for discussion and assistance.</abstract>
<title confidence="0.694524">References</title>
<author confidence="0.675861">Effective mapping of</author>
<affiliation confidence="0.558819333333333">biomedical text to the UMLS metathesaurus: The program. In of Ameri- Medical Informatics Association</affiliation>
<note confidence="0.96097225">pages 17–21. Collin F. Baker, Charles J. Fillmore, and Beau Cronin. 2003. The structure of the Framenet Journal 16(3):281–296. Barton. 2002. BMJ Publishing Group, London. Neus Catal`a, N´uria Castell, and Mario Mart´in.</note>
<abstract confidence="0.866024529411765">2003. A portable method for acquiring information extraction patterns without annotated cor- Language 9(2):151– 179. Daniel Gildea and Daniel Jurafsky. 2002. Autolabeling of semantic roles. 28(3):245–288. Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many features. In of the European on Machine Learning pages 137–142. Joachims. 2002. homepage. In Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang Rim. 2003. Two-phase biomedical NE recogbased on SVMs. In of 41st annual meeting of the Association for Compu-</abstract>
<note confidence="0.938603652173913">tational Linguistics, Workshop on Natural Lan- Processing in pages 33–40. Donald A. B. Lindberg, Betsy L. Humphreys, and Alexa. T. McCray. 1993. The Unified Medical System. of Information in 32(4):281–291. Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-Gianolli. 2003. Answering questions with role identification. In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop Natural Language Processing in pages 73–80. Bo Pang, Lillian Le, and Shivakumar 2002. Thumbs up? Sentiment classification using machine learning In of 2002 Conference on Empirical Methods in Natural Language pages 79–86. David L. Sackett and Sharon E. Straus. 1998. Finding and applying evidence during clinical rounds: “evidence cart”. of the American 280(15):1336–1338.</note>
<author confidence="0.869810666666667">David L Sackett</author>
<author confidence="0.869810666666667">Sharon E Straus</author>
<author confidence="0.869810666666667">W Scott Richardson</author>
<author confidence="0.869810666666667">William Rosenberg</author>
<author confidence="0.869810666666667">R Brian</author>
<affiliation confidence="0.673956">Practice and Teach Harcourt Publishers</affiliation>
<address confidence="0.739812">Limited, Edinburgh.</address>
<note confidence="0.962005692307692">Satoshi Sekine. 1997. Apple pie parser homepage. Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim Tan. 2003. Effective adaptation of Markov model–based named entity recfor biomedical domain. In of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Processing in pages 49– 56. Sharon E. Straus and David L. Sackett. 1999. evidence to the point of care. the American Medical 281:1171– 1172.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program.</title>
<date>2001</date>
<booktitle>In Proceedings of American Medical Informatics Association Symposium,</booktitle>
<pages>17--21</pages>
<contexts>
<context position="4583" citStr="Aronson, 2001" startWordPosition="745" endWordPosition="746">in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categories to them according to the knowledge in UMLS. For example, for the phrase immediate systemic anticoagulants, MetaMap identifies immediate as a TEMPORAL CONCEPT, systemic as a FUNCTIONAL CONCEPT, and anticoagulants as a PHARMACOLOGIC SUBSTANCE. More than one semantic category in UMLS may correspond to MEDICATION or DISEASE. For example</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Alan R. Aronson. 2001. Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program. In Proceedings of American Medical Informatics Association Symposium, pages 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>Beau Cronin</author>
</authors>
<date>2003</date>
<booktitle>The structure of the Framenet database. International Journal ofLexicography,</booktitle>
<pages>16--3</pages>
<contexts>
<context position="8176" citStr="Baker et al., 2003" startWordPosition="1332" endWordPosition="1335">ation approach that we applied for identifying outcomes is based on this observation. Our approach does not extract the whole outcome at once. Instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome. 2.2.1 Related work Rule-based methods and machine-learning approaches have been used for similar problems. Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al., 2003). A set of features were learned from a large training set, and then applied to the unseen data to detect the roles. The performance of the system was quite good. However, it requires a large training set for related roles, which is not available in many tasks, including tasks in the medical area. Rule-based methods are explored in information extraction (IE) to identify roles to fill slots in some pre-defined templates (Catal`a et al., 2003). The rules are represented by a set of patterns, and template role identification is usually conducted by pattern matching. Slots indicating roles are em</context>
</contexts>
<marker>Baker, Fillmore, Cronin, 2003</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and Beau Cronin. 2003. The structure of the Framenet database. International Journal ofLexicography, 16(3):281–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Barton</author>
</authors>
<title>Clinical Evidence.</title>
<date>2002</date>
<publisher>BMJ Publishing Group,</publisher>
<location>London.</location>
<contexts>
<context position="1119" citStr="Barton, 2002" startWordPosition="176" endWordPosition="177">. 1 Motivation The published medical literature is an important source to help clinicians make decisions in patient treatment (Sackett and Straus, 1998; Straus and Sackett, 1999). Clinicians often need to consult literature on the latest information in patient care, such as side effects of a medication, symptoms of a disease, or time constraints in the use of a medication. For example:1 Q: In a patient with a suspected MI does thrombolysis decrease the risk of death if it is administered 10 hours after the onset of chest pain? The answer to the question can be found in Clinical Evidence (CE) (Barton, 2002), a regularly updated publication that reviews and consolidates experimental results for clinical problems: A: Systematic reviews of RCTs have found that prompt thrombolytic treatment (within 6 hours and perhaps up to 12 hours and longer after the onset of symptoms) reduces mortality in people with AMI and ST elevation or bundle branch block on their presenting ECG. The goal of the EpoCare project (“Evidence at Point of Care”) at the University of Toronto is to develop methods for answering questions automatically with CE as the source text. (We do not look at 1All the examples in this paper a</context>
</contexts>
<marker>Barton, 2002</marker>
<rawString>Stuart Barton. 2002. Clinical Evidence. BMJ Publishing Group, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neus Catal`a</author>
<author>N´uria Castell</author>
<author>Mario Mart´in</author>
</authors>
<title>A portable method for acquiring information extraction patterns without annotated corpora.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>179</pages>
<marker>Catal`a, Castell, Mart´in, 2003</marker>
<rawString>Neus Catal`a, N´uria Castell, and Mario Mart´in. 2003. A portable method for acquiring information extraction patterns without annotated corpora. Natural Language Engineering, 9(2):151– 179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="7989" citStr="Gildea and Jurafsky (2002)" startWordPosition="1298" endWordPosition="1301">se. In medical text, the appearance of some words is found often to be a signal of the occurrence of an outcome, and usually several words signal the occurrence of one single outcome. The combination approach that we applied for identifying outcomes is based on this observation. Our approach does not extract the whole outcome at once. Instead, it tries to identify the different parts of an outcome that may be scattered in the sentence, and then combines them to form the complete outcome. 2.2.1 Related work Rule-based methods and machine-learning approaches have been used for similar problems. Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al., 2003). A set of features were learned from a large training set, and then applied to the unseen data to detect the roles. The performance of the system was quite good. However, it requires a large training set for related roles, which is not available in many tasks, including tasks in the medical area. Rule-based methods are explored in information extraction (IE) to identify roles to fill slots in some pre-defined</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>137--142</pages>
<contexts>
<context position="24194" citStr="Joachims, 1998" startWordPosition="3995" endWordPosition="3996">gative outcomes may be crucial even if the question does not explicitly ask about them. Finally, from the number of positive or negative descriptions of the outcome of a medication applying to a disease, clinicians can form a general idea about how “good” the medication is. As a first step in understanding opposing relations between scenarios in medical text, the polarity of outcomes was determined by an automatic classification process. We use support vector machines (SVMs) to distinguish positive outcomes from negative ones. SVMs have been shown to be efficient in text classification tasks (Joachims, 1998). Given a training sample, the SVM finds a hyperplane with the maximal margin of separation between the two classes. The classification is then just to determine which side of the hyperplane the test sample lies in. We used the SVMlight package (Joachims, 2002) in our experiment. 4.1 Training and test examples The training and test sets were built by collecting sentences from different sections in CE; 772 sentences were used, 500 for training (300 positive, 200 negative), and 272 for testing (95 positive, 177 negative). All examples were labeled manually. 4.2 Evaluation The classification used</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning (ECML), pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>SVMlight homepage.</title>
<date>2002</date>
<note>In http://svmlight.joachims.org/.</note>
<contexts>
<context position="24455" citStr="Joachims, 2002" startWordPosition="4039" endWordPosition="4040">edication is. As a first step in understanding opposing relations between scenarios in medical text, the polarity of outcomes was determined by an automatic classification process. We use support vector machines (SVMs) to distinguish positive outcomes from negative ones. SVMs have been shown to be efficient in text classification tasks (Joachims, 1998). Given a training sample, the SVM finds a hyperplane with the maximal margin of separation between the two classes. The classification is then just to determine which side of the hyperplane the test sample lies in. We used the SVMlight package (Joachims, 2002) in our experiment. 4.1 Training and test examples The training and test sets were built by collecting sentences from different sections in CE; 772 sentences were used, 500 for training (300 positive, 200 negative), and 272 for testing (95 positive, 177 negative). All examples were labeled manually. 4.2 Evaluation The classification used four different sets of features. The first feature set includes every unigram that appears at least three times in the whole training set. To improve the performance by attenuating the sparse data problem, in the second feature set, all names of diseases were </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. SVMlight homepage. In http://svmlight.joachims.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ki-Joong Lee</author>
<author>Young-Sook Hwang</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Two-phase biomedical NE recognition based on SVMs.</title>
<date>2003</date>
<booktitle>In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="4227" citStr="Lee et al. (2003)" startWordPosition="687" endWordPosition="690">ne; is this medication applied to this disease? These are the kind of relations that we are interested in. In this work, we use a cue-word–based approach to identify semantic classes in the treatment scenario and analyze the relations between them. We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns prope</context>
</contexts>
<marker>Lee, Hwang, Rim, 2003</marker>
<rawString>Ki-Joong Lee, Young-Sook Hwang, and Hae-Chang Rim. 2003. Two-phase biomedical NE recognition based on SVMs. In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T McCray</author>
</authors>
<title>The Unified Medical Language System.</title>
<date>1993</date>
<journal>Methods of Information in Medicine,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>McCray, 1993</marker>
<rawString>Donald A. B. Lindberg, Betsy L. Humphreys, and Alexa. T. McCray. 1993. The Unified Medical Language System. Methods of Information in Medicine, 32(4):281–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Niu</author>
<author>Graeme Hirst</author>
<author>Gregory McArthur</author>
<author>Patricia Rodriguez-Gianolli</author>
</authors>
<title>Answering clincal questions with role identification.</title>
<date>2003</date>
<booktitle>In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="2575" citStr="Niu et al., 2003" startWordPosition="424" endWordPosition="427"> (Sackett et al., 2000). In this format, a clinical question is represented by a set of four fields that correspond to the basic elements of the question: P: a description of the patient (or the problem); I: an intervention; C: a comparison or control intervention (may be omitted); O: the clinical outcome. For example, the question shown above can be represented in PICO format as follows: P: myocardial infarction I: thrombolysis C: — O: mortality Our work in the project is to extend the keyword retrieval to a system that can answer questions expressed in natural language. In our earlier work (Niu et al., 2003), we showed that current technologies for factoid question answering (QA) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context. To adapt to this new characteristic of QA in the medical domain, we exploit semantic classes and relations between them in medical text. Semantic classes are important for our task because the information contained in them is often a good candidate for answering clinical questions. In the example above, PICO elements correspond to three semantic classes: DISEASE (medical problem of the patient), INTERVENTION (m</context>
</contexts>
<marker>Niu, Hirst, McArthur, Rodriguez-Gianolli, 2003</marker>
<rawString>Yun Niu, Graeme Hirst, Gregory McArthur, and Patricia Rodriguez-Gianolli. 2003. Answering clincal questions with role identification. In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Le</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>79--86</pages>
<contexts>
<context position="26063" citStr="Pang et al. (2002)" startWordPosition="4308" endWordPosition="4311">s. Our observation is that outcomes often involve the change in a clinical value. For example, after a medication was applied to a disease, something was increased (enhanced, more, ...) or decreased (reduced, less, ... ). Thus the polarity of an outcome is often determined by how change happens: if a bad thing (e.g., mortality) is reduced then it is a positive outcome; if the bad thing is increased, then the outcome is negative. We try to capture this observation by adding context features to the feature set. The way they were added is similar to incorporating the negation effect described by Pang et al. (2002). But instead of just finding a “negation word” (not, isn’t, didn’t, etc.), we need to find two groups of words: those indicating more and those indicating less. In the training text, we found 9 words in the first group and 7 words in the second group. When pre-processing text for classification, following the method of Pang et al., we attached the tag MORE to all words between the more-words and the following punctuation mark, and the tag LESS to the words after the less-words. The fourth feature set is the combination of the effects of feature set two and three. In representing each sentence</context>
<context position="27314" citStr="Pang et al. (2002)" startWordPosition="4527" endWordPosition="4530">oth presence (feature appears or not) and frequency (count the number of occurrences of the feature in the sentence). The accuracy of the classification is shown in Table 5. The baseline is to assign a random class (here we use negative, as they are more frequent in the test set) to all test samples. The presence of features performs better than frequency of features in general. Using a more general category instead of specific diseases has a positive effect on the presence-based classification. We speculate that the effect of this generalization will be bigger if a larger test set were used. Pang et al. (2002) did not compare the result of using and not using the negation context effect, so it is not clear how much it improved their result. In our task, it is clear that the MORE/ LESS feature has a significant effect on the performance, especially for the frequency features. Table 5: Results of outcome polarity classification Features Presence Frequency (%) (%) Baseline 65.07 65.07 Original unigrams 88.97 87.87 Unigrams with disease 90.07 88.24 Unigrams with MORE/ LESS tag 91.54 91.91 Unigrams with disease 92.65 92.28 and MORE/ LESS tag 5 Conclusion We have described our work in medical text analys</context>
</contexts>
<marker>Pang, Le, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Le, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Sackett</author>
<author>Sharon E Straus</author>
</authors>
<title>Finding and applying evidence during clinical rounds: The “evidence cart”.</title>
<date>1998</date>
<journal>Journal of the American Medical Association,</journal>
<volume>280</volume>
<issue>15</issue>
<contexts>
<context position="657" citStr="Sackett and Straus, 1998" startWordPosition="89" endWordPosition="92">edical Text for Question Answering Yun Niu and Graeme Hirst Department of Computer Science University of Toronto Toronto, Ontario M5S 3G4 Canada yun@cs.toronto.edu, gh@cs.toronto.edu Abstract To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes — disease, medication, patient outcome — that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative. 1 Motivation The published medical literature is an important source to help clinicians make decisions in patient treatment (Sackett and Straus, 1998; Straus and Sackett, 1999). Clinicians often need to consult literature on the latest information in patient care, such as side effects of a medication, symptoms of a disease, or time constraints in the use of a medication. For example:1 Q: In a patient with a suspected MI does thrombolysis decrease the risk of death if it is administered 10 hours after the onset of chest pain? The answer to the question can be found in Clinical Evidence (CE) (Barton, 2002), a regularly updated publication that reviews and consolidates experimental results for clinical problems: A: Systematic reviews of RCTs </context>
</contexts>
<marker>Sackett, Straus, 1998</marker>
<rawString>David L. Sackett and Sharon E. Straus. 1998. Finding and applying evidence during clinical rounds: The “evidence cart”. Journal of the American Medical Association, 280(15):1336–1338.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David L Sackett</author>
<author>Sharon E Straus</author>
<author>W Scott Richardson</author>
<author>William Rosenberg</author>
<author>R Brian</author>
</authors>
<marker>Sackett, Straus, Richardson, Rosenberg, Brian, </marker>
<rawString>David L. Sackett, Sharon E. Straus, W. Scott Richardson, William Rosenberg, and R. Brian</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haynes</author>
</authors>
<title>Evidence-Based Medicine: How to Practice and Teach EBM.</title>
<date>2000</date>
<publisher>Harcourt Publishers Limited,</publisher>
<location>Edinburgh.</location>
<marker>Haynes, 2000</marker>
<rawString>Haynes. 2000. Evidence-Based Medicine: How to Practice and Teach EBM. Harcourt Publishers Limited, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Apple pie parser homepage.</title>
<date>1997</date>
<note>In http://nlp.cs.nyu.edu/app/.</note>
<contexts>
<context position="10533" citStr="Sekine, 1997" startWordPosition="1721" endWordPosition="1722">section (267 sentences in total) was taken as the analysis examples for collecting the cue words, and the rest (156 sentences) as the test set. Some words we found in the analysis are the following: Nouns: death, benefit, dependency, outcome, evidence, harm, difference. Verbs: improve, reduce, prevent, produce, increase. Adjectives: beneficial, harmful, negative, adverse, superior. After the cue words are identified, the next question is what portion of text each cue word suggests as the outcome, which determines the boundary of the outcome. The text was pre-processed by the Apple Pie parser (Sekine, 1997) to obtain the partof-speech and phrase information. We found that for the noun cues, the noun phrase that contains the noun will be part of the outcome. For the verb cue words, the verb and its object together constitute one portion of the outcome. For the adjective cue words, often the corresponding adjective phrase or the noun phrase belongs to the outcome. Cue words for the results of clinical trials are processed in a slightly different way. For example, for difference and superior, any immediately following prepositional phrase is also included in the results of the trial. Our approach d</context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. Apple pie parser homepage. In http://nlp.cs.nyu.edu/app/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Jie Zhang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Effective adaptation of hidden Markov model–based named entity recognizer for biomedical domain.</title>
<date>2003</date>
<booktitle>In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="4247" citStr="Shen et al. (2003)" startWordPosition="691" endWordPosition="694">ion applied to this disease? These are the kind of relations that we are interested in. In this work, we use a cue-word–based approach to identify semantic classes in the treatment scenario and analyze the relations between them. We also apply an automatic classification process to determine the polarity of an outcome, as it is important in answering clinical questions. 2 Identifying Semantic Classes in Medical Text 2.1 Diseases and Medications The identification of named entities (NEs) in the biomedical area, such as PROTEINS and CELLS, has been extensively explored; e.g., Lee et al. (2003), Shen et al. (2003). However, we are not aware of any satisfactory solution that focuses on the recognition of semantic classes such as MEDICATION and DISEASE. To straightforwardly identify DISEASE and MEDICATION in the text, we use the knowledge base Unified Medical Language System (UMLS) (Lindberg et al., 1993) and the software MetaMap (Aronson, 2001). UMLS contains three knowledge sources: the Metathesaurus, the Semantic Network, and the Specialist Lexicon. Given an input sentence, MetaMap separates it into phrases, identifies the medical concepts embedded in the phrases, and assigns proper semantic categorie</context>
</contexts>
<marker>Shen, Zhang, Zhou, Su, Tan, 2003</marker>
<rawString>Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim Tan. 2003. Effective adaptation of hidden Markov model–based named entity recognizer for biomedical domain. In Proceedings of 41st annual meeting of the Association for Computational Linguistics, Workshop on Natural Language Processing in Biomedicine, pages 49– 56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon E Straus</author>
<author>David L Sackett</author>
</authors>
<title>Bringing evidence to the point of care.</title>
<date>1999</date>
<journal>Journal of the American Medical Association,</journal>
<volume>281</volume>
<pages>1172</pages>
<contexts>
<context position="684" citStr="Straus and Sackett, 1999" startWordPosition="93" endWordPosition="96">nswering Yun Niu and Graeme Hirst Department of Computer Science University of Toronto Toronto, Ontario M5S 3G4 Canada yun@cs.toronto.edu, gh@cs.toronto.edu Abstract To answer questions from clinical-evidence texts, we identify occurrences of the semantic classes — disease, medication, patient outcome — that are candidate elements of the answer, and the relations among them. Additionally, we determine whether an outcome is positive or negative. 1 Motivation The published medical literature is an important source to help clinicians make decisions in patient treatment (Sackett and Straus, 1998; Straus and Sackett, 1999). Clinicians often need to consult literature on the latest information in patient care, such as side effects of a medication, symptoms of a disease, or time constraints in the use of a medication. For example:1 Q: In a patient with a suspected MI does thrombolysis decrease the risk of death if it is administered 10 hours after the onset of chest pain? The answer to the question can be found in Clinical Evidence (CE) (Barton, 2002), a regularly updated publication that reviews and consolidates experimental results for clinical problems: A: Systematic reviews of RCTs have found that prompt thro</context>
</contexts>
<marker>Straus, Sackett, 1999</marker>
<rawString>Sharon E. Straus and David L. Sackett. 1999. Bringing evidence to the point of care. Journal of the American Medical Association, 281:1171– 1172.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>