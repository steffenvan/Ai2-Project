<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000261">
<title confidence="0.9984875">
Modeling the Noun Phrase versus Sentence Coordination Ambiguity in
Dutch: Evidence from Surprisal Theory
</title>
<author confidence="0.99742">
Harm Brouwer
</author>
<affiliation confidence="0.989695">
University of Groningen
Groningen, the Netherlands
</affiliation>
<email confidence="0.988543">
harm.brouwer@rug.nl
</email>
<author confidence="0.984643">
Hartmut Fitz
</author>
<affiliation confidence="0.988465">
University of Groningen
Groningen, the Netherlands
</affiliation>
<email confidence="0.988684">
h.fitz@rug.nl
</email>
<author confidence="0.986133">
John C. J. Hoeks
</author>
<affiliation confidence="0.9879045">
University of Groningen
Groningen, the Netherlands
</affiliation>
<email confidence="0.995133">
j.c.j.hoeks@rug.nl
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888733333334">
This paper investigates whether surprisal
theory can account for differential pro-
cessing difficulty in the NP-/S-coordina-
tion ambiguity in Dutch. Surprisal is es-
timated using a Probabilistic Context-Free
Grammar (PCFG), which is induced from
an automatically annotated corpus. We
find that our lexicalized surprisal model
can account for the reading time data from
a classic experiment on this ambiguity by
Frazier (1987). We argue that syntactic
and lexical probabilities, as specified in a
PCFG, are sufficient to account for what is
commonly referred to as an NP-coordina-
tion preference.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990149">
Language comprehension is incremental in that
meaning is continuously assigned to utterances
as they are encountered word-by-word (Altmann
and Kamide, 1999). Not all words, however, are
equally easy to process. A word’s processing dif-
ficulty is affected by, for instance, its frequency or
its effect on the syntactic and semantic interpreta-
tion of a sentence. A recent theory of sentence pro-
cessing, surprisal theory (Hale, 2001; Levy, 2008),
combines several of these aspects into one single
concept, namely the surprisal of a word. A word’s
surprisal is proportional to its expectancy, i.e., the
extent to which that word is expected (or pre-
dicted). The processing difficulty a word causes
during comprehension is argued to be related lin-
early to its surprisal; the higher the surprisal value
of a word, the more difficult it is to process.
In this paper we investigate whether surprisal
theory can account for the processing difficulty
involved in sentences containing the noun phrase
(NP) versus sentence (S) coordination ambiguity.
The sentences in (1), from a self-paced reading ex-
periment by Frazier (1987), exemplify this ambi-
guity:
</bodyText>
<equation confidence="0.561173">
(1) a. Piet kuste
Piet kissed
[1,222ms; NP-coordination]
haar zusje
her sister
[1,596ms; S-coordination]
</equation>
<bodyText confidence="0.9998860625">
Both sentences are temporarily ambiguous in the
boldface region. Sentence (1-a) is disambiguated
as an NP-coordination by the sentence-final ad-
verb ook. Sentence (1-b), on the other hand, is dis-
ambiguated as an S-coordination by the sentence-
final verb lachte. Frazier found that the verb lachte
in sentence (1-b) takes longer to process (1,596
ms) than the adverb ook (1,222 ms) in (1-a).
Frazier (1987) explained these findings by as-
suming that the human language processor ad-
heres to the so-called minimal attachment prin-
ciple. According to this principle, the sentence
processor projects the simplest syntactic struc-
ture which is compatible with the material read
at any point in time. NP-coordination is syntac-
tically simpler than S-coordination in that it re-
quires less phrasal nodes to be projected. Hence,
the processor is biased towards NP- over S-coor-
dination. Processing costs are incurred when this
initial preference has to be revised in the disam-
biguating region, as in sentence (1-b), resulting in
longer reading times. Hoeks et al. (2006) have
shown that the NP-coordination preference can be
reduced, but not entirely eliminated, when poor
thematic fit between the verb and a potential object
make an NP-coordination less likely (e.g., Jasper
sanded the board and the carpenter laughed). We
argue here that this residual preference for NP-
coordination can be explained in terms of syntac-
tic and lexical expectation within the framework
of surprisal theory. In contrast to the minimal at-
tachment principle, surprisal theory does not pos-
</bodyText>
<figure confidence="0.996957461538462">
haar zusje
her sister
/
/
Marie
Marie
en
and
/
/
ook
too
/
/
b. Piet
Piet
kuste
kissed
Marie
Marie
en
and
/
/
lachte
laughed
</figure>
<page confidence="0.973227">
72
</page>
<note confidence="0.624707">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 72–80,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921307692308">
tulate specific kinds of syntactic representations or
rely on a metric of syntactic complexity to predict
processing behavior.
This paper is organized as follows. In section
2 below, we briefly sketch basic surprisal theory.
Then we describe how we induced a grammar
from a large annotated Dutch corpus and how sur-
prisal was estimated from this grammar (section
3). In section 4, we describe Frazier’s experiment
on the NP-/S-coordination ambiguity in more de-
tail, and present our surprisal-based simulations of
this data. We conclude with a discussion of our re-
sults in section 5.
</bodyText>
<sectionHeader confidence="0.95848" genericHeader="method">
2 Surprisal Theory
</sectionHeader>
<bodyText confidence="0.999986448275862">
As was mentioned in the introduction, language
processing is highly incremental, and proceeds on
a more or less word-by-word basis. This suggests
that a person’s difficulty with processing a sen-
tence can be modeled on a word level as proposed
by Attneave (1959). Furthermore, it has recently
been suggested that one of the characteristics of
the comprehension system that makes it so fast,
is its ability to anticipate what a speaker will say
next. In other words, the language comprehension
system works predictively (Otten et al., 2007; van
Berkum et al., 2005). Surprisal theory is a model
of differential processing difficulty which accom-
modates both these properties of the comprehen-
sion system, incremental processing and word pre-
diction (Hale, 2001; Levy, 2008). In this theory,
the processing difficulty of a sentence is a func-
tion of word processing difficulty. A word’s dif-
ficulty is inversely proportional to its expectancy,
i.e., the extent to which the word was expected or
predicted in the context in which it occurred. The
lower a word’s expectancy, the more difficult it is
to process. A word’s surprisal is linearly related to
its difficulty. Consequently, words with lower con-
ditional probabilities (expectancy) lead to higher
surprisal than words with higher conditional prob-
abilities.
Surprisal theory is, to some extent, indepen-
dent of the language model that generates condi-
tional word probabilities. Different models can
be used to estimate these probabilities. For all
such models, however, a clear distinction can be
made between lexicalized and unlexicalized sur-
prisal. In lexicalized surprisal, the input to the lan-
guage model is a sequence of words (i.e., a sen-
tence). In unlexicalized surprisal, the input is a
sequence of word categories (i.e., part-of-speech
tags). While previous studies have used unlexical-
ized surprisal to predict reading times, evidence
for lexicalized surprisal is rather sparse. Smith
and Levy (2008) investigated the relation between
lexicalized surprisal and reading time data for nat-
uralistic texts. Using a trigram language model,
they showed that there was a linear relationship
between the two measures. Demberg and Keller
(2008) examined whether this relation extended
beyond transitional probabilities and found no sig-
nificant effects. This state of affairs is somewhat
unfortunate for surprisal theory since input to the
human language processor consists of sequences
of words, not part-of-speech tags. In our study we
therefore used lexicalized surprisal to investigate
whether it can account for reading time data from
the NP-/S-coordination ambiguity in Dutch. Lex-
icalized surprisal furthermore allows us to study
how syntactic expectations might be modulated or
even reversed by lexical expectations in temporar-
ily ambiguous sentences.
</bodyText>
<subsectionHeader confidence="0.941044">
2.1 Probabilistic Context Free Grammars
</subsectionHeader>
<bodyText confidence="0.742272">
Both Hale (2001) and Levy (2008) used a Prob-
abilistic Context Free Grammar (PCFG) as a lan-
guage model in their implementations of surprisal
theory. A PCFG consists of a set of rewrite rules
which are assigned some probability (Charniak,
1993):
</bodyText>
<equation confidence="0.990344">
S NP, VP 1.0
NP Det, N 0.5
NP NP, VP 0.5
. . . � . . . . . .
</equation>
<bodyText confidence="0.996970941176471">
In this toy grammar, for instance, a noun phrase
placeholder can be rewritten to a determiner fol-
lowed by a noun symbol with probability 0.5.
From such a PCFG, the probability of a sentence
can be estimated as the product of the probabili-
ties of all the rules used to derive the sentence. If
a sentence has multiple derivations, its probabil-
ity is the sum of the probabilities for each deriva-
tion. For our purpose, we also needed to obtain the
probability of partial sentences, called prefix prob-
abilities. The prefix probability P(w1...wz) of a
partial sentence w1...wz is the sum of the probabil-
ities of all sentences generated by the PCFG which
share the initial segment w1...wz. Hale (2001)
pointed out that the ratio of the prefix probabilities
P(w1 ... wz) and P(w1 ... wz_1) equals precisely
the conditional probability of word wz. Given a
</bodyText>
<page confidence="0.993467">
73
</page>
<bodyText confidence="0.99310725">
PCFG, the difficulty of word wi can therefore be
defined as:
only a small number of ‘relevant’ analyses would
be considered in parallel.
</bodyText>
<equation confidence="0.610216">
difficulty(wi) a −loge IP(wi ... wi) l 3 Grammar and Parser
P(w1 ... wi−1)J
</equation>
<bodyText confidence="0.999976893617021">
Surprisal theory requires a probabilistic lan-
guage model that generates some form of word
expectancy. The theory itself, however, is largely
neutral with respect to which model is employed.
Models other than PCFGs can be used to esti-
mate surprisal. Nederhof et al. (1998), for in-
stance, show that prefix probabilities, and there-
fore surprisal, can be estimated from Tree Adjoin-
ing Grammars. This approach was taken in Dem-
berg and Keller (2009). Other approaches have
used trigram models (Smith and Levy, 2008), Sim-
ple Recurrent Networks of the Elman type (Frank,
2009), Markov models and Echo-state Networks
(Frank and Bod, 2010). This illustrates that sur-
prisal theory is not committed to specific claims
about the structural representations that language
takes in the human mind. It rather functions as a
“causal bottleneck” between the representations of
a language model, and expectation-based compre-
hension difficulty (Levy, 2008). In other words,
comprehension difficulty does not critically de-
pend on the structural representations postulated
by the language model which is harnessed to gen-
erate word expectancy.
The use of PCFGs raises some important ques-
tions on parallelism in language processing. A
prefix probability can be interpreted as a prob-
ability distribution over all analyses compatible
with a partial sentence. Since partial sentences
can sometimes be completed in an indefinite num-
ber of ways, it seems both practically and psycho-
logically implausible to implement this distribu-
tion as an enumeration over complete structures.
Instead, prefix probabilities should be estimated
as a by-product of incremental processing, as in
Stolcke’s (1995) parser (see section 3.2). This
approach, however, still leaves open how many
analyses are considered in parallel; does the hu-
man sentence processor employ full or limited par-
allelism? Jurafsky (1996) showed that full par-
allelism becomes more and more unmanageable
when the amount of information used for disam-
biguation increases. Levy, on the other hand, ar-
gued that studies of probabilistic parsing reveal
that typically a small number of analyses are as-
signed the majority of probability mass (Roark,
2001). Thus, even when assuming full parallelism,
</bodyText>
<subsectionHeader confidence="0.997127">
3.1 Grammar Induction
</subsectionHeader>
<bodyText confidence="0.999960227272727">
In our simulations, we used a PCFG to model
the phrase structure of natural language. To in-
duce such a grammar, an annotated corpus was
required. We used Alpino (van Noord, 2006)—
a robust and wide-coverage dependency parser
for Dutch—to automatically generate such a cor-
pus, annotated with phrase structure, for 204.000
sentences, which were randomly extracted from
Dutch newspapers. These analyses were then
used to induce a PCFG consisting of 650 gram-
mar rules, 89 non-terminals, and 208.133 termi-
nals (lexical items).1 Moreover, 29 of the 89 non-
terminals could result in epsilon productions.
The Alpino parser constructed the phrase struc-
ture analyses automatically. Despite Alpino’s high
accuracy, some analyses might not be entirely cor-
rect. Nonetheless, the overall quality of Alpino’s
analyses is sufficient for corpus studies, and since
surprisal theory relies largely on corpus features,
we believe the small number of (partially) incor-
rect analyses should not affect the surprisal esti-
mates computed from our PCFG.
</bodyText>
<subsectionHeader confidence="0.998274">
3.2 Earley-Stolcke Parser
</subsectionHeader>
<bodyText confidence="0.997201">
To compute prefix probabilities in our model we
implemented Stolcke’s (1995) probabilistic modi-
fication of Earley’s (1970) parsing algorithm. An
Earley-Stolcke parser is a breadth-first parser. At
each point in processing, the parser maintains a
collection of states that reflect all possible analy-
ses of a partial sentence thus far. A state is a record
that keeps track of:
</bodyText>
<listItem confidence="0.969230333333333">
(a) the position up to which a sentence has been
processed,
(b) the grammar rule that is applied,
(c) a “dot position” indicating which part of the
rule has been processed thus far, and
(d) the leftmost edge of the partial string gener-
ated by the rule.
1A PCFG can be induced by estimating the relative fre-
quency of each CFG rule A --+ α:
</listItem>
<equation confidence="0.893038">
P(A α) �� count�A�α)
� count�A�β)�
</equation>
<page confidence="0.965449">
74
</page>
<bodyText confidence="0.999947516129032">
The collection of states is constantly expanded by
three operations. First upcoming structural and
lexical material is predicted. For all predictions,
new states are added with the “dot” placed on
the leftmost side of the rule. Then it is deter-
mined whether there is a state that predicts the next
word in the input sentence. If this is the case, a
new state is added with the “dot” placed right to
the predicted word. A third operation looks for
states with the “dot” rightmost to a grammar rule,
and then tries to find states which have the com-
pleted state as their leftmost edge. If such states
are found, the “dot” in these states is moved to
the right of this edge. This step is repeated until
no more new states are added. These three op-
erations are cyclically performed until the entire
sentence is processed. Our grammar contained
29 non-terminals that could result in epsilon pro-
ductions. Due to the way epsilon productions are
handled within the Earley-Stolcke parser (i.e., by
means of “spontaneous dot shifting”), having a
large number of epsilon productions leads to a
large number of predicted and completed edges.
As a consequence, pursuing all possible analyses
may become computationally infeasible. To over-
come this problem, we modified the Earley-Stol-
cke parser with a beam A. In prediction and com-
pletion, only the A-number of states with the high-
est probabilities are added.2 This constrains the
number of states generated by the parser and en-
forces limited parallelism.
</bodyText>
<sectionHeader confidence="0.995693" genericHeader="method">
4 NP-/S-coordination ambiguities
</sectionHeader>
<subsectionHeader confidence="0.928047">
4.1 Frazier’s experiment
</subsectionHeader>
<bodyText confidence="0.999988933333333">
Our aim was to determine to what extent lexi-
calized surprisal theory can account for reading
time data for the NP-/S-coordination ambiguity in
Dutch. This type of ambiguity was investigated
by Frazier (1987) using a self-paced reading ex-
periment. The sentences in (2) are part of Fra-
zier’s materials. Sentence (2-a) and (2-b) exem-
plify an NP-/S-coordination ambiguity. The sen-
tences are identical and temporarily ambiguous up
to the NP haar zusje (her sister). In (2-a) this
NP is followed by the adverb ook, and therefore
disambiguated to be part of an NP-coordination;
Marie and haar zusje are conjoined. In (2-b), on
other hand, the same NP is followed by the verb
lachte, and therefore disambiguated as the sub-
</bodyText>
<footnote confidence="0.888226">
2A similar approach was used in Roark (2001) and
Frank (2009).
</footnote>
<bodyText confidence="0.730674">
ject of a conjoined sentence; Piet kuste Marie and
haar zusje lachte are conjoined.
</bodyText>
<figure confidence="0.9779575">
(2) a. Piet kuste
Pete kissed
(Ambiguous; NP-coordination)
b. Piet
Pete
(Ambiguous; S-coordination)
c. Annie
Annie
(Unambiguous; NP-control)
d. Annie
Annie
(Unambiguous; S-control)
</figure>
<bodyText confidence="0.996924275862069">
Sentence (2-c) and (2-d) functioned as unambigu-
ous controls. These sentences are identical up to
the verb zag. In (2-c), the verb is followed by
the single NP haar zusje, and subsequently the ad-
verb ook. The adverb eliminates the possibility of
an NP-coordination. In (2-d), on the other hand,
the same verb is followed by the complementizer
dat, indicating that the clause her sister laughed is
a subordinate clause (the complementizer is oblig-
atory in Dutch).
Frazier constructed twelve sets consisting of
four of such sentences each. The 48 sentences
were divided into three frames. The first frame
included all the material up to the critical NP
haar zusje in (2). The second frame contained only
the critical NP itself, and the third frame contained
all the material that followed this NP.
40 native Dutch speakers participated in the ex-
periment. Reading times for the final frames were
collected using a self-paced reading task. Figure 1
depicts the mean reading times for each of the four
conditions.
Frazier found a significant interaction between
Type of Coordination (NP- versus S-coordination)
and Ambiguity (ambiguous versus control) indi-
cating that the effect of disambiguation was larger
for S-coordinations (ambiguous: 1596 ms; con-
trol: 1141 ms) than for NP-coordinations (ambigu-
ous: 1222 ms; control: 1082 ms).
</bodyText>
<subsectionHeader confidence="0.975328">
4.2 Simulations
</subsectionHeader>
<bodyText confidence="0.9998545">
We simulated Frazier’s experiment in our model.
Since one set of sentences contained a word that
was not covered by our lexicon (set 11; “Lor-
raine”), we used only eleven of the twelve sets
of test items from her study. The remaining 44
sentences were successfully analyzed. In our first
</bodyText>
<figure confidence="0.993550821428572">
ook
Marie
Marie
zusje
sister
haar
her
too
en
and
en
and
kuste
kissed
Marie
Marie
zusje
sister
haar
her
lachte
laughed
ook
zusje
sister
haar
her
zag
saw
too
zag
saw
zusje
sister
haar
her
dat
that
lachte
laughed
75
ambiguous
unambiguous
NP−coord/control S−coord/control
type of coordination
mean reading times (ms)
400 800 1200 1600
NP−/S−control
NP−/S−coordination
32 16 8 4
difference in means (NP* − S*)
−200
−400
−600
200
0
</figure>
<figureCaption confidence="0.9753855">
Figure 1: Reading time data for the NP-/S-coordi-
nation ambiguity (Frazier, 1987).
</figureCaption>
<bodyText confidence="0.820261428571428">
simulation we fixed a beam of A = 16. Figure 2
depicts surprisal values in the sentence-final frame
as estimated by our model. When final frames
contained multiple words, we averaged the sur-
prisal values for these words. As Figure 2 shows,
NP−coord/control S−coord/control
type of coordination
</bodyText>
<figureCaption confidence="0.91535">
Figure 2: Mean surprisal values for the final frame
in the model (A = 16).
</figureCaption>
<bodyText confidence="0.999758166666667">
our model successfully replicated the effects re-
ported in Frazier (1987): In both types of coordi-
nations there was a difference in mean surprisal
between the ambiguous sentences and the con-
trols, but in the S-coordinations this effect was
larger than in the sentences with NP-coordination.
Statistical analyses confirmed our findings. An
ANOVA on surprisal values per item revealed an
interaction between Type of Coordination (NP- vs.
S-coordination) and Ambiguity (ambiguous vs.
control), which was marginally significant (p =
0.06), most probably due to the small number of
</bodyText>
<figure confidence="0.591876">
beam
</figure>
<figureCaption confidence="0.852929">
Figure 3: Differences between NP versus S sur-
</figureCaption>
<bodyText confidence="0.989209242424243">
prisal for different beam sizes (As).
items (i.e., 11) available for this statistical test (re-
call that the test in the original experiment was
based on 40 participants). Follow-up analyses re-
vealed that the difference between S-coordination
and S-control was significant (p &lt; 0.05), whereas
the difference between NP-coordination and NP-
control was not (p = 0.527).
To test the robustness of these findings, we re-
peated the simulation with different beam sizes
(As) by iteratively halving the beam, starting with
A = 32. Figure 3 shows the differences in
mean surprisal between NP-coordination and S-
coordination, and NP-control and S-control. With
the beam set to four (A = 4), we did not obtain full
analyses for all test items. Consequently, two sets
of items had to be disregarded (sets 8 and 9). For
the remaining items, however, we obtained an NP-
coordination preference for all beam sizes. The
largest difference occurred for A = 16. When
the beam was set to A &lt; 8, the difference stabi-
lized. Taking everything into account, the model
with A = 16 led to the best overall match with
Frazier’s reading time data.
As for the interaction, Figure 4 depicts the dif-
ferences in mean surprisal between NP-coordina-
tion and NP-control, and S-coordination and S-
control. These results indicate that we robustly
replicated the interaction between coordination
type and ambiguity. For all beam sizes, S-co-
ordination benefited more from disambiguation
than NP-coordination, i.e., the difference in means
between S-coordination and S-control was larger
</bodyText>
<figure confidence="0.962554833333333">
ambiguous
unambiguous
5000 5500 6000 6500 7000 7500
mean surprisal
76
beam
</figure>
<figureCaption confidence="0.9178895">
Figure 4: Differences in coordination versus con-
trol surprisal for different beam sizes (As).
</figureCaption>
<bodyText confidence="0.99793775">
than the difference in means between NP-coordi-
nation and NP-control.
In our simulations, we found that surprisal the-
ory can account for reading time data from a clas-
sic experiment on the NP-/S-coordination ambigu-
ity in Dutch reported by Frazier (1987). This sug-
gests that the interplay between syntactic and lex-
ical expectancy might be sufficient to explain an
NP-coordination preference in human subjects. In
the remainder of this section, we analyze our re-
sults and explain how this preference arises in the
model.
</bodyText>
<subsectionHeader confidence="0.999905">
4.3 Model Analysis
</subsectionHeader>
<bodyText confidence="0.999993">
To determine what caused the NP-preference in
our model, we inspected surprisal differences
item-by-item. Whether the NP-coordination pref-
erence was syntactic or lexical in nature should
be reflected in the grammar. If it was syntactic,
NP-coordination would have a higher probability
than S-coordination according to our PCFG. If, on
the other hand, it was lexical, NP- and S-coor-
dination should be equally probable syntactically.
Another possibility, however, is that syntactic and
lexical probabilities interacted. If this was the
case, we should expect NP-coordinations to lead
to lower surprisal values on average only, but not
necessarily on every item. Figure 5 shows the es-
timated surprisal values per sentence-final frame
for the ambiguous condition and Figure 6 for the
unambiguous condition. Figure 5 indicates that
although NP-coordination led to lower surprisal
</bodyText>
<figure confidence="0.629109">
sentences
</figure>
<figureCaption confidence="0.991455">
Figure 5: Surprisal per sentence for final frames in
the ambiguous condition.
</figureCaption>
<figure confidence="0.587073">
sentences
</figure>
<figureCaption confidence="0.9754295">
Figure 6: Surprisal per sentence for final frames in
the unambiguous condition.
</figureCaption>
<bodyText confidence="0.993218466666667">
overall (see Figure 2), this was not the case for all
tested items. A similar pattern was found for the
NP-control versus S-control items in Figure 6. S-
controls led to lower surprisal overall, but not for
all items. Manual inspection of the grammar re-
vealed a bias towards NP-coordination. A total of
115 PCFG rules concerned coordination (Pz� 18%
of the entire grammar). As these rules expanded
the same grammatical category, their probabilities
summed to 1. A rule-by-rule inspection showed
that approximately 48% of the probability mass
was assigned to rules that dealt with NP-coordi-
nations, 22% to rules that dealt with S-coordina-
tions, and the remaining 30% to rules that dealt
with coordination in other structures. In other
</bodyText>
<figure confidence="0.995343695652174">
NP−coordination/NP−control
S−coordination/S−control
32 16 8 4
1500
1000
500
0
difference in means (&amp;quot;−coord. − &amp;quot;−control)
NP−coordination
S−coordination
8000
7000
6000
5000
surprisals
1 2 3 4 5 6 7 8 9 10 12
NP−control
S−control
7000
6000
5000
surprisals
1 2 3 4 5 6 7 8 9 10 12
</figure>
<page confidence="0.996218">
77
</page>
<bodyText confidence="0.999994047619048">
words, there was a clear preference for NP-coordi-
nation in the grammar. Despite this bias, for some
tested items the S-coordination received lower sur-
prisal than the NP-coordination (Figure 5). In-
dividual NP-coordination rules might have lower
probability than individual S-coordination rules,
so the overall preference for NP-coordination in
the grammar therefore does not have to be re-
flected in every test item. Secondly, syntactic
probabilities could be modified by lexical proba-
bilities. Suppose for a pair of test items that NP-
coordination was syntactically preferred over S-
coordination. If the sentence was disambiguated
as an NP-coordination by a highly improbable lex-
ical item, and disambiguated as an S-coordination
by a highly probable lexical item, surprisal for the
NP-coordination might turn out higher than sur-
prisal for the S-coordination. In this way, lexical
factors could override the NP-coordination bias in
the grammar, leading to a preference for S-coordi-
nation in some items.
To summarize, the PCFG displayed an over-
all NP-coordination preference when surprisal was
averaged over the test sentences and this result is
consistent with the findings of Frazier (1987). The
NP-coordination preference, however, was not in-
variably reflected on an item-by-item basis. Some
S-coordinations showed lower surprisal than the
corresponding NP-coordinations. This reversal of
processing difficulty can be explained in terms of
differences in individual rules, and in terms of in-
teractions between syntactic and lexical probabil-
ities. This suggests that specific lexical expecta-
tions might have a much stronger effect on disam-
biguation preferences than supposed by the min-
imal attachment principle. Unfortunately, Frazier
(1987) only reported mean reading times for the
two coordination types.3 It would be interesting to
compare the predictions from our surprisal model
with human data item-by-item in order to validate
the magnitude of lexical effects we found in the
model.
</bodyText>
<sectionHeader confidence="0.999763" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99123225">
In this paper we have shown that a model of lex-
icalized surprisal, based on an automatically in-
duced PCFG, can account for the NP-/S-ambiguity
reading time data of Frazier (1987). We found
</bodyText>
<footnote confidence="0.812532">
3Thus it was not possible to determine the strength of the
correlation between reading times in Frazier’s study and sur-
prisal in our model.
</footnote>
<bodyText confidence="0.999951395833334">
these results to be robust for a critical model pa-
rameter (beam size), which suggests that syntac-
tic processing in human comprehension might be
based on limited parallelism only. Surprisal the-
ory models processing difficulty on a word level.
A word’s difficulty is related to the expectations
the language processor forms, given the structural
and lexical material that precedes it. The model
showed a clear preference for NP-coordination
which suggests that structural and lexical expec-
tations as estimated from a corpus might be suffi-
cient to explain the NP-coordination bias in human
sentence processing.
Our account of this bias differs considerably
from the original account proposed by Frazier
(minimal attachment principle) in a number of
ways. Frazier’s explanation is based on a met-
ric of syntactic complexity which in turn depends
on quite specific syntactic representations of a
language’s phrase structure. Surprisal theory, on
the other hand, is largely neutral with respect to
the form syntactic representations take in the hu-
man mind.4 Moreover, differential processing in
surprisal-based models does not require the speci-
fication of a notion of syntactic complexity. Both
these aspects make surprisal theory a parsimo-
nious explanatory framework. The minimal at-
tachment principle postulates that the bias towards
NP-coordination is an initial processing primitive.
In contrast, the bias in our simulations is a func-
tion of the model’s input history and linguistic
experience from which the grammar is induced.
It is further modulated by the immediate context
from which upcoming words are predicted dur-
ing processing. Consequently, the model’s prefer-
ence for one structural type can vary across sen-
tence tokens and even be reversed on occasion.
We argued that our grammar showed an over-
all preference for NP-coordination but this pref-
erence was not necessarily reflected on each and
every rule that dealt with coordinations. Some S-
coordination rules could have higher probability
than NP-coordination rules. In addition, syntac-
tic expectations were modified by lexical expec-
tations. Thus, even when NP-coordination was
structurally favored over S-coordination, highly
unexpected lexical material could lead to more
processing difficulty for NP-coordination than for
</bodyText>
<footnote confidence="0.99885725">
4This is not to say, of course, that the choice of language
model to estimate surprisal is completely irrelevant; differ-
ent models will yield different degrees of fit, see Frank and
Bod (2010).
</footnote>
<page confidence="0.997635">
78
</page>
<bodyText confidence="0.999964688888889">
S-coordination. Surprisal theory allows us to build
a formally precise computational model of read-
ing time data which generates testable, quantita-
tive predictions about the differential processing
of individual test items. These predictions (Figure
5) indicate that mean reading times for a set of NP-
/S-coordination sentences may not be adequate to
tap the origin of differential processing difficulty.
Our results are consistent with the findings of
Hoeks et al. (2002), who also found evidence
for an NP-coordination preference in a self-paced
reading experiment as well as in an eye-tracking
experiment. They suggested that NP-coordination
might be easier to process because it has a sim-
pler topic structure than S-coordination. The for-
mer only has one topic, whereas the latter has two.
Hoeks et al. (2002) argue that having more than
one topic is unexpected. Sentences with more than
one topic will therefore cause more processing dif-
ficulty. This preference for simple topic-structure
that was evident in language comprehension may
also be present in language production, and hence
in language corpora. Thus, it may very well be
the case that the NP-coordination preference that
was present in our training corpus may have had
a pragmatic origin related to topic-structure. The
outcome of our surprisal model is also compati-
ble with the results of Hoeks et al. (2006) who
found that thematic information can strongly re-
duce but not completely eliminate the NP-coordi-
nation preference. Surprisal theory is explicitly
built on the assumption that multiple sources of
information can interact in parallel at any point in
time during sentence processing. Accordingly, we
suggest here that the residual preference for NP-
coordination found in the study of Hoeks et al.
(2006) might be explained in terms of syntactic
and lexical expectation. And finally, our approach
is consistent with a large body of evidence indi-
cating that language comprehension is incremen-
tal and makes use of expectation-driven word pre-
diction (Pickering and Garrod, 2007). It remains
to be tested whether our model can explain behav-
ioral data from the processing of ambiguities other
than the Dutch NP- versus S-coordination case.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999787701754386">
G. Altmann and Y. Kamide. 1999. Incremental inter-
pretation at verbs: Restricting the domain of subse-
quent reference. Cognition, 73:247–264.
F. Attneave. 1959. Applications of Information Theory
to Psychology: A summary of basic concepts, meth-
ods, and results. Holt, Rinehart and Winston.
E. Charniak. 1993. Statistical Language Learning.
MIT Press.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193–210.
V. Demberg and F. Keller. 2009. A computational
model of prediction in human parsing: Unifying lo-
cality and surprisal effects. In Proceedings of the
31st Annual Conference of the Cognitive Science So-
ciety, Amsterdam, the Netherlands.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 6:451–455.
S. Frank and R. Bod. 2010. The irrelevance of hi-
erarchical structure to human sentence processing.
Unpublished manuscript.
S. Frank. 2009. Surprisal-based comparison between a
symbolic and a connectionist model of sentence pro-
cessing. In Proceedings of the 31th Annual Confer-
ence of the Cognitive Science Society, pages 1139–
1144, Amsterdam, the Netherlands.
L. Frazier. 1987. Syntactic processing: Evidence from
Dutch. Natural Langauge and Linguistic Theory,
5:519–559.
J. Hale. 2001. A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of the 2nd Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, volume 2,
pages 159–166.
J. Hoeks, W. Vonk, and H. Schriefers. 2002. Process-
ing coordinated structures in context: The effect of
topic-structure on ambiguity resolution. Journal of
Memory and Language, 46:99–119.
J. Hoeks, P. Hendriks, W. Vonk, C. Brown, and P. Ha-
goort. 2006. Processing the noun phrase versus sen-
tence coordination ambiguity: Thematic informa-
tion does not completely eliminate processing dif-
ficulty. The Quarterly Journal of Experimental Psy-
chology, 59:1581–1599.
D. Jurafsky. 1996. A probabilistic model of lexical
and syntactic access and disambiguation. Cognitive
Science, 20:137–147.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.
M. Nederhof, A. Sarkar, and G. Satta. 1998. Prefix
probabilities from stochastic tree adjoining gram-
mar. In Proceedings of COLING-ACL ’98, pages
953–959, Montreal.
M. Otten, M. Nieuwland, and J. van Berkum. 2007.
Great expectations: Specific lexical anticipation in-
fluences the processing of spoken language. BMC
Neuroscience.
</reference>
<page confidence="0.97741">
79
</page>
<reference confidence="0.99959036">
M. Pickering and S. Garrod. 2007. Do people use lan-
guage production to make predictions during com-
prehension? Trends in Cognitive Sciences, 11:105–
110.
B. Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27:249–276.
N. Smith and R. Levy. 2008. Optimal processing times
in reading: A formal model and empirical investi-
gation. In Proceedings of the 30th annual confer-
ence of the Cognitive Science Society, pages 595–
600, Austin, TX.
A. Stolcke. 1995. An efficient probabilistic context-
free parsing algorithm that computes prefix proba-
bilities. Computational linguistics, 21:165–201.
J. van Berkum, C. Brown, P. Zwitserlood, V. Kooij-
man, and P. Hagoort. 2005. Anticipating upcom-
ing words in discourse: Evidence from ERPs and
reading times. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 31:443–467.
G. van Noord. 2006. At last parsing is now op-
erational. In Verbum Ex Machina. Actes de la
13e conf´erence sur le traitement automatique des
langues naturelles, pages 20–42. Presses universi-
taires de Louvain.
</reference>
<page confidence="0.998229">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.113695">
<title confidence="0.9783955">Modeling the Noun Phrase versus Sentence Coordination Ambiguity Dutch: Evidence from Surprisal Theory</title>
<author confidence="0.957737">Harm</author>
<affiliation confidence="0.986627">University of</affiliation>
<address confidence="0.631883">Groningen, the</address>
<email confidence="0.859901">harm.brouwer@rug.nl</email>
<affiliation confidence="0.766342">Hartmut University of</affiliation>
<address confidence="0.577471">Groningen, the</address>
<email confidence="0.747752">h.fitz@rug.nl</email>
<author confidence="0.946249">C J John</author>
<affiliation confidence="0.993431">University of</affiliation>
<address confidence="0.697221">Groningen, the</address>
<email confidence="0.883833">j.c.j.hoeks@rug.nl</email>
<abstract confidence="0.9975373125">This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch. Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus. We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Altmann</author>
<author>Y Kamide</author>
</authors>
<title>Incremental interpretation at verbs: Restricting the domain of subsequent reference.</title>
<date>1999</date>
<journal>Cognition,</journal>
<pages>73--247</pages>
<contexts>
<context position="1125" citStr="Altmann and Kamide, 1999" startWordPosition="152" endWordPosition="155">n Dutch. Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus. We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference. 1 Introduction Language comprehension is incremental in that meaning is continuously assigned to utterances as they are encountered word-by-word (Altmann and Kamide, 1999). Not all words, however, are equally easy to process. A word’s processing difficulty is affected by, for instance, its frequency or its effect on the syntactic and semantic interpretation of a sentence. A recent theory of sentence processing, surprisal theory (Hale, 2001; Levy, 2008), combines several of these aspects into one single concept, namely the surprisal of a word. A word’s surprisal is proportional to its expectancy, i.e., the extent to which that word is expected (or predicted). The processing difficulty a word causes during comprehension is argued to be related linearly to its sur</context>
</contexts>
<marker>Altmann, Kamide, 1999</marker>
<rawString>G. Altmann and Y. Kamide. 1999. Incremental interpretation at verbs: Restricting the domain of subsequent reference. Cognition, 73:247–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Attneave</author>
</authors>
<title>Applications of Information Theory to Psychology: A summary of basic concepts, methods, and results.</title>
<date>1959</date>
<contexts>
<context position="4944" citStr="Attneave (1959)" startWordPosition="768" endWordPosition="769">e induced a grammar from a large annotated Dutch corpus and how surprisal was estimated from this grammar (section 3). In section 4, we describe Frazier’s experiment on the NP-/S-coordination ambiguity in more detail, and present our surprisal-based simulations of this data. We conclude with a discussion of our results in section 5. 2 Surprisal Theory As was mentioned in the introduction, language processing is highly incremental, and proceeds on a more or less word-by-word basis. This suggests that a person’s difficulty with processing a sentence can be modeled on a word level as proposed by Attneave (1959). Furthermore, it has recently been suggested that one of the characteristics of the comprehension system that makes it so fast, is its ability to anticipate what a speaker will say next. In other words, the language comprehension system works predictively (Otten et al., 2007; van Berkum et al., 2005). Surprisal theory is a model of differential processing difficulty which accommodates both these properties of the comprehension system, incremental processing and word prediction (Hale, 2001; Levy, 2008). In this theory, the processing difficulty of a sentence is a function of word processing di</context>
</contexts>
<marker>Attneave, 1959</marker>
<rawString>F. Attneave. 1959. Applications of Information Theory to Psychology: A summary of basic concepts, methods, and results. Holt, Rinehart and Winston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7768" citStr="Charniak, 1993" startWordPosition="1201" endWordPosition="1202">ur study we therefore used lexicalized surprisal to investigate whether it can account for reading time data from the NP-/S-coordination ambiguity in Dutch. Lexicalized surprisal furthermore allows us to study how syntactic expectations might be modulated or even reversed by lexical expectations in temporarily ambiguous sentences. 2.1 Probabilistic Context Free Grammars Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory. A PCFG consists of a set of rewrite rules which are assigned some probability (Charniak, 1993): S NP, VP 1.0 NP Det, N 0.5 NP NP, VP 0.5 . . . � . . . . . . In this toy grammar, for instance, a noun phrase placeholder can be rewritten to a determiner followed by a noun symbol with probability 0.5. From such a PCFG, the probability of a sentence can be estimated as the product of the probabilities of all the rules used to derive the sentence. If a sentence has multiple derivations, its probability is the sum of the probabilities for each derivation. For our purpose, we also needed to obtain the probability of partial sentences, called prefix probabilities. The prefix probability P(w1...</context>
</contexts>
<marker>Charniak, 1993</marker>
<rawString>E. Charniak. 1993. Statistical Language Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>109--193</pages>
<contexts>
<context position="6872" citStr="Demberg and Keller (2008)" startWordPosition="1065" endWordPosition="1068">ed and unlexicalized surprisal. In lexicalized surprisal, the input to the language model is a sequence of words (i.e., a sentence). In unlexicalized surprisal, the input is a sequence of word categories (i.e., part-of-speech tags). While previous studies have used unlexicalized surprisal to predict reading times, evidence for lexicalized surprisal is rather sparse. Smith and Levy (2008) investigated the relation between lexicalized surprisal and reading time data for naturalistic texts. Using a trigram language model, they showed that there was a linear relationship between the two measures. Demberg and Keller (2008) examined whether this relation extended beyond transitional probabilities and found no significant effects. This state of affairs is somewhat unfortunate for surprisal theory since input to the human language processor consists of sequences of words, not part-of-speech tags. In our study we therefore used lexicalized surprisal to investigate whether it can account for reading time data from the NP-/S-coordination ambiguity in Dutch. Lexicalized surprisal furthermore allows us to study how syntactic expectations might be modulated or even reversed by lexical expectations in temporarily ambiguo</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>V. Demberg and F. Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>A computational model of prediction in human parsing: Unifying locality and surprisal effects.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Cognitive Science Society,</booktitle>
<location>Amsterdam, the Netherlands.</location>
<contexts>
<context position="9337" citStr="Demberg and Keller (2009)" startWordPosition="1477" endWordPosition="1481"> can therefore be defined as: only a small number of ‘relevant’ analyses would be considered in parallel. difficulty(wi) a −loge IP(wi ... wi) l 3 Grammar and Parser P(w1 ... wi−1)J Surprisal theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language model, and expectation-based comprehension difficulty (Levy, 2008). In other words, comprehension difficulty does not critically depend on the structural representations post</context>
</contexts>
<marker>Demberg, Keller, 2009</marker>
<rawString>V. Demberg and F. Keller. 2009. A computational model of prediction in human parsing: Unifying locality and surprisal effects. In Proceedings of the 31st Annual Conference of the Cognitive Science Society, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<pages>6--451</pages>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6:451–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Frank</author>
<author>R Bod</author>
</authors>
<title>The irrelevance of hierarchical structure to human sentence processing.</title>
<date>2010</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="9523" citStr="Frank and Bod, 2010" startWordPosition="1507" endWordPosition="1510"> theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language model, and expectation-based comprehension difficulty (Levy, 2008). In other words, comprehension difficulty does not critically depend on the structural representations postulated by the language model which is harnessed to generate word expectancy. The use of PCFGs raises some important questions on parallelism in language processing. A prefix probability </context>
<context position="27778" citStr="Frank and Bod (2010)" startWordPosition="4403" endWordPosition="4406">ce was not necessarily reflected on each and every rule that dealt with coordinations. Some Scoordination rules could have higher probability than NP-coordination rules. In addition, syntactic expectations were modified by lexical expectations. Thus, even when NP-coordination was structurally favored over S-coordination, highly unexpected lexical material could lead to more processing difficulty for NP-coordination than for 4This is not to say, of course, that the choice of language model to estimate surprisal is completely irrelevant; different models will yield different degrees of fit, see Frank and Bod (2010). 78 S-coordination. Surprisal theory allows us to build a formally precise computational model of reading time data which generates testable, quantitative predictions about the differential processing of individual test items. These predictions (Figure 5) indicate that mean reading times for a set of NP/S-coordination sentences may not be adequate to tap the origin of differential processing difficulty. Our results are consistent with the findings of Hoeks et al. (2002), who also found evidence for an NP-coordination preference in a self-paced reading experiment as well as in an eye-tracking </context>
</contexts>
<marker>Frank, Bod, 2010</marker>
<rawString>S. Frank and R. Bod. 2010. The irrelevance of hierarchical structure to human sentence processing. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 31th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<location>Amsterdam, the Netherlands.</location>
<contexts>
<context position="9462" citStr="Frank, 2009" startWordPosition="1500" endWordPosition="1501"> wi) l 3 Grammar and Parser P(w1 ... wi−1)J Surprisal theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language model, and expectation-based comprehension difficulty (Levy, 2008). In other words, comprehension difficulty does not critically depend on the structural representations postulated by the language model which is harnessed to generate word expectancy. The use of PCFGs raises some important questions</context>
<context position="15263" citStr="Frank (2009)" startWordPosition="2433" endWordPosition="2434">ambiguity was investigated by Frazier (1987) using a self-paced reading experiment. The sentences in (2) are part of Frazier’s materials. Sentence (2-a) and (2-b) exemplify an NP-/S-coordination ambiguity. The sentences are identical and temporarily ambiguous up to the NP haar zusje (her sister). In (2-a) this NP is followed by the adverb ook, and therefore disambiguated to be part of an NP-coordination; Marie and haar zusje are conjoined. In (2-b), on other hand, the same NP is followed by the verb lachte, and therefore disambiguated as the sub2A similar approach was used in Roark (2001) and Frank (2009). ject of a conjoined sentence; Piet kuste Marie and haar zusje lachte are conjoined. (2) a. Piet kuste Pete kissed (Ambiguous; NP-coordination) b. Piet Pete (Ambiguous; S-coordination) c. Annie Annie (Unambiguous; NP-control) d. Annie Annie (Unambiguous; S-control) Sentence (2-c) and (2-d) functioned as unambiguous controls. These sentences are identical up to the verb zag. In (2-c), the verb is followed by the single NP haar zusje, and subsequently the adverb ook. The adverb eliminates the possibility of an NP-coordination. In (2-d), on the other hand, the same verb is followed by the comple</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>S. Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In Proceedings of the 31th Annual Conference of the Cognitive Science Society, pages 1139– 1144, Amsterdam, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
</authors>
<date>1987</date>
<booktitle>Syntactic processing: Evidence from Dutch. Natural Langauge and Linguistic Theory,</booktitle>
<pages>5--519</pages>
<contexts>
<context position="784" citStr="Frazier (1987)" startWordPosition="104" endWordPosition="105">arm.brouwer@rug.nl Hartmut Fitz University of Groningen Groningen, the Netherlands h.fitz@rug.nl John C. J. Hoeks University of Groningen Groningen, the Netherlands j.c.j.hoeks@rug.nl Abstract This paper investigates whether surprisal theory can account for differential processing difficulty in the NP-/S-coordination ambiguity in Dutch. Surprisal is estimated using a Probabilistic Context-Free Grammar (PCFG), which is induced from an automatically annotated corpus. We find that our lexicalized surprisal model can account for the reading time data from a classic experiment on this ambiguity by Frazier (1987). We argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference. 1 Introduction Language comprehension is incremental in that meaning is continuously assigned to utterances as they are encountered word-by-word (Altmann and Kamide, 1999). Not all words, however, are equally easy to process. A word’s processing difficulty is affected by, for instance, its frequency or its effect on the syntactic and semantic interpretation of a sentence. A recent theory of sentence processing, surprisal theor</context>
<context position="2082" citStr="Frazier (1987)" startWordPosition="309" endWordPosition="310">t, namely the surprisal of a word. A word’s surprisal is proportional to its expectancy, i.e., the extent to which that word is expected (or predicted). The processing difficulty a word causes during comprehension is argued to be related linearly to its surprisal; the higher the surprisal value of a word, the more difficult it is to process. In this paper we investigate whether surprisal theory can account for the processing difficulty involved in sentences containing the noun phrase (NP) versus sentence (S) coordination ambiguity. The sentences in (1), from a self-paced reading experiment by Frazier (1987), exemplify this ambiguity: (1) a. Piet kuste Piet kissed [1,222ms; NP-coordination] haar zusje her sister [1,596ms; S-coordination] Both sentences are temporarily ambiguous in the boldface region. Sentence (1-a) is disambiguated as an NP-coordination by the sentence-final adverb ook. Sentence (1-b), on the other hand, is disambiguated as an S-coordination by the sentencefinal verb lachte. Frazier found that the verb lachte in sentence (1-b) takes longer to process (1,596 ms) than the adverb ook (1,222 ms) in (1-a). Frazier (1987) explained these findings by assuming that the human language pr</context>
<context position="14695" citStr="Frazier (1987)" startWordPosition="2336" endWordPosition="2337">ce, pursuing all possible analyses may become computationally infeasible. To overcome this problem, we modified the Earley-Stolcke parser with a beam A. In prediction and completion, only the A-number of states with the highest probabilities are added.2 This constrains the number of states generated by the parser and enforces limited parallelism. 4 NP-/S-coordination ambiguities 4.1 Frazier’s experiment Our aim was to determine to what extent lexicalized surprisal theory can account for reading time data for the NP-/S-coordination ambiguity in Dutch. This type of ambiguity was investigated by Frazier (1987) using a self-paced reading experiment. The sentences in (2) are part of Frazier’s materials. Sentence (2-a) and (2-b) exemplify an NP-/S-coordination ambiguity. The sentences are identical and temporarily ambiguous up to the NP haar zusje (her sister). In (2-a) this NP is followed by the adverb ook, and therefore disambiguated to be part of an NP-coordination; Marie and haar zusje are conjoined. In (2-b), on other hand, the same NP is followed by the verb lachte, and therefore disambiguated as the sub2A similar approach was used in Roark (2001) and Frank (2009). ject of a conjoined sentence; </context>
<context position="17666" citStr="Frazier, 1987" startWordPosition="2820" endWordPosition="2821">elve sets of test items from her study. The remaining 44 sentences were successfully analyzed. In our first ook Marie Marie zusje sister haar her too en and en and kuste kissed Marie Marie zusje sister haar her lachte laughed ook zusje sister haar her zag saw too zag saw zusje sister haar her dat that lachte laughed 75 ambiguous unambiguous NP−coord/control S−coord/control type of coordination mean reading times (ms) 400 800 1200 1600 NP−/S−control NP−/S−coordination 32 16 8 4 difference in means (NP* − S*) −200 −400 −600 200 0 Figure 1: Reading time data for the NP-/S-coordination ambiguity (Frazier, 1987). simulation we fixed a beam of A = 16. Figure 2 depicts surprisal values in the sentence-final frame as estimated by our model. When final frames contained multiple words, we averaged the surprisal values for these words. As Figure 2 shows, NP−coord/control S−coord/control type of coordination Figure 2: Mean surprisal values for the final frame in the model (A = 16). our model successfully replicated the effects reported in Frazier (1987): In both types of coordinations there was a difference in mean surprisal between the ambiguous sentences and the controls, but in the S-coordinations this e</context>
<context position="20625" citStr="Frazier (1987)" startWordPosition="3296" endWordPosition="3297">ion type and ambiguity. For all beam sizes, S-coordination benefited more from disambiguation than NP-coordination, i.e., the difference in means between S-coordination and S-control was larger ambiguous unambiguous 5000 5500 6000 6500 7000 7500 mean surprisal 76 beam Figure 4: Differences in coordination versus control surprisal for different beam sizes (As). than the difference in means between NP-coordination and NP-control. In our simulations, we found that surprisal theory can account for reading time data from a classic experiment on the NP-/S-coordination ambiguity in Dutch reported by Frazier (1987). This suggests that the interplay between syntactic and lexical expectancy might be sufficient to explain an NP-coordination preference in human subjects. In the remainder of this section, we analyze our results and explain how this preference arises in the model. 4.3 Model Analysis To determine what caused the NP-preference in our model, we inspected surprisal differences item-by-item. Whether the NP-coordination preference was syntactic or lexical in nature should be reflected in the grammar. If it was syntactic, NP-coordination would have a higher probability than S-coordination according </context>
<context position="24163" citStr="Frazier (1987)" startWordPosition="3851" endWordPosition="3852"> over Scoordination. If the sentence was disambiguated as an NP-coordination by a highly improbable lexical item, and disambiguated as an S-coordination by a highly probable lexical item, surprisal for the NP-coordination might turn out higher than surprisal for the S-coordination. In this way, lexical factors could override the NP-coordination bias in the grammar, leading to a preference for S-coordination in some items. To summarize, the PCFG displayed an overall NP-coordination preference when surprisal was averaged over the test sentences and this result is consistent with the findings of Frazier (1987). The NP-coordination preference, however, was not invariably reflected on an item-by-item basis. Some S-coordinations showed lower surprisal than the corresponding NP-coordinations. This reversal of processing difficulty can be explained in terms of differences in individual rules, and in terms of interactions between syntactic and lexical probabilities. This suggests that specific lexical expectations might have a much stronger effect on disambiguation preferences than supposed by the minimal attachment principle. Unfortunately, Frazier (1987) only reported mean reading times for the two coo</context>
</contexts>
<marker>Frazier, 1987</marker>
<rawString>L. Frazier. 1987. Syntactic processing: Evidence from Dutch. Natural Langauge and Linguistic Theory, 5:519–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<contexts>
<context position="1397" citStr="Hale, 2001" startWordPosition="199" endWordPosition="200">e argue that syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference. 1 Introduction Language comprehension is incremental in that meaning is continuously assigned to utterances as they are encountered word-by-word (Altmann and Kamide, 1999). Not all words, however, are equally easy to process. A word’s processing difficulty is affected by, for instance, its frequency or its effect on the syntactic and semantic interpretation of a sentence. A recent theory of sentence processing, surprisal theory (Hale, 2001; Levy, 2008), combines several of these aspects into one single concept, namely the surprisal of a word. A word’s surprisal is proportional to its expectancy, i.e., the extent to which that word is expected (or predicted). The processing difficulty a word causes during comprehension is argued to be related linearly to its surprisal; the higher the surprisal value of a word, the more difficult it is to process. In this paper we investigate whether surprisal theory can account for the processing difficulty involved in sentences containing the noun phrase (NP) versus sentence (S) coordination am</context>
<context position="5438" citStr="Hale, 2001" startWordPosition="845" endWordPosition="846"> that a person’s difficulty with processing a sentence can be modeled on a word level as proposed by Attneave (1959). Furthermore, it has recently been suggested that one of the characteristics of the comprehension system that makes it so fast, is its ability to anticipate what a speaker will say next. In other words, the language comprehension system works predictively (Otten et al., 2007; van Berkum et al., 2005). Surprisal theory is a model of differential processing difficulty which accommodates both these properties of the comprehension system, incremental processing and word prediction (Hale, 2001; Levy, 2008). In this theory, the processing difficulty of a sentence is a function of word processing difficulty. A word’s difficulty is inversely proportional to its expectancy, i.e., the extent to which the word was expected or predicted in the context in which it occurred. The lower a word’s expectancy, the more difficult it is to process. A word’s surprisal is linearly related to its difficulty. Consequently, words with lower conditional probabilities (expectancy) lead to higher surprisal than words with higher conditional probabilities. Surprisal theory is, to some extent, independent o</context>
<context position="7542" citStr="Hale (2001)" startWordPosition="1163" endWordPosition="1164">l probabilities and found no significant effects. This state of affairs is somewhat unfortunate for surprisal theory since input to the human language processor consists of sequences of words, not part-of-speech tags. In our study we therefore used lexicalized surprisal to investigate whether it can account for reading time data from the NP-/S-coordination ambiguity in Dutch. Lexicalized surprisal furthermore allows us to study how syntactic expectations might be modulated or even reversed by lexical expectations in temporarily ambiguous sentences. 2.1 Probabilistic Context Free Grammars Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory. A PCFG consists of a set of rewrite rules which are assigned some probability (Charniak, 1993): S NP, VP 1.0 NP Det, N 0.5 NP NP, VP 0.5 . . . � . . . . . . In this toy grammar, for instance, a noun phrase placeholder can be rewritten to a determiner followed by a noun symbol with probability 0.5. From such a PCFG, the probability of a sentence can be estimated as the product of the probabilities of all the rules used to derive the sentence. If a sentence has mult</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>J. Hale. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguistics, volume 2, pages 159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoeks</author>
<author>W Vonk</author>
<author>H Schriefers</author>
</authors>
<title>Processing coordinated structures in context: The effect of topic-structure on ambiguity resolution.</title>
<date>2002</date>
<journal>Journal of Memory and Language,</journal>
<pages>46--99</pages>
<contexts>
<context position="28253" citStr="Hoeks et al. (2002)" startWordPosition="4475" endWordPosition="4478">of language model to estimate surprisal is completely irrelevant; different models will yield different degrees of fit, see Frank and Bod (2010). 78 S-coordination. Surprisal theory allows us to build a formally precise computational model of reading time data which generates testable, quantitative predictions about the differential processing of individual test items. These predictions (Figure 5) indicate that mean reading times for a set of NP/S-coordination sentences may not be adequate to tap the origin of differential processing difficulty. Our results are consistent with the findings of Hoeks et al. (2002), who also found evidence for an NP-coordination preference in a self-paced reading experiment as well as in an eye-tracking experiment. They suggested that NP-coordination might be easier to process because it has a simpler topic structure than S-coordination. The former only has one topic, whereas the latter has two. Hoeks et al. (2002) argue that having more than one topic is unexpected. Sentences with more than one topic will therefore cause more processing difficulty. This preference for simple topic-structure that was evident in language comprehension may also be present in language prod</context>
</contexts>
<marker>Hoeks, Vonk, Schriefers, 2002</marker>
<rawString>J. Hoeks, W. Vonk, and H. Schriefers. 2002. Processing coordinated structures in context: The effect of topic-structure on ambiguity resolution. Journal of Memory and Language, 46:99–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hoeks</author>
<author>P Hendriks</author>
<author>W Vonk</author>
<author>C Brown</author>
<author>P Hagoort</author>
</authors>
<title>Processing the noun phrase versus sentence coordination ambiguity: Thematic information does not completely eliminate processing difficulty.</title>
<date>2006</date>
<journal>The Quarterly Journal of Experimental Psychology,</journal>
<pages>59--1581</pages>
<contexts>
<context position="3268" citStr="Hoeks et al. (2006)" startWordPosition="493" endWordPosition="496">suming that the human language processor adheres to the so-called minimal attachment principle. According to this principle, the sentence processor projects the simplest syntactic structure which is compatible with the material read at any point in time. NP-coordination is syntactically simpler than S-coordination in that it requires less phrasal nodes to be projected. Hence, the processor is biased towards NP- over S-coordination. Processing costs are incurred when this initial preference has to be revised in the disambiguating region, as in sentence (1-b), resulting in longer reading times. Hoeks et al. (2006) have shown that the NP-coordination preference can be reduced, but not entirely eliminated, when poor thematic fit between the verb and a potential object make an NP-coordination less likely (e.g., Jasper sanded the board and the carpenter laughed). We argue here that this residual preference for NPcoordination can be explained in terms of syntactic and lexical expectation within the framework of surprisal theory. In contrast to the minimal attachment principle, surprisal theory does not poshaar zusje her sister / / Marie Marie en and / / ook too / / b. Piet Piet kuste kissed Marie Marie en a</context>
<context position="29156" citStr="Hoeks et al. (2006)" startWordPosition="4621" endWordPosition="4624">e topic, whereas the latter has two. Hoeks et al. (2002) argue that having more than one topic is unexpected. Sentences with more than one topic will therefore cause more processing difficulty. This preference for simple topic-structure that was evident in language comprehension may also be present in language production, and hence in language corpora. Thus, it may very well be the case that the NP-coordination preference that was present in our training corpus may have had a pragmatic origin related to topic-structure. The outcome of our surprisal model is also compatible with the results of Hoeks et al. (2006) who found that thematic information can strongly reduce but not completely eliminate the NP-coordination preference. Surprisal theory is explicitly built on the assumption that multiple sources of information can interact in parallel at any point in time during sentence processing. Accordingly, we suggest here that the residual preference for NPcoordination found in the study of Hoeks et al. (2006) might be explained in terms of syntactic and lexical expectation. And finally, our approach is consistent with a large body of evidence indicating that language comprehension is incremental and mak</context>
</contexts>
<marker>Hoeks, Hendriks, Vonk, Brown, Hagoort, 2006</marker>
<rawString>J. Hoeks, P. Hendriks, W. Vonk, C. Brown, and P. Hagoort. 2006. Processing the noun phrase versus sentence coordination ambiguity: Thematic information does not completely eliminate processing difficulty. The Quarterly Journal of Experimental Psychology, 59:1581–1599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<pages>20--137</pages>
<contexts>
<context position="10758" citStr="Jurafsky (1996)" startWordPosition="1693" endWordPosition="1694">s a probability distribution over all analyses compatible with a partial sentence. Since partial sentences can sometimes be completed in an indefinite number of ways, it seems both practically and psychologically implausible to implement this distribution as an enumeration over complete structures. Instead, prefix probabilities should be estimated as a by-product of incremental processing, as in Stolcke’s (1995) parser (see section 3.2). This approach, however, still leaves open how many analyses are considered in parallel; does the human sentence processor employ full or limited parallelism? Jurafsky (1996) showed that full parallelism becomes more and more unmanageable when the amount of information used for disambiguation increases. Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001). Thus, even when assuming full parallelism, 3.1 Grammar Induction In our simulations, we used a PCFG to model the phrase structure of natural language. To induce such a grammar, an annotated corpus was required. We used Alpino (van Noord, 2006)— a robust and wide-coverage dependency parser</context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>D. Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20:137–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="1410" citStr="Levy, 2008" startWordPosition="201" endWordPosition="202"> syntactic and lexical probabilities, as specified in a PCFG, are sufficient to account for what is commonly referred to as an NP-coordination preference. 1 Introduction Language comprehension is incremental in that meaning is continuously assigned to utterances as they are encountered word-by-word (Altmann and Kamide, 1999). Not all words, however, are equally easy to process. A word’s processing difficulty is affected by, for instance, its frequency or its effect on the syntactic and semantic interpretation of a sentence. A recent theory of sentence processing, surprisal theory (Hale, 2001; Levy, 2008), combines several of these aspects into one single concept, namely the surprisal of a word. A word’s surprisal is proportional to its expectancy, i.e., the extent to which that word is expected (or predicted). The processing difficulty a word causes during comprehension is argued to be related linearly to its surprisal; the higher the surprisal value of a word, the more difficult it is to process. In this paper we investigate whether surprisal theory can account for the processing difficulty involved in sentences containing the noun phrase (NP) versus sentence (S) coordination ambiguity. The </context>
<context position="5451" citStr="Levy, 2008" startWordPosition="847" endWordPosition="848">on’s difficulty with processing a sentence can be modeled on a word level as proposed by Attneave (1959). Furthermore, it has recently been suggested that one of the characteristics of the comprehension system that makes it so fast, is its ability to anticipate what a speaker will say next. In other words, the language comprehension system works predictively (Otten et al., 2007; van Berkum et al., 2005). Surprisal theory is a model of differential processing difficulty which accommodates both these properties of the comprehension system, incremental processing and word prediction (Hale, 2001; Levy, 2008). In this theory, the processing difficulty of a sentence is a function of word processing difficulty. A word’s difficulty is inversely proportional to its expectancy, i.e., the extent to which the word was expected or predicted in the context in which it occurred. The lower a word’s expectancy, the more difficult it is to process. A word’s surprisal is linearly related to its difficulty. Consequently, words with lower conditional probabilities (expectancy) lead to higher surprisal than words with higher conditional probabilities. Surprisal theory is, to some extent, independent of the languag</context>
<context position="7558" citStr="Levy (2008)" startWordPosition="1166" endWordPosition="1167">and found no significant effects. This state of affairs is somewhat unfortunate for surprisal theory since input to the human language processor consists of sequences of words, not part-of-speech tags. In our study we therefore used lexicalized surprisal to investigate whether it can account for reading time data from the NP-/S-coordination ambiguity in Dutch. Lexicalized surprisal furthermore allows us to study how syntactic expectations might be modulated or even reversed by lexical expectations in temporarily ambiguous sentences. 2.1 Probabilistic Context Free Grammars Both Hale (2001) and Levy (2008) used a Probabilistic Context Free Grammar (PCFG) as a language model in their implementations of surprisal theory. A PCFG consists of a set of rewrite rules which are assigned some probability (Charniak, 1993): S NP, VP 1.0 NP Det, N 0.5 NP NP, VP 0.5 . . . � . . . . . . In this toy grammar, for instance, a noun phrase placeholder can be rewritten to a determiner followed by a noun symbol with probability 0.5. From such a PCFG, the probability of a sentence can be estimated as the product of the probabilities of all the rules used to derive the sentence. If a sentence has multiple derivations</context>
<context position="9403" citStr="Levy, 2008" startWordPosition="1490" endWordPosition="1491">e considered in parallel. difficulty(wi) a −loge IP(wi ... wi) l 3 Grammar and Parser P(w1 ... wi−1)J Surprisal theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language model, and expectation-based comprehension difficulty (Levy, 2008). In other words, comprehension difficulty does not critically depend on the structural representations postulated by the language model which is harnessed to generate word e</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>R. Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nederhof</author>
<author>A Sarkar</author>
<author>G Satta</author>
</authors>
<title>Prefix probabilities from stochastic tree adjoining grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL ’98,</booktitle>
<pages>953--959</pages>
<location>Montreal.</location>
<contexts>
<context position="9165" citStr="Nederhof et al. (1998)" startWordPosition="1449" endWordPosition="1452">the ratio of the prefix probabilities P(w1 ... wz) and P(w1 ... wz_1) equals precisely the conditional probability of word wz. Given a 73 PCFG, the difficulty of word wi can therefore be defined as: only a small number of ‘relevant’ analyses would be considered in parallel. difficulty(wi) a −loge IP(wi ... wi) l 3 Grammar and Parser P(w1 ... wi−1)J Surprisal theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language mo</context>
</contexts>
<marker>Nederhof, Sarkar, Satta, 1998</marker>
<rawString>M. Nederhof, A. Sarkar, and G. Satta. 1998. Prefix probabilities from stochastic tree adjoining grammar. In Proceedings of COLING-ACL ’98, pages 953–959, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Otten</author>
<author>M Nieuwland</author>
<author>J van Berkum</author>
</authors>
<title>Great expectations: Specific lexical anticipation influences the processing of spoken language.</title>
<date>2007</date>
<journal>BMC Neuroscience.</journal>
<marker>Otten, Nieuwland, van Berkum, 2007</marker>
<rawString>M. Otten, M. Nieuwland, and J. van Berkum. 2007. Great expectations: Specific lexical anticipation influences the processing of spoken language. BMC Neuroscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pickering</author>
<author>S Garrod</author>
</authors>
<title>Do people use language production to make predictions during comprehension? Trends in Cognitive Sciences,</title>
<date>2007</date>
<pages>11--105</pages>
<marker>Pickering, Garrod, 2007</marker>
<rawString>M. Pickering and S. Garrod. 2007. Do people use language production to make predictions during comprehension? Trends in Cognitive Sciences, 11:105– 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--249</pages>
<contexts>
<context position="11067" citStr="Roark, 2001" startWordPosition="1743" endWordPosition="1744">fix probabilities should be estimated as a by-product of incremental processing, as in Stolcke’s (1995) parser (see section 3.2). This approach, however, still leaves open how many analyses are considered in parallel; does the human sentence processor employ full or limited parallelism? Jurafsky (1996) showed that full parallelism becomes more and more unmanageable when the amount of information used for disambiguation increases. Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001). Thus, even when assuming full parallelism, 3.1 Grammar Induction In our simulations, we used a PCFG to model the phrase structure of natural language. To induce such a grammar, an annotated corpus was required. We used Alpino (van Noord, 2006)— a robust and wide-coverage dependency parser for Dutch—to automatically generate such a corpus, annotated with phrase structure, for 204.000 sentences, which were randomly extracted from Dutch newspapers. These analyses were then used to induce a PCFG consisting of 650 grammar rules, 89 non-terminals, and 208.133 terminals (lexical items).1 Moreover, </context>
<context position="15246" citStr="Roark (2001)" startWordPosition="2430" endWordPosition="2431">ch. This type of ambiguity was investigated by Frazier (1987) using a self-paced reading experiment. The sentences in (2) are part of Frazier’s materials. Sentence (2-a) and (2-b) exemplify an NP-/S-coordination ambiguity. The sentences are identical and temporarily ambiguous up to the NP haar zusje (her sister). In (2-a) this NP is followed by the adverb ook, and therefore disambiguated to be part of an NP-coordination; Marie and haar zusje are conjoined. In (2-b), on other hand, the same NP is followed by the verb lachte, and therefore disambiguated as the sub2A similar approach was used in Roark (2001) and Frank (2009). ject of a conjoined sentence; Piet kuste Marie and haar zusje lachte are conjoined. (2) a. Piet kuste Pete kissed (Ambiguous; NP-coordination) b. Piet Pete (Ambiguous; S-coordination) c. Annie Annie (Unambiguous; NP-control) d. Annie Annie (Unambiguous; S-control) Sentence (2-c) and (2-d) functioned as unambiguous controls. These sentences are identical up to the verb zag. In (2-c), the verb is followed by the single NP haar zusje, and subsequently the adverb ook. The adverb eliminates the possibility of an NP-coordination. In (2-d), on the other hand, the same verb is follo</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>B. Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27:249–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Smith</author>
<author>R Levy</author>
</authors>
<title>Optimal processing times in reading: A formal model and empirical investigation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 30th annual conference of the Cognitive Science Society,</booktitle>
<pages>595--600</pages>
<location>Austin, TX.</location>
<contexts>
<context position="6637" citStr="Smith and Levy (2008)" startWordPosition="1030" endWordPosition="1033"> extent, independent of the language model that generates conditional word probabilities. Different models can be used to estimate these probabilities. For all such models, however, a clear distinction can be made between lexicalized and unlexicalized surprisal. In lexicalized surprisal, the input to the language model is a sequence of words (i.e., a sentence). In unlexicalized surprisal, the input is a sequence of word categories (i.e., part-of-speech tags). While previous studies have used unlexicalized surprisal to predict reading times, evidence for lexicalized surprisal is rather sparse. Smith and Levy (2008) investigated the relation between lexicalized surprisal and reading time data for naturalistic texts. Using a trigram language model, they showed that there was a linear relationship between the two measures. Demberg and Keller (2008) examined whether this relation extended beyond transitional probabilities and found no significant effects. This state of affairs is somewhat unfortunate for surprisal theory since input to the human language processor consists of sequences of words, not part-of-speech tags. In our study we therefore used lexicalized surprisal to investigate whether it can accou</context>
<context position="9403" citStr="Smith and Levy, 2008" startWordPosition="1488" endWordPosition="1491">es would be considered in parallel. difficulty(wi) a −loge IP(wi ... wi) l 3 Grammar and Parser P(w1 ... wi−1)J Surprisal theory requires a probabilistic language model that generates some form of word expectancy. The theory itself, however, is largely neutral with respect to which model is employed. Models other than PCFGs can be used to estimate surprisal. Nederhof et al. (1998), for instance, show that prefix probabilities, and therefore surprisal, can be estimated from Tree Adjoining Grammars. This approach was taken in Demberg and Keller (2009). Other approaches have used trigram models (Smith and Levy, 2008), Simple Recurrent Networks of the Elman type (Frank, 2009), Markov models and Echo-state Networks (Frank and Bod, 2010). This illustrates that surprisal theory is not committed to specific claims about the structural representations that language takes in the human mind. It rather functions as a “causal bottleneck” between the representations of a language model, and expectation-based comprehension difficulty (Levy, 2008). In other words, comprehension difficulty does not critically depend on the structural representations postulated by the language model which is harnessed to generate word e</context>
</contexts>
<marker>Smith, Levy, 2008</marker>
<rawString>N. Smith and R. Levy. 2008. Optimal processing times in reading: A formal model and empirical investigation. In Proceedings of the 30th annual conference of the Cognitive Science Society, pages 595– 600, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<booktitle>Computational linguistics,</booktitle>
<pages>21--165</pages>
<marker>Stolcke, 1995</marker>
<rawString>A. Stolcke. 1995. An efficient probabilistic contextfree parsing algorithm that computes prefix probabilities. Computational linguistics, 21:165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J van Berkum</author>
<author>C Brown</author>
<author>P Zwitserlood</author>
<author>V Kooijman</author>
<author>P Hagoort</author>
</authors>
<title>Anticipating upcoming words in discourse: Evidence from ERPs and reading times.</title>
<date>2005</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<pages>31--443</pages>
<marker>van Berkum, Brown, Zwitserlood, Kooijman, Hagoort, 2005</marker>
<rawString>J. van Berkum, C. Brown, P. Zwitserlood, V. Kooijman, and P. Hagoort. 2005. Anticipating upcoming words in discourse: Evidence from ERPs and reading times. Journal of Experimental Psychology: Learning, Memory, and Cognition, 31:443–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>At last parsing is now operational. In Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles, pages 20–42. Presses universitaires de Louvain.</title>
<date>2006</date>
<marker>van Noord, 2006</marker>
<rawString>G. van Noord. 2006. At last parsing is now operational. In Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles, pages 20–42. Presses universitaires de Louvain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>