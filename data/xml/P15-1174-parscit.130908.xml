<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001856">
<title confidence="0.986195">
Improving Evaluation of Machine Translation Quality Estimation
</title>
<author confidence="0.970574">
Yvette Graham
</author>
<affiliation confidence="0.97866">
ADAPT Centre
School of Computer Science and Statistics
Trinity College Dublin
</affiliation>
<email confidence="0.992773">
yvette.graham@scss.tcd.ie
</email>
<sectionHeader confidence="0.993726" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998334125">
Quality estimation evaluation commonly
takes the form of measurement of the error
that exists between predictions and gold
standard labels for a particular test set of
translations. Issues can arise during com-
parison of quality estimation prediction
score distributions and gold label distribu-
tions, however. In this paper, we provide
an analysis of methods of comparison and
identify areas of concern with respect to
widely used measures, such as the ability
to gain by prediction of aggregate statistics
specific to gold label distributions or by
optimally conservative variance in predic-
tion score distributions. As an alternative,
we propose the use of the unit-free Pear-
son correlation, in addition to providing an
appropriate method of significance testing
improvements over a baseline. Compo-
nents of WMT-13 and WMT-14 quality es-
timation shared tasks are replicated to re-
veal substantially increased conclusivity in
system rankings, including identification
of outright winners of tasks.
</bodyText>
<sectionHeader confidence="0.999128" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901604166667">
Machine Translation (MT) Quality Estimation
(QE) is the automatic prediction of machine trans-
lation quality without the use of reference trans-
lations (Blatz et al., 2004; Specia et al., 2009).
Human assessment of translation quality in theory
provides the most meaningful evaluation of sys-
tems, but human assessors are known to be incon-
sistent and this causes challenges for quality es-
timation evaluation. For instance, there is a gen-
eral lack of consensus both with respect to what
provides the most meaningful gold standard rep-
resentation, as well as best method of compari-
son of gold labels and system predictions. For ex-
ample, in the 2014 Workshop on Statistical Ma-
chine Translation (WMT), which since 2012 has
provided a main venue for evaluation of systems,
sentence-level systems were evaluated with re-
spect to three distinct gold standard representa-
tions and each of those compared to predictions
using four different measures, resulting in a total
of 12 different system rankings, 6 identified as of-
ficial rankings (Bojar et al., 2014).
Although the aim of several methods of evalua-
tion is to provide more insight into performance of
systems, this also produces conflicting results and
raises the question which method of evaluation re-
ally identifies the system(s) or method(s) that best
predicts translation quality. For example, an ex-
treme case in WMT-14 occurred for sentence-level
quality estimation for English-to-Spanish. In each
of the 12 system rankings, many systems were tied
and this resulted in a total of 22 official winning
systems for this language pair. Besides leaving po-
tential users of quality estimation systems at a loss
as to what the best system may be, a large number
of inconclusive evaluation methodologies is also
likely to lead to confusion about which evaluation
methods should be applied in general in QE re-
search, or worse still, researchers simply choos-
ing the methodology that favors their system from
among the many different methodologies.
In this paper, we provide an analysis of each of
the methodologies used in WMT and widely ap-
plied to evaluation of quality estimation systems
in general. Our analysis reveals potential flaws in
existing methods and we subsequently provide de-
tail of a single method that overcomes previous
challenges. To demonstrate, we replicate com-
</bodyText>
<page confidence="0.968231">
1804
</page>
<note confidence="0.977476333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1804–1813,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999540166666667">
ponents of evaluations previously carried out at
WMT-13 and WMT-14 sentence-level quality es-
timation shared tasks. Results reveal substantially
more conclusive system rankings, revealing out-
right winners that had not previously been identi-
fied.
</bodyText>
<sectionHeader confidence="0.997459" genericHeader="introduction">
2 Relevant Work
</sectionHeader>
<bodyText confidence="0.999272888888889">
The Workshop on Statistical Machine Transla-
tion (WMT) provides a main venue for evalua-
tion of quality estimation systems, in addition to
the rare and highly-valued effort of provision of
publicly available data sets to facilitate further re-
search. We provide an analysis of current evalu-
ation methodologies applied not only in the most
recent WMT shared task but also widely within
quality estimation research.
</bodyText>
<subsectionHeader confidence="0.811086">
2.1 WMT-style Evaluation
</subsectionHeader>
<bodyText confidence="0.9971228125">
WMT-14 quality estimation evaluation at the
sentence-level, Task 1, is comprised of three sub-
tasks. In Task 1.1, human gold labels comprise
three levels of translation quality or “perceived
post-edit effort” (1 = perfect translation; 2 = near
miss translation; 3 = very low quality translation).
A possible downside of the evaluation methodol-
ogy applied in Task 1.1 is firstly that the gold stan-
dard representation may be overly coarse-grained.
Considering the vast range of possible errors oc-
curring in translations, limiting the levels of trans-
lation quality to only three may impact negatively
on systems’ ability to discriminate between trans-
lations of various quality.
More importantly, however, the combination of
such coarse-grained gold labels (1, 2 or 3) and
comparison of gold labels and system predictions
by mean absolute error (MAE) has a counter-
intuitive effect on system rankings, as systems that
produce continuous predictions are at an advan-
tage over those that produce discrete predictions
even though gold labels are also discrete. Figure
1(a) shows discrete gold label distributions for the
scoring variant of Task 1.1 in WMT-14 and Figure
1(b) prediction distributions for an example sys-
tem that was at a disadvantage because it restricted
its predictions to discrete ratings like those of gold
labels, and Figure 1(c) a system that achieves ap-
parent better performance (lower MAE) despite
prediction representations mismatching the dis-
crete nature of gold labels.
Evaluation of the ranking variant of Task 1.1
</bodyText>
<table confidence="0.624206333333333">
r
Post-edit Time 0.36
Post-edit Rate 0.69***
</table>
<tableCaption confidence="0.983108">
Table 1: Pearson correlation with HTER scores of
</tableCaption>
<bodyText confidence="0.993485613636364">
post-edit times (PETs) and post-edit rates (PERs)
for WMT-14 Task 1.2 and Task 1.3 gold labels,
correlation marked with *** is significantly greater
at p &lt; 0.001.
again includes a significant mismatch between
representations used as gold labels, which again
were limited to the ratings 1, 2 or 3, while sys-
tems were required to provide a total-order rank-
ing of test set translations, for example ranks 1-
600 or 1-450, depending on language pair. Evalu-
ation methodologies applied to ranking tasks may
be better facilitated by application of more fine-
grained gold standard labels that more closely rep-
resent total-order rankings of system predictions.
Evaluation methodologies applied in Task 1.3
employ the more fine-grained post-edit times
(PETs) as translation quality gold labels. PETs
potentially provide a good indication of the un-
derlying quality of translations, as a translation
that takes longer to manually correct is thought
to have lower quality. However, we propose what
may correspond more directly to translation qual-
ity is an alteration of this, a post-edit rate (PER),
where PETs are normalized by the number of
words in translations. This takes into account the
fact that, all else being equal, longer translations
simply take a greater amount of time to post-edit
than shorter ones. To investigate to what degree
PERs may correspond better to translation quality
than PETs, we compute correlations of each with
HTER gold labels of translations from Task 1.2.
Table 6 reveals a significantly higher correlation
that exists between PER and HTER compared to
PET and HTER (p &lt; 0.001) , and we conclude
therefore that the PER of a translation provides a
more faithful representation of translation quality
than PET, and convert PETs for both predictions
and gold labels to PERs (in seconds per word) in
our later replication of Task 1.3.
In Task 1.2 of WMT-14, gold standard labels
used to evaluate systems were in the form of hu-
man translation error rates (HTERs) (Snover et
al., 2009). HTER scores provide an effective
representation for evaluation of quality estima-
</bodyText>
<page confidence="0.976493">
1805
</page>
<figure confidence="0.999869363636364">
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
1 2 3
(a) Gold Labels
MAE: 0.687
1 2 3
(b) System A Predictions
0.5 1.0 1.5 2.0 2.5 3.0 3.5
(c) System B Predictions
MAE: 0.603
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<figureCaption confidence="0.907642333333333">
Figure 1: WMT-14 English-to-German quality estimation Task 1.1 where mismatched prediction/gold
labels achieves apparent better performance, where (a) gold label distribution; (b) example system disad-
vantaged by its discrete predictions; (c) example system gaining advantage by its continuous predictions.
</figureCaption>
<bodyText confidence="0.999556428571428">
tion systems, as scores are individually computed
per translation using custom post-edited reference
translations, avoiding the bias that can occur with
metrics that employ generic reference translations.
In our later evaluation, we therefore use HTER
scores in addition to PERs as suitable gold stan-
dard labels.
</bodyText>
<subsectionHeader confidence="0.999256">
2.2 Mean Absolute Error
</subsectionHeader>
<bodyText confidence="0.999981125">
Mean absolute error is likely the most widely ap-
plied comparison measure of quality estimation
system predictions and gold labels, in addition to
being the official measure applied to scoring vari-
ants of tasks in WMT (Bojar et al., 2014). MAE is
the average absolute difference that exists between
a system’s predictions and gold standard labels for
translations, and a system achieving a lower MAE
is considered a better system. Significant issues
arise for evaluation of quality estimation systems
with MAE when comparing distributions for pre-
dictions and gold labels, however. Firstly, a sys-
tem’s MAE can be lowered not only by individ-
ual predictions closer to corresponding gold la-
bels, but also by prediction of aggregate statistics
specific to the distribution of gold labels in the par-
ticular test set used for evaluation. MAE is most
susceptible in this respect when gold labels have
a unimodal distribution with relatively low stan-
dard deviation. For example, Figure 2(a) shows
test set gold label HTER distribution for Task 1.2
in WMT-14 where the bulk of HTERs are located
around one main peak with relatively low variance
in the distribution. Unfortunately with MAE, a
system that correctly predicts the location of the
mode of the test set gold distribution and centers
predictions around it with an optimally conserva-
tive variance can achieve lower MAE and appar-
ent better performance. Figure 2(b) shows a lower
MAE can be achieved by rescaling the original
prediction distribution for an example system to
a distribution with lower variance.
A disadvantage of an ability to gain in perfor-
mance by prediction of such features of a given
test set is that prediction of aggregates is, in gen-
eral, far easier than individual predictions. In ad-
dition, inclusion of confounding test set aggre-
gates such as these in evaluations will likely lead
to both an overestimate of the ability of some sys-
tems to predict the quality of unseen translations
and an underestimate of the accuracy of systems
that courageously attempt to predict the quality of
translations in the tails of gold distributions, and
it follows that systems optimized for MAE can
be expected to perform badly when predicting the
quality of translations in the tails of gold label dis-
tributions (Moreau and Vogel, 2014).
Table 2 shows how MAEs of original predicted
score distributions for all systems participating in
Task 1.2 WMT-14 can be reduced by shifting and
rescaling the prediction score distribution accord-
ing to gold label aggregates. Table 3 shows that
for similar reasons other measures commonly ap-
plied to evaluation of quality estimation systems,
such as root mean squared error (RMSE), that are
also not unit-free, encounter the same problem.
</bodyText>
<subsectionHeader confidence="0.999586">
2.3 Significance Testing
</subsectionHeader>
<bodyText confidence="0.9999418">
In quality estimation, it is common to apply boot-
strap resampling to assess the likelihood that a de-
crease in MAE (an improvement) has occurred by
chance. In contrast to other areas of MT, where
the accuracy of randomized methods of signifi-
</bodyText>
<page confidence="0.956861">
1806
</page>
<figure confidence="0.992668826086956">
Original Rescaled
RMSE RMSE
FBK-UPV-UEDIN-wp 0.167 0.166
DCU-rtm-svr 0.167 0.165
DCU-rtm-tree 0.175 0.169
DFKI-svr 0.177 0.171
USHEFF 0.178 0.178
FBK-UPV-UEDIN-nowp 0.181 0.180
SHEFF-lite-sparse 0.184 0.179
baseline 0.195 0.194
DFKI-svr-xdata 0.195 0.187
Multilizer 0.209 0.181
SHEFF-lite 0.234 0.216
(a) Original Predictions
0 1 2 3 4 5 6
MAE: 0.1504
Multilizer
Gold
0.0 0.2 0.4 0.6 0.8 1.0
HTER
(b) Rescaled Predictions
0.0 0.2 0.4 0.6 0.8 1.0
HTER
</figure>
<figureCaption confidence="0.999484">
Figure 2: Comparison of example system from
</figureCaption>
<bodyText confidence="0.934988414634146">
WMT-14 English-to-Spanish Task 1.2 (a) original
prediction distribution and gold labels and (b) the
same when the prediction distribution is rescaled
to half its original standard deviation, showing a
lower MAE can be achieved by reducing the vari-
ance in prediction distributions.
Original Rescaled
MAE MAE
FBK-UPV-UEDIN-wp 0.129 0.125
DCU-rtm-svr 0.134 0.127
USHEFF 0.136 0.133
DCU-rtm-tree 0.140 0.129
DFKI-svr 0.143 0.132
FBK-UPV-UEDIN-nowp 0.144 0.137
SHEFF-lite-sparse 0.150 0.141
Multilizer 0.150 0.135
baseline 0.152 0.149
DFKI-svr-xdata 0.161 0.146
SHEFF-lite 0.182 0.168
Table 2: MAE of WMT-14 Task 1.2 systems for
original HTER prediction distributions and when
distributions are shifted and rescaled to the mean
and half the standard deviation of the gold label
distribution.
Table 3: RMSE of WMT-14 Task 1.2 systems for
original HTER prediction distributions and when
distributions are shifted and rescaled to the mean
and half the standard deviation of the gold label
distribution.
cance testing such as bootstrap resampling in com-
bination with BLEU and other metrics have been
empirically evaluated (Koehn, 2004; Graham et
al., 2014), to the best of our knowledge no re-
search has been carried out to assess the accuracy
of similar methods specifically for quality estima-
tion evaluation. In addition, since data used for
evaluation of quality estimation systems are not
independent, methods of significance testing dif-
ferences in performance will be inaccurate unless
the dependent nature of the data is taken into ac-
count.
</bodyText>
<sectionHeader confidence="0.952176" genericHeader="method">
3 Quality Estimation Evaluation by
</sectionHeader>
<subsectionHeader confidence="0.86886">
Pearson Correlation
</subsectionHeader>
<bodyText confidence="0.999991842105263">
The Pearson correlation is a measure of the linear
correlation between two variables, and in the case
of quality estimation evaluation this amounts to
the linear correlation between system predictions
and gold labels. Pearson’s r overcomes the out-
lined challenges of previous approaches, such as
mean absolute error, for several reasons. Firstly,
Pearson’s r is a unit-free measure with a key prop-
erty being that the correlation coefficient is invari-
ant to separate changes in location and scale in
either of the two variables. This has the obvious
advantage over MAE that the coefficient cannot
be altered by shifting or rescaling prediction score
distributions according to aggregates specific to
the test set.
To illustrate, Figure 3 depicts a pair of systems
for which the baseline system appears to outper-
form the other when evaluated with MAE, but this
is only due to the conservative variance in its pre-
</bodyText>
<figure confidence="0.998353833333333">
0 1 2 3 4 5 6
MAE: 0.1416
Rescaled Multilizer
Gold
1807
(b) baseline (c) Stand. baseline
HTER Prediction PER (z)
(e) CMU−ISL−full (f) Stand. CMU−ISL−full
Density
0 1 2 3 4 5 6 7
0.0 0.2 0.4 0.6 0.8 1.0
Gold PER (z)
−1 0 1 2 3 4
−2 0 2 4
r: 0.451
MAE: 0.148
Gold
Prediction
(a) Raw baseline
0.0 0.2 0.4 0.6 0.8 1.0
HTER Prediction (raw)
(d) Raw CMU−ISL−full
HTER Gold (raw)
0.0 0.2 0.4 0.6 0.8 1.0
MAE: 0.148
r: 0.494
−2 0 2 4
Prediction PER (z)
−1 0 1 2 3 4
Gold PER (z)
MAE: 0.152
Gold
Prediction
Density
0 1 2 3 4 5 6 7
0.0 0.2 0.4 0.6 0.8 1.0
HTER Prediction (raw)
0.0 0.2 0.4 0.6 0.8 1.0
HTER
MAE: 0.152
HTER Gold (raw)
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<figureCaption confidence="0.9893955">
Figure 3: WMT-13 Task 1.1 systems showing baseline with better MAE than CMU-ISL-FULL only due
to conservative variance in prediction distribution and despite its weaker correlation with gold labels.
</figureCaption>
<bodyText confidence="0.99938425">
diction score distribution, as can be seen by the
narrow blue spike in Figure 3(b). Figure 3(e)
shows how the prediction distribution of CMU-
ISL-FULL, on the other hand, has higher variance,
and subsequently higher MAE. Figures 3(c) and
3(f) depict what occurs in computation of the Pear-
son correlation where raw prediction and gold la-
bel scores are replaced by standardized scores, i.e.
numbers of standard deviations from the mean of
each distribution, where CMU-ISL-FULL in fact
achieves a significantly higher correlation than the
baseline system at p &lt; 0.001.
An additional advantage of the Pearson corre-
lation is that coefficients do not change depend-
ing on the representation used in the gold standard
in the way they do with MAE, making possible a
comparison of performance across evaluations that
employ different gold label representations. Addi-
tionally, there is no longer a need for training and
test representations to directly correspond to one
another. To demonstrate, in our later evaluation we
include the evaluation of systems trained on both
HTER and PETs for prediction of both HTER
and PERs.
Finally, when evaluated with the Pearson cor-
relation significance tests can be applied without
resorting to randomized methods, in addition to
taking into account the dependent nature of data
used in evaluations.
The fact that the Pearson correlation is invariant
to separate shifts in location and scale of either of
the two variables is nonproblematic for evaluation
of quality estimation systems. Take, for instance,
the possible counter-argument: a pair of systems,
one of which predicts the precise gold distribution,
and another system predicting the gold distribution
+ 1, would unfairly receive the same Pearson cor-
relation coefficient. Firstly, it is just as difficult to
predict the gold distribution + 1, as it is to pre-
dict the gold distribution itself. More importantly,
however, the scenario is extremely unlikely to oc-
cur in practice, it is highly unlikely that a system
would ever accurately predict the gold distribution
+ 1, as opposed to the actual gold distribution un-
less training labels were adjusted in the same man-
ner, or indeed predict the gold distribution shifted
or rescaled by any other constant value. It is im-
portant to understand that invariance of the Pear-
</bodyText>
<page confidence="0.971484">
1808
</page>
<bodyText confidence="0.999590307692308">
son correlation to a shift in location or scale means
that the measure is only invariant to a shift in loca-
tion or scale applied to the entire distribution (of
either of the two variables), such as the shift in
location and scale that can be used to boost appar-
ent performance of systems when measures like
MAE and RMSE, that are not unit-free, are em-
ployed. Increasing the distance between system
predictions and gold labels for anything less than
the entire distribution, a more realistic scenario,
or by something other than a constant across the
entire distribution, will result in an appropriately
weaker Pearson correlation.
</bodyText>
<sectionHeader confidence="0.9837355" genericHeader="method">
4 Quality Estimation Significance
Testing
</sectionHeader>
<bodyText confidence="0.999850178571429">
Previous work has shown the suitability of
Williams significance test (Williams, 1959) for
evaluation of automatic MT metrics (Graham and
Baldwin, 2014; Graham et al., 2015), and, for sim-
ilar reasons, Williams test is appropriate for signif-
icance testing differences in performance of com-
peting quality estimation systems which we detail
further below.
Evaluation of a given quality estimation system,
Pnew, by Pearson correlation takes the form of
quantifying the correlation, r(Pnew, G), that ex-
ists between system prediction scores and corre-
sponding gold standard labels, and contrasting this
correlation with the correlation for some baseline
system, r(Pbase, G).
At first it might seem reasonable to perform sig-
nificance testing in the following manner when
an increase in correlation with gold labels is ob-
served: apply a significance test separately to the
correlation of each quality estimation system with
gold labels, with the hope that the new system will
achieve a significant correlation where the base-
line system does not. The reasoning here is flawed
however: the fact that one correlation is signifi-
cantly higher than zero (r(Pnew, G)) and that of
another is not, does not necessarily mean that the
difference between the two correlations is signif-
icant. Instead, a specific test should be applied
to the difference in correlations on the data. For
this same reason, confidence intervals for individ-
ual correlations with gold labels are also not use-
ful.
In psychology, it is often the case that sam-
ples that data are drawn from are independent,
and differences in correlations are computed on
independent data sets. In such cases, the Fisher
r to z transformation is applied to test for sig-
nificant differences in correlations. Data used
for evaluation of quality estimation systems are
not independent, however, and this means that if
r(Pbase, G) and r(Pnew, G) are both &gt; 0, the cor-
relation between both sets of predictions them-
selves, r(Pbase, Pnew), must also be &gt; 0. The
strength of this correlation, directly between pre-
dictions of pairs of quality estimation systems,
should be taken into account using a signifi-
cance test of the difference in correlation between
r(Pbase, G) and r(Pnew, G).
Williams test 1 (Williams, 1959) evaluates the
significance of a difference in dependent correla-
tions (Steiger, 1980). It is formulated as follows
as a test of whether the population correlation be-
tween X1 and X3 equals the population correla-
tion between X2 and X3:
where rij is the correlation between Xi and Xj, n
is the size of the population, and:
</bodyText>
<equation confidence="0.998919">
K = 1 − r122 − r132 − r232 + 2r12r13r23
</equation>
<bodyText confidence="0.9999664">
As part of this research, we have made avail-
able an open-source implementation of statisti-
cal tests tailored to the assessment of quality es-
timation systems, at https://github.com/
ygraham/mt-qe-eval.
</bodyText>
<sectionHeader confidence="0.977895" genericHeader="evaluation">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.9972994">
To demonstrate the use of the Pearson correlation
as an effective mechanism for evaluation of quality
estimation systems, we rerun components of pre-
vious evaluations originally carried out at WMT-
13 and WMT-14.
Table 4 shows Pearson correlations for systems
participating in WMT-13 Task 1.1 where gold la-
bels were in the form of HTER scores. System
rankings diverge considerably from original rank-
ings, notably the top system according to the Pear-
son correlation is tied in fifth place when evaluated
with MAE.
Table 5 shows Pearson correlations of systems
that took part in Task 1.2 of WMT-14, where gold
labels were again in the form of HTER scores,
</bodyText>
<footnote confidence="0.509427">
1Also known as Hotelling-Williams.
</footnote>
<equation confidence="0.9971008">
t( 3) (r13 − r23) Y (n − 1)(1 + r12)
I — _
1/ 2K (n−1) + (r23+r13)2 1 − r 3
VV (n−3) 4 ( 12)
,
</equation>
<page confidence="0.986014">
1809
</page>
<table confidence="0.999589944444444">
System r MAE
DCU-SYMC-rc 0.595 0.135
SHEFMIN-FS 0.575 0.124
DCU-SYMC-ra 0.572 0.135
CNGL-SVRPLS 0.560 0.133
CMU-ISL-noB 0.516 0.138
CNGL-SVR 0.508 0.138
CMU-ISL-full 0.494 0.152
fbk-uedin-extra 0.483 0.144
LIMSI-ELASTIC 0.475 0.133
SHEFMIN-FS-AL 0.474 0.130
LORIA-INCTRA-CONT 0.474 0.148
fbk-uedin-rsvr 0.464 0.145
LORIA-INCTRA 0.461 0.148
baseline 0.451 0.148
TCD-CNGL-OPEN 0.329 0.148
TCD-CNGL-RESTR 0.291 0.152
UMAC-EBLEU 0.113 0.170
</table>
<tableCaption confidence="0.755949666666667">
Table 4: Pearson correlation and MAE of system
HTER predictions and gold labels for English-to-
Spanish WMT-13 Task 1.1.
</tableCaption>
<bodyText confidence="0.9999295">
and to demonstrate the ability of evaluation of sys-
tems trained on a representation distinct from that
of gold labels made possible by the unit-free Pear-
son correlation, we also include evaluation of sys-
tems originally trained on PET labels to predict
HTER scores. Since PET systems also produce
predictions in the form of PET, we convert pre-
dictions for all systems to PERs prior to compu-
tation of correlations, as PERs more closely cor-
respond to translation quality. Results reveal that
systems originally trained on PETs in general per-
form worse than HTER trained systems, and this
is not all that surprising considering the training
representation did not correspond well to transla-
tion quality. Again system rankings diverge from
MAE rankings with the second best system ac-
cording to MAE moved to the initial position.
Table 6 shows Pearson correlations for predic-
tions of PER for systems trained on either PETs
or HTER, and predictions for systems trained on
PETs are converted to PER for evaluation. Sys-
tem rankings diverge most for this data set from
the original rankings by MAE, as the system hold-
ing initial position according to MAE moves to po-
sition 13 according to the Pearson correlation.
Many of the differences in correlation between
systems in Tables 4, 5 and 6 are small and instead
of assuming that an increase in correlation of one
system over another corresponds to an improve-
ment in performance, we first apply significance
testing to differences in correlation with gold la-
bels that exist between correlations for each pair
</bodyText>
<table confidence="0.999950782608696">
Training QE r MAE
Labels System
HTER DCU-rtm-svr 0.550 0.134
HTER FBK-UPV-UEDIN-wp 0.540 0.129
HTER DCU-rtm-tree 0.518 0.140
HTER DFKI-svr 0.501 0.143
HTER USHEFF 0.432 0.136
HTER SHEFF-lite-sparse 0.428 0.150
HTER FBK-UPV-UEDIN-nowp 0.414 0.144
HTER Multilizer 0.409 0.150
PET DCU-rtm-rr 0.350 −
HTER DFKI-svr-xdata 0.349 0.161
PET FBK-UPV-UEDIN-wp 0.346 −
PET Multilizer-2 0.331 −
PET Multilizer-1 0.328 −
PET DCU-rtm-svr 0.315 −
HTER baseline 0.283 0.152
PET FBK-UPV-UEDIN-nowp 0.279 −
PET USHEFF 0.246 −
PET baseline 0.246 −
PET SHEFF-lite-sparse 0.229 −
PET SHEFF-lite 0.194 −
HTER SHEFF-lite 0.052 0.182
</table>
<tableCaption confidence="0.92811625">
Table 5: Pearson correlation and MAE of system
HTER predictions and gold labels for English-to-
Spanish WMT-14 Task 1.2 and 1.3 systems trained
on either HTER or PET labelled data.
</tableCaption>
<table confidence="0.999978217391305">
Training QE r MAE
Labels System
HTER FBK-UPV-UEDIN-wp 0.529 −
PET FBK-UPV-UEDIN-wp 0.472 0.972
HTER FBK-UPV-UEDIN-nowp 0.452 −
HTER USHEFF 0.444 −
HTER DCU-rtm-svr 0.444 −
HTER DCU-rtm-tree 0.442 −
HTER SHEFF-lite-sparse 0.441 −
PET DCU-rtm-rr 0.430 0.932
PET FBK-UPV-UEDIN-nowp 0.423 1.012
HTER DFKI-svr 0.412 −
PET USHEFF 0.394 1.358
PET baseline 0.394 1.359
PET DCU-rtm-svr 0.365 0.915
HTER Multilizer 0.361 −
PET SHEFF-lite-sparse 0.337 0.951
PET SHEFF-lite 0.323 0.940
PET Multilizer-1 0.288 0.993
HTER baseline 0.286 −
HTER DFKI-svr-xdata 0.277 −
PET Multilizer-2 0.271 0.972
HTER SHEFF-lite 0.011 −
</table>
<tableCaption confidence="0.9531036">
Table 6: Pearson correlation of system PER pre-
dictions and gold labels for English-to-Spanish
WMT-14 Task 1.2 and 1.3 systems trained on ei-
ther HTER or PET labelled data, mean absolute
error (MAE) provided are in seconds per word.
</tableCaption>
<page confidence="0.895876">
1810
</page>
<figure confidence="0.999877760869566">
DCU−rtm−svr
FBK−UPV−UEDIN−wp
DCU−rtm−tree
DFKI−svr
USHEFF
SHEFF−lite−sparse
FBK−UPV−UEDIN−nowp
Multilizer
DFKI−svr−xdata
baseline
SHEFF−lite
DCU−SYMC−rc
SHEFMIN−FS
DCU−SYMC−ra
CNGL−SVRPLS
CMU−ISL−noB
CNGL−SVR
CMU−ISL−full
fbk−uedin−extra
LIMSI−ELASTIC
SHEFMIN−FS−AL
LORIA_INCTR−CONT
fbk−uedin−rand−svr
LORIA−INCTRA
baseline
TCD−CNGL−OPEN
TCD−CNGL−RESTR
UMAC_EBLEU
r
DCU.rtm.svr DCU.SYMC.rc
FBK.UPV.UEDIN.wp SHEFMIN.FS
DCU.rtm.tree DCU.SYMC.ra
DFKI.svr CNGL.SVRPLS
USHEFF CMU.ISL.noB
SHEFF.lite.sparse CNGL.SVR
FBK.UPV.UEDIN.nowp CMU.ISL.full
Multilizer fbk.uedin.extra
DFKI.svr.xdata LIMSI.ELASTIC
baseline SHEFMIN.FS.AL
SHEFF.lite LORIA.INCTR.CONT
fbk.uedin.r.svr
LORIA_INCTR
baseline
TCD.CNGL.OPEN
TCD.CNGL.RESTR
UMAC_EBLEU
</figure>
<figureCaption confidence="0.977965">
Figure 4: Pearson correlation between prediction
scores for all pairs of systems participating in
WMT-14 Task 1.2
</figureCaption>
<bodyText confidence="0.935396">
of systems.
</bodyText>
<subsectionHeader confidence="0.997425">
5.1 Significance Tests
</subsectionHeader>
<bodyText confidence="0.997030754716981">
When an increase in correlation with gold labels
is present for a pair of systems, significance tests
provide insight into the likelihood that such an in-
crease has occurred by chance. As described in
detail in Section 4, the Williams test (Williams,
1959), a test also appropriate for MT metrics eval-
uated by the Pearson correlation (Graham and
Baldwin, 2014), is appropriate for testing the sig-
nificance of a difference in dependent correlations
and therefore provides a suitable method of signif-
icance testing for quality estimation systems. Fig-
ure 4 provides an example of the strength of cor-
relations that commonly exist between predictions
of quality estimation systems.
Figure 5 shows significance test outcomes of
the Williams test for systems originally taking part
in WMT-13 Task 1.1, with systems ordered by
strongest to least Pearson correlation with gold la-
bels, where a green cell in (row i, column j) signi-
fies a significant win for row i system over column
j system, where darker shades of green signify
conclusions made with more certainty. Test out-
comes allow identification of significant increases
Figure 5: HTER prediction significance test out-
comes for all pairs of systems from English-to-
Spanish WMT-13 Task 1.1, colored cells denote a
significant increase in correlation with gold labels
for row i system over column j system.
in correlation with gold labels of one system over
another, and subsequently the systems shown to
outperform others. Test outcomes in Figure 5 re-
veal substantially increased conclusivity in sys-
tem rankings made possible with the application
of the Pearson correlation and Williams test, with
almost an unambiguous total-order ranking of sys-
tems and an outright winner of the task.
Figure 6 shows outcomes of Williams signifi-
cance tests for prediction of HTER and Figure 7
shows outcomes of tests for PER prediction for
WMT-14 English-to-Spanish, again showing sub-
stantially increased conclusivity in system rank-
ings for tasks.
It is important to note that the number of com-
peting systems a system significantly outperforms
should not be used as the criterion for ranking
competing quality estimation systems, since the
power of the Williams test changes depending on
the degree to which predictions of a pair of sys-
tems correlate with each other. A system with pre-
dictions that happen to correlate strongly with pre-
dictions of many other systems would be at an un-
fair advantage, were numbers of significant wins
to be used to rank systems. For this reason, it is
</bodyText>
<page confidence="0.993024">
1811
</page>
<figureCaption confidence="0.947780666666667">
Figure 6: HTER prediction significance test out-
comes for all pairs of systems from English-to-
Spanish WMT-14 Task 1.2, colored cells denote a
significant increase in correlation with gold labels
for row i system over column j system.
Figure 7: PER prediction significance test out-
</figureCaption>
<bodyText confidence="0.94734">
comes for all pairs of systems from English-to-
Spanish WMT-14 Task 1.3, colored cells denote a
significant increase in correlation with gold labels
for row i system over column j system.
best to interpret pairwise system tests in isolation.
</bodyText>
<sectionHeader confidence="0.997178" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999821">
We have provided a critique of current widely used
methods of evaluation of quality estimation sys-
tems and highlighted potential flaws in existing
methods, with respect to the ability to boost scores
by prediction of aggregate statistics specific to the
particular test set in use or conservative variance in
prediction distributions. We provide an alternate
mechanism, and since the Pearson correlation is a
unit-free measure, it can be applied to evaluation
of quality estimation systems avoiding the previ-
ous vulnerabilities of measures such as MAE and
RMSE. Advantages also outlined are that training
and test representations no longer need to directly
correspond in evaluations as long as labels com-
prise a representation that closely reflects transla-
tion quality. We demonstrated the suitability of the
proposed measures through replication of compo-
nents of WMT-13 and WMT-14 quality estima-
tion shared tasks, revealing substantially increased
conclusivity of system rankings.
</bodyText>
<sectionHeader confidence="0.99397" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999481833333333">
We wish to thank the anonymous reviewers for their valu-
able comments and WMT organizers for the provision of data
sets. This research is supported by Science Foundation Ire-
land through the CNGL Programme (Grant 12/CE/I2267) in
the ADAPT Centre (www.adaptcentre.ie) at Trinity College
Dublin.
</bodyText>
<sectionHeader confidence="0.998592" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995728476190476">
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence estimation for machine transla-
tion. In Proceedings of the 20th international con-
ference on Computational Linguistics, pages 315–
321. Association for Computational Linguistics.
O. Bojar, C. Buck, C. Federmann, B. Haddow,
P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post,
H. Saint-Amand, R. Soricut, L. Specia, and A. Tam-
chyna. 2014. Findings of the 2014 Workshop
on Statistical Machine Translation. In Proc. 9th
Wkshp. Statistical Machine Translation, Baltimore,
MA. Association for Computational Linguistics.
Y. Graham and T. Baldwin. 2014. Testing for sig-
nificance of increased correlation with human judg-
ment. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
172–176, Doha, Qatar. Association for Computa-
tional Linguistics.
Y. Graham, N. Mathur, and T. Baldwin. 2014. Ran-
domized significance tests in machine translation.
</reference>
<figure confidence="0.956505936507937">
HTER−DCU−rtm−svr
HTER−FBK−UPV−UEDIN−wp
HTER−DCU−rtm−tree
HTER−DFKI−svr
HTER−USHEFF
HTER−SHEFF−lite−sparse
HTER−FBK−UPV−UEDIN−nowp
HTER−Multilizer
PET−DCU−rtm−rr
HTER−DFKI−svr−xdata
PET−FBK−UPV−UEDIN−wp
PET−Multilizer−2
PET−Multilizer−1
PET−DCU−rtm−svr
HTER−baseline
PET−FBK−UPV−UEDIN−nowp
PET−USHEFF
PET−baseline
PET−SHEFF−lite−sparse
PET−SHEFF−lite
HTER−SHEFF−lite
HTER−FBK−UPV−UEDIN−wp
PET−FBK−UPV−UEDIN−wp
HTER−FBK−UPV−UEDIN−nowp
HTER−USHEFF
HTER−DCU−rtm−svr
HTER−DCU−rtm−tree
HTER−SHEFF−lite−sparse
PET−DCU−rtm−rr
PET−FBK−UPV−UEDIN−nowp
HTER−DFKI−svr
PET−USHEFF
PET−baseline
PET−DCU−rtm−svr
HTER−Multilizer
PET−SHEFF−lite−sparse
PET−SHEFF−lite
PET−Multilizer−1
HTER−baseline
HTER−DFKI−svr−xdata
PET−Multilizer−2
HTER−SHEFF−lite
HTER.DCU.rtm.svr
HTER.FBK.UPV.UEDIN.wp
HTER.DCU.rtm.tree
HTER.DFKI.svr
HTER.USHEFF
HTER.SHEFF.lite.sparse
HTER.FBK.UPV.UEDIN.nowp
HTER.Multilizer
PET.DCU.rtm.rr
HTER.DFKI.svr.xdata
PET.FBK.UPV.UEDIN.wp
PET.Multilizer.2
PET.Multilizer.1
PET.DCU.rtm.svr
HTER.baseline
PET.FBK.UPV.UEDIN.nowp
PET.USHEFF
PET.baseline
PET.SHEFF.lite.sparse
PET.SHEFF.lite
HTER.SHEFF.lite
</figure>
<bodyText confidence="0.979141571428571">
HTER.FBK.UPV.UEDIN.wp
PET.FBK.UPV.UEDIN.wp
HTER.FBK.UPV.UEDIN.nowp
HTER.USHEFF
HTER.DCU.rtm.svr
HTER.DCU.rtm.tree
HTER.SHEFF.lite.sparse
PET.DCU.rtm.rr
PET.FBK.UPV.UEDIN.nowp
HTER.DFKI.svr
PET.USHEFF
PET.baseline
PET.DCU.rtm.svr
HTER.Multilizer
PET.SHEFF.lite.sparse
PET.SHEFF.lite
PET.Multilizer.1
HTER.baseline
HTER.DFKI.svr.xdata
PET.Multilizer.2
HTER.SHEFF.lite
</bodyText>
<page confidence="0.99414">
1812
</page>
<reference confidence="0.999230971428571">
In Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, pages 266–274. As-
sociation for Computational Linguistics.
Yvette Graham, Nitika Mathur, and Timothy Bald-
win. 2015. Accurate evaluation of segment-level
machine translation metrics. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics Hu-
man Language Technologies, Denver, Colorado.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of Empiri-
cal Methods in Natural Language Processing, pages
388–395, Barcelona, Spain. Association for Compu-
tational Linguistics.
E. Moreau and C. Vogel. 2014. Limitations of mt
quality estimation supervised systems: The tails pre-
diction problem. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics, pages 2205–2216.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or hter?: exploring dif-
ferent human judgments with a tunable mt metric.
In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation, pages 259–268. Associa-
tion for Computational Linguistics.
L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and
N. Cristianini. 2009. Estimating the sentence-level
quality of machine translation systems. In 13th Con-
ference of the European Association for Machine
Translation, pages 28–37.
J.H. Steiger. 1980. Tests for comparing elements
of a correlation matrix. Psychological Bulletin,
87(2):245.
E.J. Williams. 1959. Regression analysis, volume 14.
Wiley New York.
</reference>
<page confidence="0.980054">
1813
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.682399">
<title confidence="0.999796">Improving Evaluation of Machine Translation Quality Estimation</title>
<author confidence="0.900323">Yvette</author>
<affiliation confidence="0.914458333333333">ADAPT School of Computer Science and Trinity College Dublin</affiliation>
<email confidence="0.959264">yvette.graham@scss.tcd.ie</email>
<abstract confidence="0.99914492">Quality estimation evaluation commonly takes the form of measurement of the error that exists between predictions and gold standard labels for a particular test set of translations. Issues can arise during comparison of quality estimation prediction score distributions and gold label distributions, however. In this paper, we provide an analysis of methods of comparison and identify areas of concern with respect to widely used measures, such as the ability to gain by prediction of aggregate statistics specific to gold label distributions or by optimally conservative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Compoof and quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>315--321</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1368" citStr="Blatz et al., 2004" startWordPosition="195" endWordPosition="198">onservative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components of WMT-13 and WMT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the automatic prediction of machine translation quality without the use of reference translations (Blatz et al., 2004; Specia et al., 2009). Human assessment of translation quality in theory provides the most meaningful evaluation of systems, but human assessors are known to be inconsistent and this causes challenges for quality estimation evaluation. For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example, in the 2014 Workshop on Statistical Machine Translation (WMT), which since 2012 has provided a main venue for evaluation of systems, sente</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confidence estimation for machine translation. In Proceedings of the 20th international conference on Computational Linguistics, pages 315– 321. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>J Leveling</author>
<author>C Monz</author>
<author>P Pecina</author>
<author>M Post</author>
<author>H Saint-Amand</author>
<author>R Soricut</author>
<author>L Specia</author>
<author>A Tamchyna</author>
</authors>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. 9th Wkshp. Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, MA.</location>
<contexts>
<context position="2243" citStr="Bojar et al., 2014" startWordPosition="338" endWordPosition="341"> is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example, in the 2014 Workshop on Statistical Machine Translation (WMT), which since 2012 has provided a main venue for evaluation of systems, sentence-level systems were evaluated with respect to three distinct gold standard representations and each of those compared to predictions using four different measures, resulting in a total of 12 different system rankings, 6 identified as official rankings (Bojar et al., 2014). Although the aim of several methods of evaluation is to provide more insight into performance of systems, this also produces conflicting results and raises the question which method of evaluation really identifies the system(s) or method(s) that best predicts translation quality. For example, an extreme case in WMT-14 occurred for sentence-level quality estimation for English-to-Spanish. In each of the 12 system rankings, many systems were tied and this resulted in a total of 22 official winning systems for this language pair. Besides leaving potential users of quality estimation systems at </context>
<context position="9274" citStr="Bojar et al., 2014" startWordPosition="1451" endWordPosition="1454">ing advantage by its continuous predictions. tion systems, as scores are individually computed per translation using custom post-edited reference translations, avoiding the bias that can occur with metrics that employ generic reference translations. In our later evaluation, we therefore use HTER scores in addition to PERs as suitable gold standard labels. 2.2 Mean Absolute Error Mean absolute error is likely the most widely applied comparison measure of quality estimation system predictions and gold labels, in addition to being the official measure applied to scoring variants of tasks in WMT (Bojar et al., 2014). MAE is the average absolute difference that exists between a system’s predictions and gold standard labels for translations, and a system achieving a lower MAE is considered a better system. Significant issues arise for evaluation of quality estimation systems with MAE when comparing distributions for predictions and gold labels, however. Firstly, a system’s MAE can be lowered not only by individual predictions closer to corresponding gold labels, but also by prediction of aggregate statistics specific to the distribution of gold labels in the particular test set used for evaluation. MAE is </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, Tamchyna, 2014</marker>
<rawString>O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proc. 9th Wkshp. Statistical Machine Translation, Baltimore, MA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Graham</author>
<author>T Baldwin</author>
</authors>
<title>Testing for significance of increased correlation with human judgment.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar.</location>
<contexts>
<context position="19012" citStr="Graham and Baldwin, 2014" startWordPosition="3045" endWordPosition="3048">e shift in location and scale that can be used to boost apparent performance of systems when measures like MAE and RMSE, that are not unit-free, are employed. Increasing the distance between system predictions and gold labels for anything less than the entire distribution, a more realistic scenario, or by something other than a constant across the entire distribution, will result in an appropriately weaker Pearson correlation. 4 Quality Estimation Significance Testing Previous work has shown the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015), and, for similar reasons, Williams test is appropriate for significance testing differences in performance of competing quality estimation systems which we detail further below. Evaluation of a given quality estimation system, Pnew, by Pearson correlation takes the form of quantifying the correlation, r(Pnew, G), that exists between system prediction scores and corresponding gold standard labels, and contrasting this correlation with the correlation for some baseline system, r(Pbase, G). At first it might seem reasonable to perform significance testing in the following </context>
<context position="27452" citStr="Graham and Baldwin, 2014" startWordPosition="4349" endWordPosition="4352">EFMIN.FS.AL SHEFF.lite LORIA.INCTR.CONT fbk.uedin.r.svr LORIA_INCTR baseline TCD.CNGL.OPEN TCD.CNGL.RESTR UMAC_EBLEU Figure 4: Pearson correlation between prediction scores for all pairs of systems participating in WMT-14 Task 1.2 of systems. 5.1 Significance Tests When an increase in correlation with gold labels is present for a pair of systems, significance tests provide insight into the likelihood that such an increase has occurred by chance. As described in detail in Section 4, the Williams test (Williams, 1959), a test also appropriate for MT metrics evaluated by the Pearson correlation (Graham and Baldwin, 2014), is appropriate for testing the significance of a difference in dependent correlations and therefore provides a suitable method of significance testing for quality estimation systems. Figure 4 provides an example of the strength of correlations that commonly exist between predictions of quality estimation systems. Figure 5 shows significance test outcomes of the Williams test for systems originally taking part in WMT-13 Task 1.1, with systems ordered by strongest to least Pearson correlation with gold labels, where a green cell in (row i, column j) signifies a significant win for row i system</context>
</contexts>
<marker>Graham, Baldwin, 2014</marker>
<rawString>Y. Graham and T. Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 172–176, Doha, Qatar. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Graham</author>
<author>N Mathur</author>
<author>T Baldwin</author>
</authors>
<title>Randomized significance tests in machine translation.</title>
<date>2014</date>
<contexts>
<context position="13680" citStr="Graham et al., 2014" startWordPosition="2147" endWordPosition="2150">49 DFKI-svr-xdata 0.161 0.146 SHEFF-lite 0.182 0.168 Table 2: MAE of WMT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. Table 3: RMSE of WMT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. cance testing such as bootstrap resampling in combination with BLEU and other metrics have been empirically evaluated (Koehn, 2004; Graham et al., 2014), to the best of our knowledge no research has been carried out to assess the accuracy of similar methods specifically for quality estimation evaluation. In addition, since data used for evaluation of quality estimation systems are not independent, methods of significance testing differences in performance will be inaccurate unless the dependent nature of the data is taken into account. 3 Quality Estimation Evaluation by Pearson Correlation The Pearson correlation is a measure of the linear correlation between two variables, and in the case of quality estimation evaluation this amounts to the </context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2014</marker>
<rawString>Y. Graham, N. Mathur, and T. Baldwin. 2014. Randomized significance tests in machine translation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>266--274</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker></marker>
<rawString>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, pages 266–274. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Nitika Mathur</author>
<author>Timothy Baldwin</author>
</authors>
<title>Accurate evaluation of segment-level machine translation metrics.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="19034" citStr="Graham et al., 2015" startWordPosition="3049" endWordPosition="3052">ale that can be used to boost apparent performance of systems when measures like MAE and RMSE, that are not unit-free, are employed. Increasing the distance between system predictions and gold labels for anything less than the entire distribution, a more realistic scenario, or by something other than a constant across the entire distribution, will result in an appropriately weaker Pearson correlation. 4 Quality Estimation Significance Testing Previous work has shown the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015), and, for similar reasons, Williams test is appropriate for significance testing differences in performance of competing quality estimation systems which we detail further below. Evaluation of a given quality estimation system, Pnew, by Pearson correlation takes the form of quantifying the correlation, r(Pnew, G), that exists between system prediction scores and corresponding gold standard labels, and contrasting this correlation with the correlation for some baseline system, r(Pbase, G). At first it might seem reasonable to perform significance testing in the following manner when an increas</context>
</contexts>
<marker>Graham, Mathur, Baldwin, 2015</marker>
<rawString>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2015. Accurate evaluation of segment-level machine translation metrics. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="13658" citStr="Koehn, 2004" startWordPosition="2145" endWordPosition="2146">ine 0.152 0.149 DFKI-svr-xdata 0.161 0.146 SHEFF-lite 0.182 0.168 Table 2: MAE of WMT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. Table 3: RMSE of WMT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. cance testing such as bootstrap resampling in combination with BLEU and other metrics have been empirically evaluated (Koehn, 2004; Graham et al., 2014), to the best of our knowledge no research has been carried out to assess the accuracy of similar methods specifically for quality estimation evaluation. In addition, since data used for evaluation of quality estimation systems are not independent, methods of significance testing differences in performance will be inaccurate unless the dependent nature of the data is taken into account. 3 Quality Estimation Evaluation by Pearson Correlation The Pearson correlation is a measure of the linear correlation between two variables, and in the case of quality estimation evaluatio</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Moreau</author>
<author>C Vogel</author>
</authors>
<title>Limitations of mt quality estimation supervised systems: The tails prediction problem.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics,</booktitle>
<pages>2205--2216</pages>
<contexts>
<context position="11328" citStr="Moreau and Vogel, 2014" startWordPosition="1788" endWordPosition="1791">prediction of aggregates is, in general, far easier than individual predictions. In addition, inclusion of confounding test set aggregates such as these in evaluations will likely lead to both an overestimate of the ability of some systems to predict the quality of unseen translations and an underestimate of the accuracy of systems that courageously attempt to predict the quality of translations in the tails of gold distributions, and it follows that systems optimized for MAE can be expected to perform badly when predicting the quality of translations in the tails of gold label distributions (Moreau and Vogel, 2014). Table 2 shows how MAEs of original predicted score distributions for all systems participating in Task 1.2 WMT-14 can be reduced by shifting and rescaling the prediction score distribution according to gold label aggregates. Table 3 shows that for similar reasons other measures commonly applied to evaluation of quality estimation systems, such as root mean squared error (RMSE), that are also not unit-free, encounter the same problem. 2.3 Significance Testing In quality estimation, it is common to apply bootstrap resampling to assess the likelihood that a decrease in MAE (an improvement) has </context>
</contexts>
<marker>Moreau, Vogel, 2014</marker>
<rawString>E. Moreau and C. Vogel. 2014. Limitations of mt quality estimation supervised systems: The tails prediction problem. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics, pages 2205–2216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B J Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>259--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8107" citStr="Snover et al., 2009" startWordPosition="1264" endWordPosition="1267">y than PETs, we compute correlations of each with HTER gold labels of translations from Task 1.2. Table 6 reveals a significantly higher correlation that exists between PER and HTER compared to PET and HTER (p &lt; 0.001) , and we conclude therefore that the PER of a translation provides a more faithful representation of translation quality than PET, and convert PETs for both predictions and gold labels to PERs (in seconds per word) in our later replication of Task 1.3. In Task 1.2 of WMT-14, gold standard labels used to evaluate systems were in the form of human translation error rates (HTERs) (Snover et al., 2009). HTER scores provide an effective representation for evaluation of quality estima1805 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 1 2 3 (a) Gold Labels MAE: 0.687 1 2 3 (b) System A Predictions 0.5 1.0 1.5 2.0 2.5 3.0 3.5 (c) System B Predictions MAE: 0.603 0.0 0.2 0.4 0.6 0.8 1.0 Figure 1: WMT-14 English-to-German quality estimation Task 1.1 where mismatched prediction/gold labels achieves apparent better performance, where (a) gold label distribution; (b) example system disadvantaged by its discrete predictions; (c) example system gaining advantage by its continuous predictions. tion sy</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz. 2009. Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259–268. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>M Turchi</author>
<author>N Cancedda</author>
<author>M Dymetman</author>
<author>N Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In 13th Conference of the European Association for Machine Translation,</booktitle>
<pages>28--37</pages>
<contexts>
<context position="1390" citStr="Specia et al., 2009" startWordPosition="199" endWordPosition="202"> in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components of WMT-13 and WMT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the automatic prediction of machine translation quality without the use of reference translations (Blatz et al., 2004; Specia et al., 2009). Human assessment of translation quality in theory provides the most meaningful evaluation of systems, but human assessors are known to be inconsistent and this causes challenges for quality estimation evaluation. For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example, in the 2014 Workshop on Statistical Machine Translation (WMT), which since 2012 has provided a main venue for evaluation of systems, sentence-level systems were</context>
</contexts>
<marker>Specia, Turchi, Cancedda, Dymetman, Cristianini, 2009</marker>
<rawString>L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In 13th Conference of the European Association for Machine Translation, pages 28–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Steiger</author>
</authors>
<title>Tests for comparing elements of a correlation matrix.</title>
<date>1980</date>
<journal>Psychological Bulletin,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="21193" citStr="Steiger, 1980" startWordPosition="3399" endWordPosition="3400">ferences in correlations. Data used for evaluation of quality estimation systems are not independent, however, and this means that if r(Pbase, G) and r(Pnew, G) are both &gt; 0, the correlation between both sets of predictions themselves, r(Pbase, Pnew), must also be &gt; 0. The strength of this correlation, directly between predictions of pairs of quality estimation systems, should be taken into account using a significance test of the difference in correlation between r(Pbase, G) and r(Pnew, G). Williams test 1 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980). It is formulated as follows as a test of whether the population correlation between X1 and X3 equals the population correlation between X2 and X3: where rij is the correlation between Xi and Xj, n is the size of the population, and: K = 1 − r122 − r132 − r232 + 2r12r13r23 As part of this research, we have made available an open-source implementation of statistical tests tailored to the assessment of quality estimation systems, at https://github.com/ ygraham/mt-qe-eval. 5 Evaluation and Discussion To demonstrate the use of the Pearson correlation as an effective mechanism for evaluation of qu</context>
</contexts>
<marker>Steiger, 1980</marker>
<rawString>J.H. Steiger. 1980. Tests for comparing elements of a correlation matrix. Psychological Bulletin, 87(2):245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Williams</author>
</authors>
<title>Regression analysis,</title>
<date>1959</date>
<volume>14</volume>
<publisher>Wiley</publisher>
<location>New York.</location>
<contexts>
<context position="18947" citStr="Williams, 1959" startWordPosition="3037" endWordPosition="3038">istribution (of either of the two variables), such as the shift in location and scale that can be used to boost apparent performance of systems when measures like MAE and RMSE, that are not unit-free, are employed. Increasing the distance between system predictions and gold labels for anything less than the entire distribution, a more realistic scenario, or by something other than a constant across the entire distribution, will result in an appropriately weaker Pearson correlation. 4 Quality Estimation Significance Testing Previous work has shown the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015), and, for similar reasons, Williams test is appropriate for significance testing differences in performance of competing quality estimation systems which we detail further below. Evaluation of a given quality estimation system, Pnew, by Pearson correlation takes the form of quantifying the correlation, r(Pnew, G), that exists between system prediction scores and corresponding gold standard labels, and contrasting this correlation with the correlation for some baseline system, r(Pbase, G). At first it might </context>
<context position="21108" citStr="Williams, 1959" startWordPosition="3387" endWordPosition="3388">In such cases, the Fisher r to z transformation is applied to test for significant differences in correlations. Data used for evaluation of quality estimation systems are not independent, however, and this means that if r(Pbase, G) and r(Pnew, G) are both &gt; 0, the correlation between both sets of predictions themselves, r(Pbase, Pnew), must also be &gt; 0. The strength of this correlation, directly between predictions of pairs of quality estimation systems, should be taken into account using a significance test of the difference in correlation between r(Pbase, G) and r(Pnew, G). Williams test 1 (Williams, 1959) evaluates the significance of a difference in dependent correlations (Steiger, 1980). It is formulated as follows as a test of whether the population correlation between X1 and X3 equals the population correlation between X2 and X3: where rij is the correlation between Xi and Xj, n is the size of the population, and: K = 1 − r122 − r132 − r232 + 2r12r13r23 As part of this research, we have made available an open-source implementation of statistical tests tailored to the assessment of quality estimation systems, at https://github.com/ ygraham/mt-qe-eval. 5 Evaluation and Discussion To demonstr</context>
<context position="27348" citStr="Williams, 1959" startWordPosition="4334" endWordPosition="4335">PV.UEDIN.nowp CMU.ISL.full Multilizer fbk.uedin.extra DFKI.svr.xdata LIMSI.ELASTIC baseline SHEFMIN.FS.AL SHEFF.lite LORIA.INCTR.CONT fbk.uedin.r.svr LORIA_INCTR baseline TCD.CNGL.OPEN TCD.CNGL.RESTR UMAC_EBLEU Figure 4: Pearson correlation between prediction scores for all pairs of systems participating in WMT-14 Task 1.2 of systems. 5.1 Significance Tests When an increase in correlation with gold labels is present for a pair of systems, significance tests provide insight into the likelihood that such an increase has occurred by chance. As described in detail in Section 4, the Williams test (Williams, 1959), a test also appropriate for MT metrics evaluated by the Pearson correlation (Graham and Baldwin, 2014), is appropriate for testing the significance of a difference in dependent correlations and therefore provides a suitable method of significance testing for quality estimation systems. Figure 4 provides an example of the strength of correlations that commonly exist between predictions of quality estimation systems. Figure 5 shows significance test outcomes of the Williams test for systems originally taking part in WMT-13 Task 1.1, with systems ordered by strongest to least Pearson correlatio</context>
</contexts>
<marker>Williams, 1959</marker>
<rawString>E.J. Williams. 1959. Regression analysis, volume 14. Wiley New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>