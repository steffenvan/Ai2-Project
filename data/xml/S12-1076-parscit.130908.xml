<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.032741">
<title confidence="0.96441">
ETS: Discriminative Edit Models for Paraphrase Scoring
</title>
<author confidence="0.900714">
Michael Heilman and Nitin Madnani
</author>
<affiliation confidence="0.832509">
Educational Testing Service
</affiliation>
<address confidence="0.893939">
660 Rosedale Road
Princeton, NJ 08541, USA
</address>
<email confidence="0.998796">
{mheilman,nmadnani}@ets.org
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999658944444444">
Many problems in natural language process-
ing can be viewed as variations of the task of
measuring the semantic textual similarity be-
tween short texts. However, many systems
that address these tasks focus on a single task
and may or may not generalize well. In this
work, we extend an existing machine transla-
tion metric, TERp (Snover et al., 2009a), by
adding support for more detailed feature types
and by implementing a discriminative learning
algorithm. These additions facilitate applica-
tions of our system, called PERP, to similar-
ity tasks other than machine translation eval-
uation, such as paraphrase recognition. In
the SemEval 2012 Semantic Textual Similar-
ity task, PERP performed competitively, par-
ticularly at the two surprise subtasks revealed
shortly before the submission deadline.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903319148936">
Techniques for measuring the similarity of two sen-
tences have various potential applications: auto-
mated short answer scoring (Nielsen et al., 2008;
Leacock and Chodorow, 2003), question answering
(Wang et al., 2007), machine translation evaluation
(Przybocki et al., 2009; Snover et al., 2009a), etc.
An important aspect of this problem is that sim-
ilarity is not binary. Sentences can be very seman-
tically similar, such that they might be called para-
phrases of each other. They might be completely
different. Or, they might be somewhere in between.
Indeed, it is arguable that all sentence pairs (except
exact duplicates) lie somewhere on a continuum of
similarity. Therefore, it is desirable to develop meth-
ods that model sentence pair similarity on a contin-
uous, or at least ordinal, scale.
In this paper, we describe a system for measuring
the semantic similarity of pairs of short texts. As a
starting point, we use the Translation Error Rate Plus
(Snover et al., 2009a), or TERp, system, which was
specifically developed for machine translation eval-
uation. TERp takes two sentences as input, finds a
set of weighted edits that convert one into the other
with low overall weight, and then produces a length-
normalized score. TERp also has a greedy, heuris-
tic learning algorithm for inducing weights from la-
beled sentence pairs in order to increase correlations
with human similarity scores.
Some features of the original TERp make adap-
tation to other semantic similarity tasks difficult, in-
cluding its largely one-to-one mapping of features
to edits and its heuristic, greedy learning algorithm.
For example, there is a single feature for lexical sub-
stitution, even though it is clear that different types
of substitutions have different effects on similarity
(e.g., substituting “43.6” with “17” versus substitut-
ing “a” for “an”). In addition, the heuristic learn-
ing algorithm, which involves perturbing the weight
vector by small amounts as in grid search, seems un-
scalable to larger sets of overlapping features.
Therefore, here, we use TERp’s inference algo-
rithms that find low cost edit sequences but use a dis-
criminative learning algorithm based on the Percep-
tron (Rosenblatt, 1958; Collins, 2002) to estimate
edit cost parameters, along with an expanded fea-
ture set for broader coverage of the phenomena that
are relevant to sentence-to-sentence similarity. We
</bodyText>
<page confidence="0.977135">
529
</page>
<note confidence="0.5281495">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529–535,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9981932">
refer to this new approach as Paraphrase Edit Rate
with the Perceptron (PERP).
In addition to describing PERP, we discuss how it
was applied for the SemEval 2012 Semantic Textual
Similarity (STS) task.
</bodyText>
<sectionHeader confidence="0.969884" genericHeader="method">
2 Problem Definition
</sectionHeader>
<bodyText confidence="0.99998676">
In this work, our goal is to create a system that can
take as input two sentences (or short texts) x1 and x2
and produce as output a prediction y� for how simi-
lar they are. Here, we use the 0 to 5 ordinal scale
from the STS task, where increasing values indicate
greater semantic similarity.
The STS task data includes five subtasks with text
pairs from different sources: the Microsoft Research
Paraphrase Corpus (Dolan et al., 2004) (MSRpar),
The Microsoft Research Video corpus (Chen and
Dolan, 2011) (MSRvid), statistical machine transla-
tion output of parliament proceedings (Koehn, 2005)
(SMT-eur). For each of these sources, approxi-
mately 750 sentence pairs x1 and x2 and gold stan-
dard similarity values y were provided for training
and development.
In addition, there were two surprise data sources
revealed shortly before the submission deadline:
pairs of sentences from Ontonotes (Pradhan and
Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN),
and machine translations of sentences from news
conversations (SMT-news). For all five sources,
the held-out test set contained several hundred text
pairs. See the task description (Agirre et al., 2012)
for additional details.
</bodyText>
<sectionHeader confidence="0.951603" genericHeader="method">
3 TER, TERp, and PERP
</sectionHeader>
<bodyText confidence="0.999822909090909">
In this section, we briefly describe the TER and
TERp machine translation metrics, and how the
PERP system extends them in order to better model
semantic textual similarity.
TER (Snover et al., 2006) uses a greedy search al-
gorithm to find a set of edits to convert one of the
paired input sentences into the other. We can view
this set of edits as an alignment a between the two
input sentences x1 and x2, and when two words in
x1 and x2, respectively, are part of an edit operation,
we say that those words are aligned.1 Unlike tradi-
</bodyText>
<footnote confidence="0.9996605">
1For machine translation evaluation with TERp and PERP,
x1 is a system’s hypothesis and x2 is a reference translation. For
</footnote>
<bodyText confidence="0.9999012">
tional edit distance measures, TER allow for shifts—
that is, edits that change the positions of words or
phrases in the input sentence x1. Essentially, TER
searches among a set of possible shifts of the phrases
in x1 to find a set of shifts that result in the least
cost alignment, using edits of other types, between
x2 and the shifted version of x1. TER allows one to
specify costs for different edit types, but it does not
include a method for learning those costs from data.
TERp (Snover et al., 2009b; Snover et al., 2009a)
extends TER in two key ways. First, TERp in-
cludes new types of edits, including edits for substi-
tution of synonyms, word stems, and phrasal para-
phrases extracted from a pivot-based paraphrase ta-
ble (§3.1). Second, it includes a heuristic learning
algorithm for inferring cost parameters from labeled
data. TERp includes 8 types of edits: match (M), in-
sertion (I), deletion (D), substitution (S), stemming
(T), synonymy (Y), shift (Sh), and phrase substitu-
tion (P). The edits are mutually exclusive, such that
synonymy edits do not count as substitutions, for ex-
ample. TERp has 11 total parameters, with a single
parameter for each edit except for phrase substition,
which has four.
PERP has a general framework similar to that
of TERp. It extends TERp, however, by includ-
ing additional edit parameters, and by using a dis-
criminative learning algorithm (see §5) to learn pa-
rameters rather than the heuristic technique used by
TERp. Thus, PERP uses the same greedy algorithm
as TERp for finding the optimal sets of edits given
the cost parameters, but it allows the cost for an indi-
vidual edit to depend on multiple, overlapping fea-
tures of that edit. For example, costs for substitu-
tion edits depend on whether the aligned words are
pronouns, whether the aligned words represent num-
bers, the lengths of the aligned words, etc. See §4 for
the full list of features in PERP.
An alignment from the MSRpar portion of the
STS training data is illustrated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997541">
3.1 Phrasal Paraphrases
</subsectionHeader>
<bodyText confidence="0.996843166666667">
PERP uses probabilistic phrasal substitutions to
align phrases in the hypothesis with phrases in the
all STS subtasks, we assigned sentences in the first and second
columns of the input files to x2 and x1, respectively, so that
the hypotheses and references in the SMT-eur subtask would be
assigned appropriately.
</bodyText>
<page confidence="0.952249">
530
</page>
<figure confidence="0.995897363636363">
the firm had predicted earlier this year a 4.9 percent increase .
shift shift
the firm earlier had predicted increase this
delete synonymy delete delete
firm earlier had forecast an increase of 4.9 percent .
x1
x2 the research
insert
insert
insert
year a 4.9 percent .
</figure>
<figureCaption confidence="0.980782666666667">
Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus.
The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match
individual words are not shown.
</figureCaption>
<bodyText confidence="0.9999061">
reference. It does so by looking up—in a pre-
computed phrase table—paraphrases of phrases in
the reference and using its associated edit cost as
the cost of performing a match against the hypoth-
esis. The paraphrase table used in PERP was iden-
tical to the one used by Snover et al. (2009a). It
was extracted using the pivot-based method as de-
scribed by Bannard and Callison-Burch (2005) with
several additional filtering mechanisms to increase
the precision of the extracted pairs. The pivot-based
method utilizes the inherent monolingual semantic
knowledge from bilingual corpora: we first iden-
tify phrasal correspondences between English and a
given foreign language F, then map from English to
English by following translation units from English
to the other language and back. For example, if the
two English phrases e1 and e2 both correspond to
the same foreign phrase f, then they may be consid-
ered to be paraphrases of each other with the follow-
ing probability:
</bodyText>
<equation confidence="0.722838">
p(e1|e2) ;z:� p(e1|f)p(f|e2)
</equation>
<bodyText confidence="0.988110333333333">
If there are several pivot phrases that link the two
English phrases, then they are all used in computing
the probability:
</bodyText>
<equation confidence="0.987897">
p(e1|e2) � � p(e1|ft)p(ft|e2)
P
</equation>
<bodyText confidence="0.999983833333333">
We used the same phrasal paraphrase database as
in TERp (Snover et al., 2009a), which was extracted
from an Arabic-English newswire bitext containing
a million sentences. A few examples of the para-
phrase pairs used in the MSRpar portion of the STS
training data are shown below:
</bodyText>
<equation confidence="0.9477864">
(commission —* panel)
(the spying —* espionage)
(suffered —* underwent)
(room to —* space for)
(per cent —* percent)
</equation>
<sectionHeader confidence="0.996898" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.975227685714286">
As discussed in §3, PERP expands on TERp’s origi-
nal features in order to better model semantic textual
similarity.
PERP models a pair of sentences x1 and x2 us-
ing a feature function f(a) that extracts a vector of
real-valued features from an alignment a between
x1 and x2. This alignment is found with TERp’s
inference algorithm and consists of a set of edits
of various types along with information about the
words on which those edits operate. For example,
the alignment might contain an edit with the infor-
mation, “The token ‘the’ in x1 was substituted for
the token ‘an’ in x2.” This edit would increment the
features in f(a) for the number of substitutions and
the number of substitutions of stopwords, along with
other relevant substitution features.
The set of features encoded in f(a) are described
in Table 1.2 It includes general features that always
fire for edits of a particular type (e.g., the “Substi-
tution” feature) as well as specific features that fire
only in specific situations (e.g., the “Sub-Pronoun-
Both” edit, which fires only when one pronoun is
substituted for another).
The function f(a) is normalized for sentence
2All words were converted to lower-case. Word frequen-
cies were calculated from the NYT stories in the fifth edition
of the English Gigaword corpus. The stories were tokenized
using NLTK and words occurring fewer than 100 times were
excluded. Words occurring at least 100 times constituted the vo-
cabulary used for computing the OOV features. The OOV and
frequency features only fired for words that consisted only of
letters, and the frequency features did not fire for OOV words.
The set of negation words including the following: “no”, “not”,
“never”, and “n’t”. The stopword list contained 158 common
words and punctuation symbols.
</bodyText>
<page confidence="0.965614">
531
</page>
<table confidence="0.999593025641026">
Edits Feature Name Description
- Intercept Always 1 (and not normalized by text lengths)
T Stemming The number of times that two words with the same stem, according to the Porter
(1980) stemmer, were aligned.
Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum,
1998), were aligned.
Sh Shift The number of shifts.
P Paraphrase1 The number of phrasal paraphrasing operations.
P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table
for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover
et al. (2009a) for further explanation.
P Paraphrase3 The sum of pq, where p and q are as above.
P Paraphrase4 The sum of q, where q is as above.
I Insertion The number of insertions.
D Deletion The number of deletions.
I, D Insert-Delete- The sum of log10 freq(w) over all insertions and deletions, where w is the word
LogFreq being inserted or deleted and freq(w) is the relative frequency of w.
I, D Insert-Delete- The sum of log10 length(w) over all insertions and deletions, where w is the word
LogWordLen being inserted or deleted.
I, D Insert-Delete- The number of insertions and deletions of X in alignment, where X is: (a) punctu-
X ation, (b) numbers, (c) personal pronouns, (d) negation words, (e) stop words, or (f)
out-of-vocabulary (OOV) words (6 features in all).
S Substitution The number of substitutions.
S Sub-X-Both The number of substitutions where both words are: (a) punctuation, (b) numbers, (c)
personal pronouns, (d) negation words, (e) stop words, or (f) OOV words (6 features
in all).
S Sub-X-1only The number of substitutions where only one word is: (a) punctuation, (b) a number,
(c) a personal pronoun, (d) a negation word, (e) a stop word, or (f) an OOV word (6
features in all).
S Sub-LogFreq- The sum of  |log10 freq(w1) − log10 freq(w2) |over all substitutions.
Diff
S Sub-Contain The number of substitutions where both words have more than 5 characters and one
is a proper substring of the other.
S Sub-Diff-By- The number of substitutions where the words differ only by non-alphanumeric char-
NonWord acters.
S Sub-Small- The number of substitutions where both words have more than 5 characters and the
LevDist Levenshtein distance between them is 1.
S Sub-Norm- The sum of the following over all substitutions: the Levenshtein distance between
LevDist the words normalized by the length of the longer word.
</table>
<tableCaption confidence="0.999968">
Table 1: The set of features in PERP. The first column lists which edits for which each feature is relevant.
</tableCaption>
<bodyText confidence="0.999474583333333">
lengths by dividing all the values in Table 1 by the
sum of the number of words in x1 and x2, except for
the intercept feature that models the base similarity
value in the training data and always has value 1.
There are 36 features and corresponding parame-
ters in all, compared to 11 for TERp.
It is worth pointing out that while the mutual ex-
clusivity between most of the original TERp edits
is preserved, PERP does have shared features be-
tween insert and delete edits (e.g., “Insert-Delete-
Number”), and could in principle share features be-
tween substitution, stemming, and synonymy edits.
</bodyText>
<sectionHeader confidence="0.995397" genericHeader="method">
5 Learning
</sectionHeader>
<bodyText confidence="0.987504333333333">
Given a training set consisting of paired sentences
x1 and x2 and gold standard semantic similarity rat-
ings y, PERP uses Algorithm 1 to induce a good set
</bodyText>
<page confidence="0.993452">
532
</page>
<bodyText confidence="0.985033166666667">
Algorithm 1 learn(w, T, α, x1, x2, y):
An Averaged Perceptron algorithm for learning edit
cost parameters. T is the number of iterations
through the dataset. α is a learning rate. x1 and
x2 are paired lists of sentences, and y is a list of
similarities that correspond to those sentence pairs.
</bodyText>
<equation confidence="0.902574846153846">
wsum = 0
for t = 1,2,...,T do
x1, x2, y = shuffle(x1, x2, y)
for i = 1, 2, ... , jyj do
a = TERpAlign(w, x1i, x2i)
y�= w · f(a)
w = w + α(yi − 9)f(a)
w = applyShiftConstraint(w)
wsum = wsum + w
end for
end for
return wsum
Tlyl
</equation>
<bodyText confidence="0.999944733333333">
of cost parameters for its various features.3 The al-
gorithm is a fairly straightforward application of the
Perceptron algorithm described by Collins (2002).4
The only notable difference is that the algorithm
constrains PERP’s shift parameter to be at least 0.01
in the step labeled “applyShiftConstraint.” We found
that TERp’s inference algorithm would fail if the
shift cost reached zero.5 In our experiments, we ini-
tialized all weights to 0, except for the following: the
“Substitution,” “Insertion,” and “Deletion” weights
were initialized to 1.0, and the “Shift” weight was
initialized to 0.1. Following Collins (2002), the al-
gorithm returns an averaged version of the weights,
though this did not appear to substantially impact
performance.
</bodyText>
<footnote confidence="0.927905692307692">
3The “shuffle” step shuffles the lists of sentence pairs and
scores together such that their orderings are randomized but that
they stay aligned with each other.
4There are a few hyperparameters in the learning algorithms.
For our experiments, we set the number of iterations through
the training data T to 200. We set the learning rate α to 0.01 to
avoid large oscillations in the parameters. We did not system-
atically tune the hyperparameters. Other values might lead to
better performance.
5With zero cost shifts, TERp would enter a loop and even-
tually exceed the amount of available memory. We also set the
same minimum cost of 0.01 for shifts in our experiments with
the original TERp.
</footnote>
<sectionHeader confidence="0.995417" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999979195652174">
In this section, we report results for the STS shared
task. For a full description of the task, see Agirre et
al. (2012).
The task consisted of three known subtasks
(MSRpar, MSRvid, and SMT-eur) and two surprise
subtasks (On-WN, SMT-news). For the known sub-
tasks, we trained models with task-specific data
only. For the On-WN subtask, we used the model
trained for MSRpar. For SMT-news, we used the
model trained for SMT-eur.
Our submissions to the task included results from
two variations, one using the full system (PERP-
phrases) and one with the paraphrase substitution
edits disabled (PERP), in order to isolate the effect
of including phrasal paraphrases. In our original
submission, the PERPphrases system included a mi-
nor bug that affected the calculation of the phrasal
paraphrasing features. Here, we report both the orig-
inal results and a corrected version (“PERPphrases
(fix)”), though the correction only minimally af-
fected performance. We also tested two variations
of the original TERp system: one with the weights
set as reported by Snover et al. (2009a) (“TERp
(default)”), and one tuned in the same task-specific
manner as PERP (“TERp (tuned)”). We multiplied
TERp’s predictions by −1 since it produces costs
rather than similarities.
The results, in terms of Pearson correlations with
test set gold standard scores, are shown in Table 2.
In addition to correlations for each subtask, we in-
clude the three aggregated measures used for the
task. The “ALL” measure is the Pearson correlations
on the concatenation of all the data for all five sub-
tasks. It was the original measured used to aggregate
the results for the different subtasks. The second ag-
gregated measure is the “Allnrm” measure, which
we view as an oracle because it uses the gold stan-
dard similarity values from the test set to adjust sys-
tem predictions. The final aggregate measure is the
mean of the correlations for the subtasks, weighted
by the number of examples in each subtask’s test set
(“Mean”). See Agirre et al. (2012) for a full descrip-
tion of the metrics.
For comparison, the table also includes the re-
sults from the top-ranked submission according to
the “ALL” measure, the results for the word-overlap
</bodyText>
<page confidence="0.995616">
533
</page>
<table confidence="0.9991118">
Aggregated Measures Subtask Measures
ALL ALLnrm Mean MSRpar MSRvid SMT-eur On-WN SMT-news
UKP (top-ranked) .8239 .8579 .6773 .6830 .8739 .5280 .6641 .4937
PERPphrases (fix) † .7837 — .6405 .6410 .7209 .4852 .7127 .5312
PERPphrases .7834 .8089 .6399 .6397 .7200 .4850 .7124 .5312
PERP .7808 .8064 .6305 .6211 .7210 .4722 .7080 .5149
TERp (tuned) † .5558 — .5582 .5400 .6099 .4967 .5862 .5135
TERp (default) .4477 .7291 .5253 .5049 .5217 .4748 .6169 .4566
baseline .3110 .6732 .4356 .4334 .2996 .4542 .5864 .3908
mean of submissions .5864 .7773 .5286 .4894 .7049 .3958 .5557 .3731
</table>
<tableCaption confidence="0.997104">
Table 2: Pearson correlations between predictions about the test data and gold standard scores. “†” marks experiments
that were not parts of the official SemEval task 6 evaluation. The highest correlation in each column is given in bold.
ALLnrm results are not included for all runs because we did not have an implementation of that measure.
</tableCaption>
<bodyText confidence="0.999822714285714">
baseline from the organizers (Agirre et al., 2012),
and the means across all 88 submissions (not includ-
ing the baseline).
Table 3 shows the rankings in the official results
of the PERPphrases submission, for each subtask
and overall, along with Pearson correlations from
PERP and the best submission for each subtask.
</bodyText>
<table confidence="0.9987775">
Aggregated Measure Rank p pbest
ALL 6 .7834 .8239
ALLnrm 27 .8089 .8635
Mean 7 .6399 .6773
Subtask Measure Rank p pbest
MSRpar 8 .6397 .7343
MSRvid 52 .7200 .8803
SMT-eur 21 .4850 .5666
On-WN 2 .7124 .7273
SMT-news 4 .5312 .6085
</table>
<tableCaption confidence="0.998846">
Table 3: The ranking and correlation (p) obtained by
</tableCaption>
<bodyText confidence="0.878177">
PERPphrases for each of the five datasets as well for all
datasets combined. The STS task had a total of 88 sub-
missions. pbe3t shows the correlation for the best submis-
sion, across all submissions, for each dataset.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999887714285714">
From the results in §6, PERP appears to be com-
petitive at measuring semantic textual similarity. It
performed particularly well on the surprise subtasks,
indicating that it generalizes well to new data. Fi-
nally, with the exception of the SMT-eur machine
translation evaluation subtask, PERP outperformed
the TERp system for all of the STS subtasks.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999565">
We would like to thank the organizers of SemEval
and the Semantic Textual Similarity task. We would
also like to thank Matt Snover for making the origi-
nal TERp code available.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774444444445">
E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.
2012. SemEval-2012 task 6: A pilot on semantic tex-
tual similarity. In Proc. of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. ofACL, pages
597–604.
D. Chen and W. B. Dolan. 2011. Collecting highly par-
allel data for paraphrase evaluation. In Proc. of ACL,
pages 190–200.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
the perceptron algorithm. In Proc. of EMNLP.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Proc. of
COLING, pages 350–356, Geneva, Switzerland.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In Proc. of Machine Transla-
tion Summit.
C. Leacock and M. Chodorow. 2003. c-rater: Scoring of
short-answer questions. Computers and the Humani-
ties, 37.
</reference>
<page confidence="0.983736">
534
</page>
<reference confidence="0.999907475">
R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Clas-
sification errors in a domain-independent assessment
system. In Proc. of the Third Workshop on Innova-
tive Use of Natural Language Processing for Building
Educational Applications.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130–137.
S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90%
solution. In Proc. of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Companion Volume: Tutorial Abstracts, pages
11–12.
M. A. Przybocki, K. Peterson, S. Bronsart, and G. A.
Sanders. 2009. The NIST 2008 metrics for machine
translation challenge - overview, methodology, met-
rics, and results. Machine Translation, 23(2-3):71–
103.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of Translation Edit Rate
with targeted human annotation. In Proc. of the Con-
ference of the Association for Machine Translation in
the Americas (AMTA).
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009a. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric. In
Proc. of the Fourth Workshop on Statistical Machine
Translation at the 12th Meeting of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2009), March.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009b. TER-Plus: Paraphrase, semantic, and align-
ment enhancements to Translation Edit Rate. Machine
Translation, 23(2–3):117–127.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of of EMNLP.
</reference>
<page confidence="0.998536">
535
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.918700">
<title confidence="0.999856">ETS: Discriminative Edit Models for Paraphrase Scoring</title>
<author confidence="0.956749">Heilman</author>
<affiliation confidence="0.942512">Educational Testing</affiliation>
<address confidence="0.9981865">660 Rosedale Princeton, NJ 08541,</address>
<abstract confidence="0.999482105263158">Many problems in natural language processing can be viewed as variations of the task of measuring the semantic textual similarity between short texts. However, many systems that address these tasks focus on a single task and may or may not generalize well. In this work, we extend an existing machine translation metric, TERp (Snover et al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Cer</author>
<author>M Diab</author>
<author>A Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proc. of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="4926" citStr="Agirre et al., 2012" startWordPosition="766" endWordPosition="769">e translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other. We can view this set of edits as an alignment a between the two input sentences x1 and x2, and when two words in x1 and x2, respectively, are part of an edit operation, we say that those words are aligned.1 Unlike tradi1For machine transl</context>
<context position="17227" citStr="Agirre et al. (2012)" startWordPosition="2861" endWordPosition="2864">rning algorithms. For our experiments, we set the number of iterations through the training data T to 200. We set the learning rate α to 0.01 to avoid large oscillations in the parameters. We did not systematically tune the hyperparameters. Other values might lead to better performance. 5With zero cost shifts, TERp would enter a loop and eventually exceed the amount of available memory. We also set the same minimum cost of 0.01 for shifts in our experiments with the original TERp. 6 Experiments In this section, we report results for the STS shared task. For a full description of the task, see Agirre et al. (2012). The task consisted of three known subtasks (MSRpar, MSRvid, and SMT-eur) and two surprise subtasks (On-WN, SMT-news). For the known subtasks, we trained models with task-specific data only. For the On-WN subtask, we used the model trained for MSRpar. For SMT-news, we used the model trained for SMT-eur. Our submissions to the task included results from two variations, one using the full system (PERPphrases) and one with the paraphrase substitution edits disabled (PERP), in order to isolate the effect of including phrasal paraphrases. In our original submission, the PERPphrases system included</context>
<context position="19118" citStr="Agirre et al. (2012)" startWordPosition="3170" endWordPosition="3173">h subtask, we include the three aggregated measures used for the task. The “ALL” measure is the Pearson correlations on the concatenation of all the data for all five subtasks. It was the original measured used to aggregate the results for the different subtasks. The second aggregated measure is the “Allnrm” measure, which we view as an oracle because it uses the gold standard similarity values from the test set to adjust system predictions. The final aggregate measure is the mean of the correlations for the subtasks, weighted by the number of examples in each subtask’s test set (“Mean”). See Agirre et al. (2012) for a full description of the metrics. For comparison, the table also includes the results from the top-ranked submission according to the “ALL” measure, the results for the word-overlap 533 Aggregated Measures Subtask Measures ALL ALLnrm Mean MSRpar MSRvid SMT-eur On-WN SMT-news UKP (top-ranked) .8239 .8579 .6773 .6830 .8739 .5280 .6641 .4937 PERPphrases (fix) † .7837 — .6405 .6410 .7209 .4852 .7127 .5312 PERPphrases .7834 .8089 .6399 .6397 .7200 .4850 .7124 .5312 PERP .7808 .8064 .6305 .6211 .7210 .4722 .7080 .5149 TERp (tuned) † .5558 — .5582 .5400 .6099 .4967 .5862 .5135 TERp (default) .4</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. In Proc. of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>C Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. ofACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="8874" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1449" endWordPosition="1452">: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus. The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match individual words are not shown. reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrase table used in PERP was identical to the one used by Snover et al. (2009a). It was extracted using the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify phrasal correspondences between English and a given foreign language F, then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they may be considered to be paraphrases of each other with the following probability: p(e1|e2) ;z:� p(e</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>C. Bannard and C. Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. ofACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>W B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>190--200</pages>
<contexts>
<context position="4277" citStr="Chen and Dolan, 2011" startWordPosition="670" endWordPosition="673">PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction y� for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pair</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>D. Chen and W. B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proc. of ACL, pages 190–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="3213" citStr="Collins, 2002" startWordPosition="501" endWordPosition="502">rithm. For example, there is a single feature for lexical substitution, even though it is clear that different types of substitutions have different effects on similarity (e.g., substituting “43.6” with “17” versus substituting “a” for “an”). In addition, the heuristic learning algorithm, which involves perturbing the weight vector by small amounts as in grid search, seems unscalable to larger sets of overlapping features. Therefore, here, we use TERp’s inference algorithms that find low cost edit sequences but use a discriminative learning algorithm based on the Perceptron (Rosenblatt, 1958; Collins, 2002) to estimate edit cost parameters, along with an expanded feature set for broader coverage of the phenomena that are relevant to sentence-to-sentence similarity. We 529 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529–535, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics refer to this new approach as Paraphrase Edit Rate with the Perceptron (PERP). In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a s</context>
<context position="15811" citStr="Collins (2002)" startWordPosition="2631" endWordPosition="2632">algorithm for learning edit cost parameters. T is the number of iterations through the dataset. α is a learning rate. x1 and x2 are paired lists of sentences, and y is a list of similarities that correspond to those sentence pairs. wsum = 0 for t = 1,2,...,T do x1, x2, y = shuffle(x1, x2, y) for i = 1, 2, ... , jyj do a = TERpAlign(w, x1i, x2i) y�= w · f(a) w = w + α(yi − 9)f(a) w = applyShiftConstraint(w) wsum = wsum + w end for end for return wsum Tlyl of cost parameters for its various features.3 The algorithm is a fairly straightforward application of the Perceptron algorithm described by Collins (2002).4 The only notable difference is that the algorithm constrains PERP’s shift parameter to be at least 0.01 in the step labeled “applyShiftConstraint.” We found that TERp’s inference algorithm would fail if the shift cost reached zero.5 In our experiments, we initialized all weights to 0, except for the following: the “Substitution,” “Insertion,” and “Deletion” weights were initialized to 1.0, and the “Shift” weight was initialized to 0.1. Following Collins (2002), the algorithm returns an averaged version of the weights, though this did not appear to substantially impact performance. 3The “shu</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with the perceptron algorithm. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>350--356</pages>
<publisher>Bradford Books.</publisher>
<location>Geneva,</location>
<contexts>
<context position="4208" citStr="Dolan et al., 2004" startWordPosition="660" endWordPosition="663">se Edit Rate with the Perceptron (PERP). In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction y� for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all fi</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>W. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proc. of COLING, pages 350–356, Geneva, Switzerland. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of Machine Translation Summit.</booktitle>
<contexts>
<context position="4366" citStr="Koehn, 2005" startWordPosition="683" endWordPosition="684"> Problem Definition In this work, our goal is to create a system that can take as input two sentences (or short texts) x1 and x2 and produce as output a prediction y� for how similar they are. Here, we use the 0 to 5 ordinal scale from the STS task, where increasing values indicate greater semantic similarity. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, an</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. of Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>c-rater: Scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<contexts>
<context position="1182" citStr="Leacock and Chodorow, 2003" startWordPosition="172" endWordPosition="175">g support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at leas</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>C. Leacock and M. Chodorow. 2003. c-rater: Scoring of short-answer questions. Computers and the Humanities, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Nielsen</author>
<author>W Ward</author>
<author>J H Martin</author>
</authors>
<title>Classification errors in a domain-independent assessment system.</title>
<date>2008</date>
<booktitle>In Proc. of the Third Workshop on Innovative Use of Natural Language Processing for Building Educational Applications.</booktitle>
<contexts>
<context position="1153" citStr="Nielsen et al., 2008" startWordPosition="168" endWordPosition="171"> al., 2009a), by adding support for more detailed feature types and by implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarit</context>
</contexts>
<marker>Nielsen, Ward, Martin, 2008</marker>
<rawString>R. D. Nielsen, W. Ward, and J. H. Martin. 2008. Classification errors in a domain-independent assessment system. In Proc. of the Third Workshop on Innovative Use of Natural Language Processing for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>3</volume>
<issue>14</issue>
<contexts>
<context position="12013" citStr="Porter (1980)" startWordPosition="1967" endWordPosition="1968">r than 100 times were excluded. Words occurring at least 100 times constituted the vocabulary used for computing the OOV features. The OOV and frequency features only fired for words that consisted only of letters, and the frequency features did not fire for OOV words. The set of negation words including the following: “no”, “not”, “never”, and “n’t”. The stopword list contained 158 common words and punctuation symbols. 531 Edits Feature Name Description - Intercept Always 1 (and not normalized by text lengths) T Stemming The number of times that two words with the same stem, according to the Porter (1980) stemmer, were aligned. Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum, 1998), were aligned. Sh Shift The number of shifts. P Paraphrase1 The number of phrasal paraphrasing operations. P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover et al. (2009a) for further explanation. P Paraphrase3 The sum of pq, where p and q are as above. P Paraphrase4 The sum of q, where q is as above. I Insertion The number of insertions. D D</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 3(14):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Pradhan</author>
<author>N Xue</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2009</date>
<booktitle>In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts,</booktitle>
<pages>11--12</pages>
<contexts>
<context position="4686" citStr="Pradhan and Xue, 2009" startWordPosition="731" endWordPosition="734">. The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). For each of these sources, approximately 750 sentence pairs x1 and x2 and gold standard similarity values y were provided for training and development. In addition, there were two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other</context>
</contexts>
<marker>Pradhan, Xue, 2009</marker>
<rawString>S. S. Pradhan and N. Xue. 2009. OntoNotes: The 90% solution. In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts, pages 11–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
<author>G A Sanders</author>
</authors>
<title>metrics for machine translation challenge - overview, methodology, metrics, and results.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<booktitle>The NIST</booktitle>
<pages>23--2</pages>
<contexts>
<context position="1278" citStr="Przybocki et al., 2009" startWordPosition="185" endWordPosition="188">ese additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we describe a system for measuring the semantic similarity of p</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, Sanders, 2009</marker>
<rawString>M. A. Przybocki, K. Peterson, S. Bronsart, and G. A. Sanders. 2009. The NIST 2008 metrics for machine translation challenge - overview, methodology, metrics, and results. Machine Translation, 23(2-3):71– 103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rosenblatt</author>
</authors>
<title>The perceptron: A probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<contexts>
<context position="3197" citStr="Rosenblatt, 1958" startWordPosition="499" endWordPosition="500">eedy learning algorithm. For example, there is a single feature for lexical substitution, even though it is clear that different types of substitutions have different effects on similarity (e.g., substituting “43.6” with “17” versus substituting “a” for “an”). In addition, the heuristic learning algorithm, which involves perturbing the weight vector by small amounts as in grid search, seems unscalable to larger sets of overlapping features. Therefore, here, we use TERp’s inference algorithms that find low cost edit sequences but use a discriminative learning algorithm based on the Perceptron (Rosenblatt, 1958; Collins, 2002) to estimate edit cost parameters, along with an expanded feature set for broader coverage of the phenomena that are relevant to sentence-to-sentence similarity. We 529 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 529–535, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics refer to this new approach as Paraphrase Edit Rate with the Perceptron (PERP). In addition to describing PERP, we discuss how it was applied for the SemEval 2012 Semantic Textual Similarity (STS) task. 2 Problem Definition In this work, our goal </context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>F. Rosenblatt. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A study of Translation Edit Rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="5172" citStr="Snover et al., 2006" startWordPosition="807" endWordPosition="810">e two surprise data sources revealed shortly before the submission deadline: pairs of sentences from Ontonotes (Pradhan and Xue, 2009) and Wordnet (Fellbaum, 1998) (OnWN), and machine translations of sentences from news conversations (SMT-news). For all five sources, the held-out test set contained several hundred text pairs. See the task description (Agirre et al., 2012) for additional details. 3 TER, TERp, and PERP In this section, we briefly describe the TER and TERp machine translation metrics, and how the PERP system extends them in order to better model semantic textual similarity. TER (Snover et al., 2006) uses a greedy search algorithm to find a set of edits to convert one of the paired input sentences into the other. We can view this set of edits as an alignment a between the two input sentences x1 and x2, and when two words in x1 and x2, respectively, are part of an edit operation, we say that those words are aligned.1 Unlike tradi1For machine translation evaluation with TERp and PERP, x1 is a system’s hypothesis and x2 is a reference translation. For tional edit distance measures, TER allow for shifts— that is, edits that change the positions of words or phrases in the input sentence x1. Es</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of Translation Edit Rate with targeted human annotation. In Proc. of the Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proc. of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-2009),</booktitle>
<contexts>
<context position="1299" citStr="Snover et al., 2009" startWordPosition="189" endWordPosition="192"> applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we describe a system for measuring the semantic similarity of pairs of short texts. </context>
<context position="6135" citStr="Snover et al., 2009" startWordPosition="985" endWordPosition="988">ine translation evaluation with TERp and PERP, x1 is a system’s hypothesis and x2 is a reference translation. For tional edit distance measures, TER allow for shifts— that is, edits that change the positions of words or phrases in the input sentence x1. Essentially, TER searches among a set of possible shifts of the phrases in x1 to find a set of shifts that result in the least cost alignment, using edits of other types, between x2 and the shifted version of x1. TER allows one to specify costs for different edit types, but it does not include a method for learning those costs from data. TERp (Snover et al., 2009b; Snover et al., 2009a) extends TER in two key ways. First, TERp includes new types of edits, including edits for substitution of synonyms, word stems, and phrasal paraphrases extracted from a pivot-based paraphrase table (§3.1). Second, it includes a heuristic learning algorithm for inferring cost parameters from labeled data. TERp includes 8 types of edits: match (M), insertion (I), deletion (D), substitution (S), stemming (T), synonymy (Y), shift (Sh), and phrase substitution (P). The edits are mutually exclusive, such that synonymy edits do not count as substitutions, for example. TERp ha</context>
<context position="8775" citStr="Snover et al. (2009" startWordPosition="1434" endWordPosition="1437">f 4.9 percent . x1 x2 the research insert insert insert year a 4.9 percent . Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus. The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match individual words are not shown. reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrase table used in PERP was identical to the one used by Snover et al. (2009a). It was extracted using the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify phrasal correspondences between English and a given foreign language F, then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they </context>
<context position="12438" citStr="Snover et al. (2009" startWordPosition="2037" endWordPosition="2040">ols. 531 Edits Feature Name Description - Intercept Always 1 (and not normalized by text lengths) T Stemming The number of times that two words with the same stem, according to the Porter (1980) stemmer, were aligned. Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum, 1998), were aligned. Sh Shift The number of shifts. P Paraphrase1 The number of phrasal paraphrasing operations. P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover et al. (2009a) for further explanation. P Paraphrase3 The sum of pq, where p and q are as above. P Paraphrase4 The sum of q, where q is as above. I Insertion The number of insertions. D Deletion The number of deletions. I, D Insert-Delete- The sum of log10 freq(w) over all insertions and deletions, where w is the word LogFreq being inserted or deleted and freq(w) is the relative frequency of w. I, D Insert-Delete- The sum of log10 length(w) over all insertions and deletions, where w is the word LogWordLen being inserted or deleted. I, D Insert-Delete- The number of insertions and deletions of X in alignme</context>
<context position="18174" citStr="Snover et al. (2009" startWordPosition="3011" endWordPosition="3014"> task included results from two variations, one using the full system (PERPphrases) and one with the paraphrase substitution edits disabled (PERP), in order to isolate the effect of including phrasal paraphrases. In our original submission, the PERPphrases system included a minor bug that affected the calculation of the phrasal paraphrasing features. Here, we report both the original results and a corrected version (“PERPphrases (fix)”), though the correction only minimally affected performance. We also tested two variations of the original TERp system: one with the weights set as reported by Snover et al. (2009a) (“TERp (default)”), and one tuned in the same task-specific manner as PERP (“TERp (tuned)”). We multiplied TERp’s predictions by −1 since it produces costs rather than similarities. The results, in terms of Pearson correlations with test set gold standard scores, are shown in Table 2. In addition to correlations for each subtask, we include the three aggregated measures used for the task. The “ALL” measure is the Pearson correlations on the concatenation of all the data for all five subtasks. It was the original measured used to aggregate the results for the different subtasks. The second a</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009a. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proc. of the Fourth Workshop on Statistical Machine Translation at the 12th Meeting of the European Chapter of the Association for Computational Linguistics (EACL-2009), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>N Madnani</author>
<author>B Dorr</author>
<author>R Schwartz</author>
</authors>
<title>TER-Plus: Paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="1299" citStr="Snover et al., 2009" startWordPosition="189" endWordPosition="192"> applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we describe a system for measuring the semantic similarity of pairs of short texts. </context>
<context position="6135" citStr="Snover et al., 2009" startWordPosition="985" endWordPosition="988">ine translation evaluation with TERp and PERP, x1 is a system’s hypothesis and x2 is a reference translation. For tional edit distance measures, TER allow for shifts— that is, edits that change the positions of words or phrases in the input sentence x1. Essentially, TER searches among a set of possible shifts of the phrases in x1 to find a set of shifts that result in the least cost alignment, using edits of other types, between x2 and the shifted version of x1. TER allows one to specify costs for different edit types, but it does not include a method for learning those costs from data. TERp (Snover et al., 2009b; Snover et al., 2009a) extends TER in two key ways. First, TERp includes new types of edits, including edits for substitution of synonyms, word stems, and phrasal paraphrases extracted from a pivot-based paraphrase table (§3.1). Second, it includes a heuristic learning algorithm for inferring cost parameters from labeled data. TERp includes 8 types of edits: match (M), insertion (I), deletion (D), substitution (S), stemming (T), synonymy (Y), shift (Sh), and phrase substitution (P). The edits are mutually exclusive, such that synonymy edits do not count as substitutions, for example. TERp ha</context>
<context position="8775" citStr="Snover et al. (2009" startWordPosition="1434" endWordPosition="1437">f 4.9 percent . x1 x2 the research insert insert insert year a 4.9 percent . Figure 1: An example of a PERP alignment for a sentence pair from the Microsoft Research Paraphrase Corpus. The search algorithm first performs shifts on x1 and then performs other edits on x2. The zero cost edits that match individual words are not shown. reference. It does so by looking up—in a precomputed phrase table—paraphrases of phrases in the reference and using its associated edit cost as the cost of performing a match against the hypothesis. The paraphrase table used in PERP was identical to the one used by Snover et al. (2009a). It was extracted using the pivot-based method as described by Bannard and Callison-Burch (2005) with several additional filtering mechanisms to increase the precision of the extracted pairs. The pivot-based method utilizes the inherent monolingual semantic knowledge from bilingual corpora: we first identify phrasal correspondences between English and a given foreign language F, then map from English to English by following translation units from English to the other language and back. For example, if the two English phrases e1 and e2 both correspond to the same foreign phrase f, then they </context>
<context position="12438" citStr="Snover et al. (2009" startWordPosition="2037" endWordPosition="2040">ols. 531 Edits Feature Name Description - Intercept Always 1 (and not normalized by text lengths) T Stemming The number of times that two words with the same stem, according to the Porter (1980) stemmer, were aligned. Y Synonymy The number of times that a pair of synonyms, according to WordNet (Fellbaum, 1998), were aligned. Sh Shift The number of shifts. P Paraphrase1 The number of phrasal paraphrasing operations. P Paraphrase2 The sum of q log10(p), where p is the probability in the pivot-based paraphrase table for a paraphrase edit and q is the number of edits for that paraphrase edit. See Snover et al. (2009a) for further explanation. P Paraphrase3 The sum of pq, where p and q are as above. P Paraphrase4 The sum of q, where q is as above. I Insertion The number of insertions. D Deletion The number of deletions. I, D Insert-Delete- The sum of log10 freq(w) over all insertions and deletions, where w is the word LogFreq being inserted or deleted and freq(w) is the relative frequency of w. I, D Insert-Delete- The sum of log10 length(w) over all insertions and deletions, where w is the word LogWordLen being inserted or deleted. I, D Insert-Delete- The number of insertions and deletions of X in alignme</context>
<context position="18174" citStr="Snover et al. (2009" startWordPosition="3011" endWordPosition="3014"> task included results from two variations, one using the full system (PERPphrases) and one with the paraphrase substitution edits disabled (PERP), in order to isolate the effect of including phrasal paraphrases. In our original submission, the PERPphrases system included a minor bug that affected the calculation of the phrasal paraphrasing features. Here, we report both the original results and a corrected version (“PERPphrases (fix)”), though the correction only minimally affected performance. We also tested two variations of the original TERp system: one with the weights set as reported by Snover et al. (2009a) (“TERp (default)”), and one tuned in the same task-specific manner as PERP (“TERp (tuned)”). We multiplied TERp’s predictions by −1 since it produces costs rather than similarities. The results, in terms of Pearson correlations with test set gold standard scores, are shown in Table 2. In addition to correlations for each subtask, we include the three aggregated measures used for the task. The “ALL” measure is the Pearson correlations on the concatenation of all the data for all five subtasks. It was the original measured used to aggregate the results for the different subtasks. The second a</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009b. TER-Plus: Paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation, 23(2–3):117–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of of EMNLP.</booktitle>
<contexts>
<context position="1222" citStr="Wang et al., 2007" startWordPosition="178" endWordPosition="181">implementing a discriminative learning algorithm. These additions facilitate applications of our system, called PERP, to similarity tasks other than machine translation evaluation, such as paraphrase recognition. In the SemEval 2012 Semantic Textual Similarity task, PERP performed competitively, particularly at the two surprise subtasks revealed shortly before the submission deadline. 1 Introduction Techniques for measuring the similarity of two sentences have various potential applications: automated short answer scoring (Nielsen et al., 2008; Leacock and Chodorow, 2003), question answering (Wang et al., 2007), machine translation evaluation (Przybocki et al., 2009; Snover et al., 2009a), etc. An important aspect of this problem is that similarity is not binary. Sentences can be very semantically similar, such that they might be called paraphrases of each other. They might be completely different. Or, they might be somewhere in between. Indeed, it is arguable that all sentence pairs (except exact duplicates) lie somewhere on a continuum of similarity. Therefore, it is desirable to develop methods that model sentence pair similarity on a continuous, or at least ordinal, scale. In this paper, we desc</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for QA. In Proc. of of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>