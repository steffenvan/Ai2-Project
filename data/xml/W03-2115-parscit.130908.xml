<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.956538">
Ontology-based Contextual Coherence Scoring
</title>
<author confidence="0.757566">
Robert Porzel Iryna Gurevych Christof E. M¨uller
</author>
<affiliation confidence="0.665626">
European Media Laboratory
</affiliation>
<address confidence="0.924305">
Schloss-Wolfsbrunnenweg 31c
D-69118 Heidelberg
</address>
<email confidence="0.99935">
{porzel,gurevych,mueller2@eml.org}
</email>
<sectionHeader confidence="0.996676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999712318181818">
In this paper we present a contextual ex-
tension to ONTOSCORE, a system for
scoring sets of concepts on the basis of
an ontology. We apply the contextually
enhanced system to the task of scoring
alternative speech recognition hypothe-
ses (SRH) in terms of their semantic co-
herence. We conducted several annota-
tion experiments and showed that human
annotators can reliably differentiate be-
tween semantically coherent and incoher-
ent speech recognition hypotheses (both
with and without discourse context). We
also showed, that annotators can reliably
identify the overall best hypothesis from
a given n-best list. While the original
ONTOSCORE system correctly assigns the
highest score to 84.06% of the corpus,
the inclusion of the conceptual context in-
creases the number of correct classifica-
tions to yield 86.76%, given a baseline of
63.91% in both cases.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999394911111111">
Following Allen et al. (2001), we can distinguish
between controlled and conversational dialogue sys-
tems. Since controlled and restricted interactions
between the user and the system increase recogni-
tion and understanding accuracy, such systems are
reliable enough to be deployed in various real world
applications, e.g. public transportation or cinema in-
formation systems. The more conversational a dia-
logue system becomes, the less predictable are the
users’ utterances. Recognition and processing be-
come increasingly difficult and unreliable.
Today’s dialogue systems employ domain- and
discourse-specific knowledge bases, so-called on-
tologies, to represent the individual discourse enti-
ties as concepts as well as their relations to each
other. In this paper we employ an algorithm for mea-
suring the semantic coherence of sets of concepts us-
ing such an ontology and show how its performance
can be improved by means of an inclusion of the
conceptual context. Thereby creating a method for
scoring the contextual coherence of individual sets
of concepts.
In the following, we will show how the contex-
tual coherence measurement can be applied to esti-
mate how well a given speech recognition hypoth-
esis (SRH) fits with respect to the existing knowl-
edge representation and the given conceptual con-
text, thereby providing a mechanism that increases
the robustness and reliability of dialogue systems.
We can, therefore, show how the algorithm can be
successfully employed by a spoken dialogue system
to enhance the interface between automatic speech
recognition (ASR) and natural language understand-
ing (NLU).
In Section 2 we discuss the problem of scoring
and classifying SRHs in terms of their semantic co-
herence followed by a description of our annotation
experiments and the corresponding results in Sec-
tion 3. Section 4 contains a description of the kind
of knowledge representations and the algorithm em-
ployed by ONTOSCORE. In Section 5 we present the
contextually enhanced system. Evaluations of the
corresponding system for scoring SRHs are given in
Section 6. A conclusion and additional applications
are given in Section 7.
</bodyText>
<sectionHeader confidence="0.9029645" genericHeader="introduction">
2 Semantic Coherence and Speech
Recognition Hypotheses
</sectionHeader>
<bodyText confidence="0.9996003">
While a simple one-best hypothesis interface be-
tween ASR and NLU suffices for restricted dialogue
systems, more complex systems either operate on n-
best lists as ASR output or convert ASR word graphs
(Oerder and Ney, 1993) into n-best lists, given the
distribution of acoustic and language model scores
(Schwartz and Chow, 1990; Tran et al., 1996). For
example, in our data a user expressed the wish to get
from Cologne to Heidelberg and then to continue his
visit in Heidelberg, as:1
</bodyText>
<figure confidence="0.338675833333333">
(1) ich m¨ochte auf dem schnellsten Weg
I want on the fastest way
von K¨oln nach Heidelberg.
from Cologne to Heidelberg.
(2) wie komme ich in Heidelberg weiter.
how can I in Heidelberg continue.
</figure>
<bodyText confidence="0.692116">
Looking at the SRHs from the ensuing n-best list of
Example (1) we found that Example (1a) constituted
the best representation of the utterance, whereas
all others constituted less adequate representations
thereof.
</bodyText>
<listItem confidence="0.9867442">
(1c) ich m¨ochte Folk Weg von K¨oln
I want folk way from Cologne
nach Heidelberg.
to Heidelberg.
(1d) ich m¨ochte auf schnellsten Weg vor
</listItem>
<bodyText confidence="0.359949">
I want on fastest way before
Heidelberg.
Heidelberg.
</bodyText>
<footnote confidence="0.9650765">
1All examples are displayed with the German original on top
and a glossed translation below.
</footnote>
<bodyText confidence="0.999794314285714">
Facing multiple representations of a single utter-
ance consequently poses the question, which of the
different hypotheses corresponds most likely to the
user’s utterance. Several ways of solving this prob-
lem have been proposed and implemented in var-
ious systems. Frequently the scores provided by
the ASR system itself are used, e.g. acoustic and
language model probabilities. More recently also
scores provided by the NLU system have been em-
ployed, e.g. parsing scores (Engel, 2002) or dis-
course model scores (Pfleger et al., 2002). However,
these methods often assign very high scores to SRHs
which are semantically incoherent and low scores to
semantically coherent ones.
In the case of Example (1) all scores, i.e. the
acoustic, language model, parsing and the ON-
TOSCORE scores assign the highest score to Exam-
ple (1a) (see Table 2 for the actual numbers). SRH
1a can consequently be chosen as the best SRH.
As we will show in Section 6, the scoring of the
SRHs from Example (2) differs substantially, and
only the contextual coherence score manages to pick
an adequate SRH. The fact that neither of the other
scoring approaches systematically employs the sys-
tem’s knowledge of the domains at hand, can re-
sult in passing suboptimal SRHs through the system.
This means that, while there was a better represen-
tation of the actual utterance in the n-best list, the
NLU system is processing an inferior one, thereby
causing overall dialogue metrics, in the sense of
Walker et al. (2000), to decrease. We, therefore,
propose an alternative way to rank SRHs on the ba-
sis of their contextual coherence, i.e. with respect
to a given ontology representing the domains of the
system and the given conceptual context.
</bodyText>
<sectionHeader confidence="0.979072" genericHeader="method">
3 Annotation Experiments
</sectionHeader>
<bodyText confidence="0.99854575">
The experiments reported here are based on the
data collected in hidden-operator tests where sub-
jects were prompted to say certain inputs. We ob-
tained 232 dialogues, which were divided into 1479
</bodyText>
<figure confidence="0.997645176470588">
(1a) ich
I
m¨ochte auf
want on
schnellsten
fastest
Weg von
way from
K¨oln nach Heidelberg.
Cologne to Heidelberg.
(1b) ich
I
nach
to
m¨ochte auf
want on
Heidelberg.
Heidelberg.
schnellsten
fastest
Weg K¨oln
way Cologne
K¨oln nach
Cologne to
K¨oln nach Heidelberg.
Cologne to Heidelberg.
(1e) ich
I
m¨ochte vor
want before
schnellsten
fastest
Weg von
way from
</figure>
<bodyText confidence="0.999172419354839">
audio files with single user utterances. Each ut-
terance corresponded to a single intention, e.g. a
route- or a sight information request. Firstly, all ut-
terances were also transcribed. Then the audio files
were sent to the speech recognizer. We logged the
speech recognition output, i.e. n-best lists of SRHs
for all utterances. A subset of the corpus was used to
log also the scores of the recognizer, parser and that
of OntoScore - including context-independent and
context-dependent semantic coherence scores. This
trial resulted in a sub-corpus of 552 utterances cor-
responding to 1,375 SRHs along with the respective
confidence scores.
We, then, conducted several annotation experi-
ments with a two-fold motivation. In the first place,
it was necessary to produce a hand-annotated corpus
to be used as a gold standard for the evaluation of
the contextual coherence scores. Furthermore, we
wanted to test whether human subjects were able
to annotate the data reliably according to our anno-
tation schemata. We had two annotators specially
trained for each of these particular annotation tasks.
In an earlier annotation experiment reported in
Gurevych et al. (2002), the task of annotators was
to classify a subset of the corpus of SRHs as either
coherent or incoherent. Here we randomly mixed
SRHs in order to avoid contextual priming.2 In the
first new experiment, a sub-corpus of 552 utterances
was annotated within the discourse context, i.e. the
SRHs were presented in their original dialogue or-
der. For each SRH, a decision again had to be made
whether it is semantically coherent or incoherent
with respect to the best SRH representing the previ-
ous user utterance. Given a total of 1,375 markables,
the annotators reached an agreement of 79.71%, i.e.
1,096 markables.
In the second new annotation experiment, the an-
notators saw the SRHs together with the transcribed
user utterances. The task of annotators was to deter-
mine the best SRH from the n-best list of SRHs cor-
responding to a single user utterance. The decision
had to be made on the basis of several criteria. The
most important criteria was how well the SRH cap-
tures the intentional content of the user’s utterance.
2As reported elsewhere the resulting Kappa statistics (Car-
letta, 1996) over the annotated data yields κ = 0.7, which in-
dicates that human annotators can reliably distinguish between
coherent samples and incoherent ones.
If none of the SRHs captured the user’s intention ad-
equately, the decision had to be made by looking at
the actual word error rate. In this experiment the
inter-annotator agreement was 90.69%, i.e. 1,247
markables out of 1,375.3 Each corpus was then tran-
formed into an evaluation gold standard by means of
the annotators agreeing on a single solution for the
cases of disagreement.
The aim of the work presented here, then, was to
provide a knowledge-based score, that can be em-
ployed by any NLU system to select the best hypoth-
esis from a given n-best list. The corresponding ON-
TOSCORE system will be described below, followed
by its evaluation against the human gold standards.
</bodyText>
<sectionHeader confidence="0.968163" genericHeader="method">
4 The Knowledge Base and OntoScore
</sectionHeader>
<bodyText confidence="0.998807186666666">
In this section, we provide a description of the
underlying algorithm and knowledge sources em-
ployed by the original ONTOSCORE system (in
press). It is important to note that the ontology
employed in this and the previous evaluations ex-
isted already and was crafted as a general knowl-
edge representation for various processing modules
within the system.4 Ontologies have traditionally
been used to represent general and domain specific
knowledge and are employed for various natural lan-
guage understanding tasks, e.g. semantic interpreta-
tion (Allen, 1987) and in spoken dialogue systems,
e.g. for discourse modeling, modality fusion and
dialogue management, see also Porzel et al. (2003)
for an overview. ONTOSCORE offers an additional
way of employing ontologies, i.e. to use the knowl-
edge modeled therein as the basis for evaluating
the semantic coherence of sets of concepts. It can
be employed independently of the specific ontology
language used, as the underlying algorithm oper-
ates only on the nodes and named edges of the di-
rected graph represented by the ontology. The spe-
cific knowledge base, e.g. written in DAML+OIL
3A Kappa-statistic suitable for measuring the reliability of
annotations is not possible in this case. The Kappa-statistic is
class-based and cannot, therefore, be applied to the best SRH
labeling, due to the different number of SRHs in the n-best lists.
Therefore, we calculated the percentage of utterances, where
the annotators agreed on the best SRH.
4Alternative knowledge representations, such as WORD-
NET, could have been employed in theory as well, however
most of the modern domains of the system, e.g. electronic me-
dia or program guides, are not covered by WORDNET.
or OWL,5 is converted into a graph, consisting of
the class hierarchy, with each class corresponding to
a concept representing either an entity or a process
and their slots, i.e. the named edges of the graph cor-
responding to the class properties, constraints and
restrictions.
The ontology employed for the evaluation has
about 730 concepts and 200 relations. It includes
a generic top-level ontology whose purpose is to
provide a basic structure of the world, i.e. abstract
classes to divide the universe in distinct parts as re-
sulting from the ontological analysis.6 The model-
ing of Processes and Physical Objects as a kind of
event that is continuous and homogeneous in nature,
follows the frame semantic analysis used for gener-
ating the FRAMENET data (Baker et al., 1998). The
hierarchy of Processes is connected to the hierarchy
of Physical Objects via slot-constraint definitions.
See also (Gurevych et al., 2003b) for a further de-
scription of the ontology.
ONTOSCORE performs a number of processing
steps. A first preprocessing step is to convert each
SRH into a concept representation (CR). For that
purpose we augmented the system’s lexicon with
specific concept mappings. That is, for each entry in
the lexicon either zero, one or many corresponding
concepts where added. A simple vector of concepts -
corresponding to the words in the SRH for which en-
tries in the lexicon exist - constitutes each resulting
CR. All other words with empty concept mappings,
e.g. articles and aspectual markers, are ignored in
the conversion. Due to lexical ambiguity, i.e. the
one to many word - concept mappings, this process-
ing step yields a set I = {CR1, CR2, ... , CRn} of
possible interpretations for each SRH.
ONTOSCORE converts the domain model, i.e. an
ontology, into a directed graph with concepts as
nodes and relations as edges. In order to find the
shortest path between two concepts, ONTOSCORE
employs the single source shortest path algorithm
of Dijkstra (Cormen et al., 1990). Thus, the minimal
paths connecting a given concept ci with every other
</bodyText>
<footnote confidence="0.994811">
5DAML+OIL and OWL are frequently used knowl-
edge modeling languages originating in W3C and Seman-
tic Web projects. For more details, see www.w3c.org and
www.daml.org.
6The top-level was developed following the procedure out-
lined in Russell and Norvig (1995).
</footnote>
<bodyText confidence="0.999455885245902">
concept in CR (excluding ci itself) are selected, re-
sulting in an n x n matrix of the respective paths.
To score the minimal paths connecting
all concepts with each other in a given
CR, we adopted a method proposed by
Demetriou and Atwell (1994) to score the se-
mantic coherence of alternative sentence inter-
pretations against graphs based on the Longman
Dictionary of Contemporary English (LDOCE).
As defined by Demetriou and Atwell (1994),
R = {r1, r2, ... , rn} is the set of direct relations
(both isa and semantic relations) that can connect
two nodes (concepts); and W = {w1, w2, ... , wn}
is the set of corresponding weights, where the
weight of each isa relation is set to 0 and that of
each other relation to 1.
The algorithm selects from the set of all paths
between two concepts the one with the smallest
weight, i.e. the cheapest. The distances between all
concept pairs in CR are summed up to a total score.
The set of concepts with the lowest aggregate score
represents the combination with the highest seman-
tic relatedness. The ensuing distance between two
concepts, e.g. D(ci, cj) is, then, defined as the min-
imum score derived between ci and cj.
Demetriou and Atwell (1994) do not provide con-
crete evaluation results for the method. Also, their
algorithm only allows for a relative judgment stat-
ing which of a set of interpretations given a single
sentence is more semantically related.
Since our objective is to compute coherence
scores of arbitrary CRs on an absolute scale, certain
extensions were necessary. In this application the
CRs to be scored can differ in terms of their content,
the number of concepts contained therein and their
mappings to the original SRH. Moreover, in order to
achieve absolute values, the final score should be re-
lated to the number of concepts in an individual set
and the number of words in the original SRH. There-
fore, the results must be normalized in order to allow
for evaluation, comparability and clearer interpreta-
tion of the semantic coherence scores.
We modified the algorithm described above to
make it applicable and evaluatable with respect to
the task at hand as well as other possible tasks. The
basic idea is to calculate a score based on the path
distances in CR. Since short distances indicate co-
herence and many concept pairs in a given CR may
have no connecting path, we define the distance be-
tween two concepts ci and cj that are not connected
in the knowledge base as Dmax. This maximum
value can also serve as a maximum for long dis-
tances and can thus help to prune the search tree for
long paths. This constant has to be set according to
the structure of the knowledge base. For example,
employing the ontology described above, the max-
imum distance between two concepts does not ex-
ceed ten and we chose in that case Dmax = 10.
We can now define the semantic coherence score
for CR as the average path length between all con-
cept pairs in CR:
</bodyText>
<equation confidence="0.997256333333333">
�ci,cj∈CR,ci6=cj D(ci, cj)
5(CR) =
|CR|2 − |CR|
</equation>
<bodyText confidence="0.999898888888889">
Since the ontology is a directed graph, we have
|CR|2 − |CR |pairs of concepts with possible di-
rected connections, i.e., a path from concept ci to
concept cj may be completely different to that from
cj to ci or even be missing. As a symmetric alter-
native, we may want to consider a path from ci to cj
and a path from cj to ci to be semantically equivalent
and thus model every relation in a bidirectional way.
We can then compute a symmetric score 50(CR) as
</bodyText>
<equation confidence="0.996365">
�ci,cj∈CR,i&lt;j min(D(ci, cj)D(cj, ci))
50(CR) = 2
|CR|2 − |CR|
</equation>
<bodyText confidence="0.998060875">
ONTOSCORE implements both options. As the
ontology currently employed features mostly unidi-
rectional relations we chose the 50(CR) function for
the evaluation, i.e. only the best path D(ci, cj) be-
tween a given pair of concepts, regardless of the di-
rection, is taken into account. A detailed description
of the original system can be found in (Gurevych et
al., 2003a).
</bodyText>
<sectionHeader confidence="0.993048" genericHeader="method">
5 Contextual Coherence Scoring
</sectionHeader>
<bodyText confidence="0.999925666666667">
The contextually enhanced ONTOSCORE system
performs a number of additional processing steps,
each of them will be described below.
</bodyText>
<subsectionHeader confidence="0.7374395">
5.1 Scoring Conceptual Context
Representations
</subsectionHeader>
<bodyText confidence="0.940656806451613">
A necessary preprocessing step for the conceptual
context scoring of SRHs is to build a conceptual con-
text representation CR0(5RHn+1) resulting from a
pair of concept representations:
- a concept representation of the SRH to be
scored, i.e. CR(5RHn+1),
- and a concept representation of the preceding
utterance’s SRH, i.e. CR(5RHn).
For that purpose, the ONTOSCORE stores the best
concept representation from each dialogue turn as
CRbest(5RH). By the best CR we mean the in-
terpretation which received the highest score from
the ONTOSCORE system, from the list of alter-
native interpretations of the utterance. For ex-
ample CRbest for the utterance shown in Exam-
ple (1) is the CR of the SRH given in (1e), i.e.
{EmotionExperiencerSubjectProcess, Person, Two-
PointRelation, Route, Town, Town}.
To produce a conceptual context representation
for 5RHn+1, we build a union of each of its possible
interpretations I = {CR1, CR2, ... , CRn} with
the stored CRbest(5RHn) from the previous utter-
ance. This results in a contextually augmented new
set I0 = {CR01, CR02, ... , CR0n} representing pos-
sible conceptual context interpretations of 5RHn+1
as shown in Table 1.
I(5RHn +1) I0(5RHn+1)
CR1 U CRbest(5RHn) = CR01
CR2 U CRbest(5RHn) = CR02
... ... ...
CRn U CRbest(5RHn) = CR0n
</bodyText>
<tableCaption confidence="0.993566">
Table 1: Creating conceptual context representations
</tableCaption>
<bodyText confidence="0.999915222222222">
If, however, the calculated score of CRbest(5RHn)
is below a certain threshold, meaning that even the
best prior hypothesis is most likely not semanti-
cally coherent, then CRbest(5RHn) = {0}. See
Section 6.2 for the corresponding numbers with re-
spect to the coherent versus incoherent classifica-
tion. Thusly, only if CRbest(5RHn) is empty then
solely the concept representations of 5RHn+1 are
taken into account. This is, of course, also the case
at the first dialogue turn.
In order to score the alternative conceptual con-
text representations defined by I0(5RHn+1), the
formula for 50(CR) is employed. This means that
we calculate a conceptual context coherence score
50 for each conceptual context representation CR0.
We also perform an inverse linear transformation of
the scores resulting in numbers from 0 to 1, so that
higher scores indicate better contextual coherence.
</bodyText>
<subsectionHeader confidence="0.946664">
5.2 ONTOSCORE at Work
</subsectionHeader>
<bodyText confidence="0.984951">
CR{Genre}
CR0{Genre,
EmotionExperiencerSubjectProcess, Person,
TwoPointRelation, Route }
Looking at an example of ONTOSCORE at work, we
will examine the following discourse fragment con-
sisting of the two sequential utterances given in Ex-
ample (1) and (2). As shown in Table 2, in the case
of Example (1) all scores indicate the SRH given in
Example (1a) to be the best one.
</bodyText>
<table confidence="0.999169833333333">
SRH recognizer parser OntoScore
1a 1 1 .6
1b .74 .94 .6
1c .63 .94 .54
1d .78 .89 .54
1e .74 .88 .54
</table>
<tableCaption confidence="0.998924">
Table 2: The scores for the SRHs of Example (1).
</tableCaption>
<bodyText confidence="0.6858345">
Example (2) yields the following SRHs with the cor-
responding context-independent CRs and context-
</bodyText>
<sectionHeader confidence="0.098156" genericHeader="method">
dependent CR0s:
</sectionHeader>
<reference confidence="0.547183181818182">
2a Rennen Lied Comedy Show Heidelberg
Race song comedy show Heidelberg
weiter.
continue.
CR{MusicPiece, Genre, Genre, Town}
CR0{MusicPiece, Genre, Genre, Town,
EmotionExperiencerSubjectProcess,Person,
TwoPointRelation, Route }
weiter.
continue.
CR{MotionDirectedTransliterated, Person,
</reference>
<table confidence="0.863273083333333">
Town}
CR0{MotionDirectedTransliterated, Person,
Town, EmotionExperiencerSubjectProcess,
TwoPointRelation, Route }
Adding the conceptual context we get the results
shown in Table 3 for Example (2):
SRH recognizer parser OntoScore
2a 1 .25 .32
2b .52 .2 .48
2c .34 .2 .39
2d .35 .12 0
2e .52 .08 .71
</table>
<tableCaption confidence="0.814403">
Table 3: The scores for the SRHs of Example 2.
As evident from Table 3, CR0 corresponds to Ex-
</tableCaption>
<bodyText confidence="0.776522">
best
ample 2e. This means that 2e constitutes a more
contextually coherent concept structure than the al-
ternative SRHs. This SRH was also labeled both as
the best and as a coherent SRH by the annotators.
2e denn wie komme ich in Heidelberg
then how can I in Heidelberg
</bodyText>
<sectionHeader confidence="0.992586" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999003461538462">
The ONTOSCORE software runs as a module in the
SMARTKOM multi-modal and multi-domain spoken
dialogue system (Wahlster et al., 2001). The sys-
tem features the combination of speech and gesture
as its input and output modalities. The domains of
the system include cinema and TV program infor-
mation, home electronic device control as well as
mobile services for tourists, e.g. tour planning and
sights information.
ONTOSCORE operates on n-best lists of SRHs
produced by the language interpretation module out
of the ASR word graphs. It computes a numerical
ranking of alternative SRHs and thus provides an
</bodyText>
<footnote confidence="0.836388214285714">
2b denn wie Comedy Heidelberg weiter.
then how comedy Heidelberg continue.
CR{Genre, Town}
CR0{Genre, Town,
EmotionExperiencerSubjectProcess,Person,
TwoPointRelation, Route }
2c denn wie Comedy Show weiter.
then how comedy show continue.
CR{Genre, Genre}
CR0{Genre, Genre,
EmotionExperiencerSubjectProcess, Person,
TwoPointRelation, Route }
2d denn wie Comedy weiter.
then how comedy continue.
</footnote>
<bodyText confidence="0.999977944444445">
important aid to the spoken language understand-
ing component. More precisely, the task of ON-
TOSCORE in the system is to identify the best SRH
suitable for further processing and evaluate it in
terms of its contextual coherence against the domain
and discourse knowledge.
The ONTOSCORE module currently employs two
knowledge sources, an ontology (about 730 con-
cepts and 200 relations) and a lexicon (ca. 3.600
words) with word to concept mappings, covering the
respective domains of the system. The evaluation
of ONTOSCORE was carried out on a set of 95 di-
alogues. The resulting dataset contained 552 utter-
ances resulting in 1,375 SRHs, corresponding to an
average of 2.49 SRHs per user utterance. The corpus
had been annotated by humans subjects according to
two separate annotation schemata. The results of an-
notation experiments are reported in Section 3.
</bodyText>
<subsectionHeader confidence="0.997405">
6.1 Identifying the Best SRH
</subsectionHeader>
<bodyText confidence="0.9998874">
The task of ONTOSCORE in our multimodal dia-
logue system is to determine the best SRH from
the n-best list of SRHs corresponding to a given
user utterance. The baseline for this evaluation
was computed by adding the individual ratios of ut-
terance/SRHs - corresponding to the likelihood of
guessing the best one in each individual case - and
dividing it by the number of utterances - yielding the
overall likelihood of guessing the best one 63.91%.
The accuracy of ONTOSCORE on this task
amounts to 86.76%. This means that in 86.76%
of all cases the best SRH defined by the human
gold standard is among the best scored by the ON-
TOSCORE module. The ONTOSCORE module with-
out the conceptual context feature yields the accu-
racy of only 84.06% on the same task. This suggests
that the overall results in identifying the best SRH
in the speech recognizer output can by improved by
taking the knowledge of conceptual context into ac-
count.
</bodyText>
<subsectionHeader confidence="0.9820775">
6.2 Classifying the SRHs as Semantically
Coherent versus Incoherent
</subsectionHeader>
<bodyText confidence="0.999911962962963">
For this evaluation we used the same corpus, where
each SRH was labeled as being either semantically
coherent versus incoherent with respect to the previ-
ous discourse context. We defined a baseline based
on the majority class, i.e. coherent, in the corpus,
63.05%. In order to obtain a binary classification
into semantically coherent and incoherent SRHs, a
cutoff threshold must be set.
Employing a cutoff threshold of 0.44, we find that
the contextually enhanced ONTOSCORE system cor-
rectly classifies 70.98% of SRHs in the corpus. This
indicates the improvement of 7.93% over the base-
line. We also conducted the same classification ex-
periment with ONTOSCORE without using the con-
ceptual context feature. In this case we obtained
69.96% accuracy.
From these results we can conclude that the task
of an absolute classification of coherent versus inco-
herent is substantially more difficult than that of de-
termining the best SRH, both for human annotators
(see Section 3) and for ONTOSCORE. Both human
and the system’s reliability is lower in the coherent
versus incoherent classification task, which allows
to classify zero, one or multiple SRHs from one ut-
terance as coherent or incoherent. In both tasks,
however, ONTOSCORE’s performance mirrors and
approaches human performance.
</bodyText>
<sectionHeader confidence="0.971471" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999996047619048">
The contextually enhanced ONTOSCORE system
described herein automatically performs ontology-
based scoring of sets of concepts which constitute
an adequate and suitable representation of a speech
recognition hypothesis and the prior conceptual con-
text. This conceptual context is an analogous con-
ceptual representation of the previous user utterance.
To date, the algorithm has been implemented in a
software which is employed by a multi-domain spo-
ken dialogue system and applied to the task of scor-
ing n-best lists of SRH, thus producing a score ex-
pressing how well a given SRH fits within the do-
main model and the given discourse. In the evalu-
ation of our system we employed an ontology that
was not designed for this task, but already existed as
the system’s internal knowledge representation. As
shown above, the inclusion of the conceptual dis-
course context yields an improvement of almost 3%
as compared to the context-independent system.
As future work we will examine how the com-
putation of a contextual coherence score, i.e. how
well a given SRH fits within the domain model
with respect to the previous discourse, can be em-
ployed to detect domain changes in complex multi-
modal and multi-domain spoken dialogue systems.
As one would expect, a contextual coherence score
as described above actually decreases when the user
changed from one domain to another, which most
likely also accounts for a set of the actual misclassi-
fications. As a future enhancement we will integrate
and evaluate an automatic domain change detection
function, which, if activated, will cause the system
to employ the context-independent scoring function.
Currently, we are also investigating whether the pro-
posed method can be applied to scoring sets of po-
tential candidates for resolving the semantic inter-
pretation of ambiguous, polysemous and metonymic
language use (Porzel and Gurevych, 2003). Addi-
tionally, As ontology building is constly, we exam-
ine the feasibility to employ alternative knowledge
sources, that are generated automatically from cor-
pora, e.g. via self organizing maps.
</bodyText>
<sectionHeader confidence="0.998301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99996925">
There work described herein was conducted within
the SmartKom project partly funded by the German
ministry of Research and Technology under grant
01IL95I7 and by the Klaus Tschira Foundation.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878896103896">
James F. Allen, Georga Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional system. In Proceedings of Intelligent User In-
terfaces, pages 1–8, Santa Fe, NM.
James F. Allen. 1987. Natural Language Understanding.
Menlo Park, Cal.: Benjamin Cummings.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of COLING-ACL, Montreal, Canada.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
Thomas H. Cormen, Charles E. Leiserson, and Ronald R.
Rivest. 1990. Introduction to Algorithms. MIT press,
Cambridge, MA.
George Demetriou and Eric Atwell. 1994. A seman-
tic network for large vocabulary speech recognition.
In Lindsay Evett and Tony Rose, editors, Proceedings
of AISB workshop on Computational Linguistics for
Speech and Handwriting Recognition, University of
Leeds.
Ralf Engel. 2002. SPIN: Language understanding for
spoken dialogue systems using a production system
approach. In Proceedings ofICSLP 2002.
Iryna Gurevych, Robert Porzel, and Michael Strube.
2002. Annotating the semantic consistency of speech
recognition hypotheses. In Proceedings of the Third
SIGdial Workshop on Discourse and Dialogue, pages
46–49, Philadelphia, USA, July.
Iryna Gurevych, Rainer Malaka, Robert Porzel, and
Hans-Peter Zorn. 2003a. Semantic coherence scoring
using an ontology. In Proceedings of the HLT-NAACL
Conference. to appear.
Iryna Gurevych, Robert Porzel, Elena Slinko, Nor-
bert Pfleger, Jan Alexandersson, and Stefan Merten.
2003b. Less is more: Using a single knowledge rep-
resentation in dialogue systems. In Proceedings of the
HLT-NAACL’03 Workshop on Text Meaning, Edmon-
ton, Canada.
Martin Oerder and Hermann Ney. 1993. Word
graphs: An efficient interface between continuous-
speech recognition and language understanding. In
ICASSP Volume 2, pages 119–122.
Norbert Pfleger, Jan Alexandersson, and Tilman Becker.
2002. Scoring functions for overlay and their ap-
plication in discourse processing. In KONVENS-02,
Saarbr¨ucken, September – October.
Robert Porzel and Iryna Gurevych. 2003. Contextual
coherence in natural language processing. Modeling
and Using Context, Springer, LNCS:to appear.
Robert Porzel, Norbert Pfleger, Stefan Merten, Markus
L¨ockelt, Iryna Gurevych, Ralf Engel, and Jan Alexan-
dersson. 2003. More on less: Further applications of
ontologies in multi-modal dialogue systems. In Pro-
ceedings of the IJCAI’03 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems, page to ap-
pear.
Stuart J. Russell and Peter Norvig. 1995. Artificial In-
telligence. A Modern Approach. Prentice Hall, Engle-
wood Cliffs, N.J.
Richard Schwartz and Ye-Lo Chow. 1990. The n-best
algorithm: an efficient and exact procedure for finding
the n most likely sentence hypotheses. In Proceedings
ofICASSP’90, Albuquerque, USA.
Bach-Hiep Tran, Frank Seide, Volker Steinbiss, Richard
Schwartz, and Ye-Lo Chow. 1996. A word graph
based n-best search in continuous speech recognition.
In Proceedings ofICSLP’96.
Wolfgang Wahlster, Norbert Reithinger, and Anselm
Blocher. 2001. SmartKom: Multimodal communi-
cation with a life-like character. In Proceedings of the
7th European Conference on Speech Communication
and Technology., pages 1547–1550.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards developing general model of
usability with PARADISE. Natural Language Enge-
neering, 6.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.510746">
<title confidence="0.999846">Ontology-based Contextual Coherence Scoring</title>
<author confidence="0.999906">Robert Porzel Iryna Gurevych Christof E M¨uller</author>
<affiliation confidence="0.8462185">European Media Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.671081">D-69118</address>
<abstract confidence="0.994006608695652">In this paper we present a contextual exto a system for scoring sets of concepts on the basis of an ontology. We apply the contextually enhanced system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence. We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>2a Rennen Lied Comedy Show Heidelberg Race song comedy show Heidelberg weiter.</title>
<tech>continue.</tech>
<marker></marker>
<rawString>2a Rennen Lied Comedy Show Heidelberg Race song comedy show Heidelberg weiter. continue.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Genre CRMusicPiece</author>
<author>Genre</author>
</authors>
<location>Town} CR0{MusicPiece, Genre, Genre, Town, EmotionExperiencerSubjectProcess,Person, TwoPointRelation, Route }</location>
<marker>CRMusicPiece, Genre, </marker>
<rawString>CR{MusicPiece, Genre, Genre, Town} CR0{MusicPiece, Genre, Genre, Town, EmotionExperiencerSubjectProcess,Person, TwoPointRelation, Route }</rawString>
</citation>
<citation valid="false">
<authors>
<author>continue</author>
</authors>
<marker>continue, </marker>
<rawString>weiter. continue.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Person CRMotionDirectedTransliterated</author>
</authors>
<marker>CRMotionDirectedTransliterated, </marker>
<rawString>CR{MotionDirectedTransliterated, Person,</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Georga Ferguson</author>
<author>Amanda Stent</author>
</authors>
<title>An architecture for more realistic conversational system.</title>
<date>2001</date>
<booktitle>In Proceedings of Intelligent User Interfaces,</booktitle>
<pages>1--8</pages>
<location>Santa Fe, NM.</location>
<contexts>
<context position="1108" citStr="Allen et al. (2001)" startWordPosition="158" endWordPosition="161"> We conducted several annotation experiments and showed that human annotators can reliably differentiate between semantically coherent and incoherent speech recognition hypotheses (both with and without discourse context). We also showed, that annotators can reliably identify the overall best hypothesis from a given n-best list. While the original ONTOSCORE system correctly assigns the highest score to 84.06% of the corpus, the inclusion of the conceptual context increases the number of correct classifications to yield 86.76%, given a baseline of 63.91% in both cases. 1 Introduction Following Allen et al. (2001), we can distinguish between controlled and conversational dialogue systems. Since controlled and restricted interactions between the user and the system increase recognition and understanding accuracy, such systems are reliable enough to be deployed in various real world applications, e.g. public transportation or cinema information systems. The more conversational a dialogue system becomes, the less predictable are the users’ utterances. Recognition and processing become increasingly difficult and unreliable. Today’s dialogue systems employ domain- and discourse-specific knowledge bases, so-</context>
</contexts>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>James F. Allen, Georga Ferguson, and Amanda Stent. 2001. An architecture for more realistic conversational system. In Proceedings of Intelligent User Interfaces, pages 1–8, Santa Fe, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Natural Language Understanding.</title>
<date>1987</date>
<location>Menlo Park, Cal.: Benjamin Cummings.</location>
<contexts>
<context position="10408" citStr="Allen, 1987" startWordPosition="1664" endWordPosition="1665">human gold standards. 4 The Knowledge Base and OntoScore In this section, we provide a description of the underlying algorithm and knowledge sources employed by the original ONTOSCORE system (in press). It is important to note that the ontology employed in this and the previous evaluations existed already and was crafted as a general knowledge representation for various processing modules within the system.4 Ontologies have traditionally been used to represent general and domain specific knowledge and are employed for various natural language understanding tasks, e.g. semantic interpretation (Allen, 1987) and in spoken dialogue systems, e.g. for discourse modeling, modality fusion and dialogue management, see also Porzel et al. (2003) for an overview. ONTOSCORE offers an additional way of employing ontologies, i.e. to use the knowledge modeled therein as the basis for evaluating the semantic coherence of sets of concepts. It can be employed independently of the specific ontology language used, as the underlying algorithm operates only on the nodes and named edges of the directed graph represented by the ontology. The specific knowledge base, e.g. written in DAML+OIL 3A Kappa-statistic suitable</context>
</contexts>
<marker>Allen, 1987</marker>
<rawString>James F. Allen. 1987. Natural Language Understanding. Menlo Park, Cal.: Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="12331" citStr="Baker et al., 1998" startWordPosition="1974" endWordPosition="1977"> and their slots, i.e. the named edges of the graph corresponding to the class properties, constraints and restrictions. The ontology employed for the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. ONTOSCORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts - corresponding to the words in the SRH for which entries in the </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="8988" citStr="Carletta, 1996" startWordPosition="1432" endWordPosition="1434">ing the previous user utterance. Given a total of 1,375 markables, the annotators reached an agreement of 79.71%, i.e. 1,096 markables. In the second new annotation experiment, the annotators saw the SRHs together with the transcribed user utterances. The task of annotators was to determine the best SRH from the n-best list of SRHs corresponding to a single user utterance. The decision had to be made on the basis of several criteria. The most important criteria was how well the SRH captures the intentional content of the user’s utterance. 2As reported elsewhere the resulting Kappa statistics (Carletta, 1996) over the annotated data yields κ = 0.7, which indicates that human annotators can reliably distinguish between coherent samples and incoherent ones. If none of the SRHs captured the user’s intention adequately, the decision had to be made by looking at the actual word error rate. In this experiment the inter-annotator agreement was 90.69%, i.e. 1,247 markables out of 1,375.3 Each corpus was then tranformed into an evaluation gold standard by means of the annotators agreeing on a single solution for the cases of disagreement. The aim of the work presented here, then, was to provide a knowledge</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald R Rivest</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>1990</date>
<publisher>MIT press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="13539" citStr="Cormen et al., 1990" startWordPosition="2173" endWordPosition="2176">ries in the lexicon exist - constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored in the conversion. Due to lexical ambiguity, i.e. the one to many word - concept mappings, this processing step yields a set I = {CR1, CR2, ... , CRn} of possible interpretations for each SRH. ONTOSCORE converts the domain model, i.e. an ontology, into a directed graph with concepts as nodes and relations as edges. In order to find the shortest path between two concepts, ONTOSCORE employs the single source shortest path algorithm of Dijkstra (Cormen et al., 1990). Thus, the minimal paths connecting a given concept ci with every other 5DAML+OIL and OWL are frequently used knowledge modeling languages originating in W3C and Semantic Web projects. For more details, see www.w3c.org and www.daml.org. 6The top-level was developed following the procedure outlined in Russell and Norvig (1995). concept in CR (excluding ci itself) are selected, resulting in an n x n matrix of the respective paths. To score the minimal paths connecting all concepts with each other in a given CR, we adopted a method proposed by Demetriou and Atwell (1994) to score the semantic co</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, 1990</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, and Ronald R. Rivest. 1990. Introduction to Algorithms. MIT press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Demetriou</author>
<author>Eric Atwell</author>
</authors>
<title>A semantic network for large vocabulary speech recognition.</title>
<date>1994</date>
<booktitle>Proceedings of AISB workshop on Computational Linguistics for Speech and Handwriting Recognition,</booktitle>
<editor>In Lindsay Evett and Tony Rose, editors,</editor>
<location>University of Leeds.</location>
<contexts>
<context position="14114" citStr="Demetriou and Atwell (1994)" startWordPosition="2269" endWordPosition="2272">est path algorithm of Dijkstra (Cormen et al., 1990). Thus, the minimal paths connecting a given concept ci with every other 5DAML+OIL and OWL are frequently used knowledge modeling languages originating in W3C and Semantic Web projects. For more details, see www.w3c.org and www.daml.org. 6The top-level was developed following the procedure outlined in Russell and Norvig (1995). concept in CR (excluding ci itself) are selected, resulting in an n x n matrix of the respective paths. To score the minimal paths connecting all concepts with each other in a given CR, we adopted a method proposed by Demetriou and Atwell (1994) to score the semantic coherence of alternative sentence interpretations against graphs based on the Longman Dictionary of Contemporary English (LDOCE). As defined by Demetriou and Atwell (1994), R = {r1, r2, ... , rn} is the set of direct relations (both isa and semantic relations) that can connect two nodes (concepts); and W = {w1, w2, ... , wn} is the set of corresponding weights, where the weight of each isa relation is set to 0 and that of each other relation to 1. The algorithm selects from the set of all paths between two concepts the one with the smallest weight, i.e. the cheapest. The</context>
</contexts>
<marker>Demetriou, Atwell, 1994</marker>
<rawString>George Demetriou and Eric Atwell. 1994. A semantic network for large vocabulary speech recognition. In Lindsay Evett and Tony Rose, editors, Proceedings of AISB workshop on Computational Linguistics for Speech and Handwriting Recognition, University of Leeds.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Engel</author>
</authors>
<title>SPIN: Language understanding for spoken dialogue systems using a production system approach.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP</booktitle>
<contexts>
<context position="4927" citStr="Engel, 2002" startWordPosition="762" endWordPosition="763">t way before Heidelberg. Heidelberg. 1All examples are displayed with the German original on top and a glossed translation below. Facing multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Several ways of solving this problem have been proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores (Engel, 2002) or discourse model scores (Pfleger et al., 2002). However, these methods often assign very high scores to SRHs which are semantically incoherent and low scores to semantically coherent ones. In the case of Example (1) all scores, i.e. the acoustic, language model, parsing and the ONTOSCORE scores assign the highest score to Example (1a) (see Table 2 for the actual numbers). SRH 1a can consequently be chosen as the best SRH. As we will show in Section 6, the scoring of the SRHs from Example (2) differs substantially, and only the contextual coherence score manages to pick an adequate SRH. The </context>
</contexts>
<marker>Engel, 2002</marker>
<rawString>Ralf Engel. 2002. SPIN: Language understanding for spoken dialogue systems using a production system approach. In Proceedings ofICSLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
<author>Michael Strube</author>
</authors>
<title>Annotating the semantic consistency of speech recognition hypotheses.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>46--49</pages>
<location>Philadelphia, USA,</location>
<contexts>
<context position="7899" citStr="Gurevych et al. (2002)" startWordPosition="1247" endWordPosition="1250">orpus of 552 utterances corresponding to 1,375 SRHs along with the respective confidence scores. We, then, conducted several annotation experiments with a two-fold motivation. In the first place, it was necessary to produce a hand-annotated corpus to be used as a gold standard for the evaluation of the contextual coherence scores. Furthermore, we wanted to test whether human subjects were able to annotate the data reliably according to our annotation schemata. We had two annotators specially trained for each of these particular annotation tasks. In an earlier annotation experiment reported in Gurevych et al. (2002), the task of annotators was to classify a subset of the corpus of SRHs as either coherent or incoherent. Here we randomly mixed SRHs in order to avoid contextual priming.2 In the first new experiment, a sub-corpus of 552 utterances was annotated within the discourse context, i.e. the SRHs were presented in their original dialogue order. For each SRH, a decision again had to be made whether it is semantically coherent or incoherent with respect to the best SRH representing the previous user utterance. Given a total of 1,375 markables, the annotators reached an agreement of 79.71%, i.e. 1,096 m</context>
</contexts>
<marker>Gurevych, Porzel, Strube, 2002</marker>
<rawString>Iryna Gurevych, Robert Porzel, and Michael Strube. 2002. Annotating the semantic consistency of speech recognition hypotheses. In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, pages 46–49, Philadelphia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Rainer Malaka</author>
<author>Robert Porzel</author>
<author>Hans-Peter Zorn</author>
</authors>
<title>Semantic coherence scoring using an ontology.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL Conference.</booktitle>
<note>to appear.</note>
<contexts>
<context position="12474" citStr="Gurevych et al., 2003" startWordPosition="1995" endWordPosition="1998">d for the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. ONTOSCORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts - corresponding to the words in the SRH for which entries in the lexicon exist - constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored in</context>
<context position="17717" citStr="Gurevych et al., 2003" startWordPosition="2907" endWordPosition="2910">want to consider a path from ci to cj and a path from cj to ci to be semantically equivalent and thus model every relation in a bidirectional way. We can then compute a symmetric score 50(CR) as �ci,cj∈CR,i&lt;j min(D(ci, cj)D(cj, ci)) 50(CR) = 2 |CR|2 − |CR| ONTOSCORE implements both options. As the ontology currently employed features mostly unidirectional relations we chose the 50(CR) function for the evaluation, i.e. only the best path D(ci, cj) between a given pair of concepts, regardless of the direction, is taken into account. A detailed description of the original system can be found in (Gurevych et al., 2003a). 5 Contextual Coherence Scoring The contextually enhanced ONTOSCORE system performs a number of additional processing steps, each of them will be described below. 5.1 Scoring Conceptual Context Representations A necessary preprocessing step for the conceptual context scoring of SRHs is to build a conceptual context representation CR0(5RHn+1) resulting from a pair of concept representations: - a concept representation of the SRH to be scored, i.e. CR(5RHn+1), - and a concept representation of the preceding utterance’s SRH, i.e. CR(5RHn). For that purpose, the ONTOSCORE stores the best concep</context>
</contexts>
<marker>Gurevych, Malaka, Porzel, Zorn, 2003</marker>
<rawString>Iryna Gurevych, Rainer Malaka, Robert Porzel, and Hans-Peter Zorn. 2003a. Semantic coherence scoring using an ontology. In Proceedings of the HLT-NAACL Conference. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iryna Gurevych</author>
<author>Robert Porzel</author>
<author>Elena Slinko</author>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
<author>Stefan Merten</author>
</authors>
<title>Less is more: Using a single knowledge representation in dialogue systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL’03 Workshop on Text Meaning,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="12474" citStr="Gurevych et al., 2003" startWordPosition="1995" endWordPosition="1998">d for the evaluation has about 730 concepts and 200 relations. It includes a generic top-level ontology whose purpose is to provide a basic structure of the world, i.e. abstract classes to divide the universe in distinct parts as resulting from the ontological analysis.6 The modeling of Processes and Physical Objects as a kind of event that is continuous and homogeneous in nature, follows the frame semantic analysis used for generating the FRAMENET data (Baker et al., 1998). The hierarchy of Processes is connected to the hierarchy of Physical Objects via slot-constraint definitions. See also (Gurevych et al., 2003b) for a further description of the ontology. ONTOSCORE performs a number of processing steps. A first preprocessing step is to convert each SRH into a concept representation (CR). For that purpose we augmented the system’s lexicon with specific concept mappings. That is, for each entry in the lexicon either zero, one or many corresponding concepts where added. A simple vector of concepts - corresponding to the words in the SRH for which entries in the lexicon exist - constitutes each resulting CR. All other words with empty concept mappings, e.g. articles and aspectual markers, are ignored in</context>
<context position="17717" citStr="Gurevych et al., 2003" startWordPosition="2907" endWordPosition="2910">want to consider a path from ci to cj and a path from cj to ci to be semantically equivalent and thus model every relation in a bidirectional way. We can then compute a symmetric score 50(CR) as �ci,cj∈CR,i&lt;j min(D(ci, cj)D(cj, ci)) 50(CR) = 2 |CR|2 − |CR| ONTOSCORE implements both options. As the ontology currently employed features mostly unidirectional relations we chose the 50(CR) function for the evaluation, i.e. only the best path D(ci, cj) between a given pair of concepts, regardless of the direction, is taken into account. A detailed description of the original system can be found in (Gurevych et al., 2003a). 5 Contextual Coherence Scoring The contextually enhanced ONTOSCORE system performs a number of additional processing steps, each of them will be described below. 5.1 Scoring Conceptual Context Representations A necessary preprocessing step for the conceptual context scoring of SRHs is to build a conceptual context representation CR0(5RHn+1) resulting from a pair of concept representations: - a concept representation of the SRH to be scored, i.e. CR(5RHn+1), - and a concept representation of the preceding utterance’s SRH, i.e. CR(5RHn). For that purpose, the ONTOSCORE stores the best concep</context>
</contexts>
<marker>Gurevych, Porzel, Slinko, Pfleger, Alexandersson, Merten, 2003</marker>
<rawString>Iryna Gurevych, Robert Porzel, Elena Slinko, Norbert Pfleger, Jan Alexandersson, and Stefan Merten. 2003b. Less is more: Using a single knowledge representation in dialogue systems. In Proceedings of the HLT-NAACL’03 Workshop on Text Meaning, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Oerder</author>
<author>Hermann Ney</author>
</authors>
<title>Word graphs: An efficient interface between continuousspeech recognition and language understanding.</title>
<date>1993</date>
<booktitle>In ICASSP</booktitle>
<volume>2</volume>
<pages>119--122</pages>
<contexts>
<context position="3488" citStr="Oerder and Ney, 1993" startWordPosition="527" endWordPosition="530">ing results in Section 3. Section 4 contains a description of the kind of knowledge representations and the algorithm employed by ONTOSCORE. In Section 5 we present the contextually enhanced system. Evaluations of the corresponding system for scoring SRHs are given in Section 6. A conclusion and additional applications are given in Section 7. 2 Semantic Coherence and Speech Recognition Hypotheses While a simple one-best hypothesis interface between ASR and NLU suffices for restricted dialogue systems, more complex systems either operate on nbest lists as ASR output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to get from Cologne to Heidelberg and then to continue his visit in Heidelberg, as:1 (1) ich m¨ochte auf dem schnellsten Weg I want on the fastest way von K¨oln nach Heidelberg. from Cologne to Heidelberg. (2) wie komme ich in Heidelberg weiter. how can I in Heidelberg continue. Looking at the SRHs from the ensuing n-best list of Example (1) we found that Example (1a) constituted the best representation of the utteranc</context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Martin Oerder and Hermann Ney. 1993. Word graphs: An efficient interface between continuousspeech recognition and language understanding. In ICASSP Volume 2, pages 119–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Pfleger</author>
<author>Jan Alexandersson</author>
<author>Tilman Becker</author>
</authors>
<title>Scoring functions for overlay and their application in discourse processing.</title>
<date>2002</date>
<booktitle>In KONVENS-02, Saarbr¨ucken, September –</booktitle>
<contexts>
<context position="4976" citStr="Pfleger et al., 2002" startWordPosition="769" endWordPosition="772">l examples are displayed with the German original on top and a glossed translation below. Facing multiple representations of a single utterance consequently poses the question, which of the different hypotheses corresponds most likely to the user’s utterance. Several ways of solving this problem have been proposed and implemented in various systems. Frequently the scores provided by the ASR system itself are used, e.g. acoustic and language model probabilities. More recently also scores provided by the NLU system have been employed, e.g. parsing scores (Engel, 2002) or discourse model scores (Pfleger et al., 2002). However, these methods often assign very high scores to SRHs which are semantically incoherent and low scores to semantically coherent ones. In the case of Example (1) all scores, i.e. the acoustic, language model, parsing and the ONTOSCORE scores assign the highest score to Example (1a) (see Table 2 for the actual numbers). SRH 1a can consequently be chosen as the best SRH. As we will show in Section 6, the scoring of the SRHs from Example (2) differs substantially, and only the contextual coherence score manages to pick an adequate SRH. The fact that neither of the other scoring approaches</context>
</contexts>
<marker>Pfleger, Alexandersson, Becker, 2002</marker>
<rawString>Norbert Pfleger, Jan Alexandersson, and Tilman Becker. 2002. Scoring functions for overlay and their application in discourse processing. In KONVENS-02, Saarbr¨ucken, September – October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Porzel</author>
<author>Iryna Gurevych</author>
</authors>
<title>Contextual coherence in natural language processing. Modeling and Using Context, Springer,</title>
<date>2003</date>
<note>LNCS:to appear.</note>
<marker>Porzel, Gurevych, 2003</marker>
<rawString>Robert Porzel and Iryna Gurevych. 2003. Contextual coherence in natural language processing. Modeling and Using Context, Springer, LNCS:to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Porzel</author>
<author>Norbert Pfleger</author>
<author>Stefan Merten</author>
<author>Markus L¨ockelt</author>
<author>Iryna Gurevych</author>
<author>Ralf Engel</author>
<author>Jan Alexandersson</author>
</authors>
<title>More on less: Further applications of ontologies in multi-modal dialogue systems.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI’03 Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<pages>page</pages>
<note>to appear.</note>
<marker>Porzel, Pfleger, Merten, L¨ockelt, Gurevych, Engel, Alexandersson, 2003</marker>
<rawString>Robert Porzel, Norbert Pfleger, Stefan Merten, Markus L¨ockelt, Iryna Gurevych, Ralf Engel, and Jan Alexandersson. 2003. More on less: Further applications of ontologies in multi-modal dialogue systems. In Proceedings of the IJCAI’03 Workshop on Knowledge and Reasoning in Practical Dialogue Systems, page to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart J Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence. A Modern Approach.</title>
<date>1995</date>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, N.J.</location>
<contexts>
<context position="13867" citStr="Russell and Norvig (1995)" startWordPosition="2224" endWordPosition="2227">etations for each SRH. ONTOSCORE converts the domain model, i.e. an ontology, into a directed graph with concepts as nodes and relations as edges. In order to find the shortest path between two concepts, ONTOSCORE employs the single source shortest path algorithm of Dijkstra (Cormen et al., 1990). Thus, the minimal paths connecting a given concept ci with every other 5DAML+OIL and OWL are frequently used knowledge modeling languages originating in W3C and Semantic Web projects. For more details, see www.w3c.org and www.daml.org. 6The top-level was developed following the procedure outlined in Russell and Norvig (1995). concept in CR (excluding ci itself) are selected, resulting in an n x n matrix of the respective paths. To score the minimal paths connecting all concepts with each other in a given CR, we adopted a method proposed by Demetriou and Atwell (1994) to score the semantic coherence of alternative sentence interpretations against graphs based on the Longman Dictionary of Contemporary English (LDOCE). As defined by Demetriou and Atwell (1994), R = {r1, r2, ... , rn} is the set of direct relations (both isa and semantic relations) that can connect two nodes (concepts); and W = {w1, w2, ... , wn} is </context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>Stuart J. Russell and Peter Norvig. 1995. Artificial Intelligence. A Modern Approach. Prentice Hall, Englewood Cliffs, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Schwartz</author>
<author>Ye-Lo Chow</author>
</authors>
<title>The n-best algorithm: an efficient and exact procedure for finding the n most likely sentence hypotheses.</title>
<date>1990</date>
<booktitle>In Proceedings ofICASSP’90,</booktitle>
<location>Albuquerque, USA.</location>
<contexts>
<context position="3593" citStr="Schwartz and Chow, 1990" startWordPosition="543" endWordPosition="546"> the algorithm employed by ONTOSCORE. In Section 5 we present the contextually enhanced system. Evaluations of the corresponding system for scoring SRHs are given in Section 6. A conclusion and additional applications are given in Section 7. 2 Semantic Coherence and Speech Recognition Hypotheses While a simple one-best hypothesis interface between ASR and NLU suffices for restricted dialogue systems, more complex systems either operate on nbest lists as ASR output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to get from Cologne to Heidelberg and then to continue his visit in Heidelberg, as:1 (1) ich m¨ochte auf dem schnellsten Weg I want on the fastest way von K¨oln nach Heidelberg. from Cologne to Heidelberg. (2) wie komme ich in Heidelberg weiter. how can I in Heidelberg continue. Looking at the SRHs from the ensuing n-best list of Example (1) we found that Example (1a) constituted the best representation of the utterance, whereas all others constituted less adequate representations thereof. (1c) ich m¨ochte Folk Weg von K¨</context>
</contexts>
<marker>Schwartz, Chow, 1990</marker>
<rawString>Richard Schwartz and Ye-Lo Chow. 1990. The n-best algorithm: an efficient and exact procedure for finding the n most likely sentence hypotheses. In Proceedings ofICASSP’90, Albuquerque, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bach-Hiep Tran</author>
<author>Frank Seide</author>
<author>Volker Steinbiss</author>
<author>Richard Schwartz</author>
<author>Ye-Lo Chow</author>
</authors>
<title>A word graph based n-best search in continuous speech recognition.</title>
<date>1996</date>
<booktitle>In Proceedings ofICSLP’96.</booktitle>
<contexts>
<context position="3613" citStr="Tran et al., 1996" startWordPosition="547" endWordPosition="550">y ONTOSCORE. In Section 5 we present the contextually enhanced system. Evaluations of the corresponding system for scoring SRHs are given in Section 6. A conclusion and additional applications are given in Section 7. 2 Semantic Coherence and Speech Recognition Hypotheses While a simple one-best hypothesis interface between ASR and NLU suffices for restricted dialogue systems, more complex systems either operate on nbest lists as ASR output or convert ASR word graphs (Oerder and Ney, 1993) into n-best lists, given the distribution of acoustic and language model scores (Schwartz and Chow, 1990; Tran et al., 1996). For example, in our data a user expressed the wish to get from Cologne to Heidelberg and then to continue his visit in Heidelberg, as:1 (1) ich m¨ochte auf dem schnellsten Weg I want on the fastest way von K¨oln nach Heidelberg. from Cologne to Heidelberg. (2) wie komme ich in Heidelberg weiter. how can I in Heidelberg continue. Looking at the SRHs from the ensuing n-best list of Example (1) we found that Example (1a) constituted the best representation of the utterance, whereas all others constituted less adequate representations thereof. (1c) ich m¨ochte Folk Weg von K¨oln I want folk way </context>
</contexts>
<marker>Tran, Seide, Steinbiss, Schwartz, Chow, 1996</marker>
<rawString>Bach-Hiep Tran, Frank Seide, Volker Steinbiss, Richard Schwartz, and Ye-Lo Chow. 1996. A word graph based n-best search in continuous speech recognition. In Proceedings ofICSLP’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Norbert Reithinger</author>
<author>Anselm Blocher</author>
</authors>
<title>SmartKom: Multimodal communication with a life-like character.</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology.,</booktitle>
<pages>1547--1550</pages>
<marker>Wahlster, Reithinger, Blocher, 2001</marker>
<rawString>Wolfgang Wahlster, Norbert Reithinger, and Anselm Blocher. 2001. SmartKom: Multimodal communication with a life-like character. In Proceedings of the 7th European Conference on Speech Communication and Technology., pages 1547–1550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Candace A Kamm</author>
<author>Diane J Litman</author>
</authors>
<title>Towards developing general model of usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engeneering,</journal>
<volume>6</volume>
<contexts>
<context position="5930" citStr="Walker et al. (2000)" startWordPosition="932" endWordPosition="935">a can consequently be chosen as the best SRH. As we will show in Section 6, the scoring of the SRHs from Example (2) differs substantially, and only the contextual coherence score manages to pick an adequate SRH. The fact that neither of the other scoring approaches systematically employs the system’s knowledge of the domains at hand, can result in passing suboptimal SRHs through the system. This means that, while there was a better representation of the actual utterance in the n-best list, the NLU system is processing an inferior one, thereby causing overall dialogue metrics, in the sense of Walker et al. (2000), to decrease. We, therefore, propose an alternative way to rank SRHs on the basis of their contextual coherence, i.e. with respect to a given ontology representing the domains of the system and the given conceptual context. 3 Annotation Experiments The experiments reported here are based on the data collected in hidden-operator tests where subjects were prompted to say certain inputs. We obtained 232 dialogues, which were divided into 1479 (1a) ich I m¨ochte auf want on schnellsten fastest Weg von way from K¨oln nach Heidelberg. Cologne to Heidelberg. (1b) ich I nach to m¨ochte auf want on He</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn A. Walker, Candace A. Kamm, and Diane J. Litman. 2000. Towards developing general model of usability with PARADISE. Natural Language Engeneering, 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>