<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002430">
<title confidence="0.985464">
Dialogue Act Modeling in a Complex Task-Oriented Domain
</title>
<author confidence="0.967671666666667">
Kristy Eun Robert Michael D. Mladen A. James C.
Elizabeth Young Ha Phillips* Wallis* Vouk Lester
Boyer
</author>
<affiliation confidence="0.953942">
Department of Computer Science, North Carolina State University
Raleigh, North Carolina, USA
*Dual affiliation with Applied Research Associates, Inc.
Raleigh, North Carolina, USA
</affiliation>
<email confidence="0.827628">
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu
</email>
<sectionHeader confidence="0.995401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975704">
Classifying the dialogue act of a user utterance
is a key functionality of a dialogue
management system. This paper presents a
data-driven dialogue act classifier that is
learned from a corpus of human textual
dialogue. The task-oriented domain involves
tutoring in computer programming exercises.
While engaging in the task, students generate a
task event stream that is separate from and in
parallel with the dialogue. To deal with this
complex task-oriented dialogue, we propose a
vector-based representation that encodes
features from both the dialogue and the
hierarchically structured task for training a
maximum likelihood classifier. This classifier
also leverages knowledge of the hidden
dialogue state as learned separately by an
HMM, which in previous work has increased
the accuracy of models for predicting tutorial
moves and is hypothesized to improve the
accuracy for classifying student utterances.
This work constitutes a step toward learning a
fully data-driven dialogue management model
that leverages knowledge of the user-generated
task event stream.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999824425531915">
Two central challenges for dialogue systems are
interpreting user utterances and selecting system
dialogue moves. Recent years have seen an
increased focus on data-driven techniques for
addressing these challenging tasks (Bangalore et
al., 2008; Frampton &amp; Lemon, 2009; Hardy et
al., 2006; Sridar et al., 2009; Young et al., 2009).
Much of this work utilizes dialogue acts, built on
the notion of speech acts (Austin, 1962), which
provide a valuable intermediate representation
that can be used for dialogue management.
Data-driven approaches to dialogue act
interpretation have included models that take into
account a variety of lexical, syntactic, acoustic,
and prosodic features for dialogue act tagging
(Sridhar et al., 2009; Stolcke et al., 2000). In
task-oriented domains, recent work has
approached dialogue act classification by
learning dialogue management models entirely
from human-human corpora (Bangalore et al.,
2008; Chotimongkol, 2008; Hardy et al., 2006).
Our work adopts this approach for a corpus of
human-human dialogue in a task-oriented
tutoring domain. Unlike the majority of task-
oriented domains that have been studied to date,
our domain involves the separate creation of a
persistent artifact, in our case a computer
program, by the user during the course of the
dialogue. Our corpus consists of human-human
textual dialogue utterances and a separate,
parallel stream of user-generated task actions.
We utilize structural features including
task/subtask, speaker, and hidden dialogue state
along with lexical and syntactic features to
interpret user (student) utterances.
This paper makes three contributions. First, it
addresses representational issues in creating a
dialogue model that integrates task actions with
hierarchical task/subtask structure. The task is
captured within a separate synchronous event
stream that exists in parallel with the dialogue.
Second, this paper explores the performance of
dialogue act classifiers using different
lexical/syntactic and structural feature sets. This
comparison includes one model trained entirely
on lexical/syntactic features, an important step
toward robust unsupervised dialogue act tagging
</bodyText>
<note confidence="0.6230245">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297–305,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</note>
<page confidence="0.996609">
297
</page>
<bodyText confidence="0.994600714285714">
(Sridhar et al., 2009). Finally, it investigates
whether the addition of HMM and task/subtask
features improves the performance of the
dialogue act classifiers. The findings support this
hypothesis for three student dialogue moves,
each with important implications for tutorial
dialogue.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955816326531">
A variety of modeling approaches have been
investigated for statistical dialogue act
classification, including sequential approaches
and vector-based classifiers. Sequential
approaches typically formulate dialogue as a
Markov chain in which an observation depends
on a finite number of preceding observations.
HMM-based approaches make use of the Markov
assumption in a doubly stochastic framework
that allows fitting optimal dialogue act sequences
using the Viterbi algorithm (Rabiner, 1989;
Stolcke et al., 2000). Like this work, the
approach reported here adopts a first-order
Markov formulation to train an HMM on
sequences of dialogue acts, but the prediction of
this HMM is subsequently encoded in a feature
vector for training a vector-based classifier.
Vector-based approaches, such as maximum
entropy modeling, also frequently take into
account both lexical/syntactic and structural
features. Lexical and syntactic cues are extracted
from local utterance context, while structural
features involve longer dialogue act sequences
and, in task-oriented domains, task/subtask
history. Work by Bangalore et al. (2008) on
learning the structure of human-human dialogue
in a catalogue-ordering domain (also extended to
the Maptask and Switchboard corpora) utilizes
features including words, part of speech tags,
supertags, and named entities, and structural
features including dialogue acts and task/subtask
labels. In order to perform incremental decoding
of dialogue acts and task/subtask structure, they
take a greedy approach that does not require the
search of complete dialogue sequences. Our
work also accomplishes left-to-right incremental
interpretation with a greedy approach. Our
feature vectors differ from the aforementioned
work slightly with respect to lexical/syntactic
features and notably in the addition of a set of
structural features generated by a separately
trained HMM, as described in Section 4.2.
Recent work has explored the use of lexical,
syntactic, and prosodic features for online
dialogue act tagging (Sridhar et al., 2009); that
work explores the notion that structural (history)
features could be omitted altogether from
incremental left-to-right decoding, resulting in
computationally inexpensive and robust dialogue
act classification. Although our textual dialogue
does not feature prosodic cues, we report on the
use of lexical/syntactic features alone to perform
dialogue act classification, a step toward a fully
unsupervised approach.
Like Bangalore et al. (2008), we treat task
structure as an integral part of the dialogue
model. Other work that has taken this approach
includes the Amiti6s project, in which a dialogue
manager for a financial domain was derived
entirely from a human-human corpus (Hardy et
al., 2006). The TRIPS dialogue system also
closely integrated task and dialogue models, for
example, by utilizing the task model to facilitate
indirect speech act interpretation (Allen et al.,
2001). Work on the Maptask corpus has modeled
task structure in the form of conversational
games (Wright Hastie et al., 2002). Recent work
in task-oriented domains has focused on learning
task structure with unsupervised approaches
(Chotimongkol, 2008). Emerging unsupervised
methods, such as for detecting actions in multi-
party discourse, also implicitly capture a task
structure (Purver et al., 2006).
Our domain differs from the task-oriented
domains described above in that our dialogues
center on the user creating a persistent artifact of
intrinsic value through a separate, synchronous
stream of task actions. To illustrate, consider a
catalogue-ordering task in which one subtask is
to obtain the customer’s name. The fulfillment of
this subtask occurs entirely through the dialogue,
and the resulting artifact (a completed order) is
produced by the system. In contrast, our task
involves the user constructing a solution to a
computer programming problem. The fulfillment
of this task occurs partially in the dialogue
through tutoring, and partially in a separate
synchronous stream of user-driven task actions
about which the tutor must reason. The stream of
user-driven task actions produces an artifact of
value in itself (a functioning computer program),
and that artifact is the subject of much of the
dialogue. We propose a representation that
integrates task actions and dialogue acts from
these streams into a shared vector-based
representation, and we investigate the use of the
resulting structural, lexical, and syntactic
features for dialogue act classification.
</bodyText>
<page confidence="0.994562">
298
</page>
<sectionHeader confidence="0.943992" genericHeader="method">
3 Corpus and Annotation
</sectionHeader>
<bodyText confidence="0.99998352">
The corpus was collected during a controlled
human-human tutoring study in which tutors and
students worked through textual dialogue to
solve an introductory computer programming
problem. The dialogues were effective: on
average, students exhibited significant learning
and self-confidence gains (Boyer et al., 2009).
The corpus contains 48 dialogues each with a
separate, synchronous task event stream as
depicted in Excerpt 1 of the appendix. There is
exactly one dialogue (tutoring session) per
student. The corpus captures approximately 48
hours of dialogue and contains 1,468 student
utterances and 3,338 tutor utterances. Because
the dialogue was textual, utterance segmentation
consisted of splitting at existing sentence
boundaries when more than one dialogue act was
present in the utterance. This segmentation was
conducted manually by the principal dialogue act
annotator.1
The corpus was manually annotated with
dialogue act labels and task/subtask features.
Lexical and syntactic features were extracted
automatically. The remainder of this section
describes the manual annotation.
</bodyText>
<subsectionHeader confidence="0.998888">
3.1 Dialogue Act Annotation
</subsectionHeader>
<bodyText confidence="0.999989454545455">
The dialogue act annotation scheme was inspired
by schemes for conversational speech (Stolcke et
al., 2000) and task-oriented dialogue (Core &amp;
Allen, 1997). It was also influenced by tutoring-
specific tagsets (Litman &amp; Forbes-Riley, 2006).
Inter-rater reliability for the dialogue act tagging
on 10% of the corpus selected via stratified (by
tutor) random sampling was ĸ=0.80. The
dialogue act tags, their relative frequencies, and
their individual kappa scores from manual
annotation are displayed in Table 1.
</bodyText>
<subsectionHeader confidence="0.999493">
3.2 Task Annotation
</subsectionHeader>
<bodyText confidence="0.94979025">
All task actions were generated by the student
while implementing the solution to an
introductory computer programming problem in
Java. These task actions were recorded as a
separate event stream in parallel with the
dialogue corpus. This stream included 97,509
keystroke-level user task events, which were
manually aggregated into task/subtask event
clusters and annotated for subtask structure and
then for correctness. A total of 3,793 aggregated
1 Automatic segmentation is a challenging problem in itself
and is left to future work.
student subtask actions were identified through
manual annotation. The task annotation scheme
is hierarchical, reflecting the nested nature of the
subtasks. A subset of this task annotation scheme
is depicted in Figure 1. In the models reported in
this paper, the 66 leaves of the task/subtask
hierarchy were encoded in the input feature
vectors.
</bodyText>
<tableCaption confidence="0.999549">
Table 1. Student dialogue acts
</tableCaption>
<table confidence="0.998802933333333">
Student Dialogue Act Rel. Human
Freq. κ
ACKNOWLEDGMENT (ACK) .17 .90
REQUEST FOR FEEDBACK (RF) .20 .91
EXTRA‐DOMAIN (EX) .08 .79
GREETING (GR) .04 .92
UNCERTAIN FEEDBACK WITH ELABORATION (UE) .01 .53
UNCERTAIN FEEDBACK (U) .02 .49
NEGATIVE FEEDBACK WITH ELABORATION (NE) .01 .61
NEGATIVE FEEDBACK (N) .05 .76
POSITIVE FEEDBACK WITH ELABORATION (PE) .02 .43
POSITIVE FEEDBACK (P) .09 .81
QUESTION (Q) .09 .85
STATEMENT (S) .16 .82
THANKS (T) .05 1
</table>
<bodyText confidence="0.99967048">
Each group of task events that occurred between
dialogue utterances was tagged, possibly with
many subtask labels, by a human judge. The
judge aggregated the raw task keystrokes and
tagged the task/subtask hierarchy for each
cluster. (Please see Excerpt 1 in the appendix.) A
second judge tagged 20% of the corpus in a
reliability study for which one-to-one subtask
identification was not enforced, an approach that
was intended to give judges maximum flexibility
to cluster task actions and subsequently apply the
tags. All unmatched subtask tags were treated as
disagreements. The resulting kappa statistic at
the leaves was ĸ= 0.58. However, we also
observe that the sequential nature of the subtasks
within the larger task produces an ordinal
relationship between subtasks. For example, in
Figure 1, the “distance” between subtasks 1-a
and 1-b can be thought of as “less than” the
distance between subtasks 1-a vs. 3-d because
those subtasks are farther from each other within
the larger task. The weighted Kappa statistic
(Artstein &amp; Poesio, 2008) takes into account
such an ordinal relationship and its implicit
distance function. The weighted Kappa is
</bodyText>
<page confidence="0.991178">
299
</page>
<bodyText confidence="0.9621035">
ĸweighted=0.86, which indicates acceptable inter-
rater reliability on the task/subtask annotation.
</bodyText>
<figureCaption confidence="0.999057">
Figure 1. Portion of task annotation scheme
</figureCaption>
<bodyText confidence="0.99896575">
Along with its tag for hierarchical subtask
structure, each task event was also judged for
correctness according to the requirements of the
task as depicted in Table 2. The agreement
statistic for correctness was calculated for task
events on which the two judges agreed on
subtask tag. The resulting unweighted agreement
statistic for correctness was ĸ=0.80.
</bodyText>
<tableCaption confidence="0.993045">
Table 2. Task correctness labels
</tableCaption>
<subsectionHeader confidence="0.829557">
Label Description
</subsectionHeader>
<bodyText confidence="0.868290666666667">
CORRECT Fully satisfying the requirements of
the learning task. Does not require
tutorial remediation.
BUGGY Violating the requirements of the
learning task. Often requires tutorial
remediation.
INCOMPLETE Not violating, but not yet fully
satisfying, the requirements of the
learning task. May require tutorial
remediation.
DISPREFERRED Technically satisfying the
requirements of the learning task,
but not adhering to its pedagogical
intentions. Usually requires tutorial
remediation.
</bodyText>
<sectionHeader confidence="0.99982" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999964166666667">
The vector-based representation for training the
dialogue act classifiers integrates several sources
of features: lexical and syntactic features, and
structural features that include dialogue act
labels, task/subtask labels, and set of hidden
dialogue state prediction features.
</bodyText>
<subsectionHeader confidence="0.99354">
4.1 Lexical and Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999483842105263">
Lexical and syntactic features were automatically
extracted from the utterances using the Stanford
Parser default tokenizer and part of speech (pos)
tagger (De Marneffe et al., 2006). The parser
created both phrase structure trees and typed
dependencies for individual sentences. From the
phrase structure trees, we extracted the top-most
syntactic node and its first two children. In the
case where an utterance consisted of more than
one sentence, only the phrase structure tree of the
first sentence was considered. Typed
dependencies between pairs of words were
extracted from each sentence. Individual word
tokens in the utterances were further processed
with the Porter Stemmer (Porter, 1980) in the
NLTK package (Loper &amp; Bird, 2004). The pos
features were extracted in a similar way.
Unigram and bigram word and pos tags were
included for feature selection in the classifiers.
</bodyText>
<subsectionHeader confidence="0.972826">
4.2 Structural Features
</subsectionHeader>
<bodyText confidence="0.999997222222222">
Structural features include the annotated
dialogue acts, the annotated task/subtask labels,
and attributes that represent the hidden dialogue
state. Our previous work has found that a set of
hidden dialogue states, which correspond to
widely accepted notions of dialogue modes in
tutoring, can be identified in an unsupervised
fashion (without hand labeling of the modes) by
HMMs trained on manually labeled dialogue acts
and task/subtask features (Boyer et al., 2009).
These HMMs performed significantly better than
bigram models for predicting human tutor moves
(Boyer et al., 2010), which indicates that the
hidden dialogue state leveraged by the HMMs
has predictive value even in the presence of
“true” (manually annotated) dialogue act labels.
Therefore, we hypothesized that an HMM could
also improve the performance of models to
classify student dialogue acts. To explore this
hypothesis, we trained an HMM utilizing the
methodology described in (Boyer et al., 2009)
and used it to generate hidden dialogue state
predictions in the form of a probability
distribution over possible user utterances at each
step in the dialogue. This set of stochastic
features was subsequently passed to the classifier
as part of the input vector (Figure 2).
</bodyText>
<subsectionHeader confidence="0.991664">
4.3 Input Vectors
</subsectionHeader>
<bodyText confidence="0.999983">
The features were combined into a shared vector-
based representation for training the classifier.
As depicted in Table 3, the components of the
</bodyText>
<page confidence="0.993858">
300
</page>
<bodyText confidence="0.991013473684211">
feature vector include binary existence vectors
for lexical and syntactic features for the current
(target) utterance as well as for three utterances
of left context (this left context may include both
tutor and student utterances, which are
distinguished by a separate indicator for the
speaker). The task/subtask and correctness
history features encode the separate stream of
task events. There is no one-to-one
correspondence between these history features
and the left-hand dialogue context, because
several task events could have occurred between
a pair of dialogue events (or vice versa). This
distinction is indicated in the table by the
representation of dialogue time steps as [t, t-1,...]
and task history steps as [task(t), task(t-1),...]. In
total, the feature vectors included 11,432
attributes that were made available for feature
selection.
</bodyText>
<figureCaption confidence="0.979769">
Figure 2. Generation of hidden dialogue state
prediction features
</figureCaption>
<sectionHeader confidence="0.998124" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999938">
This section describes the learning of maximum
likelihood vector-based models for classification
of user dialogue acts. In addition to investigating
the accuracy of the overall model, we also
performed experiments regarding the utility of
feature types for discriminating between
particular dialogue acts of interest.
The classifiers are based on logistic
regression, which learns a discriminant for each
pair of dialogue acts by assigning weights in a
maximum likelihood fashion.2 The logistic
regression models were learned using the Weka
machine learning toolkit (Hall et al., 2009). For
</bodyText>
<footnote confidence="0.820372333333333">
2 In general, the model that maximizes likelihood also
maximizes entropy under the same constraints
(Berger et al., 1996).
</footnote>
<bodyText confidence="0.999966166666667">
feature selection, we performed attribute subset
evaluation with a best-first approach that
greedily searched the space of possible features
using a hill climbing approach with
backtracking. The prediction accuracy of the
classifiers was determined through ten-fold
cross-validation on the corpus, and the results
below are presented in terms of prediction
accuracy (number of correct classifications
divided by total number of classifications) as
well as by the kappa statistic, which adjusts for
expected agreement by chance.
</bodyText>
<tableCaption confidence="0.988492">
Table 3. Feature vectors
</tableCaption>
<table confidence="0.68772875">
Feature vector f Description
[wt,1,...wt,|w|, Binary existence vector for word
pt,1,...,pt,|p|, unigrams &amp; bigrams, pos unigrams &amp;
dt,1,...,dt,|d|, bigrams, dependency types, and syntactic
st,1,...,st,|s|] nodes for current target utterance t
Binary existence vector for word
unigrams &amp; bigrams, pos unigrams &amp;
bigrams, dependency types, and syntactic
nodes for three utterances of left context
Probability distribution for emission
[p(o1),...,p(o|S|)] symbols in predicted next hidden state as
generated by HMM
[dat-1, dat-2, dat-3] Dialogue act left context
[spt-1,spt-2, spt-3] Speaker label left context
tktask(t-3)]
[tktask(t-1), tktask(t-2), Three steps of subtask history (each level
feature)
of hierarchy represented as a separate
[ctask(t-1), ctask(t-2), Three steps of task correctness history
ctask(t-3)]
</table>
<tableCaption confidence="0.522221666666667">
pt Indicator for whether the target
utterance was immediately preceded by
a task event
</tableCaption>
<subsectionHeader confidence="0.973307">
5.1 Overall Classification Task
</subsectionHeader>
<bodyText confidence="0.996689857142857">
The overall dialogue act classification model was
trained to classify each utterance with respect to
the thirteen dialogue acts (Table 1). For this task,
the feature selection algorithm selected 63
attributes including some syntax, dependency,
pos, and word attributes as well as dialogue act,
speaker, and task/subtask features. No hidden
dialogue state features or task correctness
attributes were selected. The overall
classification accuracy was 62.8%. This accuracy
constitutes a 369% improvement over baseline
chance of 17% (the relative frequency of the
most frequently occurring dialogue act, ACK).
An alternate nontrivial baseline is a bigram
model on true dialogue acts (including speaker
tags); this model’s accuracy was 36.8%. The
[wt-k,1,...wt-k,|w|,
pt-k,1,...,pt-k,|p|,
dt-k,1,...,dt-k,|d|,
st-k,1,...,st-k,|s|]
where k=1,...,3
</bodyText>
<page confidence="0.995299">
301
</page>
<figureCaption confidence="0.953215">
Figure 4. Kappa for binary DA classifiers by
features available for selection
</figureCaption>
<sectionHeader confidence="0.997709" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999979842105263">
We have presented a maximum likelihood
classifier that assigns dialogue act labels to user
utterances from a corpus of human-human
tutorial dialogue given a set of lexical, syntactic,
and structural features. Overall, this classifier
achieved 62.8% accuracy in ten-fold cross-
validation on the corpus. This performance is on
par with other automatic dialogue act tagging
models, both sequential and vector-based, in
task-oriented domains that do not feature
complex, user-driven parallel tasks.
In a catalogue ordering domain with an
integrated task and dialogue model, Bangalore et
al. (2009) report 75% classification accuracy for
user utterances using a maximum entropy
classifier, a 275% improvement over baseline.
Poesio &amp; Mikheev (1998) report 54%
classification accuracy by utilizing
conversational game structure and speaker
changes in the Maptask corpus, an improvement
of 170% over baseline. Recent work on Maptask
reports a classification accuracy of 65.7% using
local utterance (such as lexical/syntactic)
features alone, with prosodic cues yielding
further slight improvement (Sridhar et al., 2009).
This classifier is analogous to our
lexical/syntactic feature model, which achieved
60.2% accuracy.
The results of these models demonstrate that,
consistent with the findings in other task-oriented
domains, lexical/syntactic features are highly
useful for classifying student dialogue moves in
this complex task-oriented domain. Models
trained using those lexical/syntactic features
performed almost universally better (with the
exception of the binary classifier for GREETING)
than models that were given the same left context
of true dialogue act tags.
It was hypothesized that leveraging both the
hidden dialogue state and hierarchical subtask
features would improve the performance of the
classifiers. There is some evidence that the
subtask structure was helpful for the overall
classifier; however, no HMM features were kept
during feature selection for the overall model. Of
the binary models, approximately half performed
better than the overall model in terms of true
positive rate; of those, three did so by including
HMM or task/subtask features among the
selected attributes to differentiate different tones
of student feedback. However, this difference in
performance was not statistically reliable. This
finding suggests that, given lexical and syntactic
features which are strong predictors of dialogue
acts, the hidden dialogue state as captured by an
an HMM may not contribute significantly to the
dialogue act classification task.
</bodyText>
<sectionHeader confidence="0.951457" genericHeader="evaluation">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999972206896552">
Dialogue modeling for complex task-oriented
domains poses significant challenges. An
effective dialogue model allows systems to
detect user dialogue acts so that they can respond
in a manner that maximizes the chance of
success. Experiments with the data-driven
classifiers presented in this paper demonstrate
that lexical/syntactic features can effectively
classify student dialogue acts in the task-oriented
tutoring domain. For POSITIVE, NEGATIVE, and
UNCERTAIN ELABORATED student feedback acts,
which play a key role in tutorial dialogue system,
the addition of hidden dialogue state features (as
learned by an HMM) and task/subtask features
(annotated manually) improve classification
accuracy, but not statistically reliably.
The overarching goal of this work is to create
a data-driven tutorial dialogue system that learns
its behavior from corpora of effective human
tutoring. The dialogue act classification models
reported here constitute an important step toward
that goal, by integrating the dialogue stream with
a parallel user-driven task event stream. The next
generation of data-driven systems should
leverage models that capture the rich interplay
between dialogue and task. Future work will
focus on data-driven approaches to task
recognition and tutorial planning. Additionally,
as dialogue system research addresses
</bodyText>
<page confidence="0.997803">
303
</page>
<bodyText confidence="0.9997266">
increasingly complex task-oriented domains, it
becomes increasingly important to investigate
unsupervised approaches for dialogue act
classification and task recognition.
Acknowledgements. This work is supported in
part by the North Carolina State University
Department of Computer Science and the
National Science Foundation through a Graduate
Research Fellowship and Grants CNS-0540523,
REC-0632450 and IIS-0812291. Any opinions,
findings, conclusions, or recommendations
expressed in this report are those of the
participants, and do not necessarily represent the
official views, opinions, or policy of the National
Science Foundation.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.999763282608695">
Allen, J., Ferguson, G., &amp; Stent, A. (2001). An
architecture for more realistic conversational
systems. Proceedings of the IUI, 1-8.
Artstein, R., &amp; Poesio, M. (2008). Inter-coder
agreement for computational linguistics.
Computational Linguistics, 34(4), 555-596.
Austin, J. L. (1962). How to do things with words.
Oxford: Oxford University Press.
Bangalore, S., Di Fabbrizio, G., &amp; Stent, A. (2008).
Learning the structure of task-driven human-human
dialogs. IEEE Transactions on Audio, Speech, and
Language Processing, 16(7), 1249-1259.
Berger, A. L., Pietra, V. J. D., &amp; Pietra, S. A. D.
(1996). A maximum entropy approach to natural
language processing. Comp. Ling., 22(1), 71.
Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D.,
Vouk, M. A., &amp; Lester, J. C. (2009). Modeling
dialogue structure with adjacency pair analysis and
hidden markov models. Proceedings of NAACL-
HLT, Short Papers, 49-52.
Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D.,
Vouk, M. A., &amp; Lester, J. C. (2010). Leveraging
hidden dialogue state to select tutorial moves.
Proceedings of the 5th NAACL HLT Workshop on
Innovative use of NLP for Building Educational
Applications, Los Angeles, California.
Chotimongkol, A. (2008). Learning the structure of
task-oriented conversations from the corpus of in-
domain dialogs. (Unpublished Ph.D. Dissertation).
Carnegie Mellon University School of Computer
Science.
Core, M., &amp; Allen, J. (1997). Coding dialogs with the
DAMSL annotation scheme. AAAI Fall Symposium
on Communicative Action in Humans and
Machines, 28–35.
De Marneffe, M. C., MacCartney, B., &amp; Manning, C.
D. (2006). Generating typed dependency parses
from phrase structure parses. Proceedings of LREC,
Genoa, Italy.
Forbes-Riley, K., &amp; Litman, D. (2009). Adapting to
student uncertainty improves tutoring dialogues.
Proceedings of AIED, 33-40.
Frampton, M., &amp; Lemon, O. (2009). Recent research
advances in reinforcement learning in spoken
dialogue systems. The Knowledge Engineering
Review, 24(4), 375-408.
Hall, M., Frank, E., Holmes, G., Pfahringer, B.,
Reutemann, P., &amp; Witten, I. (2009). The WEKA
data mining software: An update. SIGKDD
Explorations, 11(1)
Hardy, H., Biermann, A., Inouye, R. B., McKenzie,
A., Strzalkowski, T., Ursu, C., Webb, N., &amp; Wu, M.
(2006). The Amities system: Data-driven techniques
for automated dialogue. Speech Comm., 48(3-4),
354-373.
Litman, D., &amp; Forbes-Riley, K. (2006). Correlations
between dialogue acts and learning in spoken
tutoring dialogues. Natural Language Engineering,
12(2), 161-176.
Loper, E., &amp; Bird, S. (2004). NLTK: The natural
language toolkit. Proceedings of the ACL
Demonstration Session, Barcelona, Spain. 214-217.
Porter, M. F. (1980). An algorithm for suffix
stripping. Program, 14(3), 130-137.
Purver, M., Kording, K. P., Griffiths, T. L., &amp;
Tenenbaum, J. B. (2006). Unsupervised topic
modelling for multi-party spoken discourse.
Proceedings of the ACL, Sydney, Australia. , 44(1)
17.
Rabiner, L. R. (1989). A tutorial on hidden markov
models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2), 257-
286.
Sridhar, V. K. R., Bangalore, S., &amp; Narayanan, S.
(2009). Combining lexical, syntactic and prosodic
cues for improved online dialog act tagging.
Computer Speech &amp; Language, 23(4), 407-422.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates,
R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-
Dykema, C., &amp; Meteer, M. (2000). Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Comp. Ling., 26(3), 339-373.
Wright Hastie, H., Poesio, M., &amp; Isard, S. (2002).
Automatically predicting dialogue structure using
prosodic features. Speech Communication, 36(1-2),
63-79.
Young, S., Gasic, M., Keizer, S., Mairesse, F.,
Schatzmann, J., Thomson, B., &amp; Yu, K. (2009). The
hidden information state model: A practical
framework for POMDP-based spoken dialogue
management. Computer Speech and Language,
24(2), 150-174.
</reference>
<page confidence="0.999253">
304
</page>
<sectionHeader confidence="0.799188" genericHeader="references">
Appendix
</sectionHeader>
<figure confidence="0.969220954545455">
Task
Time Stamp Dialogue Stream Stream
2008-04-11 18:23:45 Student: so do i have to manipulate the
array this time? [Q]
2008-04-11 18:23:53 Tutor: this time, we need to do two things
[S]
2008-04-11 18:24:02 Tutor: first, we need to create a new
array to hold the changed values
[S]
2008 -04 -11 18:24:28 i
2008 -04 -11 18:24:28 n
2008 -04 -11 18:24:28 t
2008 -04 -11 18:24:28 \sp
2008-04-11 18:24:35 \del
2008-04-11 18:24:36 \sp
2008 -04 -11 18:24:36 d
2008 -04 -11 18:24:36 o
2008 -04 -11 18:24:36 u
2008 -04 -11 18:24:36 b
2008 -04 -11 18:24:37 l
2008 -04 -11 18:24:37 e
2008 -04 -11 18:24:37 \sp
2008 -04 -11 18:24:39 []
2008-04-11 18:24:40 \s
2008 -04 -11 18:24:42 n
2008 -04 -11 18:24:42 e
2008 -04 -11 18:24:42 w
2008 -04 -11 18:24:43 \sp
2008 -04 -11 18:24:44 \del
2008 -04 -11 18:24:45 T
2008 -04 -11 18:24:46 \del
2008 -04 -11 18:24:54 T
2008 -04 -11 18:24:54 i
2008 -04 -11 18:24:54 m
2008 -04 -11 18:24:54 e
2008 -04 -11 18:24:54 s
2008 -04 -11 18:24:55 3
2008 -04 -11 18:24:57 ;
1-a-i
BUGGY
1-a-i
CORRECT
1-a-ii
CORRECT
</figure>
<reference confidence="0.970229866666667">
2008-04-11 18:25:11 Student: good? [RF]
2008-04-11 18:25:14 Tutor: good so far, yes [PF]
2008-04-11 18:25:29 Student: so now i have to change parts of
the times array right? [Q]
2008-04-11 18:25:34 Tutor: not quite [LF]
2008-04-11 18:25:57 Tutor: So, when you create a new object,
like a String for example, you&apos;d say
something like String s = new
String() [S]
2008-04-11 18:25:59 Tutor: right? [AQ]
2008-04-11 18:26:06 Student: right [P]
2008-04-11 18:26:14 Tutor: arrays are similar [S]
Excerpt 1. Parallel synchronous dialogue and task event streams
with annotations. (Note tutor dialogue acts: AQ=ASSESSING
QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK)
</reference>
<page confidence="0.999163">
305
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.823268">
<title confidence="0.99713">Dialogue Act Modeling in a Complex Task-Oriented Domain</title>
<author confidence="0.974839666666667">Kristy Eun Robert Michael D Mladen A James C Young Ha Lester Boyer</author>
<affiliation confidence="0.996881">Department of Computer Science, North Carolina State</affiliation>
<address confidence="0.959559">Raleigh, North Carolina, USA</address>
<affiliation confidence="0.960674">with Applied Research Associates,</affiliation>
<address confidence="0.999587">Raleigh, North Carolina, USA</address>
<email confidence="0.999794">keboyer@ncsu.edu</email>
<email confidence="0.999794">eha@ncsu.edu</email>
<email confidence="0.999794">rphilli@ncsu.edu</email>
<email confidence="0.999794">mdwallis@ncsu.edu</email>
<email confidence="0.999794">vouk@ncsu.edu</email>
<email confidence="0.999794">lester@ncsu.edu</email>
<abstract confidence="0.998802115384615">Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>G Ferguson</author>
<author>A Stent</author>
</authors>
<title>An architecture for more realistic conversational systems.</title>
<date>2001</date>
<booktitle>Proceedings of the IUI,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="7143" citStr="Allen et al., 2001" startWordPosition="1020" endWordPosition="1023">es, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach. Like Bangalore et al. (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti6s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al., 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al., 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al., 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multiparty discourse, also implicitly capture a task structure (Purver et al., 2006). Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of ta</context>
</contexts>
<marker>Allen, Ferguson, Stent, 2001</marker>
<rawString>Allen, J., Ferguson, G., &amp; Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<pages>555--596</pages>
<contexts>
<context position="12818" citStr="Artstein &amp; Poesio, 2008" startWordPosition="1878" endWordPosition="1881"> to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ĸ= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the “distance” between subtasks 1-a and 1-b can be thought of as “less than” the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein &amp; Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 299 ĸweighted=0.86, which indicates acceptable interrater reliability on the task/subtask annotation. Figure 1. Portion of task annotation scheme Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Artstein, R., &amp; Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Austin</author>
</authors>
<title>How to do things with words. Oxford:</title>
<date>1962</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1914" citStr="Austin, 1962" startWordPosition="279" endWordPosition="280">udent utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al., 2008; Frampton &amp; Lemon, 2009; Hardy et al., 2006; Sridar et al., 2009; Young et al., 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al., 2008; Chotimongkol, 2008; Hardy et al., 2006). Our work adopts this approach for a corpus of hum</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>G Di Fabbrizio</author>
<author>A Stent</author>
</authors>
<title>Learning the structure of task-driven human-human dialogs.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<pages>1249--1259</pages>
<marker>Bangalore, Di Fabbrizio, Stent, 2008</marker>
<rawString>Bangalore, S., Di Fabbrizio, G., &amp; Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>V J D Pietra</author>
<author>S A D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Comp. Ling.,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>71</pages>
<contexts>
<context position="18262" citStr="Berger et al., 1996" startWordPosition="2685" endWordPosition="2688">tion of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest. The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion.2 The logistic regression models were learned using the Weka machine learning toolkit (Hall et al., 2009). For 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al., 1996). feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance. Table 3. Feature vectors Feature vector f Description [wt,1,...wt,|w|,</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, A. L., Pietra, V. J. D., &amp; Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>E Y Ha</author>
<author>M D Wallis</author>
<author>M A Vouk</author>
<author>J C Lester</author>
</authors>
<title>Modeling dialogue structure with adjacency pair analysis and hidden markov models.</title>
<date>2009</date>
<booktitle>Proceedings of NAACLHLT, Short Papers,</booktitle>
<pages>49--52</pages>
<contexts>
<context position="9066" citStr="Boyer et al., 2009" startWordPosition="1304" endWordPosition="1307">of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification. 298 3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al., 2009). The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1 The corpus was manually annotat</context>
<context position="15669" citStr="Boyer et al., 2009" startWordPosition="2293" endWordPosition="2296"> were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers. 4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al., 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al., 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of “true” (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al., 2009) and used it to generate hidden dialogue state predictions in the form of a probability distrib</context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Vouk, Lester, 2009</marker>
<rawString>Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., &amp; Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACLHLT, Short Papers, 49-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K E Boyer</author>
<author>R Phillips</author>
<author>E Y Ha</author>
<author>M D Wallis</author>
<author>M A Vouk</author>
<author>J C Lester</author>
</authors>
<title>Leveraging hidden dialogue state to select tutorial moves.</title>
<date>2010</date>
<booktitle>Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications,</booktitle>
<location>Los Angeles, California.</location>
<contexts>
<context position="15785" citStr="Boyer et al., 2010" startWordPosition="2310" endWordPosition="2313">assifiers. 4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al., 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al., 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of “true” (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al., 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently p</context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Vouk, Lester, 2010</marker>
<rawString>Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., &amp; Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chotimongkol</author>
</authors>
<title>Learning the structure of task-oriented conversations from the corpus of indomain dialogs.</title>
<date>2008</date>
<institution>(Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science.</institution>
<contexts>
<context position="2442" citStr="Chotimongkol, 2008" startWordPosition="351" endWordPosition="352">uch of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al., 2008; Chotimongkol, 2008; Hardy et al., 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of taskoriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic f</context>
<context position="7390" citStr="Chotimongkol, 2008" startWordPosition="1058" endWordPosition="1059">r work that has taken this approach includes the Amiti6s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al., 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al., 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al., 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multiparty discourse, also implicitly capture a task structure (Purver et al., 2006). Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer’s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced b</context>
</contexts>
<marker>Chotimongkol, 2008</marker>
<rawString>Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of indomain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>J Allen</author>
</authors>
<title>Coding dialogs with the DAMSL annotation scheme.</title>
<date>1997</date>
<booktitle>AAAI Fall Symposium on Communicative Action in Humans and Machines,</booktitle>
<pages>28--35</pages>
<contexts>
<context position="10028" citStr="Core &amp; Allen, 1997" startWordPosition="1442" endWordPosition="1445">l, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1 The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al., 2000) and task-oriented dialogue (Core &amp; Allen, 1997). It was also influenced by tutoringspecific tagsets (Litman &amp; Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ĸ=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1. 3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue</context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Core, M., &amp; Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M C De Marneffe</author>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>Proceedings of LREC,</booktitle>
<location>Genoa, Italy.</location>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>De Marneffe, M. C., MacCartney, B., &amp; Manning, C. D. (2006). Generating typed dependency parses from phrase structure parses. Proceedings of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Adapting to student uncertainty improves tutoring dialogues.</title>
<date>2009</date>
<booktitle>Proceedings of AIED,</booktitle>
<pages>33--40</pages>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>Forbes-Riley, K., &amp; Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frampton</author>
<author>O Lemon</author>
</authors>
<title>Recent research advances in reinforcement learning in spoken dialogue systems.</title>
<date>2009</date>
<journal>The Knowledge Engineering Review,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>375--408</pages>
<contexts>
<context position="1759" citStr="Frampton &amp; Lemon, 2009" startWordPosition="249" endWordPosition="252"> by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al., 2008; Frampton &amp; Lemon, 2009; Hardy et al., 2006; Sridar et al., 2009; Young et al., 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management m</context>
</contexts>
<marker>Frampton, Lemon, 2009</marker>
<rawString>Frampton, M., &amp; Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hall</author>
<author>E Frank</author>
<author>G Holmes</author>
<author>B Pfahringer</author>
<author>P Reutemann</author>
<author>I Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="18135" citStr="Hall et al., 2009" startWordPosition="2665" endWordPosition="2668">rediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest. The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion.2 The logistic regression models were learned using the Weka machine learning toolkit (Hall et al., 2009). For 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al., 1996). feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa st</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., &amp; Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>A Biermann</author>
<author>R B Inouye</author>
<author>A McKenzie</author>
<author>T Strzalkowski</author>
<author>C Ursu</author>
<author>N Webb</author>
<author>M Wu</author>
</authors>
<title>The Amities system: Data-driven techniques for automated dialogue.</title>
<date>2006</date>
<journal>Speech Comm.,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>354--373</pages>
<contexts>
<context position="1779" citStr="Hardy et al., 2006" startWordPosition="253" endWordPosition="256">vious work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al., 2008; Frampton &amp; Lemon, 2009; Hardy et al., 2006; Sridar et al., 2009; Young et al., 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from </context>
<context position="6955" citStr="Hardy et al., 2006" startWordPosition="992" endWordPosition="995">ogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach. Like Bangalore et al. (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti6s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al., 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al., 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al., 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multiparty discourse, also implicitly capture a task structure (Purver et al., 2006). Our domain di</context>
</contexts>
<marker>Hardy, Biermann, Inouye, McKenzie, Strzalkowski, Ursu, Webb, Wu, 2006</marker>
<rawString>Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., &amp; Wu, M. (2006). The Amities system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Correlations between dialogue acts and learning in spoken tutoring dialogues.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<pages>161--176</pages>
<contexts>
<context position="10110" citStr="Litman &amp; Forbes-Riley, 2006" startWordPosition="1454" endWordPosition="1457">ndaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1 The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al., 2000) and task-oriented dialogue (Core &amp; Allen, 1997). It was also influenced by tutoringspecific tagsets (Litman &amp; Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ĸ=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1. 3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were </context>
</contexts>
<marker>Litman, Forbes-Riley, 2006</marker>
<rawString>Litman, D., &amp; Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Loper</author>
<author>S Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Demonstration Session,</booktitle>
<pages>214--217</pages>
<location>Barcelona,</location>
<contexts>
<context position="15032" citStr="Loper &amp; Bird, 2004" startWordPosition="2196" endWordPosition="2199">zer and part of speech (pos) tagger (De Marneffe et al., 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper &amp; Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers. 4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/</context>
</contexts>
<marker>Loper, Bird, 2004</marker>
<rawString>Loper, E., &amp; Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>130--137</pages>
<contexts>
<context position="14991" citStr="Porter, 1980" startWordPosition="2190" endWordPosition="2191"> the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al., 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper &amp; Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers. 4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>K P Kording</author>
<author>T L Griffiths</author>
<author>J B Tenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>Proceedings of the ACL,</booktitle>
<volume>44</volume>
<issue>1</issue>
<pages>17</pages>
<location>Sydney, Australia. ,</location>
<contexts>
<context position="7540" citStr="Purver et al., 2006" startWordPosition="1078" endWordPosition="1081">n-human corpus (Hardy et al., 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al., 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al., 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multiparty discourse, also implicitly capture a task structure (Purver et al., 2006). Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer’s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs p</context>
</contexts>
<marker>Purver, Kording, Griffiths, Tenenbaum, 2006</marker>
<rawString>Purver, M., Kording, K. P., Griffiths, T. L., &amp; Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>257--286</pages>
<contexts>
<context position="4683" citStr="Rabiner, 1989" startWordPosition="665" endWordPosition="666"> this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue. 2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al., 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. W</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V K R Sridhar</author>
<author>S Bangalore</author>
<author>S Narayanan</author>
</authors>
<title>Combining lexical, syntactic and prosodic cues for improved online dialog act tagging.</title>
<date>2009</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>23</volume>
<issue>4</issue>
<pages>407--422</pages>
<contexts>
<context position="2220" citStr="Sridhar et al., 2009" startWordPosition="320" endWordPosition="323">ves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al., 2008; Frampton &amp; Lemon, 2009; Hardy et al., 2006; Sridar et al., 2009; Young et al., 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al., 2008; Chotimongkol, 2008; Hardy et al., 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of taskoriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-</context>
<context position="3911" citStr="Sridhar et al., 2009" startWordPosition="558" endWordPosition="561">ed within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297–305, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 297 (Sridhar et al., 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue. 2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based</context>
<context position="6249" citStr="Sridhar et al., 2009" startWordPosition="888" endWordPosition="891">m incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2. Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al., 2009); that work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach. Like Bangalore et al. (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti6s project, in which a d</context>
<context position="21800" citStr="Sridhar et al., 2009" startWordPosition="3177" endWordPosition="3180">sks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al. (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio &amp; Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al., 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags. It was hypothesized that leveraging both th</context>
</contexts>
<marker>Sridhar, Bangalore, Narayanan, 2009</marker>
<rawString>Sridhar, V. K. R., Bangalore, S., &amp; Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech &amp; Language, 23(4), 407-422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van EssDykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Comp. Ling.,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>339--373</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van EssDykema, Meteer, 2000</marker>
<rawString>Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van EssDykema, C., &amp; Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wright Hastie</author>
<author>H Poesio</author>
<author>M</author>
<author>S Isard</author>
</authors>
<title>Automatically predicting dialogue structure using prosodic features.</title>
<date>2002</date>
<journal>Speech Communication,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>63--79</pages>
<contexts>
<context position="7263" citStr="Hastie et al., 2002" startWordPosition="1040" endWordPosition="1043">lly unsupervised approach. Like Bangalore et al. (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti6s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al., 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al., 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al., 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multiparty discourse, also implicitly capture a task structure (Purver et al., 2006). Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer’s name. The</context>
</contexts>
<marker>Hastie, Poesio, M, Isard, 2002</marker>
<rawString>Wright Hastie, H., Poesio, M., &amp; Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gasic</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The hidden information state model: A practical framework for POMDP-based spoken dialogue management.</title>
<date>2009</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>150--174</pages>
<contexts>
<context position="1821" citStr="Young et al., 2009" startWordPosition="261" endWordPosition="264">models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al., 2008; Frampton &amp; Lemon, 2009; Hardy et al., 2006; Sridar et al., 2009; Young et al., 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al., 2009; Stolcke et al., 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al., 200</context>
</contexts>
<marker>Young, Gasic, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2009</marker>
<rawString>Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., &amp; Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.</rawString>
</citation>
<citation valid="false">
<title>Student: good? [RF] 2008-04-11 18:25:14 Tutor: good so far, yes [PF] 2008-04-11 18:25:29 Student: so now i have to change parts of the times array right? [Q] 2008-04-11 18:25:34 Tutor: not quite [LF] 2008-04-11 18:25:57 Tutor: So, when you create a new object, like a String for example, you&apos;d say something like</title>
<booktitle>String s = new String() [S] 2008-04-11 18:25:59 Tutor: right? [AQ] 2008-04-11 18:26:06 Student: right [P]</booktitle>
<pages>2008--04</pages>
<note>Tutor: arrays are similar [S]</note>
<marker></marker>
<rawString>2008-04-11 18:25:11 Student: good? [RF] 2008-04-11 18:25:14 Tutor: good so far, yes [PF] 2008-04-11 18:25:29 Student: so now i have to change parts of the times array right? [Q] 2008-04-11 18:25:34 Tutor: not quite [LF] 2008-04-11 18:25:57 Tutor: So, when you create a new object, like a String for example, you&apos;d say something like String s = new String() [S] 2008-04-11 18:25:59 Tutor: right? [AQ] 2008-04-11 18:26:06 Student: right [P] 2008-04-11 18:26:14 Tutor: arrays are similar [S]</rawString>
</citation>
<citation valid="false">
<authors>
<author>Excerpt</author>
</authors>
<title>Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts:</title>
<journal>AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK</journal>
<marker>Excerpt, </marker>
<rawString>Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>