<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001403">
<title confidence="0.989993">
Construction of a German HPSG grammar from a detailed treebank
</title>
<author confidence="0.999223">
Bart Cramer† and Yi Zhang†�
</author>
<affiliation confidence="0.996054">
Department of Computational Linguistics &amp; Phonetics, Saarland University, Germany†
LT-Lab, German Research Center for Artificial Intelligence, Germany$
</affiliation>
<email confidence="0.992065">
{bcramer,yzhang}@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997294" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999827230769231">
Grammar extraction in deep formalisms
has received remarkable attention in re-
cent years. We recognise its value, but try
to create a more precision-oriented gram-
mar, by hand-crafting a core grammar, and
learning lexical types and lexical items
from a treebank. The study we performed
focused on German, and we used the Tiger
treebank as our resource. A completely
hand-written grammar in the framework of
HPSG forms the inspiration for our core
grammar, and is also our frame of refer-
ence for evaluation. 1
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994605">
Previous studies have shown that treebanks can
be helpful when constructing grammars. The
most well-known example is PCFG-based statis-
tical parsing (Charniak and Johnson, 2005), where
a PCFG is induced from, for instance, the Penn
Treebank. The underlying statistical techniques
have been refined in the last decade, and previ-
ous work indicates that the labelled f-score of this
method converges to around 91%.
An alternative to PCFGs, with more linguistic
relevance, is formed by deeper formalisms, such
as TAG (Joshi and Schabes, 1997), CCG (Steed-
man, 1996), LFG (Kaplan and Bresnan, 1995)
and HPSG (Pollard and Sag, 1994). For LFG
(Butt et al., 2002) and HPSG (Flickinger, 2000;
M¨uller, 2002), large hand-written grammars have
been developed. In the case of HPSG, the gram-
mar writers found the small number of principles
too restrictive, and created more rules (approxi-
mately 50 to 300) to accommodate for phenomena
</bodyText>
<footnote confidence="0.97511025">
1The research reported in this paper has been carried out
with financial support from the Deutsche Forschungsgemein-
schaft and the German Excellence Cluster of Multimodal
Computing &amp; Interaction.
</footnote>
<bodyText confidence="0.998456317073171">
that vanilla HPSG cannot describe correctly. The
increased linguistic preciseness comes at a cost,
though: such grammars have a lower out-of-the-
box coverage, i.e. they will not give an analysis on
a certain portion of the corpus.
Experiments have been conducted, where a
lexicalised grammar is learnt from treebanks, a
methodology for which we coin the name deep
grammar extraction. The basic architecture of
such an experiment is to convert the treebank to
a format that is compatible with the chosen lin-
guistic formalism, and read off the lexicon from
that converted treebank. Because all these for-
malisms are heavily lexicalised, the core gram-
mars only consist of a small number of principles
or operators. In the case of CCG (Hockenmaier
and Steedman, 2002), the core grammar consists
of the operators that CCG stipulates: function ap-
plication, composition and type-raising. Standard
HPSG defines a few schemata, but these are usu-
ally adapted for a large-scale grammar. Miyao et
al. (2004) tailor their core grammar for optimal use
with the Penn Treebank and the English language,
for example by adding a new schema for relative
clauses.
Hockenmaier and Steedman (2002), Miyao et
al. (2004) and Cahill et al. (2004) show fairly good
results on the Penn Treebank (for CCG, HPSG and
LFG, respectively): these parsers achieve accura-
cies on predicate-argument relations between 80%
and 87%, which show the feasibility and scalabil-
ity of this approach. However, while this is a sim-
ple method for a highly configurational language
like English, it is more difficult to extend to lan-
guages with more complex morphology or with
word orders that display more freedom. Hocken-
maier (2006) is the only study known to the au-
thors that applies this method to German, a lan-
guage that displays these properties.
This article reports on experiments where the
advantages of hand-written and derived grammars
</bodyText>
<page confidence="0.994264">
37
</page>
<note confidence="0.9989885">
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37–45,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.991620555555556">
are combined. Compared to previous deep gram-
mar extraction approaches, a more sophisticated
core grammar (in the framework of HPSG) is cre-
ated. Also, more detailed syntactic features are
learnt from the resource treebank, which leads to
a more precise lexicon. Parsing results are com-
pared with GG (German Grammar), a previously
hand-written German HPSG grammar (M¨uller,
2002; Crysmann, 2003; Crysmann, 2005).
</bodyText>
<sectionHeader confidence="0.991941" genericHeader="method">
2 Core grammar
</sectionHeader>
<subsectionHeader confidence="0.998389">
2.1 Head-driven phrase structure grammar
</subsectionHeader>
<bodyText confidence="0.999924869565218">
This study has been entirely embedded in the
HPSG framework (Pollard and Sag, 1994). This
is a heavily lexicalised, constraint-based theory of
syntax, and it uses typed feature structures as its
representation. HPSG introduces a small num-
ber of principles (most notably, the Head Feature
Principle) that guide the construction of a few Im-
mediate Dominance schemata. These schemata
are meant to be the sole basis to combine words
and phrases. Examples of schemata are head-
complement, head-subject, head-specifier, head-
filler (for long-distance dependencies) and head-
modifier.
In this study, the core grammar is an extension
of the off-the-shelf version of HPSG. The type hi-
erarchy is organised by a typed feature structure
hierarchy (Carpenter, 1992), and can be read by
the LKB system (Copestake, 2002) and the PET
parser (Callmeier, 2000). The output is given in
Minimal Recursion Semantics (Copestake et al.,
2005) format, which can be minimally described
as a way to include scope information in depen-
dency output.
</bodyText>
<subsectionHeader confidence="0.999511">
2.2 The German language
</subsectionHeader>
<bodyText confidence="0.998631181818182">
Not unlike English, German uses verb position
to distinguish between different clause types. In
declarative sentences, verbs are positioned in the
second position, while subordinate classes are
verb-final. Questions and imperatives are verb-
initial. However, German displays some more
freedom with respect to the location of subjects,
complements and adjuncts: they can be scram-
bled rather freely. The following sentences are
all grammatical, and have approximately the same
meaning:
</bodyText>
<equation confidence="0.8494975">
(1) a. Der Pr¨asident hat
The.NOM President.NOM has
gestern das Buch
yesterday the.ACC book.ACC
gelesen.
read.PERF.
</equation>
<bodyText confidence="0.9570685">
‘The president read the book yester-
day’
</bodyText>
<listItem confidence="0.9803055">
b. Gestern hat der Pr¨asident das Buch
gelesen.
c. Das Buch hat der Pr¨asident gestern
gelesen.
</listItem>
<bodyText confidence="0.947315073170731">
As can be seen, the main verb is placed at sec-
ond position (the so-called ‘left bracket’), but all
other verbs remain at the end of the sentence,
in the ‘right bracket’. Most linguistic theories
about German recognise the existence of topolog-
ical fields: the Vorfeld before the left bracket, the
Mittelfeld between both brackets, and the Nach-
feld after the right bracket. The first two are
mainly used for adjuncts and arguments, whereas
the Nachfeld is typically, but not necessarily, used
for extraposed material (e.g. relative clauses or
comparative phrases) and some VPs. Again, the
following examples mean roughly the same:
(2) a. Er hat das Buch,
He has the.ACC Book.ACC,
empfohlen hat, gelesen.
recommended has, read.PERF.
He has read the book that she recom-
mended.
b. Er hat das Buch gelesen, das sie emp-
fohlen hat.
c. Das Buch hat er gelesen, das sie emp-
fohlen hat.
Another distinctive feature of German is its rela-
tively rich morphology. Nominals are marked with
case, gender and number, and verbs with number,
person, tense and mood. Adjectives and nouns
have to agree with respect to gender, number and
declension type, the latter being determined by
the (non-)existence and type of determiner used
in the noun phrase. Verbs and subjects have to
agree with respect to number and person. Ger-
man also displays highly productive noun com-
pounding, which amplifies the need for effective
unknown word handling. Verb particles can ei-
ther be separated from or concatenated to the verb:
compare ‘Er schl¨aft aus’ (‘He sleeps in’) and ‘Er
sie
she
das
that
</bodyText>
<page confidence="0.991335">
38
</page>
<figure confidence="0.745788">
arbeiten
</figure>
<figureCaption confidence="0.9826905">
Figure 1: This figure shows a (simplified) parse tree of the sentence ‘Amerikaner m¨ussen hart arbeiten’
(‘Americans have to work hard’).
</figureCaption>
<figure confidence="0.999560410714286">
�
�
1
hart
m¨ussen
verb-inf
VAL [ SUBJ hnp-nomi ]
SLASH hi
1
�
�
�no-det
VAL SPEC
I
SUBCAT hi
head-cluster
VAL [SUBJ hi]
SLASH hnp-nomi
�
�
1
�
�
filler-head
VAL [SUBJ hi]
SLASH hi
�noun
VAL SPEC hdeti 1
SUBCAT hi J
Amerikaner
verb
VAL 1
SLASH 2 �
�XCOMP
mod-head
VAL [SUBJ hi]
SLASH hnp-nomi
slash-subj
VAL [SUBJ hi]
SLASH hnp-nomi
�
�
�
� � � � � � �
�
�
1
1
� adverb �
MOD verb
�
�
verb
VAL 1
SLASH 2
1�
</figure>
<bodyText confidence="0.997600692307692">
wird ausschlafen’ (‘He will sleep in’). In such
verbs, the word ‘zu’ (which translates to the En-
glish ‘to’ in ‘to sleep’) can be infixed as well: ‘er
versucht auszuschlafen’ (‘He tries to sleep in’).
These characteristics make German a compar-
atively complex language to parse with CFGs:
more variants of the same lemma have to be mem-
orised, and the expansion of production rules will
be more diverse, with a less peaked statistical dis-
tribution. Efforts have been made to adapt existing
CFG models to German (Dubey and Keller, 2003),
but the results still don’t compare to state-of-the-
art parsing of English.
</bodyText>
<subsectionHeader confidence="0.998951">
2.3 Structure of the core grammar
</subsectionHeader>
<bodyText confidence="0.999983511627907">
The grammar uses the main tenets from Head-
driven Phrase Structure Grammar (Pollard and
Sag, 1994). However, different from earlier deep
grammar extraction studies, more sophisticated
structures are added. M¨uller (2002) proposes a
new schema (head-cluster) to account for verb
clusters in the right bracket, which includes the
possibility to merge subcategorisation frames of
e.g. object-control verbs and its dependent verb.
Separate rules for determinerless NPs, genitive
modification, coordination of common phrases,
relative phrases and direct speech are also created.
The free word order of German is accounted for
by scrambling arguments with lexical rules, and
by allowing adjuncts to be a modifier of unsat-
urated verb phrases. All declarative phrases are
considered to be head-initial, with an adjunct or
argument fronted using the SLASH feature, which
is then discharged using the head-filler schema.
The idea put forward by, among others, (Kiss and
Wesche, 1991) that all sentences should be right-
branching is linguistically pleasing, but turns out
be computationally very expensive (Crysmann,
2003), and the right-branching reading should be
replaced by a left-branching reading when the
right bracket is empty (i.e. when there is no auxil-
iary verb present).
An example of a sentence is presented in fig-
ure 1. It receives a right-branching analysis, be-
cause the infinitive ‘arbeiten’ resides in the right
bracket. The unary rule slash-subj moves the re-
quired subject towards the SLASH value, so that it
can be discharged in the Vorfeld by the head-filler
schema. ‘m¨ussen’ is an example of an argument
attraction verb, because it pulls the valence fea-
ture (containing SUBJ, SUBCAT etc; not visible
in the diagram) to itself. The head-cluster rule as-
sures that the VAL value then percolates upwards.
Because ‘Amerikaner’ does not have a specifier, a
separate unary rule (no-det) takes care of discharg-
ing the SPEC feature, before it can be combined
with the filler-head rule.
As opposed to (Hockenmaier, 2006), this study
</bodyText>
<page confidence="0.993584">
39
</page>
<figure confidence="0.999728181818182">
HD
SB
OC
VP
DA
NG HD
S
(a)
teure Detektive kann sich der Supermarkt nicht leisten
OA
NP
MO HD
DET HD
NP
(b)
NP
DET HD
MO HD
S
OA HD REFL SB MO VC
teure Detektive kann sich der Supermarkt nicht leisten
NP
</figure>
<figureCaption confidence="0.959274333333333">
Figure 2: (a) shows the original sentence, whereas (b) shows the sentence after preprocessing. Note that
NP is now headed, that the VP node is deleted, and that the verbal cluster is explicitly marked in (b). The
glossary of this sentence is ‘Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not
afford’
employs a core lexicon for words that have marked
semantic behaviour. These are usually closed
word classes, and include items such as raising
and auxiliary verbs, possessives, reflexives, arti-
cles, complementisers etc. The size of this core
lexicon is around 550 words. Note that, because
the core lexicon only contains function words, its
coverage is negligible without additional entries.
</figureCaption>
<sectionHeader confidence="0.85921" genericHeader="method">
3 Derivation of the lexicon
</sectionHeader>
<subsectionHeader confidence="0.998723">
3.1 The Tiger treebank
</subsectionHeader>
<bodyText confidence="0.99999248">
The Tiger treebank (Brants et al., 2002) is a tree-
bank that embraces the concept of constituency,
but can have crossing branches, i.e. the tree might
be non-projective. This allowed the annotators to
capture the German free word order. Around one-
third of the sentences received a non-projective
analysis. An example can be found in figure 2.
Additionally, it annotates each branch with a syn-
tactic function.
The text comes from a German newspaper
(Frankfurter Rundschau). It was annotated semi-
automatically, using a cascaded HMM model. Af-
ter each phase of the HMM model, the output was
corrected by human annotators. The corpus con-
sists of over 50,000 sentences, with an average
sentence length of 17.6 tokens (including punc-
tuation). The treebank employs 26 phrase cate-
gories, 56 PoS tags and 48 edge labels. It also en-
codes number, case and gender at the noun termi-
nals, and tense, person, number and mood at verbs.
Whether a verb is finite, an infinitive or a partici-
ple is encoded in the PoS tag. A peculiarity in the
annotation of noun phrases is the lack of headed-
ness, which was meant to keep the annotation as
theory-independent as reasonably possible.
</bodyText>
<subsectionHeader confidence="0.998348">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9985415">
A number of changes had to be applied to the tree-
bank to facilitate the read-off procedure:
</bodyText>
<listItem confidence="0.960731888888889">
• A heuristic head-finding procedure is applied
in the spirit of (Magerman, 1995). We use
priority lists to find the NP’s head, deter-
miner, appositions and modifiers. PPs and
CPs are also split into a head and its depen-
dent.
• If a verb has a separated verb particle, this
particle is attached to the lemma of the verb.
For instance, if the verb ‘schlafen’ has a parti-
cle ‘aus’, the lemma will be turned into ‘auss-
chlafen’ (‘sleep in’). If this is not done, sub-
categorisation frames will be attributed to the
wrong lemma.
• Sentences with auxiliaries are non-projective,
if the adjunct of the embedded VP is in the
Vorfeld. This can be solved by flattening the
tree (removing the VP node), and marking
the verbal cluster (VC) explicitly. See fig-
ure 2 for an example. 67.6% of the origi-
nal Tiger treebank is projective, and with this
procedure, this is lifted to 80.1%.
• The Tiger treebank annotates reflexive pro-
nouns with the PoS tag PRF, but does not
distinguish the syntactic role. Therefore, if a
verb has an object that has PRF as its part-of-
speech, the label of that edge is changed into
REFL, so that reflexive verbs can be found.
</listItem>
<page confidence="0.966673">
40
</page>
<figure confidence="0.997431555555556">
(a) 50000 (b)
1000 (c)
0.5
40000 800 0.4
30000 600 0.3
20000 400 0.2
10000 200 0.1
0 0 0
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
</figure>
<figureCaption confidence="0.831653333333333">
Figure 3: These graphs show learning curves of the algorithm on the first 45,000 sentences of the Tiger
treebank. Graph (a) indicates the amount of lemmas learnt (from top to bottom: nouns, names, adjec-
tives, verbs and adverbs). The graph in (b) shows the number of distinct lexical types for verbs that are
</figureCaption>
<bodyText confidence="0.9639276">
learnt. Graph (c) shows the average proportion of morphological forms that is observed per verb lemma,
assuming that each verb has 28 different forms: infinitive, zu (to) + infinitive, participle, imperative and
24 finite forms (3 (person) * 2 (number) * 2 (tense) * 2 (mood)).
The preprocessing stage failed in 1.1% of the
instances.
</bodyText>
<subsectionHeader confidence="0.869173">
3.3 Previous work
</subsectionHeader>
<bodyText confidence="0.99989624">
The method described in Hockenmaier (2006) first
converts the Tiger analysis to a tree, after which
the lexical types were derived. Because it was
the author’s goal to convert all sentences, some
rather crude actions had to be taken to render
non-projective trees projective: whenever a cer-
tain node introduces non-projectivity, some of its
daughters are moved to the parent tree, until that
node is projective. Below, we give two examples
where this will lead to incorrect semantic compo-
sition, with the consequence of flawed lexicon en-
tries. We argue that it is questionable whether the
impressive conversion scores actually represent a
high conversion quality. It would be interesting to
see how this grammar performs in a real parsing
task, but no such study has been carried out so far.
The first case deals with extraposed material in
the Nachfeld. Typical examples include relative
phrases, comparatives and PH/RE constructions2.
2NPs, AVPs and PPs can, instead of their usual headed
structure, be divided in two parts: a ‘placeholder’ and
a ‘repeated element’. These nodes often introduce non-
projectivity, and it is not straightforward to create a valid lin-
guistic analysis for these phenomena. Example sentences of
these categories (NPs, AVPs and PPs, respectively) are:
</bodyText>
<listItem confidence="0.993553666666667">
(1) [ PH Es ] ist wirklich schwer zu sagen, [ RE welche
Positionen er einnimmt ]
(2) Man muß sie also [ PH so ] behandeln , [ RE wie man
eine Weltanschauungsbewegung behandelt ]
(3) Alles deutet [ PH darauf ] hin [ RE daß sie es nicht
schaffen wird ]
</listItem>
<bodyText confidence="0.977663416666667">
These examples all have the RE in the Nachfeld, but their
placement actually has a large variety.
The consequence is that the head of the extraposed
material will be connected to the verb, instead of
to the genuine head.
Another example where Hockenmaier’s algo-
rithm will create incorrect lexical entries is when
the edge label is PAR (for ‘parentheses’) or in the
case of appositions. Consider the following sen-
tence:
(3) mit 160 Planstellen (etliche sind
with 160 permanent posts (several are
allerdings noch unbesetzt)
however still unoccupied)
The conclusion that will be drawn from this sen-
tence is that ‘sind’ can modify nouns, which is
only true due to the parentheses, and has no re-
lation with the specific characteristics of ‘sind’.
Similarly, appositions will act as modifiers of
nouns. Although one might argue that this is the
canonical CCG derivation for these phenomena, it
is not in the spirit of the HPSG grammars, and we
believe that these constructions are better handled
in rules than in the lexicon.
</bodyText>
<subsectionHeader confidence="0.957035">
3.4 Procedure
</subsectionHeader>
<bodyText confidence="0.999989727272727">
In our approach, we will be more conservative,
and the algorithm will only add facts to its knowl-
edge base if the evidence is convincing. That
means that less Tiger graphs will get projective
analyses, but that doesn’t have to be a curse: we
can derive lexical types from non-projective anal-
yses just as well, and leave the responsibility for
solving the more complex grammatical phenom-
ena to the core grammar. For example, lexical
rules will deal with fronting and Mittelfeld scram-
bling, as we have stated before. This step of the
</bodyText>
<page confidence="0.998728">
41
</page>
<bodyText confidence="0.999556571428571">
procedure has indeed strong affinity with deep lex-
ical acquisition, except for the fact that in DLA all
lexical types are known, and this is not the case in
this study: the hand-written lexical type hierarchy
is still extended with new types that are derived
from the resource treebank, mostly for verbs.
The basic procedure is as follows:
</bodyText>
<listItem confidence="0.999366">
• Traverse the graph top-down.
• For each node:
</listItem>
<bodyText confidence="0.900032">
– Identify the node’s head (or the deepest
verb in the verb cluster3);
– For each complement of this node, add
this complement to the head’s subcate-
gorisation frame.
– For each modifier, add this head to the
possible MOD values of the modifier’s
head.
</bodyText>
<listItem confidence="0.83893325">
• For each lexical item, a mapping of (lemma,
morphology) —* word form is created.
After this procedure, the following information
is recorded for the verb lemma ‘leisten’ from fig-
ure 2:
• It has a subcategorisation frame ‘npnom-refl-
npacc’.
• Its infinitive form is ‘leisten’.
</listItem>
<bodyText confidence="0.999962166666667">
The core grammar defines that possible sub-
jects are nominative NPs, expletive ‘es’ and CPs.
Expletives are considered to be entirely syntac-
tic (and not semantic), so they will not receive a
dependency relation. Complements may include
predicative APs, predicative NPs, genitive, dative
and accusative NPs, prepositional complements,
CPs, reflexives, separable particles (also purely
syntactic), and any combination of these. For non-
verbs, the complements are ordered (i.e. it is a
list, and not a verb). Verb complementation pat-
terns are sets, which means that duplicate com-
plements are not allowed. For verbs, it is also
recorded whether the auxiliary verb to mark the
perfect tense should be either ‘haben’ (default) or
‘sein’ (mostly verbs that have to do with move-
ment). Nouns are annotated with whether they can
have appositions or not.
</bodyText>
<footnote confidence="0.967282333333333">
3That means that the head of a S/VP-node is assumed
to be contained in the lexicon, as it must be some sort of
auxiliary.
</footnote>
<bodyText confidence="0.999602181818182">
Results from the derivation procedure are
graphed in figure 3. The number of nouns and
names is still growing after 45,000 sentences,
which is an expected result, given the infinite na-
ture of names and frequent noun compounding.
However, it appears that verbs, adjectives and ad-
verbs are converging to a stable level. On the other
hand, lexical types are still learnt, and this shows a
downside of our approach: the deeper the extrac-
tion procedure is, the more data is needed to reach
the same level of learning.
The core grammar contains a little less than 100
lexical types, and on top of that, 636 lexical types
are learnt, of which 579 are for verbs. It is inter-
esting to see that the number of lexical types is
considerably lower than in (Hockenmaier, 2006),
where around 2,500 lexical types are learnt. This
shows that our approach has a higher level of gen-
eralisation, and is presumably a consequence of
the fact that the German CCG grammar needs dis-
tinct lexical types for verb-initial and verb-final
constructions, and for different argument scram-
blings in the Mittelfeld, whereas in our approach,
hand-written lexical rules are used to do the scram-
bling.
The last graph shows that the number of word
forms is still insufficient. We assume that each
verb can have 28 different word forms. As can be
seen, it is clear that only a small part of this area
is learnt. One direction for future research might
be to find ways to automatically expand the lexi-
con after the derivation procedure, or to hand-code
morphological rules in the core grammar.
</bodyText>
<sectionHeader confidence="0.996228" genericHeader="method">
4 Parsing
</sectionHeader>
<subsectionHeader confidence="0.978574">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.999975066666667">
All experiments in this article use the first 45,000
sentences as training data, and the consecutive
5,000 sentences as test data. The remaining 472
sentences are not used. We used the PET parser
(Callmeier, 2000) to do all parsing experiments.
The parser was instructed to yield a parse error af-
ter 50,000 passive edges were used. Ambiguity
packing (Oepen and Carroll, 2000) and selective
unpacking (Zhang et al., 2007) were used to re-
duce memory footprint and speed up the selection
of the top-1000 analyses. The maximum entropy
model, used for selective unpacking, was based on
200 treebanked sentences of up to 20 words from
the training set. Part-of-speech tags delivered by
the stock version of the TnT tagger (Brants, ) were
</bodyText>
<page confidence="0.998049">
42
</page>
<table confidence="0.999786111111111">
Tiger T.+TnT GG
Out of vocabulary 71.9 % 5.2 % 55.6 %
Parse error 0.2 % 1.5 % 0.2 %
Unparsed 7.9 % 37.7 % 28.2 %
Parsed 20.0 % 55.6 % 16.0 %
Total 100.0 % 100.0 % 100.0 %
Avg. length 8.6 12.8 8.0
Avg. nr. of parses 399.0 573.1 19.2
Avg. time (s) 9.3 15.8 11.6
</table>
<tableCaption confidence="0.9024025">
Table 1: This table shows coverage results on the held-out test set. The first column denotes how the
extracted grammar performs without unknown word guessing. The second column uses PoS tags and
</tableCaption>
<bodyText confidence="0.6945406">
generic types to guide the grammar when an unknown word is encountered. The third column is the
performance of the fully hand-written HPSG German grammar by (M¨uller, 2002; Crysmann, 2003).
OOV stands for out-of-vocabulary. A parse error is recorded when the passive edge limit (set to 50,000)
has been reached. The bottom three rows only gives information about the sentences where the grammar
actually returns at least one parse.
</bodyText>
<table confidence="0.999925428571429">
Training set Test set
All 100.0 % 100.0 %
Avg. length 14.2 13.5
Coverage 79.0 % 69.0 %
Avg. length 13.2 12.8
Correct (top-1000) 52.0% 33.5 %
Avg. length 10.4 8.5
</table>
<tableCaption confidence="0.998147">
Table 2: Shown are the treebanking results, giv-
</tableCaption>
<bodyText confidence="0.974952090909091">
ing an impression of the quality of the parses.
The ‘training set’ and ‘test set’ are subsets of 200
sentences from the training and test set, respec-
tively. ‘Coverage’ means that at least one analysis
is found, and ‘correct’ indicates that the perfect
solution was found in the top-1000 parses.
used when unknown word handling was turned
on. These tags were connected to generic lexical
types by a hand-written mapping. The version of
GG that was employed (M¨uller, 2002; Crysmann,
2003) was dated October 20084.
</bodyText>
<subsectionHeader confidence="0.824661">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.999103833333333">
Table 1 shows coverage figures in three different
settings. It is clear that the resulting grammar has
a higher coverage than the GG, but this comes at a
cost: more ambiguity, and possibly unnecessary
ambiguity. Remarkably, the average processing
time is lower, even when the sentence lengths and
</bodyText>
<footnote confidence="0.9842576">
4It should be noted that little work has gone in to provid-
ing unknown word handling mechanisms, and that is why we
didn’t include it in our results. However, in a CoNLL-2009
shared task paper (Zhang et al., 2009), a coverage of 28.6%
was reported when rudimentary methods were used.
</footnote>
<bodyText confidence="0.999836636363637">
ambiguity rates are higher. We attribute this to
the smaller feature structure geometry that is in-
troduced by the core grammar (compared to the
GG). Using unknown word handling immediately
improved the coverage, by a large margin. Larger
ambiguity rates were recorded, and the number of
parser errors slightly increased.
Because coverage does not imply quality, we
wanted to look at the results in a qualitative fash-
ion. We took a sample of 200 sentences from
both the training and the test set, where the ones
from the training set did not overlap with the set
used to train the MaxEnt model, so that both set-
tings were equally influenced by the rudimentary
MaxEnt model. We evaluated for how many sen-
tences the exactly correct parse tree could be found
among the top-1000 parses (see table 2). The dif-
ference between the performance on the training
and test set give an idea of how well the gram-
mar performs on unknown data: if the difference
is small, the grammar extends well to unseen data.
Compared to evaluating on lexical coverage, we
believe this is a more empirical estimation of how
close the acquisition process is to convergence.
Based on the kind of parse trees we observed,
the impression was that on both sets, performance
was reduced due to the limited predictive power
of the disambiguation model. There were quite
a few sentences for which good parses could be
expected, because all lexical entries were present.
This experiment also showed that there were sys-
tematic ambiguities that were introduced by in-
consistent annotation in the Tiger treebank. For in-
</bodyText>
<page confidence="0.999249">
43
</page>
<bodyText confidence="0.9999855">
stance, the word ‘ein’ was learnt as both a number
(the English ‘one’) and as an article (‘a’), leading
to spurious ambiguities for each noun phrase con-
taining the word ‘ein’, or one of its morphological
variants. These two factors reinforced each other:
if there is spurious ambiguity, it is even harder for
a sparsely trained disambiguation model to pull
the correct parse inside the top-1000.
The difference between the two ‘correct’ num-
bers in table 2 is rather large, meaning that the
’real’ coverage might seem disappointingly low.
Not unexpectedly, we found that the generic lex-
ical types for verbs (transitive verb, third person
singular) and nouns (any gender, no appositions
allowed) was not always correct, harming the re-
sults considerably.
A quantitative comparison between deep gram-
mars is always hard. Between DELPH-IN gram-
mars, coverage has been the main method of eval-
uation. However, this score does not reward rich-
ness of the semantic output. Recent evidence from
the ERG (Ytrestøl et al., 2009) suggests that the
ERG reaches a top-500 coverage of around 70%
on an unseen domain, a result that this experiment
did not approximate. The goal of GG is not com-
putational, but it serves as a testing ground for lin-
guistic hypotheses. Therefore, the developers have
never aimed at high coverage figures, and crafted
the GG to give more detailed analyses and to be
suited for both parsing and generation. We are
happy to observe that the coverage figures in this
study are higher than GG’s (Zhang et al., 2009),
but we realise the limited value of this evaluation
method. Future studies will certainly include a
more granular evaluation of the grammar’s perfor-
mance.
</bodyText>
<sectionHeader confidence="0.994931" genericHeader="conclusions">
5 Conclusion and discussion
</sectionHeader>
<bodyText confidence="0.999983818181818">
We showed how a precise, wide-coverage HPSG
grammar for German can be created successfully,
by constructing a core grammar by hand, and ap-
pending it with linguistic information from the
Tiger treebank. Although this extracted gram-
mar suffers considerably more from overgenera-
tion than the hand-written GG, we argue that our
conservative derivation procedure delivers a more
detailed, compact and correct compared to pre-
vious deep grammar extraction efforts. The use
of the core lexicon allows us to have more lin-
guistically motivated analyses of German than ap-
proaches where the core lexicon only comprises
the textbook principles/operators. We compared
our lexicon extraction results to those from (Hock-
enmaier, 2006). Also, preliminary parsing exper-
iments are reported, in which we show that this
grammar produces reasonable coverage on unseen
text.
Although we feel confident about the successful
acquisition of the grammar, there still remain some
limiting factors in the performance of the grammar
when actually parsing. Compared to coverage fig-
ures of around 80%, reported by (Riezler et al.,
2001), the proportion of parse forests containing
the correct parse in this study is rather low. The
first limit is the constructional coverage, mean-
ing that the core grammar is not able to construct
the correct analysis, even though all lexical en-
tries have been derived correctly before. The most
frequent phenomena that are not captured yet are
PH/RE constructions and extraposed clauses, and
we plan to do an efficient implementation (Crys-
mann, 2005) of these in a next version of the gram-
mar. Second, as shown in figure 3, data scarcity in
the learning of the surface forms of lemmas neg-
atively influences the parser’s performance on un-
seen text.
In this paper, we focused mostly on the cor-
rectness of the derivation procedure. We would
like to address the real performance of the gram-
mar/parser combination in future work, which can
only be done when parses are evaluated according
to a more granular method than we have done in
this study. Furthermore, we ran into the issue that
there is no straightforward way to train larger sta-
tistical models automatically, which is due to the
fact that our approach does not convert the source
treebank to the target formalism’s format (in our
case HPSG), but instead reads off lexical types
and lexical entries directly. We plan to investigate
possibilities to have the annotation be guided auto-
matically by the Tiger treebank, so that the disam-
biguation model can be trained on a much larger
amount of training data.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99900425">
We would like to thank Rebecca Dridan, Antske
Fokkens, Stephan Oepen and the anonymous re-
viewers for their valuable contributions to this pa-
per.
</bodyText>
<page confidence="0.998862">
44
</page>
<sectionHeader confidence="0.998337" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999743525252525">
T. Brants. TnT: a statistical part-of-speech tagger. In
Proceedings of the Sixth Conference on Applied Nat-
ural Language Processing.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, pages 24–41.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In International Conference On Computational Lin-
guistics, pages 1–7.
A. Cahill, M. Burke, R. ODonovan, J. Van Genabith,
and A. Way. 2004. Long-distance dependency
resolution in automatically acquired wide-coverage
PCFG-based LFG approximations. In Proceedings
of ACL-2004, pages 320–327.
U. Callmeier. 2000. PET–a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99–107.
B. Carpenter. 1992. The Logic of Typed Feature Struc-
tures: With Applications to Unification Grammars,
Logic Programs, and Constraint Resolution. Cam-
bridge University Press, Cambridge, UK.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking.
In Proceedings of ACL-2005, pages 173–180.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2005. Minimal Recursion Semantics: An Intro-
duction. Research on Language &amp; Computation,
3(4):281–332.
A. Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications, Stanford,
CA, USA.
B. Crysmann. 2003. On the efficient implementation
of German verb placement in HPSG. In Proceedings
of RANLP-2003, pages 112–116.
B. Crysmann. 2005. Relative Clause Extraposition in
German: An Efficient and Portable Implementation.
Research on Language &amp; Computation, 3(1):61–82.
A. Dubey and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics, pages 96–103.
D. Flickinger. 2000. On building a more effcient gram-
mar by exploiting types. Natural Language Engi-
neering, 6(1):15–28.
J. Hockenmaier and M. Steedman. 2002. Acquiring
compact lexicalized grammars from a cleaner tree-
bank. In Proceedings of LREC-2002, pages 1974–
1981.
J. Hockenmaier. 2006. Creating a CCGbank and a
Wide-Coverage CCG Lexicon for German. In Pro-
ceedings of ACL-2006, pages 505–512.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. Handbook of formal languages, 3:69–
124.
R.M. Kaplan and J. Bresnan. 1995. Lexical-Functional
Grammar: A formal system for grammatical rep-
resentation. Formal Issues in Lexical-Functional
Grammar, pages 29–130.
T. Kiss and B. Wesche. 1991. Verb order and head
movement. Text Understanding in LILOG, Lecture
Notes in Artificial Intelligence, 546:216–242.
D. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of ACL-1995, pages
276–283.
Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpus-
oriented grammar development for acquiring a
Head-driven Phrase Structure Grammar from the
Penn Treebank. In Proceedings of IJCNLP-2004.
S. M¨uller. 2002. Complex Predicates: Verbal
Complexes, Resultative Constructions, and Particle
Verbs in German. CSLI Publications, Stanford, CA,
USA.
S. Oepen and J. Carroll. 2000. Ambiguity packing in
constraint-based parsing: practical results. In Pro-
ceedings of NAACL-2000, pages 162–169.
C.J. Pollard and I.A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press,
Chicago, IL, USA.
S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell III, and M. Johnson. 2001. Parsing
the Wall Street Journal using a Lexical-Functional
Grammar and discriminative estimation techniques.
In Proceedings ofACL-2001, pages 271–278.
M. Steedman. 1996. Surface structure and interpreta-
tion. MIT Press, Cambridge, MA, USA.
G. Ytrestøl, D. Flickinger, and S. Oepen. 2009. Ex-
tracting and Annotating Wikipedia Sub-Domains.
In Proceedings of the Seventh International Work-
shop on Treebanks and Linguistic Theories, pages
185–197.
Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency
in Unification-Based N-Best Parsing. In Proceed-
ings of the Tenth International Conference on Pars-
ing Technologies, pages 48–59.
Y. Zhang, R. Wang, and S.. Oepen. 2009. Hybrid Mul-
tilingual Parsing with HPSG for SRL. In Proceed-
ings of CoNLL-2009, to appear.
</reference>
<page confidence="0.999391">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713335">
<title confidence="0.99878">Construction of a German HPSG grammar from a detailed treebank</title>
<author confidence="0.999383">Yi</author>
<affiliation confidence="0.9771465">of Computational Linguistics &amp; Phonetics, Saarland University, German Research Center for Artificial Intelligence,</affiliation>
<abstract confidence="0.981879285714286">Grammar extraction in deep formalisms has received remarkable attention in recent years. We recognise its value, but try to create a more precision-oriented grammar, by hand-crafting a core grammar, and learning lexical types and lexical items from a treebank. The study we performed focused on German, and we used the Tiger treebank as our resource. A completely hand-written grammar in the framework of HPSG forms the inspiration for our core grammar, and is also our frame of referfor evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>T Brants</author>
</authors>
<title>TnT: a statistical part-of-speech tagger.</title>
<booktitle>In Proceedings of the Sixth Conference on Applied Natural Language Processing.</booktitle>
<marker>Brants, </marker>
<rawString>T. Brants. TnT: a statistical part-of-speech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER Treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>24--41</pages>
<contexts>
<context position="12026" citStr="Brants et al., 2002" startWordPosition="1942" endWordPosition="1945">is explicitly marked in (b). The glossary of this sentence is ‘Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not afford’ employs a core lexicon for words that have marked semantic behaviour. These are usually closed word classes, and include items such as raising and auxiliary verbs, possessives, reflexives, articles, complementisers etc. The size of this core lexicon is around 550 words. Note that, because the core lexicon only contains function words, its coverage is negligible without additional entries. 3 Derivation of the lexicon 3.1 The Tiger treebank The Tiger treebank (Brants et al., 2002) is a treebank that embraces the concept of constituency, but can have crossing branches, i.e. the tree might be non-projective. This allowed the annotators to capture the German free word order. Around onethird of the sentences received a non-projective analysis. An example can be found in figure 2. Additionally, it annotates each branch with a syntactic function. The text comes from a German newspaper (Frankfurter Rundschau). It was annotated semiautomatically, using a cascaded HMM model. After each phase of the HMM model, the output was corrected by human annotators. The corpus consists of </context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002. The TIGER Treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, pages 24–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>H Dyvik</author>
<author>T H King</author>
<author>H Masuichi</author>
<author>C Rohrer</author>
</authors>
<title>The parallel grammar project.</title>
<date>2002</date>
<booktitle>In International Conference On Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="1465" citStr="Butt et al., 2002" startWordPosition="220" endWordPosition="223">eebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cannot describe correctly. The increased linguistic preciseness comes at a cost, though: such grammars have a lower out-</context>
</contexts>
<marker>Butt, Dyvik, King, Masuichi, Rohrer, 2002</marker>
<rawString>M. Butt, H. Dyvik, T.H. King, H. Masuichi, and C. Rohrer. 2002. The parallel grammar project. In International Conference On Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cahill</author>
<author>M Burke</author>
<author>R ODonovan</author>
<author>J Van Genabith</author>
<author>A Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-2004,</booktitle>
<pages>320--327</pages>
<marker>Cahill, Burke, ODonovan, Van Genabith, Way, 2004</marker>
<rawString>A. Cahill, M. Burke, R. ODonovan, J. Van Genabith, and A. Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage PCFG-based LFG approximations. In Proceedings of ACL-2004, pages 320–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
</authors>
<title>PET–a platform for experimentation with efficient HPSG processing techniques.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>01</issue>
<contexts>
<context position="5290" citStr="Callmeier, 2000" startWordPosition="826" endWordPosition="827">ces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier. In this study, the core grammar is an extension of the off-the-shelf version of HPSG. The type hierarchy is organised by a typed feature structure hierarchy (Carpenter, 1992), and can be read by the LKB system (Copestake, 2002) and the PET parser (Callmeier, 2000). The output is given in Minimal Recursion Semantics (Copestake et al., 2005) format, which can be minimally described as a way to include scope information in dependency output. 2.2 The German language Not unlike English, German uses verb position to distinguish between different clause types. In declarative sentences, verbs are positioned in the second position, while subordinate classes are verb-final. Questions and imperatives are verbinitial. However, German displays some more freedom with respect to the location of subjects, complements and adjuncts: they can be scrambled rather freely. </context>
<context position="21974" citStr="Callmeier, 2000" startWordPosition="3656" endWordPosition="3657">ph shows that the number of word forms is still insufficient. We assume that each verb can have 28 different word forms. As can be seen, it is clear that only a small part of this area is learnt. One direction for future research might be to find ways to automatically expand the lexicon after the derivation procedure, or to hand-code morphological rules in the core grammar. 4 Parsing 4.1 Methodology All experiments in this article use the first 45,000 sentences as training data, and the consecutive 5,000 sentences as test data. The remaining 472 sentences are not used. We used the PET parser (Callmeier, 2000) to do all parsing experiments. The parser was instructed to yield a parse error after 50,000 passive edges were used. Ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007) were used to reduce memory footprint and speed up the selection of the top-1000 analyses. The maximum entropy model, used for selective unpacking, was based on 200 treebanked sentences of up to 20 words from the training set. Part-of-speech tags delivered by the stock version of the TnT tagger (Brants, ) were 42 Tiger T.+TnT GG Out of vocabulary 71.9 % 5.2 % 55.6 % Parse error 0.2 % 1.5 % </context>
</contexts>
<marker>Callmeier, 2000</marker>
<rawString>U. Callmeier. 2000. PET–a platform for experimentation with efficient HPSG processing techniques. Natural Language Engineering, 6(01):99–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures: With Applications to Unification Grammars, Logic Programs, and Constraint Resolution.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="5200" citStr="Carpenter, 1992" startWordPosition="810" endWordPosition="811">theory of syntax, and it uses typed feature structures as its representation. HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier. In this study, the core grammar is an extension of the off-the-shelf version of HPSG. The type hierarchy is organised by a typed feature structure hierarchy (Carpenter, 1992), and can be read by the LKB system (Copestake, 2002) and the PET parser (Callmeier, 2000). The output is given in Minimal Recursion Semantics (Copestake et al., 2005) format, which can be minimally described as a way to include scope information in dependency output. 2.2 The German language Not unlike English, German uses verb position to distinguish between different clause types. In declarative sentences, verbs are positioned in the second position, while subordinate classes are verb-final. Questions and imperatives are verbinitial. However, German displays some more freedom with respect to</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>B. Carpenter. 1992. The Logic of Typed Feature Structures: With Applications to Unification Grammars, Logic Programs, and Constraint Resolution. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine nbest parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL-2005,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="988" citStr="Charniak and Johnson, 2005" startWordPosition="141" endWordPosition="144"> in recent years. We recognise its value, but try to create a more precision-oriented grammar, by hand-crafting a core grammar, and learning lexical types and lexical items from a treebank. The study we performed focused on German, and we used the Tiger treebank as our resource. A completely hand-written grammar in the framework of HPSG forms the inspiration for our core grammar, and is also our frame of reference for evaluation. 1 1 Introduction Previous studies have shown that treebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the gram</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine nbest parsing and MaxEnt discriminative reranking. In Proceedings of ACL-2005, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Minimal Recursion Semantics: An Introduction.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5367" citStr="Copestake et al., 2005" startWordPosition="836" endWordPosition="839">iple) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier. In this study, the core grammar is an extension of the off-the-shelf version of HPSG. The type hierarchy is organised by a typed feature structure hierarchy (Carpenter, 1992), and can be read by the LKB system (Copestake, 2002) and the PET parser (Callmeier, 2000). The output is given in Minimal Recursion Semantics (Copestake et al., 2005) format, which can be minimally described as a way to include scope information in dependency output. 2.2 The German language Not unlike English, German uses verb position to distinguish between different clause types. In declarative sentences, verbs are positioned in the second position, while subordinate classes are verb-final. Questions and imperatives are verbinitial. However, German displays some more freedom with respect to the location of subjects, complements and adjuncts: they can be scrambled rather freely. The following sentences are all grammatical, and have approximately the same </context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>A. Copestake, D. Flickinger, C. Pollard, and I. Sag. 2005. Minimal Recursion Semantics: An Introduction. Research on Language &amp; Computation, 3(4):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA, USA.</location>
<contexts>
<context position="5253" citStr="Copestake, 2002" startWordPosition="820" endWordPosition="821">s as its representation. HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier. In this study, the core grammar is an extension of the off-the-shelf version of HPSG. The type hierarchy is organised by a typed feature structure hierarchy (Carpenter, 1992), and can be read by the LKB system (Copestake, 2002) and the PET parser (Callmeier, 2000). The output is given in Minimal Recursion Semantics (Copestake et al., 2005) format, which can be minimally described as a way to include scope information in dependency output. 2.2 The German language Not unlike English, German uses verb position to distinguish between different clause types. In declarative sentences, verbs are positioned in the second position, while subordinate classes are verb-final. Questions and imperatives are verbinitial. However, German displays some more freedom with respect to the location of subjects, complements and adjuncts: </context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>A. Copestake. 2002. Implementing Typed Feature Structure Grammars. CSLI Publications, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crysmann</author>
</authors>
<title>On the efficient implementation of German verb placement in HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003,</booktitle>
<pages>112--116</pages>
<contexts>
<context position="4376" citStr="Crysmann, 2003" startWordPosition="684" endWordPosition="685"> where the advantages of hand-written and derived grammars 37 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37–45, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP are combined. Compared to previous deep grammar extraction approaches, a more sophisticated core grammar (in the framework of HPSG) is created. Also, more detailed syntactic features are learnt from the resource treebank, which leads to a more precise lexicon. Parsing results are compared with GG (German Grammar), a previously hand-written German HPSG grammar (M¨uller, 2002; Crysmann, 2003; Crysmann, 2005). 2 Core grammar 2.1 Head-driven phrase structure grammar This study has been entirely embedded in the HPSG framework (Pollard and Sag, 1994). This is a heavily lexicalised, constraint-based theory of syntax, and it uses typed feature structures as its representation. HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (</context>
<context position="10096" citStr="Crysmann, 2003" startWordPosition="1619" endWordPosition="1620">nation of common phrases, relative phrases and direct speech are also created. The free word order of German is accounted for by scrambling arguments with lexical rules, and by allowing adjuncts to be a modifier of unsaturated verb phrases. All declarative phrases are considered to be head-initial, with an adjunct or argument fronted using the SLASH feature, which is then discharged using the head-filler schema. The idea put forward by, among others, (Kiss and Wesche, 1991) that all sentences should be rightbranching is linguistically pleasing, but turns out be computationally very expensive (Crysmann, 2003), and the right-branching reading should be replaced by a left-branching reading when the right bracket is empty (i.e. when there is no auxiliary verb present). An example of a sentence is presented in figure 1. It receives a right-branching analysis, because the infinitive ‘arbeiten’ resides in the right bracket. The unary rule slash-subj moves the required subject towards the SLASH value, so that it can be discharged in the Vorfeld by the head-filler schema. ‘m¨ussen’ is an example of an argument attraction verb, because it pulls the valence feature (containing SUBJ, SUBCAT etc; not visible </context>
<context position="23140" citStr="Crysmann, 2003" startWordPosition="3865" endWordPosition="3866">lary 71.9 % 5.2 % 55.6 % Parse error 0.2 % 1.5 % 0.2 % Unparsed 7.9 % 37.7 % 28.2 % Parsed 20.0 % 55.6 % 16.0 % Total 100.0 % 100.0 % 100.0 % Avg. length 8.6 12.8 8.0 Avg. nr. of parses 399.0 573.1 19.2 Avg. time (s) 9.3 15.8 11.6 Table 1: This table shows coverage results on the held-out test set. The first column denotes how the extracted grammar performs without unknown word guessing. The second column uses PoS tags and generic types to guide the grammar when an unknown word is encountered. The third column is the performance of the fully hand-written HPSG German grammar by (M¨uller, 2002; Crysmann, 2003). OOV stands for out-of-vocabulary. A parse error is recorded when the passive edge limit (set to 50,000) has been reached. The bottom three rows only gives information about the sentences where the grammar actually returns at least one parse. Training set Test set All 100.0 % 100.0 % Avg. length 14.2 13.5 Coverage 79.0 % 69.0 % Avg. length 13.2 12.8 Correct (top-1000) 52.0% 33.5 % Avg. length 10.4 8.5 Table 2: Shown are the treebanking results, giving an impression of the quality of the parses. The ‘training set’ and ‘test set’ are subsets of 200 sentences from the training and test set, resp</context>
</contexts>
<marker>Crysmann, 2003</marker>
<rawString>B. Crysmann. 2003. On the efficient implementation of German verb placement in HPSG. In Proceedings of RANLP-2003, pages 112–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crysmann</author>
</authors>
<title>Relative Clause Extraposition in German: An Efficient and Portable Implementation.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="4393" citStr="Crysmann, 2005" startWordPosition="686" endWordPosition="687">tages of hand-written and derived grammars 37 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37–45, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP are combined. Compared to previous deep grammar extraction approaches, a more sophisticated core grammar (in the framework of HPSG) is created. Also, more detailed syntactic features are learnt from the resource treebank, which leads to a more precise lexicon. Parsing results are compared with GG (German Grammar), a previously hand-written German HPSG grammar (M¨uller, 2002; Crysmann, 2003; Crysmann, 2005). 2 Core grammar 2.1 Head-driven phrase structure grammar This study has been entirely embedded in the HPSG framework (Pollard and Sag, 1994). This is a heavily lexicalised, constraint-based theory of syntax, and it uses typed feature structures as its representation. HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance</context>
<context position="29536" citStr="Crysmann, 2005" startWordPosition="4923" endWordPosition="4925">l remain some limiting factors in the performance of the grammar when actually parsing. Compared to coverage figures of around 80%, reported by (Riezler et al., 2001), the proportion of parse forests containing the correct parse in this study is rather low. The first limit is the constructional coverage, meaning that the core grammar is not able to construct the correct analysis, even though all lexical entries have been derived correctly before. The most frequent phenomena that are not captured yet are PH/RE constructions and extraposed clauses, and we plan to do an efficient implementation (Crysmann, 2005) of these in a next version of the grammar. Second, as shown in figure 3, data scarcity in the learning of the surface forms of lemmas negatively influences the parser’s performance on unseen text. In this paper, we focused mostly on the correctness of the derivation procedure. We would like to address the real performance of the grammar/parser combination in future work, which can only be done when parses are evaluated according to a more granular method than we have done in this study. Furthermore, we ran into the issue that there is no straightforward way to train larger statistical models </context>
</contexts>
<marker>Crysmann, 2005</marker>
<rawString>B. Crysmann. 2005. Relative Clause Extraposition in German: An Efficient and Portable Implementation. Research on Language &amp; Computation, 3(1):61–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
<author>F Keller</author>
</authors>
<title>Probabilistic parsing for German using sister-head dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="8876" citStr="Dubey and Keller, 2003" startWordPosition="1435" endWordPosition="1438">] SLASH hnp-nomi � � � � � � � � � � � � 1 1 � adverb � MOD verb � � verb VAL 1 SLASH 2 1� wird ausschlafen’ (‘He will sleep in’). In such verbs, the word ‘zu’ (which translates to the English ‘to’ in ‘to sleep’) can be infixed as well: ‘er versucht auszuschlafen’ (‘He tries to sleep in’). These characteristics make German a comparatively complex language to parse with CFGs: more variants of the same lemma have to be memorised, and the expansion of production rules will be more diverse, with a less peaked statistical distribution. Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don’t compare to state-of-theart parsing of English. 2.3 Structure of the core grammar The grammar uses the main tenets from Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). However, different from earlier deep grammar extraction studies, more sophisticated structures are added. M¨uller (2002) proposes a new schema (head-cluster) to account for verb clusters in the right bracket, which includes the possibility to merge subcategorisation frames of e.g. object-control verbs and its dependent verb. Separate rules for determinerless NPs, genitive modification, c</context>
</contexts>
<marker>Dubey, Keller, 2003</marker>
<rawString>A. Dubey and F. Keller. 2003. Probabilistic parsing for German using sister-head dependencies. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more effcient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="1492" citStr="Flickinger, 2000" startWordPosition="226" endWordPosition="227">onstructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cannot describe correctly. The increased linguistic preciseness comes at a cost, though: such grammars have a lower out-of-thebox coverage, i.e. th</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>D. Flickinger. 2000. On building a more effcient grammar by exploiting types. Natural Language Engineering, 6(1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
<author>M Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC-2002,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2688" citStr="Hockenmaier and Steedman, 2002" startWordPosition="414" endWordPosition="417">wer out-of-thebox coverage, i.e. they will not give an analysis on a certain portion of the corpus. Experiments have been conducted, where a lexicalised grammar is learnt from treebanks, a methodology for which we coin the name deep grammar extraction. The basic architecture of such an experiment is to convert the treebank to a format that is compatible with the chosen linguistic formalism, and read off the lexicon from that converted treebank. Because all these formalisms are heavily lexicalised, the core grammars only consist of a small number of principles or operators. In the case of CCG (Hockenmaier and Steedman, 2002), the core grammar consists of the operators that CCG stipulates: function application, composition and type-raising. Standard HPSG defines a few schemata, but these are usually adapted for a large-scale grammar. Miyao et al. (2004) tailor their core grammar for optimal use with the Penn Treebank and the English language, for example by adding a new schema for relative clauses. Hockenmaier and Steedman (2002), Miyao et al. (2004) and Cahill et al. (2004) show fairly good results on the Penn Treebank (for CCG, HPSG and LFG, respectively): these parsers achieve accuracies on predicate-argument r</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>J. Hockenmaier and M. Steedman. 2002. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of LREC-2002, pages 1974– 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hockenmaier</author>
</authors>
<title>Creating a CCGbank and a Wide-Coverage CCG Lexicon for German.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL-2006,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="3608" citStr="Hockenmaier (2006)" startWordPosition="564" endWordPosition="566">the English language, for example by adding a new schema for relative clauses. Hockenmaier and Steedman (2002), Miyao et al. (2004) and Cahill et al. (2004) show fairly good results on the Penn Treebank (for CCG, HPSG and LFG, respectively): these parsers achieve accuracies on predicate-argument relations between 80% and 87%, which show the feasibility and scalability of this approach. However, while this is a simple method for a highly configurational language like English, it is more difficult to extend to languages with more complex morphology or with word orders that display more freedom. Hockenmaier (2006) is the only study known to the authors that applies this method to German, a language that displays these properties. This article reports on experiments where the advantages of hand-written and derived grammars 37 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 37–45, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP are combined. Compared to previous deep grammar extraction approaches, a more sophisticated core grammar (in the framework of HPSG) is created. Also, more detailed syntactic features are learnt from the resource treebank, wh</context>
<context position="11006" citStr="Hockenmaier, 2006" startWordPosition="1770" endWordPosition="1771">s in the right bracket. The unary rule slash-subj moves the required subject towards the SLASH value, so that it can be discharged in the Vorfeld by the head-filler schema. ‘m¨ussen’ is an example of an argument attraction verb, because it pulls the valence feature (containing SUBJ, SUBCAT etc; not visible in the diagram) to itself. The head-cluster rule assures that the VAL value then percolates upwards. Because ‘Amerikaner’ does not have a specifier, a separate unary rule (no-det) takes care of discharging the SPEC feature, before it can be combined with the filler-head rule. As opposed to (Hockenmaier, 2006), this study 39 HD SB OC VP DA NG HD S (a) teure Detektive kann sich der Supermarkt nicht leisten OA NP MO HD DET HD NP (b) NP DET HD MO HD S OA HD REFL SB MO VC teure Detektive kann sich der Supermarkt nicht leisten NP Figure 2: (a) shows the original sentence, whereas (b) shows the sentence after preprocessing. Note that NP is now headed, that the VP node is deleted, and that the verbal cluster is explicitly marked in (b). The glossary of this sentence is ‘Expensive.ACC detectives.ACC can REFL the.NOM supermarket.NOM not afford’ employs a core lexicon for words that have marked semantic beha</context>
<context position="15242" citStr="Hockenmaier (2006)" startWordPosition="2517" endWordPosition="2518">ences of the Tiger treebank. Graph (a) indicates the amount of lemmas learnt (from top to bottom: nouns, names, adjectives, verbs and adverbs). The graph in (b) shows the number of distinct lexical types for verbs that are learnt. Graph (c) shows the average proportion of morphological forms that is observed per verb lemma, assuming that each verb has 28 different forms: infinitive, zu (to) + infinitive, participle, imperative and 24 finite forms (3 (person) * 2 (number) * 2 (tense) * 2 (mood)). The preprocessing stage failed in 1.1% of the instances. 3.3 Previous work The method described in Hockenmaier (2006) first converts the Tiger analysis to a tree, after which the lexical types were derived. Because it was the author’s goal to convert all sentences, some rather crude actions had to be taken to render non-projective trees projective: whenever a certain node introduces non-projectivity, some of its daughters are moved to the parent tree, until that node is projective. Below, we give two examples where this will lead to incorrect semantic composition, with the consequence of flawed lexicon entries. We argue that it is questionable whether the impressive conversion scores actually represent a hig</context>
<context position="20943" citStr="Hockenmaier, 2006" startWordPosition="3482" endWordPosition="3483">n expected result, given the infinite nature of names and frequent noun compounding. However, it appears that verbs, adjectives and adverbs are converging to a stable level. On the other hand, lexical types are still learnt, and this shows a downside of our approach: the deeper the extraction procedure is, the more data is needed to reach the same level of learning. The core grammar contains a little less than 100 lexical types, and on top of that, 636 lexical types are learnt, of which 579 are for verbs. It is interesting to see that the number of lexical types is considerably lower than in (Hockenmaier, 2006), where around 2,500 lexical types are learnt. This shows that our approach has a higher level of generalisation, and is presumably a consequence of the fact that the German CCG grammar needs distinct lexical types for verb-initial and verb-final constructions, and for different argument scramblings in the Mittelfeld, whereas in our approach, hand-written lexical rules are used to do the scrambling. The last graph shows that the number of word forms is still insufficient. We assume that each verb can have 28 different word forms. As can be seen, it is clear that only a small part of this area </context>
<context position="28701" citStr="Hockenmaier, 2006" startWordPosition="4790" endWordPosition="4792">ructing a core grammar by hand, and appending it with linguistic information from the Tiger treebank. Although this extracted grammar suffers considerably more from overgeneration than the hand-written GG, we argue that our conservative derivation procedure delivers a more detailed, compact and correct compared to previous deep grammar extraction efforts. The use of the core lexicon allows us to have more linguistically motivated analyses of German than approaches where the core lexicon only comprises the textbook principles/operators. We compared our lexicon extraction results to those from (Hockenmaier, 2006). Also, preliminary parsing experiments are reported, in which we show that this grammar produces reasonable coverage on unseen text. Although we feel confident about the successful acquisition of the grammar, there still remain some limiting factors in the performance of the grammar when actually parsing. Compared to coverage figures of around 80%, reported by (Riezler et al., 2001), the proportion of parse forests containing the correct parse in this study is rather low. The first limit is the constructional coverage, meaning that the core grammar is not able to construct the correct analysi</context>
</contexts>
<marker>Hockenmaier, 2006</marker>
<rawString>J. Hockenmaier. 2006. Creating a CCGbank and a Wide-Coverage CCG Lexicon for German. In Proceedings of ACL-2006, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars. Handbook of formal languages,</title>
<date>1997</date>
<pages>3--69</pages>
<contexts>
<context position="1349" citStr="Joshi and Schabes, 1997" startWordPosition="199" endWordPosition="202"> our core grammar, and is also our frame of reference for evaluation. 1 1 Introduction Previous studies have shown that treebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cann</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. Handbook of formal languages, 3:69– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Grammar: A formal system for grammatical representation. Formal Issues in Lexical-Functional Grammar,</title>
<date>1995</date>
<pages>29--130</pages>
<contexts>
<context position="1403" citStr="Kaplan and Bresnan, 1995" startWordPosition="208" endWordPosition="211"> for evaluation. 1 1 Introduction Previous studies have shown that treebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cannot describe correctly. The increased linguistic precis</context>
</contexts>
<marker>Kaplan, Bresnan, 1995</marker>
<rawString>R.M. Kaplan and J. Bresnan. 1995. Lexical-Functional Grammar: A formal system for grammatical representation. Formal Issues in Lexical-Functional Grammar, pages 29–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kiss</author>
<author>B Wesche</author>
</authors>
<title>Verb order and head movement. Text Understanding</title>
<date>1991</date>
<booktitle>in LILOG, Lecture Notes in Artificial Intelligence,</booktitle>
<pages>546--216</pages>
<contexts>
<context position="9959" citStr="Kiss and Wesche, 1991" startWordPosition="1598" endWordPosition="1601">bcategorisation frames of e.g. object-control verbs and its dependent verb. Separate rules for determinerless NPs, genitive modification, coordination of common phrases, relative phrases and direct speech are also created. The free word order of German is accounted for by scrambling arguments with lexical rules, and by allowing adjuncts to be a modifier of unsaturated verb phrases. All declarative phrases are considered to be head-initial, with an adjunct or argument fronted using the SLASH feature, which is then discharged using the head-filler schema. The idea put forward by, among others, (Kiss and Wesche, 1991) that all sentences should be rightbranching is linguistically pleasing, but turns out be computationally very expensive (Crysmann, 2003), and the right-branching reading should be replaced by a left-branching reading when the right bracket is empty (i.e. when there is no auxiliary verb present). An example of a sentence is presented in figure 1. It receives a right-branching analysis, because the infinitive ‘arbeiten’ resides in the right bracket. The unary rule slash-subj moves the required subject towards the SLASH value, so that it can be discharged in the Vorfeld by the head-filler schema</context>
</contexts>
<marker>Kiss, Wesche, 1991</marker>
<rawString>T. Kiss and B. Wesche. 1991. Verb order and head movement. Text Understanding in LILOG, Lecture Notes in Artificial Intelligence, 546:216–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-1995,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="13338" citStr="Magerman, 1995" startWordPosition="2167" endWordPosition="2168">e treebank employs 26 phrase categories, 56 PoS tags and 48 edge labels. It also encodes number, case and gender at the noun terminals, and tense, person, number and mood at verbs. Whether a verb is finite, an infinitive or a participle is encoded in the PoS tag. A peculiarity in the annotation of noun phrases is the lack of headedness, which was meant to keep the annotation as theory-independent as reasonably possible. 3.2 Preprocessing A number of changes had to be applied to the treebank to facilitate the read-off procedure: • A heuristic head-finding procedure is applied in the spirit of (Magerman, 1995). We use priority lists to find the NP’s head, determiner, appositions and modifiers. PPs and CPs are also split into a head and its dependent. • If a verb has a separated verb particle, this particle is attached to the lemma of the verb. For instance, if the verb ‘schlafen’ has a particle ‘aus’, the lemma will be turned into ‘ausschlafen’ (‘sleep in’). If this is not done, subcategorisation frames will be attributed to the wrong lemma. • Sentences with auxiliaries are non-projective, if the adjunct of the embedded VP is in the Vorfeld. This can be solved by flattening the tree (removing the V</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL-1995, pages 276–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>T Ninomiya</author>
<author>J Tsujii</author>
</authors>
<title>Corpusoriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP-2004.</booktitle>
<contexts>
<context position="2920" citStr="Miyao et al. (2004)" startWordPosition="450" endWordPosition="453">raction. The basic architecture of such an experiment is to convert the treebank to a format that is compatible with the chosen linguistic formalism, and read off the lexicon from that converted treebank. Because all these formalisms are heavily lexicalised, the core grammars only consist of a small number of principles or operators. In the case of CCG (Hockenmaier and Steedman, 2002), the core grammar consists of the operators that CCG stipulates: function application, composition and type-raising. Standard HPSG defines a few schemata, but these are usually adapted for a large-scale grammar. Miyao et al. (2004) tailor their core grammar for optimal use with the Penn Treebank and the English language, for example by adding a new schema for relative clauses. Hockenmaier and Steedman (2002), Miyao et al. (2004) and Cahill et al. (2004) show fairly good results on the Penn Treebank (for CCG, HPSG and LFG, respectively): these parsers achieve accuracies on predicate-argument relations between 80% and 87%, which show the feasibility and scalability of this approach. However, while this is a simple method for a highly configurational language like English, it is more difficult to extend to languages with m</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Y. Miyao, T. Ninomiya, and J. Tsujii. 2004. Corpusoriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of IJCNLP-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M¨uller</author>
</authors>
<title>Complex Predicates: Verbal Complexes, Resultative Constructions, and Particle Verbs in German.</title>
<date>2002</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA, USA.</location>
<marker>M¨uller, 2002</marker>
<rawString>S. M¨uller. 2002. Complex Predicates: Verbal Complexes, Resultative Constructions, and Particle Verbs in German. CSLI Publications, Stanford, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing: practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL-2000,</booktitle>
<pages>162--169</pages>
<contexts>
<context position="22136" citStr="Oepen and Carroll, 2000" startWordPosition="3681" endWordPosition="3684">only a small part of this area is learnt. One direction for future research might be to find ways to automatically expand the lexicon after the derivation procedure, or to hand-code morphological rules in the core grammar. 4 Parsing 4.1 Methodology All experiments in this article use the first 45,000 sentences as training data, and the consecutive 5,000 sentences as test data. The remaining 472 sentences are not used. We used the PET parser (Callmeier, 2000) to do all parsing experiments. The parser was instructed to yield a parse error after 50,000 passive edges were used. Ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007) were used to reduce memory footprint and speed up the selection of the top-1000 analyses. The maximum entropy model, used for selective unpacking, was based on 200 treebanked sentences of up to 20 words from the training set. Part-of-speech tags delivered by the stock version of the TnT tagger (Brants, ) were 42 Tiger T.+TnT GG Out of vocabulary 71.9 % 5.2 % 55.6 % Parse error 0.2 % 1.5 % 0.2 % Unparsed 7.9 % 37.7 % 28.2 % Parsed 20.0 % 55.6 % 16.0 % Total 100.0 % 100.0 % 100.0 % Avg. length 8.6 12.8 8.0 Avg. nr. of parses 399.0 573.1 19.2 Avg. tim</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>S. Oepen and J. Carroll. 2000. Ambiguity packing in constraint-based parsing: practical results. In Proceedings of NAACL-2000, pages 162–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University Of Chicago Press,</publisher>
<location>Chicago, IL, USA.</location>
<contexts>
<context position="1436" citStr="Pollard and Sag, 1994" startWordPosition="214" endWordPosition="217">evious studies have shown that treebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cannot describe correctly. The increased linguistic preciseness comes at a cost, though: su</context>
<context position="4534" citStr="Pollard and Sag, 1994" startWordPosition="706" endWordPosition="709">09, pages 37–45, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP are combined. Compared to previous deep grammar extraction approaches, a more sophisticated core grammar (in the framework of HPSG) is created. Also, more detailed syntactic features are learnt from the resource treebank, which leads to a more precise lexicon. Parsing results are compared with GG (German Grammar), a previously hand-written German HPSG grammar (M¨uller, 2002; Crysmann, 2003; Crysmann, 2005). 2 Core grammar 2.1 Head-driven phrase structure grammar This study has been entirely embedded in the HPSG framework (Pollard and Sag, 1994). This is a heavily lexicalised, constraint-based theory of syntax, and it uses typed feature structures as its representation. HPSG introduces a small number of principles (most notably, the Head Feature Principle) that guide the construction of a few Immediate Dominance schemata. These schemata are meant to be the sole basis to combine words and phrases. Examples of schemata are headcomplement, head-subject, head-specifier, headfiller (for long-distance dependencies) and headmodifier. In this study, the core grammar is an extension of the off-the-shelf version of HPSG. The type hierarchy is </context>
<context position="9084" citStr="Pollard and Sag, 1994" startWordPosition="1469" endWordPosition="1472">p’) can be infixed as well: ‘er versucht auszuschlafen’ (‘He tries to sleep in’). These characteristics make German a comparatively complex language to parse with CFGs: more variants of the same lemma have to be memorised, and the expansion of production rules will be more diverse, with a less peaked statistical distribution. Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don’t compare to state-of-theart parsing of English. 2.3 Structure of the core grammar The grammar uses the main tenets from Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). However, different from earlier deep grammar extraction studies, more sophisticated structures are added. M¨uller (2002) proposes a new schema (head-cluster) to account for verb clusters in the right bracket, which includes the possibility to merge subcategorisation frames of e.g. object-control verbs and its dependent verb. Separate rules for determinerless NPs, genitive modification, coordination of common phrases, relative phrases and direct speech are also created. The free word order of German is accounted for by scrambling arguments with lexical rules, and by allowing adjuncts to be a </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>C.J. Pollard and I.A. Sag. 1994. Head-Driven Phrase Structure Grammar. University Of Chicago Press, Chicago, IL, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>T H King</author>
<author>R M Kaplan</author>
<author>R Crouch</author>
<author>J T Maxwell</author>
<author>M Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL-2001,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="29087" citStr="Riezler et al., 2001" startWordPosition="4849" endWordPosition="4852">on allows us to have more linguistically motivated analyses of German than approaches where the core lexicon only comprises the textbook principles/operators. We compared our lexicon extraction results to those from (Hockenmaier, 2006). Also, preliminary parsing experiments are reported, in which we show that this grammar produces reasonable coverage on unseen text. Although we feel confident about the successful acquisition of the grammar, there still remain some limiting factors in the performance of the grammar when actually parsing. Compared to coverage figures of around 80%, reported by (Riezler et al., 2001), the proportion of parse forests containing the correct parse in this study is rather low. The first limit is the constructional coverage, meaning that the core grammar is not able to construct the correct analysis, even though all lexical entries have been derived correctly before. The most frequent phenomena that are not captured yet are PH/RE constructions and extraposed clauses, and we plan to do an efficient implementation (Crysmann, 2005) of these in a next version of the grammar. Second, as shown in figure 3, data scarcity in the learning of the surface forms of lemmas negatively influ</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2001</marker>
<rawString>S. Riezler, T.H. King, R.M. Kaplan, R. Crouch, J.T. Maxwell III, and M. Johnson. 2001. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings ofACL-2001, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Surface structure and interpretation.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1371" citStr="Steedman, 1996" startWordPosition="204" endWordPosition="206">our frame of reference for evaluation. 1 1 Introduction Previous studies have shown that treebanks can be helpful when constructing grammars. The most well-known example is PCFG-based statistical parsing (Charniak and Johnson, 2005), where a PCFG is induced from, for instance, the Penn Treebank. The underlying statistical techniques have been refined in the last decade, and previous work indicates that the labelled f-score of this method converges to around 91%. An alternative to PCFGs, with more linguistic relevance, is formed by deeper formalisms, such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 1996), LFG (Kaplan and Bresnan, 1995) and HPSG (Pollard and Sag, 1994). For LFG (Butt et al., 2002) and HPSG (Flickinger, 2000; M¨uller, 2002), large hand-written grammars have been developed. In the case of HPSG, the grammar writers found the small number of principles too restrictive, and created more rules (approximately 50 to 300) to accommodate for phenomena 1The research reported in this paper has been carried out with financial support from the Deutsche Forschungsgemeinschaft and the German Excellence Cluster of Multimodal Computing &amp; Interaction. that vanilla HPSG cannot describe correctly.</context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>M. Steedman. 1996. Surface structure and interpretation. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ytrestøl</author>
<author>D Flickinger</author>
<author>S Oepen</author>
</authors>
<title>Extracting and Annotating Wikipedia Sub-Domains.</title>
<date>2009</date>
<booktitle>In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>185--197</pages>
<contexts>
<context position="27290" citStr="Ytrestøl et al., 2009" startWordPosition="4561" endWordPosition="4564">000. The difference between the two ‘correct’ numbers in table 2 is rather large, meaning that the ’real’ coverage might seem disappointingly low. Not unexpectedly, we found that the generic lexical types for verbs (transitive verb, third person singular) and nouns (any gender, no appositions allowed) was not always correct, harming the results considerably. A quantitative comparison between deep grammars is always hard. Between DELPH-IN grammars, coverage has been the main method of evaluation. However, this score does not reward richness of the semantic output. Recent evidence from the ERG (Ytrestøl et al., 2009) suggests that the ERG reaches a top-500 coverage of around 70% on an unseen domain, a result that this experiment did not approximate. The goal of GG is not computational, but it serves as a testing ground for linguistic hypotheses. Therefore, the developers have never aimed at high coverage figures, and crafted the GG to give more detailed analyses and to be suited for both parsing and generation. We are happy to observe that the coverage figures in this study are higher than GG’s (Zhang et al., 2009), but we realise the limited value of this evaluation method. Future studies will certainly </context>
</contexts>
<marker>Ytrestøl, Flickinger, Oepen, 2009</marker>
<rawString>G. Ytrestøl, D. Flickinger, and S. Oepen. 2009. Extracting and Annotating Wikipedia Sub-Domains. In Proceedings of the Seventh International Workshop on Treebanks and Linguistic Theories, pages 185–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Efficiency in Unification-Based N-Best Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies,</booktitle>
<pages>48--59</pages>
<contexts>
<context position="22181" citStr="Zhang et al., 2007" startWordPosition="3688" endWordPosition="3691">ction for future research might be to find ways to automatically expand the lexicon after the derivation procedure, or to hand-code morphological rules in the core grammar. 4 Parsing 4.1 Methodology All experiments in this article use the first 45,000 sentences as training data, and the consecutive 5,000 sentences as test data. The remaining 472 sentences are not used. We used the PET parser (Callmeier, 2000) to do all parsing experiments. The parser was instructed to yield a parse error after 50,000 passive edges were used. Ambiguity packing (Oepen and Carroll, 2000) and selective unpacking (Zhang et al., 2007) were used to reduce memory footprint and speed up the selection of the top-1000 analyses. The maximum entropy model, used for selective unpacking, was based on 200 treebanked sentences of up to 20 words from the training set. Part-of-speech tags delivered by the stock version of the TnT tagger (Brants, ) were 42 Tiger T.+TnT GG Out of vocabulary 71.9 % 5.2 % 55.6 % Parse error 0.2 % 1.5 % 0.2 % Unparsed 7.9 % 37.7 % 28.2 % Parsed 20.0 % 55.6 % 16.0 % Total 100.0 % 100.0 % 100.0 % Avg. length 8.6 12.8 8.0 Avg. nr. of parses 399.0 573.1 19.2 Avg. time (s) 9.3 15.8 11.6 Table 1: This table shows</context>
</contexts>
<marker>Zhang, Oepen, Carroll, 2007</marker>
<rawString>Y. Zhang, S. Oepen, and J. Carroll. 2007. Efficiency in Unification-Based N-Best Parsing. In Proceedings of the Tenth International Conference on Parsing Technologies, pages 48–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>R Wang</author>
<author>S Oepen</author>
</authors>
<title>Hybrid Multilingual Parsing with HPSG for SRL.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-2009,</booktitle>
<note>to appear.</note>
<contexts>
<context position="24627" citStr="Zhang et al., 2009" startWordPosition="4115" endWordPosition="4118">ing. The version of GG that was employed (M¨uller, 2002; Crysmann, 2003) was dated October 20084. 4.2 Results Table 1 shows coverage figures in three different settings. It is clear that the resulting grammar has a higher coverage than the GG, but this comes at a cost: more ambiguity, and possibly unnecessary ambiguity. Remarkably, the average processing time is lower, even when the sentence lengths and 4It should be noted that little work has gone in to providing unknown word handling mechanisms, and that is why we didn’t include it in our results. However, in a CoNLL-2009 shared task paper (Zhang et al., 2009), a coverage of 28.6% was reported when rudimentary methods were used. ambiguity rates are higher. We attribute this to the smaller feature structure geometry that is introduced by the core grammar (compared to the GG). Using unknown word handling immediately improved the coverage, by a large margin. Larger ambiguity rates were recorded, and the number of parser errors slightly increased. Because coverage does not imply quality, we wanted to look at the results in a qualitative fashion. We took a sample of 200 sentences from both the training and the test set, where the ones from the training </context>
<context position="27798" citStr="Zhang et al., 2009" startWordPosition="4651" endWordPosition="4654">his score does not reward richness of the semantic output. Recent evidence from the ERG (Ytrestøl et al., 2009) suggests that the ERG reaches a top-500 coverage of around 70% on an unseen domain, a result that this experiment did not approximate. The goal of GG is not computational, but it serves as a testing ground for linguistic hypotheses. Therefore, the developers have never aimed at high coverage figures, and crafted the GG to give more detailed analyses and to be suited for both parsing and generation. We are happy to observe that the coverage figures in this study are higher than GG’s (Zhang et al., 2009), but we realise the limited value of this evaluation method. Future studies will certainly include a more granular evaluation of the grammar’s performance. 5 Conclusion and discussion We showed how a precise, wide-coverage HPSG grammar for German can be created successfully, by constructing a core grammar by hand, and appending it with linguistic information from the Tiger treebank. Although this extracted grammar suffers considerably more from overgeneration than the hand-written GG, we argue that our conservative derivation procedure delivers a more detailed, compact and correct compared to</context>
</contexts>
<marker>Zhang, Wang, Oepen, 2009</marker>
<rawString>Y. Zhang, R. Wang, and S.. Oepen. 2009. Hybrid Multilingual Parsing with HPSG for SRL. In Proceedings of CoNLL-2009, to appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>