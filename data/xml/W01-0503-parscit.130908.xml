<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012965">
<title confidence="0.915427">
Learning Within-Sentence Semantic Coherence
</title>
<author confidence="0.915697">
Elena Enevat, Rose Hoberman*, and Lucian Litat
</author>
<affiliation confidence="0.894917">
ICenter for Automated Learning and Discovery *Computer Science Department
Carnegie Mellon University
</affiliation>
<address confidence="0.563431">
Pittsburgh, Pennsylvania 15213
</address>
<email confidence="0.998974">
feneva,roseh,llital@cs.cmu.edu
</email>
<sectionHeader confidence="0.987703" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999374375">
In many domains such as speech recognition
and machine translation it is extremely useful
to be able able to distinguish coherent from
non-coherent sentences. We introduce a set of
word-based statistical features which measure
semantic coherence and can be used to enhance
any language application where coherent sen-
tences need to be generated or recognized. We
train a decision tree using the constructed fea-
ture set to automatically classify sentences as
coherent or not. We find that our combination
of boosted decision trees and coherence features
achieves an accuracy of 80% when distinguish-
ing trigram-generated sentences (non-coherent)
from those in the Broadcast News dataset (co-
herent).
</bodyText>
<sectionHeader confidence="0.995599" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920666666667">
In order to improve conventional statistical
language models, we need to find aspects of
language which are not adequately captured,
and then incorporate them into the model.
Although adding any computable feature of
a language which discriminates between the
&apos;true&apos; English language model and the statisti-
cal model will give some improvement, an ideal
feature should occur frequently, yet exhibit a
significant discrepancy (Rosenfeld et al., 2001).
Perhaps the most salient deficiency of conven-
tional language models is their complete failure
at modeling semantic coherence. These mod-
els capture short distance correlations among
words in a sentence, yet are unable to distin-
guish meaningful sentences (where the content
words come from the same semantic domain)
from &apos;fake&apos; sentences (where the content words
</bodyText>
<footnote confidence="0.488238">
* Supported by a National Science and Engineering
Graduate Fellowship
</footnote>
<bodyText confidence="0.999874925">
are drawn randomly). As a result, in many lan-
guage technology applications such as speech
recognition, errors that are obvious to a human
observer (e.g. a noun replaced by an acousti-
cally similar but semantically different noun)
cannot be salvaged by the model. For example,
if the speaker says I would like a glass full of
water, and the system recognizes class instead
of glass, we would like to automatically identify
and correct this error.
One way to identify and thus avoid these
types of errors is to identify sentences which are
not semantically coherent. This work is an at-
tempt to automatically construct features which
can then be used to learn a function which will
distinguish between coherent and non-coherent
sentences.
Once we have learned such a measure of se-
mantic coherence, we can combine it with the
initial baseline model. One model which is nat-
urally suited for this task is the maximum en-
tropy model introduced by Rosenfeld (1997),
which directly models the probability of an en-
tire sentence. The model is ideal for model-
ing whole-sentence phenomena because it treats
each sentence as a &apos;bag of features&apos; where fea-
tures are arbitrary computable properties of the
sentence.
To incorporate semantic coherence into this
exponential model, (Cai et al., 2000) did some
initial work on constructing within-sentence se-
mantic coherence features. A set of five features
was extracted from each sentence and added di-
rectly to the exponential model. These features
decreased perplexity over the baseline trigram
model by 3 to 5%.
In this paper we augment the limited fea-
ture set presented in (Cai et al., 2000) by con-
structing roughly seventy features, which fall
into nine broad classes. The majority of the fea-
</bodyText>
<tableCaption confidence="0.977877">
Table 1: Yule&apos;s Statistic
</tableCaption>
<bodyText confidence="0.999731">
ture classes rely on word-pair correlation values
in sentences. To derive a measure of whole sen-
tence coherence, we used our feature set to build
a classifier which distinguishes between real sen-
tences generated by a human (taken as exam-
ples of coherent sentences) and fake sentences
generated by a conventional language model
(taken as examples of non-coherent sentences).
The confidence score produced by our classifier
has an accuracy of 80%, which is a significant
performance improvement over the results ob-
tained by using only the features developed in
previous work. This measure of semantic co-
herence can be used to improve the original
language model, and thus improve performance
in any application where language modeling is
used.
</bodyText>
<sectionHeader confidence="0.915694" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.9999124">
We use semantic association between word-pairs
and other sentence characteristics (described
later) to construct useful features for distin-
guishing between coherent and non-coherent
sentences.
</bodyText>
<subsectionHeader confidence="0.882103">
2.1 Semantic Association between
Word-Pairs
</subsectionHeader>
<bodyText confidence="0.9992675">
We use the same correlation measure as in
(Cai et al., 2000). For each word-pair in our
dataset, we calculate a measure of association
called Q (Yule&apos;s statistic) based on the appro-
priate 2 x2 contingency table of training data
co-occurrences of the two words in the pair. Ta-
ble 1 shows such a contingency table for two
words. CIA is the count of sentences in the train-
ing corpus which contain both words, C12 is ob-
tained by subtracting CIA from the number of
sentences with Word&apos; in it; C21 is obtained by
subtracting Cll from the number of sentences
with Word2 in it; C22 is the total number of
sentences minus the other three counts. Based
on such tables we can compute the Q statistic
for each pair of words using equation 1.
</bodyText>
<equation confidence="0.9996705">
Q =
C11 • C22 — C12 • C21 1
, ()
v11 • C22 + C12 • C21
</equation>
<bodyText confidence="0.999952307692308">
The values of Q thus range from -1 to 1; the
higher the Q value, the stronger the correlation
between the two words. Q is -1 when two words
have never occurred in the same sentence, and
is 1 when they always occur together. Each sen-
tence, coherent or non-coherent, can be repre-
sented by a variable length list of Q values—a Q
value for each pair of content words. By defining
some statistics on these Q lists and comparing
the true and fake data, we observe differences
in the distributions of the statistics. A classifier
can use these statistics to distinguish between
coherent and non-coherent sentences.
</bodyText>
<subsectionHeader confidence="0.667158">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.928722866666667">
In order to train a classifier to discern between
coherent and non-coherent sentences, we first
generated a large set of features. We did not
explore features which use syntactic knowledge,
information about specific words, or lexical re-
sources like thesauri. The features are based
only on the word-pair correlation values, the co-
occurrence counts, and statistics on stop words.
We have implemented roughly seventy features,
which fall into nine classes. These classes are:
1. Simple statistics - the simple correlation
characteristics of a sentence: mean, me-
dian, maximum, minimum, range and vari-
ance of word pair correlation values. These
fall into two sub-groups:
</bodyText>
<listItem confidence="0.9964141">
(a) Statistics considering all word-pairs
(b) Statistics considering only pairs sepa-
rated by at least 5 words - short dis-
tance correlations and local coherence
are usually well modelled by trigram
language models
2. Sentence Length
3. Percentage of correlation values above
some threshold.
4. High/low correlations across large/small
</listItem>
<bodyText confidence="0.9242934">
distances - very high correlations across
large distances are strong evidence that
the sentence is from Broadcast News, as
are negative correlations at very close dis-
tances. We use thresholds to vary what is
</bodyText>
<figure confidence="0.993759839285714">
WORD 1 WORD 1
YES No
WORD 2 YES C11 C12
WORD 2 No C21 C22
Distribution of Q values at Distance 1
Density
0.0 0.5 1.0 1.5 2.0 2.5 3.0
Broadcast News
Trigram−generated
−1.0 −0.5 0.0 0.5 1.0
Q value
Distribution of Q values at Distance 3
Density
0.0 0.5 1.0 1.5 2.0
Broadcast News
Trigram−generated
−1.0 −0.5 0.0 0.5 1.0
Q value
Distribution of the Likelihood Ratio Feature
Broadcast News
Trigram−generated
Density
0.00 0.02 0.04 0.06 0.08 0.10
−30 −20 −10 0 10 20 30
Likelihood Ratio Value
Maximum Q Value at Distance 4
Density
0 1 2 3 4 5
Broadcast News
Trigram−generated
−1.0 −0.5 0.0 0.5 1.0
Q Value
Distribution of Median Q Value
Density
0.0 0.5 1.0 1.5 2.0
Broadcast News
Trigram−generated
−1.0 −0.5 0.0 0.5 1.0
Q value
Perfect Agreement
Users Split
50% Agreement
10 Sentences
15 Sentences
60−69%
Agreement
18 Sentences
90−99%
Agreement
20 Sentences
70−79%
Agreement
11 Sentences
80−89% Agreement
25 Sentences
provement remains to be done as future work.
</figure>
<sectionHeader confidence="0.755706" genericHeader="related work">
5 Future Work and Summary
</sectionHeader>
<bodyText confidence="0.999358166666667">
Yule&apos;s Q statistic is not necessarily the best
measure of word-pair correlation because it is
extremely susceptible to data sparseness. The
point estimates of the Q values for words that
appear infrequently in our corpus are often un-
reliable. One idea for reducing the effect of these
unreliable estimates is to use a confidence dis-
tribution instead of the current point estimate.
Another possible approach would be to use a
different correlation statistic; one possibility is
the x statistic. Another possibility is to use
the method of (Dagan et al., 1995) to improve
the estimates of the likelihood of co-occurrences
that are rare in the training data.
When people are given this classification task
of distinguishing coherent and non-coherent
sentences, they tend to group together words
in the sentence that belong to the same topic.
We would like to model this process by using
other clustering methods to group the seman-
tically similar words together in each sentence.
Then statistics based on these clusters could be
used as additional features to improve the co-
herence measure.
In this paper we developed a successful ap-
proach for automatically distinguishing between
coherent and non-coherent sentences. Our com-
bination of feature sets and learners achieved a
reduction in error of 24% from the features used
in previous work (Cai et al., 2000). Our suc-
cessful approach used the semantic association
between word-pairs as well as simple features
such as sentence length and percentage of stop
words to construct a highly accurate classifier.
More work remains to be done to evaluate
how well the semantic coherence measure im-
proves the baseline language model. This eval-
uation can be performed by incorporating the
semantic coherence features into an exponential
language model and then using the model for a
practical application such as n-best list rescor-
ing.
</bodyText>
<sectionHeader confidence="0.998947" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996408848484848">
C. Cai, R. Rosenfeld, and L. Wasserman. 2000.
Exponential language models, logisitc regres-
sion, and semantic coherence. In Proceedings
of NIST/DARPA Speech Transciption Work-
shop.
I. Dagan, S. Marcus, and S. Markovitch. 1995.
Contextual word similarity and estimation
from sparse data. Computer Speech and lan-
guage.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language mod-
eling. In Proceedings of the IEEE Interna-
tional Conyerence on Acoustics, Speech, and
Signal Processing.
S. Della Pietra, V. Della Pietra, and J.Lafferty.
1997. Inducing features of random fields. In
IEEE Transactions on Pattern Analysis and
Machine Intelligence.
J. R. Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufmann, San Ma-
teo, CA.
Ronald Rosenfeld, Stanley F. Chen, and Xi-
aojin Zhu. 2001. Whole-sentence exponen-
tial language models: a vehicle for linguistic-
statistical integration.
R. Rosenfeld. 1997. A whole sentence maxi-
mum entropy language model. In Proceed-
ings of IEEE Workshop on Automated Speech
Recognition and Understanding.
Robert E. Schapire. 1999. A brief introduction
to boosting. In Proceedings of the Sixteenth
International Joint Conference on Artificial
Intelligence.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621023">
<title confidence="0.999809">Learning Within-Sentence Semantic Coherence</title>
<author confidence="0.999901">Elena Enevat</author>
<author confidence="0.999901">Rose Hoberman</author>
<author confidence="0.999901">Lucian Litat</author>
<affiliation confidence="0.99963">ICenter for Automated Learning and Discovery *Computer Science Department Carnegie Mellon</affiliation>
<address confidence="0.97528">Pittsburgh, Pennsylvania</address>
<email confidence="0.999961">feneva,roseh,llital@cs.cmu.edu</email>
<abstract confidence="0.977761470588235">In many domains such as speech recognition and machine translation it is extremely useful to be able able to distinguish coherent from non-coherent sentences. We introduce a set of word-based statistical features which measure semantic coherence and can be used to enhance any language application where coherent sentences need to be generated or recognized. We train a decision tree using the constructed feature set to automatically classify sentences as coherent or not. We find that our combination of boosted decision trees and coherence features an accuracy of distinguishing trigram-generated sentences (non-coherent) from those in the Broadcast News dataset (coherent).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cai</author>
<author>R Rosenfeld</author>
<author>L Wasserman</author>
</authors>
<title>Exponential language models, logisitc regression, and semantic coherence.</title>
<date>2000</date>
<booktitle>In Proceedings of NIST/DARPA Speech Transciption Workshop.</booktitle>
<contexts>
<context position="3133" citStr="Cai et al., 2000" startWordPosition="477" endWordPosition="480">ction which will distinguish between coherent and non-coherent sentences. Once we have learned such a measure of semantic coherence, we can combine it with the initial baseline model. One model which is naturally suited for this task is the maximum entropy model introduced by Rosenfeld (1997), which directly models the probability of an entire sentence. The model is ideal for modeling whole-sentence phenomena because it treats each sentence as a &apos;bag of features&apos; where features are arbitrary computable properties of the sentence. To incorporate semantic coherence into this exponential model, (Cai et al., 2000) did some initial work on constructing within-sentence semantic coherence features. A set of five features was extracted from each sentence and added directly to the exponential model. These features decreased perplexity over the baseline trigram model by 3 to 5%. In this paper we augment the limited feature set presented in (Cai et al., 2000) by constructing roughly seventy features, which fall into nine broad classes. The majority of the feaTable 1: Yule&apos;s Statistic ture classes rely on word-pair correlation values in sentences. To derive a measure of whole sentence coherence, we used our fe</context>
<context position="4656" citStr="Cai et al., 2000" startWordPosition="717" endWordPosition="720">racy of 80%, which is a significant performance improvement over the results obtained by using only the features developed in previous work. This measure of semantic coherence can be used to improve the original language model, and thus improve performance in any application where language modeling is used. 2 Approach We use semantic association between word-pairs and other sentence characteristics (described later) to construct useful features for distinguishing between coherent and non-coherent sentences. 2.1 Semantic Association between Word-Pairs We use the same correlation measure as in (Cai et al., 2000). For each word-pair in our dataset, we calculate a measure of association called Q (Yule&apos;s statistic) based on the appropriate 2 x2 contingency table of training data co-occurrences of the two words in the pair. Table 1 shows such a contingency table for two words. CIA is the count of sentences in the training corpus which contain both words, C12 is obtained by subtracting CIA from the number of sentences with Word&apos; in it; C21 is obtained by subtracting Cll from the number of sentences with Word2 in it; C22 is the total number of sentences minus the other three counts. Based on such tables we</context>
</contexts>
<marker>Cai, Rosenfeld, Wasserman, 2000</marker>
<rawString>C. Cai, R. Rosenfeld, and L. Wasserman. 2000. Exponential language models, logisitc regression, and semantic coherence. In Proceedings of NIST/DARPA Speech Transciption Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Marcus</author>
<author>S Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data. Computer Speech and language.</title>
<date>1995</date>
<contexts>
<context position="8754" citStr="Dagan et al., 1995" startWordPosition="1418" endWordPosition="1421">remains to be done as future work. 5 Future Work and Summary Yule&apos;s Q statistic is not necessarily the best measure of word-pair correlation because it is extremely susceptible to data sparseness. The point estimates of the Q values for words that appear infrequently in our corpus are often unreliable. One idea for reducing the effect of these unreliable estimates is to use a confidence distribution instead of the current point estimate. Another possible approach would be to use a different correlation statistic; one possibility is the x statistic. Another possibility is to use the method of (Dagan et al., 1995) to improve the estimates of the likelihood of co-occurrences that are rare in the training data. When people are given this classification task of distinguishing coherent and non-coherent sentences, they tend to group together words in the sentence that belong to the same topic. We would like to model this process by using other clustering methods to group the semantically similar words together in each sentence. Then statistics based on these clusters could be used as additional features to improve the coherence measure. In this paper we developed a successful approach for automatically dist</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1995</marker>
<rawString>I. Dagan, S. Marcus, and S. Markovitch. 1995. Contextual word similarity and estimation from sparse data. Computer Speech and language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conyerence on Acoustics, Speech, and Signal Processing.</booktitle>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE International Conyerence on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<booktitle>In IEEE Transactions on Pattern Analysis and Machine Intelligence.</booktitle>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J.Lafferty. 1997. Inducing features of random fields. In IEEE Transactions on Pattern Analysis and Machine Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: a vehicle for linguisticstatistical integration.</title>
<date>2001</date>
<contexts>
<context position="1422" citStr="Rosenfeld et al., 2001" startWordPosition="199" endWordPosition="202">erence features achieves an accuracy of 80% when distinguishing trigram-generated sentences (non-coherent) from those in the Broadcast News dataset (coherent). 1 Introduction In order to improve conventional statistical language models, we need to find aspects of language which are not adequately captured, and then incorporate them into the model. Although adding any computable feature of a language which discriminates between the &apos;true&apos; English language model and the statistical model will give some improvement, an ideal feature should occur frequently, yet exhibit a significant discrepancy (Rosenfeld et al., 2001). Perhaps the most salient deficiency of conventional language models is their complete failure at modeling semantic coherence. These models capture short distance correlations among words in a sentence, yet are unable to distinguish meaningful sentences (where the content words come from the same semantic domain) from &apos;fake&apos; sentences (where the content words * Supported by a National Science and Engineering Graduate Fellowship are drawn randomly). As a result, in many language technology applications such as speech recognition, errors that are obvious to a human observer (e.g. a noun replace</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: a vehicle for linguisticstatistical integration.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proceedings of IEEE Workshop on Automated Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="2809" citStr="Rosenfeld (1997)" startWordPosition="428" endWordPosition="429"> system recognizes class instead of glass, we would like to automatically identify and correct this error. One way to identify and thus avoid these types of errors is to identify sentences which are not semantically coherent. This work is an attempt to automatically construct features which can then be used to learn a function which will distinguish between coherent and non-coherent sentences. Once we have learned such a measure of semantic coherence, we can combine it with the initial baseline model. One model which is naturally suited for this task is the maximum entropy model introduced by Rosenfeld (1997), which directly models the probability of an entire sentence. The model is ideal for modeling whole-sentence phenomena because it treats each sentence as a &apos;bag of features&apos; where features are arbitrary computable properties of the sentence. To incorporate semantic coherence into this exponential model, (Cai et al., 2000) did some initial work on constructing within-sentence semantic coherence features. A set of five features was extracted from each sentence and added directly to the exponential model. These features decreased perplexity over the baseline trigram model by 3 to 5%. In this pap</context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>R. Rosenfeld. 1997. A whole sentence maximum entropy language model. In Proceedings of IEEE Workshop on Automated Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>A brief introduction to boosting.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Schapire, 1999</marker>
<rawString>Robert E. Schapire. 1999. A brief introduction to boosting. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>