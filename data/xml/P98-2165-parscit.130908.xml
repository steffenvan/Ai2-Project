<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016682">
<title confidence="0.997231">
Learning Intonation Rules for Concept to Speech Generation
</title>
<author confidence="0.998021">
Shimei Pan and Kathleen McKeown
</author>
<affiliation confidence="0.9964575">
Dept. of Computer Science
Columbia University
</affiliation>
<address confidence="0.970969">
New York, NY 10027, USA
</address>
<email confidence="0.72555">
{pan, Icathy}acs.columbia.edu
</email>
<sectionHeader confidence="0.976262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999755">
In this paper, we report on an effort to pro-
vide a general-purpose spoken language gener-
ation tool for Concept-to-Speech (CTS) appli-
cations by extending a widely used text gener-
ation package, FUF/SURGE, with an intona-
tion generation component. As a first step, we
applied machine learning and statistical models
to learn intonation rules based on the semantic
and syntactic information typically represented
in FUF/SURGE at the sentence level. The re-
sults of this study are a set of intonation rules
learned automatically which can be directly im-
plemented in our intonation generation compo-
nent. Through 5-fold cross-validation, we show
that the learned rules achieve around 90% accu-
racy for break index, boundary tone and phrase
accent and 80% accuracy for pitch accent. Our
study is unique in its use of features produced by
language generation to control intonation. The
methodology adopted here can be employed di-
rectly when more discourse/pragmatic informa-
tion is to be considered in the future.
</bodyText>
<sectionHeader confidence="0.988644" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999957577777778">
Speech is rapidly becoming a viable medium for
interaction with real-world applications. Spo-
ken language interfaces to on-line informa-
tion, such as plane or train schedules, through
display-less systems, such as telephone inter-
faces, are well under development. Speech in-
terfaces are also widely used in applications
where eyes-free and hands-free communication
is critical, such as car navigation. Natural lan-
guage generation (NLG) can enhance the abil-
ity of such systems to communicate naturally
and effectively by allowing the system to tailor,
reorganize, or summarize lengthy database re-
sponses. For example, in our work on a mul-
timedia generation system where speech and
graphics generation techniques are used to au-
tomatically summarize patient&apos;s pre-, during,
and post-, operation status to different care-
givers (Dalai et al., 1996), records relevant to
patient status can easily number in the thou-
sands. Through content planning, sentence
planning and lexical selection, the NLG com-
ponent is able to provide a concise, yet infor-
mative, briefing automatically through spoken
and written language coordinated with graph-
ics (McKeown et al., 1997) .
Integrating language generation with speech
synthesis within a Concept-to-Speech (CTS)
system not only brings the individual benefits
of each; as an integrated system, CTS can take
advantage of the availability of rich structural
information constructed by the underlying NLG
component to improve the quality of synthe-
sized speech. Together, they have the potential
of generating better speech than Text-to-Speech
(TTS) systems. In this paper, we present a se-
ries of experiments that use machine learning to
identify correlation between intonation and fea-
tures produced by a robust language generation
tool, the FUF/SURGE system (Elhadad, 1993;
Robin, 1994). The ultimate goal of this study
is to provide a spoken language generation tool
based on FUF/SURGE, extended with an in-
tonation generation component to facilitate the
development of new CTS applications.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="introduction">
2 Related Theories
</sectionHeader>
<bodyText confidence="0.9981521">
Two elements form the theoretical back-
ground of this work: the grammar used in
FUF/SURGE and Pierrehumbert&apos;s intonation
theory (Pierrehumbert, 1980). Our study
aims at identifying the relations between the
semantic/syntactic information produced by
FUF/SURGE and four intonational features of
Pierrehumbert: pitch accent, phrase accent,
boundary tone and intermediate/intonational
phrase boundaries.
</bodyText>
<page confidence="0.975015">
1003
</page>
<bodyText confidence="0.999974948717949">
The FUF/SURGE grammar is primarily
based on systemic grammar (Halliday, 1985).
In systemic grammar, the process (ultimately
realized as the verb) is the core of a clause&apos;s
semantic structure. Obligatory semantic roles,
called participants, are associated with each
process. Usually, participants convey who/what
is involved in the process. The process also
has non-obligatory peripheral semantic roles
called circumstances. Circumstances answer
questions such as when/where/how/why. In
FUF/SURGE, this semantic description is uni-
fied with a syntactic grammar to generate a syn-
tactic description. All semantic, syntactic and
lexical information, which are produced during
the generation process, are kept in a final Func-
tional Description (FD), before linearizing the
syntactic structure into a linear string. The fea-
tures used in our intonation model are mainly
extracted from this final FD.
The intonation theory proposed in (Pierre-
humbert, 1980) is used to describe the intona-
tion structure. Based on her intonation gram-
mar, the FO pitch contour is described by a set
of intonational features. The tune of a sen-
tence is formed by one or more intonational
phrases. Each intonational phrase consists of
one or more intermediate phrases followed by
a boundary tone. A well-formed intermediate
phrase has one or more pitch accents followed
by a phrase accent. Based on this theory, there
are four features which are critical in deciding
the FO contour: the placement of intonational or
intermediate phrase boundaries (break index 4
and 3 in ToBI annotation convention (Beckman
and Hirschberg, 1994)), the tonal type at these
boundaries (the phrase accent and the bound-
ary tone), and the FO local maximum or mini-
mum (the pitch accent).
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999941419354839">
Previous work on intonation modeling primar-
ily focused on TTS applications. For exam-
ple, in (Ba,chenko and Fitzpatrick, 1990), a
set of hand-crafted rules are used to determine
discourse neutral prosodic phrasing, achieving
an accuracy of approximately 85%. Recently,
researchers improved on manual development
of rules by acquiring prosodic phrasing rules
with machine learning tools. In (Wang and
Hirschberg, 1992), Classification And Regres-
sion Tree (CART) (Brieman et al., 1984) was
used to produce a decision tree to predict the
location of prosodic phrase boundaries, yielding
a high accuracy, around 90%. Similar methods
were also employed in predicting pitch accent
for TTS in (Hirschberg, 1993). Hirschberg ex-
ploited various features derived from text analy-
sis, such as part of speech tags, information sta-
tus (i.g. given/new, contrast), and cue phrases;
both hand-crafted and automatically learned
rules achieved 80-98% success depending on the
type of speech corpus. Until recently, there has
been only limited effort on modeling intonation
for CTS (Davis and Hirschberg, 1988; Young
and Fallside, 1979; Prevost, 1995). Many CTS
systems were simplified as text generation fol-
lowed by TTS. Others that do integrate genera-
tion make use of the structural information pro-
vided by the NLG component (Prevost, 1995).
However, most previous CTS systems are not
based on large scale general NLG systems.
</bodyText>
<sectionHeader confidence="0.987921" genericHeader="method">
4 Modeling Intonation
</sectionHeader>
<bodyText confidence="0.999938166666667">
While previous research provides some correla-
tion between linguistic features and intonation,
more knowledge is needed. The NLG compo-
nent provides very rich syntactic and semantic
information which has not been explored before
for intonation modeling. This includes, for ex-
ample, the semantic role played by each seman-
tic constituent. In developing a CTS, it is worth
taking advantage of these features.
Previous TTS research results cannot be im-
plemented directly in our intonation generation
component. Many features studied in TTS are
not provided by FUF/SURGE. For example,
the part-of-speech (POS) tags in FUF/SURGE
are different from those used in TTS. Further-
more, it make little sense to apply part of speech
tagging to generated text instead of using the
accurate POS provided in a NLG system. Fi-
nally, NLG provides information that is difficult
to accurately obtain from full text (e.g., com-
plete syntactic parses).
These motivating factors led us to carry out a
study consisting of a series of three experiments
designed to answer the following questions:
</bodyText>
<listItem confidence="0.999386125">
• How do the different features produced
by FUF/SURGE contribute to determin-
ing intonation?
• What is the minimal number of features
needed to achieve the best accuracy for
each of the four intonation features?
• Does intra-sentential context improve ac-
curacy?
</listItem>
<page confidence="0.984385">
1004
</page>
<figure confidence="0.9985585">
((cat clause)
(process ((type ascriptive)
(mode equative)))
(participant
((identified ((lex &amp;quot;John&amp;quot;)
(cat proper)))
(identifier ((lex &amp;quot;teacher&amp;quot;)
(cat common))))))
</figure>
<figureCaption confidence="0.999962">
Figure 1: Semantic description
</figureCaption>
<subsectionHeader confidence="0.972091">
4.1 Tools and Data
</subsectionHeader>
<bodyText confidence="0.999989919354839">
In order to model intonational features au-
tomatically, features from FUF/SURGE and
a speech corpus are provided as input to a
machine learning tool called RIPPER (Co-
hen, 1995), which produces a set of classifi-
cation rules based on the training examples.
The performance of RIPPER is comparable to
benchmark decision tree induction systems such
as CART and C4.5. We also employ a sta-
tistical method based on a generalized linear
model (Chambers and Hastie, 1992) provided
in the S package to select salient predictors for
input to RIPPER.
Figure 1 shows the input Functional Descrip-
tion(FD) for the sentence &amp;quot;John is the teacher&amp;quot;.
After this FD is unified with the syntactic gram-
mar, SURGE, the resulting FD includes hun-
dreds of semantic, syntactic and lexical features.
We extract 13 features shown in Table 1 which
are more closely related to intonation as indi-
cated by previous research. We have chosen
features which are applicable to most words to
avoid unspecified values in the training data.
For example, &amp;quot;tense&amp;quot; is not extracted simply
because it can be only applied to verbs. Table 1
includes descriptions for each of the features
used. These are divided into semantic, syntac-
tic, and semi-syntactic/semantic features which
describe the syntactic properties of semantic
constituents. Finally, word position (NO.) and
the actual word (LEX) are extracted directly
from the linearized string.
About 400 isolated sentences with wide cov-
erage of various linguistic phenomena were cre-
ated as test cases for FUF/SURGE when it was
developed. We asked two male native speakers
to read 258 sentences, each sentence may be re-
peated several times. The speech was recorded
on a DAT in an office. The most fluent version
of each sentence was kept. The resulting speech
was transcribed by one author based on ToBI
with break index, pitch accent, phrase accent
and boundary tone labeled, using the XWAVE
speech analysis tool. The 13 features described
in Table 1 as well as one intonation feature are
used as predictors for the response intonation
feature. The final corpus contains 258 sentences
for each speaker, including 119 noun phrases, 37
of which have embeded sentences, and 139 sen-
tences. The average sentence/phrase length is
5.43 words. The baseline performance achieved
by always guessing the majority class is 67.09%
for break index, 54.10% for pitch accent, 66.23%
for phrase accent and 79.37% for boundary tone
based on the speech corpus from one speaker.
The relatively high baseline for boundary tone
is because for most of the cases, there is only
one L% boundary tone at the end of each sen-
tence in our training data. Speaker effect on in-
tonation is briefly studied in experiment 2. All
other experiments used data from one speaker
with the above baselines.
</bodyText>
<sectionHeader confidence="0.473635" genericHeader="method">
4.2 Experiments
</sectionHeader>
<subsectionHeader confidence="0.778736">
4.2.1 Interesting Combinations
</subsectionHeader>
<bodyText confidence="0.999912966666667">
Our first set of experiments was designed
as an initial test of how the features from
FUF/SURGE contribute to intonation. We fo-
cused on how the newly available semantic fea-
tures affect intonation. We were also interested
in finding out whether the 13 selected features
are redundant in making intonation decisions.
We started from a simple model which in-
cludes only 3 factors, the type of semantic con-
stituent boundary before (BB) and after (BA)
the word, and part of speech (POS). The seman-
tic constituent boundary can take on 6 different
values; for example, it can be a clause boundary,
a boundary associated with a primary semantic
role (e.g., a participant), with a secondary se-
mantic role (e.g., a type of modifier), among
others. Our purpose in this experiment was
to test how well the model can do with a lim-
ited number of parameters. Applying RIPPER
to the simple model yielded rules that signifi-
cantly improved performance over the baseline
models. For example, the accuracy of the rules
learned for break index increases to 87.37% from
67.09%; the average improvement on all 4 into-
national features is 19.33%.
Next, we ran two additional tests, one with
additional syntactic features and another with
additional semantic features. The results show
that the two new models behave similarly on all
intonational features; they both achieve some
</bodyText>
<page confidence="0.829004">
1005
</page>
<table confidence="0.99988536">
Category Label Description Examples
Semantic BB The semantic constituent boundary before the participant boundaries or circumstance
word, boundaries etc.
BA The semantic constituent -boundary after the participant boundaries or circumstance
word. boundaries etc.
SEMFUN The semantic feature of the word. The semantic feature of &amp;quot;did&amp;quot; in &amp;quot;I did
know him.&amp;quot; is &amp;quot;insistence&amp;quot;.
SP The semantic role played by the immediate The SP of &amp;quot;teacher&amp;quot; in &amp;quot;John is the
parental semantic constituent of the word. teacher&amp;quot; is &amp;quot;identifier&amp;quot;.
GSP The generic semantic role played by the imme- The GSP of &amp;quot;teacher&amp;quot; in &amp;quot;John is the
diate parental semantic constituent of the word. teacher&amp;quot; is &amp;quot;participant&amp;quot;
Syntactic POS The part of speech of the word common noun, proper noun etc.
GPOS The generic part of speech of the word noun is the corresponding GPOS of both
common noun and proper noun.
SYNFUN The syntactic function of the word The SYNFUN of &amp;quot;teacher&amp;quot; in &amp;quot;the teacher&amp;quot;
is &amp;quot;head&amp;quot;.
Semi- SPPOS The part of speech of the immediate parental The SPPOS of &amp;quot;teacher&amp;quot; is &amp;quot;common
semantic&amp; semantic constituent of the word. noun&amp;quot;.
syntactic
SPGPOS The generic part of speech of the immediate The SPGPOS of &amp;quot;teacher&amp;quot; in &amp;quot;the teacher&amp;quot;
parental semantic constituent of the word. is &amp;quot;noun phrase&amp;quot;.
SPSYNFUN The syntactic function of the immediate parental The SPSYNFUN of &amp;quot;teacher&amp;quot; in &amp;quot;John is
semantic constituent of the word. the teacher&amp;quot; is &amp;quot;subject complement.
Misc. NO. The position of the word in a sentence 1, 2, 3, 4 etc.
LEX The lexical form of the word &amp;quot;John&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;teacher&amp;quot;etc.
</table>
<tableCaption confidence="0.999516">
Table 1: Features extracted from FUF and SURGE
</tableCaption>
<bodyText confidence="0.99390636">
improvements over the simple model, and the
new semantic model (containing the features
SEMFUN, SP and GSP in addition to BB, BA
and POS) also achieves some improvements over
the syntactic model (containing GPOS, SYN-
FUN, SPPOS, SPGPOS and SPSYNFUN in ad-
dition to BB, BA and POS), but none of these
improvements are statistically significant using
binomial test.
Finally, we ran an experiment using all 13
features, plus one intonational feature. The per-
formance achieved by using all predictors was a
little worse than the semantic model but a little
better than the simple model. Again none of
these changes are statistically significant.
This experiment suggests that there is some
redundancy among features. All the more com-
plicated models failed to achieve significant im-
provements over the simple model which only
has three features. Thus, overall, we can con-
clude from this first set of experiments that
FUF/SURGE features do improve performance
over the baseline, but they do not indicate con-
clusively which features are best for each of the
4 intonation models.
</bodyText>
<subsubsectionHeader confidence="0.671508">
4.2.2 Salient Predictors
</subsubsectionHeader>
<bodyText confidence="0.999912828571428">
Although RIPPER has the ability to select pre-
dictors for its rules which increase accuracy, it&apos;s
not clear whether all the features in the RIP-
PER rules are necessary. Our first experiment
seems to suggest that irrelevant features could
damage the performance of RIPPER because
the model with all features generally performs
worse than the semantic model. Therefore, the
purpose of the second experiment is to find the
salient predictors and eliminate redundant and
irrelevant ones. The result of this study also
helps us gain a better understanding of the re-
lations between FUF/SURGE features and in-
tonation.
Since the response variables, such as break
index and pitch accent, are categorical values,
a generalized linear model is appropriate. We
mapped all intonation features into binary val-
ues as required in this framework (e.g., pitch
accent is mapped to either &amp;quot;accent&amp;quot; or
accent&amp;quot;). The The resulting data are analyzed by
the generalized linear model in a step-wise fash-
ion. At each step, a predictor is selected and
dropped based on how well the new model can
fit the data. For example, in the break index
model, after GSP is dropped, the new model
achieves the same performance as the initial
model. This suggests that GSP is redundant
for break index.
Since the mapping process removes distinc-
tions within the original categories, it is possi-
ble that the simplified model will not perform
as well as the original model. To confirm that
the simplified model still performs reasonably
well, the new simplified models are tested by
</bodyText>
<page confidence="0.955288">
1006
</page>
<table confidence="0.999882875">
Model Selected Features Dropped features Model Accuracy Rule No. Conditions
New initial New initial New initial
Break BB BA GPOS SPGPOS SP- NO LEX POS SPPOS SP 87.94% 88.29% 7 9 18 16
Index SYNFUN GSP SEMFUN SYNFUN
ACCENT
X&apos;&apos;itch NO BB BA POS GPOS LEX SP 73.87% 73.95% 11 11 20 21
Accent SYNFUN SEMFUN GSP
SPPOS SPGPOS SPSYN-
FUN INDEX
Khrase NO BB BA POS GPOS LEX SP GSP SEMFUN 86.72% 88.08% 5 9 15 25
ccent SYNFUN SPPOS SPGPOS
SPSYNFUN ACCENT
Boundary NO BB BA GSP LEX POS GPOS SYN- 97.36% 96.79% 2 5 4 8
Tone FUN SEMFUN SP SPPOS
SPGPOS SPSYNFUN AC-
CENT
</table>
<tableCaption confidence="0.999519">
Table 2: The New model v.s. the original model
</tableCaption>
<bodyText confidence="0.999833914285714">
letting RIPPER learn new rules based only on
the selected predictors.
Table 2 shows the performance of the new
models versus the original models. As shown
in the &amp;quot;selected features&amp;quot; and &amp;quot;dropped fea-
tures&amp;quot; column, almost half of the predictors are
dropped (average number of factors dropped is
44.64%), and the new model achieves similar
performance.
For boundary tone, the accuracy of the rules
learned from the new model is higher than the
original model. For all other three models, the
accuracy is slightly less but very close to the old
models. Another interesting observation is that
the pitch accent model appears to be more com-
plicated than the other models. Twelve features
are kept in this model, which include syntactic,
semantic and intonational features. The other
three models are associated with fewer features.
The boundary tone model appears to be the
simplest with only 4 features selected.
A similar experiment was done for data com-
bined from the two speakers. An additional
variable called &amp;quot;speaker&amp;quot; is added into the
model. Again, the data is analyzed by the gen-
eralized linear model. The results show that
&amp;quot;speaker&amp;quot; is consistently selected by the sys-
tem as an important factor in all 4 models.
This means that different speakers will result
in different intonational models. As a result, we
based our experiments on a single speaker in-
stead of combining the data from both speakers
into a single model. At this point, we carried
out no other experiments to study speaker dif-
ference.
</bodyText>
<subsectionHeader confidence="0.886664">
4.2.3 Sequential Rules
</subsectionHeader>
<bodyText confidence="0.999952828571429">
The simplified model acquired from Experiment
2 was quite helpful in reducing the complexity
of the remaining experiments which were de-
signed to take the intra-sentential context into
consideration. Much of intonation is not only
affected by features from isolated words, but
also by words in context. For example, usually
there are no adjacent intonational or intermedi-
ate phrase boundaries. Therefore, assigning one
boundary affects when the next boundary can
be assigned. In order to account for this type of
interaction, we extract features of words within
a window of size 2i-1-1 for 1=0,1,2,3; thus, for
each experiment, the features of the i previous
adjacent words, the i following adjacent words
and the current word are extracted. Only the
salient predictors selected by experiment 2 are
explored here.
The results in Table 3 show that intra-
sentential context appears to be important in
improving the performance of the intonation
models. The accuracies of break index, phrase
accent and boundary tone model, shown in the
&amp;quot;Accuracy&amp;quot; columns, are around 90% after the
window size is increased from 1 to 7. The accu-
racy of pitch accent model is around 80%. Ex-
cept the boundary tone model, the best perfor-
mance for all other three models improve sig-
nificantly over the simple model with p=0.0017
for break index model, p=0 for both pitch ac-
cent and phrase accent model. Similarly, they
are also significantly improved over the model
without context information with p=0.0135 for
break index, p=0 for both phrase accent and
pitch accent.
</bodyText>
<subsectionHeader confidence="0.999791">
4.3 The Rules Learned
</subsectionHeader>
<bodyText confidence="0.999736666666667">
In this section we describe some typical rules
learned with relatively high accuracy. The fol-
lowing is a 5-word window pitch accent rule.
</bodyText>
<sectionHeader confidence="0.740272" genericHeader="method">
IF ACCENT1=NA and POS=adv
THEN ACCENT=H* (12/0)
</sectionHeader>
<bodyText confidence="0.998394">
This states that if the following word is de-
accented and the current word&apos;s part of speech
is &amp;quot;adv&amp;quot;, then the current word should be ac-
cented. It covers 12 positive examples and no
</bodyText>
<page confidence="0.956193">
1007
</page>
<table confidence="0.998748">
Size Break Index Pitch Accent Phrase Accent Boundary tone
Accuracy rule condi- Accuracy rule condi- Accuracy rule condi- Accuracy rule condi-
# tion# # tion# # tion# # tion#
1 87.94% 7 18 73.87% 11 20 86.72% 5 15 97.36% 2 4
3 89.87% 5 11 78.87% 11 25 88.22% 7 15 97.36% 2 4
5 89.86% 8 26 80.30% 12 29 90.29% 8 23 97.15% 2 4
7 88.44% 8 20 77.73% 11 20 89.58% 9 26 97.07% 3 5
</table>
<tableCaption confidence="0.999531">
Table 3: System performance with different window size
</tableCaption>
<bodyText confidence="0.956087176470588">
negative examples in the training data.
A break index rule with a 5-word window is:
IF BB1=CB and SPPOS1=relative-pronoun
THEN INDEX=3 (23/0)
This rule tells us if the boundary before the
next word is a clause boundary and the next
word&apos;s semantic parent&apos;s part of speech is rel-
ative pronoun, then there is an intermediate
phrase boundary after the current word. This
rule is supported by 23 examples in the training
data and contradicted by none.
Although the above 5-word window rules only
involve words within a 3-word window, none
of these rules reappears in the 3-word window
rules. They are partially covered by other rules.
For example, there is a similar pitch accent rule
in the 3-word window model:
</bodyText>
<subsectionHeader confidence="0.601434">
IF POS=adv THEN ACCENT=H* (22/5)
</subsectionHeader>
<bodyText confidence="0.999935166666667">
This indicates a strong interaction between
rules learned before and after. Since RIPPER
uses a local optimization strategy, the final re-
sults depend on the order of selecting classifiers.
If the data set is large enough, this problem can
be alleviated.
</bodyText>
<sectionHeader confidence="0.996279" genericHeader="method">
5 Generation Architecture
</sectionHeader>
<bodyText confidence="0.999964736842106">
The final rules learned in Experiment 3 include
intonation features as predictors. In order to
make use of these rules, the following procedure
is applied twice in our generation component.
First, intonation is modeled with FUF/SURGE
features only. Although this model is not as
good as the final model, it still accounts for
the majority of the success with more than 73%
accuracy for all 4 intonation features. Then,
after all words have been assigned an initial
value, the final rules learned in Experiment 3
are applied and the refined results are used
to generate an abstract intonation description
represented in the Speech Integrating Markup
Language(SIML) format (Pan and McKeown,
1997). This abstract description is then trans-
formed into specific TTS control parameters.
Our current corpus is very small. Expand-
ing the corpus with new sentences is necessary.
</bodyText>
<figureCaption confidence="0.999258">
Figure 2: Generation System Architecture
</figureCaption>
<bodyText confidence="0.999966192307692">
Discourse, pragmatic and other semantic fea-
tures will be added into our future intonation
model. Therefore, the rules implemented in the
generation component must be continuously up-
graded. Implementing a fixed set of rules is un-
desirable. As a result, our current generation
component shown in Figure 2 focuses on facil-
itating the updating of the intonation model.
Two separate rule sets (with or without intona-
tion features as predictors) are learned as before
and stored in rulebasel and rulebase2 respec-
tively. A rule interpreter is designed to parse
the rules in the rule bases. The interpreter ex-
tracts features and values encoded in the rules
and passes them to the intonation generator.
The features extracted from the FUF/SURGE
are compared with the features from the rules.
If all conditions of a rule match the features
from FUF/SURGE, a word is assigned the clas-
sified value (the RHS of the rule). Otherwise,
other rules are tried until it is assigned a value.
The rules are tried one by one based on the or-
der in which they are learned. After every word
is tagged with all 4 intonation features, a con-
verter transforms the abstract description into
specific TTS control parameters.
</bodyText>
<sectionHeader confidence="0.998674" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999341375">
In this paper, we describe an effective way to
automatically learn intonation rules. This work
is unique and original in its use of linguistic fea-
tures provided in a general purpose NLG tool to
build intonation models. The machine-learned
rules consistently performed well over all into-
nation features with accuracies around 90% for
break index, phrase accent and boundary tone.
</bodyText>
<figure confidence="0.554613">
Generation
Component
Feature Extractor-41 NLG System
</figure>
<page confidence="0.969244">
1008
</page>
<bodyText confidence="0.999946333333333">
For pitch accent, the model accuracy is around
80%. This yields a significant improvement over
the baseline models and compares well with
other TTS evaluations. Since we used differ-
ent data set than those used in previous TTS
experiments, we cannot accurately quantify the
difference in results, we plan to carry out experi-
ments to evaluate CTS versus TTS performance
using the same data set in the future. We also
designed an intonation generation architecture
for our spoken language generation component
where the intonation generation module dynam-
ically applies newly learned rules to facilitate
the updating of the intonation model.
In the future, discourse and pragmatic infor-
mation will be investigated based on the same
methodology. We will collect a larger speech
corpus to improve accuracy of the rules. Fi-
nally, an integrated spoken language generation
system based on FUF/SURGE will be devel-
oped based on the results of this research.
</bodyText>
<sectionHeader confidence="0.998246" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998361">
Thanks to J. Hirschberg, D. Litman, J. Klavans,
V. Hatzivassiloglou and J. Shaw for comments.
This material is based upon work supported by
the National Science Foundation under Grant
No. IRI 9528998 and the Columbia University
Center for Advanced Technology in High Per-
formance Computing and Communications in
Healthcare (funded by the New York state Sci-
ence and Technology Foundation under Grant
No. NYSSTF CAT 97013 SC1).
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998723071428572">
J. Bachenko and E. Fitzpatrick. 1990. A
computational grammar of discourse-neutral
prosodic phrasing in English. Computational
Linguistics, 16(3):155-170.
Mary Beckman and Julia Hirschberg. 1994.
The ToBI annotation conventions. Technical
report, Ohio State University, Columbus.
L. Brieman, J.H. Friedman, R.A. Olshen, and
C.J. Stone. 1984. Classification and Regres-
sion Trees. Wadsworth and Brooks, Monter-
rey, CA.
John Chambers and Trevor Hastie. 1992.
Statistical Models In S. Wadsworth &amp;
Brooks/Cole Advanced Book &amp; Software, Pa-
cific Grove, California.
William Cohen. 1995. Fast effective rule induc-
tion. In Proceedings of the 12th International
Conference on Machine Learning.
Mukesh Dalal, Steve Feiner, Kathy McKeown,
Shimei Pan, Michelle Zhou, Tobias Hoellerer,
James Shaw, Yong Feng, and Jeanne Fromer.
1996. Negotiation for automated generation
of temporal multimedia presentations. In
Proceedings of ACM Multimedia 1996, pages
55-64.
J. Davis and J. Hirschberg. 1988. Assigning
intonational features in synthesized spoken
discourse. In Proceedings of the 26th An-
nual Meeting of the Association for Compu-
tational Linguistics, pages 187-193, Buffalo,
New York.
M. Elhadad. 1993. Using Argumentation
to Control Lexical Choice: A Functional
Unification Implementation. Ph.D. thesis,
Columbia University.
Michael A. K. Halliday. 1985. An Introduction
to Functional Grammar. Edward Arnold,
London.
Julia Hirschberg. 1993. Pitch accent in con-
text:predicting intonational prominence from
text. Artificial Intelligence, 63:305-340.
Kathleen McKeown, Shimei Pan, James Shaw,
Desmond Jordan, and Barry Allen. 1997.
Language generation for multimedia health-
care briefings. In Proc. of the Fifth ACL
Conf. on ANLP, pages 277-282.
Shimei Pan and Kathleen McKeown. 1997. In-
tegrating language generation with speech
synthesis in a concept to speech system.
In Proceedings of ACL/EACL&apos;97 Concept to
Speech Workshop, Madrid, Spain.
Janet Pierrehumbert. 1980. The Phonology and
Phonetics of English Intonation. Ph.D. the-
sis, Massachusetts Institute of Technology.
S. Prevost. 1995. A Semantics of Contrast and
Information Structure for Specifying Intona-
tion in Spoken Language Generation. Ph.D.
thesis, University of Pennsylvania.
Jacques Robin. 1994. Revision-Based Gener-
ation of Natural Language Summaries Pro-
viding Historical Background. Ph.D. thesis,
Columbia University.
Michelle Wang and Julia Hirschberg. 1992. Au-
tomatic classification of intonational phrase
boundaries. Computer Speech and Language,
6:175-196.
S. Young and F. Fallside. 1979. Speech synthe-
sis from concept: a method for speech out-
put from information systems. Journal of the
Acoustical Society of America, 66:685-695.
</reference>
<page confidence="0.995044">
1009
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000262">
<title confidence="0.999986">Learning Intonation Rules for Concept to Speech Generation</title>
<author confidence="0.999694">Pan McKeown</author>
<affiliation confidence="0.9999135">Dept. of Computer Science Columbia University</affiliation>
<address confidence="0.999954">New York, NY 10027, USA</address>
<email confidence="0.998512">panacs.columbia.edu</email>
<email confidence="0.998512">Icathyacs.columbia.edu</email>
<abstract confidence="0.989653992740473">In this paper, we report on an effort to provide a general-purpose spoken language generation tool for Concept-to-Speech (CTS) applications by extending a widely used text generation package, FUF/SURGE, with an intonation generation component. As a first step, we applied machine learning and statistical models to learn intonation rules based on the semantic and syntactic information typically represented at the sentence The results of this study are a set of intonation rules learned automatically which can be directly implemented in our intonation generation component. Through 5-fold cross-validation, we show the learned rules achieve around accuracy for break index, boundary tone and phrase accent and 80% accuracy for pitch accent. Our study is unique in its use of features produced by language generation to control intonation. The methodology adopted here can be employed directly when more discourse/pragmatic information is to be considered in the future. 1 Motivation Speech is rapidly becoming a viable medium for interaction with real-world applications. Spoken language interfaces to on-line information, such as plane or train schedules, through display-less systems, such as telephone interfaces, are well under development. Speech interfaces are also widely used in applications where eyes-free and hands-free communication is critical, such as car navigation. Natural language generation (NLG) can enhance the ability of such systems to communicate naturally and effectively by allowing the system to tailor, reorganize, or summarize lengthy database re- For example, in work on a multimedia generation system where speech and generation techniques are used to automatically summarize patient&apos;s pre-, during, and post-, operation status to different caregivers (Dalai et al., 1996), records relevant to patient status can easily number in the thousands. Through content planning, sentence planning and lexical selection, the NLG component is able to provide a concise, yet informative, briefing automatically through spoken and written language coordinated with graphics (McKeown et al., 1997) . Integrating language generation with speech synthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural information constructed by the underlying NLG component to improve the quality of synthesized speech. Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language generation tool, the FUF/SURGE system (Elhadad, 1993; Robin, 1994). The ultimate goal of this study is to provide a spoken language generation tool based on FUF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications. 2 Related Theories Two elements form the theoretical background of this work: the grammar used in Pierrehumbert&apos;s intonation theory (Pierrehumbert, 1980). Our study aims at identifying the relations between the semantic/syntactic information produced by FUF/SURGE and four intonational features of Pierrehumbert: pitch accent, phrase accent, tone intermediate/intonational phrase boundaries. 1003 The FUF/SURGE grammar is primarily based on systemic grammar (Halliday, 1985). In systemic grammar, the process (ultimately realized as the verb) is the core of a clause&apos;s semantic structure. Obligatory semantic roles, called participants, are associated with each process. Usually, participants convey who/what is involved in the process. The process also has non-obligatory peripheral semantic roles called circumstances. Circumstances answer questions such as when/where/how/why. In FUF/SURGE, this semantic description is unified with a syntactic grammar to generate a syntactic description. All semantic, syntactic and lexical information, which are produced during the generation process, are kept in a final Functional Description (FD), before linearizing the syntactic structure into a linear string. The features used in our intonation model are mainly extracted from this final FD. The intonation theory proposed in (Pierrehumbert, 1980) is used to describe the intonation structure. Based on her intonation grammar, the FO pitch contour is described by a set of intonational features. The tune of a sentence is formed by one or more intonational phrases. Each intonational phrase consists of one or more intermediate phrases followed by a boundary tone. A well-formed intermediate phrase has one or more pitch accents followed by a phrase accent. Based on this theory, there are four features which are critical in deciding the FO contour: the placement of intonational or intermediate phrase boundaries (break index 4 and 3 in ToBI annotation convention (Beckman and Hirschberg, 1994)), the tonal type at these boundaries (the phrase accent and the boundary tone), and the FO local maximum or minimum (the pitch accent). 3 Related Work Previous work on intonation modeling primarily focused on TTS applications. For example, in (Ba,chenko and Fitzpatrick, 1990), a set of hand-crafted rules are used to determine discourse neutral prosodic phrasing, achieving an accuracy of approximately 85%. Recently, researchers improved on manual development of rules by acquiring prosodic phrasing rules with machine learning tools. In (Wang and Hirschberg, 1992), Classification And Regression Tree (CART) (Brieman et al., 1984) was used to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg, 1988; Young and Fallside, 1979; Prevost, 1995). Many CTS systems were simplified as text generation followed by TTS. Others that do integrate generation make use of the structural information provided by the NLG component (Prevost, 1995). However, most previous CTS systems are not based on large scale general NLG systems. 4 Modeling Intonation While previous research provides some correlation between linguistic features and intonation, more knowledge is needed. The NLG component provides very rich syntactic and semantic information which has not been explored before for intonation modeling. This includes, for example, the semantic role played by each semantic constituent. In developing a CTS, it is worth taking advantage of these features. Previous TTS research results cannot be implemented directly in our intonation generation component. Many features studied in TTS are not provided by FUF/SURGE. For example, the part-of-speech (POS) tags in FUF/SURGE are different from those used in TTS. Furthermore, it make little sense to apply part of speech tagging to generated text instead of using the accurate POS provided in a NLG system. Finally, NLG provides information that is difficult to accurately obtain from full text (e.g., complete syntactic parses). These motivating factors led us to carry out a study consisting of a series of three experiments designed to answer the following questions: • How do the different features produced to determining intonation? • What is the minimal number of features needed to achieve the best accuracy for each of the four intonation features? • Does intra-sentential context improve accuracy? 1004 ((cat clause) (process ((type ascriptive) (mode equative))) (participant ((identified ((lex &amp;quot;John&amp;quot;) (cat proper))) (identifier ((lex &amp;quot;teacher&amp;quot;) (cat common)))))) Figure 1: Semantic description 4.1 Tools and Data In order to model intonational features automatically, features from FUF/SURGE and a speech corpus are provided as input to a machine learning tool called RIPPER (Cohen, 1995), which produces a set of classification rules based on the training examples. The performance of RIPPER is comparable to benchmark decision tree induction systems such as CART and C4.5. We also employ a statistical method based on a generalized linear model (Chambers and Hastie, 1992) provided in the S package to select salient predictors for input to RIPPER. Figure 1 shows the input Functional Description(FD) for the sentence &amp;quot;John is the teacher&amp;quot;. After this FD is unified with the syntactic grammar, SURGE, the resulting FD includes hundreds of semantic, syntactic and lexical features. We extract 13 features shown in Table 1 which are more closely related to intonation as indicated by previous research. We have chosen features which are applicable to most words to avoid unspecified values in the training data. For example, &amp;quot;tense&amp;quot; is not extracted simply because it can be only applied to verbs. Table 1 includes descriptions for each of the features used. These are divided into semantic, syntactic, and semi-syntactic/semantic features which describe the syntactic properties of semantic constituents. Finally, word position (NO.) and the actual word (LEX) are extracted directly from the linearized string. About 400 isolated sentences with wide coverage of various linguistic phenomena were created as test cases for FUF/SURGE when it was developed. We asked two male native speakers to read 258 sentences, each sentence may be repeated several times. The speech was recorded on a DAT in an office. The most fluent version of each sentence was kept. The resulting speech by one author based on ToBI with break index, pitch accent, phrase accent and boundary tone labeled, using the XWAVE speech analysis tool. The 13 features described in Table 1 as well as one intonation feature are used as predictors for the response intonation feature. The final corpus contains 258 sentences for each speaker, including 119 noun phrases, 37 of which have embeded sentences, and 139 sentences. The average sentence/phrase length is 5.43 words. The baseline performance achieved by always guessing the majority class is 67.09% for break index, 54.10% for pitch accent, 66.23% for phrase accent and 79.37% for boundary tone based on the speech corpus from one speaker. The relatively high baseline for boundary tone is because for most of the cases, there is only one L% boundary tone at the end of each sentence in our training data. Speaker effect on intonation is briefly studied in experiment 2. All other experiments used data from one speaker with the above baselines. 4.2 Experiments 4.2.1 Interesting Combinations Our first set of experiments was designed as an initial test of how the features from FUF/SURGE contribute to intonation. We focused on how the newly available semantic features affect intonation. We were also interested in finding out whether the 13 selected features are redundant in making intonation decisions. We started from a simple model which includes only 3 factors, the type of semantic constituent boundary before (BB) and after (BA) the word, and part of speech (POS). The semantic constituent boundary can take on 6 different values; for example, it can be a clause boundary, a boundary associated with a primary semantic role (e.g., a participant), with a secondary semantic role (e.g., a type of modifier), among others. Our purpose in this experiment was to test how well the model can do with a limited number of parameters. Applying RIPPER to the simple model yielded rules that significantly improved performance over the baseline models. For example, the accuracy of the rules learned for break index increases to 87.37% from 67.09%; the average improvement on all 4 intonational features is 19.33%. Next, we ran two additional tests, one with additional syntactic features and another with additional semantic features. The results show that the two new models behave similarly on all intonational features; they both achieve some 1005 Category Label Description Examples Semantic BB The semantic constituent boundary before the word, participant boundaries or circumstance boundaries etc. BA semantic constituent after the word. participant boundaries or circumstance boundaries etc. SEMFUN The semantic feature of the word. semantic feature of &amp;quot;did&amp;quot; in know him.&amp;quot; is &amp;quot;insistence&amp;quot;. SP The semantic role played by the immediate parental semantic constituent of the word. The SP of &amp;quot;teacher&amp;quot; in &amp;quot;John is the teacher&amp;quot; is &amp;quot;identifier&amp;quot;. GSP The generic semantic role played by the immediate parental semantic constituent of the word. &amp;quot;teacher&amp;quot; in &amp;quot;John is the teacher&amp;quot; is &amp;quot;participant&amp;quot; Syntactic POS The part of speech of the word common noun, proper noun etc. GPOS The generic part of speech of the word noun is the corresponding GPOS of both common noun and proper noun. SYNFUN The syntactic function of the word The SYNFUN of &amp;quot;teacher&amp;quot; in &amp;quot;the teacher&amp;quot; is &amp;quot;head&amp;quot;. Semi- SPPOS The part of speech of the immediate parental semantic constituent of the word. The SPPOS of &amp;quot;teacher&amp;quot; is &amp;quot;common noun&amp;quot;. semantic&amp; syntactic SPGPOS The generic part of speech of the immediate parental semantic constituent of the word. The SPGPOS of &amp;quot;teacher&amp;quot; in &amp;quot;the teacher&amp;quot; is &amp;quot;noun phrase&amp;quot;. SPSYNFUN The syntactic function of the immediate parental semantic constituent of the word. The SPSYNFUN of &amp;quot;teacher&amp;quot; in &amp;quot;John is the teacher&amp;quot; is &amp;quot;subject complement. Misc. NO. The position of the word in a sentence 1, 2, 3, 4 etc. LEX The lexical form of the word &amp;quot;John&amp;quot;, &amp;quot;is&amp;quot;, &amp;quot;the&amp;quot;, &amp;quot;teacher&amp;quot;etc. Table 1: Features extracted from FUF and SURGE improvements over the simple model, and the new semantic model (containing the features SEMFUN, SP and GSP in addition to BB, BA and POS) also achieves some improvements over the syntactic model (containing GPOS, SYN- FUN, SPPOS, SPGPOS and SPSYNFUN in addition to BB, BA and POS), but none of these improvements are statistically significant using binomial test. Finally, we ran an experiment using all 13 features, plus one intonational feature. The performance achieved by using all predictors was a little worse than the semantic model but a little better than the simple model. Again none of these changes are statistically significant. This experiment suggests that there is some redundancy among features. All the more complicated models failed to achieve significant improvements over the simple model which only has three features. Thus, overall, we can conclude from this first set of experiments that FUF/SURGE features do improve performance over the baseline, but they do not indicate conclusively which features are best for each of the 4 intonation models. 4.2.2 Salient Predictors Although RIPPER has the ability to select predictors for its rules which increase accuracy, it&apos;s not clear whether all the features in the RIP- PER rules are necessary. Our first experiment seems to suggest that irrelevant features could damage the performance of RIPPER because the model with all features generally performs worse than the semantic model. Therefore, the purpose of the second experiment is to find the salient predictors and eliminate redundant and irrelevant ones. The result of this study also helps us gain a better understanding of the refeatures and intonation. Since the response variables, such as break index and pitch accent, are categorical values, a generalized linear model is appropriate. We mapped all intonation features into binary values as required in this framework (e.g., pitch accent is mapped to either &amp;quot;accent&amp;quot; or accent&amp;quot;). The The resulting data are analyzed by the generalized linear model in a step-wise fashion. At each step, a predictor is selected and dropped based on how well the new model can fit the data. For example, in the break index model, after GSP is dropped, the new model achieves the same performance as the initial model. This suggests that GSP is redundant for break index. Since the mapping process removes distinctions within the original categories, it is possible that the simplified model will not perform as well as the original model. To confirm that the simplified model still performs reasonably well, the new simplified models are tested by 1006 Model Selected Features Dropped features Model Accuracy Rule No. Conditions New initial New initial New initial Break BB BA GPOS SPGPOS SP- SYNFUN NO LEX POS SPPOS SP GSP SEMFUN SYNFUN ACCENT 87.94% 88.29% 7 9 18 16 Index X&apos;&apos;itch Accent NO BB BA POS GPOS SYNFUN SEMFUN GSP SPPOS SPGPOS SPSYN-FUN INDEX LEX SP 73.87% 73.95% 11 11 20 21 Khrase NO BB BA POS GPOS SYNFUN SPPOS SPGPOS SPSYNFUN ACCENT LEX SP GSP SEMFUN 86.72% 88.08% 5 9 15 25 ccent Boundary Tone NO BB BA GSP LEX POS GPOS SYN- FUN SEMFUN SP SPPOS SPGPOS SPSYNFUN AC-CENT 97.36% 96.79% 2 5 4 8 Table 2: The New model v.s. the original model letting RIPPER learn new rules based only on the selected predictors. Table 2 shows the performance of the new models versus the original models. As shown in the &amp;quot;selected features&amp;quot; and &amp;quot;dropped features&amp;quot; column, almost half of the predictors are dropped (average number of factors dropped is 44.64%), and the new model achieves similar performance. For boundary tone, the accuracy of the rules learned from the new model is higher than the original model. For all other three models, the accuracy is slightly less but very close to the old models. Another interesting observation is that the pitch accent model appears to be more complicated than the other models. Twelve features are kept in this model, which include syntactic, semantic and intonational features. The other three models are associated with fewer features. The boundary tone model appears to be the simplest with only 4 features selected. A similar experiment was done for data combined from the two speakers. An additional variable called &amp;quot;speaker&amp;quot; is added into the model. Again, the data is analyzed by the generalized linear model. The results show that &amp;quot;speaker&amp;quot; is consistently selected by the system as an important factor in all 4 models. This means that different speakers will result in different intonational models. As a result, we based our experiments on a single speaker instead of combining the data from both speakers into a single model. At this point, we carried out no other experiments to study speaker difference. 4.2.3 Sequential Rules The simplified model acquired from Experiment 2 was quite helpful in reducing the complexity of the remaining experiments which were designed to take the intra-sentential context into consideration. Much of intonation is not only affected by features from isolated words, but also by words in context. For example, usually there are no adjacent intonational or intermediate phrase boundaries. Therefore, assigning one boundary affects when the next boundary can be assigned. In order to account for this type of interaction, we extract features of words within a window of size 2i-1-1 for 1=0,1,2,3; thus, for each experiment, the features of the i previous adjacent words, the i following adjacent words and the current word are extracted. Only the salient predictors selected by experiment 2 are explored here. The results in Table 3 show that intrasentential context appears to be important in improving the performance of the intonation models. The accuracies of break index, phrase accent and boundary tone model, shown in the &amp;quot;Accuracy&amp;quot; columns, are around 90% after the window size is increased from 1 to 7. The accuracy of pitch accent model is around 80%. Except the boundary tone model, the best performance for all other three models improve significantly over the simple model with p=0.0017 for break index model, p=0 for both pitch accent and phrase accent model. Similarly, they are also significantly improved over the model without context information with p=0.0135 for break index, p=0 for both phrase accent and pitch accent. 4.3 The Rules Learned In this section we describe some typical rules learned with relatively high accuracy. The following is a 5-word window pitch accent rule. IF ACCENT1=NA and POS=adv THEN ACCENT=H* (12/0) This states that if the following word is deaccented and the current word&apos;s part of speech is &amp;quot;adv&amp;quot;, then the current word should be accented. It covers 12 positive examples and no 1007 Size Break Index Pitch Accent Phrase Accent Boundary tone Accuracy rule condi- Accuracy rule condi- Accuracy rule condi- Accuracy rule condi- 1 87.94% 7 18 73.87% 11 20 86.72% 5 15 97.36% 2 4 3 89.87% 5 11 78.87% 11 25 88.22% 7 15 97.36% 2 4 5 89.86% 8 26 80.30% 12 29 90.29% 8 23 97.15% 2 4 7 88.44% 8 20 77.73% 11 20 89.58% 9 26 97.07% 3 5 Table 3: System performance with different window size negative examples in the training data. A break index rule with a 5-word window is: IF BB1=CB and SPPOS1=relative-pronoun THEN INDEX=3 (23/0) This rule tells us if the boundary before the next word is a clause boundary and the next word&apos;s semantic parent&apos;s part of speech is relative pronoun, then there is an intermediate phrase boundary after the current word. This rule is supported by 23 examples in the training data and contradicted by none. Although the above 5-word window rules only involve words within a 3-word window, none of these rules reappears in the 3-word window rules. They are partially covered by other rules. For example, there is a similar pitch accent rule in the 3-word window model: IF POS=adv THEN ACCENT=H* (22/5) This indicates a strong interaction between rules learned before and after. Since RIPPER uses a local optimization strategy, the final results depend on the order of selecting classifiers. If the data set is large enough, this problem can be alleviated. 5 Generation Architecture The final rules learned in Experiment 3 include intonation features as predictors. In order to make use of these rules, the following procedure is applied twice in our generation component. intonation is modeled with features only. Although this model is not as good as the final model, it still accounts for the majority of the success with more than 73% accuracy for all 4 intonation features. Then, after all words have been assigned an initial value, the final rules learned in Experiment 3 are applied and the refined results are used to generate an abstract intonation description represented in the Speech Integrating Markup Language(SIML) format (Pan and McKeown, 1997). This abstract description is then transformed into specific TTS control parameters. Our current corpus is very small. Expanding the corpus with new sentences is necessary. Figure 2: Generation System Architecture Discourse, pragmatic and other semantic features will be added into our future intonation model. Therefore, the rules implemented in the generation component must be continuously upgraded. Implementing a fixed set of rules is undesirable. As a result, our current generation component shown in Figure 2 focuses on facilitating the updating of the intonation model. Two separate rule sets (with or without intonation features as predictors) are learned as before and stored in rulebasel and rulebase2 respectively. A rule interpreter is designed to parse the rules in the rule bases. The interpreter extracts features and values encoded in the rules and passes them to the intonation generator. features extracted from the are compared with the features from the rules. If all conditions of a rule match the features from FUF/SURGE, a word is assigned the classified value (the RHS of the rule). Otherwise, other rules are tried until it is assigned a value. The rules are tried one by one based on the order in which they are learned. After every word is tagged with all 4 intonation features, a converter transforms the abstract description into specific TTS control parameters. 6 Conclusion and Future Work In this paper, we describe an effective way to automatically learn intonation rules. This work is unique and original in its use of linguistic features provided in a general purpose NLG tool to build intonation models. The machine-learned rules consistently performed well over all intonation features with accuracies around 90% for break index, phrase accent and boundary tone.</abstract>
<title confidence="0.778454">Generation Component Feature Extractor-41 NLG System</title>
<date confidence="0.506034">1008</date>
<abstract confidence="0.972134">For pitch accent, the model accuracy is around 80%. This yields a significant improvement over the baseline models and compares well with other TTS evaluations. Since we used different data set than those used in previous TTS experiments, we cannot accurately quantify the difference in results, we plan to carry out experiments to evaluate CTS versus TTS performance using the same data set in the future. We also designed an intonation generation architecture for our spoken language generation component where the intonation generation module dynamically applies newly learned rules to facilitate the updating of the intonation model. In the future, discourse and pragmatic information will be investigated based on the same methodology. We will collect a larger speech corpus to improve accuracy of the rules. Finally, an integrated spoken language generation system based on FUF/SURGE will be developed based on the results of this research. 7 Acknowledgement Thanks to J. Hirschberg, D. Litman, J. Klavans, V. Hatzivassiloglou and J. Shaw for comments.</abstract>
<note confidence="0.91290175">This material is based upon work supported by the National Science Foundation under Grant No. IRI 9528998 and the Columbia University Center for Advanced Technology in High Per-</note>
<title confidence="0.643192">formance Computing and Communications in Healthcare (funded by the New York state Science and Technology Foundation under Grant</title>
<note confidence="0.785792344827586">No. NYSSTF CAT 97013 SC1). References J. Bachenko and E. Fitzpatrick. 1990. A computational grammar of discourse-neutral phrasing in English. Mary Beckman and Julia Hirschberg. 1994. The ToBI annotation conventions. Technical report, Ohio State University, Columbus. L. Brieman, J.H. Friedman, R.A. Olshen, and Stone. 1984. and Regres- Trees. and Brooks, Monterrey, CA. John Chambers and Trevor Hastie. 1992. Models In S. &amp; Brooks/Cole Advanced Book &amp; Software, Pacific Grove, California. William Cohen. 1995. Fast effective rule induc- In of the 12th International Conference on Machine Learning. Mukesh Dalal, Steve Feiner, Kathy McKeown, Shimei Pan, Michelle Zhou, Tobias Hoellerer, James Shaw, Yong Feng, and Jeanne Fromer. 1996. Negotiation for automated generation of temporal multimedia presentations. In of ACM Multimedia 1996, 55-64. J. Davis and J. Hirschberg. 1988. Assigning intonational features in synthesized spoken In of the 26th An-</note>
<affiliation confidence="0.694618">nual Meeting of the Association for Compu-</affiliation>
<address confidence="0.9052775">Linguistics, 187-193, Buffalo, New York.</address>
<note confidence="0.548083666666667">Elhadad. 1993. Argumentation to Control Lexical Choice: A Functional Implementation. thesis,</note>
<affiliation confidence="0.76001">Columbia University.</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bachenko</author>
<author>E Fitzpatrick</author>
</authors>
<title>A computational grammar of discourse-neutral prosodic phrasing in English.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--3</pages>
<marker>Bachenko, Fitzpatrick, 1990</marker>
<rawString>J. Bachenko and E. Fitzpatrick. 1990. A computational grammar of discourse-neutral prosodic phrasing in English. Computational Linguistics, 16(3):155-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Beckman</author>
<author>Julia Hirschberg</author>
</authors>
<title>The ToBI annotation conventions.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>Ohio State University,</institution>
<location>Columbus.</location>
<contexts>
<context position="5252" citStr="Beckman and Hirschberg, 1994" startWordPosition="788" endWordPosition="791"> the intonation structure. Based on her intonation grammar, the FO pitch contour is described by a set of intonational features. The tune of a sentence is formed by one or more intonational phrases. Each intonational phrase consists of one or more intermediate phrases followed by a boundary tone. A well-formed intermediate phrase has one or more pitch accents followed by a phrase accent. Based on this theory, there are four features which are critical in deciding the FO contour: the placement of intonational or intermediate phrase boundaries (break index 4 and 3 in ToBI annotation convention (Beckman and Hirschberg, 1994)), the tonal type at these boundaries (the phrase accent and the boundary tone), and the FO local maximum or minimum (the pitch accent). 3 Related Work Previous work on intonation modeling primarily focused on TTS applications. For example, in (Ba,chenko and Fitzpatrick, 1990), a set of hand-crafted rules are used to determine discourse neutral prosodic phrasing, achieving an accuracy of approximately 85%. Recently, researchers improved on manual development of rules by acquiring prosodic phrasing rules with machine learning tools. In (Wang and Hirschberg, 1992), Classification And Regression </context>
</contexts>
<marker>Beckman, Hirschberg, 1994</marker>
<rawString>Mary Beckman and Julia Hirschberg. 1994. The ToBI annotation conventions. Technical report, Ohio State University, Columbus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Brieman</author>
<author>J H Friedman</author>
<author>R A Olshen</author>
<author>C J Stone</author>
</authors>
<title>Classification and Regression Trees. Wadsworth and Brooks,</title>
<date>1984</date>
<location>Monterrey, CA.</location>
<contexts>
<context position="5886" citStr="Brieman et al., 1984" startWordPosition="886" endWordPosition="889">type at these boundaries (the phrase accent and the boundary tone), and the FO local maximum or minimum (the pitch accent). 3 Related Work Previous work on intonation modeling primarily focused on TTS applications. For example, in (Ba,chenko and Fitzpatrick, 1990), a set of hand-crafted rules are used to determine discourse neutral prosodic phrasing, achieving an accuracy of approximately 85%. Recently, researchers improved on manual development of rules by acquiring prosodic phrasing rules with machine learning tools. In (Wang and Hirschberg, 1992), Classification And Regression Tree (CART) (Brieman et al., 1984) was used to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg,</context>
</contexts>
<marker>Brieman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Brieman, J.H. Friedman, R.A. Olshen, and C.J. Stone. 1984. Classification and Regression Trees. Wadsworth and Brooks, Monterrey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chambers</author>
<author>Trevor Hastie</author>
</authors>
<title>Statistical Models In</title>
<date>1992</date>
<booktitle>S. Wadsworth &amp; Brooks/Cole Advanced Book &amp; Software,</booktitle>
<location>Pacific Grove, California.</location>
<contexts>
<context position="8839" citStr="Chambers and Hastie, 1992" startWordPosition="1348" endWordPosition="1351"> equative))) (participant ((identified ((lex &amp;quot;John&amp;quot;) (cat proper))) (identifier ((lex &amp;quot;teacher&amp;quot;) (cat common)))))) Figure 1: Semantic description 4.1 Tools and Data In order to model intonational features automatically, features from FUF/SURGE and a speech corpus are provided as input to a machine learning tool called RIPPER (Cohen, 1995), which produces a set of classification rules based on the training examples. The performance of RIPPER is comparable to benchmark decision tree induction systems such as CART and C4.5. We also employ a statistical method based on a generalized linear model (Chambers and Hastie, 1992) provided in the S package to select salient predictors for input to RIPPER. Figure 1 shows the input Functional Description(FD) for the sentence &amp;quot;John is the teacher&amp;quot;. After this FD is unified with the syntactic grammar, SURGE, the resulting FD includes hundreds of semantic, syntactic and lexical features. We extract 13 features shown in Table 1 which are more closely related to intonation as indicated by previous research. We have chosen features which are applicable to most words to avoid unspecified values in the training data. For example, &amp;quot;tense&amp;quot; is not extracted simply because it can be</context>
</contexts>
<marker>Chambers, Hastie, 1992</marker>
<rawString>John Chambers and Trevor Hastie. 1992. Statistical Models In S. Wadsworth &amp; Brooks/Cole Advanced Book &amp; Software, Pacific Grove, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings of the 12th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8553" citStr="Cohen, 1995" startWordPosition="1302" endWordPosition="1304">by FUF/SURGE contribute to determining intonation? • What is the minimal number of features needed to achieve the best accuracy for each of the four intonation features? • Does intra-sentential context improve accuracy? 1004 ((cat clause) (process ((type ascriptive) (mode equative))) (participant ((identified ((lex &amp;quot;John&amp;quot;) (cat proper))) (identifier ((lex &amp;quot;teacher&amp;quot;) (cat common)))))) Figure 1: Semantic description 4.1 Tools and Data In order to model intonational features automatically, features from FUF/SURGE and a speech corpus are provided as input to a machine learning tool called RIPPER (Cohen, 1995), which produces a set of classification rules based on the training examples. The performance of RIPPER is comparable to benchmark decision tree induction systems such as CART and C4.5. We also employ a statistical method based on a generalized linear model (Chambers and Hastie, 1992) provided in the S package to select salient predictors for input to RIPPER. Figure 1 shows the input Functional Description(FD) for the sentence &amp;quot;John is the teacher&amp;quot;. After this FD is unified with the syntactic grammar, SURGE, the resulting FD includes hundreds of semantic, syntactic and lexical features. We ex</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>William Cohen. 1995. Fast effective rule induction. In Proceedings of the 12th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mukesh Dalal</author>
<author>Steve Feiner</author>
<author>Kathy McKeown</author>
<author>Shimei Pan</author>
<author>Michelle Zhou</author>
<author>Tobias Hoellerer</author>
<author>James Shaw</author>
<author>Yong Feng</author>
<author>Jeanne Fromer</author>
</authors>
<title>Negotiation for automated generation of temporal multimedia presentations.</title>
<date>1996</date>
<booktitle>In Proceedings of ACM Multimedia</booktitle>
<pages>55--64</pages>
<marker>Dalal, Feiner, McKeown, Pan, Zhou, Hoellerer, Shaw, Feng, Fromer, 1996</marker>
<rawString>Mukesh Dalal, Steve Feiner, Kathy McKeown, Shimei Pan, Michelle Zhou, Tobias Hoellerer, James Shaw, Yong Feng, and Jeanne Fromer. 1996. Negotiation for automated generation of temporal multimedia presentations. In Proceedings of ACM Multimedia 1996, pages 55-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Davis</author>
<author>J Hirschberg</author>
</authors>
<title>Assigning intonational features in synthesized spoken discourse.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Buffalo, New York.</location>
<contexts>
<context position="6491" citStr="Davis and Hirschberg, 1988" startWordPosition="979" endWordPosition="982">Brieman et al., 1984) was used to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg, 1988; Young and Fallside, 1979; Prevost, 1995). Many CTS systems were simplified as text generation followed by TTS. Others that do integrate generation make use of the structural information provided by the NLG component (Prevost, 1995). However, most previous CTS systems are not based on large scale general NLG systems. 4 Modeling Intonation While previous research provides some correlation between linguistic features and intonation, more knowledge is needed. The NLG component provides very rich syntactic and semantic information which has not been explored before for intonation modeling. This i</context>
</contexts>
<marker>Davis, Hirschberg, 1988</marker>
<rawString>J. Davis and J. Hirschberg. 1988. Assigning intonational features in synthesized spoken discourse. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 187-193, Buffalo, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Using Argumentation to Control Lexical Choice: A Functional Unification Implementation.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3012" citStr="Elhadad, 1993" startWordPosition="458" endWordPosition="459">n with speech synthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural information constructed by the underlying NLG component to improve the quality of synthesized speech. Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language generation tool, the FUF/SURGE system (Elhadad, 1993; Robin, 1994). The ultimate goal of this study is to provide a spoken language generation tool based on FUF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications. 2 Related Theories Two elements form the theoretical background of this work: the grammar used in FUF/SURGE and Pierrehumbert&apos;s intonation theory (Pierrehumbert, 1980). Our study aims at identifying the relations between the semantic/syntactic information produced by FUF/SURGE and four intonational features of Pierrehumbert: pitch accent, phrase accent, boundary tone and interm</context>
</contexts>
<marker>Elhadad, 1993</marker>
<rawString>M. Elhadad. 1993. Using Argumentation to Control Lexical Choice: A Functional Unification Implementation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar. Edward</title>
<date>1985</date>
<location>Arnold, London.</location>
<contexts>
<context position="3733" citStr="Halliday, 1985" startWordPosition="558" endWordPosition="559">UF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications. 2 Related Theories Two elements form the theoretical background of this work: the grammar used in FUF/SURGE and Pierrehumbert&apos;s intonation theory (Pierrehumbert, 1980). Our study aims at identifying the relations between the semantic/syntactic information produced by FUF/SURGE and four intonational features of Pierrehumbert: pitch accent, phrase accent, boundary tone and intermediate/intonational phrase boundaries. 1003 The FUF/SURGE grammar is primarily based on systemic grammar (Halliday, 1985). In systemic grammar, the process (ultimately realized as the verb) is the core of a clause&apos;s semantic structure. Obligatory semantic roles, called participants, are associated with each process. Usually, participants convey who/what is involved in the process. The process also has non-obligatory peripheral semantic roles called circumstances. Circumstances answer questions such as when/where/how/why. In FUF/SURGE, this semantic description is unified with a syntactic grammar to generate a syntactic description. All semantic, syntactic and lexical information, which are produced during the ge</context>
</contexts>
<marker>Halliday, 1985</marker>
<rawString>Michael A. K. Halliday. 1985. An Introduction to Functional Grammar. Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
</authors>
<title>Pitch accent in context:predicting intonational prominence from text.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--305</pages>
<contexts>
<context position="6107" citStr="Hirschberg, 1993" startWordPosition="923" endWordPosition="924">e, in (Ba,chenko and Fitzpatrick, 1990), a set of hand-crafted rules are used to determine discourse neutral prosodic phrasing, achieving an accuracy of approximately 85%. Recently, researchers improved on manual development of rules by acquiring prosodic phrasing rules with machine learning tools. In (Wang and Hirschberg, 1992), Classification And Regression Tree (CART) (Brieman et al., 1984) was used to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg, 1988; Young and Fallside, 1979; Prevost, 1995). Many CTS systems were simplified as text generation followed by TTS. Others that do integrate generation make use of the structural information provided by the NLG componen</context>
</contexts>
<marker>Hirschberg, 1993</marker>
<rawString>Julia Hirschberg. 1993. Pitch accent in context:predicting intonational prominence from text. Artificial Intelligence, 63:305-340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Shimei Pan</author>
<author>James Shaw</author>
<author>Desmond Jordan</author>
<author>Barry Allen</author>
</authors>
<title>Language generation for multimedia healthcare briefings.</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth ACL Conf. on ANLP,</booktitle>
<pages>277--282</pages>
<contexts>
<context position="2366" citStr="McKeown et al., 1997" startWordPosition="360" endWordPosition="363">ystem to tailor, reorganize, or summarize lengthy database responses. For example, in our work on a multimedia generation system where speech and graphics generation techniques are used to automatically summarize patient&apos;s pre-, during, and post-, operation status to different caregivers (Dalai et al., 1996), records relevant to patient status can easily number in the thousands. Through content planning, sentence planning and lexical selection, the NLG component is able to provide a concise, yet informative, briefing automatically through spoken and written language coordinated with graphics (McKeown et al., 1997) . Integrating language generation with speech synthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural information constructed by the underlying NLG component to improve the quality of synthesized speech. Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language genera</context>
</contexts>
<marker>McKeown, Pan, Shaw, Jordan, Allen, 1997</marker>
<rawString>Kathleen McKeown, Shimei Pan, James Shaw, Desmond Jordan, and Barry Allen. 1997. Language generation for multimedia healthcare briefings. In Proc. of the Fifth ACL Conf. on ANLP, pages 277-282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimei Pan</author>
<author>Kathleen McKeown</author>
</authors>
<title>Integrating language generation with speech synthesis in a concept to speech system.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL&apos;97 Concept to Speech Workshop,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="23017" citStr="Pan and McKeown, 1997" startWordPosition="3716" endWordPosition="3719">redictors. In order to make use of these rules, the following procedure is applied twice in our generation component. First, intonation is modeled with FUF/SURGE features only. Although this model is not as good as the final model, it still accounts for the majority of the success with more than 73% accuracy for all 4 intonation features. Then, after all words have been assigned an initial value, the final rules learned in Experiment 3 are applied and the refined results are used to generate an abstract intonation description represented in the Speech Integrating Markup Language(SIML) format (Pan and McKeown, 1997). This abstract description is then transformed into specific TTS control parameters. Our current corpus is very small. Expanding the corpus with new sentences is necessary. Figure 2: Generation System Architecture Discourse, pragmatic and other semantic features will be added into our future intonation model. Therefore, the rules implemented in the generation component must be continuously upgraded. Implementing a fixed set of rules is undesirable. As a result, our current generation component shown in Figure 2 focuses on facilitating the updating of the intonation model. Two separate rule se</context>
</contexts>
<marker>Pan, McKeown, 1997</marker>
<rawString>Shimei Pan and Kathleen McKeown. 1997. Integrating language generation with speech synthesis in a concept to speech system. In Proceedings of ACL/EACL&apos;97 Concept to Speech Workshop, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Pierrehumbert</author>
</authors>
<title>The Phonology and Phonetics of English Intonation.</title>
<date>1980</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="3399" citStr="Pierrehumbert, 1980" startWordPosition="516" endWordPosition="517">ech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language generation tool, the FUF/SURGE system (Elhadad, 1993; Robin, 1994). The ultimate goal of this study is to provide a spoken language generation tool based on FUF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications. 2 Related Theories Two elements form the theoretical background of this work: the grammar used in FUF/SURGE and Pierrehumbert&apos;s intonation theory (Pierrehumbert, 1980). Our study aims at identifying the relations between the semantic/syntactic information produced by FUF/SURGE and four intonational features of Pierrehumbert: pitch accent, phrase accent, boundary tone and intermediate/intonational phrase boundaries. 1003 The FUF/SURGE grammar is primarily based on systemic grammar (Halliday, 1985). In systemic grammar, the process (ultimately realized as the verb) is the core of a clause&apos;s semantic structure. Obligatory semantic roles, called participants, are associated with each process. Usually, participants convey who/what is involved in the process. The</context>
</contexts>
<marker>Pierrehumbert, 1980</marker>
<rawString>Janet Pierrehumbert. 1980. The Phonology and Phonetics of English Intonation. Ph.D. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prevost</author>
</authors>
<title>A Semantics of Contrast and Information Structure for Specifying Intonation in Spoken Language Generation.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="6533" citStr="Prevost, 1995" startWordPosition="987" endWordPosition="988">ree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg, 1988; Young and Fallside, 1979; Prevost, 1995). Many CTS systems were simplified as text generation followed by TTS. Others that do integrate generation make use of the structural information provided by the NLG component (Prevost, 1995). However, most previous CTS systems are not based on large scale general NLG systems. 4 Modeling Intonation While previous research provides some correlation between linguistic features and intonation, more knowledge is needed. The NLG component provides very rich syntactic and semantic information which has not been explored before for intonation modeling. This includes, for example, the semantic role pl</context>
</contexts>
<marker>Prevost, 1995</marker>
<rawString>S. Prevost. 1995. A Semantics of Contrast and Information Structure for Specifying Intonation in Spoken Language Generation. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Revision-Based Generation of Natural Language Summaries Providing Historical Background.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3026" citStr="Robin, 1994" startWordPosition="460" endWordPosition="461">ynthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural information constructed by the underlying NLG component to improve the quality of synthesized speech. Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language generation tool, the FUF/SURGE system (Elhadad, 1993; Robin, 1994). The ultimate goal of this study is to provide a spoken language generation tool based on FUF/SURGE, extended with an intonation generation component to facilitate the development of new CTS applications. 2 Related Theories Two elements form the theoretical background of this work: the grammar used in FUF/SURGE and Pierrehumbert&apos;s intonation theory (Pierrehumbert, 1980). Our study aims at identifying the relations between the semantic/syntactic information produced by FUF/SURGE and four intonational features of Pierrehumbert: pitch accent, phrase accent, boundary tone and intermediate/intonat</context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Jacques Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Wang</author>
<author>Julia Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrase boundaries.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--175</pages>
<contexts>
<context position="5820" citStr="Wang and Hirschberg, 1992" startWordPosition="876" endWordPosition="879"> ToBI annotation convention (Beckman and Hirschberg, 1994)), the tonal type at these boundaries (the phrase accent and the boundary tone), and the FO local maximum or minimum (the pitch accent). 3 Related Work Previous work on intonation modeling primarily focused on TTS applications. For example, in (Ba,chenko and Fitzpatrick, 1990), a set of hand-crafted rules are used to determine discourse neutral prosodic phrasing, achieving an accuracy of approximately 85%. Recently, researchers improved on manual development of rules by acquiring prosodic phrasing rules with machine learning tools. In (Wang and Hirschberg, 1992), Classification And Regression Tree (CART) (Brieman et al., 1984) was used to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only li</context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Michelle Wang and Julia Hirschberg. 1992. Automatic classification of intonational phrase boundaries. Computer Speech and Language, 6:175-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>F Fallside</author>
</authors>
<title>Speech synthesis from concept: a method for speech output from information systems.</title>
<date>1979</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>66--685</pages>
<contexts>
<context position="6517" citStr="Young and Fallside, 1979" startWordPosition="983" endWordPosition="986">ed to produce a decision tree to predict the location of prosodic phrase boundaries, yielding a high accuracy, around 90%. Similar methods were also employed in predicting pitch accent for TTS in (Hirschberg, 1993). Hirschberg exploited various features derived from text analysis, such as part of speech tags, information status (i.g. given/new, contrast), and cue phrases; both hand-crafted and automatically learned rules achieved 80-98% success depending on the type of speech corpus. Until recently, there has been only limited effort on modeling intonation for CTS (Davis and Hirschberg, 1988; Young and Fallside, 1979; Prevost, 1995). Many CTS systems were simplified as text generation followed by TTS. Others that do integrate generation make use of the structural information provided by the NLG component (Prevost, 1995). However, most previous CTS systems are not based on large scale general NLG systems. 4 Modeling Intonation While previous research provides some correlation between linguistic features and intonation, more knowledge is needed. The NLG component provides very rich syntactic and semantic information which has not been explored before for intonation modeling. This includes, for example, the </context>
</contexts>
<marker>Young, Fallside, 1979</marker>
<rawString>S. Young and F. Fallside. 1979. Speech synthesis from concept: a method for speech output from information systems. Journal of the Acoustical Society of America, 66:685-695.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>