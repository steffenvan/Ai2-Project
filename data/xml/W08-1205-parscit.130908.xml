<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000266">
<title confidence="0.9989495">
Native Judgments of Non-Native Usage:
Experiments in Preposition Error Detection
</title>
<author confidence="0.939063">
Joel R. Tetreault
</author>
<affiliation confidence="0.832992">
Educational Testing Service
</affiliation>
<address confidence="0.919019">
660 Rosedale Road
Princeton, NJ, USA
</address>
<email confidence="0.997912">
JTetreault@ets.org
</email>
<author confidence="0.958563">
Martin Chodorow
</author>
<affiliation confidence="0.937421">
Hunter College of CUNY
</affiliation>
<address confidence="0.9573395">
695 Park Avenue
New York, NY, USA
</address>
<email confidence="0.999663">
martin.chodorow@hunter.cuny.edu
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747083333333">
Evaluation and annotation are two of the
greatest challenges in developing NLP in-
structional or diagnostic tools to mark
grammar and usage errors in the writing of
non-native speakers. Past approaches have
commonly used only one rater to annotate
a corpus of learner errors to compare to
system output. In this paper, we show how
using only one rater can skew system eval-
uation and then we present a sampling ap-
proach that makes it possible to evaluate a
system more efficiently.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999187">
In this paper, we present a series of experiments
that explore the reliability of human judgments
in rating preposition usage. While one tends to
think of annotator disagreements about discourse
and semantics as being quite common, our studies
show that judgments of preposition usage, which is
largely lexically driven, can be just as contentious.
As a result, this unreliability poses a serious issue
for the development and evaluation of NLP tools
in the task of automatically detecting preposition
usage errors in the writing of non-native speakers
of English.
To date, single human annotation has typically
been the gold standard for grammatical error de-
tection, such as in the work of (Izumi et al., 2004),
(Han et al., 2006), (Nagata et al., 2006), (Gamon et
al., 2008)1. Although there are several learner cor-
</bodyText>
<footnote confidence="0.880778571428571">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1(Eeg-Olofsson and Knuttson, 2003) had a small evalu-
ation of 40 prepositions and it is unclear whether they used
multiple annotators or not.
</footnote>
<bodyText confidence="0.999007">
pora annotated for preposition and determiner er-
rors (such as the Cambridge Learners Corpus2 and
the Chinese Learner English Corpus3), it is unclear
which portions of these, if any, were doubly anno-
tated. This previous work has side-stepped the is-
sue of annotator reliability, which we address here
through the following three contributions:
</bodyText>
<listItem confidence="0.993692222222222">
• Judgments of Native Usage To motivate our
work in non-native usage, we first illustrate
the difficulty of preposition selection with
two experiments: a cloze test and a choice
test, where native speakers judge native texts
(section 4).
• Judgments of Non-Native Usage As stated
earlier, most computational work in the field
of error detection tools for non-native speak-
ers has relied on a single rater to annotate
a gold standard corpus to check a system’s
output. We conduct an extensive double-
annotation evaluation to measure inter-rater
reliability and show that using one rater can
be unreliable and may produce misleading re-
sults in a system test (section 5).
• Sampling Approach Multiple annotation can
be very costly and time-consuming, which
may explain why previous work employed
only one rater. As an alternative to the
standard exhaustive annotation, we propose
a sampling approach in which estimates of
the rates of hits, false positives, and misses
are derived from random samples of the sys-
tem’s output, and then precision and recall
of the system can be calculated. We show
that estimates of system performance derived
</listItem>
<footnote confidence="0.9978435">
2http://www.cambridge.org/elt
3http://langbank.engl.polyu.edu.hk/corpus/clec.html
</footnote>
<page confidence="0.984104">
24
</page>
<note confidence="0.8494115">
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 24–32
Manchester, August 2008
</note>
<bodyText confidence="0.9999025">
from the sampling approach are comparable
to those derived from an exhaustive annota-
tion, but require only a fraction of the effort
(section 6).
In short, through a battery of experiments we
show how rating preposition usage, in either na-
tive or non-native texts, is a task that has sur-
prisingly low inter-annotator reliability and thus
greatly impacts system evaluation. We then de-
scribe a method for efficiently annotating non-
native texts to make multiple annotation more fea-
sible.
In section 2, we discuss in more depth the mo-
tivation for detecting usage errors in non-native
writing, as well as the complexities of preposition
usage. In section 3, we describe a system that au-
tomatically detects preposition errors involving in-
correct selection and extraneous usage. In sections
4 and 5 respectively, we discuss experiments on the
reliability of judging native and non-native prepo-
sition usage. In section 6, we present results of our
system and results from comparing the sampling
approach with the standard approach of exhaustive
annotation.
</bodyText>
<sectionHeader confidence="0.975993" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999959642857143">
The long-term goal of our work is to develop a
system which detects errors in grammar and us-
age so that appropriate feedback can be given
to non-native English writers, a large and grow-
ing segment of the world’s population. Estimates
are that in China alone as many as 300 million
people are currently studying English as a for-
eign language. Even in predominantly English-
speaking countries, the proportion of non-native
speakers can be very substantial. For example,
the US National Center for Educational Statistics
(2002) reported that nearly 10% of the students in
the US public school population speak a language
other than English and have limited English pro-
ficiency . At the university level in the US, there
are estimated to be more than half a million for-
eign students whose native language is not English
(Burghardt, 2002). Clearly, there is an increasing
demand for tools for instruction in English as a
Second Language (ESL).
Some of the most common types of ESL usage
errors involve prepositions, determiners and col-
locations. In the work discussed here, we target
preposition usage errors, specifically those of in-
correct selection (“we arrived to the station”) and
extraneous use (“he went to outside”)4. Preposi-
tion errors account for a substantial proportion of
all ESL usage errors. For example, (Bitchener et
al., 2005) found that preposition errors accounted
for 29% of all the errors made by intermediate to
advanced ESL students. In addition, such errors
are relatively common. In our learner corpora, we
found that 6% of all prepositions were incorrectly
used. Some other estimates are even higher: for
example, (Izumi et al., 2003) reported error rates
that were as high as 10% in a Japanese learner cor-
pus.
At least part of the difficulty in mastering prepo-
sitions seems to be due to the great variety of lin-
guistic functions that they serve. When a prepo-
sition marks the argument of a predicate, such as
a verb, an adjective, or a noun, preposition se-
lection is constrained by the argument role that it
marks, the noun which fills that role, and the par-
ticular predicate. Many English verbs also display
alternations (Levin, 1993) in which an argument
is sometimes marked by a preposition and some-
times not (e.g., “They loaded the wagon with hay”
/ “They loaded hay on the wagon”). When prepo-
sitions introduce adjuncts, such as those of time
or manner, selection is constrained by the object
of the preposition (“at length”, “in time”, “with
haste”). Finally, the selection of a preposition for
a given context also depends upon the intention of
the writer (“we sat at the beach”, “on the beach”,
“near the beach”, “by the beach”).
</bodyText>
<sectionHeader confidence="0.8377415" genericHeader="method">
3 Automatically Detecting Preposition
Usage Errors
</sectionHeader>
<bodyText confidence="0.998497">
In this section, we give a description of our sys-
tem and compare its performance to other sys-
tems. Although the focus of this paper is on hu-
man judgments in the task of error detection, we
describe our system to show that variability in hu-
man judgments can impact the evaluation of a sys-
tem in this task. A full description of our system
and its performance can be found in (Tetreault and
Chodorow, 2008).
</bodyText>
<subsectionHeader confidence="0.996807">
3.1 System
</subsectionHeader>
<bodyText confidence="0.999722">
Our approach treats preposition error detection as
a classification problem: that is, given a context of
two words before and two words after the writer’s
preposition, what is the best preposition to use?
</bodyText>
<footnote confidence="0.990225">
4There is a third error type, omission (“we are fond null
beer”), that is a topic for our future research.
</footnote>
<page confidence="0.999176">
25
</page>
<bodyText confidence="0.999978606060606">
An error is marked when the system’s sugges-
tion differs from the writer’s by a certain threshold
amount.
We have used a maximum entropy (ME) clas-
sifier (Ratnaparkhi, 1998) to select the most prob-
able preposition for a given context from a set of
34 common English prepositions. One advantage
of using ME is that there are implementations of it
which can handle very large models built from mil-
lions of training events and consisting of hundreds
of thousands of feature-value pairs. To construct
a model, we begin with a training corpus that is
POS-tagged and heuristically chunked into noun
phrases and verb phrases5. For each preposition
that occurs in the training corpus, a preprocessing
program extracts a total of 25 features. These con-
sist of words and POS tags in positions adjacent to
the preposition and in the heads of nearby phrases.
In addition, we include combination features that
merge the head features. We also include features
representing only the tags to be able to cover cases
in testing where the words in the context were not
seen in training.
In many NLP tasks (parsing, POS-tagging, pro-
noun resolution), it is easy to acquire training data
that is similar to the testing data. However, in the
case of grammatical error detection, one does not
have that luxury because reliable error-annotated
ESL corpora that are large enough for training a
statistical classifier simply do not exist. To circum-
vent this problem, we have trained our classifier on
examples of prepositions used correctly, as in news
text.
</bodyText>
<subsectionHeader confidence="0.997325">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.987085275862069">
Before evaluating our system on non-native writ-
ing, we evaluated how well it does on the task of
preposition selection in native text, an area where
there has been relatively little work to date. In this
task, the system predicts the writer’s preposition
based on its context. Its prediction is scored au-
tomatically by comparison to what the writer actu-
ally wrote. Most recently, (Gamon et al., 2008) ad-
dressed preposition selection by developing a sys-
tem that combined a decision tree and a language
model. Besides the difference in algorithms, there
is also a difference in coverage between their sys-
tem, which selects among 13 prepositions plus a
category for Other, and the system presented here,
5We have avoided parsing because our ultimate test corpus
is non-native writing, text that is difficult to parse due to the
presence of numerous errors in spelling and syntax.
Prep (Gamon et al., 2008) (Tetreault et al., 2008)
in 0.592 0.845
for 0.459 0.698
of 0.759 0.906
on 0.322 0.751
to 0.627 0.775
with 0.361 0.675
at 0.372 0.685
by 0.502 0.747
as 0.699 0.711
from 0.528 0.591
about 0.800 0.654
</bodyText>
<tableCaption confidence="0.992497">
Table 1: Comparison of F-measures on En-
carta/Reuters Corpus
</tableCaption>
<bodyText confidence="0.999851289473684">
which selects among 34 prepositions. In their sys-
tem evaluation, they split a corpus of Reuters News
text and Microsoft Encarta into two sets: 70% for
training (3.2M examples), and the remaining 30%
for testing (1.4M examples). For purposes of com-
parison, we used the same corpus and evaluation
method. While (Gamon et al., 2008) do not present
their overall accuracy figures on the Encarta eval-
uation, they do present the precision and recall
scores for each preposition. In Table 3.2, we dis-
play their results in terms of F-measures and show
the performance of our system for each preposi-
tion. Our model outperforms theirs for 9 out of the
10 prepositions that both systems handle. Over-
all accuracy for our system is 77.4% and increases
to 79.0% when 7M more training examples are
added. For comparison purposes, using a major-
ity baseline (always selecting the preposition qf) in
this domain results in an accuracy of 27.2%.
(Felice and Pullman, 2007) used perceptron
classifiers for preposition selection in BNC News
Text at 85% accuracy. For each of the five most
frequent prepositions, they used a separate binary
classifier to decide whether that preposition should
be used or not. The classifiers are not combined
into a unified model. When we reconfigured our
system and evaluation to be comparable to (Felice
and Pullman, 2007), our model achieved an accu-
racy of 90% on the same five prepositions when
tested on Wall Street Journal News, which is simi-
lar, though not identical, to BNC News.
While systems can perform at close to 80% ac-
curacy in the task of preposition selection in native
texts, this high performance does not transfer to
the end-task of detecting preposition errors in es-
says by non-native writers. For example, (Izumi et
al., 2003) reported precision and recall as low as
25% and 7% respectively when detecting different
</bodyText>
<page confidence="0.972526">
26
</page>
<bodyText confidence="0.999897526315789">
grammar errors (one of which was prepositions)
in English essays by non-native writers. (Gamon
et al., 2008) reported precision up to 80% in their
evaluation on the CLEC corpus, but no recall fig-
ure was reported. We have found that our system
(the model which performs at 77.4%), also per-
forms as high as 80% precision, but recall ranged
from 12% to 26% depending on the non-native test
corpus.
While our recall figures may seem low, espe-
cially when compared to other NLP tasks such as
parsing and anaphora resolution, this is really a re-
flection of how difficult the task is. In addition, in
error detection tasks, high precision (and thus low
recall) is favored since one wants to minimize the
number of false positives a student may see. This
is a common practice in grammatical error detec-
tion applications, such as in (Han et al., 2006) and
(Gamon et al., 2008).
</bodyText>
<sectionHeader confidence="0.964195" genericHeader="method">
4 Human Judgments of Native Usage
</sectionHeader>
<subsectionHeader confidence="0.992692">
4.1 Cloze Test
</subsectionHeader>
<bodyText confidence="0.999279448275862">
With so many sources of variation in English
preposition usage, we wondered if the task of se-
lecting a preposition for a given context might
prove challenging even for native speakers. To
investigate this possibility, we randomly selected
200 sentences from Microsoft’s Encarta Encyclo-
pedia, and, in each sentence, we replaced a ran-
domly selected preposition with a blank. We then
asked two native English speakers to perform a
cloze task by filling in the blank with the best
preposition, given the context provided by the rest
of the sentence. In addition, we had our system
predict which preposition should fill each blank as
well. Our results (Table 2) showed only about 76%
agreement between the two raters (bottom row),
and between 74% and 78% when each rater was
compared individually with the original preposi-
tion used in Encarta. Surprisingly, the system
performed just as well as the two native raters,
when compared with Encarta (third row). Al-
though these results seem very promising, it should
be noted that in many cases where the system dis-
agreed with Encarta, its prediction was not a good
fit for the context. But in the cases where the
raters disagreed with Encarta, their prepositions
were also licensed by the context, and thus were
acceptable alternatives to the preposition that was
used in the text.
Our cloze study shows that even with well-
</bodyText>
<table confidence="0.9994962">
Agreement Kappa
Encarta vs. Rater 1 0.78 0.73
Encarta vs. Rater 2 0.74 0.68
Encarta vs. System 0.75 0.68
Rater 1 vs. Rater 2 0.76 0.70
</table>
<tableCaption confidence="0.998599">
Table 2: Cloze Experiment on Encarta
</tableCaption>
<bodyText confidence="0.9990656">
formed text, native raters can disagree with each
other by 25% in the task of preposition selec-
tion. We can expect even more disagreement when
the task is preposition error detection in “noisy”
learner texts.
</bodyText>
<subsectionHeader confidence="0.993493">
4.2 Choice Test
</subsectionHeader>
<bodyText confidence="0.999989741935484">
The cloze test presented above was scored by au-
tomatically comparing the system’s choice (or the
rater’s choice) with the preposition that was actu-
ally written. But there are many contexts that li-
cense multiple prepositions, and in these cases, re-
quiring an exact match is too stringent a scoring
criterion.
To investigate how the exact match metric might
underestimate system performance, and to further
test the reliability of human judgments in native
text, we conducted a choice test in which two
native English speakers were presented with 200
sentences from Encarta and were asked to select
which of two prepositions better fit the context.
One was the originally written preposition and the
other was the system’s suggestion, displayed in
random order. The human raters were also given
the option of marking both prepositions as equally
good or equally bad. The results indicated that
both Rater 1 and Rater 2 considered the system’s
preposition equal to or better than the writer’s
preposition in 28% of the cases. This suggests
that 28% of the mismatched cases in the automatic
evaluation are not system errors but rather are in-
stances where the context licenses multiple prepo-
sitions. If these mismatches in the automatic eval-
uation are actually cases of correct system perfor-
mance, then the Encarta/Reuters test which per-
forms at 75% accuracy (third row of Table 2), is
more realistically around 82% accuracy (28% of
the 25% mismatch rate is 7%).
</bodyText>
<sectionHeader confidence="0.990418" genericHeader="method">
5 Annotator Reliability
</sectionHeader>
<bodyText confidence="0.9913845">
In this section, we address the central problem of
evaluating NLP error detection tools on learner
data. As stated earlier, most previous work has re-
lied on only one rater to either create an annotated
</bodyText>
<page confidence="0.990473">
27
</page>
<bodyText confidence="0.999980947368421">
corpus of learner errors, or to check the system’s
output. While some grammatical errors, such as
number disagreement between subject and verb,
no doubt show very high reliability, others, such as
usage errors involving prepositions or determiners
are likely to be much less reliable. In section 5.1,
we describe our efforts in annotating a large cor-
pus of student learner essays for preposition us-
age errors. Unlike previous work such as (Izumi
et al., 2004) which required the rater to check for
almost 40 different error types, we focus on anno-
tating only preposition errors in hopes that having
a single type of target will insure higher reliabil-
ity by reducing the cognitive demands on the rater.
Section 5.2 asks whether, under these conditions,
one rater is acceptable for this task. In section 6,
we describe an approach to efficiently evaluating a
system that does not require the amount of effort
needed in the standard approach to annotation.
</bodyText>
<subsectionHeader confidence="0.996872">
5.1 Annotation Scheme
</subsectionHeader>
<bodyText confidence="0.99997737037037">
To create a gold-standard corpus of error anno-
tations for system evaluation, and also to deter-
mine whether multiple raters are better than one,
we trained two native English speakers to anno-
tate preposition errors in ESL text. Both annota-
tors had prior experience in NLP annotation and
also in ESL error detection. The training was very
extensive: both raters were trained on 2000 prepo-
sition contexts and the annotation manual was it-
eratively refined as necessary. To our knowledge,
this is the first scheme that specifically targets an-
notating preposition errors6.
The two raters were shown sentences randomly
selected from student essays, with each preposi-
tion highlighted in the sentence. The raters were
also shown the sentence which preceded the one
containing the preposition that they rated. The an-
notator was first asked to indicate if there were any
spelling errors within the context of the preposi-
tion (±2-word window and the commanding verb).
Next the annotator noted determiner or plural er-
rors in the context, and then checked if there were
any other grammatical errors (for example, wrong
verb form). The reason for having the annota-
tors check spelling and grammar is that other mod-
ules in a grammatical error detection system would
be responsible for these error types. For an ex-
</bodyText>
<footnote confidence="0.782519">
6(Gamon et al., 2008) did not have a scheme for annotat-
ing preposition errors to create a gold standard corpus, but did
use a scheme for the similar problem of verifying a system’s
output in preposition error detection.
</footnote>
<bodyText confidence="0.999686033333333">
ample of a sentence with multiple spelling, gram-
matical and collocational errors, consider the fol-
lowing sentence: “In consion, for some reasons,
museums, particuraly known travel place, get on
many people.” A spelling error follows the prepo-
sition In, and a collocational error surrounds on. If
the contexts are not corrected, it is impossible to
discern if the prepositions are correct. Of course,
there is the chance that by removing these we will
screen out cases where there are multiple interact-
ing errors in the context that involve prepositions.
When comparing human judgments to the perfor-
mance of the preposition module, the latter should
not be penalized for other kinds of errors in the
context.
Finally, the annotator judged the writer’s prepo-
sition with a rating of “0-extraneous preposition”,
“1-incorrect preposition”, “2-correct preposition”,
or “e-equally good prepositions”. If the writer
used an incorrect preposition, the rater supplied the
best preposition(s) given the context. Very often,
when the writer’s preposition was correct, several
other prepositions could also have occurred in the
same context. In these cases, the annotator was in-
structed to use the “e” category and list the other
equally plausible alternatives. After judging the
use of the preposition and, if applicable, supplying
alternatives, the annotator indicated her confidence
in her judgment on a 2-point scale of “1-low” and
“2-high”.
</bodyText>
<subsectionHeader confidence="0.998024">
5.2 Two Raters vs. One?
</subsectionHeader>
<bodyText confidence="0.999940764705882">
Following training, each annotator judged approxi-
mately 18,000 occurrences of preposition use. An-
notation of 500 occurrences took an average of 3 to
4 hours. In order to calculate agreement and kappa
values, we periodically provided identical sets of
100 preposition occurrences for both annotators to
judge (totaling 1800 in all). After removing in-
stances where there were spelling or grammar er-
rors, and after combining categories “2” and “e”,
both of which were judgments of correct usage,
we computed the kappa values for the remaining
doubly judged sets. These ranged from 0.411 to
0.786, with an overall combined value of 0.6307.
The confusion matrix for the combined set (to-
taling 1336 contexts) is shown in Table 3. The
rows represent Rater 1’s (R1) judgments while the
columns represent Rater 2’s judgments. As one
</bodyText>
<footnote confidence="0.8637045">
7When including spelling and grammar annotations,
kappa ranged from 0.474 to 0.773.
</footnote>
<page confidence="0.998779">
28
</page>
<bodyText confidence="0.999842428571428">
would expect given the prior reports of preposition
error rates in non-native writing, the raters’ agree-
ment for this task was quite high overall (0.952)
due primarily to the large agreement count where
both annotators rated the usage “OK” (1213 total
contexts). However there were 42 prepositions that
both raters marked as a “Wrong Choice” and 17 as
“Extraneous.” It is important to note the disagree-
ments in judging these errors: for example, Rater
1 judged 26 prepositions to be errors that Rater 2
judged to be OK, for a disagreement rate of .302
(26/86). Similarly, Rater 2 judged 37 prepositions
to be errors that Rater 1 judged to be OK, for a
disagreement rate of .381 (37/97).
</bodyText>
<table confidence="0.997929">
R1↓; R2→ Extraneous Wrong-Choice OK
Extraneous 17 0 6
Wrong-Choice 1 42 20
OK 4 33 1213
</table>
<tableCaption confidence="0.999787">
Table 3: Confusion Matrix
</tableCaption>
<bodyText confidence="0.991535476190476">
The kappa of 0.630 and the off-diagonal cells
in the confusion matrix both show the difficulty
of this task and also show how two highly trained
raters can produce very different judgments. This
suggests that for certain error annotation tasks,
such as preposition usage, it may not be appropri-
ate to use only one rater and that using two or more
raters to produce an adjudicated gold-standard set
is the more acceptable path.
As a second test, we used a set of 2,000 prepo-
sition contexts from ESL essays (Chodorow et al.,
2007) that were doubly annotated by native speak-
ers with a scheme similar to that described above.
We then compared an earlier version of our sys-
tem to both raters’ judgments, and found that there
was a 10% difference in precision and a 5% differ-
ence in recall between the two system/rater com-
parisons. That means that if one is using only a
single rater as a gold standard, there is the potential
to over- or under-estimate precision by as much as
10%. Clearly this is problematic when evaluating
</bodyText>
<tableCaption confidence="0.750027">
a system’s performance. The results are shown in
Table 4.
</tableCaption>
<table confidence="0.999588333333333">
Precision Recall
System vs. Rater 1 0.78 0.26
System vs. Rater 2 0.68 0.21
</table>
<tableCaption confidence="0.994697">
Table 4: Rater/System Comparison
</tableCaption>
<sectionHeader confidence="0.70272" genericHeader="method">
6 Sampling Approach
</sectionHeader>
<bodyText confidence="0.999966133333333">
If one uses multiple raters for error annotation,
there is the possibility of creating an adjudicated
set, or at least calculating the variability of sys-
tem evaluation. However, annotation with multiple
raters has its own disadvantages in that it is much
more expensive and time-consuming. Even using
one rater to produce a sizeable evaluation corpus
of preposition errors is extremely costly. For ex-
ample, if we assume that 500 prepositions can be
annotated in 4 hours using our annotation scheme,
and that the error rate for prepositions is 10%, then
it would take at least 80 hours for a rater to find
and mark 1000 errors. In this section, we propose
a more efficient annotation approach to circumvent
this problem.
</bodyText>
<subsectionHeader confidence="0.987714">
6.1 Methodology
</subsectionHeader>
<bodyText confidence="0.9999213">
The sampling procedure outlined here is inspired
by the one described in (Chodorow and Leacock,
2000). The central idea is to skew the annotation
corpus so that it contains a greater proportion of
errors. The result is that an annotator checks more
potential errors since he or she is spending less
time checking prepositions used correctly.
Here are the steps in the procedure. Figure 1 il-
lustrates this procedure with a hypothetical corpus
of 10,000 preposition examples.
</bodyText>
<listItem confidence="0.68792475">
1. Process a test corpus of sentences so that each
preposition in the corpus is labeled “OK” or
“Error” by the system.
2. Divide the processed corpus into two sub-
corpora, one consisting of the system’s “OK”
prepositions and the other of the system’s
“Error” prepositions. For the hypothetical
data in Figure 1, the “OK” sub-corpus con-
tains 90% of the prepositions, and the “Error”
sub-corpus contains the remaining 10%.
3. Randomly sample cases from each sub-
corpus and combine the samples into an an-
notation set that is given to a “blind” human
rater. We generally use a higher sampling
rate for the “Error” sub-corpus because we
want to “enrich” the annotation set with a
larger proportion of errors than is found in the
test corpus as a whole. In Figure 1, 75% of
the “Error” sub-corpus is sampled while only
16% of the “OK” sub-corpus is sampled.
</listItem>
<page confidence="0.998827">
29
</page>
<figureCaption confidence="0.99963">
Figure 1: Sampling Approach (with hypothetical sample calculations)
</figureCaption>
<bodyText confidence="0.892722444444445">
4. For each case that the human rater judges to
be an error, check to see which sub-corpus it
came from. If it came from the “OK” sub-
corpus, then the case is a Miss (an error that
the system failed to detect). If it came from
the “Error” sub-corpus, then the case is a Hit
(an error that the system detected). If the rater
judges a case to be a correct usage and it came
from the “Error” sub-corpus, then it is a False
Positive (FP).
5. Calculate the proportions of Hits and FPs in
the sample from the “Error” sub-corpus. For
the hypothetical data in Figure 1, these val-
ues are 600/750 = 0.80 for Hits, and 150/750
= 0.20 for FPs. Calculate the proportion of
Misses in the sample from the “OK” sub-
corpus. For the hypothetical data, this is
450/1500 = 0.30 for Misses.
</bodyText>
<listItem confidence="0.761640533333333">
6. The values computed in step 5 are conditional
proportions based on the sub-corpora. To cal-
culate the overall proportions in the test cor-
pus, it is necessary to multiply each value
by the relative size of its sub-corpus. This
is shown in Table 5, where the proportion of
Hits in the “Error” sub-corpus (0.80) is mul-
tiplied by the relative size of the “Error” sub-
corpus (0.10) to produce an overall Hit rate
(0.08). Overall rates for FPs and Misses are
calculated in a similar manner.
7. Using the values from step 6, calculate Preci-
sion (Hits/(Hits + FP)) and Recall (Hits/(Hits
+ Misses)). These are shown in the last two
rows of Table 5.
</listItem>
<table confidence="0.999372">
Estimated Overall Rates
Sample Proportion * Sub-Corpus Proportion
Hits 0.80 * 0.10 = 0.08
FP 0.20 * 0.10 = 0.02
Misses 0.30 * 0.90 = 0.27
Precision 0.08/(0.08 + 0.02) = 0.80
Recall 0.08/(0.08 + 0.27) = 0.23
</table>
<tableCaption confidence="0.99947">
Table 5: Sampling Calculations (Hypothetical)
</tableCaption>
<bodyText confidence="0.999456888888889">
This method is similar in spirit to active learning
((Dagan and Engelson, 1995) and (Engelson and
Dagan, 1996)), which has been used to iteratively
build up an annotated corpus, but it differs from
active learning applications in that there are no it-
erative loops between the system and the human
annotator(s). In addition, while our methodology
is used for evaluating a system, active learning is
commonly used for training a system.
</bodyText>
<subsectionHeader confidence="0.995141">
6.2 Application
</subsectionHeader>
<bodyText confidence="0.999971333333333">
Next, we tested whether our proposed sampling
approach provides good estimates of a system’s
performance. For this task, we split a large corpus
of ESL essays into two sets: first, a set of 8,269
preposition contexts (standard approach corpus) to
be annotated using the scheme in section 5.1, and
</bodyText>
<page confidence="0.993768">
30
</page>
<bodyText confidence="0.999912681818182">
second, a set of 22,000 preposition contexts to be
rated using the sampling approach (sampling cor-
pus). We used two non-overlapping sets because
the raters were the same for this test of the two ap-
proaches.
Using the standard approach, the sampling cor-
pus of 22,000 prepositions would normally take
several weeks for two raters to double annotate
and then adjudicate. After this corpus was di-
vided into “OK” and “Error” sub-corpora, the two
sub-corpora were proportionally sampled, result-
ing in an annotation set of 750 preposition con-
texts (500 contexts from the “OK” sub-corpus and
250 contexts from the “Error” sub-corpus). This
required roughly 6 hours for annotation, which is
substantially more manageable than the standard
approach. We had both raters work together to
make judgments for each preposition context.
The precision and recall scores for both ap-
proaches are shown in Table 6 and are quite simi-
lar, thus suggesting that the sampling approach can
be used as an alternative to exhaustive annotation.
</bodyText>
<table confidence="0.999594666666667">
Precision Recall
Standard Approach 0.80 0.12
Sampling Approach 0.79 0.14
</table>
<tableCaption confidence="0.995585">
Table 6: Sampling Results
</tableCaption>
<subsectionHeader confidence="0.989647">
6.3 Confidence Intervals
</subsectionHeader>
<bodyText confidence="0.937967625">
It is important with the sampling approach to use
appropriate sample sizes when drawing from the
sub-corpora, because the accuracy of the estimates
of hits and misses will depend upon the propor-
tion of errors in each sub-corpus as well as on the
sample sizes. The “OK” sub-corpus is expected
to have even fewer errors than the overall base
rate, so it is especially important to have a rela-
tively large sample from this sub-corpus. The com-
parison study described above used an “OK” sub-
corpus sample that was twice as large as the Error
sub-corpus sample.
One can compute the 95% confidence interval
(CI) for the estimated rates of hits, misses and false
positives by using the formula:
CI=pf1.96xup
where p is the proportion and up is the standard
error of the proportion given by:
p(1 − p)
v N
where N is the sample size.
For the example in Figure 1, the confidence in-
terval for the proportion of Hits from the sample of
the “Error” sub-corpus is:
</bodyText>
<equation confidence="0.728731">
�0.8 x (1 − 0.80)
</equation>
<bodyText confidence="0.988054">
ple size will
yield narrower confidence intervals.
system output with respect to the
preposi-
tion. Like the sampling method, it has the advan-
tages of efficiency and use of multiple raters (when
compared to the standard method). But the dis-
advantage of verification is that it does not permit
estimation of recall. Both verification and vam-
pling methods require re-annotation for system re-
testing and comparison. In terms of system devel-
opment, sampling (and to a lesser extent, verifica-
tion) allows one to quickly assess system perfor-
mance on a new corpus.
In short, the sampling approach is intended to
alleviate the burden on annotators when faced with
the task of having to rate several thousand errors of
writer’s
cular type to produce a sizeable error corpus.
</bodyText>
<sectionHeader confidence="0.999528" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99977725">
which yields an interval of 0.077 and 0.083. Using
these values, the confidence interval for precision
is 0.77 to 0.83. The interval for recall can be com-
puted in a similar manner. Of course, a larger sam-
</bodyText>
<subsectionHeader confidence="0.896179">
6.4 Summary
</subsectionHeader>
<bodyText confidence="0.9960725">
Table 7 summarizes the advantages and disadvan-
tages of three methods for evaluating error detec-
tion systems. The standard (or exhaustive) ap-
proach refers to the method of annotating the er-
rors in a large corpus. Its advantage is that the an-
notated corpus can be reused to evaluate the same
system or compare multiple systems. However,
it is costly and time-consuming which often pre-
cludes the use of multiple raters. The verification
method (as used in (Gamon et al., 2008)), refers to
the method of simply checking the acceptability of
a parti
In this paper, we showed that the standard ap-
proach to evaluating NLP error detection sys-
tems (comparing the
output with a gold-
standard annotation) can greatly skew system re-
sults when the annotation is done by only one rater.
However, one reason why a single rater is com-
monly used is that building a corpus of learner er-
rors can be extremely costly an
system’s
d time-consuming.
To address this efficiency issue, we presented a
</bodyText>
<equation confidence="0.7881775">
up =
CIhits = 0.80 f 1.96 x
</equation>
<page confidence="0.7513215">
750
31
</page>
<table confidence="0.9991994">
Approach Advantages Disadvantages
Standard Easy to retest system (no re-annotation required) Costly
Easy to compare systems Time-Consuming
Most reliably estimates precision and recall Difficult to use multiple raters
Sampling Efficient, especially for low-frequency errors Less reliable estimate of recall
Permits estimation of precision and recall Hard to re-test system (re-annotation required)
More easily allows use of multiple raters Hard to compare systems
Verification Efficient, especially for low-frequency errors Does not permit estimation of recall
More easily allows use of multiple raters Hard to re-test system (re-annotation required)
Hard to compare systems
</table>
<tableCaption confidence="0.999843">
Table 7: Comparison of Evaluation Methods
</tableCaption>
<bodyText confidence="0.99994528">
sampling approach that produces results compa-
rable to exhaustive annotation. This makes using
multiple raters possible since less time is required
to assess the system’s performance. While the
work presented here has focused on prepositions,
the reasons for using multiple raters and a sam-
pling approach apply equally to other error types,
such as determiners and collocations.
It should be noted that the work here uses two
raters. For future work, we plan on annotating
preposition errors with more than two raters to de-
rive a range of judgments. We also plan to look at
the effects of feedback for errors involving prepo-
sitions and determiners, on the quality of ESL writ-
ing.
The preposition error detection system de-
scribed here was recently integrated into Cri-
terionSM Online Writing Evaluation Service
developed by Educational Testing Service.
Acknowledgements We would first like to
thank our two annotators Sarah Ohls and Waverly
VanWinkle for their hours of hard work. We would
also like to acknowledge the three anonymous
reviewers and Derrick Higgins for their helpful
comments and feedback.
</bodyText>
<sectionHeader confidence="0.999181" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999529055555555">
Bitchener, J., S. Young, and D. Cameron. 2005. The ef-
fect of different types of corrective feedback on ESL
student writing. Journal of Second Language Writ-
ing.
Burghardt, L. 2002. Foreign applications soar at uni-
versities. New York Times, April.
Chodorow, M. and C. Leacock. 2000. An unsupervised
method for detecting grammatical errors. In NAACL.
Chodorow, M., J. Tetreault, and N-R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the Fourth ACL-SIGSEM Work-
shop on Prepositions.
Dagan, I. and S. Engelson. 1995. Committee-based
sampling for training probabilistic classifiers. In
Proceedings of ICML, pages 150–157.
Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic
grammar checking for second language learners - the
use of prepositions. In Nodalida.
Engelson, S. and I. Dagan. 1996. Minimizing manual
annotation cost in supervised training from corpora.
In Proceedings of ACL, pages 319–326.
Felice, R. De and S. Pullman. 2007. Automatically ac-
quiring models of preposition use. In Proceedings of
the Fourth ACL-SIGSEM Workshop on Prepositions.
Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language mod-
eling for esl error correction. In IJCNLP.
Han, N-R., M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12:115–
129.
Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and
H. Isahara. 2003. Automatic error detection in the
Japanese leaners’ English spoken data. In ACL.
Izumi, E., K. Uchimoto, and H. Isahara. 2004. The
overview of the sst speech corpus of Japanese learner
English and evaluation through the experiment on
automatic detection of learners’ errors. In LREC.
Levin, B. 1993. English verb classes and alternations:
a preliminary investigation. Univ. of Chicago Press.
Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors
in the writing of learners of English. In Proceedings
of the ACL/COLING.
NCES. 2002. National center for educational statis-
tics: Public school student counts, staff, and graduate
counts by state: School year 2000-2001.
Ratnaparkhi, A. 1998. Maximum Entropy Models for
natural language ambiguity resolution. Ph.D. thesis,
University of Pennsylvania.
Tetreault, J. and M. Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In COLING.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.290814">
<title confidence="0.994235">Native Judgments of Non-Native Experiments in Preposition Error Detection</title>
<author confidence="0.999993">Joel R Tetreault</author>
<affiliation confidence="0.968187">Educational Testing Service</affiliation>
<address confidence="0.9996915">660 Rosedale Road Princeton, NJ, USA</address>
<email confidence="0.988687">JTetreault@ets.org</email>
<author confidence="0.433084">Martin</author>
<affiliation confidence="0.636504">Hunter College of</affiliation>
<address confidence="0.969236">695 Park</address>
<author confidence="0.794707">New York</author>
<author confidence="0.794707">NY</author>
<email confidence="0.99915">martin.chodorow@hunter.cuny.edu</email>
<abstract confidence="0.999842153846154">Evaluation and annotation are two of the greatest challenges in developing NLP instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers. Past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output. In this paper, we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bitchener</author>
<author>S Young</author>
<author>D Cameron</author>
</authors>
<title>The effect of different types of corrective feedback on ESL student writing.</title>
<date>2005</date>
<journal>Journal of Second Language Writing.</journal>
<contexts>
<context position="6017" citStr="Bitchener et al., 2005" startWordPosition="937" endWordPosition="940"> are estimated to be more than half a million foreign students whose native language is not English (Burghardt, 2002). Clearly, there is an increasing demand for tools for instruction in English as a Second Language (ESL). Some of the most common types of ESL usage errors involve prepositions, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection (“we arrived to the station”) and extraneous use (“he went to outside”)4. Preposition errors account for a substantial proportion of all ESL usage errors. For example, (Bitchener et al., 2005) found that preposition errors accounted for 29% of all the errors made by intermediate to advanced ESL students. In addition, such errors are relatively common. In our learner corpora, we found that 6% of all prepositions were incorrectly used. Some other estimates are even higher: for example, (Izumi et al., 2003) reported error rates that were as high as 10% in a Japanese learner corpus. At least part of the difficulty in mastering prepositions seems to be due to the great variety of linguistic functions that they serve. When a preposition marks the argument of a predicate, such as a verb, </context>
</contexts>
<marker>Bitchener, Young, Cameron, 2005</marker>
<rawString>Bitchener, J., S. Young, and D. Cameron. 2005. The effect of different types of corrective feedback on ESL student writing. Journal of Second Language Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burghardt</author>
</authors>
<title>Foreign applications soar at universities.</title>
<date>2002</date>
<location>New York Times,</location>
<contexts>
<context position="5511" citStr="Burghardt, 2002" startWordPosition="859" endWordPosition="860">lation. Estimates are that in China alone as many as 300 million people are currently studying English as a foreign language. Even in predominantly Englishspeaking countries, the proportion of non-native speakers can be very substantial. For example, the US National Center for Educational Statistics (2002) reported that nearly 10% of the students in the US public school population speak a language other than English and have limited English proficiency . At the university level in the US, there are estimated to be more than half a million foreign students whose native language is not English (Burghardt, 2002). Clearly, there is an increasing demand for tools for instruction in English as a Second Language (ESL). Some of the most common types of ESL usage errors involve prepositions, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection (“we arrived to the station”) and extraneous use (“he went to outside”)4. Preposition errors account for a substantial proportion of all ESL usage errors. For example, (Bitchener et al., 2005) found that preposition errors accounted for 29% of all the errors made by intermediate to adv</context>
</contexts>
<marker>Burghardt, 2002</marker>
<rawString>Burghardt, L. 2002. Foreign applications soar at universities. New York Times, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>An unsupervised method for detecting grammatical errors.</title>
<date>2000</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="24749" citStr="Chodorow and Leacock, 2000" startWordPosition="4083" endWordPosition="4086">has its own disadvantages in that it is much more expensive and time-consuming. Even using one rater to produce a sizeable evaluation corpus of preposition errors is extremely costly. For example, if we assume that 500 prepositions can be annotated in 4 hours using our annotation scheme, and that the error rate for prepositions is 10%, then it would take at least 80 hours for a rater to find and mark 1000 errors. In this section, we propose a more efficient annotation approach to circumvent this problem. 6.1 Methodology The sampling procedure outlined here is inspired by the one described in (Chodorow and Leacock, 2000). The central idea is to skew the annotation corpus so that it contains a greater proportion of errors. The result is that an annotator checks more potential errors since he or she is spending less time checking prepositions used correctly. Here are the steps in the procedure. Figure 1 illustrates this procedure with a hypothetical corpus of 10,000 preposition examples. 1. Process a test corpus of sentences so that each preposition in the corpus is labeled “OK” or “Error” by the system. 2. Divide the processed corpus into two subcorpora, one consisting of the system’s “OK” prepositions and the</context>
</contexts>
<marker>Chodorow, Leacock, 2000</marker>
<rawString>Chodorow, M. and C. Leacock. 2000. An unsupervised method for detecting grammatical errors. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>N-R Han</author>
</authors>
<title>Detection of grammatical errors involving prepositions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</booktitle>
<contexts>
<context position="23233" citStr="Chodorow et al., 2007" startWordPosition="3826" endWordPosition="3829">Wrong-Choice OK Extraneous 17 0 6 Wrong-Choice 1 42 20 OK 4 33 1213 Table 3: Confusion Matrix The kappa of 0.630 and the off-diagonal cells in the confusion matrix both show the difficulty of this task and also show how two highly trained raters can produce very different judgments. This suggests that for certain error annotation tasks, such as preposition usage, it may not be appropriate to use only one rater and that using two or more raters to produce an adjudicated gold-standard set is the more acceptable path. As a second test, we used a set of 2,000 preposition contexts from ESL essays (Chodorow et al., 2007) that were doubly annotated by native speakers with a scheme similar to that described above. We then compared an earlier version of our system to both raters’ judgments, and found that there was a 10% difference in precision and a 5% difference in recall between the two system/rater comparisons. That means that if one is using only a single rater as a gold standard, there is the potential to over- or under-estimate precision by as much as 10%. Clearly this is problematic when evaluating a system’s performance. The results are shown in Table 4. Precision Recall System vs. Rater 1 0.78 0.26 Sys</context>
</contexts>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>Chodorow, M., J. Tetreault, and N-R. Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>S Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>150--157</pages>
<contexts>
<context position="27785" citStr="Dagan and Engelson, 1995" startWordPosition="4622" endWordPosition="4625">the “Error” subcorpus (0.10) to produce an overall Hit rate (0.08). Overall rates for FPs and Misses are calculated in a similar manner. 7. Using the values from step 6, calculate Precision (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)). These are shown in the last two rows of Table 5. Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion Hits 0.80 * 0.10 = 0.08 FP 0.20 * 0.10 = 0.02 Misses 0.30 * 0.90 = 0.27 Precision 0.08/(0.08 + 0.02) = 0.80 Recall 0.08/(0.08 + 0.27) = 0.23 Table 5: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 6.2 Application Next, we tested whether our proposed sampling approach provides good estimates of a system’s performance. For this task, we split a large corpus of ESL essays into two sets: first, a set of 8,269 preposition contexts (standard a</context>
</contexts>
<marker>Dagan, Engelson, 1995</marker>
<rawString>Dagan, I. and S. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In Proceedings of ICML, pages 150–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eeg-Olofsson</author>
<author>O Knuttson</author>
</authors>
<title>Automatic grammar checking for second language learners - the use of prepositions.</title>
<date>2003</date>
<booktitle>In Nodalida.</booktitle>
<contexts>
<context position="1821" citStr="Eeg-Olofsson and Knuttson, 2003" startWordPosition="271" endWordPosition="274">issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1. Although there are several learner cor© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1(Eeg-Olofsson and Knuttson, 2003) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not. pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus2 and the Chinese Learner English Corpus3), it is unclear which portions of these, if any, were doubly annotated. This previous work has side-stepped the issue of annotator reliability, which we address here through the following three contributions: • Judgments of Native Usage To motivate our work in non-native usage, we first illustrate the difficulty of preposition selection with two experiments:</context>
</contexts>
<marker>Eeg-Olofsson, Knuttson, 2003</marker>
<rawString>Eeg-Olofsson, J. and O. Knuttson. 2003. Automatic grammar checking for second language learners - the use of prepositions. In Nodalida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>319--326</pages>
<contexts>
<context position="27816" citStr="Engelson and Dagan, 1996" startWordPosition="4627" endWordPosition="4630"> produce an overall Hit rate (0.08). Overall rates for FPs and Misses are calculated in a similar manner. 7. Using the values from step 6, calculate Precision (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)). These are shown in the last two rows of Table 5. Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion Hits 0.80 * 0.10 = 0.08 FP 0.20 * 0.10 = 0.02 Misses 0.30 * 0.90 = 0.27 Precision 0.08/(0.08 + 0.02) = 0.80 Recall 0.08/(0.08 + 0.27) = 0.23 Table 5: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 6.2 Application Next, we tested whether our proposed sampling approach provides good estimates of a system’s performance. For this task, we split a large corpus of ESL essays into two sets: first, a set of 8,269 preposition contexts (standard approach corpus) to be annotated</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Engelson, S. and I. Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In Proceedings of ACL, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De and S Pullman Felice</author>
</authors>
<title>Automatically acquiring models of preposition use.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</booktitle>
<marker>Felice, 2007</marker>
<rawString>Felice, R. De and S. Pullman. 2007. Automatically acquiring models of preposition use. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W B Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using contextual speller techniques and language modeling for esl error correction.</title>
<date>2008</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="1568" citStr="Gamon et al., 2008" startWordPosition="244" endWordPosition="247">notator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1. Although there are several learner cor© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1(Eeg-Olofsson and Knuttson, 2003) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not. pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus2 and the Chinese Learner English Corpus3), it is unclear which portions of these, if any, were doubly annotated. This previous work has side-stepped</context>
<context position="10066" citStr="Gamon et al., 2008" startWordPosition="1629" endWordPosition="1632">ra that are large enough for training a statistical classifier simply do not exist. To circumvent this problem, we have trained our classifier on examples of prepositions used correctly, as in news text. 3.2 Evaluation Before evaluating our system on non-native writing, we evaluated how well it does on the task of preposition selection in native text, an area where there has been relatively little work to date. In this task, the system predicts the writer’s preposition based on its context. Its prediction is scored automatically by comparison to what the writer actually wrote. Most recently, (Gamon et al., 2008) addressed preposition selection by developing a system that combined a decision tree and a language model. Besides the difference in algorithms, there is also a difference in coverage between their system, which selects among 13 prepositions plus a category for Other, and the system presented here, 5We have avoided parsing because our ultimate test corpus is non-native writing, text that is difficult to parse due to the presence of numerous errors in spelling and syntax. Prep (Gamon et al., 2008) (Tetreault et al., 2008) in 0.592 0.845 for 0.459 0.698 of 0.759 0.906 on 0.322 0.751 to 0.627 0.</context>
<context position="12787" citStr="Gamon et al., 2008" startWordPosition="2086" endWordPosition="2089"> model achieved an accuracy of 90% on the same five prepositions when tested on Wall Street Journal News, which is similar, though not identical, to BNC News. While systems can perform at close to 80% accuracy in the task of preposition selection in native texts, this high performance does not transfer to the end-task of detecting preposition errors in essays by non-native writers. For example, (Izumi et al., 2003) reported precision and recall as low as 25% and 7% respectively when detecting different 26 grammar errors (one of which was prepositions) in English essays by non-native writers. (Gamon et al., 2008) reported precision up to 80% in their evaluation on the CLEC corpus, but no recall figure was reported. We have found that our system (the model which performs at 77.4%), also performs as high as 80% precision, but recall ranged from 12% to 26% depending on the non-native test corpus. While our recall figures may seem low, especially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. In addition, in error detection tasks, high precision (and thus low recall) is favored since one wants to minimize the number of fa</context>
<context position="19337" citStr="Gamon et al., 2008" startWordPosition="3187" endWordPosition="3191">aters were also shown the sentence which preceded the one containing the preposition that they rated. The annotator was first asked to indicate if there were any spelling errors within the context of the preposition (±2-word window and the commanding verb). Next the annotator noted determiner or plural errors in the context, and then checked if there were any other grammatical errors (for example, wrong verb form). The reason for having the annotators check spelling and grammar is that other modules in a grammatical error detection system would be responsible for these error types. For an ex6(Gamon et al., 2008) did not have a scheme for annotating preposition errors to create a gold standard corpus, but did use a scheme for the similar problem of verifying a system’s output in preposition error detection. ample of a sentence with multiple spelling, grammatical and collocational errors, consider the following sentence: “In consion, for some reasons, museums, particuraly known travel place, get on many people.” A spelling error follows the preposition In, and a collocational error surrounds on. If the contexts are not corrected, it is impossible to discern if the prepositions are correct. Of course, t</context>
<context position="32031" citStr="Gamon et al., 2008" startWordPosition="5338" endWordPosition="5341">onfidence interval for precision is 0.77 to 0.83. The interval for recall can be computed in a similar manner. Of course, a larger sam6.4 Summary Table 7 summarizes the advantages and disadvantages of three methods for evaluating error detection systems. The standard (or exhaustive) approach refers to the method of annotating the errors in a large corpus. Its advantage is that the annotated corpus can be reused to evaluate the same system or compare multiple systems. However, it is costly and time-consuming which often precludes the use of multiple raters. The verification method (as used in (Gamon et al., 2008)), refers to the method of simply checking the acceptability of a parti In this paper, we showed that the standard approach to evaluating NLP error detection systems (comparing the output with a goldstandard annotation) can greatly skew system results when the annotation is done by only one rater. However, one reason why a single rater is commonly used is that building a corpus of learner errors can be extremely costly an system’s d time-consuming. To address this efficiency issue, we presented a up = CIhits = 0.80 f 1.96 x 750 31 Approach Advantages Disadvantages Standard Easy to retest syste</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>Gamon, M., J. Gao, C. Brockett, A. Klementiev, W. B. Dolan, D. Belenko, and L. Vanderwende. 2008. Using contextual speller techniques and language modeling for esl error correction. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N-R Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<pages>129</pages>
<contexts>
<context position="1523" citStr="Han et al., 2006" startWordPosition="236" endWordPosition="239">ition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1. Although there are several learner cor© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1(Eeg-Olofsson and Knuttson, 2003) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not. pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus2 and the Chinese Learner English Corpus3), it is unclear which portions of these, if any, were doubly a</context>
<context position="13520" citStr="Han et al., 2006" startWordPosition="2217" endWordPosition="2220">nd that our system (the model which performs at 77.4%), also performs as high as 80% precision, but recall ranged from 12% to 26% depending on the non-native test corpus. While our recall figures may seem low, especially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. In addition, in error detection tasks, high precision (and thus low recall) is favored since one wants to minimize the number of false positives a student may see. This is a common practice in grammatical error detection applications, such as in (Han et al., 2006) and (Gamon et al., 2008). 4 Human Judgments of Native Usage 4.1 Cloze Test With so many sources of variation in English preposition usage, we wondered if the task of selecting a preposition for a given context might prove challenging even for native speakers. To investigate this possibility, we randomly selected 200 sentences from Microsoft’s Encarta Encyclopedia, and, in each sentence, we replaced a randomly selected preposition with a blank. We then asked two native English speakers to perform a cloze task by filling in the blank with the best preposition, given the context provided by the </context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Han, N-R., M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12:115– 129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>T Supnithi</author>
<author>H Isahara</author>
</authors>
<title>Automatic error detection in the Japanese leaners’ English spoken data.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6334" citStr="Izumi et al., 2003" startWordPosition="988" endWordPosition="991">ns. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection (“we arrived to the station”) and extraneous use (“he went to outside”)4. Preposition errors account for a substantial proportion of all ESL usage errors. For example, (Bitchener et al., 2005) found that preposition errors accounted for 29% of all the errors made by intermediate to advanced ESL students. In addition, such errors are relatively common. In our learner corpora, we found that 6% of all prepositions were incorrectly used. Some other estimates are even higher: for example, (Izumi et al., 2003) reported error rates that were as high as 10% in a Japanese learner corpus. At least part of the difficulty in mastering prepositions seems to be due to the great variety of linguistic functions that they serve. When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is constrained by the argument role that it marks, the noun which fills that role, and the particular predicate. Many English verbs also display alternations (Levin, 1993) in which an argument is sometimes marked by a preposition and sometimes not (e.g., “They loaded th</context>
<context position="12586" citStr="Izumi et al., 2003" startWordPosition="2054" endWordPosition="2057">whether that preposition should be used or not. The classifiers are not combined into a unified model. When we reconfigured our system and evaluation to be comparable to (Felice and Pullman, 2007), our model achieved an accuracy of 90% on the same five prepositions when tested on Wall Street Journal News, which is similar, though not identical, to BNC News. While systems can perform at close to 80% accuracy in the task of preposition selection in native texts, this high performance does not transfer to the end-task of detecting preposition errors in essays by non-native writers. For example, (Izumi et al., 2003) reported precision and recall as low as 25% and 7% respectively when detecting different 26 grammar errors (one of which was prepositions) in English essays by non-native writers. (Gamon et al., 2008) reported precision up to 80% in their evaluation on the CLEC corpus, but no recall figure was reported. We have found that our system (the model which performs at 77.4%), also performs as high as 80% precision, but recall ranged from 12% to 26% depending on the non-native test corpus. While our recall figures may seem low, especially when compared to other NLP tasks such as parsing and anaphora </context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Supnithi, Isahara, 2003</marker>
<rawString>Izumi, E., K. Uchimoto, T. Saiga, T. Supnithi, and H. Isahara. 2003. Automatic error detection in the Japanese leaners’ English spoken data. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>The overview of the sst speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners’ errors.</title>
<date>2004</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1503" citStr="Izumi et al., 2004" startWordPosition="232" endWordPosition="235">ments in rating preposition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1. Although there are several learner cor© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1(Eeg-Olofsson and Knuttson, 2003) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not. pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus2 and the Chinese Learner English Corpus3), it is unclear which portions of these, i</context>
<context position="17503" citStr="Izumi et al., 2004" startWordPosition="2884" endWordPosition="2887">em of evaluating NLP error detection tools on learner data. As stated earlier, most previous work has relied on only one rater to either create an annotated 27 corpus of learner errors, or to check the system’s output. While some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, others, such as usage errors involving prepositions or determiners are likely to be much less reliable. In section 5.1, we describe our efforts in annotating a large corpus of student learner essays for preposition usage errors. Unlike previous work such as (Izumi et al., 2004) which required the rater to check for almost 40 different error types, we focus on annotating only preposition errors in hopes that having a single type of target will insure higher reliability by reducing the cognitive demands on the rater. Section 5.2 asks whether, under these conditions, one rater is acceptable for this task. In section 6, we describe an approach to efficiently evaluating a system that does not require the amount of effort needed in the standard approach to annotation. 5.1 Annotation Scheme To create a gold-standard corpus of error annotations for system evaluation, and al</context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>Izumi, E., K. Uchimoto, and H. Isahara. 2004. The overview of the sst speech corpus of Japanese learner English and evaluation through the experiment on automatic detection of learners’ errors. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English verb classes and alternations: a preliminary investigation.</title>
<date>1993</date>
<publisher>Univ. of Chicago Press.</publisher>
<contexts>
<context position="6835" citStr="Levin, 1993" startWordPosition="1080" endWordPosition="1081">all prepositions were incorrectly used. Some other estimates are even higher: for example, (Izumi et al., 2003) reported error rates that were as high as 10% in a Japanese learner corpus. At least part of the difficulty in mastering prepositions seems to be due to the great variety of linguistic functions that they serve. When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is constrained by the argument role that it marks, the noun which fills that role, and the particular predicate. Many English verbs also display alternations (Levin, 1993) in which an argument is sometimes marked by a preposition and sometimes not (e.g., “They loaded the wagon with hay” / “They loaded hay on the wagon”). When prepositions introduce adjuncts, such as those of time or manner, selection is constrained by the object of the preposition (“at length”, “in time”, “with haste”). Finally, the selection of a preposition for a given context also depends upon the intention of the writer (“we sat at the beach”, “on the beach”, “near the beach”, “by the beach”). 3 Automatically Detecting Preposition Usage Errors In this section, we give a description of our s</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English verb classes and alternations: a preliminary investigation. Univ. of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>A Kawai</author>
<author>K Morihiro</author>
<author>N Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of English.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL/COLING.</booktitle>
<contexts>
<context position="1546" citStr="Nagata et al., 2006" startWordPosition="240" endWordPosition="243">ne tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1. Although there are several learner cor© 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1(Eeg-Olofsson and Knuttson, 2003) had a small evaluation of 40 prepositions and it is unclear whether they used multiple annotators or not. pora annotated for preposition and determiner errors (such as the Cambridge Learners Corpus2 and the Chinese Learner English Corpus3), it is unclear which portions of these, if any, were doubly annotated. This previous</context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>Nagata, R., A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in the writing of learners of English. In Proceedings of the ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NCES</author>
</authors>
<title>National center for educational statistics: Public school student counts, staff, and graduate counts by state: School year</title>
<date>2002</date>
<pages>2000--2001</pages>
<marker>NCES, 2002</marker>
<rawString>NCES. 2002. National center for educational statistics: Public school student counts, staff, and graduate counts by state: School year 2000-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8291" citStr="Ratnaparkhi, 1998" startWordPosition="1334" endWordPosition="1335">ystem in this task. A full description of our system and its performance can be found in (Tetreault and Chodorow, 2008). 3.1 System Our approach treats preposition error detection as a classification problem: that is, given a context of two words before and two words after the writer’s preposition, what is the best preposition to use? 4There is a third error type, omission (“we are fond null beer”), that is a topic for our future research. 25 An error is marked when the system’s suggestion differs from the writer’s by a certain threshold amount. We have used a maximum entropy (ME) classifier (Ratnaparkhi, 1998) to select the most probable preposition for a given context from a set of 34 common English prepositions. One advantage of using ME is that there are implementations of it which can handle very large models built from millions of training events and consisting of hundreds of thousands of feature-value pairs. To construct a model, we begin with a training corpus that is POS-tagged and heuristically chunked into noun phrases and verb phrases5. For each preposition that occurs in the training corpus, a preprocessing program extracts a total of 25 features. These consist of words and POS tags in </context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Ratnaparkhi, A. 1998. Maximum Entropy Models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>The ups and downs of preposition error detection in ESL writing.</title>
<date>2008</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="7792" citStr="Tetreault and Chodorow, 2008" startWordPosition="1247" endWordPosition="1250">, the selection of a preposition for a given context also depends upon the intention of the writer (“we sat at the beach”, “on the beach”, “near the beach”, “by the beach”). 3 Automatically Detecting Preposition Usage Errors In this section, we give a description of our system and compare its performance to other systems. Although the focus of this paper is on human judgments in the task of error detection, we describe our system to show that variability in human judgments can impact the evaluation of a system in this task. A full description of our system and its performance can be found in (Tetreault and Chodorow, 2008). 3.1 System Our approach treats preposition error detection as a classification problem: that is, given a context of two words before and two words after the writer’s preposition, what is the best preposition to use? 4There is a third error type, omission (“we are fond null beer”), that is a topic for our future research. 25 An error is marked when the system’s suggestion differs from the writer’s by a certain threshold amount. We have used a maximum entropy (ME) classifier (Ratnaparkhi, 1998) to select the most probable preposition for a given context from a set of 34 common English preposit</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Tetreault, J. and M. Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>