<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.998581">
A SVM-based Model for Chinese Functional Chunk Parsing
</title>
<author confidence="0.994251">
Yingze Zhao
</author>
<affiliation confidence="0.993029">
State Key Laboratory of Intelligent Technol-
ogy and Systems
Dept. of Computer Science and Technology,
Tsinghua University
</affiliation>
<address confidence="0.829082">
Beijing 100084, P. R. China
</address>
<email confidence="0.998086">
zhaoyingze@gmail.com
</email>
<author confidence="0.99268">
Qiang Zhou
</author>
<affiliation confidence="0.99250225">
State Key Laboratory of Intelligent Technol-
ogy and Systems
Dept. of Computer Science and Technology,
Tsinghua University
</affiliation>
<address confidence="0.82866">
Beijing 100084, P. R. China
</address>
<email confidence="0.997544">
zq-lxd@mail.tsinghua.edu.cn
</email>
<sectionHeader confidence="0.995603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967173913043">
Functional chunks are defined as a series
of non-overlapping, non-nested segments
of text in a sentence, representing the im-
plicit grammatical relations between the
sentence-level predicates and their argu-
ments. Its top-down scheme and com-
plexity of internal constitutions bring in a
new challenge for automatic parser. In
this paper, a new parsing model is pro-
posed to formulate the complete chunk-
ing problem as a series of boundary de-
tection sub tasks. Each of these sub tasks
is only in charge of detecting one type of
the chunk boundaries. As each sub task
could be modeled as a binary classifica-
tion problem, a lot of machine learning
techniques could be applied.
In our experiments, we only focus on
the subject-predicate (SP) and predicate-
object (PO) boundary detection sub tasks.
By applying SVM algorithm to these sub
tasks, we have achieved the best F-Score
of 76.56% and 82.26% respectively.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964895833334">
Parsing is a basic task in natural language proc-
essing; however, it has not been successful in
achieving the accuracy and efficiency required
by real world applications. As an alternative,
shallow parsing or partial parsing has been pro-
posed to meet the current needs by obtaining
only a limited amount of syntactic information
needed by the application. In recent years, there
has been an increasing interest in chunk parsing.
From CoNLL-2000 to CoNLL-2005, a lot of ef-
forts have been made in the identification of ba-
sic chunks and the methods of combining them
from bottom-up to form large, complex units. In
this paper, we will apply functional chunks to
Chinese shallow parsing.
Functional chunks are defined as a series of
non-overlapping, non-nested functional units in a
sentence, such as subjects, predicates, objects,
adverbs, complements and so on. These units
represent the implicit grammatical relations be-
tween the sentence-level predicates and their ar-
guments. Different from the basic chunks de-
fined by Abney (1991), functional chunks are
generated from a top-down scheme, and thus
their constitutions may be very complex. In addi-
tion, the type of a functional chunk could not be
simply determined by its constitution, but de-
pends heavily on the context. Therefore, we will
have new challenges in the functional chunk
parsing.
Ramshaw and Marcus (1995) first introduced
the machine learning techniques to chunking
problem. By formulating the NP-chunking task
as a tagging process, they marked each word
with a tag from set {B, I, O}, and successfully
applied TBL to it. Inspired by their work, we
introduce SVM algorithm to our functional
chunking problem. Instead of using the BIO tag-
ging system, we propose a new model for solv-
ing this problem. In this model, we do not tag the
words with BIO tags, but directly discover the
chunk boundaries between every two adjacent
functional chunks. Each of these chunk bounda-
ries will be assigned a type to it, which contains
the information of the functional chunk types
before and after it. Then we further decompose
this model into a series of sub modules, each of
which is in charge of detecting only one type of
</bodyText>
<page confidence="0.991714">
94
</page>
<note confidence="0.6615245">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 94–101,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999927636363637">
the chunk boundaries. As each sub module can
be modeled as a binary classifier, various ma-
chine learning techniques could be applied.
In our experiments, we focus on the subject-
predicate (SP) and predicate-object (PO) bound-
ary detection tasks, which are the most difficult
but important parts in our parsing model. By ap-
plying SVM algorithm to these tasks, we achieve
the best F-Score of 76.56% and 82.26% respec-
tively.
This paper is organized as follows. In section
2, we give a brief introduction to the concept of
our functional chunks. In section 3, we propose
the parsing model for Chinese functional chunk
parsing. In section 4, we compare SVM with sev-
eral other machine learning techniques, and illus-
trate how competitive SVM is in our chunking
task. In section 5, we build 2 sub modules based
on SVM algorithm for SP and PO boundary de-
tection tasks. In section 6, some related work on
functional chunk parsing is introduced. Section 7
is the conclusion.
</bodyText>
<sectionHeader confidence="0.980296" genericHeader="method">
2 Functional Chunk Scheme
</sectionHeader>
<bodyText confidence="0.964654555555556">
Functional chunks are defined as a series of
non-overlapping, non-nested segments of text at
the sentence level without leaving any words
outside. Each chunk is labeled with a functional
tag, such as subject, predicate, object and so on.
These functional chunks in the sentence form a
linear structure within which the grammatical
relations between sentence-level predicates and
their arguments or adjuncts are kept implicitly.
</bodyText>
<tableCaption confidence="0.988836">
Table 1 lists all the tags used in our functional
chunk scheme:
Table 1. Functional Chunk Tag Set.
</tableCaption>
<table confidence="0.992316777777778">
Chunk Tag Basic Function Description
S Subject
P Predicate
O Object
J Raised Object
D Adverbial adjunct
C Complement
T Independent constituent
Y Modal particle
</table>
<bodyText confidence="0.996783">
Here, we list some examples to illustrate how
these functional tags are used in Chinese sen-
tences.
</bodyText>
<equation confidence="0.946363615384615">
1. “[D /t (afternoon) / [D /p
(when) /rN (I) /v (come to)
/nS (Xi Bai Po village) /s (eastern entrance)
/n / [D /d (already) [P /v (there is)
[J /m /qN (a) /n (brainman) [D /p
/rS (there) [P /v (waiting) [Y /y
/ “
2. “[T /l (frankly speaking) / [S
/rN (that) [P /vC (was) [O /rN (I) /d
(lifetime) /dN /vM (can’t) /v (forget)
/u / “
3. “[S /n (time) [P /v /u (schedule)
[C /dD (very) /a (tight) / “
</equation>
<bodyText confidence="0.892184428571429">
Compared with the basic chunk scheme de-
fined by Abney (1991), our functional chunk
scheme has the following two main differences:
(1) Functional chunks are not constituted from
bottom-up, but generated from top-down, thus
some functional chunks are usually longer and
more complex than the basic chunks.
We have a collection of 185 news files as our
functional chunk corpus. Each file is manually
annotated with functional chunks. There are
about 200,000 Chinese words in the corpus. To
investigate the complex constitutions of func-
tional chunks, we list the average chunk lengths
(ACL) of different types in Table 2:
</bodyText>
<tableCaption confidence="0.972422">
Table 2. Average Chunk Lengths of Different
Types.
</tableCaption>
<table confidence="0.999048333333333">
Chunk Type Count Word Sum ACL
P 21988 27618 1.26
D 19795 46919 2.37
O 14289 61401 4.30
S 11920 34479 2.89
J 855 2083 2.44
Y 594 604 1.02
T 407 909 2.23
C 244 444 1.82
</table>
<bodyText confidence="0.987828555555556">
From the table above, we can find that O
chunk has the longest average length of 4.30
words, and S chunk has the second longest aver-
age length of 2.89 words, and D chunk has an
average length of 2.37 words. Although the aver-
age length doesn’t seem so long, the length of a
specific chunk varies greatly.
In Table 3, we list some detailed length distri-
butional data of three chunks.
</bodyText>
<tableCaption confidence="0.9404755">
Table 3. Length Distribution of S, O and D
Chunks.
</tableCaption>
<table confidence="0.995474">
Chunk Length # of S # of O # of D
1 5322 3537 12147
2 2093 2228 2499
3 1402 2117 1431
4 917 1624 1010
5 627 1108 696
&gt;5 1559 3675 2013
Sum 11920 14289 19796
</table>
<page confidence="0.999208">
95
</page>
<bodyText confidence="0.9999858">
From the table above, we can find that there
are totally 1559 S chunks with a length of more
than 5 words which takes up 13.08% of the total
number. And when we refer to the S chunks with
more than 3 words, the percentage will increase
to 26.03%. These long chunks are usually consti-
tuted with several complex phrases or clauses as
the modifiers of a head word. Among the O
chunks, 25.72% of them have a length of more
than 5 words, and 44.84% of them are longer
than 3 words. The reason why O chunks have a
longer length may be that many of them contain
the entire clauses. Although most of the D
chunks are less than 5 words, some constituted
with complex preposition phrases can still be
very long.
The complex constitutions of S, O, D chunks
are the main parsing difficulties.
(2) The type of functional chunks can’t be
simply determined by their constitutions, but de-
pends heavily on their contexts.
As the constitution of a basic chunk is very
simple, its type can be largely determined by its
head word, but in the case of functional chunks,
the relationships between the functional chunks
play an important role. For example, a NP phrase
before a P chunk can be identified as a subject
chunk, but in other sentences, when it follows
another P chunk, it will be recognized as an ob-
ject chunk. Thus we can’t determine the type of a
functional chunk simply by its constitution.
The context dependencies of functional
chunks bring a new challenge for our chunk
parser.
In the next section, we will propose a top-
down model for Chinese functional chunk pars-
ing. Since the functional chunk boundaries have
the information of linking two adjacent chunks,
they will be very helpful in the determination of
chunk types.
</bodyText>
<sectionHeader confidence="0.995633" genericHeader="method">
3 Parsing Model
</sectionHeader>
<bodyText confidence="0.999967125">
The Chinese functional chunk parser takes a
stream of segmented and tagged words as its in-
put, and outputs all the functional chunk bounda-
ries in a sentence. In this section, we will present
a parsing model which formulates the functional
chunk parsing problem as a boundary detection
task, and then decompose this model into a series
of sub modules that are easy to build.
</bodyText>
<subsectionHeader confidence="0.991641">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.9993444">
Functional chunks have the property of exhaust-
ibility and no words will be left outside the
chunks. Thus we don’t need to find the end posi-
tion for a functional chunk as it could be identi-
fied by the start of the next one. In this case, we
can simply regard the chunking task as a process
of cutting the input sentence into several seg-
ments of words, each of which is labeled with a
functional tag. Based on this idea, we can model
the functional chunk parsing problem as a
boundary detection task.
Let S=&lt;W, T&gt; denote the input sentence to be
parsed by the functional chunk parser, where
W=w1w2w3...wn is the sequence of words in S,
and T=t1t2t3...tn is sequence of the POS tags as-
signed to each word in W. If wi is a punctuation
mark, ti will be equal to wi.
A chunk boundary is defined as a pair &lt;C1,
C2&gt; where C1 ,C2 {S, P, O, J, D, C, T, Y}, C1
is the chunk type before this boundary and C2 is
the chunk type following it. The output of the
chunk parser is denoted as O=&lt;B, P&gt; where
B=b1b2b3...bm is the sequence of chunk bounda-
ries generated by the parser, and P=p1p2p3...pm is
the corresponding positions of b1b2b3...bm in the
sentence.
Chinese functional chunk parser can be con-
sidered as a function h(S) which maps the input
sentence S to the chunk boundary sequence O.
Take the following sentence for example:
</bodyText>
<equation confidence="0.9792782">
“14 /n(Nuclear electricity) 1 /vC(is) 2
/m(a) 3 /qN(kind) 4 /a(safe) 5 / 6
/a(safe) 7 / /a(economical) 9 /u
8
10 /n(energy) 11 / ”“Nuclear electricity is a kind of safe, clean and
</equation>
<bodyText confidence="0.909758722222222">
economical energy.”
In this sentence, there are totally 12 Chinese
words (punctuation marks are treated the same
way as words) with 11 numbers falling between
them indicating the positions where a functional
chunk boundary may appear. If the input sen-
tence is parsed correctly by the functional chunk
parser, a series of boundaries will arise at posi-
tion 1 and 2, which are illustrated as below:
“14 /n &lt;S, P&gt; /vC &lt;P, O&gt; /m /qN
/a / /a / /a /u /n
/ ”
From the information provided by these
boundaries, we can easily identify the functional
chunks in the sentence:
“14 [S /n [P /vC [O /m /qN
/a / /a / /a /u
/n / ”
</bodyText>
<page confidence="0.95182">
96
</page>
<subsectionHeader confidence="0.999725">
3.2 Decomposition of Parsing Model
</subsectionHeader>
<bodyText confidence="0.967326222222222">
The functional chunk parser presented above
could be further divided into several sub modules,
each of which is only in charge of detecting one
type of the chunk boundaries in a sentence. The
sub module in charge of detecting boundary b
could be formulated as a Boolean function hb(S, i)
where S is the input sentence and i is the position
between word wi and wi+1. Function hb(S, i) will
take true if there is a chunk boundary of type b at
position i, and it will take false if there’s not.
Since the Boolean function hb(S, i) can be treated
as a binary classifier, many machine learning
techniques could be applied.
If we combine every two chunk types in the
tag set, we can make a total number of 8*8=64
boundary types in our chunking task. However,
not all of them appear in the natural language
text, for example, we don’t have any SO bounda-
ries in our corpus as S and O chunks can’t be-
come neighbors in a sentence without any P
chunks between them. In our corpus, we could
find 43 boundary types, but only a small number
of them are used very frequently. In table 4, we
list the 5 most frequently used boundaries in our
corpus:
Table 4. The 5 Most Frequently Used Bounda-
ries in the Corpus.
</bodyText>
<table confidence="0.9934375">
Boundary Type Count
PO 14209
DP 11459
SD 6156
DD 5238
SP 5233
</table>
<bodyText confidence="0.996954285714286">
The top 5 boundaries take up 67.76% of all the
62418 boundaries in our corpus. If we further
investigate the chunk types associated with these
boundaries, we can find that only four types are
involved: P, D, O and S. Referred to Table 2, we
can find that these chunks are also the 4 most
frequently used chunks in our corpus.
In most cases, S, P, and O chunks constitute
the backbone of a Chinese sentence, and they
usually contain the most useful information we
need. Therefore, we are more concerned about S,
P and O chunks. In the following sections, we
will focus on the construction of sub modules for
SP and PO boundary detection tasks.
</bodyText>
<sectionHeader confidence="0.99932" genericHeader="method">
4 Statistical Model Selection
</sectionHeader>
<bodyText confidence="0.999894">
After decomposing the parsing model into sev-
eral sub modules, a lot of machine learning tech-
niques could be applied to the constructions of
these sub modules.
SVM 1 is a machine learning technique for
solving the binary classification problems. It is
well known for its good generalization perform-
ance and high efficiency. In this section, we will
make a performance comparison between SVM
(Vapnik, 1995) and several other machine learn-
ing techniques including Naïve Bayes, ID3 2
(Quinlan, 1986) and C4.53 (Quinlan, 1993), and
then illustrates how competitive SVM is in the
boundary detection tasks.
</bodyText>
<subsectionHeader confidence="0.983853">
4.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.999752117647059">
The corpus we use here is a collection of 185
news files which are manually corrected after
automatic sentence-split, word segmentation and
part-of-speech tagging. After these processes,
they have been manually annotated with func-
tional chunks. Among the 185 files, 167 of them
are taken as the training data and the remaining
18 are left as the test data, which takes up ap-
proximately 10% of all the data.
In our experiments, we will use feature tem-
plates to describe which features are to be used
in the generation of feature vectors. For example,
if the current feature template we use is w-1t2,
then the feature vector generated at position i
will take the first word on the left and the second
word tag on the right as its features.
Before we perform any experiments, all the
data have been converted to the vectors that are
acceptable by different machine learning algo-
rithms. Thus we have a total number of 199268
feature vectors generated from the 185 files.
Among them, 172465 vectors are in the training
data and 26803 vectors are in the test data. Two
sets of training and test data are prepared respec-
tively for the SP and PO boundary detection
tasks.
The performance of each experiment is meas-
ured with 3 rates: precision, recall and Fβ=1,
where precision is the percentage of detected
boundaries that are correct, recall is the percent-
age of boundaries in the test data that are found
by the parser, and Fβ=1 is defined as
Fβ=(β2+1)*precision*recall/(β2*precision + recall)
with β=1.
</bodyText>
<footnote confidence="0.971787875">
1 The software package we use is SVMlight v6.00, it is avail-
able at http://svmlight.joachims.org/. We use linear kernel
function and other default parameters in our experiments.
2 We use the weka’s implementation of Naïve Bayes and
ID3 algorithms. Weak 3.4 is available at
http://www.cs.waikato.ac.nz/ml/weka/.
3 We use Quinlan’s C4.5 software package with its default
parameters in our experiments.
</footnote>
<page confidence="0.998785">
97
</page>
<subsectionHeader confidence="0.962859">
4.2 Algorithm Comparison
</subsectionHeader>
<bodyText confidence="0.9836915">
We first use t-3t-2t-1t1t2 as the feature tem-
plate, and list all the experimental results in Ta-
ble 5 and Table 6. From these results, we can
find that SVM has achieved the best precision,
recall and F-Score in SP boundary detection task,
while C4.5 has an overwhelming advantage in
PO boundary detection task. In both tasks, Naïve
Bayes algorithm performs the worst, which
</bodyText>
<tableCaption confidence="0.783944">
makes us very disappointed.
Table 5. Results of Different Algorithms in SP
Boundary Detection Task.
</tableCaption>
<table confidence="0.999951">
Algorithm Precision Recall Fβ=1
SVM 82.21% 57.10% 67.39%
ID3 67.60% 50.70% 57.94%
C4.5 81.10% 44.60% 57.55%
Naïve Bayes 47.90% 51.00% 49.40%
</table>
<tableCaption confidence="0.502723">
Table 6. Results of Different Algorithms in
PO Boundary Detection Task.
</tableCaption>
<table confidence="0.9992422">
Algorithm Precision Recall Fβ=1
C4.5 72.00% 74.70% 73.33%
SVM 67.27% 64.96% 66.09%
ID3 70.70% 59.90% 64.85%
Naïve Bayes 48.10% 60.10% 53.43%
</table>
<bodyText confidence="0.999370466666667">
As the feature template we use here is too sim-
ple, the results we have got may not seem so per-
suasive. Therefore we decide to conduct another
experiment using a more complex feature tem-
plate.
In the following experiments, we will use w-
2w-1w1w2t-2t-1t1t2 as the feature template. The
experimental results are listed in Table 7 and Ta-
ble 8.
After adding the word information to the fea-
ture template, the dimensions of feature vectors
used by some algorithms increase dramatically.
We remove Naïve Bayes algorithm from the fol-
lowing experiments, as it fails to deal with such
high dimensional data.
</bodyText>
<tableCaption confidence="0.863783">
Table 7. Results of Different Algorithms in SP
Boundary Detection Task.
</tableCaption>
<table confidence="0.99989275">
Algorithm Precision Recall Fβ=1
SVM 82.25% 61.22% 70.19%
ID3 64.70% 51.70% 57.47%
C4.5 79.70% 37.40% 50.91%
</table>
<tableCaption confidence="0.8418025">
Table 8. Results of Different Algorithms in
PO Boundary Detection Task.
</tableCaption>
<table confidence="0.99235925">
Algorithm Precision Recall Fβ=1
SVM 74.83% 86.99% 80.45%
C4.5 67.90% 79.90% 73.41%
ID3 75.10% 57.70% 65.26%
</table>
<bodyText confidence="0.9999389">
After applying the complex feature template,
SVM still keeps the first place in SP boundary
detection task. In PO boundary detection task,
SVM successfully takes the place of C4.5, and
achieves the best recall and F-Score among all
the algorithms. Although the precision of ID3 is
a little better than SVM, we still prefer SVM to
ID3. It seems that the word information in the
feature vectors is not so beneficial to decision
tree algorithms as to SVM.
We also notice that SVM can perform very ef-
ficiently even with a large number of features. In
the second set experiments, it usually takes sev-
eral hours to train a decision tree model, but for
SVM, the time cost is no more than 20 minutes.
In addition, we can expect a better result by add-
ing more information to SVM algorithm without
worrying about the dimension disaster problem
in other algorithms. Therefore, we decide to base
our parsing model on SVM algorithm.
</bodyText>
<sectionHeader confidence="0.983967" genericHeader="method">
5 The SVM-based Parsing Model
</sectionHeader>
<subsectionHeader confidence="0.777478">
5.1 Baseline Models
</subsectionHeader>
<bodyText confidence="0.999666133333333">
In this section, we will build 2 baseline models
based on SVM for SP and PO boundary detec-
tion tasks respectively. By comprising the results
of two different feature templates, we will illus-
trate how useful the word information is in our
SVM based models.
One feature template we use here is the simple
template which only takes the POS tag informa-
tion as its features. The other one is the complex
template which takes both word and tag informa-
tion as its features. To make sure the results are
comparable, we restrict the context window to 4
words.
In the SP boundary detection sub task, we got
the following results:
</bodyText>
<tableCaption confidence="0.975167">
Table 9. SP Boundary Detection Results.
</tableCaption>
<table confidence="0.862794166666667">
Feature template Precision Recall Fβ=1
t-2t-1t1t2 76.25% 51.99% 61.83%
w-2w-1w1w2t- 82.25% 61.22% 70.19%
2t-1t1t2
In the PO boundary detection sub task, we got
the following results:
</table>
<tableCaption confidence="0.978821">
Table 10. PO Boundary Detection Results.
</tableCaption>
<table confidence="0.8370685">
Feature template Precision Recall Fβ=1
t-2t-1t1t2 66.42% 65.27% 65.84%
w-2w-1w1w2t- 74.83% 86.99% 80.45%
2t-1t1t2
</table>
<bodyText confidence="0.999891428571429">
By taking the complex feature template, we
have achieved the best Fβ=1 value of 70.19% in
SP boundary detection experiment and 80.45%
in PO experiment, both of which are much
higher than those of the simple feature templates.
From these results we can conclude that word
information is very helpful in our SVM based
</bodyText>
<page confidence="0.995254">
98
</page>
<bodyText confidence="0.999654333333333">
models. Thus we will only use the feature tem-
plates with word information in the succeeding
experiments.
</bodyText>
<subsectionHeader confidence="0.998763">
5.2 Expanding the Context Window
</subsectionHeader>
<bodyText confidence="0.966569647058824">
In the previous section, the feature templates we
use are restricted to a context window of 4 words,
which might not be large enough to detect the
boundaries between complex chunks. For exam-
ple, when parsing the sentence “[P /v 1 [O
/a 2 /u 3 /n 4 /n 5 /vN 6
/n”, the algorithm fails to detect the PO boundary
at position 1. If we expand the context window to
the noun word “ /n”, some of these errors
may disappear. In the following experiments, we
will expand the context window from a size of 4
words to 10 words, and make a comparison be-
tween the different results.
The 4 feature templates used here are listed
below:
T1: w-2w-1w1w2t-2t-1t1t2,
T2: w-3w-2w-1w1w2w3t-3t-2t-1t1t2t3,
</bodyText>
<figure confidence="0.35755375">
T3: w-4w-3w-2w-1w1w2w3w4t-4t-3t-2t-
1t1t2t3t4
T4: w-5w-4w-3w-2w-1w1w2w3w4w5t-5t-4t-
3t-2t-1t1t2t3t4t5.
</figure>
<figureCaption confidence="0.994131">
Figure 1. SP Boundary Detection Results.
</figureCaption>
<bodyText confidence="0.999769909090909">
As we have expected, the performance of SP
boundary detection experiment has been im-
proved as the context window expands from a
size of 4 words to 8 words. However, the preci-
sion value meets its turning point at T3 after
which it goes down, while F-Score and recall
value still keep rising. From the curves shown in
figure 1, we can find that the expansion of con-
text window size from 4 words to 6 words has an
obvious improvement for performance, and after
that only F-Score and recall could be improved.
</bodyText>
<figureCaption confidence="0.963164">
Figure 2. SP Boundary Detection Results.
</figureCaption>
<bodyText confidence="0.999978444444445">
In contrast to the significant improvement we
have achieved in the SP experiments, the results
of PO experiments are not so exciting. As the
context window expands, the precision value
keeps rising while the recall value keeps declin-
ing. Fortunately, we have obtained a very slight
increase of F-Score from these efforts.
Although it is very difficult to improve the
performance of PO boundary detection by simply
expanding the context window, we’ve still got a
better result than that of SP. If we examine the
results of the two tasks carefully, we can find a
very interesting difference between them: in SP
boundary detection task, it’s very easier to get a
better precision than recall, but in PO experiment,
as the O chunks have a longer length, they are
more likely to be cut into small pieces, and thus
it’s easier to get a better recall than precision.
</bodyText>
<subsectionHeader confidence="0.984539">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.9996290625">
In our experiments, the recall value can be sim-
ply raised by adding a positive bias value to the
SVM classifier. However, we can’t do the same
thing to improve the precision value. Thus, in the
following analysis, we are only focus on the er-
rors that deter the improvement of precision
value.
There are 2 kinds of errors influencing the
precision value of the test results: One is the
wrongly detected chunk boundaries (WDB)
within chunks (these chunk boundaries are de-
tected by the program, but they don’t exist in the
training data). This kind of error tends to cut a
large chunk into several small pieces. The other
is the misclassification of chunk boundary types
(MBT) at the chunk boundaries (There exists a
</bodyText>
<table confidence="0.704531647058824">
SP Boundary Detection Results
83.84% 86.44% 86.15%
82.25%
74.07% 76.06% 76.56%
70.19% 68.89%
66.34% 67.90%
61.22%
T1 T2 T3 T4
85.00%
80.00%
75.00%
70.00%
65.00%
60.00%
55.00%
Precision Recall F-Score
PO Boundary Detection Results
86.83% 86.42% 86.17% 86.12%
81.97% 82.26%
80.41% 81.53%
78.16% 78.74%
77.17%
74.87%
88.00%
86.00%
84.00%
82.00%
80.00%
78.00%
76.00%
74.00%
72.00%
T1 T2 T3 T4
Precision Recall F-Score
</table>
<page confidence="0.995413">
99
</page>
<bodyText confidence="0.9998368">
chunk boundary at that position, but chunk
boundary type labeled by the program is wrong).
In the following analysis, by comparing the
numbers of errors in the test results of T1 (w-2w-
1w1w2t-2t-1t1t2) and T4 (w-5w-4w-3w-2w-
1w1w2w3w4w5t-5t-4t-3t-2t-1t1t2t3t4t5), we
will point out which kind of errors could be ef-
fectively eliminated by the expansion of context
window and which of them couldn’t. Through
this analysis, we hope to get some knowledge of
what efforts should be made in our further study.
In SP boundary detection task, we list the
number of wrongly detected chunk boundaries
(#WDB) and the corresponding chunk types (CT)
where WDB arises in the following table.
</bodyText>
<tableCaption confidence="0.658975">
Table 11. Wrongly Detected Chunk Bounda-
ries in the Test Results of T1 and T4.
</tableCaption>
<table confidence="0.993531625">
CT #WDB of T1 #WDB of T4 T4-T1
O 17 18 1
S 17 18 1
D 7 6 -1
C 0 1 1
P 2 1 -1
T 1 1 0
Sum 44 45 1
</table>
<bodyText confidence="0.999834666666666">
From the above table, we find that the number
of wrongly detected boundaries seems to be un-
changed during the expansion of context window.
But when we refer to the second type of errors,
the expansion of context window does help. We
list the misclassified boundary types (MBT) and
the error numbers (#MB) in the below table. In
SP boundary detection task, MBT is wrongly
recognized as boundary type SP.
</bodyText>
<tableCaption confidence="0.80866">
Table 12. Misclassified Chunk Boundaries in
the Test Results of T1 and T4.
</tableCaption>
<table confidence="0.995412571428571">
MBT #MB of T1 #MB of T4 T4-T1
OP 9 3 -6
JP 8 2 -6
DP 23 20 -3
SD 6 6 0
DS 1 1 0
Sum 47 32 -15
</table>
<bodyText confidence="0.999798888888889">
From the above table, we can find that the
misclassifications of OP, JP and DP as SP have
been largely reduced by expanding the context
window, but the misclassifications of DS and SD
remain the same. Therefore, we should try some
other methods for D chunks in our future work.
In PO boundary detection task, the expansion
of context window seems to be very effective.
We list all the results in the below table:
</bodyText>
<tableCaption confidence="0.714408">
Table 14. Wrongly Detected Chunk Bounda-
ries in the Test Results of T1 and T4.
</tableCaption>
<table confidence="0.984206666666667">
CT #WDB of T1 #WDB of T4 T4-T1
O 251 196 -55
S 106 76 -30
D 92 55 -37
P 56 64 8
T 4 4 0
C 1 1 0
J 0 1 1
Sum 510 397 -113
</table>
<bodyText confidence="0.99447925">
It’s very exciting to see that by expanding the
window size, the number of WDB decreases dra-
matically from 510 to 397. But it fails to elimi-
nate the WDB errors within P, T, C, and J
chunks.
In PO boundary detection task, MBT is
wrongly recognized as boundary type PO. We
list the error data of T1 and T4 in the below table.
</bodyText>
<tableCaption confidence="0.865265">
Table 13. Misclassified Chunk Boundaries in
the Test Results of T1 and T4.
</tableCaption>
<table confidence="0.9968395">
MBT #MB of T1 #MB of T4 T4-T1
PJ 17 18 1
PD 9 9 0
PC 8 8 0
SP 6 6 0
PS 5 5 0
SD 5 4 -1
DP 3 2 -1
TS 3 3 0
OD 1 0 -1
PY 1 1 0
Sum 58 56 -2
</table>
<bodyText confidence="0.999939">
In contrast to the results of SP boundary detec-
tion task, the MBT errors could not be largely
reduced by simply expanding the context win-
dow. Therefore, we need to pay more attention to
these problems in our future work.
</bodyText>
<sectionHeader confidence="0.999936" genericHeader="method">
6 Related works
</sectionHeader>
<bodyText confidence="0.999649133333333">
After the work of Ramshaw and Marcus (1995) ,
many machine learning techniques have been
applied to the basic chunking task, such as Sup-
port Vector Machines (Kudo and Matsumoto,
2001), Hidden Markov Model(Molina and Pla
2002), Memory Based Learning (Sang, 2002),
Conditional Random Fields (Sha and Pereira,
2003), and so on. But only a small amount of
attention has been paid to the functional chunk
parsing problem.
Sandra and Erhard (2001) tried to construct
the function-argument structures based on the
pre-chunked input. They proposed a similarity
based algorithm to assign the functional labels to
complete syntactic structures, and achieved a
</bodyText>
<page confidence="0.969337">
100
</page>
<bodyText confidence="0.999970526315789">
precision of 89.73% and 90.40% for German and
English respectively. Different from our top-
down scheme, their function-argument structures
are still constituted from bottom-up, and the pre-
chunked input helps simplify the chunking proc-
ess.
Elliott and Qiang Zhou (2001) used the BIO
tagging system to identify the functional chunks
in a sentence. In their experiments, they used
C4.5 algorithm to build the parsing model, and
focused their efforts on the selection of feature
sets. After testing 5 sets of features, they have
achieved the best f-measure of 0.741 by using
feature set E which contains all the features in
other feature sets. Instead of using BIO tags in
our chunking task, we introduced chunk bounda-
ries to help us identify the functional chunks,
which could provide more relational information
between the functional chunks.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="conclusions">
7 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.99996715625">
In this paper, we have applied functional chunks
to Chinese shallow parsing. Since the functional
chunks have the properties of linearity and ex-
haustibility, we can formulate the functional
chunk parsing problem as a boundary detection
task. By applying the divide-and-conquer strat-
egy, we have further decomposed the parsing
model into a series of sub modules, each of
which is only in charge of one boundary type. In
this way, we provide a very flexible framework
within which different machine learning tech-
niques could be applied. In our experiments, we
build two sub modules based on SVM for solv-
ing the SP and PO boundary detection tasks.
Thanks to the good generalization performance
and high efficiency of SVM, we can successfully
deal with a large number of features. By expand-
ing the context window, we have achieved the
best F-Score of 76.56% and 82.26 for SP and PO
boundary detection tasks.
The 2 sub modules we have built are only
parts of the Chinese functional chunk parser. Al-
though the results we have got here seem some-
what coarse, they could already be used in some
simple tasks. In the future, we will build the
other sub modules for the remaining types of the
chunk boundaries. After all these work, there
may be some inconsistent chunk boundaries in
the results, thus we need to solve the inconsis-
tency problems and try to identify all the func-
tional chunks in a sentence by combining these
chunk boundaries.
</bodyText>
<sectionHeader confidence="0.99699" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.761451333333333">
This work was supported by the Chinese National
Science Foundation (Grant No. 60573185,
60520130299).
</reference>
<sectionHeader confidence="0.829132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999755142857143">
Elliott Franco Drábek and Qiang Zhou. 2001. Ex-
periments in Learning Models for Functional
Chunking of Chinese Text. IEEE International
Workshop on Natural Language processing and
Knowledge Engineering, Tucson, Arizona, pages
859-864.
E.F. Tjong Kim Sang. 2002. Memory-based shallow
parsing, Journal of Machine Learning Research 2,
pages 559-594.
F. Sha and F. Pereira. 2003. Shallow parsing with
conditional random fields. In Proceedings of Hu-
man Language Technology Conference / North
American Chapter of the Association for Computa-
tional Linguistics annual meeting.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition, Morgan Kaufmann, San Francisco.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. Proceedings of the
Second Meeting of the North American Chapter of
the Association for Computational Linguistics.
Pittsburgh, PA.
Lance Ramshaw and Mitch Marcus. 1995. Text
chunking using transformation-based learning. In
Proceedings of the Third Workshop on Very Large
Corpora, pages 82—94.
Quinlan, J. Ross. 1986. Induction of decision trees.
Machine Learning, 1(1), pages 81-106.
Quinlan, J. Ross. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
Steven Abney. 1991. Parsing by chunks. In Principle-
Based Parsing, Kluwer Academic Publishers,
Dordrecht, pages 257—278.
Sandra Kübler and Erhard W. Hinrichs. 2001. From
chunks to function-argument structure: A similar-
ity-based approach. In Proceedings of ACL/EACL
2001, Toulouse, France, 2001, pages 338 - 345.
Thorsten Joachims. 1999. Advances in Kernel Meth-
ods - Support Vector Learning, chapter Making
large-Scale SVM Learning Practical. MIT-Press.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
</reference>
<page confidence="0.998607">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.067576">
<title confidence="0.999086">A SVM-based Model for Chinese Functional Chunk Parsing</title>
<author confidence="0.277053">Yingze</author>
<affiliation confidence="0.66414325">Key Laboratory of Intelligent ogy and Dept. of Computer Science and Tsinghua</affiliation>
<address confidence="0.90858">Beijing 100084, P. R. China</address>
<email confidence="0.999475">zhaoyingze@gmail.com</email>
<author confidence="0.519917">Qiang</author>
<affiliation confidence="0.80032825">Key Laboratory of Intelligent ogy and Dept. of Computer Science and Tsinghua</affiliation>
<address confidence="0.953647">Beijing 100084, P. R. China</address>
<email confidence="0.980516">zq-lxd@mail.tsinghua.edu.cn</email>
<abstract confidence="0.997632375">Functional chunks are defined as a series of non-overlapping, non-nested segments of text in a sentence, representing the implicit grammatical relations between the sentence-level predicates and their arguments. Its top-down scheme and complexity of internal constitutions bring in a new challenge for automatic parser. In this paper, a new parsing model is proposed to formulate the complete chunking problem as a series of boundary detection sub tasks. Each of these sub tasks is only in charge of detecting one type of the chunk boundaries. As each sub task could be modeled as a binary classification problem, a lot of machine learning techniques could be applied. In our experiments, we only focus on the subject-predicate (SP) and predicateobject (PO) boundary detection sub tasks. By applying SVM algorithm to these sub tasks, we have achieved the best F-Score of 76.56% and 82.26% respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This work was supported by the Chinese National Science Foundation (Grant No.</title>
<volume>60573185</volume>
<pages>60520130299</pages>
<marker></marker>
<rawString>This work was supported by the Chinese National Science Foundation (Grant No. 60573185, 60520130299).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elliott Franco Drábek</author>
<author>Qiang Zhou</author>
</authors>
<title>Experiments in Learning Models for Functional Chunking of Chinese Text.</title>
<date>2001</date>
<booktitle>IEEE International Workshop on Natural Language processing and Knowledge Engineering,</booktitle>
<pages>859--864</pages>
<location>Tucson, Arizona,</location>
<marker>Drábek, Zhou, 2001</marker>
<rawString>Elliott Franco Drábek and Qiang Zhou. 2001. Experiments in Learning Models for Functional Chunking of Chinese Text. IEEE International Workshop on Natural Language processing and Knowledge Engineering, Tucson, Arizona, pages 859-864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Memory-based shallow parsing,</title>
<date>2002</date>
<journal>Journal of Machine Learning Research</journal>
<volume>2</volume>
<pages>559--594</pages>
<contexts>
<context position="26697" citStr="Sang, 2002" startWordPosition="4683" endWordPosition="4684">4-T1 PJ 17 18 1 PD 9 9 0 PC 8 8 0 SP 6 6 0 PS 5 5 0 SD 5 4 -1 DP 3 2 -1 TS 3 3 0 OD 1 0 -1 PY 1 1 0 Sum 58 56 -2 In contrast to the results of SP boundary detection task, the MBT errors could not be largely reduced by simply expanding the context window. Therefore, we need to pay more attention to these problems in our future work. 6 Related works After the work of Ramshaw and Marcus (1995) , many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model(Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. But only a small amount of attention has been paid to the functional chunk parsing problem. Sandra and Erhard (2001) tried to construct the function-argument structures based on the pre-chunked input. They proposed a similarity based algorithm to assign the functional labels to complete syntactic structures, and achieved a 100 precision of 89.73% and 90.40% for German and English respectively. Different from our topdown scheme, their function-argument structures are still constituted from bottom-up, and the prechunked input helps </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E.F. Tjong Kim Sang. 2002. Memory-based shallow parsing, Journal of Machine Learning Research 2, pages 559-594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics annual meeting.</booktitle>
<contexts>
<context position="26748" citStr="Sha and Pereira, 2003" startWordPosition="4688" endWordPosition="4691"> 0 PS 5 5 0 SD 5 4 -1 DP 3 2 -1 TS 3 3 0 OD 1 0 -1 PY 1 1 0 Sum 58 56 -2 In contrast to the results of SP boundary detection task, the MBT errors could not be largely reduced by simply expanding the context window. Therefore, we need to pay more attention to these problems in our future work. 6 Related works After the work of Ramshaw and Marcus (1995) , many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model(Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. But only a small amount of attention has been paid to the functional chunk parsing problem. Sandra and Erhard (2001) tried to construct the function-argument structures based on the pre-chunked input. They proposed a similarity based algorithm to assign the functional labels to complete syntactic structures, and achieved a 100 precision of 89.73% and 90.40% for German and English respectively. Different from our topdown scheme, their function-argument structures are still constituted from bottom-up, and the prechunked input helps simplify the chunking process. Elliott and Qiang Zh</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of Human Language Technology Conference / North American Chapter of the Association for Computational Linguistics annual meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques, 2nd Edition,</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques, 2nd Edition, Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="26619" citStr="Kudo and Matsumoto, 2001" startWordPosition="4670" endWordPosition="4673">. Misclassified Chunk Boundaries in the Test Results of T1 and T4. MBT #MB of T1 #MB of T4 T4-T1 PJ 17 18 1 PD 9 9 0 PC 8 8 0 SP 6 6 0 PS 5 5 0 SD 5 4 -1 DP 3 2 -1 TS 3 3 0 OD 1 0 -1 PY 1 1 0 Sum 58 56 -2 In contrast to the results of SP boundary detection task, the MBT errors could not be largely reduced by simply expanding the context window. Therefore, we need to pay more attention to these problems in our future work. 6 Related works After the work of Ramshaw and Marcus (1995) , many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model(Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. But only a small amount of attention has been paid to the functional chunk parsing problem. Sandra and Erhard (2001) tried to construct the function-argument structures based on the pre-chunked input. They proposed a similarity based algorithm to assign the functional labels to complete syntactic structures, and achieved a 100 precision of 89.73% and 90.40% for German and English respectively. Different from our topdown scheme, their function-argument st</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="2719" citStr="Ramshaw and Marcus (1995)" startWordPosition="425" endWordPosition="428">g, non-nested functional units in a sentence, such as subjects, predicates, objects, adverbs, complements and so on. These units represent the implicit grammatical relations between the sentence-level predicates and their arguments. Different from the basic chunks defined by Abney (1991), functional chunks are generated from a top-down scheme, and thus their constitutions may be very complex. In addition, the type of a functional chunk could not be simply determined by its constitution, but depends heavily on the context. Therefore, we will have new challenges in the functional chunk parsing. Ramshaw and Marcus (1995) first introduced the machine learning techniques to chunking problem. By formulating the NP-chunking task as a tagging process, they marked each word with a tag from set {B, I, O}, and successfully applied TBL to it. Inspired by their work, we introduce SVM algorithm to our functional chunking problem. Instead of using the BIO tagging system, we propose a new model for solving this problem. In this model, we do not tag the words with BIO tags, but directly discover the chunk boundaries between every two adjacent functional chunks. Each of these chunk boundaries will be assigned a type to it, </context>
<context position="26479" citStr="Ramshaw and Marcus (1995)" startWordPosition="4647" endWordPosition="4650"> PO boundary detection task, MBT is wrongly recognized as boundary type PO. We list the error data of T1 and T4 in the below table. Table 13. Misclassified Chunk Boundaries in the Test Results of T1 and T4. MBT #MB of T1 #MB of T4 T4-T1 PJ 17 18 1 PD 9 9 0 PC 8 8 0 SP 6 6 0 PS 5 5 0 SD 5 4 -1 DP 3 2 -1 TS 3 3 0 OD 1 0 -1 PY 1 1 0 Sum 58 56 -2 In contrast to the results of SP boundary detection task, the MBT errors could not be largely reduced by simply expanding the context window. Therefore, we need to pay more attention to these problems in our future work. 6 Related works After the work of Ramshaw and Marcus (1995) , many machine learning techniques have been applied to the basic chunking task, such as Support Vector Machines (Kudo and Matsumoto, 2001), Hidden Markov Model(Molina and Pla 2002), Memory Based Learning (Sang, 2002), Conditional Random Fields (Sha and Pereira, 2003), and so on. But only a small amount of attention has been paid to the functional chunk parsing problem. Sandra and Erhard (2001) tried to construct the function-argument structures based on the pre-chunked input. They proposed a similarity based algorithm to assign the functional labels to complete syntactic structures, and achi</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82—94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>81--106</pages>
<contexts>
<context position="13986" citStr="Quinlan, 1986" startWordPosition="2457" endWordPosition="2458">we will focus on the construction of sub modules for SP and PO boundary detection tasks. 4 Statistical Model Selection After decomposing the parsing model into several sub modules, a lot of machine learning techniques could be applied to the constructions of these sub modules. SVM 1 is a machine learning technique for solving the binary classification problems. It is well known for its good generalization performance and high efficiency. In this section, we will make a performance comparison between SVM (Vapnik, 1995) and several other machine learning techniques including Naïve Bayes, ID3 2 (Quinlan, 1986) and C4.53 (Quinlan, 1993), and then illustrates how competitive SVM is in the boundary detection tasks. 4.1 Experimental Data The corpus we use here is a collection of 185 news files which are manually corrected after automatic sentence-split, word segmentation and part-of-speech tagging. After these processes, they have been manually annotated with functional chunks. Among the 185 files, 167 of them are taken as the training data and the remaining 18 are left as the test data, which takes up approximately 10% of all the data. In our experiments, we will use feature templates to describe whic</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>Quinlan, J. Ross. 1986. Induction of decision trees. Machine Learning, 1(1), pages 81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="14012" citStr="Quinlan, 1993" startWordPosition="2461" endWordPosition="2462">ruction of sub modules for SP and PO boundary detection tasks. 4 Statistical Model Selection After decomposing the parsing model into several sub modules, a lot of machine learning techniques could be applied to the constructions of these sub modules. SVM 1 is a machine learning technique for solving the binary classification problems. It is well known for its good generalization performance and high efficiency. In this section, we will make a performance comparison between SVM (Vapnik, 1995) and several other machine learning techniques including Naïve Bayes, ID3 2 (Quinlan, 1986) and C4.53 (Quinlan, 1993), and then illustrates how competitive SVM is in the boundary detection tasks. 4.1 Experimental Data The corpus we use here is a collection of 185 news files which are manually corrected after automatic sentence-split, word segmentation and part-of-speech tagging. After these processes, they have been manually annotated with functional chunks. Among the 185 files, 167 of them are taken as the training data and the remaining 18 are left as the test data, which takes up approximately 10% of all the data. In our experiments, we will use feature templates to describe which features are to be used </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. Ross. 1993. C4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>In PrincipleBased Parsing,</booktitle>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="2382" citStr="Abney (1991)" startWordPosition="372" endWordPosition="373"> parsing. From CoNLL-2000 to CoNLL-2005, a lot of efforts have been made in the identification of basic chunks and the methods of combining them from bottom-up to form large, complex units. In this paper, we will apply functional chunks to Chinese shallow parsing. Functional chunks are defined as a series of non-overlapping, non-nested functional units in a sentence, such as subjects, predicates, objects, adverbs, complements and so on. These units represent the implicit grammatical relations between the sentence-level predicates and their arguments. Different from the basic chunks defined by Abney (1991), functional chunks are generated from a top-down scheme, and thus their constitutions may be very complex. In addition, the type of a functional chunk could not be simply determined by its constitution, but depends heavily on the context. Therefore, we will have new challenges in the functional chunk parsing. Ramshaw and Marcus (1995) first introduced the machine learning techniques to chunking problem. By formulating the NP-chunking task as a tagging process, they marked each word with a tag from set {B, I, O}, and successfully applied TBL to it. Inspired by their work, we introduce SVM algo</context>
<context position="5915" citStr="Abney (1991)" startWordPosition="980" endWordPosition="981"> Complement T Independent constituent Y Modal particle Here, we list some examples to illustrate how these functional tags are used in Chinese sentences. 1. “[D /t (afternoon) / [D /p (when) /rN (I) /v (come to) /nS (Xi Bai Po village) /s (eastern entrance) /n / [D /d (already) [P /v (there is) [J /m /qN (a) /n (brainman) [D /p /rS (there) [P /v (waiting) [Y /y / “ 2. “[T /l (frankly speaking) / [S /rN (that) [P /vC (was) [O /rN (I) /d (lifetime) /dN /vM (can’t) /v (forget) /u / “ 3. “[S /n (time) [P /v /u (schedule) [C /dD (very) /a (tight) / “ Compared with the basic chunk scheme defined by Abney (1991), our functional chunk scheme has the following two main differences: (1) Functional chunks are not constituted from bottom-up, but generated from top-down, thus some functional chunks are usually longer and more complex than the basic chunks. We have a collection of 185 news files as our functional chunk corpus. Each file is manually annotated with functional chunks. There are about 200,000 Chinese words in the corpus. To investigate the complex constitutions of functional chunks, we list the average chunk lengths (ACL) of different types in Table 2: Table 2. Average Chunk Lengths of Differen</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In PrincipleBased Parsing, Kluwer Academic Publishers, Dordrecht, pages 257—278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Kübler</author>
<author>Erhard W Hinrichs</author>
</authors>
<title>From chunks to function-argument structure: A similarity-based approach.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL/EACL 2001,</booktitle>
<pages>338--345</pages>
<location>Toulouse,</location>
<marker>Kübler, Hinrichs, 2001</marker>
<rawString>Sandra Kübler and Erhard W. Hinrichs. 2001. From chunks to function-argument structure: A similarity-based approach. In Proceedings of ACL/EACL 2001, Toulouse, France, 2001, pages 338 - 345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning, chapter Making large-Scale SVM Learning Practical. MIT-Press.</booktitle>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Advances in Kernel Methods - Support Vector Learning, chapter Making large-Scale SVM Learning Practical. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="13895" citStr="Vapnik, 1995" startWordPosition="2443" endWordPosition="2444">eed. Therefore, we are more concerned about S, P and O chunks. In the following sections, we will focus on the construction of sub modules for SP and PO boundary detection tasks. 4 Statistical Model Selection After decomposing the parsing model into several sub modules, a lot of machine learning techniques could be applied to the constructions of these sub modules. SVM 1 is a machine learning technique for solving the binary classification problems. It is well known for its good generalization performance and high efficiency. In this section, we will make a performance comparison between SVM (Vapnik, 1995) and several other machine learning techniques including Naïve Bayes, ID3 2 (Quinlan, 1986) and C4.53 (Quinlan, 1993), and then illustrates how competitive SVM is in the boundary detection tasks. 4.1 Experimental Data The corpus we use here is a collection of 185 news files which are manually corrected after automatic sentence-split, word segmentation and part-of-speech tagging. After these processes, they have been manually annotated with functional chunks. Among the 185 files, 167 of them are taken as the training data and the remaining 18 are left as the test data, which takes up approximat</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>