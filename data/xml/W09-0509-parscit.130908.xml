<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010169">
<title confidence="0.946428">
RUBISC - a Robust Unification-Based Incremental Semantic Chunker
</title>
<author confidence="0.983043">
Michaela Atterer David Schlangen
</author>
<affiliation confidence="0.996085">
Department for Linguistics Department for Linguistics
University of Potsdam University of Potsdam
</affiliation>
<email confidence="0.816645">
atterer@ling·uni-potsdam.de das@ling·uni-potsdam.de
</email>
<sectionHeader confidence="0.988857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999842888888889">
We present RUBISC, a new incremen-
tal chunker that can perform incremental
slot filling and revising as it receives a
stream of words. Slot values can influ-
ence each other via a unification mecha-
nism. Chunks correspond to sense units,
and end-of-sentence detection is done in-
crementally based on a notion of seman-
tic/pragmatic completeness. One of RU-
BISC’s main fields of application is in
dialogue systems where it can contribute
to responsiveness and hence naturalness,
because it can provide a partial or com-
plete semantics of an utterance while the
speaker is still speaking. The chunker is
evaluated on a German transcribed speech
corpus and achieves a concept error rate of
43.3% and an F-Score of 81.5.
</bodyText>
<sectionHeader confidence="0.998977" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999849169811321">
Real-time NLP applications such as dialogue sys-
tems can profit considerably from incremental
processing of language. When syntactic and se-
mantic structure is built on-line while the speech
recognition (ASR) is still working on the speech
stream, unnatural silences can be avoided and
the system can react in a faster and more user-
friendly way. As (Aist et al., 2007) and (Skantze
and Schlangen, 2009) show, such incremental sys-
tems are typically preferred by users over non-
incremental systems.
To achieve incrementality, most dialogue sys-
tems employ an incremental chart parser (cf.
(Stoness et al., 2004; Seginer, 2007) etc.). How-
ever, most existing dialogue systems operate in
very limited domains, e.g. moving objects, peo-
ple, trains etc. from one place to another (cf.
(Aist et al., 2007), (Skantze, 2007), (Traum et al.,
1996)). The complexity of the semantic repre-
sentations needed is thus limited. Moreover, user
behaviour (ungrammatical sentences, hesitations,
false starts) and error-prone ASR require the pars-
ing process to be robust.1 We argue that obtaining
relatively flat semantics in a limited domain while
needing exigent robustness calls for investigating
shallower incremental chunking approaches as al-
ternatives to CFG or dependency parsing. Previ-
ous work that uses a combination of shallow and
deep parsing in dialogue systems also indicates
that shallow methods can be superior to deep pars-
ing (Lewin et al., 1999).
The question addressed in this paper is how to
construct a chunker that works incrementally and
robustly and builds the semantics required in a
dialogue system. In our framework chunks are
built according to the semantic information they
contain while syntactic structure itself is less im-
portant. This approach is inspired by Selkirk’s
sense units (Selkirk, 1984). She claims such
units to be relevant for prosodic structure and dif-
ferent to syntactic structure. Similarly, (Abney,
1991) describes some characteristics of chunks as
follows—properties which also make them seem
to be useful units to be considered in spoken dia-
logue systems:
“when I read a sentence, I read it a chunk at
a time. [...] These chunks correspond in some
way to prosodic patterns. Chunks also represent a
grammatical watershed of sorts. The typical chunk
consists of a single content word surrounded by a
constellation of function words, matching a fixed
template. By contrast, the relationships between
chunks are mediated more by lexical selection
</bodyText>
<footnote confidence="0.9423755">
1cf. The incremental parser in (Skantze, 2007) can jump
over a configurable number of words in the input.
</footnote>
<note confidence="0.9442315">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 66–73,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.990621">
66
</page>
<bodyText confidence="0.999901242424242">
than by rigid templates. [...] and the order in
which chunks occur is much more flexible than the
order of words within chunks.”
In our approach chunks are built incrementally
(one at a time) and are defined semantically (a
sense unit is complete when a slot in our template
or frame semantics can be filled). Ideally, in a full
system, the definition of their boundaries will also
be aided by prosodic information. The current im-
plementation builds the chunks or sense units by
identifying a more or less fixed sequence of con-
tent and function words, similar to what Abney
describes as a fixed template. The relationships
between the units are mediated by a unification
mechanism which prevents selectional restrictions
from being violated. This allows the order of the
sense units to be flexible, even as flexible as they
appear in ungrammatical utterances. This unifi-
cation mechanism and the incremental method of
operation are also the main difference to Abney’s
work and other chunkers.
In this paper, we first present our approach of
chunking, show our grammar formalism, the main
features of the chunker (unification mechanism,
incrementality, robustness), and explain how the
chunker can cope with certain tasks that are an is-
sue in dialogue systems, such as online utterance
endpointing and revising hypotheses. In Section 3,
we evaluate the chunker on a German corpus (of
transcribed spontaneous speech) in terms of con-
cept error rate and slot filling accuracy. Then we
discuss related work, followed by a general dis-
cussion and the conclusion.
</bodyText>
<sectionHeader confidence="0.991796" genericHeader="introduction">
2 Incremental Chunking
</sectionHeader>
<bodyText confidence="0.99885275">
Figure 1 shows a simple example where the chun-
ker segments the input stream incrementally into
semantically relevant chunks. The figure also dis-
plays how the frame is being filled incrementally.
The chunk grammar developed for this work and
the dialogue corpus used were German, but we
give some examples in English for better readabil-
ity.
As time passes the chunker receives more and
more words from the ASR. It puts the words in a
queue and waits until the semantic content of the
accumulated words is enough for filling a slot in
the frame semantics. When this is the case the
chunk is completed and a new chunk is started.
At the same time the frame semantics is updated if
slot unification (see below) is possible and a check
</bodyText>
<figureCaption confidence="0.99859825">
Figure 1: Incremental robust sense unit construc-
tion by RUBISC.
Figure 2: Puzzle-task of the corpus used for gram-
mar building and testing.
</figureCaption>
<bodyText confidence="0.996906666666667">
whether the utterance is complete is made, so that
the chunker can be restarted for the next utterance
if necessary.
</bodyText>
<subsectionHeader confidence="0.996639">
2.1 A Regular Grammar for Semantics
</subsectionHeader>
<bodyText confidence="0.999672">
The grammar we are using for the experiments in
this paper was developed using a small corpus of
German dialogue (Siebert and Schlangen, 2008),
(Siebert, 2007). Figure 2 shows a picture of the
task that the subjects completed for this corpus.2 A
number of pentomino pieces were presented. The
pieces had to be moved into an animal-shaped fig-
ure. The subjects were shown partly completed
puzzles and had to give concise and detailed ver-
bal instructions of the next move that had to be
done. The locations inside this figure were usually
referred to in terms of body parts (move the x into
</bodyText>
<footnote confidence="0.99926625">
2For the corpus used here the difference was that the but-
ton labels were German and that the pentomino pieces were
not ordered in two rows. For better readability, we show the
picture with the English labels.
</footnote>
<figure confidence="0.997867492537313">
time
semantics:
chunk:
action:turning
name:−
end:−
object:
R
U
B
I
S
C
[turn]
erm
erm the
erm the piece
erm the piece erm
erm the piece erm the
action:turning
end:−
object:
name:−
turn
erm
the
piece
erm
the
second
in
the
upper
row
to
erm
clockwise
xpos:2
ypos:−
[erm the piece erm the second]
action:turning
end:−
object:
xpos:2
ypos:−1
action:turning
end:right
object:
in
in the
in the upper
[in the upper row]
to
to erm
[to erm clockwise]
xpos:2
ypos:−1
grammar:
action:turning −&gt;turn
object:xpos:2−&gt;the second
object:ypos:−1−&gt;the upper row
end:right−&gt;to the right|clockwise
input:
xpos:−
ypos:−
name:−
name:−
</figure>
<page confidence="0.993084">
67
</page>
<bodyText confidence="0.979530807692308">
the head of the elephant).
For such restricted tasks, a simple frame se-
mantics seems sufficient, representing the action
(grasping, movement, flipping or turning of an ob-
ject), the object that is involved, and where or
in which position the object will end up. In our
current grammar implementation the object can
be described with three attributes: name is the
name of the object. In our domain, the objects are
pentomino-pieces (i.e., geometrical forms that can
be built out of five squares) which have traditional
letter names such as x or w; the grammar maps
other descriptions such as cross or plus to such
canonical names. A piece can also be described
by its current position, as in the lower piece in
the third column. This is covered by the attributes
xpos and ypos demarking the x-position and y-
position of a piece. The x- or y-position can
be a positive or negative number, depending on
whether the description counts from left or right,
respectively.
The possible slots must be defined in the gram-
mar file in the following format:
@:action
@:entity:name
@:entity:xpos
@:entity:ypos
@:end
(That is: definition marker @:level
1: (optional) level 2.)
The position where or in which the piece ends
up could also be coded as a complex entry, but for
simplicity’s sake (in the data used for evaluation,
we have a very limited set of end positions that
would each be described by just one attribute re-
spectively), we restrict ourselves to a simple entry
called end which takes the value of a body part
(head, back, leg1 etc.) in the case of movement,
and the value of a direction or end position hor-
izontal, vertical, right, left in the case of a turn-
ing or flipping action. It will be (according to
our current grammar) set to empty in the case of a
grasping action, because grasping does not specify
an end position. This will also become important
later, when unification comes into play. Figure 3
shows a part of the German grammar used with
approximate translations (in curly brackets) of the
right-hand side into English. The English parts in
curly brackets is meta-notation and not part of the
grammar file. Note that one surface string can de-
termine the value of more than one semantic slot.
The grammar used in the experiments in this paper
</bodyText>
<equation confidence="0.8995322">
action:grasping,end:empty -&gt; nimm|nehme
{take}
action:turning -&gt; drehe? {turn}
action:flipping -&gt; spieg(le|el) {flip}
action:movement -&gt; bewegt {moved}
action:turning -&gt; gedreht {turned}
entity:name:x -&gt; kreuz|plus|((das|ein) x)
{cross|pluss|((the|an) x)}
entity:name:w -&gt; treppe|((das|ein) w$)
{staircase|(the|a) w}
entity:name:w -&gt; (das|ein) m$
{(the|an) m}
entity:name:z -&gt; (das|ein) z$
{(the|a) z}
end:head -&gt; (in|an) den kopf
{(on|in) the head}
end:leg2 -&gt; ins? das (hinterbein|hintere
bein|rechte bein|zweites bein) {in the hindleg|
back leg|right leg |second leg}
entity:ypos:lower -&gt; der (unteren|zweiten)
reihe {(lower|second) row}
entity:xpos:1 -&gt; das erste {the first}
entity:ypos:-1 -&gt; das letzte {the last}
end:horizontal,action:flipping -&gt; horizontal
{horizontally}
</equation>
<figureCaption confidence="0.993085333333333">
Figure 3: Fragment of the grammar file used in
the experiments (with English translations of the
patterns for illustration only).
</figureCaption>
<bodyText confidence="0.555295">
had 97 rules.
</bodyText>
<subsectionHeader confidence="0.991682">
2.2 Unification
</subsectionHeader>
<bodyText confidence="0.999959470588235">
Unification is an important feature of RUBISC
for handling aspects of long-distance dependen-
cies and preventing wrong semantic representa-
tions. Unification enables a form of ‘semantic
specification’ of verb arguments, avoiding that the
wrong arguments are combined with a given verb.
It also makes possible that rules can check for the
value of other slots and hence possibly become
inapplicable. The verb move, for instance, en-
sures that action is set to movement. For the ut-
terance schieb das ¨ah das horizontal ¨ah liegt ins
Vorderbein (move that uh which is horizontal into
the front leg). The action-slot will be filled
with movement but the end-slot remains empty
because horizontal as an end fits only with a flip-
ping action, and so is ignored here. Figure 4 illus-
trates how the slot unification mechanism works.
</bodyText>
<subsectionHeader confidence="0.984863">
2.3 Robustness
</subsectionHeader>
<bodyText confidence="0.9989856">
The chunker meets various robustness require-
ments to a high degree. First, pronunciation vari-
ants can be taken account of in the grammar in
a very flexible way, because the surface string or
terminal symbols can be expressed through regu-
</bodyText>
<page confidence="0.998005">
68
</page>
<figureCaption confidence="0.9923625">
Figure 4: Example of slot unification and failure
of unification.
</figureCaption>
<bodyText confidence="0.999931190476191">
lar expression patterns. move in German for in-
stance can be pronounced with or without a final
-e as bewege or beweg. flip (spiegle can be pro-
nounced with or without -el-inversion at the end.
Note, that this is due to the performance of speak-
ers in our corpus and does not necessarily reflect
German grammar rules. A system, however, needs
to be able to cope with performance-based varia-
tions.
Disfluencies are handled through how the chun-
ker constructs chunks as sense units. First, the
chunker only searches for relevant information in
a chunk. Irrelevant information such as an initial
uh in uh second row is put in the queue, but ig-
nored as the chunker picks only second row as the
semantically relevant part. Furthermore the chun-
ker provides a mechanism that allows it to jump
over words, so that second row will be found in
the second uh row and the cross will be found in
the strange cross, where strange is an unknown
word.
</bodyText>
<subsectionHeader confidence="0.901272">
2.4 Incrementality
</subsectionHeader>
<bodyText confidence="0.9999652">
One of the main features of RUBISC is its incre-
mentality. It can receive one word at a time and
extract semantic structure from it. Incrementality
is not strict here in the sense of (Nivre, 2004), be-
cause sometimes more than one word is needed
before parts of the frame are constructed and out-
put: into the right, for instance, needs to wait for a
word like leg that completes the chunk. We don’t
necessarily consider this a disadvantage, though,
as our chunks closely correlate to the minimal bits
of information that can usefully be reacted to. In
our corpus the first slot gets on average filled after
3.5 words (disregarding examples where no slots
are filled). The average utterance is 12.4 words
long.
</bodyText>
<subsectionHeader confidence="0.845436">
2.5 End-of-Sentence Detection
</subsectionHeader>
<bodyText confidence="0.99996575">
An incremental parser in a dialogue system needs
to know when to stop processing a sentence and
when to start the next one. This can be done by
using prosodic and syntactic information (Atterer
et al., 2008) or by checking whether a syntactic
S-node is complete. Since RUBISC builds sense
units, the completeness of an utterance can be de-
fined as semantic-pragmatic completeness, i.e. by
a certain number of slots that must be filled. In our
domain, for instance, it makes sense to restart the
chunker when the action and end slot and either
the name slot or the two position slots are filled.
</bodyText>
<subsectionHeader confidence="0.805938">
2.6 History
</subsectionHeader>
<bodyText confidence="0.999908111111111">
The chunker keeps a history of the states of the
frames. It is able to go back to a previous state
when the incremental speech recognition revokes
a word hypothesis. As an example consider the
current word hypothesis to be the L. The slot en-
tity name will be filled with l. Then the speech
recognition decides to change the hypothesis into
the elephant. This results in clearing the slot for
entity name again.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99998675">
The sense unit chunker was evaluated in terms of
how well it performed in slot filling on an unseen
part of our corpus. This corpus comes annotated
with utterance boundaries. 500 of these utterances
were manually labelled in terms of the semantic
slots defined in the grammar. The annotators were
not involved in the construction of the chunker or
grammar. The annotation guidelines detailed the
possible values for each slot. The entity names
had to be filled in with the letter names of the
pieces, the end slot with body parts or right, left,
horizontal etc., and the position slots with posi-
tive and negative numbers.3 The chunker was then
run on 400 of these utterances and the slot values
were compared with the annotated frames. 100
of the labelled utterances and 50 additional utter-
</bodyText>
<footnote confidence="0.993146666666667">
3In a small fraction (21) of the 500 cases an utterance
actually contained 2 statements that were combined with
und/and. In these cases the second statement was neglected.
</footnote>
<figure confidence="0.99732340625">
Frame:
unification component:
[schieb]
unify frame with
...
liegt
liegt ins
[liegt ins Vorderbein]
unify frame with
[end:leg1]
action:movement
end:leg1
...
time:
action:−
end:−
[action:movement]
−&gt;unification success
action:movement
end:−
...
:
das
das mh
[das mh horizontal]
unify frame with
action:flipping
end:horizontal
−&gt;unification failed:
action:movement
end:−
Input:
</figure>
<page confidence="0.996389">
69
</page>
<bodyText confidence="0.9985235">
ances were used by the author for developing the
grammar.
We examined the following evaluation mea-
sures:
</bodyText>
<listItem confidence="0.9977626">
• the concept error (concept err) rate (percentage
of wrong frames)
• the percentage of complete frames that were
correct (frames corr)
• the percentage of slots that were correct
• the percentage of action slots correct
• the percentage of end slots correct
• the percentage of object:name slots correct
• the percentage of object:xpos slots correct
• the percentage of object:ypos slots correct
</listItem>
<bodyText confidence="0.999950880952381">
The results are shown in Table 1. We used
a very simple baseline: a system that does not
fill any slots. This strategy still gets 17% of
the frames right, because some utterances do
not contain any real content. For the sentence
Also das ist recht schwer (Trans: That’s quite
difficult.), for instance, the gold standard seman-
tic representation would be: {action:None,
end:None, object:{xpos:None, name:None,
ypos:None}}. As the baseline ‘system’ always
returns the empty frame, it scores perfectly for
this example sentence. We are aware that this
appears to be a very easy baseline. However, for
some slots, such as the xpos and ypos slots it still
turned out to be quite hard to beat this baseline,
as wrong entries were common for those slots.
The chunker achieves a frame accuracy of 54.5%
and an overall slot filling accuracy of 86.80%
(compared to 17% and 64.3% baseline). Of the
individual slots the action slot was the one that
improved the most. The position slots were the
only ones to deteriorate. As 17% of our utterances
did not contain any relevant content, i.e. the frame
was completely empty, we repeated the evaluation
without these irrelevant data. The results are
shown in brackets in the table.
To check the impact of the unification mecha-
nism, we performed another evaluation with this
mechanism turned off, i.e. slots are always filled
when they are empty without regarding other slots.
In the second step in Figure 4, the end slot would
hence be filled. This resulted in a decline in per-
formance as can also be seen in Table 1. We also
turned off robustness features to test for their im-
pact. Surprisingly, turning off the skipping of one
word within a string specified by a grammar rule
(as in to erm clockwise), did not have an effect on
the results on our corpus. When we also turn off
allowing initial material (erm the piece), however,
performance drops considerably.
We also tested a variant of the system RUBISC-
o (for RUBISC-overlap) which considers overlap-
ping chunks: Take the third piece will result in
xpos:3 for the original chunker, even if the utter-
ance is continued with from the right. RUBISC-o
also considers the previous chunk the third piece
for the search of a surface representation. In this
case, it overwrites 3 with -3. In general, this be-
haviour improves the results.4
To allow a comparison with other work that re-
ports recall and precision as measures, we also
computed those values for RUBISC: for our test
corpus recall was 83.47% and precision was
79.69% (F-score 81.54). A direct comparison with
other systems is of course not possible, because
the tasks and data are different. Nevertheless, the
numbers allow an approximate feel of how well
the system performs.
To get an even better idea of the performance,
we let a second annotator label the data we tested
on; inter-annotator agreement is given in Table 1.
The accuracy for most slots is around 90% agree-
ment beween annotators. The concept error rate
is 32.25%. We also examined 50 utterances of the
test corpus for an error analysis. The largest part of
the errors was due to vocabulary restrictions or re-
strictions in the regular expressions: subjects used
names for pieces or body parts or even verbs which
had not been seen or considered during grammar
development. As our rules for end positions con-
tained pronouns like (into the back), they were
too restricted for some description variants (such
that it touches the back). Another problem that
appears is that descriptions of starting positions
can be confounded with descriptions of end po-
sitions. Sometimes subjects refer to end positions
not with body parts but with at the right side etc.
In some cases this leads to wrong entries in the
object-position slots. In some cases a full parser
might be helpful, but not always, because some
expressions are syntactically ambiguous: f¨uge das
Teil ganz rechts in das Rechteck ein. (put the piece
on the right into the square/put the piece into the
square on the right.) A minority of errors was also
</bodyText>
<footnote confidence="0.9977036">
4Testing significance, there is a significant difference be-
tween RUBISC and the baseline, and RUBISC and RIBISC
w/o rob (for all measures except xpos and ypos). The other
variants show no significance compared with RUBISC but
clear tendencies in the directions described above.
</footnote>
<page confidence="0.990984">
70
</page>
<table confidence="0.999945777777778">
baseline RUBISC w/o unif w/o rob RUBISC-o i-annotator
concept err 83.0 (100) 45.5 (44.6) 49.5 (49.7) 73.3 (85.5) 43.3 (42.8) 32.3 (35.5)
frames corr 17.0 (0) 54.5 (55.4) 50.3 (50.3) 26.8 (14.5) 56.8 (57.2) 67.8 (64.5)
slots corr 64.3 (57.0) 86.8 (87.2) 84.6 (84.5) 78.8 (74.9) 87.6 (87.6) 92.1 (91.5)
action corr 27.8 (13.0) 90.3 (92.2) 85.8 (86.7) 64.3 (57.5) 89.8 (90.7) 89.0 (88.6)
end corr 68.0 (61.4) 85.8 (87.3) 81.0 (81.6) 73.8 (69.0) 85.5 (87.0) 95.8 (95.1)
name corr 48.8 (38.3) 86.3 (88.3) 84.5 (86.1) 79.0 (76.2) 86.5 (88.0) 86.8 (85.8)
xpos corr 87.5 (84.9) 83.0 (80.7) 83.0 (80.7) 86.5 (83.7) 85.5 (83.4) 94.5 (94.0)
ypos corr 89.5 (87.3) 88.8 (87.3) 88.8 (87.3) 90.3 (88.3) 90.5 (88.9) 94.5 (94.0)
</table>
<tableCaption confidence="0.725754">
Table 1: Evaluation results (in %) for RUBISC in comparison with the baseline, RUBISC without uni-
fication mechanism (w/o unif), without robustness (w/o rob), RUBISC with overlap (RUBISC-o), and
inter-annotator aggreement (i-annotator). See the text for more information.
</tableCaption>
<bodyText confidence="0.989527666666667">
due to complex descriptions (the damaged t where
the right part has dropped downwards – referring
to the f), transcription errors (recht statt rechts) etc.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.993138966666667">
Slot filling is used in dialogue systems such as
the Ravenclaw-Olympus system5, but the slots are
filled by using output from a chart parser (Ward,
2008). The idea is similar in that word strings are
mapped onto semantic frames. A filled slot, how-
ever, does not influence other slots via unification
as in our framework, nor can the system deal with
incrementality. This is also the main difference
to systems such as Regulus (Rayner et al., 2006).
Our unification is carried out on filled slots and in
an incremental fashion. It is not directly specified
in our grammar formalism. The chunker rather
checks whether slot entries suggested by various
independent grammar rules are unifiable.
Even though not incremental either, the ap-
proach by (Milward, 2000) is similar in that it can
pick information from various parts of an utter-
ance; for example, it can extract the arrival time
from sentences like I’d like to arrive at York now
let’s see yes at 3pm. It builds a semantic chart us-
ing a Categorial grammar. The entries of this chart
are then mapped into slots. A number of settings
are compared and evaluated using recall and preci-
sion measures. The setting with the highest recall
(52%) achieves a precision of 79%. The setting
with the highest precision (96%) a recall of 22%.
These are F-scores of 62.7 and 35.8 respectively.
(Aist, 2006) incrementally identifies what they
call ‘pragmatic fragments’, which resemble the
sense units produced in this paper. However, their
</bodyText>
<footnote confidence="0.891871">
5http://www.ravenclaw-olympus.org/
</footnote>
<bodyText confidence="0.999886342857143">
system is provided with syntactic labels and the
idea is to pass those on to a parser (this part ap-
pears to not be implemented yet). No evaluation is
given.
(Zechner, 1998) also builds frame representa-
tions. Contrary to our approach, semantic infor-
mation is extracted in a second step after syntac-
tic chunks have been defined. The approach does
not address the issue of end of sentence-detection,
and also differs in that it was designed for use with
unrestricted domains and hence requires resources
such as WordNet (Miller et al., 1993). Depend-
ing on the WordNet output, usually more than one
frame representation is built. In an evaluation, in
21.4% of the cases one of the frames found is cor-
rect. Other approaches like (Rose, 2000) also need
lexicons or similar resources.
(Helbig and Hartrumpf, 1997) developed an in-
cremental word-oriented parser for German that
uses the notion of semantic kernels. This idea
is similar in that increments correspond to con-
stituents that have already been understood se-
mantically. The parser was later on mainly used
for question answering systems and, even though
strongly semantically oriented, places more em-
phasis on syntactic and morphological analysis
and less on robustness than our approach. It
uses quite complex representations in the form of
multi-layered extended semantic networks.
Finally, speech grammars such as JSFG6 are
similar in that recognition patterns for slots like
’action’ are defined via regular patterns. The main
differences are non-incrementality and that the re-
sult of employing the grammar is a legal sequential
string for each individual slot, while our grammar
</bodyText>
<footnote confidence="0.67439">
6java.sun.com/products/java-media/
speech/forDevelopers/JSGF/
</footnote>
<page confidence="0.99804">
71
</page>
<bodyText confidence="0.9984462">
also encodes, what is a legal (distributed) combi-
nation of slot entries.
This feature would allow the grammar writer to
specify that end:right requires a turning action or
a flipping action.
</bodyText>
<sectionHeader confidence="0.991562" genericHeader="discussions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999986">
The RUBISC chunker presented here is
not the first NLU component that is robust
against unknown words or structures, or non-
grammaticalities and disfluencies in the input, nor
the first that works incrementally, or chunk-based,
or focusses predominantly on semantic content
instead of syntactic structure. But we believe that
it is the first that is all of this combined, and that
the combination of these features provides an
advantage—at least for the domains that we are
working on. The novel combination of unification
and incrementality has the potential to handle
more phenomena than simple key word spotting.
Consider the sentence: Do not take the piece that
looks like an s, rather the one that looks like a w.
The idea is to introduce a negation slot or flag,
that will be set when a negation occurs. nicht das
s (not the s) will trigger the flag to be set while at
the same time the name slot is filled with s. This
negation slot could then trigger a switch of the
mode of integration of new semantic information
from unification to overwriting. We will test this
in future work.
One of the main restrictions of our approach is
that the grammar is strongly word-oriented and
does not abstract over syntactic categories. Its
expressive power is thus limited and some extra
coding work might be necessary due to the lack
of generalization. However, we feel that this is
mediated by the simplicity of the grammar for-
malism. A grammar for a restricted domain (and
the approach is mainly aiming at such domains)
like ours can be developed within a short time
and its limited size also restricts the extra cod-
ing work. Another possible objection to our ap-
proach is that handcrafting grammars like ours is
costly and to some extent arbitrary. However, for
a small specialized vocabulary as is typical for
many dialogue systems, we believe that our ap-
proach can lead to a good fast-running system in a
short developing time due to the simplicity of the
grammar formalism and algorithm, which makes
it easier to handle than systems that use large lexi-
cal resources for complexer domains (e.g. tutoring
systems). Other future directions are to expand
the unification mechanism and grammar formal-
ism such that alternatives for slots are possible.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995388888889">
We presented a novel framework for chunking.
The main new ideas are that of incremental chunk-
ing and chunking by sense units, where the rela-
tionship between chunks is established via a uni-
fication mechanism instead of syntactic bounds,
as in a full parsing approach. This mechanism
is shown to have advantages over simple keyword
spotting. The approach is suitable for online end-
of-sentence detection and can handle revised word
hypotheses. It is thus suitable for use in a spoken
dialogue system which aims at incrementality and
responsiveness. Nevertheless it can also be used
for other NLP applications. It can be used in an
incremental setting, but also for non-incremental
tasks. The grammar format is easy to grasp, and
the user can specify the slots he wants to be filled.
In an evaluation it achieved a concept error rate of
43.25% compared to a simple baseline of 83%.
</bodyText>
<sectionHeader confidence="0.998247" genericHeader="acknowledgments">
7 Acknowledgement
</sectionHeader>
<bodyText confidence="0.99933475">
This work was funded by the DFG Emmy-Noether
grant SCHL845/3-1. Many thanks to Ewan Klein
for valuable comments. All errors are of course
ours.
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9972386">
Steven Abney. 1991. Parsing by chunks. In Principle-
based Parsing: Computation and Psycholinguistics,
volume 44 of Studies in Linguistics and Philosophy.
Kluwer.
Gregory Aist, James Allen, Ellen Campana, Car-
los Gomez Gallo, Scott Stoness, Mary Swift, and
Michael K. Tanenhaus. 2007. Incremental under-
standing in human-computer dialogue and experi-
mental evidence for advantages over nonincremental
methods. In Decalog 2007, Trento, Italy.
Gregory S. Aist. 2006. Incrementally segment-
ing incoming speech into pragmatic fragments. In
The Third Midwest Computational Linguistics Col-
loquium (MCLC-2006), Urbana, USA.
Michaela Atterer, Timo Baumann, and David
Schlangen. 2008. Towards incremental end-
of-utterance detection in dialogue systems. In
Proceedings of Coling 2008, Manchester, UK.
Hermann Helbig and Sven Hartrumpf. 1997. Word
class functions for syntactic-semantic analysis. In
</reference>
<page confidence="0.972294">
72
</page>
<reference confidence="0.999460787878788">
Proceedings of the 2nd International Conference on
Recent Advances in Natural Language Processing
(RANLP’97).
I. Lewin, R. Becket, J. Boye, D. Carter, M. Rayner, and
M. Wiren. 1999. Language processing for spoken
dialogue systems: is shallow parsing enough? In
Accessing Information in Spoken Audio: Proceed-
ings of ESCA ETRW Workshop, Cambridge, USA.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1993.
Five papers on wordnet. Technical report, Princeton
University.
David Milward. 2000. Distributing representation for
robust interpretation of dialogue utterances. In Pro-
ceedings ofACL 2000, pages 133–141.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Frank Keller, Stephen
Clark, Matthew Crocker, and Mark Steedman, edi-
tors, Proceedings of the ACL Workshop Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50–57, Barcelona, Spain, July. Asso-
ciation for Computational Linguistics.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Carolyn P. Rose. 2000. A framework for robust se-
mantic interpretation. In Procs ofNACL.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings ofACL, Prague, Czech Re-
public.
E. Selkirk. 1984. Phonology and Syntax. The rela-
tion between sound and structure. MIT Press, Cam-
bridge, USA.
Alexander Siebert and David Schlangen. 2008. A sim-
ple method for resolution of definite reference in a
shared visual context. In Procs of SIGdial, Colum-
bus, Ohio.
Alexander Siebert. 2007. Maschinelles Lernen
der Bedeutung referenzierender und relationaler
Ausdr¨ucke in einem Brettspieldialog. Diploma The-
sis, University of Potsdam.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental dialogue processing in a micro-domain. In
Proceedings of EACL 2009, Athens, Greece, April.
Gabriel Skantze. 2007. Error Handling in Spoken Di-
alogue Systems. Ph.D. thesis, KTH, Stockholm.
Scott C. Stoness, Joel Tetreault, and James Allen.
2004. Incremental parsing with reference inter-
action. In Frank Keller, Stephen Clark, Matthew
Crocker, and Mark Steedman, editors, Proceedings
of the ACL Workshop Incremental Parsing: Bring-
ing Engineering and Cognition Together, Barcelona,
Spain, July.
David R. Traum, Lenhart K. Schubert, Massimo Poe-
sio, Nathaniel G. Martin, Marc Light, Chung Hee
Hwang, P. Heeman, George Ferguson, and James
Allen. 1996. Knowledge representation in the
trains-93 conversation system. International Jour-
nal of Expert Systems, 9(1):173–223.
Wayne H. Ward. 2008. The phoenix parser user man-
ual. http://cslr.colorado.edu/ whw/phoenix/phoenix-
manual.htm.
Klaus Zechner. 1998. Automatic construction of
frame representations for spontaneous speech in un-
restricted domains. In Proceedings of COLING-
ACL 1998, Montreal, Canada.
</reference>
<page confidence="0.999298">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.897840">
<title confidence="0.99875">RUBISC a Robust Unification-Based Incremental Semantic Chunker</title>
<author confidence="0.999952">Michaela Atterer David Schlangen</author>
<affiliation confidence="0.9990835">Department for Linguistics Department for Linguistics University of Potsdam University of Potsdam</affiliation>
<abstract confidence="0.994291894736842">We present RUBISC, a new incremental chunker that can perform incremental slot filling and revising as it receives a stream of words. Slot values can influence each other via a unification mechanism. Chunks correspond to sense units, and end-of-sentence detection is done incrementally based on a notion of semantic/pragmatic completeness. One of RU- BISC’s main fields of application is in dialogue systems where it can contribute to responsiveness and hence naturalness, because it can provide a partial or complete semantics of an utterance while the speaker is still speaking. The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>In Principlebased Parsing: Computation and Psycholinguistics,</booktitle>
<volume>44</volume>
<publisher>Kluwer.</publisher>
<contexts>
<context position="2906" citStr="Abney, 1991" startWordPosition="445" endWordPosition="446"> deep parsing in dialogue systems also indicates that shallow methods can be superior to deep parsing (Lewin et al., 1999). The question addressed in this paper is how to construct a chunker that works incrementally and robustly and builds the semantics required in a dialogue system. In our framework chunks are built according to the semantic information they contain while syntactic structure itself is less important. This approach is inspired by Selkirk’s sense units (Selkirk, 1984). She claims such units to be relevant for prosodic structure and different to syntactic structure. Similarly, (Abney, 1991) describes some characteristics of chunks as follows—properties which also make them seem to be useful units to be considered in spoken dialogue systems: “when I read a sentence, I read it a chunk at a time. [...] These chunks correspond in some way to prosodic patterns. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. By contrast, the relationships between chunks are mediated more by lexical selection 1cf. The incremental parser in (Skantze, 2007) can jump ove</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Principlebased Parsing: Computation and Psycholinguistics, volume 44 of Studies in Linguistics and Philosophy. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Aist</author>
<author>James Allen</author>
<author>Ellen Campana</author>
<author>Carlos Gomez Gallo</author>
<author>Scott Stoness</author>
<author>Mary Swift</author>
<author>Michael K Tanenhaus</author>
</authors>
<title>Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods. In Decalog</title>
<date>2007</date>
<location>Trento, Italy.</location>
<contexts>
<context position="1347" citStr="Aist et al., 2007" startWordPosition="203" endWordPosition="206">ecause it can provide a partial or complete semantics of an utterance while the speaker is still speaking. The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5. 1 Introduction Real-time NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitation</context>
</contexts>
<marker>Aist, Allen, Campana, Gallo, Stoness, Swift, Tanenhaus, 2007</marker>
<rawString>Gregory Aist, James Allen, Ellen Campana, Carlos Gomez Gallo, Scott Stoness, Mary Swift, and Michael K. Tanenhaus. 2007. Incremental understanding in human-computer dialogue and experimental evidence for advantages over nonincremental methods. In Decalog 2007, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory S Aist</author>
</authors>
<title>Incrementally segmenting incoming speech into pragmatic fragments.</title>
<date>2006</date>
<booktitle>In The Third Midwest Computational Linguistics Colloquium (MCLC-2006),</booktitle>
<location>Urbana, USA.</location>
<contexts>
<context position="23447" citStr="Aist, 2006" startWordPosition="3861" endWordPosition="3862">roach by (Milward, 2000) is similar in that it can pick information from various parts of an utterance; for example, it can extract the arrival time from sentences like I’d like to arrive at York now let’s see yes at 3pm. It builds a semantic chart using a Categorial grammar. The entries of this chart are then mapped into slots. A number of settings are compared and evaluated using recall and precision measures. The setting with the highest recall (52%) achieves a precision of 79%. The setting with the highest precision (96%) a recall of 22%. These are F-scores of 62.7 and 35.8 respectively. (Aist, 2006) incrementally identifies what they call ‘pragmatic fragments’, which resemble the sense units produced in this paper. However, their 5http://www.ravenclaw-olympus.org/ system is provided with syntactic labels and the idea is to pass those on to a parser (this part appears to not be implemented yet). No evaluation is given. (Zechner, 1998) also builds frame representations. Contrary to our approach, semantic information is extracted in a second step after syntactic chunks have been defined. The approach does not address the issue of end of sentence-detection, and also differs in that it was de</context>
</contexts>
<marker>Aist, 2006</marker>
<rawString>Gregory S. Aist. 2006. Incrementally segmenting incoming speech into pragmatic fragments. In The Third Midwest Computational Linguistics Colloquium (MCLC-2006), Urbana, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Atterer</author>
<author>Timo Baumann</author>
<author>David Schlangen</author>
</authors>
<title>Towards incremental endof-utterance detection in dialogue systems.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="13987" citStr="Atterer et al., 2008" startWordPosition="2277" endWordPosition="2280">ce, needs to wait for a word like leg that completes the chunk. We don’t necessarily consider this a disadvantage, though, as our chunks closely correlate to the minimal bits of information that can usefully be reacted to. In our corpus the first slot gets on average filled after 3.5 words (disregarding examples where no slots are filled). The average utterance is 12.4 words long. 2.5 End-of-Sentence Detection An incremental parser in a dialogue system needs to know when to stop processing a sentence and when to start the next one. This can be done by using prosodic and syntactic information (Atterer et al., 2008) or by checking whether a syntactic S-node is complete. Since RUBISC builds sense units, the completeness of an utterance can be defined as semantic-pragmatic completeness, i.e. by a certain number of slots that must be filled. In our domain, for instance, it makes sense to restart the chunker when the action and end slot and either the name slot or the two position slots are filled. 2.6 History The chunker keeps a history of the states of the frames. It is able to go back to a previous state when the incremental speech recognition revokes a word hypothesis. As an example consider the current </context>
</contexts>
<marker>Atterer, Baumann, Schlangen, 2008</marker>
<rawString>Michaela Atterer, Timo Baumann, and David Schlangen. 2008. Towards incremental endof-utterance detection in dialogue systems. In Proceedings of Coling 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Helbig</author>
<author>Sven Hartrumpf</author>
</authors>
<title>Word class functions for syntactic-semantic analysis.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd International Conference on Recent Advances in Natural Language Processing (RANLP’97).</booktitle>
<contexts>
<context position="24422" citStr="Helbig and Hartrumpf, 1997" startWordPosition="4015" endWordPosition="4018">uilds frame representations. Contrary to our approach, semantic information is extracted in a second step after syntactic chunks have been defined. The approach does not address the issue of end of sentence-detection, and also differs in that it was designed for use with unrestricted domains and hence requires resources such as WordNet (Miller et al., 1993). Depending on the WordNet output, usually more than one frame representation is built. In an evaluation, in 21.4% of the cases one of the frames found is correct. Other approaches like (Rose, 2000) also need lexicons or similar resources. (Helbig and Hartrumpf, 1997) developed an incremental word-oriented parser for German that uses the notion of semantic kernels. This idea is similar in that increments correspond to constituents that have already been understood semantically. The parser was later on mainly used for question answering systems and, even though strongly semantically oriented, places more emphasis on syntactic and morphological analysis and less on robustness than our approach. It uses quite complex representations in the form of multi-layered extended semantic networks. Finally, speech grammars such as JSFG6 are similar in that recognition </context>
</contexts>
<marker>Helbig, Hartrumpf, 1997</marker>
<rawString>Hermann Helbig and Sven Hartrumpf. 1997. Word class functions for syntactic-semantic analysis. In Proceedings of the 2nd International Conference on Recent Advances in Natural Language Processing (RANLP’97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Lewin</author>
<author>R Becket</author>
<author>J Boye</author>
<author>D Carter</author>
<author>M Rayner</author>
<author>M Wiren</author>
</authors>
<title>Language processing for spoken dialogue systems: is shallow parsing enough?</title>
<date>1999</date>
<booktitle>In Accessing Information in Spoken Audio: Proceedings of ESCA ETRW Workshop,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="2416" citStr="Lewin et al., 1999" startWordPosition="367" endWordPosition="370">m et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.1 We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking approaches as alternatives to CFG or dependency parsing. Previous work that uses a combination of shallow and deep parsing in dialogue systems also indicates that shallow methods can be superior to deep parsing (Lewin et al., 1999). The question addressed in this paper is how to construct a chunker that works incrementally and robustly and builds the semantics required in a dialogue system. In our framework chunks are built according to the semantic information they contain while syntactic structure itself is less important. This approach is inspired by Selkirk’s sense units (Selkirk, 1984). She claims such units to be relevant for prosodic structure and different to syntactic structure. Similarly, (Abney, 1991) describes some characteristics of chunks as follows—properties which also make them seem to be useful units t</context>
</contexts>
<marker>Lewin, Becket, Boye, Carter, Rayner, Wiren, 1999</marker>
<rawString>I. Lewin, R. Becket, J. Boye, D. Carter, M. Rayner, and M. Wiren. 1999. Language processing for spoken dialogue systems: is shallow parsing enough? In Accessing Information in Spoken Audio: Proceedings of ESCA ETRW Workshop, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on wordnet.</title>
<date>1993</date>
<tech>Technical report,</tech>
<institution>Princeton University.</institution>
<contexts>
<context position="24154" citStr="Miller et al., 1993" startWordPosition="3970" endWordPosition="3973">e units produced in this paper. However, their 5http://www.ravenclaw-olympus.org/ system is provided with syntactic labels and the idea is to pass those on to a parser (this part appears to not be implemented yet). No evaluation is given. (Zechner, 1998) also builds frame representations. Contrary to our approach, semantic information is extracted in a second step after syntactic chunks have been defined. The approach does not address the issue of end of sentence-detection, and also differs in that it was designed for use with unrestricted domains and hence requires resources such as WordNet (Miller et al., 1993). Depending on the WordNet output, usually more than one frame representation is built. In an evaluation, in 21.4% of the cases one of the frames found is correct. Other approaches like (Rose, 2000) also need lexicons or similar resources. (Helbig and Hartrumpf, 1997) developed an incremental word-oriented parser for German that uses the notion of semantic kernels. This idea is similar in that increments correspond to constituents that have already been understood semantically. The parser was later on mainly used for question answering systems and, even though strongly semantically oriented, p</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1993. Five papers on wordnet. Technical report, Princeton University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milward</author>
</authors>
<title>Distributing representation for robust interpretation of dialogue utterances.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL</booktitle>
<pages>133--141</pages>
<contexts>
<context position="22860" citStr="Milward, 2000" startWordPosition="3756" endWordPosition="3757">rd, 2008). The idea is similar in that word strings are mapped onto semantic frames. A filled slot, however, does not influence other slots via unification as in our framework, nor can the system deal with incrementality. This is also the main difference to systems such as Regulus (Rayner et al., 2006). Our unification is carried out on filled slots and in an incremental fashion. It is not directly specified in our grammar formalism. The chunker rather checks whether slot entries suggested by various independent grammar rules are unifiable. Even though not incremental either, the approach by (Milward, 2000) is similar in that it can pick information from various parts of an utterance; for example, it can extract the arrival time from sentences like I’d like to arrive at York now let’s see yes at 3pm. It builds a semantic chart using a Categorial grammar. The entries of this chart are then mapped into slots. A number of settings are compared and evaluated using recall and precision measures. The setting with the highest recall (52%) achieves a precision of 79%. The setting with the highest precision (96%) a recall of 22%. These are F-scores of 62.7 and 35.8 respectively. (Aist, 2006) incrementall</context>
</contexts>
<marker>Milward, 2000</marker>
<rawString>David Milward. 2000. Distributing representation for robust interpretation of dialogue utterances. In Proceedings ofACL 2000, pages 133–141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<editor>In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13237" citStr="Nivre, 2004" startWordPosition="2151" endWordPosition="2152"> in a chunk. Irrelevant information such as an initial uh in uh second row is put in the queue, but ignored as the chunker picks only second row as the semantically relevant part. Furthermore the chunker provides a mechanism that allows it to jump over words, so that second row will be found in the second uh row and the cross will be found in the strange cross, where strange is an unknown word. 2.4 Incrementality One of the main features of RUBISC is its incrementality. It can receive one word at a time and extract semantic structure from it. Incrementality is not strict here in the sense of (Nivre, 2004), because sometimes more than one word is needed before parts of the frame are constructed and output: into the right, for instance, needs to wait for a word like leg that completes the chunk. We don’t necessarily consider this a disadvantage, though, as our chunks closely correlate to the minimal bits of information that can usefully be reacted to. In our corpus the first slot gets on average filled after 3.5 words (disregarding examples where no slots are filled). The average utterance is 12.4 words long. 2.5 End-of-Sentence Detection An incremental parser in a dialogue system needs to know </context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner</author>
<author>B A Hockey</author>
<author>P Bouillon</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<date>2006</date>
<publisher>CSLI Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="22549" citStr="Rayner et al., 2006" startWordPosition="3706" endWordPosition="3709">. due to complex descriptions (the damaged t where the right part has dropped downwards – referring to the f), transcription errors (recht statt rechts) etc. 4 Related Work Slot filling is used in dialogue systems such as the Ravenclaw-Olympus system5, but the slots are filled by using output from a chart parser (Ward, 2008). The idea is similar in that word strings are mapped onto semantic frames. A filled slot, however, does not influence other slots via unification as in our framework, nor can the system deal with incrementality. This is also the main difference to systems such as Regulus (Rayner et al., 2006). Our unification is carried out on filled slots and in an incremental fashion. It is not directly specified in our grammar formalism. The chunker rather checks whether slot entries suggested by various independent grammar rules are unifiable. Even though not incremental either, the approach by (Milward, 2000) is similar in that it can pick information from various parts of an utterance; for example, it can extract the arrival time from sentences like I’d like to arrive at York now let’s see yes at 3pm. It builds a semantic chart using a Categorial grammar. The entries of this chart are then m</context>
</contexts>
<marker>Rayner, Hockey, Bouillon, 2006</marker>
<rawString>M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn P Rose</author>
</authors>
<title>A framework for robust semantic interpretation.</title>
<date>2000</date>
<booktitle>In Procs ofNACL.</booktitle>
<contexts>
<context position="24352" citStr="Rose, 2000" startWordPosition="4007" endWordPosition="4008">d yet). No evaluation is given. (Zechner, 1998) also builds frame representations. Contrary to our approach, semantic information is extracted in a second step after syntactic chunks have been defined. The approach does not address the issue of end of sentence-detection, and also differs in that it was designed for use with unrestricted domains and hence requires resources such as WordNet (Miller et al., 1993). Depending on the WordNet output, usually more than one frame representation is built. In an evaluation, in 21.4% of the cases one of the frames found is correct. Other approaches like (Rose, 2000) also need lexicons or similar resources. (Helbig and Hartrumpf, 1997) developed an incremental word-oriented parser for German that uses the notion of semantic kernels. This idea is similar in that increments correspond to constituents that have already been understood semantically. The parser was later on mainly used for question answering systems and, even though strongly semantically oriented, places more emphasis on syntactic and morphological analysis and less on robustness than our approach. It uses quite complex representations in the form of multi-layered extended semantic networks. F</context>
</contexts>
<marker>Rose, 2000</marker>
<rawString>Carolyn P. Rose. 2000. A framework for robust semantic interpretation. In Procs ofNACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Fast unsupervised incremental parsing.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1601" citStr="Seginer, 2007" startWordPosition="243" endWordPosition="244">ime NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.1 We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking approaches as al</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Fast unsupervised incremental parsing. In Proceedings ofACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Selkirk</author>
</authors>
<title>Phonology and Syntax. The relation between sound and structure.</title>
<date>1984</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<contexts>
<context position="2782" citStr="Selkirk, 1984" startWordPosition="426" endWordPosition="427">emental chunking approaches as alternatives to CFG or dependency parsing. Previous work that uses a combination of shallow and deep parsing in dialogue systems also indicates that shallow methods can be superior to deep parsing (Lewin et al., 1999). The question addressed in this paper is how to construct a chunker that works incrementally and robustly and builds the semantics required in a dialogue system. In our framework chunks are built according to the semantic information they contain while syntactic structure itself is less important. This approach is inspired by Selkirk’s sense units (Selkirk, 1984). She claims such units to be relevant for prosodic structure and different to syntactic structure. Similarly, (Abney, 1991) describes some characteristics of chunks as follows—properties which also make them seem to be useful units to be considered in spoken dialogue systems: “when I read a sentence, I read it a chunk at a time. [...] These chunks correspond in some way to prosodic patterns. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. By contrast, the rel</context>
</contexts>
<marker>Selkirk, 1984</marker>
<rawString>E. Selkirk. 1984. Phonology and Syntax. The relation between sound and structure. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Siebert</author>
<author>David Schlangen</author>
</authors>
<title>A simple method for resolution of definite reference in a shared visual context.</title>
<date>2008</date>
<booktitle>In Procs of SIGdial,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6485" citStr="Siebert and Schlangen, 2008" startWordPosition="1035" endWordPosition="1038">frame semantics. When this is the case the chunk is completed and a new chunk is started. At the same time the frame semantics is updated if slot unification (see below) is possible and a check Figure 1: Incremental robust sense unit construction by RUBISC. Figure 2: Puzzle-task of the corpus used for grammar building and testing. whether the utterance is complete is made, so that the chunker can be restarted for the next utterance if necessary. 2.1 A Regular Grammar for Semantics The grammar we are using for the experiments in this paper was developed using a small corpus of German dialogue (Siebert and Schlangen, 2008), (Siebert, 2007). Figure 2 shows a picture of the task that the subjects completed for this corpus.2 A number of pentomino pieces were presented. The pieces had to be moved into an animal-shaped figure. The subjects were shown partly completed puzzles and had to give concise and detailed verbal instructions of the next move that had to be done. The locations inside this figure were usually referred to in terms of body parts (move the x into 2For the corpus used here the difference was that the button labels were German and that the pentomino pieces were not ordered in two rows. For better rea</context>
</contexts>
<marker>Siebert, Schlangen, 2008</marker>
<rawString>Alexander Siebert and David Schlangen. 2008. A simple method for resolution of definite reference in a shared visual context. In Procs of SIGdial, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Siebert</author>
</authors>
<title>Maschinelles Lernen der Bedeutung referenzierender und relationaler Ausdr¨ucke in einem Brettspieldialog. Diploma Thesis,</title>
<date>2007</date>
<institution>University of Potsdam.</institution>
<contexts>
<context position="6502" citStr="Siebert, 2007" startWordPosition="1039" endWordPosition="1040">he case the chunk is completed and a new chunk is started. At the same time the frame semantics is updated if slot unification (see below) is possible and a check Figure 1: Incremental robust sense unit construction by RUBISC. Figure 2: Puzzle-task of the corpus used for grammar building and testing. whether the utterance is complete is made, so that the chunker can be restarted for the next utterance if necessary. 2.1 A Regular Grammar for Semantics The grammar we are using for the experiments in this paper was developed using a small corpus of German dialogue (Siebert and Schlangen, 2008), (Siebert, 2007). Figure 2 shows a picture of the task that the subjects completed for this corpus.2 A number of pentomino pieces were presented. The pieces had to be moved into an animal-shaped figure. The subjects were shown partly completed puzzles and had to give concise and detailed verbal instructions of the next move that had to be done. The locations inside this figure were usually referred to in terms of body parts (move the x into 2For the corpus used here the difference was that the button labels were German and that the pentomino pieces were not ordered in two rows. For better readability, we show</context>
</contexts>
<marker>Siebert, 2007</marker>
<rawString>Alexander Siebert. 2007. Maschinelles Lernen der Bedeutung referenzierender und relationaler Ausdr¨ucke in einem Brettspieldialog. Diploma Thesis, University of Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
<author>David Schlangen</author>
</authors>
<title>Incremental dialogue processing in a micro-domain.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL 2009,</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="1381" citStr="Skantze and Schlangen, 2009" startWordPosition="208" endWordPosition="211">partial or complete semantics of an utterance while the speaker is still speaking. The chunker is evaluated on a German transcribed speech corpus and achieves a concept error rate of 43.3% and an F-Score of 81.5. 1 Introduction Real-time NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone A</context>
</contexts>
<marker>Skantze, Schlangen, 2009</marker>
<rawString>Gabriel Skantze and David Schlangen. 2009. Incremental dialogue processing in a micro-domain. In Proceedings of EACL 2009, Athens, Greece, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Skantze</author>
</authors>
<title>Error Handling in Spoken Dialogue Systems.</title>
<date>2007</date>
<booktitle>Ph.D. thesis, KTH,</booktitle>
<location>Stockholm.</location>
<contexts>
<context position="1790" citStr="Skantze, 2007" startWordPosition="274" endWordPosition="275">ition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.1 We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking approaches as alternatives to CFG or dependency parsing. Previous work that uses a combination of shallow and deep parsing in dialogue systems also indicates that shallow methods can be superior to deep pa</context>
<context position="3493" citStr="Skantze, 2007" startWordPosition="539" endWordPosition="540">e. Similarly, (Abney, 1991) describes some characteristics of chunks as follows—properties which also make them seem to be useful units to be considered in spoken dialogue systems: “when I read a sentence, I read it a chunk at a time. [...] These chunks correspond in some way to prosodic patterns. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. By contrast, the relationships between chunks are mediated more by lexical selection 1cf. The incremental parser in (Skantze, 2007) can jump over a configurable number of words in the input. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 66–73, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 66 than by rigid templates. [...] and the order in which chunks occur is much more flexible than the order of words within chunks.” In our approach chunks are built incrementally (one at a time) and are defined semantically (a sense unit is complete when a slot in our template or frame semantics can be filled). Ideally, in a full system, the definition</context>
</contexts>
<marker>Skantze, 2007</marker>
<rawString>Gabriel Skantze. 2007. Error Handling in Spoken Dialogue Systems. Ph.D. thesis, KTH, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Stoness</author>
<author>Joel Tetreault</author>
<author>James Allen</author>
</authors>
<title>Incremental parsing with reference interaction.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<editor>In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors,</editor>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1585" citStr="Stoness et al., 2004" startWordPosition="239" endWordPosition="242"> 1 Introduction Real-time NLP applications such as dialogue systems can profit considerably from incremental processing of language. When syntactic and semantic structure is built on-line while the speech recognition (ASR) is still working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.1 We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking </context>
</contexts>
<marker>Stoness, Tetreault, Allen, 2004</marker>
<rawString>Scott C. Stoness, Joel Tetreault, and James Allen. 2004. Incremental parsing with reference interaction. In Frank Keller, Stephen Clark, Matthew Crocker, and Mark Steedman, editors, Proceedings of the ACL Workshop Incremental Parsing: Bringing Engineering and Cognition Together, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Lenhart K Schubert</author>
<author>Massimo Poesio</author>
<author>Nathaniel G Martin</author>
<author>Marc Light</author>
<author>Chung Hee Hwang</author>
<author>P Heeman</author>
<author>George Ferguson</author>
<author>James Allen</author>
</authors>
<title>Knowledge representation in the trains-93 conversation system.</title>
<date>1996</date>
<journal>International Journal of Expert Systems,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="1812" citStr="Traum et al., 1996" startWordPosition="276" endWordPosition="279">ill working on the speech stream, unnatural silences can be avoided and the system can react in a faster and more userfriendly way. As (Aist et al., 2007) and (Skantze and Schlangen, 2009) show, such incremental systems are typically preferred by users over nonincremental systems. To achieve incrementality, most dialogue systems employ an incremental chart parser (cf. (Stoness et al., 2004; Seginer, 2007) etc.). However, most existing dialogue systems operate in very limited domains, e.g. moving objects, people, trains etc. from one place to another (cf. (Aist et al., 2007), (Skantze, 2007), (Traum et al., 1996)). The complexity of the semantic representations needed is thus limited. Moreover, user behaviour (ungrammatical sentences, hesitations, false starts) and error-prone ASR require the parsing process to be robust.1 We argue that obtaining relatively flat semantics in a limited domain while needing exigent robustness calls for investigating shallower incremental chunking approaches as alternatives to CFG or dependency parsing. Previous work that uses a combination of shallow and deep parsing in dialogue systems also indicates that shallow methods can be superior to deep parsing (Lewin et al., 1</context>
</contexts>
<marker>Traum, Schubert, Poesio, Martin, Light, Hwang, Heeman, Ferguson, Allen, 1996</marker>
<rawString>David R. Traum, Lenhart K. Schubert, Massimo Poesio, Nathaniel G. Martin, Marc Light, Chung Hee Hwang, P. Heeman, George Ferguson, and James Allen. 1996. Knowledge representation in the trains-93 conversation system. International Journal of Expert Systems, 9(1):173–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne H Ward</author>
</authors>
<title>The phoenix parser user manual.</title>
<date>2008</date>
<note>http://cslr.colorado.edu/ whw/phoenix/phoenixmanual.htm.</note>
<contexts>
<context position="22255" citStr="Ward, 2008" startWordPosition="3657" endWordPosition="3658">.9) 94.5 (94.0) Table 1: Evaluation results (in %) for RUBISC in comparison with the baseline, RUBISC without unification mechanism (w/o unif), without robustness (w/o rob), RUBISC with overlap (RUBISC-o), and inter-annotator aggreement (i-annotator). See the text for more information. due to complex descriptions (the damaged t where the right part has dropped downwards – referring to the f), transcription errors (recht statt rechts) etc. 4 Related Work Slot filling is used in dialogue systems such as the Ravenclaw-Olympus system5, but the slots are filled by using output from a chart parser (Ward, 2008). The idea is similar in that word strings are mapped onto semantic frames. A filled slot, however, does not influence other slots via unification as in our framework, nor can the system deal with incrementality. This is also the main difference to systems such as Regulus (Rayner et al., 2006). Our unification is carried out on filled slots and in an incremental fashion. It is not directly specified in our grammar formalism. The chunker rather checks whether slot entries suggested by various independent grammar rules are unifiable. Even though not incremental either, the approach by (Milward, </context>
</contexts>
<marker>Ward, 2008</marker>
<rawString>Wayne H. Ward. 2008. The phoenix parser user manual. http://cslr.colorado.edu/ whw/phoenix/phoenixmanual.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Automatic construction of frame representations for spontaneous speech in unrestricted domains.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="23788" citStr="Zechner, 1998" startWordPosition="3912" endWordPosition="3913">r of settings are compared and evaluated using recall and precision measures. The setting with the highest recall (52%) achieves a precision of 79%. The setting with the highest precision (96%) a recall of 22%. These are F-scores of 62.7 and 35.8 respectively. (Aist, 2006) incrementally identifies what they call ‘pragmatic fragments’, which resemble the sense units produced in this paper. However, their 5http://www.ravenclaw-olympus.org/ system is provided with syntactic labels and the idea is to pass those on to a parser (this part appears to not be implemented yet). No evaluation is given. (Zechner, 1998) also builds frame representations. Contrary to our approach, semantic information is extracted in a second step after syntactic chunks have been defined. The approach does not address the issue of end of sentence-detection, and also differs in that it was designed for use with unrestricted domains and hence requires resources such as WordNet (Miller et al., 1993). Depending on the WordNet output, usually more than one frame representation is built. In an evaluation, in 21.4% of the cases one of the frames found is correct. Other approaches like (Rose, 2000) also need lexicons or similar resou</context>
</contexts>
<marker>Zechner, 1998</marker>
<rawString>Klaus Zechner. 1998. Automatic construction of frame representations for spontaneous speech in unrestricted domains. In Proceedings of COLINGACL 1998, Montreal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>