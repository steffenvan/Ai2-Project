<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000208">
<title confidence="0.9962795">
Reconstructing an Indo-European Family Tree
from Non-native English texts
</title>
<author confidence="0.995202">
Ryo Nagata1 2 Edward Whittaker3
</author>
<affiliation confidence="0.946179">
1Konan University / Kobe, Japan
2LIMSI-CNRS / Orsay, France
3Inferret Limited / Northampton, England
</affiliation>
<email confidence="0.990662">
nagata-acl@hyogo-u.ac.jp, ed@inferret.co.uk
</email>
<sectionHeader confidence="0.997287" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923411764706">
Mother tongue interference is the phe-
nomenon where linguistic systems of a
mother tongue are transferred to another
language. Although there has been plenty
of work on mother tongue interference,
very little is known about how strongly
it is transferred to another language and
about what relation there is across mother
tongues. To address these questions,
this paper explores and visualizes mother
tongue interference preserved in English
texts written by Indo-European language
speakers. This paper further explores lin-
guistic features that explain why certain
relations are preserved in English writing,
and which contribute to related tasks such
as native language identification.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962948275862">
Transfer of linguistic systems of a mother tongue
to another language, namely mother tongue inter-
ference, is often observable in the writing of non-
native speakers. The reader may be able to deter-
mine the mother tongue of the writer of the fol-
lowing sentence from the underlined article error:
The alien wouldn’t use my spaceship but
the hers.
The answer would probably be French or Span-
ish; the definite article is allowed to modify pos-
sessive pronouns in these languages, and the us-
age is sometimes negatively transferred to English
writing. Researchers such as Swan and Smith
(2001), Aarts and Granger (1998), Davidsen-
Nielsen and Harder (2001), and Altenberg and
Tapper (1998) work on mother tongue interfer-
ence to reveal overused/underused words, part of
speech (POS), or grammatical items.
In contrast, very little is known about how
strongly mother tongue interference is transferred
to another language and about what relation there
is across mother tongues. At one extreme, one
could argue that it is so strongly transferred to
texts in another language that the linguistic rela-
tions between mother tongues are perfectly pre-
served in the texts. At the other extreme, one
can counter it, arguing that other features such as
non-nativeness are more influential than mother
tongue interference. One possible reason for this
is that a large part of the distinctive language sys-
tems of a mother tongue may be eliminated when
transferred to another language from a speaker’s
mother tongue. For example, Slavic languages
have a rich inflectional case system (e.g., Czech
has seven inflectional cases) whereas French does
not. However, the difference in the richness cannot
be transferred into English because English has al-
most no inflectional case system. Thus, one can-
not determine the mother tongue of a given non-
native text from the inflectional case. A similar
argument can be made about some parts of gen-
der, tense, and aspect systems. Besides, Wong and
Dras (2009) show that there are no significant dif-
ferences, between mother tongues, in the misuse
of certain syntactic features such as subject-verb
agreement that have different tendencies depend-
ing on their mother tongues. Considering these,
one could not be so sure which argument is cor-
rect. In any case, to the best of our knowledge, no
one has yet answered this question.
In view of this background, we take the first step
in addressing this question. We hypothesize that:
Hypothesis: Mother tongue interference is so
strong that the relations in a language fam-
ily are preserved in texts written in another
language.
In other words, mother tongue interference is so
strong that one can reconstruct a language fam-
</bodyText>
<page confidence="0.954633">
1137
</page>
<note confidence="0.913575">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1137–1147,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999724041666667">
ily tree from non-native texts. One of the major
contributions of this work is to reveal and visual-
ize a language family tree preserved in non-native
texts, by examining the hypothesis. This becomes
important in native language identification) which
is useful for improving grammatical error correc-
tion systems (Chodorow et al., 2010) or for pro-
viding more targeted feedback to language learn-
ers. As we will see in Sect. 6, this paper reveals
several crucial findings that contribute to improv-
ing native language identification. In addition, this
paper shows that the findings could contribute to
reconstruction of language family trees (Enright
and Kondrak, 2011; Gray and Atkinson, 2003;
Barbanc¸on et al., 2007; Batagelj et al., 1992;
Nakhleh et al., 2005), which is one of the central
tasks in historical linguistics.
The rest of this paper is structured as follows.
Sect. 2 introduces the basic approach of this work.
Sect. 3 discusses the methods in detail. Sect. 4 de-
scribes experiments conducted to investigate the
hypothesis. Sect. 5 discusses the experimental re-
sults. Sect.6 discusses implications for work in
related domains.
</bodyText>
<sectionHeader confidence="0.996664" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.996164305555556">
To examine the hypothesis, we reconstruct a
language family tree from English texts writ-
ten by non-native speakers of English whose
mother tongue is one of the Indo-European lan-
guages (Beekes, 2011; Ramat and Ramat, 2006).
If the reconstructed tree is sufficiently similar to
the original Indo-European family tree, it will sup-
port the hypothesis. If not, it suggests that some
features other than mother tongue interference are
more influential.
The approach we use for reconstructing a lan-
guage family tree is to apply agglomerative hi-
erarchical clustering (Han and Kamber, 2006) to
English texts written by non-native speakers. Re-
searchers have already performed related work
on reconstructing language family trees. For in-
stance, Kroeber and Chri´etien (1937) and Elleg˚ard
(1959) proposed statistical methods for measuring
the similarity metric between languages. More re-
cently, Batagelj et al. (1992) and Kita (1999) pro-
posed methods for reconstructing language fam-
ily trees using clustering. Among them, the
&apos;Recently, native language identification has drawn the at-
tention of NLP researchers. For instance, a shared task on
native language identification took place at an NAACL-HLT
2013 workshop.
most related method is that of Kita (1999). In
his method, a variety of languages are modeled
by their spelling systems (i.e., character-based
n-gram language models). Then, agglomera-
tive hierarchical clustering is applied to the lan-
guage models to reconstruct a language family
tree. The similarity used for clustering is based on
a divergence-like distance between two language
models that was originally proposed by Juang and
Rabiner (1985). This method is purely data-driven
and does not require human expert knowledge for
the selection of linguistic features.
Our work closely follows Kita’s work. How-
ever, it should be emphasized that there is a signif-
icant difference between the two. Kita’s work (and
other previous work) targets clustering of a variety
of languages whereas our work tries to reconstruct
a language family tree preserved in non-native En-
glish. This significant difference prevents us from
directly applying techniques in the literature to our
task. For instance, Batagelj et al. (1992) use basic
vocabularies such as belly in English and ventre in
French to measure similarity between languages.
Obviously, this does not work on our task; belly is
belly in English writing whoever writes it. Kita’s
method is also likely not to work well because all
texts in our task share the same spelling system
(i.e., English spelling). Although spelling is some-
times influenced by mother tongues, it involves a
lot more including overuse, underuse, and misuse
of lexical, grammatical, and syntactic systems.
To solve the problem, this work adopts a word-
based language model in the expectation that word
sequences reflect mother tongue interference. At
the same time, its simple application would cause
a serious side effect. It would reflect the topics
of given texts rather than mother tongue interfer-
ence. Unfortunately, there exists no such English
corpus that covers a variety of language speakers
with uniform topics; moreover the availability of
non-native corpora is still somewhat limited. This
also means that available non-native corpora may
be too small to train reliable word-based language
models. The next section describes two methods
(language model-based and vector-based), which
address these problems.
</bodyText>
<sectionHeader confidence="0.999819" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.999709">
3.1 Language Model-based Method
</subsectionHeader>
<bodyText confidence="0.9957555">
To begin with, let us define the following symbols
used in the methods. Let Di be a set of English
</bodyText>
<page confidence="0.989393">
1138
</page>
<bodyText confidence="0.999380705882353">
texts where i denotes a mother tongue i. Similarly,
let Mi be a language model trained using Di.
To solve the problems pointed out in Sect. 2, we
use an n-gram language model based on a mixture
of word and POS tokens instead of a simple word-
based language model. In this language model,
content words in n-grams are replaced with their
corresponding POS tags. This greatly decreases
the influence of the topics of texts, as desired. It
also decreases the number of parameters in the
language model.
To build the language model, the following
three preprocessing steps are applied to Di. First,
texts in Di are split into sentences. Second, each
sentence is tokenized, POS-tagged, and mapped
entirely to lowercase. For instance, the first ex-
ample sentence in Sect. 1 would give:
</bodyText>
<equation confidence="0.532916666666667">
the/DT alien/NN would/MD not/RB
use/VB my/PRP$ spaceship/NN but/CC
the/DT hers/PRP ./.
</equation>
<bodyText confidence="0.927245677419355">
Finally, words are replaced with their correspond-
ing POS tags; for the following words, word to-
kens are used as their corresponding POS tags:
coordinating conjunctions, determiners, preposi-
tions, modals, predeterminers, possessives, pro-
nouns, question adverbs. Also, proper nouns are
treated as common nouns. At this point, the spe-
cial POS tags BOS and EOS are added at the begin-
ning and end of each sentence, respectively. For
instance, the above example would result in the
following word/POS sequence:
BOS the NN would RB VB my NN but
the hers . EOS
Note that the content of the original sentence is far
from clear while reflecting mother tongue interfer-
ence, especially in the hers.
Now, the language model Mi can be built from
Di. We set n = 3 (i.e., trigram language model)
following Kita’s work and use Kneser-Ney (KN)
smoothing (Kneser and Ney, 1995) to estimate its
conditional probabilities.
With Mi and Di, we can naturally apply Kita’s
method to our task. The clustering algorithm used
is agglomerative hierarchical clustering with the
average linkage method. The distance between
two language models is measured as follows. The
zIt is not a distance in a mathematical sense. However,
we will use the term distance following the convention in the
literature.
probability that Mi generates Di is calculated by
Pr(Di|Mi). Note that
</bodyText>
<equation confidence="0.9969904">
Pr(Di|Mi) ≈
Pr(w1,i) Pr(w2,i|w1,i)
|Di|
× ∏ Pr(wt,i|wt−2,i,wt−1,i) (1)
t=3
</equation>
<bodyText confidence="0.999947">
where wt,i and |Di |denote the tth token in Di and
the number of tokens in Di, respectively, since we
use the trigram language model. Then, the dis-
tance from Mi to Mj is defined by
</bodyText>
<equation confidence="0.9990805">
1 Pr(Dj|Mj)
d(Mi → Mj) = |Dj |log Pr(Dj|Mi) . (2)
</equation>
<bodyText confidence="0.999772333333333">
In other words, the distance is determined based
on the ratio of the probabilities that each lan-
guage model generates the language data. Because
d(Mi → Mj) and d(Mj → Mi) are not symmet-
rical, we define the distance between Mi and Mj
to be their average:
</bodyText>
<equation confidence="0.989263333333333">
d(Mi → Mj)+d(Mj → Mi)
d(Mi, Mj)= . (3)
2
</equation>
<bodyText confidence="0.968369166666667">
Equation (3) is used to calculate the distance be-
tween two language models for clustering.
To sum up, the procedure of the language fam-
ily tree construction method is as follows: (i) Pre-
process each Di; (ii) Build Mi from Di; (iii) Cal-
culate the distances between the language models;
</bodyText>
<listItem confidence="0.948496">
(iv) Cluster the language data using the distances;
(v) Output the result as a language family tree.
</listItem>
<subsectionHeader confidence="0.997383">
3.2 Vector-based Method
</subsectionHeader>
<bodyText confidence="0.999886857142857">
We also examine a vector-based method for lan-
guage family tree reconstruction. As we will see
in Sect. 5, this method allows us to interpret clus-
tering results more easily than with the language
model-based method while both result in similar
language family trees.
In this method, Di is modeled by a vector. The
vector is constructed based on the relative frequen-
cies of trigrams. As a consequence, the distance
is naturally defined by the Euclidean distance be-
tween two vectors. The clustering procedure is the
same as for the language model-based method ex-
cept that Mi is vector-based and that the distance
metric is Euclidean.
</bodyText>
<page confidence="0.992435">
1139
</page>
<sectionHeader confidence="0.999459" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999939">
We selected the ICLE corpus v.2 (Granger et al.,
2009) as the target language data. It consists of
English essays written by a wide variety of non-
native speakers of English. Among them, the 11
shown in Table 1 are of Indo-European languages.
Accordingly, we selected the subcorpora of the 11
languages in the experiments. Before the exper-
iments, we preprocessed the corpus data to con-
trol the experimental conditions. Because some of
the writers had more than one native language, we
excluded essays that did not meet the following
three conditions: (i) the writer has only one na-
tive language; (ii) the writer has only one language
at home; (iii) the two languages in (i) and (ii) are
the same as the native language of the subcorpus
to which the essay belongs3. After the selection,
markup tags such as essay IDs were removed from
the corpus data. Also, the symbols ‘ and ’ were
unified into ’4. For reference, we also used na-
tive English (British and American university stu-
dents’ essays in the LOCNESS corpus5) and two
sets of Japanese English (ICLE and the NICE cor-
pus (Sugiura et al., 2007)). Table 1 shows the
statistics on the corpus data.
Performance of POS tagging is an important
factor in our methods because they are based on
word/POS sequences. Existing POS taggers might
not perform well on non-native English texts be-
cause they are normally developed to analyze na-
tive English texts. Considering this, we tested
CRFTagger6 on non-native English texts contain-
ing various grammatical errors before the exper-
iments (Nagata et al., 2011). It turned out that
CRFTagger achieved an accuracy of 0.932 (com-
pared to 0.970 on native texts). Although it did not
perform as well as on native texts, it still achieved
a fair accuracy. Accordingly, we decided to use it
in our experiments.
Then, we generated cluster trees from the cor-
pus data using the methods described in Sect. 3.
</bodyText>
<footnote confidence="0.855254384615385">
3For example, because of (iii), essays written by native
speakers of Swedish in the Finnish subcorpus were excluded
from the experiments. This is because they were collected in
Finland and might be influenced by Finnish.
4The symbol ‘ is sometimes used for ’ (e.g., I‘m).
5The LOCNESS corpus is a corpus of native En-
glish essays made up of British pupils’ essays, British
university students’ essays, and American university
students’ essays: https://www.uclouvain.be/
en-cecl-locness.html
6Xuan-Hieu Phan, “CRFTagger: CRF English POS
Tagger,” http://crftagger.sourceforge.net/,
2006.
</footnote>
<table confidence="0.9998931875">
Native language # of essays # of tokens
Bulgarian 294 219,551
Czech 220 205,264
Dutch 244 240,861
French 273 202,439
German 395 236,841
Italian 346 219,581
Norwegian 290 218,056
Polish 354 251,074
Russian 255 236,748
Spanish 237 211,343
Swedish 301 268,361
English 298 294,357
Japanese1 (ICLE) 171 224,534
Japanese2 (NICE) 340 130,156
Total 4,018 3,159,166
</table>
<tableCaption confidence="0.999937">
Table 1: Statistics on target corpora.
</tableCaption>
<bodyText confidence="0.999988071428571">
We used the Kyoto Language Modeling toolkit7
to build language models from the corpus data.
We removed n-grams that appeared less than five
times8 in each subcorpus in the language mod-
els. Similarly, we implemented the vector-based
method with trigrams using the same frequency
cutoff (but without smoothing).
Fig.1 shows the experimental results. The
tree at the top is the Indo-European family tree
drawn based on the figure shown in Crystal
(1997). It shows that the 11 languages are divided
into three groups: Italic, Germanic, and Slavic
branches. The second and third trees are the clus-
ter trees generated by the language model-based
and vector-based methods, respectively. The num-
ber at each branching node denotes in which step
the two clusters were merged.
The experimental results strongly support the
hypothesis we made in Sect. 1. Fig. 1 reveals
that the language model-based method correctly
groups the 11 Englishes into the Italic, Ger-
manic, and Slavic branches. It first merges
Norwegian-English and Swedish-English into a
cluster. The two languages belong to the North
Germanic branch of the Germanic branch and
thus are closely related. Subsequently, the lan-
guage model-based method correctly merges the
other languages into the three branches. A dif-
</bodyText>
<footnote confidence="0.9941845">
7The Kyoto Language Modeling toolkit: http://www.
phontron.com/kylm/
8We found that the results were not sensitive to the value
of frequency cutoff so long as we set it to a small number.
</footnote>
<page confidence="0.965877">
1140
</page>
<figure confidence="0.999547818181818">
Indo-European family tree
Italic Germanic Slavic
13
12
3
Native
English
Japanese
English1
Japanese
English2
</figure>
<figureCaption confidence="0.999995">
Figure 1: Experimental results.
</figureCaption>
<bodyText confidence="0.907892379310345">
ference between its cluster tree and the Indo-
European family tree is that there are some mis-
matches within the Germanic and Slavic branches.
While the difference exists, the method strongly
distinguishes the three branches from one an-
other. The third tree shows that the vector-based
method behaves similarly while it mistakenly at-
taches Polish-English into an independent branch.
From these results, we can say that mother tongue
interference is transferred into the 11 Englishes,
strongly enough for reconstructing its language
family tree, which we propose calling the inter-
language Indo-European family tree in English.
Fig. 2 shows the experimental results with na-
tive and Japanese Englishes. It shows that the
same interlanguage Indo-European family tree
was reconstructed as before. More interestingly,
native English was detached from the interlan-
guage Indo-European family tree contrary to the
expectation that it would be attached to the Ger-
manic branch because English is of course a mem-
ber of the Germanic branch. This implies that
non-nativeness common to the 11 Englishes is
more influential than the intrafamily distance is9;
9Admittedly, we need further investigation to confirm this
argument especially because we applied CRFTagger, which is
developed to analyze native English, to both non-native and
native Englishes, which might affect the results.
Interlanguage Indo-European family tree Other family
</bodyText>
<figureCaption confidence="0.980451">
Figure 2: Experimental results with native and
Japanese Englishes.
</figureCaption>
<bodyText confidence="0.999171333333333">
otherwise, native English would be included in
the German branch. Fig.2 also shows that the
two sets of Japanese English were merged into
a cluster and that it was the most distant in the
whole tree. This shows that the interfamily dis-
tance is the most influential factor. Based on
these results, we can further hypothesize as fol-
lows: interfamily distance &gt; non-nativeness &gt;
intrafamily distance.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999960428571429">
To get a better understanding of the interlanguage
Indo-European family tree, we further explore lin-
guistic features that explain well the above phe-
nomena. When we analyze the experimental re-
sults, however, some problems arise. It is al-
most impossible to find someone who has a good
knowledge of the 11 languages and their mother
language interference in English writing. Besides,
there are a large number of language pairs to com-
pare. Thus, we need an efficient and effective way
to analyze the experimental results.
To address these problems, we did the follow-
ing. First, we focused on only a few Englishes
out of the 11. Because one of the authors had
some knowledge of French, we selected French-
English as the main target. This naturally made
us select the other Italic Englishes as its counter-
parts. Also, because we had access to a native
speaker of Russian who had a good knowledge of
English, we included Russian-English in our fo-
cus. We analyzed these Englishes and then ex-
amined whether the findings obtained apply to the
other Englishes or not. Second, we used a method
for extracting interesting trigrams from the cor-
pus data. The method compares three out of the
11 corpora (for example, French-, Spanish-, and
Russian-Englishes). If we remove instances of a
trigram from each set, the clustering tree involving
</bodyText>
<figure confidence="0.997744822222222">
9
3
5
Cluster tree generated by
LM-based method
10
8
6
Cluster tree generated by
vector-based clustering
7
1
10
9
6
4
8
3
5
7
1
2
French
English
Spanish
English
Italian
English
Dutch
English
German
English
Norwegian
English
Swedish
English
Czech
English
Russian
English
Bulgarian
English
Polish
English
2 4
</figure>
<page confidence="0.978725">
1141
</page>
<bodyText confidence="0.999289666666667">
the three may change. For example, the removal
of but the hers may result in a cluster tree merg-
ing French- and Russian-Englishes before French-
and Spanish-Englishes. Even if it does not change,
the distances may change in that direction. We an-
alyzed what trigrams had contributed to the clus-
tering results with this approach.
To formalize this approach, we will denote a tri-
gram by t. We will also denote its relative fre-
quency in the language data Di by rti. Then, the
change in the distances caused by the removal of t
from Di, Dj, and Dk is quantified by
</bodyText>
<equation confidence="0.981575">
s = (rtk − rti)2 − (rtj − rti )2 (4)
</equation>
<bodyText confidence="0.999970117647059">
in the vector-based method. The quantity (rtk −
rti)2 is directly related to the decrease in the dis-
tance between Di and Dk and similarly, (rtj −
rti)2 to that between Di and Dj in the vector-
based method. Thus, the greater s is, the higher the
chance that the cluster tree changes. Therefore, we
can obtain a list of interesting trigrams by sorting
them according to s. We could do a similar calcu-
lation in the language model-based method using
the conditional probabilities. However, it requires
a more complicated calculation. Accordingly, we
limit ourselves to the vector-based method in this
analysis, noting that both methods generated sim-
ilar cluster trees.
Table 2 shows the top 15 interesting trigrams
where Di, Dj, and Dk are French-, Spanish-, and
Russian-Englishes, respectively. Note that s is
multiplied by 106 and r is in % for readability. The
list reveals that many of the trigrams contain the
article a or the. Interestingly, their frequencies are
similar in French-English and Spanish-English,
and both are higher than in Russian-English. This
corresponds to the fact that French and Spanish
have articles whereas Russian does not. Actu-
ally, the same argument can be made about the
other Italic and Slavic Englishes (e.g., the JJ NN:
Italian-English 0.82; Polish-English 0.72)10. An
exception is that of trigrams containing the definite
article in Bulgarian-English; it tends to be higher
in Bulgarian-English than in the other Slavic En-
glishes. Surprisingly and interestingly, however, it
reflects the fact that Bulgarian does have the def-
inite article but not the indefinite article (e.g., the
JJ NN: 0.82; a JJ NN: 0.60 in Bulgarian-English).
</bodyText>
<footnote confidence="0.642659333333333">
10Due to the space limitation, other lists were not included
in this paper but are available at http://web.hyogo-u.
ac.jp/nagata/acl/.
</footnote>
<bodyText confidence="0.999486590909091">
Table 3 shows that the differences in article
use exist even between the Italic and Germanic
branches despite the fact that both have the in-
definite and definite articles. The list still con-
tains a number of trigrams containing articles. For
a better understanding of this, we looked further
into the distribution of articles in the corpus data.
It turns out that the distribution almost perfectly
groups the 11 Englishes into the corresponding
branches as shown in Fig. 3. The overall use of
articles is less frequent in the Slavic-Englishes.
The definite article is used more frequently in the
Italic-Englishes than in the Germanic Englishes
(except for Dutch-English). We speculate that
this is perhaps because the Italic languages have a
wider usage of the definite article such as its modi-
fication of possessive pronouns and proper nouns.
The Japanese Englishes form another group (this
is also true for the following findings). This corre-
sponds to the fact that the Japanese language does
not have an article system similar to that of En-
glish.
</bodyText>
<table confidence="0.9883135">
s Trigram t rti rtj rtk
5.14 the NN of 1.01 0.98 0.78
4.38 a JJ NN 0.85 0.77 0.62
2.74 the JJ NN 0.87 0.86 0.71
2.30 NN of the 0.49 0.52 0.33
1.64 ... 0.22 0.12 0.05
1.56 NNS. EOS 0.77 0.70 0.92
1.31 NNS and NNS 0.09 0.13 0.21
1.25 BOS RB , 0.25 0.22 0.14
1.22 of the NN 0.42 0.44 0.30
1.17 VBZ to VB 0.26 0.22 0.14
1.09 BOS i VBP 0.07 0.05 0.17
1.03 NN of NN 0.74 0.70 0.63
0.88 NN of JJ 0.15 0.15 0.25
0.67 the JJ NNS 0.28 0.28 0.20
0.65 NN to VB 0.40 0.38 0.31
</table>
<tableCaption confidence="0.903014">
Table 2: Interesting trigrams (French- (Di),
Spanish- (Dj), and Russian- (Dk) Englishes).
</tableCaption>
<bodyText confidence="0.999719555555556">
Another interesting trigram, though not as ob-
vious as article use, is NN of NN, which ranks
12th and 2nd in Table 2 and 3, respectively. In the
Italic Englishes, the trigram is more frequent than
the other non-native Englishes as shown in Fig. 4.
This corresponds to the fact that noun-noun com-
pounds are less common in the Italic languages
than in English and that instead, the of-phrase (NN
of NN) is preferred (Swan and Smith, 2001). For
</bodyText>
<page confidence="0.988784">
1142
</page>
<figure confidence="0.996741238095238">
s Trigram t rti rtj rtk
21.49 the NN of 1.01 0.98 0.54
5.70 NN of NN 0.74 0.70 0.50
3.26 NN of the 0.49 0.52 0.30
3.10 the JJ NN 0.87 0.86 0.70
2.62 ... 0.22 0.12 0.03
1.53 of the NN 0.42 0.44 0.29
1.50 NN , NN 0.30 0.30 0.18
1.50 BOS i VBP 0.07 0.05 0.19
0.85 NNS and NNS 0.09 0.13 0.19
0.81 JJ NN of 0.40 0.39 0.31
0.68 .. EOS 0.13 0.06 0.02
0.63 a JJ NN 0.85 0.77 0.73
0.63 RB . EOS 0.21 0.16 0.31
0.56 NN , the 0.16 0.16 0.08
0.50 NN of a 0.17 0.09 0.06
Italic
Germanic
Slavic
Japanese
Japanese1
Japanese2
Bulgarian
Polish
Russian
Czech
Norwegian
English
Swedish
German
Spanish
French
Dutch
Italian
1 1.5 2 2.5 3
Relative frequency of indefinite article (%)
6
5
4
3
Relative frequency of definite article (%)
2
</figure>
<figureCaption confidence="0.999798">
Figure 3: Distribution of articles.
</figureCaption>
<tableCaption confidence="0.839938333333333">
Table 3: Interesting trigrams (French- (Di),
Spanish- (Dj), and Swedish- (Dk) Englishes).
Relative frequency of NN of NN (%)
</tableCaption>
<bodyText confidence="0.999925903225807">
instance, orange juice is expressed as juice of or-
ange in the Italic languages (e.g., jus d’orange in
French). In contrast, noun-noun compounds or
similar constructions are more common in Russian
and Swedish. As a result, NN of NN becomes rel-
atively frequent in the Italic Englishes. Fig. 4 also
shows that its distribution roughly groups the 11
Englishes into the three branches. Therefore, the
way noun phrases (NPs) are constructed is a clue
to how the three branches were clustered.
This finding in turn reveals that the consecu-
tive repetitions of nouns occur less in the Italic
Englishes. In other words, the length tends to
be shorter than in the others where we define
the length as the number of consecutive repeti-
tions of common nouns (for example, the length
of orange juice is one because a noun is con-
secutively repeated once). To see if this is true,
we calculated the average length for each English.
Fig. 5 shows that the average length roughly dis-
tinguishes the Italic Englishes from the other non-
native Englishes; French-English is the shortest,
which is explained by the discussion above, while
Dutch- and German-Englishes are longest, which
may correspond to the fact that they have a prefer-
ence for noun-noun compounds as Snyder (1996)
argues. For instance, German allows the concate-
nated form as in Orangensaft (equivalently or-
angejuice). This tendency in the length of noun-
noun compounds provides us with a crucial insight
for native language identification, which we will
</bodyText>
<figureCaption confidence="0.981637">
Figure 4: Relative frequency of NN of NN in each
corpus (%).
</figureCaption>
<bodyText confidence="0.991787142857143">
come back to in Sect. 6.
The trigrams BOS RB , in Table 2 and RB . EOS
in Table 3 imply that there might also be a certain
pattern in adverb position in the 11 Englishes (they
roughly correspond to adverbs at the beginning
and end of sentences). Fig. 6 shows an insight into
this. The horizontal and vertical axes correspond
to the ratio of adverbs at the beginning and the end
of sentences, respectively. It turns out that the Ger-
man Englishes form a group. So do the Italic En-
glishes although it is less dense. In contrast, the
Slavic Englishes are scattered. However, the ra-
tios give a clue to how to distinguish Slavic En-
glishes from the others when combined with other
</bodyText>
<figure confidence="0.980187904761905">
0 0.5 1
French
Italian
Spanish
Polish
Russian
Bulgarian
Czech
English
Dutch
Swedish
German
Norwegian
Japanese1
Japanese2
Germanic
Japanese
Slavic
Italic
1143
Average length of noun-noun compounds
</figure>
<figureCaption confidence="0.9980705">
Figure 5: Average length of noun-noun com-
pounds in each corpus.
</figureCaption>
<figure confidence="0.9352195">
15 20 25 30
Ratio of adverbs at the beginning (%)
</figure>
<figureCaption confidence="0.999987">
Figure 6: Distribution of adverb position.
</figureCaption>
<bodyText confidence="0.99988">
trigrams. For instance, although Polish-English
is located in the middle of Swedish-English and
Bulgarian-English in the distribution of articles
(in Fig. 3), the ratios tell us that Polish-English is
much nearer to Bulgarian-English.
</bodyText>
<sectionHeader confidence="0.9735805" genericHeader="method">
6 Implications for Work in Related
Domains
</sectionHeader>
<bodyText confidence="0.998786581395349">
Researchers including Wong and Dras (2009),
Wong et al. (2011; 2012), and Koppel et al. (2005)
work on native language identification and show
that machine learning-based methods are effec-
tive. Wong and Dras (2009) propose using infor-
mation about grammatical errors such as errors in
determiners to achieve better performance while
they show that its use does not improve the perfor-
mance, contrary to the expectation. Related to this,
other researchers (Koppel and Ordan, 2011; van
Halteren, 2008) show that machine learning-based
methods can also predict the source language of
a given translated text although it should be em-
phasized that it is a different task from native lan-
guage identification because translation is not typ-
ically performed by non-native speakers but rather
native speakers of the target language11.
The experimental results show that n-grams
containing articles are predictive for identify-
ing native languages. This indicates that they
should be used in the native language identifi-
cation task. Importantly, all n-grams contain-
ing articles should be used in the classifier unlike
the previous methods that are based only on n-
grams containing article errors. Besides, no ar-
ticles should be explicitly coded in n-grams for
taking the overuse/underuse of articles into con-
sideration. We can achieve this by adding a spe-
cial symbol such as 0 to the beginning of each NP
whose head noun is a common noun and that has
no determiner in it as in “I like 0 orange juice.”
In addition, the length of noun-noun com-
pounds and the position of adverbs should also
be considered in native language identification. In
particular, the former can be modeled by the Pois-
son distribution as follows. The Poisson distribu-
tion gives the probability of the number of events
occurring in a fixed time. In our case, the number
of events in a fixed time corresponds to the num-
ber of consecutive repetitions of common nouns in
NPs, which in turn corresponds to the length. To
be precise, the probability of a noun-noun com-
pound with length l is given by
</bodyText>
<equation confidence="0.9472965">
Al
Pr(l) = l! e− � (5)
</equation>
<bodyText confidence="0.985391615384616">
where A corresponds to the average length. Fig. 7
shows that the observed values in the French-
English data very closely fit the theoretical proba-
&amp;quot;For comparison, we conducted a pilot study where we
reconstructed a language family tree from English texts
in European Parliament Proceedings Parallel Corpus (Eu-
roparl) (Koehn, 2011). It turned out that the reconstructed
tree was different from the canonical tree (available at http:
//web.hyogo-u.ac.jp/nagata/acl/). However,
we need further investigation to confirm it because each sub-
corpus in Europarl is variable in many dimensions includ-
ing its size and style (e.g., overuse of certain phrases such as
ladies and gentlemen).
</bodyText>
<figure confidence="0.994128270833333">
0 0.1
French
Italian
Spanish
Bulgarian
Czech
Russian
Polish
Swedish
Norwegian
German
Dutch
English
Japanese1
Japanese2
Germanic
Japanese
Slavic
Italic
Ratio of adverbs at the end (%)
10
5
Norwegian
Swedish
Italic
Germanic
Slavic
Japanese
Czech
Dutch
Russian
English
German
Spanish
French
Japanese2
Bulgarian
Italian
Japanese1
Polish
1144
1
Theoretical
Observed
Probability 0.5
0
0 1 2 3
Length of noun-noun compound
</figure>
<figureCaption confidence="0.8094565">
Figure 7: Distribution of noun-noun compound
length for French-English.
</figureCaption>
<bodyText confidence="0.990347242424242">
bilities given by Equation (5)12. This holds for the
other Englishes although we cannot show them be-
cause of the space limitation. Consequently, Equa-
tion (5) should be useful in native language identi-
fication. Fortunately, it can be naturally integrated
into existing classifiers.
In the domain of historical linguistics, re-
searchers have used computational and corpus-
based methods for reconstructing language fam-
ily trees. Some (Enright and Kondrak, 2011;
Gray and Atkinson, 2003; Barbanc¸on et al., 2007;
Batagelj et al., 1992; Nakhleh et al., 2005) ap-
ply clustering techniques to the task of language
family tree reconstruction. Others (Kita, 1999;
Rama and Singh, 2009) use corpus statistics for
the same purpose. These methods reconstruct lan-
guage family trees based on linguistic features that
exist within words including lexical, phonological,
and morphological features.
The experimental results in this paper suggest
the possibility of the use of non-native texts for re-
constructing language family trees. It allows us to
use linguistic features that exist between words, as
seen in our methods, which has been difficult with
previous methods. Language involves the features
between words such as phrase construction and
syntax as well as the features within words and
thus they should both be considered in reconstruc-
12The theoretical and observed values are so close that it
is difficult to distinguish between the two lines in Fig. 7. For
example, Pr(l = 1) = 0.0303 while the corresponding ob-
served value is 0.0299.
tion of language family trees.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999989333333333">
In this paper, we have shown that mother tongue
interference is so strong that the relations be-
tween members of the Indo-European language
family are preserved in English texts written by
Indo-European language speakers. To show this,
we have used clustering to reconstruct a lan-
guage family tree from 11 sets of non-native
English texts. It turned out that the recon-
structed tree correctly groups them into the Italic,
Germanic, and Slavic branches of the Indo-
European family tree. Based on the resulting
trees, we have then hypothesized that the fol-
lowing relation holds in mother tongue interfer-
ence: interfamily distance &gt; non-nativeness &gt;
intrafamily distance. We have further explored
several intriguing linguistic features that play an
important role in mother tongue interference: (i)
article use, (ii) NP construction, and (iii) adverb
position, which provide several insights for im-
proving the tasks of native language identification
and language family tree reconstruction.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99410925">
This work was partly supported by the Digiteo for-
eign guest project. We would like to thank the
three anonymous reviewers and the following per-
sons for their useful comments on this paper: Ko-
taro Funakoshi, Mitsuaki Hayase, Atsuo Kawai,
Robert Ladig, Graham Neubig, Vera Sheinman,
Hiroya Takamura, David Valmorin, Mikko Vile-
nius.
</bodyText>
<sectionHeader confidence="0.999022" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996179666666667">
Jan Aarts and Sylviane Granger, 1998. Tag sequences
in learner corpora: a key to interlanguage gram-
mar and discourse, pages 132–141. Longman, New
York.
Bengt Altenberg and Marie Tapper, 1998. The use of
adverbial connectors in advanced Swedish learners’
written English, pages 80–93. Longman, New York.
Franc¸ois Barbanc¸on, Tandy Warnow, Steven N. Evans,
Donald Ringe, and Luay Nakhleh. 2007. An exper-
imental study comparing linguistic phylogenetic re-
construction methods. Statistics Technical Reports,
page 732.
Vladimir Batagelj, Tomaˇz Pisanski, and Dami-
jana Kerˇziˇc. 1992. Automatic clustering of lan-
guages. Computational Linguistics, 18(3):339–352.
</reference>
<page confidence="0.848784">
1145
</page>
<reference confidence="0.9998146">
Robert S.P. Beekes. 2011. Comparative Indo-
European Linguistics: An Introduction (2nd ed.).
John Benjamins Publishing Company, Amsterdam.
Martin Chodorow, Michael Gamon, and Joel R.
Tetreault. 2010. The utility of article and prepo-
sition error correction systems for English language
learners: feedback and assessment. Language Test-
ing, 27(3):419–436.
David Crystal. 1997. The Cambridge Encyclopedia of
Language (2nd ed.). Cambridge University Press,
Cambridge.
Niels Davidsen-Nielsen and Peter Harder, 2001.
Speakers of Scandinavian languages: Danish, Nor-
wegian, Swedish, pages 21–36. Cambridge Univer-
sity Press, Cambridge.
Alvar Elleg˚ard. 1959. Statistical measurement of lin-
guistic relationship. Language, 35(2):131–156.
Jessica Enright and Grzegorz Kondrak. 2011. The ap-
plication of chordal graphs to inferring phylogenetic
trees of languages. In Proc. of 5th International
Joint Conference on Natural Language Processing,
pages 8–13.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English v2. Presses universitaires de Lou-
vain, Louvain.
Russell D. Gray and Quentin D. Atkinson. 2003.
Language-tree divergence times support the Ana-
tolian theory of Indo-European origin. Nature,
426:435–438.
Jiawei Han and Micheline Kamber. 2006. Data Min-
ing: Concepts and Techniques (2nd Ed.). Morgan
Kaufmann Publishers, San Francisco.
Bing-Hwang Juang and Lawrence R. Rabiner. 1985.
A probabilistic distance measure for hidden Markov
models. AT&amp;T Technical Journal, 64(2):391–408.
Kenji Kita. 1999. Automatic clustering of languages
based on probabilistic models. Journal of Quantita-
tive Linguistics, 6(2):167–171.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proc. of International Conference on Acoustics,
Speech, and Signal Processing, volume 1, pages
181–184.
Philipp Koehn. 2011. Europarl: A parallel corpus for
statistical machine translation. In Proc. of 10th Ma-
chine Translation Summit, pages 79–86.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of 49th Annual Meeting
of the Association for Computational Linguistics,
pages 1318–1326.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon.
2005. Determining an author’s native language by
mining a text for errors. In Proc. of 11th ACM
SIGKDD International Conference on Knowledge
Discovery in Data Mining, pages 624–628.
Alfred L. Kroeber and Charles D. Chri´etien. 1937.
Quantitative classification of Indo-European lan-
guages. Language, 13(2):83–103.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged and
shallow-parsed learner corpus. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1210–1219.
Luay Nakhleh, Tandy Warnow, Don Ringe, and
Steven N. Evans. 2005. A comparison of phyloge-
netic reconstruction methods on an Indo-European
dataset. Transactions of the Philological Society,
103(2):171–192.
Taraka Rama and Anil Kumar Singh. 2009. From bag
of languages to family trees from noisy corpus. In
Proc. of Recent Advances in Natural Language Pro-
cessing, pages 355–359.
Anna Giacalone Ramat and Paolo Ramat, 2006. The
Indo-European Languages. Routledge, New York.
William Snyder. 1996. The acquisitional role of the
syntax-morphology interface: Morphological com-
pounds and syntactic complex predicates. In Proc.
of Annual Boston University Conference on Lan-
guage Development, volume 2, pages 728–735.
Masatoshi Sugiura, Masumi Narita, Tomomi Ishida,
Tatsuya Sakaue, Remi Murao, and Kyoko Muraki.
2007. A discriminant analysis of non-native speak-
ers and native speakers of English. In Proc. of Cor-
pus Linguistics Conference CL2007, pages 84–89.
Michael Swan and Bernard Smith. 2001. Learner En-
glish (2nd Ed.). Cambridge University Press, Cam-
bridge.
Hans van Halteren. 2008. Source language markers
in EUROPARL translations. In Proc. of 22nd Inter-
national Conference on Computational Linguistics,
pages 937–944.
Sze-Meng J. Wong and Mark Dras. 2009. Con-
trastive analysis and native language identification.
In Proc. Australasian Language Technology Work-
shop, pages 53–61.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2011. Exploiting parse structures for native lan-
guage identification. In Proc. Conference on Em-
pirical Methods in Natural Language Processing,
pages 1600–1611.
Sze-Meng J. Wong, Mark Dras, and Mark Johnson.
2012. Exploring adaptor grammars for native lan-
guage identification. In Proc. Joint Conference on
</reference>
<page confidence="0.858321">
1146
</page>
<reference confidence="0.987894333333333">
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 699–709.
</reference>
<page confidence="0.994813">
1147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.326614">
<title confidence="0.951711">Reconstructing an Indo-European Family</title>
<author confidence="0.577832">from Non-native English texts</author>
<affiliation confidence="0.79278375">2Edward University / Kobe, / Orsay, Limited / Northampton,</affiliation>
<email confidence="0.988485">nagata-acl@hyogo-u.ac.jp,ed@inferret.co.uk</email>
<abstract confidence="0.999706388888889">Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores linguistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan Aarts</author>
<author>Sylviane Granger</author>
</authors>
<title>Tag sequences in learner corpora: a key to interlanguage grammar and discourse,</title>
<date>1998</date>
<pages>132--141</pages>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="1571" citStr="Aarts and Granger (1998)" startWordPosition="233" endWordPosition="236"> Introduction Transfer of linguistic systems of a mother tongue to another language, namely mother tongue interference, is often observable in the writing of nonnative speakers. The reader may be able to determine the mother tongue of the writer of the following sentence from the underlined article error: The alien wouldn’t use my spaceship but the hers. The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing. Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items. In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues. At one extreme, one could argue that it is so strongly transferred to texts in another language that the linguistic relations between mother tongues are perfectly preserved in the texts. At the other extreme, one can counter it, arguing that oth</context>
</contexts>
<marker>Aarts, Granger, 1998</marker>
<rawString>Jan Aarts and Sylviane Granger, 1998. Tag sequences in learner corpora: a key to interlanguage grammar and discourse, pages 132–141. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bengt Altenberg</author>
<author>Marie Tapper</author>
</authors>
<title>The use of adverbial connectors in advanced Swedish learners’ written English,</title>
<date>1998</date>
<pages>80--93</pages>
<publisher>Longman,</publisher>
<location>New York.</location>
<contexts>
<context position="1639" citStr="Altenberg and Tapper (1998)" startWordPosition="243" endWordPosition="246">o another language, namely mother tongue interference, is often observable in the writing of nonnative speakers. The reader may be able to determine the mother tongue of the writer of the following sentence from the underlined article error: The alien wouldn’t use my spaceship but the hers. The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing. Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items. In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues. At one extreme, one could argue that it is so strongly transferred to texts in another language that the linguistic relations between mother tongues are perfectly preserved in the texts. At the other extreme, one can counter it, arguing that other features such as non-nativeness are more influential than mother </context>
</contexts>
<marker>Altenberg, Tapper, 1998</marker>
<rawString>Bengt Altenberg and Marie Tapper, 1998. The use of adverbial connectors in advanced Swedish learners’ written English, pages 80–93. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franc¸ois Barbanc¸on</author>
<author>Tandy Warnow</author>
<author>Steven N Evans</author>
<author>Donald Ringe</author>
<author>Luay Nakhleh</author>
</authors>
<title>An experimental study comparing linguistic phylogenetic reconstruction methods. Statistics Technical Reports,</title>
<date>2007</date>
<pages>732</pages>
<marker>Barbanc¸on, Warnow, Evans, Ringe, Nakhleh, 2007</marker>
<rawString>Franc¸ois Barbanc¸on, Tandy Warnow, Steven N. Evans, Donald Ringe, and Luay Nakhleh. 2007. An experimental study comparing linguistic phylogenetic reconstruction methods. Statistics Technical Reports, page 732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Batagelj</author>
<author>Tomaˇz Pisanski</author>
<author>Damijana Kerˇziˇc</author>
</authors>
<title>Automatic clustering of languages.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Batagelj, Pisanski, Kerˇziˇc, 1992</marker>
<rawString>Vladimir Batagelj, Tomaˇz Pisanski, and Damijana Kerˇziˇc. 1992. Automatic clustering of languages. Computational Linguistics, 18(3):339–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S P Beekes</author>
</authors>
<title>Comparative IndoEuropean Linguistics: An Introduction (2nd ed.).</title>
<date>2011</date>
<publisher>John Benjamins Publishing Company,</publisher>
<location>Amsterdam.</location>
<contexts>
<context position="5179" citStr="Beekes, 2011" startWordPosition="810" endWordPosition="811">92; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect.6 discusses implications for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006). If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis. If not, it suggests that some features other than mother tongue interference are more influential. The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers. Researchers have already performed related work on reconstructing language family trees. For instance, Kroeber and Chri´etien (1937) and Elleg˚ard (1959) proposed sta</context>
</contexts>
<marker>Beekes, 2011</marker>
<rawString>Robert S.P. Beekes. 2011. Comparative IndoEuropean Linguistics: An Introduction (2nd ed.). John Benjamins Publishing Company, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel R Tetreault</author>
</authors>
<title>The utility of article and preposition error correction systems for English language learners: feedback and assessment.</title>
<date>2010</date>
<journal>Language Testing,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="4167" citStr="Chodorow et al., 2010" startWordPosition="646" endWordPosition="649">n other words, mother tongue interference is so strong that one can reconstruct a language fam1137 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1137–1147, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics ily tree from non-native texts. One of the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis. This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners. As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 dis</context>
</contexts>
<marker>Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Martin Chodorow, Michael Gamon, and Joel R. Tetreault. 2010. The utility of article and preposition error correction systems for English language learners: feedback and assessment. Language Testing, 27(3):419–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Crystal</author>
</authors>
<title>The Cambridge Encyclopedia of Language (2nd ed.).</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="15753" citStr="Crystal (1997)" startWordPosition="2531" endWordPosition="2532">,343 Swedish 301 268,361 English 298 294,357 Japanese1 (ICLE) 171 224,534 Japanese2 (NICE) 340 130,156 Total 4,018 3,159,166 Table 1: Statistics on target corpora. We used the Kyoto Language Modeling toolkit7 to build language models from the corpus data. We removed n-grams that appeared less than five times8 in each subcorpus in the language models. Similarly, we implemented the vector-based method with trigrams using the same frequency cutoff (but without smoothing). Fig.1 shows the experimental results. The tree at the top is the Indo-European family tree drawn based on the figure shown in Crystal (1997). It shows that the 11 languages are divided into three groups: Italic, Germanic, and Slavic branches. The second and third trees are the cluster trees generated by the language model-based and vector-based methods, respectively. The number at each branching node denotes in which step the two clusters were merged. The experimental results strongly support the hypothesis we made in Sect. 1. Fig. 1 reveals that the language model-based method correctly groups the 11 Englishes into the Italic, Germanic, and Slavic branches. It first merges Norwegian-English and Swedish-English into a cluster. The</context>
</contexts>
<marker>Crystal, 1997</marker>
<rawString>David Crystal. 1997. The Cambridge Encyclopedia of Language (2nd ed.). Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niels Davidsen-Nielsen</author>
<author>Peter Harder</author>
</authors>
<date>2001</date>
<booktitle>Speakers of Scandinavian languages: Danish, Norwegian, Swedish,</booktitle>
<pages>21--36</pages>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<marker>Davidsen-Nielsen, Harder, 2001</marker>
<rawString>Niels Davidsen-Nielsen and Peter Harder, 2001. Speakers of Scandinavian languages: Danish, Norwegian, Swedish, pages 21–36. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvar Elleg˚ard</author>
</authors>
<title>Statistical measurement of linguistic relationship.</title>
<date>1959</date>
<journal>Language,</journal>
<volume>35</volume>
<issue>2</issue>
<marker>Elleg˚ard, 1959</marker>
<rawString>Alvar Elleg˚ard. 1959. Statistical measurement of linguistic relationship. Language, 35(2):131–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Enright</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>The application of chordal graphs to inferring phylogenetic trees of languages.</title>
<date>2011</date>
<booktitle>In Proc. of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>8--13</pages>
<contexts>
<context position="4496" citStr="Enright and Kondrak, 2011" startWordPosition="698" endWordPosition="701"> the major contributions of this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis. This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners. As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect.6 discusses implications for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers</context>
<context position="32316" citStr="Enright and Kondrak, 2011" startWordPosition="5300" endWordPosition="5303">panese1 Polish 1144 1 Theoretical Observed Probability 0.5 0 0 1 2 3 Length of noun-noun compound Figure 7: Distribution of noun-noun compound length for French-English. bilities given by Equation (5)12. This holds for the other Englishes although we cannot show them because of the space limitation. Consequently, Equation (5) should be useful in native language identification. Fortunately, it can be naturally integrated into existing classifiers. In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees. Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction. Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose. These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features. The experimental results in this paper suggest the possibility of the use of non-native texts for reconstructing language family trees. It allows us to use linguistic features</context>
</contexts>
<marker>Enright, Kondrak, 2011</marker>
<rawString>Jessica Enright and Grzegorz Kondrak. 2011. The application of chordal graphs to inferring phylogenetic trees of languages. In Proc. of 5th International Joint Conference on Natural Language Processing, pages 8–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
<author>Magali Paquot</author>
</authors>
<date>2009</date>
<booktitle>International Corpus of Learner English v2. Presses universitaires de Louvain,</booktitle>
<location>Louvain.</location>
<contexts>
<context position="12490" citStr="Granger et al., 2009" startWordPosition="1999" endWordPosition="2002">e will see in Sect. 5, this method allows us to interpret clustering results more easily than with the language model-based method while both result in similar language family trees. In this method, Di is modeled by a vector. The vector is constructed based on the relative frequencies of trigrams. As a consequence, the distance is naturally defined by the Euclidean distance between two vectors. The clustering procedure is the same as for the language model-based method except that Mi is vector-based and that the distance metric is Euclidean. 1139 4 Experiments We selected the ICLE corpus v.2 (Granger et al., 2009) as the target language data. It consists of English essays written by a wide variety of nonnative speakers of English. Among them, the 11 shown in Table 1 are of Indo-European languages. Accordingly, we selected the subcorpora of the 11 languages in the experiments. Before the experiments, we preprocessed the corpus data to control the experimental conditions. Because some of the writers had more than one native language, we excluded essays that did not meet the following three conditions: (i) the writer has only one native language; (ii) the writer has only one language at home; (iii) the tw</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, Paquot, 2009</marker>
<rawString>Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. International Corpus of Learner English v2. Presses universitaires de Louvain, Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell D Gray</author>
<author>Quentin D Atkinson</author>
</authors>
<title>Language-tree divergence times support the Anatolian theory of Indo-European origin.</title>
<date>2003</date>
<journal>Nature,</journal>
<pages>426--435</pages>
<contexts>
<context position="4521" citStr="Gray and Atkinson, 2003" startWordPosition="702" endWordPosition="705"> this work is to reveal and visualize a language family tree preserved in non-native texts, by examining the hypothesis. This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners. As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect.6 discusses implications for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother </context>
<context position="32341" citStr="Gray and Atkinson, 2003" startWordPosition="5304" endWordPosition="5307">etical Observed Probability 0.5 0 0 1 2 3 Length of noun-noun compound Figure 7: Distribution of noun-noun compound length for French-English. bilities given by Equation (5)12. This holds for the other Englishes although we cannot show them because of the space limitation. Consequently, Equation (5) should be useful in native language identification. Fortunately, it can be naturally integrated into existing classifiers. In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees. Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction. Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose. These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features. The experimental results in this paper suggest the possibility of the use of non-native texts for reconstructing language family trees. It allows us to use linguistic features that exist between words</context>
</contexts>
<marker>Gray, Atkinson, 2003</marker>
<rawString>Russell D. Gray and Quentin D. Atkinson. 2003. Language-tree divergence times support the Anatolian theory of Indo-European origin. Nature, 426:435–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiawei Han</author>
<author>Micheline Kamber</author>
</authors>
<date>2006</date>
<booktitle>Data Mining: Concepts and Techniques (2nd Ed.).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="5563" citStr="Han and Kamber, 2006" startWordPosition="868" endWordPosition="871">for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006). If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis. If not, it suggests that some features other than mother tongue interference are more influential. The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers. Researchers have already performed related work on reconstructing language family trees. For instance, Kroeber and Chri´etien (1937) and Elleg˚ard (1959) proposed statistical methods for measuring the similarity metric between languages. More recently, Batagelj et al. (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering. Among them, the &apos;Recently, native language identification has drawn the attention of NLP researchers. For instance, a shared task on native language identification took place at an N</context>
</contexts>
<marker>Han, Kamber, 2006</marker>
<rawString>Jiawei Han and Micheline Kamber. 2006. Data Mining: Concepts and Techniques (2nd Ed.). Morgan Kaufmann Publishers, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing-Hwang Juang</author>
<author>Lawrence R Rabiner</author>
</authors>
<title>A probabilistic distance measure for hidden Markov models.</title>
<date>1985</date>
<journal>AT&amp;T Technical Journal,</journal>
<volume>64</volume>
<issue>2</issue>
<contexts>
<context position="6630" citStr="Juang and Rabiner (1985)" startWordPosition="1027" endWordPosition="1030">, native language identification has drawn the attention of NLP researchers. For instance, a shared task on native language identification took place at an NAACL-HLT 2013 workshop. most related method is that of Kita (1999). In his method, a variety of languages are modeled by their spelling systems (i.e., character-based n-gram language models). Then, agglomerative hierarchical clustering is applied to the language models to reconstruct a language family tree. The similarity used for clustering is based on a divergence-like distance between two language models that was originally proposed by Juang and Rabiner (1985). This method is purely data-driven and does not require human expert knowledge for the selection of linguistic features. Our work closely follows Kita’s work. However, it should be emphasized that there is a significant difference between the two. Kita’s work (and other previous work) targets clustering of a variety of languages whereas our work tries to reconstruct a language family tree preserved in non-native English. This significant difference prevents us from directly applying techniques in the literature to our task. For instance, Batagelj et al. (1992) use basic vocabularies such as b</context>
</contexts>
<marker>Juang, Rabiner, 1985</marker>
<rawString>Bing-Hwang Juang and Lawrence R. Rabiner. 1985. A probabilistic distance measure for hidden Markov models. AT&amp;T Technical Journal, 64(2):391–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Kita</author>
</authors>
<title>Automatic clustering of languages based on probabilistic models.</title>
<date>1999</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="5904" citStr="Kita (1999)" startWordPosition="919" endWordPosition="920">ree, it will support the hypothesis. If not, it suggests that some features other than mother tongue interference are more influential. The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers. Researchers have already performed related work on reconstructing language family trees. For instance, Kroeber and Chri´etien (1937) and Elleg˚ard (1959) proposed statistical methods for measuring the similarity metric between languages. More recently, Batagelj et al. (1992) and Kita (1999) proposed methods for reconstructing language family trees using clustering. Among them, the &apos;Recently, native language identification has drawn the attention of NLP researchers. For instance, a shared task on native language identification took place at an NAACL-HLT 2013 workshop. most related method is that of Kita (1999). In his method, a variety of languages are modeled by their spelling systems (i.e., character-based n-gram language models). Then, agglomerative hierarchical clustering is applied to the language models to reconstruct a language family tree. The similarity used for clusteri</context>
<context position="32511" citStr="Kita, 1999" startWordPosition="5333" endWordPosition="5334">olds for the other Englishes although we cannot show them because of the space limitation. Consequently, Equation (5) should be useful in native language identification. Fortunately, it can be naturally integrated into existing classifiers. In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees. Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction. Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose. These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features. The experimental results in this paper suggest the possibility of the use of non-native texts for reconstructing language family trees. It allows us to use linguistic features that exist between words, as seen in our methods, which has been difficult with previous methods. Language involves the features between words such as phrase construction and syntax as well as t</context>
</contexts>
<marker>Kita, 1999</marker>
<rawString>Kenji Kita. 1999. Automatic clustering of languages based on probabilistic models. Journal of Quantitative Linguistics, 6(2):167–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="10291" citStr="Kneser and Ney, 1995" startWordPosition="1621" endWordPosition="1624">ouns, question adverbs. Also, proper nouns are treated as common nouns. At this point, the special POS tags BOS and EOS are added at the beginning and end of each sentence, respectively. For instance, the above example would result in the following word/POS sequence: BOS the NN would RB VB my NN but the hers . EOS Note that the content of the original sentence is far from clear while reflecting mother tongue interference, especially in the hers. Now, the language model Mi can be built from Di. We set n = 3 (i.e., trigram language model) following Kita’s work and use Kneser-Ney (KN) smoothing (Kneser and Ney, 1995) to estimate its conditional probabilities. With Mi and Di, we can naturally apply Kita’s method to our task. The clustering algorithm used is agglomerative hierarchical clustering with the average linkage method. The distance between two language models is measured as follows. The zIt is not a distance in a mathematical sense. However, we will use the term distance following the convention in the literature. probability that Mi generates Di is calculated by Pr(Di|Mi). Note that Pr(Di|Mi) ≈ Pr(w1,i) Pr(w2,i|w1,i) |Di| × ∏ Pr(wt,i|wt−2,i,wt−1,i) (1) t=3 where wt,i and |Di |denote the tth token </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. of International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="31024" citStr="Koehn, 2011" startWordPosition="5110" endWordPosition="5111">ixed time. In our case, the number of events in a fixed time corresponds to the number of consecutive repetitions of common nouns in NPs, which in turn corresponds to the length. To be precise, the probability of a noun-noun compound with length l is given by Al Pr(l) = l! e− � (5) where A corresponds to the average length. Fig. 7 shows that the observed values in the FrenchEnglish data very closely fit the theoretical proba&amp;quot;For comparison, we conducted a pilot study where we reconstructed a language family tree from English texts in European Parliament Proceedings Parallel Corpus (Europarl) (Koehn, 2011). It turned out that the reconstructed tree was different from the canonical tree (available at http: //web.hyogo-u.ac.jp/nagata/acl/). However, we need further investigation to confirm it because each subcorpus in Europarl is variable in many dimensions including its size and style (e.g., overuse of certain phrases such as ladies and gentlemen). 0 0.1 French Italian Spanish Bulgarian Czech Russian Polish Swedish Norwegian German Dutch English Japanese1 Japanese2 Germanic Japanese Slavic Italic Ratio of adverbs at the end (%) 10 5 Norwegian Swedish Italic Germanic Slavic Japanese Czech Dutch R</context>
</contexts>
<marker>Koehn, 2011</marker>
<rawString>Philipp Koehn. 2011. Europarl: A parallel corpus for statistical machine translation. In Proc. of 10th Machine Translation Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Noam Ordan</author>
</authors>
<title>Translationese and its dialects.</title>
<date>2011</date>
<booktitle>In Proc. of 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1318--1326</pages>
<contexts>
<context position="29097" citStr="Koppel and Ordan, 2011" startWordPosition="4782" endWordPosition="4785">n of articles (in Fig. 3), the ratios tell us that Polish-English is much nearer to Bulgarian-English. 6 Implications for Work in Related Domains Researchers including Wong and Dras (2009), Wong et al. (2011; 2012), and Koppel et al. (2005) work on native language identification and show that machine learning-based methods are effective. Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation. Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identification because translation is not typically performed by non-native speakers but rather native speakers of the target language11. The experimental results show that n-grams containing articles are predictive for identifying native languages. This indicates that they should be used in the native language identification task. Importantly, all n-grams containing articles should be us</context>
</contexts>
<marker>Koppel, Ordan, 2011</marker>
<rawString>Moshe Koppel and Noam Ordan. 2011. Translationese and its dialects. In Proc. of 49th Annual Meeting of the Association for Computational Linguistics, pages 1318–1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
<author>Kfir Zigdon</author>
</authors>
<title>Determining an author’s native language by mining a text for errors.</title>
<date>2005</date>
<booktitle>In Proc. of 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining,</booktitle>
<pages>624--628</pages>
<contexts>
<context position="28715" citStr="Koppel et al. (2005)" startWordPosition="4723" endWordPosition="4726">nese1 Japanese2 Germanic Japanese Slavic Italic 1143 Average length of noun-noun compounds Figure 5: Average length of noun-noun compounds in each corpus. 15 20 25 30 Ratio of adverbs at the beginning (%) Figure 6: Distribution of adverb position. trigrams. For instance, although Polish-English is located in the middle of Swedish-English and Bulgarian-English in the distribution of articles (in Fig. 3), the ratios tell us that Polish-English is much nearer to Bulgarian-English. 6 Implications for Work in Related Domains Researchers including Wong and Dras (2009), Wong et al. (2011; 2012), and Koppel et al. (2005) work on native language identification and show that machine learning-based methods are effective. Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation. Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different task from native language identific</context>
</contexts>
<marker>Koppel, Schler, Zigdon, 2005</marker>
<rawString>Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005. Determining an author’s native language by mining a text for errors. In Proc. of 11th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, pages 624–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred L Kroeber</author>
<author>Charles D Chri´etien</author>
</authors>
<title>Quantitative classification of Indo-European languages.</title>
<date>1937</date>
<journal>Language,</journal>
<volume>13</volume>
<issue>2</issue>
<marker>Kroeber, Chri´etien, 1937</marker>
<rawString>Alfred L. Kroeber and Charles D. Chri´etien. 1937. Quantitative classification of Indo-European languages. Language, 13(2):83–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Edward Whittaker</author>
<author>Vera Sheinman</author>
</authors>
<title>Creating a manually error-tagged and shallow-parsed learner corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1210--1219</pages>
<contexts>
<context position="13985" citStr="Nagata et al., 2011" startWordPosition="2254" endWordPosition="2257">ive English (British and American university students’ essays in the LOCNESS corpus5) and two sets of Japanese English (ICLE and the NICE corpus (Sugiura et al., 2007)). Table 1 shows the statistics on the corpus data. Performance of POS tagging is an important factor in our methods because they are based on word/POS sequences. Existing POS taggers might not perform well on non-native English texts because they are normally developed to analyze native English texts. Considering this, we tested CRFTagger6 on non-native English texts containing various grammatical errors before the experiments (Nagata et al., 2011). It turned out that CRFTagger achieved an accuracy of 0.932 (compared to 0.970 on native texts). Although it did not perform as well as on native texts, it still achieved a fair accuracy. Accordingly, we decided to use it in our experiments. Then, we generated cluster trees from the corpus data using the methods described in Sect. 3. 3For example, because of (iii), essays written by native speakers of Swedish in the Finnish subcorpus were excluded from the experiments. This is because they were collected in Finland and might be influenced by Finnish. 4The symbol ‘ is sometimes used for ’ (e.g</context>
</contexts>
<marker>Nagata, Whittaker, Sheinman, 2011</marker>
<rawString>Ryo Nagata, Edward Whittaker, and Vera Sheinman. 2011. Creating a manually error-tagged and shallow-parsed learner corpus. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1210–1219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luay Nakhleh</author>
<author>Tandy Warnow</author>
<author>Don Ringe</author>
<author>Steven N Evans</author>
</authors>
<title>A comparison of phylogenetic reconstruction methods on an Indo-European dataset.</title>
<date>2005</date>
<journal>Transactions of the Philological Society,</journal>
<volume>103</volume>
<issue>2</issue>
<contexts>
<context position="4592" citStr="Nakhleh et al., 2005" startWordPosition="714" endWordPosition="717"> non-native texts, by examining the hypothesis. This becomes important in native language identification) which is useful for improving grammatical error correction systems (Chodorow et al., 2010) or for providing more targeted feedback to language learners. As we will see in Sect. 6, this paper reveals several crucial findings that contribute to improving native language identification. In addition, this paper shows that the findings could contribute to reconstruction of language family trees (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect.6 discusses implications for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and R</context>
<context position="32412" citStr="Nakhleh et al., 2005" startWordPosition="5316" endWordPosition="5319">gure 7: Distribution of noun-noun compound length for French-English. bilities given by Equation (5)12. This holds for the other Englishes although we cannot show them because of the space limitation. Consequently, Equation (5) should be useful in native language identification. Fortunately, it can be naturally integrated into existing classifiers. In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees. Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction. Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose. These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features. The experimental results in this paper suggest the possibility of the use of non-native texts for reconstructing language family trees. It allows us to use linguistic features that exist between words, as seen in our methods, which has been difficult with previous method</context>
</contexts>
<marker>Nakhleh, Warnow, Ringe, Evans, 2005</marker>
<rawString>Luay Nakhleh, Tandy Warnow, Don Ringe, and Steven N. Evans. 2005. A comparison of phylogenetic reconstruction methods on an Indo-European dataset. Transactions of the Philological Society, 103(2):171–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taraka Rama</author>
<author>Anil Kumar Singh</author>
</authors>
<title>From bag of languages to family trees from noisy corpus.</title>
<date>2009</date>
<booktitle>In Proc. of Recent Advances in Natural Language Processing,</booktitle>
<pages>355--359</pages>
<contexts>
<context position="32534" citStr="Rama and Singh, 2009" startWordPosition="5335" endWordPosition="5338"> other Englishes although we cannot show them because of the space limitation. Consequently, Equation (5) should be useful in native language identification. Fortunately, it can be naturally integrated into existing classifiers. In the domain of historical linguistics, researchers have used computational and corpusbased methods for reconstructing language family trees. Some (Enright and Kondrak, 2011; Gray and Atkinson, 2003; Barbanc¸on et al., 2007; Batagelj et al., 1992; Nakhleh et al., 2005) apply clustering techniques to the task of language family tree reconstruction. Others (Kita, 1999; Rama and Singh, 2009) use corpus statistics for the same purpose. These methods reconstruct language family trees based on linguistic features that exist within words including lexical, phonological, and morphological features. The experimental results in this paper suggest the possibility of the use of non-native texts for reconstructing language family trees. It allows us to use linguistic features that exist between words, as seen in our methods, which has been difficult with previous methods. Language involves the features between words such as phrase construction and syntax as well as the features within word</context>
</contexts>
<marker>Rama, Singh, 2009</marker>
<rawString>Taraka Rama and Anil Kumar Singh. 2009. From bag of languages to family trees from noisy corpus. In Proc. of Recent Advances in Natural Language Processing, pages 355–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Giacalone Ramat</author>
<author>Paolo Ramat</author>
</authors>
<title>The Indo-European Languages.</title>
<date>2006</date>
<publisher>Routledge,</publisher>
<location>New York.</location>
<contexts>
<context position="5203" citStr="Ramat and Ramat, 2006" startWordPosition="812" endWordPosition="815"> al., 2005), which is one of the central tasks in historical linguistics. The rest of this paper is structured as follows. Sect. 2 introduces the basic approach of this work. Sect. 3 discusses the methods in detail. Sect. 4 describes experiments conducted to investigate the hypothesis. Sect. 5 discusses the experimental results. Sect.6 discusses implications for work in related domains. 2 Approach To examine the hypothesis, we reconstruct a language family tree from English texts written by non-native speakers of English whose mother tongue is one of the Indo-European languages (Beekes, 2011; Ramat and Ramat, 2006). If the reconstructed tree is sufficiently similar to the original Indo-European family tree, it will support the hypothesis. If not, it suggests that some features other than mother tongue interference are more influential. The approach we use for reconstructing a language family tree is to apply agglomerative hierarchical clustering (Han and Kamber, 2006) to English texts written by non-native speakers. Researchers have already performed related work on reconstructing language family trees. For instance, Kroeber and Chri´etien (1937) and Elleg˚ard (1959) proposed statistical methods for mea</context>
</contexts>
<marker>Ramat, Ramat, 2006</marker>
<rawString>Anna Giacalone Ramat and Paolo Ramat, 2006. The Indo-European Languages. Routledge, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Snyder</author>
</authors>
<title>The acquisitional role of the syntax-morphology interface: Morphological compounds and syntactic complex predicates.</title>
<date>1996</date>
<booktitle>In Proc. of Annual Boston University Conference on Language Development,</booktitle>
<volume>2</volume>
<pages>728--735</pages>
<contexts>
<context position="27015" citStr="Snyder (1996)" startWordPosition="4439" endWordPosition="4440">han in the others where we define the length as the number of consecutive repetitions of common nouns (for example, the length of orange juice is one because a noun is consecutively repeated once). To see if this is true, we calculated the average length for each English. Fig. 5 shows that the average length roughly distinguishes the Italic Englishes from the other nonnative Englishes; French-English is the shortest, which is explained by the discussion above, while Dutch- and German-Englishes are longest, which may correspond to the fact that they have a preference for noun-noun compounds as Snyder (1996) argues. For instance, German allows the concatenated form as in Orangensaft (equivalently orangejuice). This tendency in the length of nounnoun compounds provides us with a crucial insight for native language identification, which we will Figure 4: Relative frequency of NN of NN in each corpus (%). come back to in Sect. 6. The trigrams BOS RB , in Table 2 and RB . EOS in Table 3 imply that there might also be a certain pattern in adverb position in the 11 Englishes (they roughly correspond to adverbs at the beginning and end of sentences). Fig. 6 shows an insight into this. The horizontal and</context>
</contexts>
<marker>Snyder, 1996</marker>
<rawString>William Snyder. 1996. The acquisitional role of the syntax-morphology interface: Morphological compounds and syntactic complex predicates. In Proc. of Annual Boston University Conference on Language Development, volume 2, pages 728–735.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masatoshi Sugiura</author>
<author>Masumi Narita</author>
</authors>
<title>Tomomi Ishida, Tatsuya Sakaue, Remi Murao, and Kyoko Muraki.</title>
<date>2007</date>
<booktitle>In Proc. of Corpus Linguistics Conference CL2007,</booktitle>
<pages>84--89</pages>
<marker>Sugiura, Narita, 2007</marker>
<rawString>Masatoshi Sugiura, Masumi Narita, Tomomi Ishida, Tatsuya Sakaue, Remi Murao, and Kyoko Muraki. 2007. A discriminant analysis of non-native speakers and native speakers of English. In Proc. of Corpus Linguistics Conference CL2007, pages 84–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Swan</author>
<author>Bernard Smith</author>
</authors>
<date>2001</date>
<booktitle>Learner English (2nd Ed.).</booktitle>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1545" citStr="Swan and Smith (2001)" startWordPosition="229" endWordPosition="232">guage identification. 1 Introduction Transfer of linguistic systems of a mother tongue to another language, namely mother tongue interference, is often observable in the writing of nonnative speakers. The reader may be able to determine the mother tongue of the writer of the following sentence from the underlined article error: The alien wouldn’t use my spaceship but the hers. The answer would probably be French or Spanish; the definite article is allowed to modify possessive pronouns in these languages, and the usage is sometimes negatively transferred to English writing. Researchers such as Swan and Smith (2001), Aarts and Granger (1998), DavidsenNielsen and Harder (2001), and Altenberg and Tapper (1998) work on mother tongue interference to reveal overused/underused words, part of speech (POS), or grammatical items. In contrast, very little is known about how strongly mother tongue interference is transferred to another language and about what relation there is across mother tongues. At one extreme, one could argue that it is so strongly transferred to texts in another language that the linguistic relations between mother tongues are perfectly preserved in the texts. At the other extreme, one can co</context>
<context position="24873" citStr="Swan and Smith, 2001" startWordPosition="4051" endWordPosition="4054">0 0.63 0.88 NN of JJ 0.15 0.15 0.25 0.67 the JJ NNS 0.28 0.28 0.20 0.65 NN to VB 0.40 0.38 0.31 Table 2: Interesting trigrams (French- (Di), Spanish- (Dj), and Russian- (Dk) Englishes). Another interesting trigram, though not as obvious as article use, is NN of NN, which ranks 12th and 2nd in Table 2 and 3, respectively. In the Italic Englishes, the trigram is more frequent than the other non-native Englishes as shown in Fig. 4. This corresponds to the fact that noun-noun compounds are less common in the Italic languages than in English and that instead, the of-phrase (NN of NN) is preferred (Swan and Smith, 2001). For 1142 s Trigram t rti rtj rtk 21.49 the NN of 1.01 0.98 0.54 5.70 NN of NN 0.74 0.70 0.50 3.26 NN of the 0.49 0.52 0.30 3.10 the JJ NN 0.87 0.86 0.70 2.62 ... 0.22 0.12 0.03 1.53 of the NN 0.42 0.44 0.29 1.50 NN , NN 0.30 0.30 0.18 1.50 BOS i VBP 0.07 0.05 0.19 0.85 NNS and NNS 0.09 0.13 0.19 0.81 JJ NN of 0.40 0.39 0.31 0.68 .. EOS 0.13 0.06 0.02 0.63 a JJ NN 0.85 0.77 0.73 0.63 RB . EOS 0.21 0.16 0.31 0.56 NN , the 0.16 0.16 0.08 0.50 NN of a 0.17 0.09 0.06 Italic Germanic Slavic Japanese Japanese1 Japanese2 Bulgarian Polish Russian Czech Norwegian English Swedish German Spanish French </context>
</contexts>
<marker>Swan, Smith, 2001</marker>
<rawString>Michael Swan and Bernard Smith. 2001. Learner English (2nd Ed.). Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
</authors>
<title>Source language markers in EUROPARL translations.</title>
<date>2008</date>
<booktitle>In Proc. of 22nd International Conference on Computational Linguistics,</booktitle>
<pages>937--944</pages>
<marker>van Halteren, 2008</marker>
<rawString>Hans van Halteren. 2008. Source language markers in EUROPARL translations. In Proc. of 22nd International Conference on Computational Linguistics, pages 937–944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng J Wong</author>
<author>Mark Dras</author>
</authors>
<title>Contrastive analysis and native language identification.</title>
<date>2009</date>
<booktitle>In Proc. Australasian Language Technology Workshop,</booktitle>
<pages>53--61</pages>
<contexts>
<context position="2932" citStr="Wong and Dras (2009)" startWordPosition="452" endWordPosition="455"> the distinctive language systems of a mother tongue may be eliminated when transferred to another language from a speaker’s mother tongue. For example, Slavic languages have a rich inflectional case system (e.g., Czech has seven inflectional cases) whereas French does not. However, the difference in the richness cannot be transferred into English because English has almost no inflectional case system. Thus, one cannot determine the mother tongue of a given nonnative text from the inflectional case. A similar argument can be made about some parts of gender, tense, and aspect systems. Besides, Wong and Dras (2009) show that there are no significant differences, between mother tongues, in the misuse of certain syntactic features such as subject-verb agreement that have different tendencies depending on their mother tongues. Considering these, one could not be so sure which argument is correct. In any case, to the best of our knowledge, no one has yet answered this question. In view of this background, we take the first step in addressing this question. We hypothesize that: Hypothesis: Mother tongue interference is so strong that the relations in a language family are preserved in texts written in anothe</context>
<context position="28663" citStr="Wong and Dras (2009)" startWordPosition="4713" endWordPosition="4716">an Czech English Dutch Swedish German Norwegian Japanese1 Japanese2 Germanic Japanese Slavic Italic 1143 Average length of noun-noun compounds Figure 5: Average length of noun-noun compounds in each corpus. 15 20 25 30 Ratio of adverbs at the beginning (%) Figure 6: Distribution of adverb position. trigrams. For instance, although Polish-English is located in the middle of Swedish-English and Bulgarian-English in the distribution of articles (in Fig. 3), the ratios tell us that Polish-English is much nearer to Bulgarian-English. 6 Implications for Work in Related Domains Researchers including Wong and Dras (2009), Wong et al. (2011; 2012), and Koppel et al. (2005) work on native language identification and show that machine learning-based methods are effective. Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation. Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that i</context>
</contexts>
<marker>Wong, Dras, 2009</marker>
<rawString>Sze-Meng J. Wong and Mark Dras. 2009. Contrastive analysis and native language identification. In Proc. Australasian Language Technology Workshop, pages 53–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng J Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Exploiting parse structures for native language identification.</title>
<date>2011</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1600--1611</pages>
<contexts>
<context position="28682" citStr="Wong et al. (2011" startWordPosition="4717" endWordPosition="4720"> Swedish German Norwegian Japanese1 Japanese2 Germanic Japanese Slavic Italic 1143 Average length of noun-noun compounds Figure 5: Average length of noun-noun compounds in each corpus. 15 20 25 30 Ratio of adverbs at the beginning (%) Figure 6: Distribution of adverb position. trigrams. For instance, although Polish-English is located in the middle of Swedish-English and Bulgarian-English in the distribution of articles (in Fig. 3), the ratios tell us that Polish-English is much nearer to Bulgarian-English. 6 Implications for Work in Related Domains Researchers including Wong and Dras (2009), Wong et al. (2011; 2012), and Koppel et al. (2005) work on native language identification and show that machine learning-based methods are effective. Wong and Dras (2009) propose using information about grammatical errors such as errors in determiners to achieve better performance while they show that its use does not improve the performance, contrary to the expectation. Related to this, other researchers (Koppel and Ordan, 2011; van Halteren, 2008) show that machine learning-based methods can also predict the source language of a given translated text although it should be emphasized that it is a different ta</context>
</contexts>
<marker>Wong, Dras, Johnson, 2011</marker>
<rawString>Sze-Meng J. Wong, Mark Dras, and Mark Johnson. 2011. Exploiting parse structures for native language identification. In Proc. Conference on Empirical Methods in Natural Language Processing, pages 1600–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng J Wong</author>
<author>Mark Dras</author>
<author>Mark Johnson</author>
</authors>
<title>Exploring adaptor grammars for native language identification.</title>
<date>2012</date>
<booktitle>In Proc. Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>699--709</pages>
<marker>Wong, Dras, Johnson, 2012</marker>
<rawString>Sze-Meng J. Wong, Mark Dras, and Mark Johnson. 2012. Exploring adaptor grammars for native language identification. In Proc. Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 699–709.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>