<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000401">
<title confidence="0.9991205">
Distributional Neural Networks for
Automatic Resolution of Crossword Puzzles
</title>
<author confidence="0.999405">
Aliaksei Severyn*, Massimo Nicosia, Gianni Barlacchi, Alessandro Moschitti†
</author>
<affiliation confidence="0.943418666666667">
DISI - University of Trento, Italy
†Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar
*Google Inc.
</affiliation>
<email confidence="0.998781">
{aseveryn,gianni.barlacchi,m.nicosia,amoschitti}@gmail.com
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999786380952381">
Automatic resolution of Crossword Puz-
zles (CPs) heavily depends on the qual-
ity of the answer candidate lists produced
by a retrieval system for each clue of the
puzzle grid. Previous work has shown
that such lists can be generated using In-
formation Retrieval (IR) search algorithms
applied to the databases containing previ-
ously solved CPs and reranked with tree
kernels (TKs) applied to a syntactic tree
representation of the clues. In this pa-
per, we create a labelled dataset of 2 mil-
lion clues on which we apply an innovative
Distributional Neural Network (DNN) for
reranking clue pairs. Our DNN is com-
putationally efficient and can thus take ad-
vantage of such large datasets showing a
large improvement over the TK approach,
when the latter uses small training data. In
contrast, when data is scarce, TKs outper-
form DNNs.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99415484">
Automatic solvers of CPs require accurate list of
answer candidates to find good solutions in little
time. Candidates can be retrieved from the DBs
of previously solved CPs (CPDBs) since clues are
often reused, and thus querying CPDBs with the
target clue allows us to recuperate the same (or
similar) clues.
In this paper, we propose for the first time the
use of Distributional Neural Networks to improve
the ranking of answer candidate lists. Most im-
portantly, we build a very large dataset for clue
retrieval, composed of 2,000,493 clues with their
associated answers, i.e., this is a supervised cor-
pus where large scale learning models can be de-
veloped and tested. This dataset is an interesting
∗Work done when student at University of Trento
resource that we make available to the research
community1. To assess the effectiveness of our
DNN model, we compare it with the current state
of the art model (Nicosia et al., 2015) in rerank-
ing CP clues, where tree kernels (Moschitti, 2006)
are used to rerank clues according to their syntac-
tic/semantic similarity with the query clue.
The experimental results on our dataset demon-
strate that:
</bodyText>
<listItem confidence="0.961422111111111">
(i) DNNs are efficient and can greatly benefit
from large amounts of data;
(ii) when DNNs are applied to large-scale data,
they largely outperform traditional feature-
based rerankers as well as kernel-based mod-
els; and
(iii) if limited training data is available for train-
ing, tree kernel-based models are more accu-
rate than DNNs
</listItem>
<sectionHeader confidence="0.930572" genericHeader="method">
2 Clue Reranking Models for CPs
</sectionHeader>
<bodyText confidence="0.999649">
In this section, we briefly introduce the general
idea of CP resolution systems and the state-of-the-
art models for reranking answer candidates.
</bodyText>
<subsectionHeader confidence="0.982762">
2.1 CP resolution systems
</subsectionHeader>
<bodyText confidence="0.999907416666667">
The main task of a CP resolution system is the
generation of candidate answer lists for each clue
of the target puzzle (Littman et al., 2002). Then
a solver for Probabilistic-Constraint Satisfaction
Problems, e.g., (Pohl, 1970), tries combinations
of letters that satisfy the crossword constraints.
The combinations are derived from words found
in dictionaries or in the lists of answer candidates.
The latter can be generated using large crossword
databases as well as several expert modules ac-
cessing domain-specific databases (e.g., movies,
writers and geography). WebCrow, one of the
</bodyText>
<footnote confidence="0.9960365">
1http://ikernels-portal.disi.unitn.it/
projects/webcrow/
</footnote>
<page confidence="0.643459">
199
</page>
<note confidence="0.248137333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 199–204,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.5368165">
Rank Clue Answer
1 Actress Pflug who played Lt. Dish in ”MASH” Jo Ann
2 Actress Pflug who played in ”MASH” (1970) Jo Ann
3 Actress Jo Ann Pflug
4 MASH Actress Jo Ann Pflug
5 MASH Crush
</figure>
<tableCaption confidence="0.996353">
Table 1: Candidate list for the query clue: Jo Ann
</tableCaption>
<bodyText confidence="0.950404133333333">
who played Lt. ”Dish” in 1970’s ”MASH” (an-
swer: Pflug)
best systems (Ernandes et al., 2005), incorporates
knowledge sources and an effective clue retrieval
model from DB. It carries out basic linguistic anal-
ysis such as part-of-speech tagging and lemmati-
zation and takes advantage of semantic relations
contained in WordNet, dictionaries and gazetteers.
It also uses a Web module constituted by a search
engine (SE), which can retrieve text snippets re-
lated to the clue.
Clearly, lists of better quality, i.e., many correct
candidates in top positions, result in higher accu-
racy and speed of the solver. Thus the design of
effective answer rankers is extremely important.
</bodyText>
<subsectionHeader confidence="0.999926">
2.2 Clue retrieval and reranking
</subsectionHeader>
<bodyText confidence="0.999974333333333">
One important source of candidate answers is the
DB of previously solved clues. In (Barlacchi et
al., 2014a), we proposed the BM25 retrieval model
to generate clue lists, which were further refined
by applying our reranking models. The latter pro-
mote the most similar, which are probably asso-
ciated with the same answer of the query clue, to
the top. The reranking step is important because
SEs often fail to retrieve the correct clues in the
first position. For example, Table 1 shows the first
five clues retrieved for the query clue: Jo Ann who
played Lt. ”Dish” in 1970’s ”MASH”. BM25 re-
trieved the wrong clue, Actress Pflug who played
Lt. Dish in ”MASH”, at the top since it has a larger
bag-of-words overlap with the query clue.
</bodyText>
<subsectionHeader confidence="0.999944">
2.3 Reranking with Kernels
</subsectionHeader>
<bodyText confidence="0.998767545454545">
We applied our reranking framework for question
answering systems (Moschitti, 2008; Severyn and
Moschitti, 2012; Severyn et al., 2013a; Severyn
et al., 2013b; Severyn and Moschitti, 2013). This
retrieves a list of related clues by using the tar-
get clue as a query in an SE (applied to the Web
or to a DB). Then, both query and candidates are
represented by shallow syntactic structures (gen-
erated by running a set of NLP parsers) and tradi-
tional similarity features which are fed to a kernel-
based reranker. Hereafter, we give a brief descrip-
tion of our models for clue reranking whereas the
reader can refer to our previous work (Barlacchi
et al., 2014a; Nicosia et al., 2015; Barlacchi et al.,
2014b) for more specific details.
Given a query clue qc and two retrieved clues
c1, c2, we can rank them by using a classifi-
cation approach: the two clues c1 and c2 are
reranked by comparing their classification scores:
SVM((q, c1)) and SVM((q, c2)). The SVM classi-
fier uses the following kernel applied to two pairs
of query/clues, p = (q, ci) and p0 = (q0,c0j):
</bodyText>
<equation confidence="0.983825333333333">
K(p, p0) = TK(q, q0) + TK(ci,c0j)+
FV (q, ci) · FV (q0,
c0j),
</equation>
<bodyText confidence="0.9999882">
where TK can be any tree kernel, e.g., the syntac-
tic tree kernel (STK) also called SST by Moschitti
(2006), and FV is the feature vector representation
of the input pair, e.g., (q, ci) or (q0,c0j). STK maps
trees into the space of all possible tree fragments
constrained by the rule that the sibling nodes from
their parents cannot be separated. It enables the
exploitation of structural features, which can be
effectively combined with more traditional fea-
tures (described hereafter).
Feature Vectors (FV). We compute the following
similarity features between clues: (i) tree kernel
similarity applied to intra-pairs, i.e., between the
query and the retrieved clues; (ii) DKPro Simi-
larity, which defines features used in the context
of the Semantic Textual Similarity (STS) chal-
lenge (B¨ar et al., 2013); and (iii) WebCrow fea-
tures (WC), which are the similarity measures
computed on the clue pairs by WebCrow (using
the Levenshtein distance) and the SE score.
</bodyText>
<sectionHeader confidence="0.9528095" genericHeader="method">
3 Distributional models for clue
reranking
</sectionHeader>
<bodyText confidence="0.999012">
The architecture of our distributional matching
model for measuring similarity between clues is
presented in Fig. 1. Its main components are:
</bodyText>
<listItem confidence="0.998451142857143">
(i) sentence matrices sci E Rd×|ci |obtained by
the concatenation of the word vectors wj E
Rd (with d being the size of the embeddings)
of the corresponding words wj from the input
clues ci;
(ii) a distributional sentence model
f : Rd×|ci |—* Rm that maps the sentence
</listItem>
<page confidence="0.990649">
200
</page>
<figureCaption confidence="0.999908">
Figure 1: Distributional sentence matching model for computing similarity between clues.
</figureCaption>
<bodyText confidence="0.999871930232558">
matrix of an input clue ci to a fixed-size
vector representations xci of size m;
(iii) a layer for computing the similarity between
the obtained intermediate vector representa-
tions of the input clues, using a similarity ma-
trix M E Rm×m – an intermediate vector
representation xc1 of a clue ci is projected to
a ˜xc1 = xc1M, which is then matched with
xc2 (Bordes et al., 2014), i.e., by computing a
dot-product ˜xc1xc2, thus resulting in a single
similarity score xsim;
(vi) a set of fully-connected hidden layers that
models the similarity between clues using
their vector representations produced by the
sentence model (also integrating the single
similarity score from the previous layer); and
(v) a softmax layer that outputs probability
scores reflecting how well the clues match
with each other.
The choice of the sentence model plays a cru-
cial role as the resulting intermediate representa-
tions of the input clues will affect the successive
step of computing their similarity. Recently, dis-
tributional sentence models, where f(s) is rep-
resented by a sequence of convolutional-pooling
feature maps, have shown state-of-the-art results
on many NLP tasks, e.g., (Kalchbrenner et al.,
2014; Kim, 2014). In this paper, we opt for a sim-
ple solution where f(sci) = Ei wi/|ci|, i.e., the
word vectors, are averaged to a single fixed-sized
vector x E Rd. Our preliminary experiments re-
vealed that this simpler model works just as well
as more complicated single or multi-layer convo-
lutional architectures. We conjecture that this is
largely due to the nature of the language used in
clues, which is very dense and where the syntactic
information plays a minor role.
Considering recent deep learning models for
matching sentences, our network is most similar
to the models in Hu et al. (2014) applied for com-
puting sentence similarity and in Yu et al.(2014)
(answer sentence selection in Question Answer-
ing) with the following differences:
</bodyText>
<listItem confidence="0.981991">
(i) In contrast to more complex convolutional
sentence models explored in (Hu et al., 2014)
and in (Yu et al., 2014), our sentence model
is composed of a single averaging operation.
(ii) To compute the similarity between the vec-
tor representation of the input sentences, our
network uses two methods: (i) computing the
similarity score obtained by transforming one
clue into another using a similarity matrix M
(explored in (Yu et al., 2014)), and (ii) di-
rectly modelling interactions between inter-
mediate vector representations of the input
</listItem>
<page confidence="0.992015">
201
</page>
<bodyText confidence="0.626948">
clues via fully-connected hidden layers (used
by (Hu et al., 2014)).
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999901">
Our experiments compare different ranking mod-
els, i.e., BM25 as the IR baseline, and several
rerankers, and our distributional neural network
(DNN) for the task of clue reranking.
</bodyText>
<subsectionHeader confidence="0.988814">
4.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999977625">
Data. We compiled our crossword corpus combin-
ing (i) CPs downloaded from the Web2 and (ii) the
clue database provided by Otsys3. We removed
duplicates, fill-in-the-blank clues (which are better
solved by using other strategies) and clues repre-
senting anagrams or linguistic games.
We collected over 6.3M pairs of clue/answer
and after removal of duplicates, we obtained a
compressed dataset containing 2M unique and
standard clues, with associated answers, which we
called CPDB. We used these clues to build a Small
Dataset (SD) and a Large Dataset (LD) for rerank-
ing. The two datasets are based on pairs of clues:
query and retrieved clues. Such clues are retrieved
using a BM25 model on CPDB.
For creating SD, we used 8k clues that (i) were
randomly extracting from CPDB and (ii) satisfy-
ing the property that at least one correct clue (i.e.,
having the same answer of the query clue) is in
the first retrieved 10 clues (of course the query
clue is eliminated from the ranked list provided
by BM25). In total we got about 120K examples,
84,040 negative and 35,960 positive clue4.
For building LD, we collected 200k clues with
the same property above. More precisely we
obtained 1,999,756 pairs (10×200k minus few
problematic examples) with 599,025 positive and
140,0731 negative pairs of queries with their re-
trieved clues. Given the large number of examples,
we only used such dataset in classification modal-
ity, i.e., we did not form reranking examples (pairs
of pairs).
</bodyText>
<footnote confidence="0.956035">
2http://www.crosswordgiant.com
3http://www.otsys.com/clue
4A true reranker should be built using pairs of clue pairs,
</footnote>
<bodyText confidence="0.999553641025641">
where the positive pairs are those having the correct pair as
the first member. This led to form 127,109 reranking exam-
ples, with 66,011 positive and 61,098 negative pairs. How-
ever, in some experiments, which we do not report in the
paper, we observed that the performance both of the simple
classifier as well as the true reranker were similar, thus we
decided to use the simpler classifier.
Structural model. We use SVM-light-TK5,
which enables the use of structural kernels (Mos-
chitti, 2006). We applied structural kernels to shal-
low tree representations and a polynomial kernel
of degree 3 to feature vectors (FV).
Distributional neural network model. We pre-
initialize the word embeddings by running the
word2vec tool (Mikolov et al., 2013) on the En-
glish Wikipedia dump. We opt for a skipgram
model with window size 5 and filtering words with
frequency less than 5. The dimensionality of the
embeddings is set to 50. The input sentences are
mapped to fixed-sized vectors by computing the
average of their word embeddings. We use a sin-
gle non-linear hidden layer (with rectified linear
(ReLU) activation function) whose size is equal to
the size of the previous layer.
The network is trained using SGD with shuf-
fled mini-batches using the Adagrad update
rule (Duchi et al., 2011). The batch size is set to
100 examples. We used 25 epochs with early stop-
ping, i.e., we stop the training if no update to the
best accuracy on the dev set (we create the dev
set by allocating 10% of the training set) is made
for the last 5 epochs. The accuracy computed on
the dev set is the Mean Average Precision (MAP)
score. To extract the DNN features we simply take
the output of the hidden layer just before the soft-
max.
Evaluation. We used standard metrics widely
used in QA: the Mean Reciprocal Rank (MRR)
and Mean Average Precision (MAP).
</bodyText>
<sectionHeader confidence="0.523212" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9197728">
Table 2 summarizes the results of our different
reranking models trained on a small dataset (SD)
of 120k examples and a large dataset (LD) with
2M examples.
The first column reports the BM25 result; the
second column shows the performance of SVM
perf (SVMP), which is a very fast variant of SVM,
using FV; the third column reports the state-of-the-
art model for crossword clue reranking (Nicosia et
al., 2015), which uses FV vector and tree kernels,
i.e., SVM(TK).
Regarding the other systems: DNNMSD is the
DNN model trained on the small data (SD) of
120k training pairs; SVMp(DNNFLD) is SVM
perf trained with (i) the features derived from
</bodyText>
<footnote confidence="0.986387">
5http://disi.unitn.it/moschitti/
Tree-Kernel.htm
</footnote>
<page confidence="0.988642">
202
</page>
<table confidence="0.97646075">
Training classifiers with the Small Dataset (SD) (120K instances)
BM25 SVMp SVM(TK) DNNMSD SVMp(DNNFLD) SVM(DNNFLD,TK)
MRR 37.57 41.95 43.59 40.08 46.12 45.50
MAP 27.76 30.06 31.79 28.25 33.75 33.71
Training classifiers with the Large Dataset (LD) (2 million instances)
BM25 SVMp SVM(TK) DNNMLD SVMp(DNNFLD,−FV) SVMp(DNNFLD)
MRR 37.57 41.47 – 46.10 46.36 46.27
MAP 27.76 29.95 – 33.81 34.07 33.86
</table>
<tableCaption confidence="0.923695">
Table 2: SVM models and DNN trained on 120k (small dataset) and 2 millions (large dataset) examples.
Feature vectors are used with all models except when indicated by −FV
</tableCaption>
<bodyText confidence="0.8577315">
DNN trained on a large clue dataset LD and (ii)
the FV; and finally, SVM(DNNFLD,TK) is SVM
using DNN features (generated from LD), FV and
TK. It should be noted that:
</bodyText>
<equation confidence="0.296638">
(i) SVMp is largely improved by TK;
</equation>
<bodyText confidence="0.943369756756757">
(ii) DNNMSD on relatively small data delivers
an accuracy lower than FV;
(iii) if SVMp is trained with DNNMLD, i.e., fea-
tures derived from the dataset of 2M clues,
the accuracy greatly increases; and
(iv) finally, the combination with TK, i.e.,
SVM(DNNFLD,TK), does not significantly
improve the previous results.
In summary, when a dataset is relatively small
DNNM fails to deliver any noticeable improve-
ment over the SE baseline even when combined
with additional similarity features. SVM and
TK models generalize much better on the smaller
dataset.
Additionally, it is interesting to see that training
an SVM on a small number of examples enriched
with the features produced by a DNN trained on
large data gives us the same results of DNN trained
on the large dataset. Hence, it is desired to use
larger training collections to build an accurate
distributional similarity matching model that can
be then effectively combined with other feature-
based or tree kernel models, although at the mo-
ment the combination does not significantly im-
prove TK models.
Regarding the LD training setting it can be ob-
served that:
(i) the second column shows that adding more
training examples to SVMp does not increase
accuracy (compared with SD result);
(ii) DNNMLD delivers high accuracy suggesting
that a large dataset is essential to its training;
and
(iii) again SVMp using DNN features deliver
state-of-the-art accuracy independently of us-
ing or not additional features (i.e., see −FV,
which excludes the latter).
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999946631578947">
In this paper, we have explored various reranker
models to improve automatic CP resolution. The
most important finding is that our distributional
neural network model is very effective in estab-
lishing similarity matching between clues. We
combine the features produced by our DNN model
with other rerankers to greatly improve over the
previous state-of-the-art results. Finally, we col-
lected a very large dataset composed of 2 millions
clue/answer pairs that can be useful to the NLP
community for developing semantic textual simi-
larity models.
Future research will be devoted to find models
to effectively combine TKs and DNN. In partic-
ular, our previous model exploiting Linked Open
Data in QA (Tymoshenko et al., 2014) seems very
promising to find correct answer to clues. This as
well as further research will be integrated in our
CP system described in (Barlacchi et al., 2015).
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99416125">
This work was supported by the EC project
CogNet, 671625 (H2020-ICT-2014-2). The first
author was supported by the Google Europe Doc-
toral Fellowship Award 2013.
</bodyText>
<page confidence="0.998528">
203
</page>
<sectionHeader confidence="0.996354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999728284313726">
Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013.
DKPro similarity: An open source framework for
text similarity. In Proceedings of ACL (System
Demonstrations).
Gianni Barlacchi, Massimo Nicosia, and Alessandro
Moschitti. 2014a. Learning to rank answer can-
didates for automatic resolution of crossword puz-
zles. In Proceedings of the Eighteenth Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Gianni Barlacchi, Massimo Nicosia, and Alessandro
Moschitti. 2014b. A retrieval model for automatic
resolution of crossword puzzles in italian language.
In The First Italian Conference on Computational
Linguistics CLiC-it 2014.
Gianni Barlacchi, Massimo Nicosia, and Alessandro
Moschitti. 2015. SACRY: Syntax-based automatic
crossword puzzle resolution system. In Proceed-
ings of 53nd Annual Meeting of the Association for
Computational Linguistics: System Demonstrations,
Beijing, China, July. Association for Computational
Linguistics.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 615–620, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159.
Marco Ernandes, Giovanni Angelini, and Marco Gori.
2005. Webcrow: A web-based system for crossword
solving. In In Proc. of AAAI 05, pages 1412–1417.
Menlo Park, Calif., AAAI Press.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
NIPS.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics, June.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. Doha, Qatar.
Michael L. Littman, Greg A. Keim, and Noam Shazeer.
2002. A probabilistic approach to solving crossword
puzzles. Artificial Intelligence, 134(12):23 – 55.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In ECML, pages 318–329.
Alessandro Moschitti. 2008. Kernel methods, syn-
tax and semantics for relational text categorization.
In Proceedings of the 17th ACM Conference on In-
formation and Knowledge Management, CIKM ’08,
pages 253–262, Napa Valley, California, USA.
Massimo Nicosia, Gianni Barlacchi, and Alessandro
Moschitti. 2015. Learning to rank aggregated an-
swers for crossword puzzles. In Allan Hanbury,
Gabriella Kazai, Andreas Rauber, and Norbert Fuhr,
editors, Advances in Information Retrieval - 37th
European Conference on IR Research, ECIR, Vi-
enna, Austria. Proceedings, volume 9022 of Lecture
Notes in Computer Science, pages 556–561.
Ira Pohl. 1970. Heuristic search viewed as path finding
in a graph. Artificial Intelligence, 1(34):193 – 204.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of ACM SIGIR,
New York, NY, USA.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 458–467, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In CIKM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 75–83, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Kateryna Tymoshenko, Alessandro Moschitti, and Ali-
aksei Severyn. 2014. Encoding semantic resources
in syntactic structures for passage reranking. In
Proceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 664–672, Gothenburg, Sweden,
April. Association for Computational Linguistics.
Lei Yu, Karl Moritz Hermann, Phil Blunsom, and
Stephen Pulman. 2014. Deep learning for answer
sentence selection. CoRR.
</reference>
<page confidence="0.998895">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.904722">
<title confidence="0.997176">Distributional Neural Networks Automatic Resolution of Crossword Puzzles</title>
<author confidence="0.997725">Massimo Nicosia</author>
<author confidence="0.997725">Gianni Barlacchi</author>
<author confidence="0.997725">Alessandro</author>
<affiliation confidence="0.998976">DISI - University of Trento, Computing Research Institute, Hamad Bin Khalifa University,</affiliation>
<abstract confidence="0.996043636363636">Automatic resolution of Crossword Puzzles (CPs) heavily depends on the quality of the answer candidate lists produced by a retrieval system for each clue of the puzzle grid. Previous work has shown that such lists can be generated using Information Retrieval (IR) search algorithms applied to the databases containing previously solved CPs and reranked with tree kernels (TKs) applied to a syntactic tree representation of the clues. In this paper, we create a labelled dataset of 2 million clues on which we apply an innovative Distributional Neural Network (DNN) for reranking clue pairs. Our DNN is computationally efficient and can thus take advantage of such large datasets showing a large improvement over the TK approach, when the latter uses small training data. In contrast, when data is scarce, TKs outperform DNNs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro similarity: An open source framework for text similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL (System Demonstrations).</booktitle>
<marker>B¨ar, Zesch, Gurevych, 2013</marker>
<rawString>Daniel B¨ar, Torsten Zesch, and Iryna Gurevych. 2013. DKPro similarity: An open source framework for text similarity. In Proceedings of ACL (System Demonstrations).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianni Barlacchi</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning to rank answer candidates for automatic resolution of crossword puzzles.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4831" citStr="Barlacchi et al., 2014" startWordPosition="757" endWordPosition="760">ic linguistic analysis such as part-of-speech tagging and lemmatization and takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. It also uses a Web module constituted by a search engine (SE), which can retrieve text snippets related to the clue. Clearly, lists of better quality, i.e., many correct candidates in top positions, result in higher accuracy and speed of the solver. Thus the design of effective answer rankers is extremely important. 2.2 Clue retrieval and reranking One important source of candidate answers is the DB of previously solved clues. In (Barlacchi et al., 2014a), we proposed the BM25 retrieval model to generate clue lists, which were further refined by applying our reranking models. The latter promote the most similar, which are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words </context>
<context position="6139" citStr="Barlacchi et al., 2014" startWordPosition="985" endWordPosition="988">work for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Given a query clue qc and two retrieved clues c1, c2, we can rank them by using a classification approach: the two clues c1 and c2 are reranked by comparing their classification scores: SVM((q, c1)) and SVM((q, c2)). The SVM classifier uses the following kernel applied to two pairs of query/clues, p = (q, ci) and p0 = (q0,c0j): K(p, p0) = TK(q, q0) + TK(ci,c0j)+ FV (q, ci) · FV (q0, c0j), where TK can be any tree kernel, e.g., the syntactic tree kernel (STK) also called SST by Moschitti (2006), and FV is the feature v</context>
</contexts>
<marker>Barlacchi, Nicosia, Moschitti, 2014</marker>
<rawString>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2014a. Learning to rank answer candidates for automatic resolution of crossword puzzles. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianni Barlacchi</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A retrieval model for automatic resolution of crossword puzzles in italian language.</title>
<date>2014</date>
<booktitle>In The First Italian Conference on Computational Linguistics CLiC-it</booktitle>
<contexts>
<context position="4831" citStr="Barlacchi et al., 2014" startWordPosition="757" endWordPosition="760">ic linguistic analysis such as part-of-speech tagging and lemmatization and takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. It also uses a Web module constituted by a search engine (SE), which can retrieve text snippets related to the clue. Clearly, lists of better quality, i.e., many correct candidates in top positions, result in higher accuracy and speed of the solver. Thus the design of effective answer rankers is extremely important. 2.2 Clue retrieval and reranking One important source of candidate answers is the DB of previously solved clues. In (Barlacchi et al., 2014a), we proposed the BM25 retrieval model to generate clue lists, which were further refined by applying our reranking models. The latter promote the most similar, which are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words </context>
<context position="6139" citStr="Barlacchi et al., 2014" startWordPosition="985" endWordPosition="988">work for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Given a query clue qc and two retrieved clues c1, c2, we can rank them by using a classification approach: the two clues c1 and c2 are reranked by comparing their classification scores: SVM((q, c1)) and SVM((q, c2)). The SVM classifier uses the following kernel applied to two pairs of query/clues, p = (q, ci) and p0 = (q0,c0j): K(p, p0) = TK(q, q0) + TK(ci,c0j)+ FV (q, ci) · FV (q0, c0j), where TK can be any tree kernel, e.g., the syntactic tree kernel (STK) also called SST by Moschitti (2006), and FV is the feature v</context>
</contexts>
<marker>Barlacchi, Nicosia, Moschitti, 2014</marker>
<rawString>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2014b. A retrieval model for automatic resolution of crossword puzzles in italian language. In The First Italian Conference on Computational Linguistics CLiC-it 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gianni Barlacchi</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>SACRY: Syntax-based automatic crossword puzzle resolution system.</title>
<date>2015</date>
<booktitle>In Proceedings of 53nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<marker>Barlacchi, Nicosia, Moschitti, 2015</marker>
<rawString>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2015. SACRY: Syntax-based automatic crossword puzzle resolution system. In Proceedings of 53nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Jason Weston</author>
</authors>
<title>Question answering with subgraph embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>615--620</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="8493" citStr="Bordes et al., 2014" startWordPosition="1383" endWordPosition="1386">f the embeddings) of the corresponding words wj from the input clues ci; (ii) a distributional sentence model f : Rd×|ci |—* Rm that maps the sentence 200 Figure 1: Distributional sentence matching model for computing similarity between clues. matrix of an input clue ci to a fixed-size vector representations xci of size m; (iii) a layer for computing the similarity between the obtained intermediate vector representations of the input clues, using a similarity matrix M E Rm×m – an intermediate vector representation xc1 of a clue ci is projected to a ˜xc1 = xc1M, which is then matched with xc2 (Bordes et al., 2014), i.e., by computing a dot-product ˜xc1xc2, thus resulting in a single similarity score xsim; (vi) a set of fully-connected hidden layers that models the similarity between clues using their vector representations produced by the sentence model (also integrating the single similarity score from the previous layer); and (v) a softmax layer that outputs probability scores reflecting how well the clues match with each other. The choice of the sentence model plays a crucial role as the resulting intermediate representations of the input clues will affect the successive step of computing their simi</context>
</contexts>
<marker>Bordes, Chopra, Weston, 2014</marker>
<rawString>Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="13761" citStr="Duchi et al., 2011" startWordPosition="2237" endWordPosition="2240">itialize the word embeddings by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump. We opt for a skipgram model with window size 5 and filtering words with frequency less than 5. The dimensionality of the embeddings is set to 50. The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings. We use a single non-linear hidden layer (with rectified linear (ReLU) activation function) whose size is equal to the size of the previous layer. The network is trained using SGD with shuffled mini-batches using the Adagrad update rule (Duchi et al., 2011). The batch size is set to 100 examples. We used 25 epochs with early stopping, i.e., we stop the training if no update to the best accuracy on the dev set (we create the dev set by allocating 10% of the training set) is made for the last 5 epochs. The accuracy computed on the dev set is the Mean Average Precision (MAP) score. To extract the DNN features we simply take the output of the hidden layer just before the softmax. Evaluation. We used standard metrics widely used in QA: the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP). 4.2 Results Table 2 summarizes the results of our d</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Ernandes</author>
<author>Giovanni Angelini</author>
<author>Marco Gori</author>
</authors>
<title>Webcrow: A web-based system for crossword solving. In</title>
<date>2005</date>
<booktitle>In Proc. of AAAI 05,</booktitle>
<pages>1412--1417</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, Calif.,</location>
<contexts>
<context position="4111" citStr="Ernandes et al., 2005" startWordPosition="643" endWordPosition="646">jects/webcrow/ 199 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 199–204, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Rank Clue Answer 1 Actress Pflug who played Lt. Dish in ”MASH” Jo Ann 2 Actress Pflug who played in ”MASH” (1970) Jo Ann 3 Actress Jo Ann Pflug 4 MASH Actress Jo Ann Pflug 5 MASH Crush Table 1: Candidate list for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH” (answer: Pflug) best systems (Ernandes et al., 2005), incorporates knowledge sources and an effective clue retrieval model from DB. It carries out basic linguistic analysis such as part-of-speech tagging and lemmatization and takes advantage of semantic relations contained in WordNet, dictionaries and gazetteers. It also uses a Web module constituted by a search engine (SE), which can retrieve text snippets related to the clue. Clearly, lists of better quality, i.e., many correct candidates in top positions, result in higher accuracy and speed of the solver. Thus the design of effective answer rankers is extremely important. 2.2 Clue retrieval </context>
</contexts>
<marker>Ernandes, Angelini, Gori, 2005</marker>
<rawString>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. Webcrow: A web-based system for crossword solving. In In Proc. of AAAI 05, pages 1412–1417. Menlo Park, Calif., AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9906" citStr="Hu et al. (2014)" startWordPosition="1610" endWordPosition="1613">renner et al., 2014; Kim, 2014). In this paper, we opt for a simple solution where f(sci) = Ei wi/|ci|, i.e., the word vectors, are averaged to a single fixed-sized vector x E Rd. Our preliminary experiments revealed that this simpler model works just as well as more complicated single or multi-layer convolutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role. Considering recent deep learning models for matching sentences, our network is most similar to the models in Hu et al. (2014) applied for computing sentence similarity and in Yu et al.(2014) (answer sentence selection in Question Answering) with the following differences: (i) In contrast to more complex convolutional sentence models explored in (Hu et al., 2014) and in (Yu et al., 2014), our sentence model is composed of a single averaging operation. (ii) To compute the similarity between the vector representation of the input sentences, our network uses two methods: (i) computing the similarity score obtained by transforming one clue into another using a similarity matrix M (explored in (Yu et al., 2014)), and (ii)</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="9309" citStr="Kalchbrenner et al., 2014" startWordPosition="1507" endWordPosition="1510">vector representations produced by the sentence model (also integrating the single similarity score from the previous layer); and (v) a softmax layer that outputs probability scores reflecting how well the clues match with each other. The choice of the sentence model plays a crucial role as the resulting intermediate representations of the input clues will affect the successive step of computing their similarity. Recently, distributional sentence models, where f(s) is represented by a sequence of convolutional-pooling feature maps, have shown state-of-the-art results on many NLP tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014). In this paper, we opt for a simple solution where f(sci) = Ei wi/|ci|, i.e., the word vectors, are averaged to a single fixed-sized vector x E Rd. Our preliminary experiments revealed that this simpler model works just as well as more complicated single or multi-layer convolutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role. Considering recent deep learning models for matching sentences, our network is most similar to the models in Hu et al. (2014) ap</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<location>Doha, Qatar.</location>
<contexts>
<context position="9321" citStr="Kim, 2014" startWordPosition="1511" endWordPosition="1512">uced by the sentence model (also integrating the single similarity score from the previous layer); and (v) a softmax layer that outputs probability scores reflecting how well the clues match with each other. The choice of the sentence model plays a crucial role as the resulting intermediate representations of the input clues will affect the successive step of computing their similarity. Recently, distributional sentence models, where f(s) is represented by a sequence of convolutional-pooling feature maps, have shown state-of-the-art results on many NLP tasks, e.g., (Kalchbrenner et al., 2014; Kim, 2014). In this paper, we opt for a simple solution where f(sci) = Ei wi/|ci|, i.e., the word vectors, are averaged to a single fixed-sized vector x E Rd. Our preliminary experiments revealed that this simpler model works just as well as more complicated single or multi-layer convolutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role. Considering recent deep learning models for matching sentences, our network is most similar to the models in Hu et al. (2014) applied for co</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Littman</author>
<author>Greg A Keim</author>
<author>Noam Shazeer</author>
</authors>
<title>A probabilistic approach to solving crossword puzzles.</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>134</volume>
<issue>12</issue>
<contexts>
<context position="3000" citStr="Littman et al., 2002" startWordPosition="477" endWordPosition="480"> large amounts of data; (ii) when DNNs are applied to large-scale data, they largely outperform traditional featurebased rerankers as well as kernel-based models; and (iii) if limited training data is available for training, tree kernel-based models are more accurate than DNNs 2 Clue Reranking Models for CPs In this section, we briefly introduce the general idea of CP resolution systems and the state-of-theart models for reranking answer candidates. 2.1 CP resolution systems The main task of a CP resolution system is the generation of candidate answer lists for each clue of the target puzzle (Littman et al., 2002). Then a solver for Probabilistic-Constraint Satisfaction Problems, e.g., (Pohl, 1970), tries combinations of letters that satisfy the crossword constraints. The combinations are derived from words found in dictionaries or in the lists of answer candidates. The latter can be generated using large crossword databases as well as several expert modules accessing domain-specific databases (e.g., movies, writers and geography). WebCrow, one of the 1http://ikernels-portal.disi.unitn.it/ projects/webcrow/ 199 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and </context>
</contexts>
<marker>Littman, Keim, Shazeer, 2002</marker>
<rawString>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(12):23 – 55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="13222" citStr="Mikolov et al., 2013" startWordPosition="2144" endWordPosition="2147">, with 66,011 positive and 61,098 negative pairs. However, in some experiments, which we do not report in the paper, we observed that the performance both of the simple classifier as well as the true reranker were similar, thus we decided to use the simpler classifier. Structural model. We use SVM-light-TK5, which enables the use of structural kernels (Moschitti, 2006). We applied structural kernels to shallow tree representations and a polynomial kernel of degree 3 to feature vectors (FV). Distributional neural network model. We preinitialize the word embeddings by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump. We opt for a skipgram model with window size 5 and filtering words with frequency less than 5. The dimensionality of the embeddings is set to 50. The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings. We use a single non-linear hidden layer (with rectified linear (ReLU) activation function) whose size is equal to the size of the previous layer. The network is trained using SGD with shuffled mini-batches using the Adagrad update rule (Duchi et al., 2011). The batch size is set to 100 examples. We used 25 epochs wi</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In ECML,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="2174" citStr="Moschitti, 2006" startWordPosition="343" endWordPosition="344">onal Neural Networks to improve the ranking of answer candidate lists. Most importantly, we build a very large dataset for clue retrieval, composed of 2,000,493 clues with their associated answers, i.e., this is a supervised corpus where large scale learning models can be developed and tested. This dataset is an interesting ∗Work done when student at University of Trento resource that we make available to the research community1. To assess the effectiveness of our DNN model, we compare it with the current state of the art model (Nicosia et al., 2015) in reranking CP clues, where tree kernels (Moschitti, 2006) are used to rerank clues according to their syntactic/semantic similarity with the query clue. The experimental results on our dataset demonstrate that: (i) DNNs are efficient and can greatly benefit from large amounts of data; (ii) when DNNs are applied to large-scale data, they largely outperform traditional featurebased rerankers as well as kernel-based models; and (iii) if limited training data is available for training, tree kernel-based models are more accurate than DNNs 2 Clue Reranking Models for CPs In this section, we briefly introduce the general idea of CP resolution systems and t</context>
<context position="6714" citStr="Moschitti (2006)" startWordPosition="1094" endWordPosition="1095">our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Given a query clue qc and two retrieved clues c1, c2, we can rank them by using a classification approach: the two clues c1 and c2 are reranked by comparing their classification scores: SVM((q, c1)) and SVM((q, c2)). The SVM classifier uses the following kernel applied to two pairs of query/clues, p = (q, ci) and p0 = (q0,c0j): K(p, p0) = TK(q, q0) + TK(ci,c0j)+ FV (q, ci) · FV (q0, c0j), where TK can be any tree kernel, e.g., the syntactic tree kernel (STK) also called SST by Moschitti (2006), and FV is the feature vector representation of the input pair, e.g., (q, ci) or (q0,c0j). STK maps trees into the space of all possible tree fragments constrained by the rule that the sibling nodes from their parents cannot be separated. It enables the exploitation of structural features, which can be effectively combined with more traditional features (described hereafter). Feature Vectors (FV). We compute the following similarity features between clues: (i) tree kernel similarity applied to intra-pairs, i.e., between the query and the retrieved clues; (ii) DKPro Similarity, which defines f</context>
<context position="12972" citStr="Moschitti, 2006" startWordPosition="2106" endWordPosition="2108">s). 2http://www.crosswordgiant.com 3http://www.otsys.com/clue 4A true reranker should be built using pairs of clue pairs, where the positive pairs are those having the correct pair as the first member. This led to form 127,109 reranking examples, with 66,011 positive and 61,098 negative pairs. However, in some experiments, which we do not report in the paper, we observed that the performance both of the simple classifier as well as the true reranker were similar, thus we decided to use the simpler classifier. Structural model. We use SVM-light-TK5, which enables the use of structural kernels (Moschitti, 2006). We applied structural kernels to shallow tree representations and a polynomial kernel of degree 3 to feature vectors (FV). Distributional neural network model. We preinitialize the word embeddings by running the word2vec tool (Mikolov et al., 2013) on the English Wikipedia dump. We opt for a skipgram model with window size 5 and filtering words with frequency less than 5. The dimensionality of the embeddings is set to 50. The input sentences are mapped to fixed-sized vectors by computing the average of their word embeddings. We use a single non-linear hidden layer (with rectified linear (ReL</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In ECML, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Kernel methods, syntax and semantics for relational text categorization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08,</booktitle>
<pages>253--262</pages>
<location>Napa Valley, California, USA.</location>
<contexts>
<context position="5569" citStr="Moschitti, 2008" startWordPosition="885" endWordPosition="886">s. The latter promote the most similar, which are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words overlap with the query clue. 2.3 Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barla</context>
</contexts>
<marker>Moschitti, 2008</marker>
<rawString>Alessandro Moschitti. 2008. Kernel methods, syntax and semantics for relational text categorization. In Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM ’08, pages 253–262, Napa Valley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Nicosia</author>
<author>Gianni Barlacchi</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning to rank aggregated answers for crossword puzzles.</title>
<date>2015</date>
<booktitle>Advances in Information Retrieval - 37th European Conference on IR Research, ECIR, Vienna, Austria. Proceedings,</booktitle>
<volume>9022</volume>
<pages>556--561</pages>
<editor>In Allan Hanbury, Gabriella Kazai, Andreas Rauber, and Norbert Fuhr, editors,</editor>
<contexts>
<context position="2114" citStr="Nicosia et al., 2015" startWordPosition="331" endWordPosition="334">n this paper, we propose for the first time the use of Distributional Neural Networks to improve the ranking of answer candidate lists. Most importantly, we build a very large dataset for clue retrieval, composed of 2,000,493 clues with their associated answers, i.e., this is a supervised corpus where large scale learning models can be developed and tested. This dataset is an interesting ∗Work done when student at University of Trento resource that we make available to the research community1. To assess the effectiveness of our DNN model, we compare it with the current state of the art model (Nicosia et al., 2015) in reranking CP clues, where tree kernels (Moschitti, 2006) are used to rerank clues according to their syntactic/semantic similarity with the query clue. The experimental results on our dataset demonstrate that: (i) DNNs are efficient and can greatly benefit from large amounts of data; (ii) when DNNs are applied to large-scale data, they largely outperform traditional featurebased rerankers as well as kernel-based models; and (iii) if limited training data is available for training, tree kernel-based models are more accurate than DNNs 2 Clue Reranking Models for CPs In this section, we brief</context>
<context position="6162" citStr="Nicosia et al., 2015" startWordPosition="989" endWordPosition="992">ng systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Given a query clue qc and two retrieved clues c1, c2, we can rank them by using a classification approach: the two clues c1 and c2 are reranked by comparing their classification scores: SVM((q, c1)) and SVM((q, c2)). The SVM classifier uses the following kernel applied to two pairs of query/clues, p = (q, ci) and p0 = (q0,c0j): K(p, p0) = TK(q, q0) + TK(ci,c0j)+ FV (q, ci) · FV (q0, c0j), where TK can be any tree kernel, e.g., the syntactic tree kernel (STK) also called SST by Moschitti (2006), and FV is the feature vector representation of</context>
<context position="14730" citStr="Nicosia et al., 2015" startWordPosition="2412" endWordPosition="2415">features we simply take the output of the hidden layer just before the softmax. Evaluation. We used standard metrics widely used in QA: the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP). 4.2 Results Table 2 summarizes the results of our different reranking models trained on a small dataset (SD) of 120k examples and a large dataset (LD) with 2M examples. The first column reports the BM25 result; the second column shows the performance of SVM perf (SVMP), which is a very fast variant of SVM, using FV; the third column reports the state-of-theart model for crossword clue reranking (Nicosia et al., 2015), which uses FV vector and tree kernels, i.e., SVM(TK). Regarding the other systems: DNNMSD is the DNN model trained on the small data (SD) of 120k training pairs; SVMp(DNNFLD) is SVM perf trained with (i) the features derived from 5http://disi.unitn.it/moschitti/ Tree-Kernel.htm 202 Training classifiers with the Small Dataset (SD) (120K instances) BM25 SVMp SVM(TK) DNNMSD SVMp(DNNFLD) SVM(DNNFLD,TK) MRR 37.57 41.95 43.59 40.08 46.12 45.50 MAP 27.76 30.06 31.79 28.25 33.75 33.71 Training classifiers with the Large Dataset (LD) (2 million instances) BM25 SVMp SVM(TK) DNNMLD SVMp(DNNFLD,−FV) SVM</context>
</contexts>
<marker>Nicosia, Barlacchi, Moschitti, 2015</marker>
<rawString>Massimo Nicosia, Gianni Barlacchi, and Alessandro Moschitti. 2015. Learning to rank aggregated answers for crossword puzzles. In Allan Hanbury, Gabriella Kazai, Andreas Rauber, and Norbert Fuhr, editors, Advances in Information Retrieval - 37th European Conference on IR Research, ECIR, Vienna, Austria. Proceedings, volume 9022 of Lecture Notes in Computer Science, pages 556–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira Pohl</author>
</authors>
<title>Heuristic search viewed as path finding in a graph.</title>
<date>1970</date>
<journal>Artificial Intelligence,</journal>
<volume>1</volume>
<issue>34</issue>
<pages>204</pages>
<contexts>
<context position="3086" citStr="Pohl, 1970" startWordPosition="489" endWordPosition="490">traditional featurebased rerankers as well as kernel-based models; and (iii) if limited training data is available for training, tree kernel-based models are more accurate than DNNs 2 Clue Reranking Models for CPs In this section, we briefly introduce the general idea of CP resolution systems and the state-of-theart models for reranking answer candidates. 2.1 CP resolution systems The main task of a CP resolution system is the generation of candidate answer lists for each clue of the target puzzle (Littman et al., 2002). Then a solver for Probabilistic-Constraint Satisfaction Problems, e.g., (Pohl, 1970), tries combinations of letters that satisfy the crossword constraints. The combinations are derived from words found in dictionaries or in the lists of answer candidates. The latter can be generated using large crossword databases as well as several expert modules accessing domain-specific databases (e.g., movies, writers and geography). WebCrow, one of the 1http://ikernels-portal.disi.unitn.it/ projects/webcrow/ 199 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), </context>
</contexts>
<marker>Pohl, 1970</marker>
<rawString>Ira Pohl. 1970. Heuristic search viewed as path finding in a graph. Artificial Intelligence, 1(34):193 – 204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<location>New York, NY, USA.</location>
<contexts>
<context position="5598" citStr="Severyn and Moschitti, 2012" startWordPosition="887" endWordPosition="890">mote the most similar, which are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words overlap with the query clue. 2.3 Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more </context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of ACM SIGIR, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic feature engineering for answer selection and extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>458--467</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5674" citStr="Severyn and Moschitti, 2013" startWordPosition="899" endWordPosition="902">f the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words overlap with the query clue. 2.3 Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Given a query clue qc and two retrieved clues c1, c2, we c</context>
</contexts>
<marker>Severyn, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic feature engineering for answer selection and extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 458–467, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Building structures from classifiers for passage reranking.</title>
<date>2013</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="5620" citStr="Severyn et al., 2013" startWordPosition="891" endWordPosition="894">are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words overlap with the query clue. 2.3 Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Give</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013a. Building structures from classifiers for passage reranking. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Learning adaptable patterns for passage reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>75--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5620" citStr="Severyn et al., 2013" startWordPosition="891" endWordPosition="894">are probably associated with the same answer of the query clue, to the top. The reranking step is important because SEs often fail to retrieve the correct clues in the first position. For example, Table 1 shows the first five clues retrieved for the query clue: Jo Ann who played Lt. ”Dish” in 1970’s ”MASH”. BM25 retrieved the wrong clue, Actress Pflug who played Lt. Dish in ”MASH”, at the top since it has a larger bag-of-words overlap with the query clue. 2.3 Reranking with Kernels We applied our reranking framework for question answering systems (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn et al., 2013a; Severyn et al., 2013b; Severyn and Moschitti, 2013). This retrieves a list of related clues by using the target clue as a query in an SE (applied to the Web or to a DB). Then, both query and candidates are represented by shallow syntactic structures (generated by running a set of NLP parsers) and traditional similarity features which are fed to a kernelbased reranker. Hereafter, we give a brief description of our models for clue reranking whereas the reader can refer to our previous work (Barlacchi et al., 2014a; Nicosia et al., 2015; Barlacchi et al., 2014b) for more specific details. Give</context>
</contexts>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013b. Learning adaptable patterns for passage reranking. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 75–83, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kateryna Tymoshenko</author>
<author>Alessandro Moschitti</author>
<author>Aliaksei Severyn</author>
</authors>
<title>Encoding semantic resources in syntactic structures for passage reranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>664--672</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<marker>Tymoshenko, Moschitti, Severyn, 2014</marker>
<rawString>Kateryna Tymoshenko, Alessandro Moschitti, and Aliaksei Severyn. 2014. Encoding semantic resources in syntactic structures for passage reranking. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664–672, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Yu</author>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
<author>Stephen Pulman</author>
</authors>
<title>Deep learning for answer sentence selection.</title>
<date>2014</date>
<publisher>CoRR.</publisher>
<contexts>
<context position="10170" citStr="Yu et al., 2014" startWordPosition="1653" endWordPosition="1656">ore complicated single or multi-layer convolutional architectures. We conjecture that this is largely due to the nature of the language used in clues, which is very dense and where the syntactic information plays a minor role. Considering recent deep learning models for matching sentences, our network is most similar to the models in Hu et al. (2014) applied for computing sentence similarity and in Yu et al.(2014) (answer sentence selection in Question Answering) with the following differences: (i) In contrast to more complex convolutional sentence models explored in (Hu et al., 2014) and in (Yu et al., 2014), our sentence model is composed of a single averaging operation. (ii) To compute the similarity between the vector representation of the input sentences, our network uses two methods: (i) computing the similarity score obtained by transforming one clue into another using a similarity matrix M (explored in (Yu et al., 2014)), and (ii) directly modelling interactions between intermediate vector representations of the input 201 clues via fully-connected hidden layers (used by (Hu et al., 2014)). 4 Experiments Our experiments compare different ranking models, i.e., BM25 as the IR baseline, and se</context>
</contexts>
<marker>Yu, Hermann, Blunsom, Pulman, 2014</marker>
<rawString>Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection. CoRR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>