<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005290">
<title confidence="0.972483">
An Evaluation Understudy for Dialogue Coherence Models
</title>
<author confidence="0.984484">
Sudeep Gandhe and David Traum
</author>
<affiliation confidence="0.9962505">
Institute for Creative Technologies
University of Southern California
</affiliation>
<address confidence="0.723277">
13274 Fiji way, Marina del Rey, CA, 90292
</address>
<email confidence="0.999545">
{gandhe,traum}@ict.usc.edu
</email>
<sectionHeader confidence="0.99861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996185">
Evaluating a dialogue system is seen as a
major challenge within the dialogue research
community. Due to the very nature of the task,
most of the evaluation methods need a sub-
stantial amount of human involvement. Fol-
lowing the tradition in machine translation,
summarization and discourse coherence mod-
eling, we introduce the the idea of evaluation
understudy for dialogue coherence models.
Following (Lapata, 2006), we use the infor-
mation ordering task as a testbed for evaluat-
ing dialogue coherence models. This paper re-
ports findings about the reliability of the infor-
mation ordering task as applied to dialogues.
We find that simple n-gram co-occurrence
statistics similar in spirit to BLEU (Papineni
et al., 2001) correlate very well with human
judgments for dialogue coherence.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999579541666667">
In computer science or any other research field, sim-
ply building a system that accomplishes a certain
goal is not enough. It needs to be thoroughly eval-
uated. One might want to evaluate the system just
to see to what degree the goal is being accomplished
or to compare two or more systems with one another.
Evaluation can also lead to understanding the short-
comings of the system and the reasons for these. Fi-
nally the evaluation results can be used as feedback
in improving the system.
The best way to evaluate a novel algorithm or a
model for a system that is designed to aid humans
in processing natural language would be to employ
it in a real system and allow users to interact with it.
The data collected by this process can then be used
for evaluation. Sometimes this data needs further
analysis - which may include annotations, collect-
ing subjective judgments from humans, etc. Since
human judgments tend to vary, we may need to em-
ploy multiple judges. These are some of the reasons
why evaluation is time consuming, costly and some-
times prohibitively expensive.
Furthermore, if the system being developed con-
tains a machine learning component, the problem of
costly evaluation becomes even more serious. Ma-
chine learning components often optimize certain
free parameters by using evaluation results on held-
out data or by using n-fold cross-validation. Eval-
uation results can also help with feature selection.
This need for repeated evaluation can forbid the use
of data-driven machine learning components.
For these reasons, using an automatic evalua-
tion measure as an understudy is quickly becoming
a common practice in natural language processing
tasks. The general idea is to find an automatic eval-
uation metric that correlates very well with human
judgments. This allows developers to use the auto-
matic metric as a stand-in for human evaluation. Al-
though it cannot replace the finesse of human evalu-
ation, it can provide a crude idea of progress which
can later be validated. e.g. BLEU (Papineni et al.,
2001) for machine translation, ROUGE (Lin, 2004)
for summarization.
Recently, the discourse coherence modeling com-
munity has started using the information ordering
task as a testbed to test their discourse coherence
models (Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). Lapata (2006) has proposed an au-
</bodyText>
<page confidence="0.964335">
172
</page>
<note confidence="0.869998">
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181,
Columbus, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999524384615385">
tomatic evaluation measure for the information or-
dering task. We propose to use the same task as a
testbed for dialogue coherence modeling. We evalu-
ate the reliability of the information ordering task as
applied to dialogues and propose an evaluation un-
derstudy for dialogue coherence models.
In the next section, we look at related work in
evaluation of dialogue systems. Section 3 sum-
marizes the information ordering task and Lap-
ata’s (2006) findings. It is followed by the details
of the experiments we carried out and our observa-
tions. We conclude with a summary future work di-
rections.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99978984057971">
Most of the work on evaluating dialogue systems fo-
cuses on human-machine communication geared to-
wards a specific task. A variety of evaluation met-
rics can be reported for such task-oriented dialogue
systems. Dialogue systems can be judged based
on the performance of their components like WER
for ASR (Jurafsky and Martin, 2000), concept er-
ror rate or F-scores for NLU, understandability for
speech synthesis etc. Usually the core component,
the dialogue model - which is responsible for keep-
ing track of the dialogue progression and coming
up with an appropriate response, is evaluated indi-
rectly. Different dialogue models can be compared
with each other by keeping the rest of components
fixed and then by comparing the dialogue systems
as a whole. Dialogue systems can report subjective
measures such as user satisfaction scores and per-
ceived task completion. SASSI (Hone and Graham,
2000) prescribes a set of questions used for elicit-
ing such subjective assessments. The objective eval-
uation metrics can include dialogue efficiency and
quality measures.
PARADISE (Walker et al., 2000) was an attempt
at reducing the human involvement in evaluation. It
builds a predictive model for user satisfaction as a
linear combination of some objective measures and
perceived task completion. Even then the system
needs to train on the data gathered from user sur-
veys and objective features retrieved from logs of di-
alogue runs. It still needs to run the actual dialogue
system and collect objective features and perceived
task completeion to predict user satisfaction.
Other efforts in saving human involvement in
evaluation include using simulated users for test-
ing (Eckert et al., 1997). This has become a popu-
lar tool for systems employing reinforcement learn-
ing (Levin et al., 1997; Williams and Young, 2006).
Some of the methods involved in user simulation
are as complex as building dialogue systems them-
selves (Schatzmann et al., 2007). User simulations
also need to be evaluated as how closely they model
human behavior (Georgila et al., 2006) or as how
good a predictor they are of dialogue system perfor-
mance (Williams, 2007).
Some researchers have proposed metrics for eval-
uating a dialogue model in a task-oriented system.
(Henderson et al., 2005) used the number of slots in
a frame filled and/or confirmed. Roque et al. (2006)
proposed hand-annotating information-states in a di-
alogue to evaluate the accuracy of information state
updates. Such measures make assumptions about
the underlying dialogue model being used (e.g.,
form-based or information-state based etc.).
We are more interested in evaluating types of di-
alogue systems that do not follow these task-based
assumptions: systems designed to imitate human-
human conversations. Such dialogue systems can
range from chatbots like Alice (Wallace, 2003),
Eliza (Weizenbaum, 1966) to virtual humans used
in simulation training (Traum et al., 2005). For
such systems, the notion of task completion or ef-
ficiency is not well defined and task specific objec-
tive measures are hardly suitable. Most evaluations
report the subjective evaluations for appropriateness
of responses. Traum et. al. (2004) propose a cod-
ing scheme for response appropriateness and scoring
functions for those categories. Gandhe et. al. (2006)
propose a scale for subjective assessment for appro-
priateness.
</bodyText>
<sectionHeader confidence="0.998087" genericHeader="method">
3 Information Ordering
</sectionHeader>
<bodyText confidence="0.9981365">
The information ordering task consists of choos-
ing a presentation sequence for a set of information
bearing elements. This task is well suited for text-
to-text generation like in single or multi-document
summarization (Barzilay et al., 2002). Recently
there has been a lot of work in discourse coher-
ence modeling (Lapata, 2003; Barzilay and Lap-
ata, 2005; Soricut and Marcu, 2006) that has used
</bodyText>
<page confidence="0.998296">
173
</page>
<bodyText confidence="0.998210333333334">
information ordering to test the coherence mod-
els. The information-bearing elements here are sen-
tences rather than high-level concepts. This frees the
models from having to depend on a hard to get train-
ing corpus which has been hand-authored for con-
cepts.
Most of the dialogue models still work at the
higher abstraction level of dialogue acts and inten-
tions. But with an increasing number of dialogue
systems finding use in non-traditional applications
such as simulation training, games, etc.; there is a
need for dialogue models which do not depend on
hand-authored corpora or rules. Recently Gandhe
and Traum (2007) proposed dialogue models that
do not need annotations for dialogue-acts, seman-
tics and hand-authored rules for information state
updates or finite state machines.
Such dialogue models focus primarily on gener-
ating an appropriate coherent response given the di-
alogue history. In certain cases the generation of
a response can be reduced to selection from a set
of available responses. For such dialogue models,
maintaining the information state can be considered
as a secondary goal. The element that is common
to the information ordering task and the task of se-
lecting next most appropriate response is the ability
to express a preference for one sequence of dialogue
turns over the other. We propose to use the informa-
tion ordering task to test dialogue coherence models.
Here the information bearing units will be dialogue
turns.1
There are certain advantages offered by using in-
formation ordering as a task to evaluate dialogue co-
herence models. First the task does not require a
dialogue model to take part in conversations in an
interactive manner. This obviates the need for hav-
ing real users engaging in the dialogue with the sys-
tem. Secondly, the task is agnostic about the under-
lying dialogue model. It can be a data-driven statis-
tical model or information-state based, form based
or even a reinforcement learning system based on
MDP or POMDP. Third, there are simple objective
measures available to evaluate the success of infor-
mation ordering task.
Recently, Purandare and Litman (2008) have used
</bodyText>
<footnote confidence="0.932481">
1These can also be at the utterance level, but for this paper
we will use dialogue turns.
</footnote>
<bodyText confidence="0.999905875">
this task for modeling dialogue coherence. But they
only allow for a binary classification of sequences
as either coherent or incoherent. For comparing dif-
ferent dialogue coherence models, we need the abil-
ity for finer distinction between sequences of infor-
mation being put together. Lapata (2003) proposed
Kendall’s T, a rank correlation measure, as one such
candidate. In a recent study they show that Kendall’s
T correlates well with human judgment (Lapata,
2006). They show that human judges can reliably
provide coherence ratings for various permutations
of text. (Pearson’s correlation for inter-rater agree-
ment is 0.56) and that Kendall’s T is a good in-
dicator for human judgment (Pearson’s correlation
for Kendall’s T with human judgment is 0.45 (p &lt;
0.01)).
Before adapting the information ordering task for
dialogues, certain questions need to be answered.
We need to validate that humans can reliably per-
form the task of information ordering and can judge
the coherence for different sequences of dialogue
turns. We also need to find which objective mea-
sures (like Kendall’s T) correlate well with human
judgments.
</bodyText>
<sectionHeader confidence="0.99471" genericHeader="method">
4 Evaluating Information Ordering
</sectionHeader>
<bodyText confidence="0.974496277777778">
One of the advantages of using information order-
ing as a testbed is that there are objective measures
available to evaluate the performance of information
ordering task. Kendall’s T (Kendall, 1938), a rank
correlation coefficient, is one such measure. Given
a reference sequence of length n, Kendall’s T for an
observed sequence can be defined as,
# concordant pairs − # discordant pairs
# total pairs
Each pair of elements in the observed sequence
is marked either as concordant - appearing in the
same order as in reference sequence or as discor-
dant otherwise. The total number of pairs is Cn2 =
n(n − 1)/2. T ranges from -1 to 1.
Another possible measure can be defined as the
fraction of n-grams from reference sequence, that
are preserved in the observed sequence.
# n-grams preserved
</bodyText>
<equation confidence="0.657112">
bn =
</equation>
<bodyText confidence="0.9175808">
# total n-grams
In this study we have used, b2, fraction of bigrams
and b3, fraction of trigrams preserved from the ref-
erence sequence. These values range from 0 to 1.
Table 1 gives examples of observed sequences and
</bodyText>
<equation confidence="0.989189">
T =
</equation>
<page confidence="0.985936">
174
</page>
<table confidence="0.998117166666667">
Observed Sequence b2 b3 T
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00
[8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29
[4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60
[6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64
[2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64
</table>
<tableCaption confidence="0.979428666666667">
Table 1: Examples of observed sequences and their re-
spective b2, b3 &amp; T values. Here the reference sequence
is [0,1,2,3,4,5,6,7,8,9].
</tableCaption>
<bodyText confidence="0.706941">
respective b2, b3 and T values. Notice how T al-
lows for long-distance relationships whereas b2, b3
are sensitive to local features only. 2
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999792482758621">
For our experiments we used segments drawn from 9
dialogues. These dialogues were two-party human-
human dialogues. To ensure applicability of our
results over different types of dialogue, we chose
these 9 dialogues from different sources. Three of
these were excerpts from role-play dialogues involv-
ing negotiations which were originally collected for
a simulation training scenario (Traum et al., 2005).
Three are from SRI’s Amex Travel Agent data which
are task-oriented dialogues about air travel plan-
ning (Bratt et al., 1995). The rest of the dialogues are
scripts from popular television shows. Fig 6 shows
an example from the air-travel domain. Each excerpt
drawn was 10 turns long with turns strictly alternat-
ing between the two speakers.
Following the experimental design of (Lapata,
2006) we created random permutations for these di-
alogue segments. We constrained our permutations
so that the permutations always start with the same
speaker as the original dialogue and turns strictly al-
ternate between the speakers. With these constraints
there are still 5! x 5! = 14400 possible permutations
per dialogue. We selected 3 random permutations
for each of the 9 dialogues. In all, we have a total
of 27 dialogue permutations. They are arranged in 3
sets, each set containing a permutation for all 9 di-
alogues. We ensured that not all permutations in a
given set are particularly very good or very bad. We
used Kendall’s T to balance the permutations across
</bodyText>
<footnote confidence="0.7573">
2For more on the relationship between b2, b3 and T see row
3,4 of table 1 and figure 4.
</footnote>
<bodyText confidence="0.999031083333333">
the given set as well as across the given dialogue.
Unlike Lapata (2006) who chose to remove the
pronouns and discourse connectives, we decided not
do any pre-processing on the text like removing
disfluencies or removing cohesive devices such as
anaphora, ellipsis, discourse connectives, etc. One
of the reason is such pre-processing if done manu-
ally defeats the purpose of removing humans from
the evaluation procedure. Moreover it is very diffi-
cult to remove certain cohesive devices such as dis-
course deixis without affecting the coherence level
of the original dialogues.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="method">
6 Experiment 1
</sectionHeader>
<bodyText confidence="0.999955878787879">
In our first experiment, we divided a total of 9 hu-
man judges among the 3 sets (3 judges per set). Each
judge was presented with 9 dialogue permutations.
They were asked to assign a single coherence rat-
ing for each dialogue permutation. The ratings were
on a scale of 1 to 7, with 1 being very incoherent
and 7 being perfectly coherent. We did not provide
any additional instructions or examples of scale as
we wanted to capture the intuitive idea of coherence
from our judges. Within each set the dialogue per-
mutations were presented in random order.
We compute the inter-rater agreement by using
Pearson’s correlation analysis. We correlate the rat-
ings given by each judge with the average ratings
given by the judges who were assigned the same set.
For inter-rater agreement we report the average of 9
such correlations which is 0.73 (std dev = 0.07). Art-
stein and Poesio (2008) have argued that Krippen-
dorff’s α (Krippendorff, 2004) can be used for inter-
rater agreement with interval scales like the one we
have. In our case for the three sets α values were
0.49, 0.58, 0.64. These moderate values of alpha in-
dicate that the task of judging coherence is indeed a
difficult task, especially when detailed instructions
or examples of scales are not given.
In order to assess whether Kendall’s T can be used
as an automatic measure of dialogue coherence, we
perform a correlation analysis of T values against
the average ratings by human judges. The Pearson’s
correlation coefficient is 0.35 and it is statistically
not significant (P=0.07). Fig 1(a) shows the rela-
tionship between coherence judgments and T val-
ues. This experiment fails to support the suitability
</bodyText>
<page confidence="0.992577">
175
</page>
<figure confidence="0.96729025">
(a) Kendall’s T does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram &amp; trigram counts correlate well
with human judgments for dialogue coherence.
</figure>
<figureCaption confidence="0.999903">
Figure 1: Experiment 1 - single coherence rating per permutation
</figureCaption>
<bodyText confidence="0.995817222222222">
of Kendall’s T as an evaluation understudy.
We also analyzed the correlation of human judg-
ments against simple n-gram statistics, specifically
(b2 + b3) /2. Fig 1(b) shows the relationship be-
tween human judgments and the average of fraction
of bigrams and fraction of trigrams that were pre-
served in the permutation. The Pearson’s correlation
coefficient is 0.62 and it is statistically significant
(P&lt;0.01).
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="method">
7 Experiment 2
</sectionHeader>
<bodyText confidence="0.98101965">
Since human judges found it relatively hard to as-
sign a single rating to a dialogue permutation, we
decided to repeat experiment 1 with some modifica-
tions. In our second experiment we asked the judges
to provide coherence ratings at every turn, based on
the dialogue that preceded that turn. The dialogue
permutations were presented to the judges through a
web interface in an incremental fashion turn by turn
as they rated each turn for coherence (see Fig 5 in
the appendix for the screenshot of this interface). We
used a scale from 1 to 5 with 1 being completely in-
coherent and 5 as perfectly coherent. 3 A total of 11
judges participated in this experiment with the first
set being judged by 5 judges and the remaining two
sets by 3 judges each.
3We believe this is a less complex task than experiment 1
and hence a narrower scale is used.
For the rest of the analysis, we use the average
coherence rating from all turns as a coherence rat-
ing for the dialogue permutation. We performed
the inter-rater agreement analysis as in experiment
1. The average of 11 correlations is 0.83 (std dev =
0.09). Although the correlation has improved, Krip-
pendorff’s α values for the three sets are 0.49, 0.35,
0.63. This shows that coherence rating is still a hard
task even when judged turn by turn.
We assessed the relationship between the aver-
age coherence rating for dialogue permutations with
Kendall’s T (see Fig 2(a)). The Pearson’s correlation
coefficient is 0.33 and is statistically not significant
(P=0.09).
Fig 2(b) shows high correlation of average coher-
ence ratings with the fraction of bigrams and tri-
grams that were preserved in permutation. The Pear-
son’s correlation coefficient is 0.75 and is statisti-
cally significant (P&lt;0.01).
Results of both experiments suggest that,
(b2 + b3) /2 correlates very well with human judg-
ments and can be used for evaluating information
ordering when applied to dialogues.
</bodyText>
<sectionHeader confidence="0.997574" genericHeader="method">
8 Experiment 3
</sectionHeader>
<bodyText confidence="0.999957333333333">
We wanted to know whether information ordering as
applied to dialogues is a valid task or not. In this ex-
periment we seek to establish a higher baseline for
</bodyText>
<page confidence="0.993589">
176
</page>
<figure confidence="0.96726925">
(a) Kendall’s T does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram &amp; trigram counts correlate well
with human judgments for dialogue coherence.
</figure>
<figureCaption confidence="0.99887">
Figure 2: Experiment 2 - turn-by-turn coherence rating
</figureCaption>
<bodyText confidence="0.992535666666667">
the task of information ordering in dialogues. We
presented the dialogue permutations to our human
judges and asked them to reorder the turns so that
the resulting order is as coherent as possible. All 11
judges who participated in experiment 2 also partic-
ipated in this experiment. They were presented with
a drag and drop interface over the web that allowed
them to reorder the dialogue permutations. The re-
ordering was constrained to keep the first speaker
of the reordering same as that of the original di-
alogue and the re-orderings must have strictly al-
ternating turns. We computed the Kendall’s T and
fraction of bigrams and trigrams (b2 + b3) /2 for
these re-orderings. There were a total of 11 x 9
= 99 reordered dialogue permutations. Fig 3(a)
and 3(b) shows the frequency distribution of T and
(b2 + b3) /2 values respectively.
Humans achieve high values for the reordering
task. For Kendall’s T, the mean of the reordered dia-
logues is 0.82 (std dev = 0.25) and for (b2 + b3) /2,
the mean is 0.71 (std dev = 0.28). These values es-
tablish an upper baseline for the information order-
ing task. These can be compared against the random
baseline. For T random performance is 0.02 4 and
</bodyText>
<footnote confidence="0.9732815">
4Theoretically this should be zero. The slight positive bias
is the result of the constraints imposed on the re-orderings -
like only allowing the permutations that have the correct starting
speaker.
</footnote>
<equation confidence="0.649259">
for (b2 + b3) /2 it is 0.11. 5
</equation>
<sectionHeader confidence="0.983421" genericHeader="method">
9 Discussion
</sectionHeader>
<bodyText confidence="0.999838608695652">
Results show that (b2 + b3) /2 correlates well with
human judgments for dialogue coherence better than
Kendall’s T. T encodes long distance relationships
in orderings where as (b2 + b3) /2 only looks at lo-
cal context. Fig 4 shows the relationship between
these two measures. Notice that most of the order-
ings have T values around zero (i.e. in the middle
range for T), whereas majority of orderings will have
a low value for (b2 + b3) /2. T seems to overesti-
mate the coherence even in the absence of immedi-
ate local coherence (See third entry in table 1). It
seems that local context is more important for dia-
logues than for discourse, which may follow from
the fact that dialogues are produced by two speakers
who must react to each other, while discourse can be
planned by one speaker from the beginning. Traum
and Allen (1994) point out that such social obliga-
tions to respond and address the contributions of the
other should be an important factor in building dia-
logue systems.
The information ordering paradigm does not take
into account the content of the information-bearing
items, e.g. the fact that turns like ”yes”, ”I agree”,
</bodyText>
<footnote confidence="0.999685">
5This value is calculated by considering all 14400 permuta-
tions as equally likely.
</footnote>
<page confidence="0.992518">
177
</page>
<figure confidence="0.9952895">
(a) Histogram of Kendall’s T for reordered se- (b) Histogram of fraction of bigrams &amp; tri-
quences grams values for reordered sequences
</figure>
<figureCaption confidence="0.999705">
Figure 3: Experiment 3 - upper baseline for information ordering task (human performance)
</figureCaption>
<bodyText confidence="0.999961818181818">
”okay” perform the same function and should be
treated as replaceable. This may suggest a need to
modify some of the objective measures to evaluate
the information ordering specially for dialogue sys-
tems that involve more of such utterances.
Human judges can find the optimal sequences
with relatively high frequency, at least for short
dialogues. It remains to be seen how this varies
with longer dialogue lengths which may contain
sub-dialogues that can be arranged independently of
each other.
</bodyText>
<sectionHeader confidence="0.957874" genericHeader="conclusions">
10 Conclusion &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999959903225806">
Evaluating dialogue systems has always been a ma-
jor challenge in dialogue systems research. The core
component of dialogue systems, the dialogue model,
has usually been only indirectly evaluated. Such
evaluations involve too much human effort and are a
bottleneck for the use of data-driven machine learn-
ing models for dialogue coherence. The information
ordering task, widely used in discourse coherence
modeling, can be adopted as a testbed for evaluating
dialogue coherence models as well. Here we have
shown that simple n-gram statistics that are sensi-
tive to local features correlate well with human judg-
ments for coherence and can be used as an evalua-
tion understudy for dialogue coherence models. As
with any evaluation understudy, one must be careful
while using it as the correlation with human judg-
ments is not perfect and may be inaccurate in some
cases – it can not completely replace the need for
full evaluation with human judges in all cases (see
(Callison-Burch et al., 2006) for a critique of BLUE
along these lines).
In the future, we would like to perform more ex-
periments with larger data sets and different types
of dialogues. It will also be interesting to see the
role cohesive devices play in coherence ratings. We
would like to see if there are any other measures or
certain modifications to the current ones that corre-
late better with human judgments. We also plan to
employ this evaluation metric as feedback in build-
ing dialogue coherence models as is done in ma-
chine translation (Och, 2003).
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999526428571429">
The effort described here has been sponsored by the U.S. Army
Research, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Govern-
ment, and no official endorsement should be inferred. We would
like to thank Radu Soricut, Ron Artstein, and the anonymous
SIGdial reviewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967282333333333">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. In To appear
in Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Proc.
ACL-05.
</reference>
<page confidence="0.991239">
178
</page>
<reference confidence="0.997345945945946">
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument summarization. JAIR, 17:35–55.
Harry Bratt, John Dowding, and Kate Hunicke-Smith.
1995. The sri telephone-based atis system. In Pro-
ceedings of the Spoken Language Systems Technology
Workshop, January.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. In proceedings of EACL-2006.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Automatic Speech Recognition and Under-
standing, pages 80–87, Dec.
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of Interspeech-07.
Sudeep Gandhe, Andrew Gordon, and David Traum.
2006. Improving question-answering with linking di-
alogues. In International Conference on Intelligent
User Interfaces (IUI), January.
Kalliroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In proceedings of Inter-
speech.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2005. Hybrid reinforcement/supervised learning for
dialogue policies from communicator data. In pro-
ceedings of IJCAI workshop.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (SASSI). Natural Language Engineering: Spe-
cial Issue on Best Practice in Spoken Dialogue Sys-
tems.
Daniel Jurafsky and James H. Martin. 2000. SPEECH
and LANGUAGE PROCESSING: An Introduction to
Natural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice-Hall.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30:81–93.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Publi-
cations.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, Japan.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering. Computational Linguistics, 32(4):471–
484.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1997. Learning dialogue strategies within the markov
decision process framework. In Automatic Speech
Recognition and Understanding, pages 72–79, Dec.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In In ACL 2003: Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics, July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022), IBM Research Division,
September.
Amruta Purandare and Diane Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In Proceedings 21st Inter-
national FLAIRS Conference, May.
Antonio Roque, Hua Ai, and David Traum. 2006. Evalu-
ation of an information state-based dialogue manager.
In Brandial 2006: The 10th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In proceedings of HLT/NAACL, Rochester, NY.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Proc.
ACL-06.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL-94), pages 1–8.
David R. Traum, Susan Robinson, and Jens Stephan.
2004. Evaluation of multi-party virtual reality dia-
logue interaction. In In Proceedings of Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1699–1702.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-team
interaction training. In AAMAS-05 Workshop on Cre-
ating Bonds with Humanoids, July.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems.
Richard Wallace. 2003. Be Your Own Botmaster, 2nd
Edition. ALICE A. I. Foundation.
Joseph Weizenbaum. 1966. Eliza–a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36–45, January.
Jason D. Williams and Steve Young. 2006. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer Speech and Language, 21:393–
422.
Jason D. Williams. 2007. A method for evaluating and
comparing user simulations: The cramer-von mises di-
vergence. In IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
</reference>
<page confidence="0.999342">
179
</page>
<sectionHeader confidence="0.954758" genericHeader="references">
Appendix
</sectionHeader>
<figureCaption confidence="0.999883333333333">
Figure 4: Distributions for Kendall’s T, (b2 + b3) /2 and the relationship between them for all possible dialogue
permutations with 10 turns and earlier mentioned constraints.
Figure 5: Screenshot of the interface used for collecting coherence rating for dialogue permutations.
</figureCaption>
<figure confidence="0.995763">
(a) (b) (c)
</figure>
<page confidence="0.719014">
180
</page>
<bodyText confidence="0.913738289473684">
Agent AAA at American Express may I help you?
User yeah this is BBB BBB I need to make some travel arrangements
Agent ok and what do you need to do?
User ok on June sixth from San Jose to Denver, United
Agent leaving at what time?
User I believe there’s one leaving at eleven o’clock in the morning
Agent leaves at eleven a.m. and arrives Denver at two twenty p.m. out of San Jose
User ok
Agent yeah that’s United flight four seventy
User that’s the one
Doctor hello i’m doctor perez
how can i help you
Captain uh well i’m with uh the local
i’m i’m the commander of the local company
and uh i’d like to talk to you about some options you have for relocating your clinic
Doctor uh we’re not uh planning to relocate the clinic captain
what uh what is this about
Captain well have you noticed that there’s been an awful lot of fighting in the area recently
Doctor yes yes i have
we’re very busy
we’ve had many more casual+ casualties many more patients than than uh usual in the
last month
but uh what what is this about relocating our clinic
have have uh you been instructed to move us
Captain no
but uh we just have some concerns about the increase in fighting xx
Doctor are you suggesting that we relocate the clinic
because we had no plans
we uh we uh we’re located here and we’ve been uh
we are located where the patients need us
Captain yeah but
yeah actually it is a suggestion that you would be a lot safer if you moved away from
this area
we can put you in an area where there’s n+ no insurgents
and we have the area completely under control with our troops
Doctor i see captain
is this a is this a suggestion from your commander
Captain i’m uh the company commander
</bodyText>
<figureCaption confidence="0.995713">
Figure 6: Examples of the dialogues used to elicit human judgments for coherence
</figureCaption>
<page confidence="0.994576">
181
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846642">
<title confidence="0.998659">An Evaluation Understudy for Dialogue Coherence Models</title>
<author confidence="0.944207">Gandhe Traum</author>
<affiliation confidence="0.9954735">Institute for Creative Technologies University of Southern California</affiliation>
<address confidence="0.97082">13274 Fiji way, Marina del Rey, CA,</address>
<abstract confidence="0.994217157894737">Evaluating a dialogue system is seen as a major challenge within the dialogue research community. Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<note>In To appear in Computational Linguistics.</note>
<contexts>
<context position="15853" citStr="Artstein and Poesio (2008)" startWordPosition="2595" endWordPosition="2599">n a scale of 1 to 7, with 1 being very incoherent and 7 being perfectly coherent. We did not provide any additional instructions or examples of scale as we wanted to capture the intuitive idea of coherence from our judges. Within each set the dialogue permutations were presented in random order. We compute the inter-rater agreement by using Pearson’s correlation analysis. We correlate the ratings given by each judge with the average ratings given by the judges who were assigned the same set. For inter-rater agreement we report the average of 9 such correlations which is 0.73 (std dev = 0.07). Artstein and Poesio (2008) have argued that Krippendorff’s α (Krippendorff, 2004) can be used for interrater agreement with interval scales like the one we have. In our case for the three sets α values were 0.49, 0.58, 0.64. These moderate values of alpha indicate that the task of judging coherence is indeed a difficult task, especially when detailed instructions or examples of scales are not given. In order to assess whether Kendall’s T can be used as an automatic measure of dialogue coherence, we perform a correlation analysis of T values against the average ratings by human judges. The Pearson’s correlation coeffici</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. In To appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proc. ACL-05.</booktitle>
<contexts>
<context position="3298" citStr="Barzilay and Lapata, 2005" startWordPosition="526" endWordPosition="529">guage processing tasks. The general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, Columbus, June 2008. c�2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models. In the next section, we look at related work in evaluation of dialogue systems. Section 3 s</context>
<context position="7835" citStr="Barzilay and Lapata, 2005" startWordPosition="1241" endWordPosition="1245"> evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002). Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used 173 information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue models which do not depend on </context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proc. ACL-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Inferring strategies for sentence ordering in multidocument summarization.</title>
<date>2002</date>
<journal>JAIR,</journal>
<pages>17--35</pages>
<contexts>
<context position="7723" citStr="Barzilay et al., 2002" startWordPosition="1222" endWordPosition="1225">well defined and task specific objective measures are hardly suitable. Most evaluations report the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002). Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used 173 information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional ap</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2002. Inferring strategies for sentence ordering in multidocument summarization. JAIR, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Bratt</author>
<author>John Dowding</author>
<author>Kate Hunicke-Smith</author>
</authors>
<title>The sri telephone-based atis system.</title>
<date>1995</date>
<booktitle>In Proceedings of the Spoken Language Systems Technology Workshop,</booktitle>
<contexts>
<context position="13357" citStr="Bratt et al., 1995" startWordPosition="2172" endWordPosition="2175"> relationships whereas b2, b3 are sensitive to local features only. 2 5 Experimental Setup For our experiments we used segments drawn from 9 dialogues. These dialogues were two-party humanhuman dialogues. To ensure applicability of our results over different types of dialogue, we chose these 9 dialogues from different sources. Three of these were excerpts from role-play dialogues involving negotiations which were originally collected for a simulation training scenario (Traum et al., 2005). Three are from SRI’s Amex Travel Agent data which are task-oriented dialogues about air travel planning (Bratt et al., 1995). The rest of the dialogues are scripts from popular television shows. Fig 6 shows an example from the air-travel domain. Each excerpt drawn was 10 turns long with turns strictly alternating between the two speakers. Following the experimental design of (Lapata, 2006) we created random permutations for these dialogue segments. We constrained our permutations so that the permutations always start with the same speaker as the original dialogue and turns strictly alternate between the speakers. With these constraints there are still 5! x 5! = 14400 possible permutations per dialogue. We selected </context>
</contexts>
<marker>Bratt, Dowding, Hunicke-Smith, 1995</marker>
<rawString>Harry Bratt, John Dowding, and Kate Hunicke-Smith. 1995. The sri telephone-based atis system. In Proceedings of the Spoken Language Systems Technology Workshop, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<date>2006</date>
<booktitle>In proceedings of EACL-2006.</booktitle>
<contexts>
<context position="24045" citStr="Callison-Burch et al., 2006" startWordPosition="3977" endWordPosition="3980"> task, widely used in discourse coherence modeling, can be adopted as a testbed for evaluating dialogue coherence models as well. Here we have shown that simple n-gram statistics that are sensitive to local features correlate well with human judgments for coherence and can be used as an evaluation understudy for dialogue coherence models. As with any evaluation understudy, one must be careful while using it as the correlation with human judgments is not perfect and may be inaccurate in some cases – it can not completely replace the need for full evaluation with human judges in all cases (see (Callison-Burch et al., 2006) for a critique of BLUE along these lines). In the future, we would like to perform more experiments with larger data sets and different types of dialogues. It will also be interesting to see the role cohesive devices play in coherence ratings. We would like to see if there are any other measures or certain modifications to the current ones that correlate better with human judgments. We also plan to employ this evaluation metric as feedback in building dialogue coherence models as is done in machine translation (Och, 2003). Acknowledgments The effort described here has been sponsored by the U.</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. In proceedings of EACL-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wieland Eckert</author>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
</authors>
<title>User modeling for spoken dialogue system evaluation.</title>
<date>1997</date>
<booktitle>In Automatic Speech Recognition and Understanding,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="5803" citStr="Eckert et al., 1997" startWordPosition="926" endWordPosition="929">DISE (Walker et al., 2000) was an attempt at reducing the human involvement in evaluation. It builds a predictive model for user satisfaction as a linear combination of some objective measures and perceived task completion. Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in </context>
</contexts>
<marker>Eckert, Levin, Pieraccini, 1997</marker>
<rawString>Wieland Eckert, Esther Levin, and Roberto Pieraccini. 1997. User modeling for spoken dialogue system evaluation. In Automatic Speech Recognition and Understanding, pages 80–87, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>David Traum</author>
</authors>
<title>Creating spoken dialogue characters from corpora without annotations.</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech-07.</booktitle>
<contexts>
<context position="8499" citStr="Gandhe and Traum (2007)" startWordPosition="1350" endWordPosition="1353">173 information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue models which do not depend on hand-authored corpora or rules. Recently Gandhe and Traum (2007) proposed dialogue models that do not need annotations for dialogue-acts, semantics and hand-authored rules for information state updates or finite state machines. Such dialogue models focus primarily on generating an appropriate coherent response given the dialogue history. In certain cases the generation of a response can be reduced to selection from a set of available responses. For such dialogue models, maintaining the information state can be considered as a secondary goal. The element that is common to the information ordering task and the task of selecting next most appropriate response</context>
</contexts>
<marker>Gandhe, Traum, 2007</marker>
<rawString>Sudeep Gandhe and David Traum. 2007. Creating spoken dialogue characters from corpora without annotations. In Proceedings of Interspeech-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sudeep Gandhe</author>
<author>Andrew Gordon</author>
<author>David Traum</author>
</authors>
<title>Improving question-answering with linking dialogues.</title>
<date>2006</date>
<booktitle>In International Conference on Intelligent User Interfaces (IUI),</booktitle>
<marker>Gandhe, Gordon, Traum, 2006</marker>
<rawString>Sudeep Gandhe, Andrew Gordon, and David Traum. 2006. Improving question-answering with linking dialogues. In International Conference on Intelligent User Interfaces (IUI), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalliroi Georgila</author>
<author>James Henderson</author>
<author>Oliver Lemon</author>
</authors>
<title>User simulation for spoken dialogue systems: Learning and evaluation.</title>
<date>2006</date>
<booktitle>In proceedings of Interspeech.</booktitle>
<contexts>
<context position="6166" citStr="Georgila et al., 2006" startWordPosition="986" endWordPosition="989">t still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that</context>
</contexts>
<marker>Georgila, Henderson, Lemon, 2006</marker>
<rawString>Kalliroi Georgila, James Henderson, and Oliver Lemon. 2006. User simulation for spoken dialogue systems: Learning and evaluation. In proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
</authors>
<title>Hybrid reinforcement/supervised learning for dialogue policies from communicator data.</title>
<date>2005</date>
<booktitle>In proceedings of IJCAI workshop.</booktitle>
<contexts>
<context position="6374" citStr="Henderson et al., 2005" startWordPosition="1020" endWordPosition="1023">ng simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations. Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual</context>
</contexts>
<marker>Henderson, Lemon, Georgila, 2005</marker>
<rawString>James Henderson, Oliver Lemon, and Kallirroi Georgila. 2005. Hybrid reinforcement/supervised learning for dialogue policies from communicator data. In proceedings of IJCAI workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate S Hone</author>
<author>Robert Graham</author>
</authors>
<title>Towards a tool for the subjective assessment of speech system interfaces (SASSI). Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</title>
<date>2000</date>
<contexts>
<context position="5013" citStr="Hone and Graham, 2000" startWordPosition="802" endWordPosition="805">r components like WER for ASR (Jurafsky and Martin, 2000), concept error rate or F-scores for NLU, understandability for speech synthesis etc. Usually the core component, the dialogue model - which is responsible for keeping track of the dialogue progression and coming up with an appropriate response, is evaluated indirectly. Different dialogue models can be compared with each other by keeping the rest of components fixed and then by comparing the dialogue systems as a whole. Dialogue systems can report subjective measures such as user satisfaction scores and perceived task completion. SASSI (Hone and Graham, 2000) prescribes a set of questions used for eliciting such subjective assessments. The objective evaluation metrics can include dialogue efficiency and quality measures. PARADISE (Walker et al., 2000) was an attempt at reducing the human involvement in evaluation. It builds a predictive model for user satisfaction as a linear combination of some objective measures and perceived task completion. Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective</context>
</contexts>
<marker>Hone, Graham, 2000</marker>
<rawString>Kate S. Hone and Robert Graham. 2000. Towards a tool for the subjective assessment of speech system interfaces (SASSI). Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>SPEECH and LANGUAGE PROCESSING: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</title>
<date>2000</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="4448" citStr="Jurafsky and Martin, 2000" startWordPosition="712" endWordPosition="715">tion, we look at related work in evaluation of dialogue systems. Section 3 summarizes the information ordering task and Lapata’s (2006) findings. It is followed by the details of the experiments we carried out and our observations. We conclude with a summary future work directions. 2 Related Work Most of the work on evaluating dialogue systems focuses on human-machine communication geared towards a specific task. A variety of evaluation metrics can be reported for such task-oriented dialogue systems. Dialogue systems can be judged based on the performance of their components like WER for ASR (Jurafsky and Martin, 2000), concept error rate or F-scores for NLU, understandability for speech synthesis etc. Usually the core component, the dialogue model - which is responsible for keeping track of the dialogue progression and coming up with an appropriate response, is evaluated indirectly. Different dialogue models can be compared with each other by keeping the rest of components fixed and then by comparing the dialogue systems as a whole. Dialogue systems can report subjective measures such as user satisfaction scores and perceived task completion. SASSI (Hone and Graham, 2000) prescribes a set of questions used</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2000. SPEECH and LANGUAGE PROCESSING: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice G Kendall</author>
</authors>
<title>A new measure of rank correlation.</title>
<date>1938</date>
<journal>Biometrika,</journal>
<pages>30--81</pages>
<contexts>
<context position="11455" citStr="Kendall, 1938" startWordPosition="1827" endWordPosition="1828">nt is 0.45 (p &lt; 0.01)). Before adapting the information ordering task for dialogues, certain questions need to be answered. We need to validate that humans can reliably perform the task of information ordering and can judge the coherence for different sequences of dialogue turns. We also need to find which objective measures (like Kendall’s T) correlate well with human judgments. 4 Evaluating Information Ordering One of the advantages of using information ordering as a testbed is that there are objective measures available to evaluate the performance of information ordering task. Kendall’s T (Kendall, 1938), a rank correlation coefficient, is one such measure. Given a reference sequence of length n, Kendall’s T for an observed sequence can be defined as, # concordant pairs − # discordant pairs # total pairs Each pair of elements in the observed sequence is marked either as concordant - appearing in the same order as in reference sequence or as discordant otherwise. The total number of pairs is Cn2 = n(n − 1)/2. T ranges from -1 to 1. Another possible measure can be defined as the fraction of n-grams from reference sequence, that are preserved in the observed sequence. # n-grams preserved bn = # </context>
</contexts>
<marker>Kendall, 1938</marker>
<rawString>Maurice G. Kendall. 1938. A new measure of rank correlation. Biometrika, 30:81–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis, An Introduction to Its Methodology 2nd Edition.</title>
<date>2004</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="15908" citStr="Krippendorff, 2004" startWordPosition="2606" endWordPosition="2607">perfectly coherent. We did not provide any additional instructions or examples of scale as we wanted to capture the intuitive idea of coherence from our judges. Within each set the dialogue permutations were presented in random order. We compute the inter-rater agreement by using Pearson’s correlation analysis. We correlate the ratings given by each judge with the average ratings given by the judges who were assigned the same set. For inter-rater agreement we report the average of 9 such correlations which is 0.73 (std dev = 0.07). Artstein and Poesio (2008) have argued that Krippendorff’s α (Krippendorff, 2004) can be used for interrater agreement with interval scales like the one we have. In our case for the three sets α values were 0.49, 0.58, 0.64. These moderate values of alpha indicate that the task of judging coherence is indeed a difficult task, especially when detailed instructions or examples of scales are not given. In order to assess whether Kendall’s T can be used as an automatic measure of dialogue coherence, we perform a correlation analysis of T values against the average ratings by human judges. The Pearson’s correlation coefficient is 0.35 and it is statistically not significant (P=</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis, An Introduction to Its Methodology 2nd Edition. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Probabilistic text structuring: Experiments with sentence ordering.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="7808" citStr="Lapata, 2003" startWordPosition="1239" endWordPosition="1240">the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002). Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used 173 information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue mod</context>
<context position="10395" citStr="Lapata (2003)" startWordPosition="1660" endWordPosition="1661">m based or even a reinforcement learning system based on MDP or POMDP. Third, there are simple objective measures available to evaluate the success of information ordering task. Recently, Purandare and Litman (2008) have used 1These can also be at the utterance level, but for this paper we will use dialogue turns. this task for modeling dialogue coherence. But they only allow for a binary classification of sequences as either coherent or incoherent. For comparing different dialogue coherence models, we need the ability for finer distinction between sequences of information being put together. Lapata (2003) proposed Kendall’s T, a rank correlation measure, as one such candidate. In a recent study they show that Kendall’s T correlates well with human judgment (Lapata, 2006). They show that human judges can reliably provide coherence ratings for various permutations of text. (Pearson’s correlation for inter-rater agreement is 0.56) and that Kendall’s T is a good indicator for human judgment (Pearson’s correlation for Kendall’s T with human judgment is 0.45 (p &lt; 0.01)). Before adapting the information ordering task for dialogues, certain questions need to be answered. We need to validate that human</context>
</contexts>
<marker>Lapata, 2003</marker>
<rawString>Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic evaluation of information ordering.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>484</pages>
<contexts>
<context position="648" citStr="Lapata, 2006" startWordPosition="92" endWordPosition="93">ue Coherence Models Sudeep Gandhe and David Traum Institute for Creative Technologies University of Southern California 13274 Fiji way, Marina del Rey, CA, 90292 {gandhe,traum}@ict.usc.edu Abstract Evaluating a dialogue system is seen as a major challenge within the dialogue research community. Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. 1 Introduction In computer science or any other research field, simply building a system that accomplishes a certain goal is not enough. It needs to be thoroughly evaluated. One might want to evaluate the system just to see to what</context>
<context position="3339" citStr="Lapata (2006)" startWordPosition="534" endWordPosition="535">n automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, Columbus, June 2008. c�2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models. In the next section, we look at related work in evaluation of dialogue systems. Section 3 summarizes the information ordering task a</context>
<context position="10564" citStr="Lapata, 2006" startWordPosition="1687" endWordPosition="1688">ng task. Recently, Purandare and Litman (2008) have used 1These can also be at the utterance level, but for this paper we will use dialogue turns. this task for modeling dialogue coherence. But they only allow for a binary classification of sequences as either coherent or incoherent. For comparing different dialogue coherence models, we need the ability for finer distinction between sequences of information being put together. Lapata (2003) proposed Kendall’s T, a rank correlation measure, as one such candidate. In a recent study they show that Kendall’s T correlates well with human judgment (Lapata, 2006). They show that human judges can reliably provide coherence ratings for various permutations of text. (Pearson’s correlation for inter-rater agreement is 0.56) and that Kendall’s T is a good indicator for human judgment (Pearson’s correlation for Kendall’s T with human judgment is 0.45 (p &lt; 0.01)). Before adapting the information ordering task for dialogues, certain questions need to be answered. We need to validate that humans can reliably perform the task of information ordering and can judge the coherence for different sequences of dialogue turns. We also need to find which objective measu</context>
<context position="13625" citStr="Lapata, 2006" startWordPosition="2217" endWordPosition="2218">ue, we chose these 9 dialogues from different sources. Three of these were excerpts from role-play dialogues involving negotiations which were originally collected for a simulation training scenario (Traum et al., 2005). Three are from SRI’s Amex Travel Agent data which are task-oriented dialogues about air travel planning (Bratt et al., 1995). The rest of the dialogues are scripts from popular television shows. Fig 6 shows an example from the air-travel domain. Each excerpt drawn was 10 turns long with turns strictly alternating between the two speakers. Following the experimental design of (Lapata, 2006) we created random permutations for these dialogue segments. We constrained our permutations so that the permutations always start with the same speaker as the original dialogue and turns strictly alternate between the speakers. With these constraints there are still 5! x 5! = 14400 possible permutations per dialogue. We selected 3 random permutations for each of the 9 dialogues. In all, we have a total of 27 dialogue permutations. They are arranged in 3 sets, each set containing a permutation for all 9 dialogues. We ensured that not all permutations in a given set are particularly very good o</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic evaluation of information ordering. Computational Linguistics, 32(4):471– 484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>Learning dialogue strategies within the markov decision process framework.</title>
<date>1997</date>
<booktitle>In Automatic Speech Recognition and Understanding,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="5900" citStr="Levin et al., 1997" startWordPosition="943" endWordPosition="946">s a predictive model for user satisfaction as a linear combination of some objective measures and perceived task completion. Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states </context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 1997</marker>
<rawString>Esther Levin, Roberto Pieraccini, and Wieland Eckert. 1997. Learning dialogue strategies within the markov decision process framework. In Automatic Speech Recognition and Understanding, pages 72–79, Dec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: a package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out.</booktitle>
<contexts>
<context position="3097" citStr="Lin, 2004" startWordPosition="499" endWordPosition="500"> forbid the use of data-driven machine learning components. For these reasons, using an automatic evaluation measure as an understudy is quickly becoming a common practice in natural language processing tasks. The general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, Columbus, June 2008. c�2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the informat</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: a package for automatic evaluation of summaries. In Proceedings of the Workshop on Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation. In</title>
<date>2003</date>
<booktitle>In ACL 2003: Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In In ACL 2003: Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore A Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation. In</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022),</tech>
<institution>IBM Research Division,</institution>
<contexts>
<context position="951" citStr="Papineni et al., 2001" startWordPosition="140" endWordPosition="143">. Due to the very nature of the task, most of the evaluation methods need a substantial amount of human involvement. Following the tradition in machine translation, summarization and discourse coherence modeling, we introduce the the idea of evaluation understudy for dialogue coherence models. Following (Lapata, 2006), we use the information ordering task as a testbed for evaluating dialogue coherence models. This paper reports findings about the reliability of the information ordering task as applied to dialogues. We find that simple n-gram co-occurrence statistics similar in spirit to BLEU (Papineni et al., 2001) correlate very well with human judgments for dialogue coherence. 1 Introduction In computer science or any other research field, simply building a system that accomplishes a certain goal is not enough. It needs to be thoroughly evaluated. One might want to evaluate the system just to see to what degree the goal is being accomplished or to compare two or more systems with one another. Evaluation can also lead to understanding the shortcomings of the system and the reasons for these. Finally the evaluation results can be used as feedback in improving the system. The best way to evaluate a novel</context>
<context position="3054" citStr="Papineni et al., 2001" startWordPosition="491" endWordPosition="494">eature selection. This need for repeated evaluation can forbid the use of data-driven machine learning components. For these reasons, using an automatic evaluation measure as an understudy is quickly becoming a common practice in natural language processing tasks. The general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, Columbus, June 2008. c�2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. In Technical Report RC22176 (W0109-022), IBM Research Division, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Diane Litman</author>
</authors>
<title>Analyzing dialog coherence using transition patterns in lexical and semantic features.</title>
<date>2008</date>
<booktitle>In Proceedings 21st International FLAIRS Conference,</booktitle>
<contexts>
<context position="9997" citStr="Purandare and Litman (2008)" startWordPosition="1593" endWordPosition="1596">by using information ordering as a task to evaluate dialogue coherence models. First the task does not require a dialogue model to take part in conversations in an interactive manner. This obviates the need for having real users engaging in the dialogue with the system. Secondly, the task is agnostic about the underlying dialogue model. It can be a data-driven statistical model or information-state based, form based or even a reinforcement learning system based on MDP or POMDP. Third, there are simple objective measures available to evaluate the success of information ordering task. Recently, Purandare and Litman (2008) have used 1These can also be at the utterance level, but for this paper we will use dialogue turns. this task for modeling dialogue coherence. But they only allow for a binary classification of sequences as either coherent or incoherent. For comparing different dialogue coherence models, we need the ability for finer distinction between sequences of information being put together. Lapata (2003) proposed Kendall’s T, a rank correlation measure, as one such candidate. In a recent study they show that Kendall’s T correlates well with human judgment (Lapata, 2006). They show that human judges can</context>
</contexts>
<marker>Purandare, Litman, 2008</marker>
<rawString>Amruta Purandare and Diane Litman. 2008. Analyzing dialog coherence using transition patterns in lexical and semantic features. In Proceedings 21st International FLAIRS Conference, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Roque</author>
<author>Hua Ai</author>
<author>David Traum</author>
</authors>
<title>Evaluation of an information state-based dialogue manager.</title>
<date>2006</date>
<booktitle>In Brandial 2006: The 10th Workshop on the Semantics and Pragmatics of Dialogue.</booktitle>
<contexts>
<context position="6455" citStr="Roque et al. (2006)" startWordPosition="1035" endWordPosition="1038">for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations. Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual humans used in simulation training (Traum et al., 2005). For such systems, the n</context>
</contexts>
<marker>Roque, Ai, Traum, 2006</marker>
<rawString>Antonio Roque, Hua Ai, and David Traum. 2006. Evaluation of an information state-based dialogue manager. In Brandial 2006: The 10th Workshop on the Semantics and Pragmatics of Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Karl Weilhammer</author>
<author>Hui Ye</author>
<author>Steve Young</author>
</authors>
<title>Agenda-based user simulation for bootstrapping a pomdp dialogue system.</title>
<date>2007</date>
<booktitle>In proceedings of HLT/NAACL,</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="6057" citStr="Schatzmann et al., 2007" startWordPosition="968" endWordPosition="971"> to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., for</context>
</contexts>
<marker>Schatzmann, Thomson, Weilhammer, Ye, Young, 2007</marker>
<rawString>Jost Schatzmann, Blaise Thomson, Karl Weilhammer, Hui Ye, and Steve Young. 2007. Agenda-based user simulation for bootstrapping a pomdp dialogue system. In proceedings of HLT/NAACL, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse generation using utility-trained coherence models.</title>
<date>2006</date>
<booktitle>In Proc. ACL-06.</booktitle>
<contexts>
<context position="3324" citStr="Soricut and Marcu, 2006" startWordPosition="530" endWordPosition="533"> general idea is to find an automatic evaluation metric that correlates very well with human judgments. This allows developers to use the automatic metric as a stand-in for human evaluation. Although it cannot replace the finesse of human evaluation, it can provide a crude idea of progress which can later be validated. e.g. BLEU (Papineni et al., 2001) for machine translation, ROUGE (Lin, 2004) for summarization. Recently, the discourse coherence modeling community has started using the information ordering task as a testbed to test their discourse coherence models (Barzilay and Lapata, 2005; Soricut and Marcu, 2006). Lapata (2006) has proposed an au172 Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172–181, Columbus, June 2008. c�2008 Association for Computational Linguistics tomatic evaluation measure for the information ordering task. We propose to use the same task as a testbed for dialogue coherence modeling. We evaluate the reliability of the information ordering task as applied to dialogues and propose an evaluation understudy for dialogue coherence models. In the next section, we look at related work in evaluation of dialogue systems. Section 3 summarizes the information </context>
<context position="7861" citStr="Soricut and Marcu, 2006" startWordPosition="1246" endWordPosition="1249">eness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for textto-text generation like in single or multi-document summarization (Barzilay et al., 2002). Recently there has been a lot of work in discourse coherence modeling (Lapata, 2003; Barzilay and Lapata, 2005; Soricut and Marcu, 2006) that has used 173 information ordering to test the coherence models. The information-bearing elements here are sentences rather than high-level concepts. This frees the models from having to depend on a hard to get training corpus which has been hand-authored for concepts. Most of the dialogue models still work at the higher abstraction level of dialogue acts and intentions. But with an increasing number of dialogue systems finding use in non-traditional applications such as simulation training, games, etc.; there is a need for dialogue models which do not depend on hand-authored corpora or r</context>
</contexts>
<marker>Soricut, Marcu, 2006</marker>
<rawString>Radu Soricut and Daniel Marcu. 2006. Discourse generation using utility-trained coherence models. In Proc. ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>James F Allen</author>
</authors>
<title>Discourse obligations in dialogue processing.</title>
<date>1994</date>
<booktitle>In proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94),</booktitle>
<pages>1--8</pages>
<contexts>
<context position="21909" citStr="Traum and Allen (1994)" startWordPosition="3632" endWordPosition="3635">l context. Fig 4 shows the relationship between these two measures. Notice that most of the orderings have T values around zero (i.e. in the middle range for T), whereas majority of orderings will have a low value for (b2 + b3) /2. T seems to overestimate the coherence even in the absence of immediate local coherence (See third entry in table 1). It seems that local context is more important for dialogues than for discourse, which may follow from the fact that dialogues are produced by two speakers who must react to each other, while discourse can be planned by one speaker from the beginning. Traum and Allen (1994) point out that such social obligations to respond and address the contributions of the other should be an important factor in building dialogue systems. The information ordering paradigm does not take into account the content of the information-bearing items, e.g. the fact that turns like ”yes”, ”I agree”, 5This value is calculated by considering all 14400 permutations as equally likely. 177 (a) Histogram of Kendall’s T for reordered se- (b) Histogram of fraction of bigrams &amp; triquences grams values for reordered sequences Figure 3: Experiment 3 - upper baseline for information ordering task </context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>David R. Traum and James F. Allen. 1994. Discourse obligations in dialogue processing. In proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL-94), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Susan Robinson</author>
<author>Jens Stephan</author>
</authors>
<title>Evaluation of multi-party virtual reality dialogue interaction. In</title>
<date>2004</date>
<booktitle>In Proceedings of Fourth International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1699--1702</pages>
<marker>Traum, Robinson, Stephan, 2004</marker>
<rawString>David R. Traum, Susan Robinson, and Jens Stephan. 2004. Evaluation of multi-party virtual reality dialogue interaction. In In Proceedings of Fourth International Conference on Language Resources and Evaluation (LREC), pages 1699–1702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>William Swartout</author>
<author>Jonathan Gratch</author>
<author>Stacy Marsella</author>
</authors>
<title>Virtual humans for non-team interaction training.</title>
<date>2005</date>
<booktitle>In AAMAS-05 Workshop on Creating Bonds with Humanoids,</booktitle>
<contexts>
<context position="7030" citStr="Traum et al., 2005" startWordPosition="1117" endWordPosition="1120">filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations. Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual humans used in simulation training (Traum et al., 2005). For such systems, the notion of task completion or efficiency is not well defined and task specific objective measures are hardly suitable. Most evaluations report the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a set of information bearing elements. This task is well suited for </context>
<context position="13231" citStr="Traum et al., 2005" startWordPosition="2151" endWordPosition="2154">s. Here the reference sequence is [0,1,2,3,4,5,6,7,8,9]. respective b2, b3 and T values. Notice how T allows for long-distance relationships whereas b2, b3 are sensitive to local features only. 2 5 Experimental Setup For our experiments we used segments drawn from 9 dialogues. These dialogues were two-party humanhuman dialogues. To ensure applicability of our results over different types of dialogue, we chose these 9 dialogues from different sources. Three of these were excerpts from role-play dialogues involving negotiations which were originally collected for a simulation training scenario (Traum et al., 2005). Three are from SRI’s Amex Travel Agent data which are task-oriented dialogues about air travel planning (Bratt et al., 1995). The rest of the dialogues are scripts from popular television shows. Fig 6 shows an example from the air-travel domain. Each excerpt drawn was 10 turns long with turns strictly alternating between the two speakers. Following the experimental design of (Lapata, 2006) we created random permutations for these dialogue segments. We constrained our permutations so that the permutations always start with the same speaker as the original dialogue and turns strictly alternate</context>
</contexts>
<marker>Traum, Swartout, Gratch, Marsella, 2005</marker>
<rawString>David Traum, William Swartout, Jonathan Gratch, and Stacy Marsella. 2005. Virtual humans for non-team interaction training. In AAMAS-05 Workshop on Creating Bonds with Humanoids, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>C Kamm</author>
<author>D Litman</author>
</authors>
<title>Towards developing general models of usability with PARADISE. Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</title>
<date>2000</date>
<contexts>
<context position="5209" citStr="Walker et al., 2000" startWordPosition="831" endWordPosition="834">is responsible for keeping track of the dialogue progression and coming up with an appropriate response, is evaluated indirectly. Different dialogue models can be compared with each other by keeping the rest of components fixed and then by comparing the dialogue systems as a whole. Dialogue systems can report subjective measures such as user satisfaction scores and perceived task completion. SASSI (Hone and Graham, 2000) prescribes a set of questions used for eliciting such subjective assessments. The objective evaluation metrics can include dialogue efficiency and quality measures. PARADISE (Walker et al., 2000) was an attempt at reducing the human involvement in evaluation. It builds a predictive model for user satisfaction as a linear combination of some objective measures and perceived task completion. Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>M. Walker, C. Kamm, and D. Litman. 2000. Towards developing general models of usability with PARADISE. Natural Language Engineering: Special Issue on Best Practice in Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wallace</author>
</authors>
<title>Be Your Own Botmaster, 2nd Edition.</title>
<date>2003</date>
<journal>ALICE A. I. Foundation.</journal>
<contexts>
<context position="6937" citStr="Wallace, 2003" startWordPosition="1105" endWordPosition="1106">in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations. Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual humans used in simulation training (Traum et al., 2005). For such systems, the notion of task completion or efficiency is not well defined and task specific objective measures are hardly suitable. Most evaluations report the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a p</context>
</contexts>
<marker>Wallace, 2003</marker>
<rawString>Richard Wallace. 2003. Be Your Own Botmaster, 2nd Edition. ALICE A. I. Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Weizenbaum</author>
</authors>
<title>Eliza–a computer program for the study of natural language communication between man and machine.</title>
<date>1966</date>
<journal>Communications of the ACM,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="6963" citStr="Weizenbaum, 1966" startWordPosition="1108" endWordPosition="1109">em. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman conversations. Such dialogue systems can range from chatbots like Alice (Wallace, 2003), Eliza (Weizenbaum, 1966) to virtual humans used in simulation training (Traum et al., 2005). For such systems, the notion of task completion or efficiency is not well defined and task specific objective measures are hardly suitable. Most evaluations report the subjective evaluations for appropriateness of responses. Traum et. al. (2004) propose a coding scheme for response appropriateness and scoring functions for those categories. Gandhe et. al. (2006) propose a scale for subjective assessment for appropriateness. 3 Information Ordering The information ordering task consists of choosing a presentation sequence for a</context>
</contexts>
<marker>Weizenbaum, 1966</marker>
<rawString>Joseph Weizenbaum. 1966. Eliza–a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1):36–45, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>422</pages>
<contexts>
<context position="5927" citStr="Williams and Young, 2006" startWordPosition="947" endWordPosition="950"> for user satisfaction as a linear combination of some objective measures and perceived task completion. Even then the system needs to train on the data gathered from user surveys and objective features retrieved from logs of dialogue runs. It still needs to run the actual dialogue system and collect objective features and perceived task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate t</context>
</contexts>
<marker>Williams, Young, 2006</marker>
<rawString>Jason D. Williams and Steve Young. 2006. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21:393– 422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
</authors>
<title>A method for evaluating and comparing user simulations: The cramer-von mises divergence.</title>
<date>2007</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="6250" citStr="Williams, 2007" startWordPosition="1003" endWordPosition="1004">d task completeion to predict user satisfaction. Other efforts in saving human involvement in evaluation include using simulated users for testing (Eckert et al., 1997). This has become a popular tool for systems employing reinforcement learning (Levin et al., 1997; Williams and Young, 2006). Some of the methods involved in user simulation are as complex as building dialogue systems themselves (Schatzmann et al., 2007). User simulations also need to be evaluated as how closely they model human behavior (Georgila et al., 2006) or as how good a predictor they are of dialogue system performance (Williams, 2007). Some researchers have proposed metrics for evaluating a dialogue model in a task-oriented system. (Henderson et al., 2005) used the number of slots in a frame filled and/or confirmed. Roque et al. (2006) proposed hand-annotating information-states in a dialogue to evaluate the accuracy of information state updates. Such measures make assumptions about the underlying dialogue model being used (e.g., form-based or information-state based etc.). We are more interested in evaluating types of dialogue systems that do not follow these task-based assumptions: systems designed to imitate humanhuman </context>
</contexts>
<marker>Williams, 2007</marker>
<rawString>Jason D. Williams. 2007. A method for evaluating and comparing user simulations: The cramer-von mises divergence. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>