<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041924">
<title confidence="0.9968195">
Sbdlrhmn: A Rule-based Human Interpretation System for
Semantic Textual Similarity Task
</title>
<author confidence="0.873107">
Samir AbdelRahman Catherine Blake
</author>
<email confidence="0.873368">
sbdlrhmn@illinois.edu, clblake@illinois.edu
</email>
<author confidence="0.271811">
s.abdelrahman@fci-cu.edu.eg
</author>
<affiliation confidence="0.4489485">
The Graduate School of Library and Information Science
University Of Illinois at Urbana-Champaign
</affiliation>
<sectionHeader confidence="0.969209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.806275619047619">
In this paper, we describe the system architec-
ture used in the Semantic Textual Similarity
(STS) task 6 pilot challenge. The goal of this
challenge is to accurately identify five levels
of semantic similarity between two sentences:
equivalent, mostly equivalent, roughly equiva-
lent, not equivalent but sharing the same topic
and no equivalence. Our participations were
two systems. The first system (rule-based)
combines both semantic and syntax features to
arrive at the overall similarity. The proposed
rules enable the system to adequately handle
domain knowledge gaps that are inherent
when working with knowledge resources. As
such one of its main goals, the system sug-
gests a set of domain-free rules to help the
human annotator in scoring semantic equiva-
lence of two sentences. The second system is
our baseline in which we use the Cosine Simi-
larity between the words in each sentence
pair.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999681577777778">
Accurately establishing sentence semantic similari-
ty would provide one of the key ingredients for
solutions to many text-related applications, such as
automatic grading systems (Mohler and Mihalcea,
2009), paraphrasing (Fernando and Stevenson,
2008), text entailment (Corley et al., 2005) and
summarization (Erkan and Radev, 2004). Current
approaches for computing semantic similarity be-
tween a pair of sentences focus on analyzing their
shared words (Salton, 1989), structures (Hu et al.
2011;Mandreoli et al. 2002), semantics (Mihalcea
et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999)
or any of their combinations (Liu et al. 2008; Foltz
et al. 1998). The goal is to arrive at a score which
increases proportionally with the relatedness be-
tween the two sentences. Yet, they are not con-
cerned with scoring the interpretations of such
relatedness (Zhang et al. 2011; Jesus et al. 2011;
Wenyin et al. 2010; Liu et al. 2008).
Semantic Textual Similarity (STS), SEMEVAL-
12 Task 6 (Agirre et al. 2012), measures the degree
of semantic equivalence between a pair of sentenc-
es by comparing meaningful contents within a sen-
tence. The assigned scores range from 0 to 5 for
each sentence pair with the following interpreta-
tions: (5) completely equivalent, (4) mostly
equivalent pair with missing unimportant infor-
mation, (3) roughly equivalent with missing im-
portant information, (2) not equivalent, but sharing
some details, (1) not equivalent but sharing the
same topic and (0) not equivalent and on different
topics. The goal of developing our rule-based sys-
tem was to identify knowledge representations
which have possibly all task human interpretations.
Meanwhile, the system domain-free rules aim to
help the human annotator in scoring semantic
equivalence of sentence pair.
The proposed rule-based solution exploits both
sentence syntax and semantics. First, it uses Stan-
ford parser (Klein and Manning, 2002) to expose
the sentence structure, part-of-speech (POS) word
tags, parse tree and Subject-Verb-Object (S-V-O)
dependencies. Second, Illinois Coreference Pack-
age (Bengtson and Roth, 2008) is used to extract
sentence named entities resolving possible men-
</bodyText>
<page confidence="0.973449">
536
</page>
<note confidence="0.851205">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99967725">
tions. Third, WordNet (Miller, 1995) and Adapted
Lesk Algorithm for word sense disambiguation
(Banerjee and Pedersen, 2010) are used to compute
each sentence word semantic relatedness to the
other sentence. ReVerb (Etzioni et al. 2011) aug-
ments WordNet in case of uncovered words and
helps us to discriminate the topics of sentences.
We use (Blake, 2007) thought to compare the sen-
tence pair words with each other. Finally, we
evolve a rule-based module to present the human
heuristics when he interprets the relatedness of the
sentence pair meaningful contents.
Throughout our training and testing experi-
ments, we used Task6 corpora (Agirre et al. 2012)
namely MSRpar, MSRvid, SMTeuroparl, OnWN
and SMTnews; where:
- MSRpar is 1500 pairs of sentences of MSR-
Paraphrase, Microsoft Research Paraphrase Cor-
pus; 750 for training and 750 for testing.
- MSRvid is 1500 pairs of sentences of MSR-
Video, Microsoft Research Video Description
Corpus; 750 for training and 750 for testing.
- SMTeuroparl is 918 pairs of sentences of
WMT2008 development dataset (Europarl sec-
tion); 459 for training and 459 for testing.
- OnWn is 750 pairs of sentences pairs of sen-
tences where the first sentence comes from On-
tonotes and the second sentence from a WordNet
definition; it is only a testing corpus.
- SMTnews is 399 pairs of sentences of news
conversation sentence pairs from WMT; it is on-
ly a testing corpus.
The reminder of this paper is organized as fol-
lows: Section 2 describes our two participations;
Section 3 discusses their official results; Section 4
draws our conclusion for both systems.
</bodyText>
<sectionHeader confidence="0.967866" genericHeader="method">
2 The Proposed Systems
</sectionHeader>
<bodyText confidence="0.9999339">
In this section, we focus on the rule-based system,
Sections 2.1, 2.2, 2.3 and 2.4, as our main task
contribution. Further, the section describes our se-
cond run, Sections 2.5, to shed light on the role of
cosine similarity for solving the task problem. To
establish the task semantic textual similarity, we
show how the rule-based system exploits the sen-
tence semantic, syntax and heuristics; also, we de-
scribe how our base-line system uses the sentence
syntax only.
</bodyText>
<subsectionHeader confidence="0.969609">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.999830193548387">
We say the two sentences are on different topics, if
all their verbs are mostly (&gt; 50%) unrelated (Table
1). Otherwise, they are on the same topic. For ex-
ample, the two sentences “A woman is putting on
makeup.”, “A band is singing.” are on different
topics as “putting”, “singing” are not equivalent.
However, the two sentences “A baby is talking.”,
A boy is trying to say firetruck.” are on the same
topics as “talking” and “trying to say” are seman-
tically equivalent.
We define the sentence important information as
its head nouns, named entities or main verbs;
where the main verbs are all verbs except auxilia-
ry, model and infinitive ones. Hence, we say that
two sentences miss important information if either
loses at least one of these mentions from the other.
Otherwise, they are candidates to be semantically
equivalent. For example, the sentence “Besides
Hampton and Newport News, the grant funds wa-
ter testing in Yorktown, King George County, Nor-
folk and Virginia Beach.” misses “Hampton and
Newport News” compared to the sentence “The
grant also funds beach testing in King George
County, Norfolk and Virginia Beach.” However,
“on a table” is unimportant information which “A
woman is tapping her fingers.” misses compared
to “A woman is tapping her fingers on a table.”
Finally, we deploy a list of stop words and non-
verbs as unimportant information. However, if any
exists in both sentences, we match them with each
other; otherwise we ignore any occurrences.
</bodyText>
<subsectionHeader confidence="0.998257">
2.2 The Syntactic Module
</subsectionHeader>
<bodyText confidence="0.9993285">
This syntactic module is a preprocessing module in
which the system calls Stanford parser, Version
2.0.1, and the Illinois coreference package, Version
1.3.2, to result in the sentence four type representa-
tions: 1) part of speech (POS) tags, 2) Subject-
Verb-Object (S-V-O), Subject-Verb (S-V) and
Verb-Object (V-O) dependencies, 3) parse tree and
4) coreference resolutions. All sentences are lem-
matized based on their POSs. Also, verbs and CDs
are utilized to determine topics/important infor-
mation and numbers respectively. All noun and
verb phrases are used to boost the sentence word
semantic scores (Section 2.3). We consider all oc-
currences of S-V-O, S-V and V-O to distinguish
</bodyText>
<page confidence="0.981697">
537
</page>
<bodyText confidence="0.999505642857143">
the topic compatibility between two comparable
sentences (Section 2.3 and 2.4).
The coreference package is used to match the
equivalent discourse entities between two sentenc-
es which improve the matching steps. For example,
in the pair of “Mrs Hillary Clinton explains her
plan towards the Middle East countries” and “Mrs
Clinton meets their ambassadors”, “Mrs Hillary
Clinton”, “her” and “Mrs Clinton” refer to the
same entity where “the Middle East countries” and
“their” are equivalent. Moreover, we consider the
second sentence doesn’t lose “Hillary” as missing
important information since the related mentions
are labeled equivalent.
</bodyText>
<subsectionHeader confidence="0.998433">
2.3 The Semantic Matching Module
</subsectionHeader>
<bodyText confidence="0.999952055555556">
WordNet, Version 3.0, has approximately 5,947
entries covering around 85% of training corpora
words (Agirre et al. 2012). Most of the remaining
15% words are abbreviations, named entities and
incorrect POS tags. We use WordNet shortest path
measure to compute the semantic similarity be-
tween two words. Also, we use Adapted Lesk algo-
rithm to obtain the best WordNet word sense. The
disambiguation algorithm compares each pair of
words through their contexts (windows) of words
coupled with their all overlapping glosses of all
WordNet relation types.
The semantic matching module inputs are the
sentence pair (S1, S2), their lemmatized words,
parse trees, S-V-O/S-V/V-O dependencies and co-
reference mentions (Section 2.2). It matches syn-
tactically the words with each other. For any
uncovered WordNet word, the module calls Re-
Verb (Section 2.4) and it assigns the returned value
to the word score. All numbers, e.g. million,
300,45.6, are mathematically compared with each
other. This module compares the noun phrases
with single words to handle the compound words,
e.g. “shot gun” with “shotgun” or “part-of-
speech” with “part of speech”. For those words
whose scores are not equal to 1, it compares each
pair of words from the sentence pair within their
Subject-VP (subject with its verb phrase) contexts
using Adapted Lesk algorithm to find best sense
for each included word. Then, it applies WordNet
shortest path measure to score such words. In our
disambiguation algorithm implementation, we
found that the runtime requirement is directly pro-
portional to the input sentence length. So, we
shortened the sentence length to Subject-VP which
includes the underlying comparable words.
</bodyText>
<table confidence="0.98754225">
Relatedness Score (S1, S2)
unrelated 0 &lt;= Ws &lt;0.3
weakly related 0.3 &lt;= Ws &lt;0.85
strongly related Ws &gt;= 0.85
</table>
<tableCaption confidence="0.997712">
Table 1 – Mapping relatedness to wordnet similarity
</tableCaption>
<bodyText confidence="0.994631571428571">
Table 1 describes the proposed system WordNet
thresholds through our relatedness definitions. The
thresholds were thoroughly selected depending on
our analysis for the WordNet hierarchary and
semantic similarity measures (Pedersen et al.,
2004). We obsereved that while most of the nearest
tree sibilings and parent-child nodes scores have
more than 0.85 Wordnet semantic scores, most of
the fartherest ones have scores less than 0.3. In
between these extremes, there is a group of
scattered tree nodes which ranges from 0.3 to 0.85.
The number of nodes per each mentioned group is
related to the semantic simlarity measure
technique.
</bodyText>
<subsectionHeader confidence="0.99268">
2.4 Semantics – Using ReVerb
</subsectionHeader>
<bodyText confidence="0.9999064">
Our working hypothesis is that verbs that use the
same arguments are more likely to be similar. To
estimate verb usage, the system uses frequencies
from the ReVerb (http://openie.cs.washington.
edu/) online interface to count the number of times
a verb is used with two arguments. For example,
consider the sentence pair “The man fires rifle”
and “The man cuts lemon”. The number of sen-
tences in ReVerb that contain the verb fires with
the argument rifle is 538 and the number of sen-
tences for the verb cuts with the argument lemon is
45, which tell us that you are more likely to find
sentences that describe firing a rifle than cutting a
lemon on the web. However, there a no ReVerb
sentences for the verb fires with the argument lem-
on or the verb cuts with the argument rifle. Which
tells us that people generally don’t fire lemons or
cut rifles.
Reverb provides the system with information
about the suitability of using argument in one sen-
tence with verbs from another. Specifically, fre-
quencies from Reverb are retrieved for each
subject-verb-object triple in each sentence, e.g.
“S1-V1-O1” and “S2-V2-O2”. The system then
retrieves ReVerb frequencies for the verb-object in
</bodyText>
<page confidence="0.989812">
538
</page>
<bodyText confidence="0.999861904761905">
each sentence of “V2-O1” and “V1-O2”. If at least
one of all of these scores equals to 0, they are con-
sidered to be weakly similar.
ReVerb is also called for any sentence word that
WordNet doesn’t cover. The system retrieves the
Reverb frequency for is-a relation using the word
missing from Wordnet, as Argument1, and each
word from the other sentence as Argument2. The
largest Reverb retrieved score is taken. Consider
the pair of “A group of girls are exiting a taxi” and
“A video clip of Rihanna leaving a taxi.”. Since
“Rihanna” is not a WordNet word, our ReVerb
interface hits the web for “Rihanna is-a girl”, “Ri-
hanna is-a group”, “Rihanna is-a taxi” and “Ri-
hanna is-a existing” and it returns “Rihanna is-a
girl” as the best candidate with strength score
equals 0.2.
We explored several relatedness scores which
specifically equal to 0, 0.2, 0.4, 0.6, 0.8 or 1 if the
frequencies are less than to 10, 50, 100, 500, 1000
or 1000+ respectively.
</bodyText>
<subsectionHeader confidence="0.991167">
2.5 The Rule-Based Module
</subsectionHeader>
<bodyText confidence="0.999756636363637">
Rule-based module aims at defining human-like
rules to interpret how the pair similar or dissimilar
from each other. Pair Similarity (P) is based on the
strong relatedness values (Table 1) and the Dissim-
ilarity (D) is based on the other types of related-
ness values. As we believe that strong and not
strong are proportional to the pair similarity and
dissimilarity respectively
Rule-based module input is sentence pair S1, S2
word semantic scores, i.e. Ws1s and Ws2s (Table
1). Then, it calculates: 1) their three types of aver-
ages for S1 and S2 semantic scores, i.e. all word
semantic scores, weakly related only and unrelated
values; 2) P as the minimum percentage of strong
Wss in (S1 and S1); 3) D as, 100-P, the percentage
of not strong Wss in S1 and S1
This module outputs the semantic textual simi-
larity semantic (STS) score which ranges from 0 to
5. Throughout this section, when we use “unrelat-
ed”, “weak” and strong terminologies, we use Ta-
ble 1 Relatedness definitions. Also, when we use
“important” term, we refer to our definition (Sec-
tion 2.1)
Human judgments for computing STS score of
the sentence pair are based on word similarities
and dissimilarities. They consider that two sen-
tences are similar if most (&gt; 50%) of their words
are strongly related, otherwise the sentences are
candidates to be dissimilar. Since all Wss range
from 0 to 1, the average of strong scores is more
than the average of weak scores. Likewise, the av-
erage of weak scores is more than the average of
non-related scores.
</bodyText>
<figure confidence="0.974334413793104">
Score(Sentence Ws1s, Sentence Ws2s)
AllAvg = (Ws1s+ Ws1s)/2
WeakAvg= the averaged weakly related scores of
Ws1s and Ws1s
UnRElAvg=the average of unrelated scores of
Ws1s and Ws1s
P = minimum (% Ws1s strong scores, % Ws2s
strong scores)
D=100-P
Value=0
If 95 &lt;= P &lt;=100 then Value = 5;
If 80 &lt;= P &lt; 95 then Value = 4;
If 50 &lt;= P &lt; 80 then Value = 3;
If 20 &lt;= P &lt; 50 then Value = 2;
If 0 &lt;= P &lt; 20 then
If all verbs are strongly related then Value=1
Else Value= 0.0001;
If (Value in [4, 5]) then
If all Ds for important words then Value= 3
If (Value ==3) then
If all Ds for not important words then Value= 2
If (Value &lt;&gt; 5 AND Value &lt;&gt; 0) then
If all Ds for weakly related words
Value= Value+ AllAvg
Else if at least half Ds for weakly related words
Value= Value+ WeakAvg
Otherwise
Value = Value + UnRelAvg
Return Value
</figure>
<bodyText confidence="0.999695384615385">
When we call Score(Ws1s,Ws2s), we take care
of the following two special cases where it goes
directly to Value 3: 1) if missing some words leads
to missing the whole verb/noun phrases and 2) if
one sentence has all past tense verbs and the other
has present verbs.
When we design P inequalities, we make them
have relaxed boundaries conformed with human
grading values. For example, we choose P between
95 and 100 in Value (5); where 95 and 100 equal
to grades 4.5 and 5 respectively. Value (3) interval
are values between more than or equal 2.5 and less
than 4. Then, we utilize the important information
</bodyText>
<page confidence="0.996547">
539
</page>
<bodyText confidence="0.99988952173913">
and verb constraints to direct classifications
through different groups.
When we design range conditions between val-
ues, we select D to present the distance between
the sentence pair. As D weak values increase, the
two sentences become closer. As D unrelated val-
ues increase, the two sentences become distant.
We carefully analyzed the training corpora to
assure that the above thresholds satisfy most of the
training sentence pairs. Each threshold output was
manually checked and adjusted to satisfy around
55% to 75% of the training corpora.
Applying the above module, the pair of “A man
is playing football” and “The man plays football”
STS score equals 5.00. The pair of “A man is sing-
ing and playing” and “The man plays” STS score
equals 3.00 since the first one misses “singing”.
The pair of “The cat is drinking milk.” and “A
white cat is licking and drinking milk kept on a
plate.” STS scores equals to 3.4 since they have
P=0.66, “white” as unimportant information but
“licking”, “ kept”, “plate” as important infor-
mation words.
</bodyText>
<subsectionHeader confidence="0.999481">
2.6 Our Baseline System Description
</subsectionHeader>
<bodyText confidence="0.999954">
Our goal in the second run is to evaluate the relat-
edness of the two sentences using only the words
in the sentence. Sentences are represented as a vec-
tor (i.e. based on the Vector Space Model) and the
similarity between the two sentences S1 and S2 is
(5* cosine similarity). We take into account all
sentence words such that they are lower-case and
non-stemmed.
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="evaluation">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999971">
3.1 Rule-based System Analysis
</subsectionHeader>
<bodyText confidence="0.9716946">
Our system was implemented in Python and used
the Natural Language Toolkit (NLTK,
www.nltk.org/), WordNet and lemmatization mod-
ules. Table 2 provides in the official results of our
system Pearson-Correlation measure.
</bodyText>
<table confidence="0.988470666666667">
D Para Vid Europ OnWn News
Tr 0.6011 0.7021 0.4528
Te 0.5440 0.7335 0.3830 0.5860 0.2445
</table>
<tableCaption confidence="0.999645">
Table 2. Run1 Official Person-Correlation measure
</tableCaption>
<bodyText confidence="0.999933304347826">
In Table 2, the first row shows the proposed sys-
tem results namely 0.6011, 0.7021 and 0.4528 for
MSRpar, MSRvid and SMTeuropel training corpo-
ra respectively. The second row shows the test re-
sults, namely 0.5440, 0.7335 and 0.3830, 0.5860
and 0.2445 for MSRpar, MSRvid and SMTeuro-
pel, On-Wn and SMTnews testing corpora respec-
tively.
In the Task-6 results (Agirre et al. 2012), our
system was ranked 21th out of 85 participants with
0.6663 Pearson-Correlation ALL competition rank.
We tested two WordNet measures, namely the
shortest path and WUP, the path length to the root
node from the least common subsumer (LCS) of
the two concepts, measures on the training corpora.
In contrast to the shortest path measure, WUP
measure increased the P versus the D scores on the
three corpora. This overestimated many training
STS scores and negatively affected the correlation
with the gold standard corpora. Using WUP meas-
ure, the correlations of MSRpar, MSRvid and
SMTeuropel corpora were 0.5553, 0.3488 and
0.4819 respectively. We decided to use WordNet
shortest path measure due to its better correlation
results. When we used WUP measure on testing
corpora, the correlations were 0.5103, 0.4617,
0.4810, 0.6422 and 0.4400 for MSRpar, MSRvid
and SMTeuropel, On-Wn and SMTnews testing
corpora respectively. We observed that when we
used WUP measure on MSRvid corpora, the corre-
lations were degraded. This is because most of
MSRvid corpus pair sentences talking about hu-
man genders which have high WUP scores when
comparing with each other. Unfortunately, Word-
Net shortest path measure underestimated SMT-
news pair sentence similarities which affected
dramatically the related correlation measure.
Hence, the choice of the suitable WordNet metric
for the whole corpora is still under our considera-
tion.
Thresholds and Semantic Pattern: Our current
efforts are directed towards statistical modeling of
the system thresholds. We intend also to use some
web semantic patterns or phrases, such as ReVerb
patterns, to boost the semantic scores of single
words.
</bodyText>
<subsectionHeader confidence="0.998219">
3.2 Baseline System Analysis
</subsectionHeader>
<bodyText confidence="0.99992">
In Table 3, the first row shows the proposed sys-
tem results namely 0.4688, 0.4175 and 0.5349 for
</bodyText>
<page confidence="0.981319">
540
</page>
<bodyText confidence="0.998619833333333">
MSRpar, MSRvid and SMTeuropel training corpo-
ra respectively. The second row shows the pro-
posed system results, namely 0.4617, 0.4489 and
0.4719, 0.6353 and 0.4353 for MSRpar, MSRvid
and SMTeuropel, On-Wn and SMTnews testing
corpora respectively.
</bodyText>
<table confidence="0.997858333333333">
D Para Vid Europ OnWn News
Tr 0.4688 0.4175 0.5349
Te 0.4617 0.4489 0.4719 0.6353 0.4353
</table>
<tableCaption confidence="0.999607">
Table 3. Run 2 Official Person-Correlation measure
</tableCaption>
<bodyText confidence="0.999964857142857">
In the Task-6 results (Agirre et al. 2012), Run2
was ranked 72th out of 85 participants with 0.4169
Pearson-Correlation ALL competition rank. As
anticipated, Run2 released fair results. Its perfor-
mance is penalized or awarded proportionally to
the number of exact matching pair words. Accord-
ingly, it may record considerable scores for pairs
which have highly percentage exact matching
words. For example, it provides competitive corre-
lation scores compared to other participants on On-
Wn and SMTnews testing corpora. Though, this
doesn’t imply that it is an ideal solution for STS
task. It usually indicates that many corpus pairs
may have some substantial exact matching words.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999419">
In this paper, we presented systems developed for
SEMEVAL12- Task6. The first run used both se-
mantics and syntax. The second run, our baseline,
uses only the words in the initial two sentences and
defines similarity as the cosine similarity between
the two sentences. The official task results suggest
that semantics and syntax (Run1) supersedes the
words alone (Run 2) with 0.2494 which indicates
that the words alone are not sufficient to capture
semantic similarity.
</bodyText>
<sectionHeader confidence="0.937513" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999403714285714">
This material is based upon work supported by the
National Science Foundation under Grant No.
(1115774). Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the author(s) and do not
necessarily reflect the views of the National Sci-
ence Foundation.
</bodyText>
<sectionHeader confidence="0.9903" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999439773584906">
Catherine Blake. 2007. The Role of Sentence Structure
in Recognizing Textual Entailment. RTE Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing:101-106.
Courtney Corley and Andras Csomai and Rada Mihal-
cea. 2005. Text Semantic Similarity, with Applica-
tions. Proceedings of the Conference on Recent
Advances in Natural Language Processing (RANLP),
Borovetz, Bulgaria.
Dan Klein and Christopher D. Manning. 2002. Fast
Exact Inference with a Factored Model for Natural
Language Parsing. In Advances in Neural Infor-
mation Processing Systems 15 (NIPS), Cambridge,
MA: MIT Press:3-10.
Dong-bin Hu and Jun Ding. 2011. Study on Similar
Engineering Decision Problem Identification Based
On Combination of Improved Edit-Distance and
Skeletal Dependency Tree with POS. Systems Engi-
neering Procedia 1: 406–413.
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012)
Eric Bengtson and Dan Roth. 2008. Understanding the
Value of Features for Coreference Resolution.
EMNLP:294-303.
Federica Mandreoli and Riccardo Martoglia and Paolo
Tiberio. 2002 . A Syntactic Approach for Searching
Similarities within Sentences. Proceeding of Interna-
tional Conference on Information and Knowledge
Management:656–637.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11): 39-
41.
Gerard Salton. 1989. Automatic Text Processing. The
Transformation, Analysis, and Retrieval of Infor-
mation by Computer. Wokingham, Mass.Addison-
Wesley.
Gunes Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence Re-
search 22:457-479.
Junsheng Zhang, Yunchuan Sun, Huilin Wang, Yanqing
He. 2011. Calculating Statistical Similarity between
Sentences. Journal of Convergence Information
Technology, Volume 6, Number 2: 22-34.
Liu Wenyin and Xiaojun Quan and Min Feng and Bite
Qiu. 2010. A Short Text Modeling Method Combin-
ing Semantic and Statistical Information. Information
Sciences 180: 4031–4041.
</reference>
<page confidence="0.972819">
541
</page>
<reference confidence="0.999618244897959">
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
Semantic Similarity for Automatic Short Answer
Grading. Proceedings of the European Chapter of the
Association for Computational Linguistics (EACL).
Oliva Jesus and Serrano I. Jose and María D. del Cas-
tillo and Ángel Iglesias .2011. SyMSS: A Syntax-
based Measure for Short-Text Semantic Similarity.
Data and Knowledge Engineering 70: 390–405.
Oren Etzioni, Anthony Fader, Janara Christensen, Ste-
phen Soderland, and Mausam. 2011. Open Infor-
mation Extraction: The Second Generation.
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence (IJCAI).
Rada Mihalcea and Courtney Corley and Carlo Strap-
parava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. Proceeding of
the Twenty-First National Conference on Artificial
Intelligence and the Eighteenth Innovative Applica-
tions of Artificial Intelligence Conference.
Peter W. Foltz and Walter Kintsch and Thomas K Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse Processes
Vol. 25, No. 2-3: 285-307.
Samuel Fernando and Mark Stevenson. 2008. A Seman-
tic Similarity Approach to Paraphrase Detection.
Computational Linguistics (CLUK) 11th Annual Re-
search Colloquium, 2008.
Satanjeev Banerjee and Ted Pedersen. 2010. An
Adapted Lesk Algorithm for Word Sense Disambigu-
ation Using WordNet. CICLING:136-145.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. 2004. WordNet::Similarity-measuring the
relatedness of concepts. In Proceedings of NAACL,
2004.
Vasileios Hatzivassiloglou , Judith L. Klavans , Eleazar
Eskin. 1999. Detecting text similarity over short
passages: Exploring linguistic feature combinations
via machine learning. Proceeding of Empirical
Methods in natural language processing and Very
Large Corpora.
Xiao-Ying Liu and Yi-Ming Zhou and Ruo-Shi Zheng.
2008. Measuring Semantic Similarity within Sentenc-
es. Proceedings of the Seventh International Confer-
ence on Machine Learning and Cybernetics.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. Sentence Similarity
based on Semantic Nets and Corpus statistics. 2006.
IEEE Transactions on Knowledgeand Data Engineer-
ing Vol. 18, No. 8: 1138-1150.
</reference>
<page confidence="0.997386">
542
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741216">
<title confidence="0.9722705">Sbdlrhmn: A Rule-based Human Interpretation System Semantic Textual Similarity Task</title>
<author confidence="0.99929">Samir AbdelRahman Catherine Blake</author>
<email confidence="0.9333665">sbdlrhmn@illinois.edu,s.abdelrahman@fci-cu.edu.eg</email>
<affiliation confidence="0.9763065">The Graduate School of Library and Information University Of Illinois at Urbana-Champaign</affiliation>
<abstract confidence="0.996734181818182">In this paper, we describe the system architecture used in the Semantic Textual Similarity (STS) task 6 pilot challenge. The goal of this challenge is to accurately identify five levels of semantic similarity between two sentences: equivalent, mostly equivalent, roughly equivalent, not equivalent but sharing the same topic and no equivalence. Our participations were two systems. The first system (rule-based) combines both semantic and syntax features to arrive at the overall similarity. The proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Catherine Blake</author>
</authors>
<title>The Role of Sentence Structure in Recognizing Textual Entailment.</title>
<date>2007</date>
<booktitle>RTE Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing:101-106.</booktitle>
<contexts>
<context position="3902" citStr="Blake, 2007" startWordPosition="583" endWordPosition="584">son and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) thought to compare the sentence pair words with each other. Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents. Throughout our training and testing experiments, we used Task6 corpora (Agirre et al. 2012) namely MSRpar, MSRvid, SMTeuroparl, OnWN and SMTnews; where: - MSRpar is 1500 pairs of sentences of MSRParaphrase, Microsoft Research Paraphrase Corpus; 750 for training and 750 for testing. - MSRvid is 1500 pairs of sentences of MSRVideo, Microsoft Research Video Description Corpus; 750 for traini</context>
</contexts>
<marker>Blake, 2007</marker>
<rawString>Catherine Blake. 2007. The Role of Sentence Structure in Recognizing Textual Entailment. RTE Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing:101-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Andras Csomai</author>
<author>Rada Mihalcea</author>
</authors>
<title>Text Semantic Similarity, with Applications.</title>
<date>2005</date>
<booktitle>Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP),</booktitle>
<location>Borovetz, Bulgaria.</location>
<contexts>
<context position="1502" citStr="Corley et al., 2005" startWordPosition="213" endWordPosition="216">inherent when working with knowledge resources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011</context>
</contexts>
<marker>Corley, Csomai, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Andras Csomai and Rada Mihalcea. 2005. Text Semantic Similarity, with Applications. Proceedings of the Conference on Recent Advances in Natural Language Processing (RANLP), Borovetz, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2002</date>
<booktitle>In Advances in Neural Information Processing Systems 15 (NIPS),</booktitle>
<publisher>MIT Press:3-10.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="3123" citStr="Klein and Manning, 2002" startWordPosition="469" endWordPosition="472">sing unimportant information, (3) roughly equivalent with missing important information, (2) not equivalent, but sharing some details, (1) not equivalent but sharing the same topic and (0) not equivalent and on different topics. The goal of developing our rule-based system was to identify knowledge representations which have possibly all task human interpretations. Meanwhile, the system domain-free rules aim to help the human annotator in scoring semantic equivalence of sentence pair. The proposed rule-based solution exploits both sentence syntax and semantics. First, it uses Stanford parser (Klein and Manning, 2002) to expose the sentence structure, part-of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies. Second, Illinois Coreference Package (Bengtson and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic rel</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Advances in Neural Information Processing Systems 15 (NIPS), Cambridge, MA: MIT Press:3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong-bin Hu</author>
<author>Jun Ding</author>
</authors>
<title>Study on Similar Engineering Decision Problem Identification Based On Combination</title>
<date>2011</date>
<booktitle>of Improved Edit-Distance and Skeletal Dependency Tree with POS. Systems Engineering Procedia</booktitle>
<volume>1</volume>
<pages>406--413</pages>
<marker>Hu, Ding, 2011</marker>
<rawString>Dong-bin Hu and Jun Ding. 2011. Study on Similar Engineering Decision Problem Identification Based On Combination of Improved Edit-Distance and Skeletal Dependency Tree with POS. Systems Engineering Procedia 1: 406–413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="2214" citStr="Agirre et al. 2012" startWordPosition="331" endWordPosition="334">ity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence. The assigned scores range from 0 to 5 for each sentence pair with the following interpretations: (5) completely equivalent, (4) mostly equivalent pair with missing unimportant information, (3) roughly equivalent with missing important information, (2) not equivalent, but sharing some details, (1) not equivalent but sharing the same topic and (0) not equivalent and on different topics. The goal of developing our rule-based system was to identify knowledge representations</context>
<context position="4202" citStr="Agirre et al. 2012" startWordPosition="629" endWordPosition="632">) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) thought to compare the sentence pair words with each other. Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents. Throughout our training and testing experiments, we used Task6 corpora (Agirre et al. 2012) namely MSRpar, MSRvid, SMTeuroparl, OnWN and SMTnews; where: - MSRpar is 1500 pairs of sentences of MSRParaphrase, Microsoft Research Paraphrase Corpus; 750 for training and 750 for testing. - MSRvid is 1500 pairs of sentences of MSRVideo, Microsoft Research Video Description Corpus; 750 for training and 750 for testing. - SMTeuroparl is 918 pairs of sentences of WMT2008 development dataset (Europarl section); 459 for training and 459 for testing. - OnWn is 750 pairs of sentences pairs of sentences where the first sentence comes from Ontonotes and the second sentence from a WordNet definition</context>
<context position="8616" citStr="Agirre et al. 2012" startWordPosition="1346" endWordPosition="1349">ces which improve the matching steps. For example, in the pair of “Mrs Hillary Clinton explains her plan towards the Middle East countries” and “Mrs Clinton meets their ambassadors”, “Mrs Hillary Clinton”, “her” and “Mrs Clinton” refer to the same entity where “the Middle East countries” and “their” are equivalent. Moreover, we consider the second sentence doesn’t lose “Hillary” as missing important information since the related mentions are labeled equivalent. 2.3 The Semantic Matching Module WordNet, Version 3.0, has approximately 5,947 entries covering around 85% of training corpora words (Agirre et al. 2012). Most of the remaining 15% words are abbreviations, named entities and incorrect POS tags. We use WordNet shortest path measure to compute the semantic similarity between two words. Also, we use Adapted Lesk algorithm to obtain the best WordNet word sense. The disambiguation algorithm compares each pair of words through their contexts (windows) of words coupled with their all overlapping glosses of all WordNet relation types. The semantic matching module inputs are the sentence pair (S1, S2), their lemmatized words, parse trees, S-V-O/S-V/V-O dependencies and coreference mentions (Section 2.2</context>
<context position="18334" citStr="Agirre et al. 2012" startWordPosition="3001" endWordPosition="3004">emmatization modules. Table 2 provides in the official results of our system Pearson-Correlation measure. D Para Vid Europ OnWn News Tr 0.6011 0.7021 0.4528 Te 0.5440 0.7335 0.3830 0.5860 0.2445 Table 2. Run1 Official Person-Correlation measure In Table 2, the first row shows the proposed system results namely 0.6011, 0.7021 and 0.4528 for MSRpar, MSRvid and SMTeuropel training corpora respectively. The second row shows the test results, namely 0.5440, 0.7335 and 0.3830, 0.5860 and 0.2445 for MSRpar, MSRvid and SMTeuropel, On-Wn and SMTnews testing corpora respectively. In the Task-6 results (Agirre et al. 2012), our system was ranked 21th out of 85 participants with 0.6663 Pearson-Correlation ALL competition rank. We tested two WordNet measures, namely the shortest path and WUP, the path length to the root node from the least common subsumer (LCS) of the two concepts, measures on the training corpora. In contrast to the shortest path measure, WUP measure increased the P versus the D scores on the three corpora. This overestimated many training STS scores and negatively affected the correlation with the gold standard corpora. Using WUP measure, the correlations of MSRpar, MSRvid and SMTeuropel corpor</context>
<context position="20565" citStr="Agirre et al. 2012" startWordPosition="3351" endWordPosition="3354">Verb patterns, to boost the semantic scores of single words. 3.2 Baseline System Analysis In Table 3, the first row shows the proposed system results namely 0.4688, 0.4175 and 0.5349 for 540 MSRpar, MSRvid and SMTeuropel training corpora respectively. The second row shows the proposed system results, namely 0.4617, 0.4489 and 0.4719, 0.6353 and 0.4353 for MSRpar, MSRvid and SMTeuropel, On-Wn and SMTnews testing corpora respectively. D Para Vid Europ OnWn News Tr 0.4688 0.4175 0.5349 Te 0.4617 0.4489 0.4719 0.6353 0.4353 Table 3. Run 2 Official Person-Correlation measure In the Task-6 results (Agirre et al. 2012), Run2 was ranked 72th out of 85 participants with 0.4169 Pearson-Correlation ALL competition rank. As anticipated, Run2 released fair results. Its performance is penalized or awarded proportionally to the number of exact matching pair words. Accordingly, it may record considerable scores for pairs which have highly percentage exact matching words. For example, it provides competitive correlation scores compared to other participants on OnWn and SMTnews testing corpora. Though, this doesn’t imply that it is an ideal solution for STS task. It usually indicates that many corpus pairs may have so</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the Value of Features for Coreference Resolution.</title>
<date>2008</date>
<pages>294--303</pages>
<contexts>
<context position="3309" citStr="Bengtson and Roth, 2008" startWordPosition="493" endWordPosition="496">0) not equivalent and on different topics. The goal of developing our rule-based system was to identify knowledge representations which have possibly all task human interpretations. Meanwhile, the system domain-free rules aim to help the human annotator in scoring semantic equivalence of sentence pair. The proposed rule-based solution exploits both sentence syntax and semantics. First, it uses Stanford parser (Klein and Manning, 2002) to expose the sentence structure, part-of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies. Second, Illinois Coreference Package (Bengtson and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) though</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the Value of Features for Coreference Resolution. EMNLP:294-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Federica Mandreoli</author>
<author>Riccardo Martoglia</author>
<author>Paolo Tiberio</author>
</authors>
<title>A Syntactic Approach for Searching Similarities within Sentences.</title>
<date>2002</date>
<booktitle>Proceeding of International Conference on Information and Knowledge Management:656–637.</booktitle>
<contexts>
<context position="1730" citStr="Mandreoli et al. 2002" startWordPosition="247" endWordPosition="250">ur baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a </context>
</contexts>
<marker>Mandreoli, Martoglia, Tiberio, 2002</marker>
<rawString>Federica Mandreoli and Riccardo Martoglia and Paolo Tiberio. 2002 . A Syntactic Approach for Searching Similarities within Sentences. Proceeding of International Conference on Information and Knowledge Management:656–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="3584" citStr="Miller, 1995" startWordPosition="532" endWordPosition="533">of sentence pair. The proposed rule-based solution exploits both sentence syntax and semantics. First, it uses Stanford parser (Klein and Manning, 2002) to expose the sentence structure, part-of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies. Second, Illinois Coreference Package (Bengtson and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) thought to compare the sentence pair words with each other. Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents. Throughout our training and testing experiments, we used Task6 corpora (A</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11): 39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
</authors>
<title>Automatic Text Processing. The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<location>Wokingham, Mass.AddisonWesley.</location>
<contexts>
<context position="1679" citStr="Salton, 1989" startWordPosition="241" endWordPosition="242">e of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of s</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard Salton. 1989. Automatic Text Processing. The Transformation, Analysis, and Retrieval of Information by Computer. Wokingham, Mass.AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>LexRank: Graph-based Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>22--457</pages>
<contexts>
<context position="1544" citStr="Erkan and Radev, 2004" startWordPosition="219" endWordPosition="222">ources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Se</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir R. Radev. 2004. LexRank: Graph-based Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research 22:457-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junsheng Zhang</author>
</authors>
<title>Yunchuan Sun, Huilin Wang, Yanqing He.</title>
<date>2011</date>
<journal>Journal of Convergence Information Technology,</journal>
<volume>6</volume>
<pages>22--34</pages>
<marker>Zhang, 2011</marker>
<rawString>Junsheng Zhang, Yunchuan Sun, Huilin Wang, Yanqing He. 2011. Calculating Statistical Similarity between Sentences. Journal of Convergence Information Technology, Volume 6, Number 2: 22-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu Wenyin</author>
<author>Xiaojun Quan</author>
<author>Min Feng</author>
<author>Bite Qiu</author>
</authors>
<title>A Short Text Modeling Method Combining Semantic and Statistical Information.</title>
<date>2010</date>
<journal>Information Sciences</journal>
<volume>180</volume>
<pages>4031--4041</pages>
<contexts>
<context position="2122" citStr="Wenyin et al. 2010" startWordPosition="315" endWordPosition="318">nd summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence. The assigned scores range from 0 to 5 for each sentence pair with the following interpretations: (5) completely equivalent, (4) mostly equivalent pair with missing unimportant information, (3) roughly equivalent with missing important information, (2) not equivalent, but sharing some details, (1) not equivalent but sharing the same topic and (0) not equivalent and on different to</context>
</contexts>
<marker>Wenyin, Quan, Feng, Qiu, 2010</marker>
<rawString>Liu Wenyin and Xiaojun Quan and Min Feng and Bite Qiu. 2010. A Short Text Modeling Method Combining Semantic and Statistical Information. Information Sciences 180: 4031–4041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Text-to-text Semantic Similarity for Automatic Short Answer Grading.</title>
<date>2009</date>
<booktitle>Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="1418" citStr="Mohler and Mihalcea, 2009" startWordPosition="202" endWordPosition="205"> The proposed rules enable the system to adequately handle domain knowledge gaps that are inherent when working with knowledge resources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with s</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>Michael Mohler and Rada Mihalcea. 2009. Text-to-text Semantic Similarity for Automatic Short Answer Grading. Proceedings of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Oliva Jesus</author>
<author>Serrano I Jose</author>
<author>D María</author>
</authors>
<title>del Castillo and Ángel Iglesias .2011. SyMSS: A Syntaxbased Measure for Short-Text Semantic Similarity.</title>
<journal>Data and Knowledge Engineering</journal>
<volume>70</volume>
<pages>390--405</pages>
<marker>Jesus, Jose, María, </marker>
<rawString>Oliva Jesus and Serrano I. Jose and María D. del Castillo and Ángel Iglesias .2011. SyMSS: A Syntaxbased Measure for Short-Text Semantic Similarity. Data and Knowledge Engineering 70: 390–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Anthony Fader</author>
<author>Janara Christensen</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
</authors>
<title>Open Information Extraction: The Second Generation.</title>
<date>2011</date>
<booktitle>Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="3783" citStr="Etzioni et al. 2011" startWordPosition="560" endWordPosition="563">of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies. Second, Illinois Coreference Package (Bengtson and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) thought to compare the sentence pair words with each other. Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents. Throughout our training and testing experiments, we used Task6 corpora (Agirre et al. 2012) namely MSRpar, MSRvid, SMTeuroparl, OnWN and SMTnews; where: - MSRpar is 1500 pairs of sentences of MSRParaphrase, Microsoft Research Paraphrase Corpus; 750 for training and 750 fo</context>
</contexts>
<marker>Etzioni, Fader, Christensen, Soderland, Mausam, 2011</marker>
<rawString>Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam. 2011. Open Information Extraction: The Second Generation. Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>Proceeding of the Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference.</booktitle>
<contexts>
<context position="1763" citStr="Mihalcea et al. 2006" startWordPosition="252" endWordPosition="255">ine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence. The assigned scores ran</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea and Courtney Corley and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. Proceeding of the Twenty-First National Conference on Artificial Intelligence and the Eighteenth Innovative Applications of Artificial Intelligence Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<volume>25</volume>
<pages>285--307</pages>
<contexts>
<context position="1870" citStr="Foltz et al. 1998" startWordPosition="271" endWordPosition="274">tic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence. The assigned scores range from 0 to 5 for each sentence pair with the following interpretations: (5) completely equivalent, (4) mo</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Peter W. Foltz and Walter Kintsch and Thomas K Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse Processes Vol. 25, No. 2-3: 285-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A Semantic Similarity Approach to Paraphrase Detection.</title>
<date>2008</date>
<booktitle>Computational Linguistics (CLUK) 11th Annual Research Colloquium,</booktitle>
<contexts>
<context position="1463" citStr="Fernando and Stevenson, 2008" startWordPosition="207" endWordPosition="210">dequately handle domain knowledge gaps that are inherent when working with knowledge resources. As such one of its main goals, the system suggests a set of domain-free rules to help the human annotator in scoring semantic equivalence of two sentences. The second system is our baseline in which we use the Cosine Similarity between the words in each sentence pair. 1 Introduction Accurately establishing sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatednes</context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A Semantic Similarity Approach to Paraphrase Detection. Computational Linguistics (CLUK) 11th Annual Research Colloquium, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2010</date>
<pages>136--145</pages>
<contexts>
<context position="3671" citStr="Banerjee and Pedersen, 2010" startWordPosition="542" endWordPosition="545">e syntax and semantics. First, it uses Stanford parser (Klein and Manning, 2002) to expose the sentence structure, part-of-speech (POS) word tags, parse tree and Subject-Verb-Object (S-V-O) dependencies. Second, Illinois Coreference Package (Bengtson and Roth, 2008) is used to extract sentence named entities resolving possible men536 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 536–542, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tions. Third, WordNet (Miller, 1995) and Adapted Lesk Algorithm for word sense disambiguation (Banerjee and Pedersen, 2010) are used to compute each sentence word semantic relatedness to the other sentence. ReVerb (Etzioni et al. 2011) augments WordNet in case of uncovered words and helps us to discriminate the topics of sentences. We use (Blake, 2007) thought to compare the sentence pair words with each other. Finally, we evolve a rule-based module to present the human heuristics when he interprets the relatedness of the sentence pair meaningful contents. Throughout our training and testing experiments, we used Task6 corpora (Agirre et al. 2012) namely MSRpar, MSRvid, SMTeuroparl, OnWN and SMTnews; where: - MSRpa</context>
</contexts>
<marker>Banerjee, Pedersen, 2010</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2010. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. CICLING:136-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity-measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<contexts>
<context position="10587" citStr="Pedersen et al., 2004" startWordPosition="1653" endWordPosition="1656">lgorithm implementation, we found that the runtime requirement is directly proportional to the input sentence length. So, we shortened the sentence length to Subject-VP which includes the underlying comparable words. Relatedness Score (S1, S2) unrelated 0 &lt;= Ws &lt;0.3 weakly related 0.3 &lt;= Ws &lt;0.85 strongly related Ws &gt;= 0.85 Table 1 – Mapping relatedness to wordnet similarity Table 1 describes the proposed system WordNet thresholds through our relatedness definitions. The thresholds were thoroughly selected depending on our analysis for the WordNet hierarchary and semantic similarity measures (Pedersen et al., 2004). We obsereved that while most of the nearest tree sibilings and parent-child nodes scores have more than 0.85 Wordnet semantic scores, most of the fartherest ones have scores less than 0.3. In between these extremes, there is a group of scattered tree nodes which ranges from 0.3 to 0.85. The number of nodes per each mentioned group is related to the semantic simlarity measure technique. 2.4 Semantics – Using ReVerb Our working hypothesis is that verbs that use the same arguments are more likely to be similar. To estimate verb usage, the system uses frequencies from the ReVerb (http://openie.c</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity-measuring the relatedness of concepts. In Proceedings of NAACL, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. Proceeding of Empirical Methods in natural language processing and Very Large Corpora.</title>
<date>1999</date>
<marker>Klavans, 1999</marker>
<rawString>Vasileios Hatzivassiloglou , Judith L. Klavans , Eleazar Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. Proceeding of Empirical Methods in natural language processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao-Ying Liu</author>
<author>Yi-Ming Zhou</author>
<author>Ruo-Shi Zheng</author>
</authors>
<title>Measuring Semantic Similarity within Sentences.</title>
<date>2008</date>
<booktitle>Proceedings of the Seventh International Conference on Machine Learning and Cybernetics.</booktitle>
<contexts>
<context position="1850" citStr="Liu et al. 2008" startWordPosition="267" endWordPosition="270">ng sentence semantic similarity would provide one of the key ingredients for solutions to many text-related applications, such as automatic grading systems (Mohler and Mihalcea, 2009), paraphrasing (Fernando and Stevenson, 2008), text entailment (Corley et al., 2005) and summarization (Erkan and Radev, 2004). Current approaches for computing semantic similarity between a pair of sentences focus on analyzing their shared words (Salton, 1989), structures (Hu et al. 2011;Mandreoli et al. 2002), semantics (Mihalcea et al. 2006; Le el al. 2006; Hatzivassiloglou, 1999) or any of their combinations (Liu et al. 2008; Foltz et al. 1998). The goal is to arrive at a score which increases proportionally with the relatedness between the two sentences. Yet, they are not concerned with scoring the interpretations of such relatedness (Zhang et al. 2011; Jesus et al. 2011; Wenyin et al. 2010; Liu et al. 2008). Semantic Textual Similarity (STS), SEMEVAL12 Task 6 (Agirre et al. 2012), measures the degree of semantic equivalence between a pair of sentences by comparing meaningful contents within a sentence. The assigned scores range from 0 to 5 for each sentence pair with the following interpretations: (5) completel</context>
</contexts>
<marker>Liu, Zhou, Zheng, 2008</marker>
<rawString>Xiao-Ying Liu and Yi-Ming Zhou and Ruo-Shi Zheng. 2008. Measuring Semantic Similarity within Sentences. Proceedings of the Seventh International Conference on Machine Learning and Cybernetics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity based on Semantic Nets and Corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledgeand Data Engineering</journal>
<volume>18</volume>
<pages>1138--1150</pages>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. Sentence Similarity based on Semantic Nets and Corpus statistics. 2006. IEEE Transactions on Knowledgeand Data Engineering Vol. 18, No. 8: 1138-1150.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>