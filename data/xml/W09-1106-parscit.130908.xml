<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.945347">
Efficient Linearization of Tree Kernel Functions
</title>
<author confidence="0.740891">
Daniele Pighin
</author>
<note confidence="0.5276045">
FBK-Irst, HLT
Via di Sommarive, 18 I-38100 Povo (TN) Italy
</note>
<email confidence="0.985081">
pighin@fbk.eu
</email>
<author confidence="0.992092">
Alessandro Moschitti
</author>
<affiliation confidence="0.994741">
University of Trento, DISI
</affiliation>
<note confidence="0.49668">
Via di Sommarive, 14 I-38100 Povo (TN) Italy
</note>
<email confidence="0.985288">
moschitti@disi.unitn.it
</email>
<sectionHeader confidence="0.993429" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994774375">
The combination of Support Vector Machines
with very high dimensional kernels, such as
string or tree kernels, suffers from two ma-
jor drawbacks: first, the implicit representa-
tion of feature spaces does not allow us to un-
derstand which features actually triggered the
generalization; second, the resulting compu-
tational burden may in some cases render un-
feasible to use large data sets for training. We
propose an approach based on feature space
reverse engineering to tackle both problems.
Our experiments with Tree Kernels on a Se-
mantic Role Labeling data set show that the
proposed approach can drastically reduce the
computational footprint while yielding almost
unaffected accuracy.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940588235294">
The use of Support Vector Machines (SVMs)
in supervised learning frameworks is spreading
across different communities, including Computa-
tional Linguistics and Natural Language Processing,
thanks to their solid mathematical foundations, ef-
ficiency and accuracy. Another important reason
for their success is the possibility of using kernel
functions to implicitly represent examples in some
high dimensional kernel space, where their similar-
ity is evaluated. Kernel functions can generate a very
large number of features, which are then weighted
by the SVM optimization algorithm obtaining a fea-
ture selection side-effect. Indeed, the weights en-
coded by the gradient of the separating hyperplane
learnt by the SVM implicitly establish a ranking be-
tween features in the kernel space. This property has
been exploited in feature selection models based on
</bodyText>
<page confidence="0.976341">
30
</page>
<bodyText confidence="0.976453138888889">
approximations or transformations of the gradient,
e.g. (Rakotomamonjy, 2003), (Weston et al., 2003)
or (Kudo and Matsumoto, 2003).
However, kernel based systems have two major
drawbacks: first, new features may be discovered
in the implicit space but they cannot be directly ob-
served. Second, since learning is carried out in the
dual space, it is not possible to use the faster SVM or
perceptron algorithms optimized for linear spaces.
Consequently, the processing of large data sets can
be computationally very expensive, limiting the use
of large amounts of data for our research or applica-
tions.
We propose an approach that tries to fill in the
gap between explicit and implicit feature represen-
tations by 1) selecting the most relevant features in
accordance with the weights estimated by the SVM
and 2) using these features to build an explicit rep-
resentation of the kernel space. The most innovative
aspect of our work is the attempt to model and im-
plement a solution in the context of structural ker-
nels. In particular we focus on Tree Kernel (TK)
functions, which are especially interesting for the
Computational Linguistics community as they can
effectively encode rich syntactic data into a kernel-
based learning algorithm. The high dimensionality
of a TK feature space poses interesting challenges in
terms of computational complexity that we need to
address in order to come up with a viable solution.
We will present a number of experiments carried
out in the context of Semantic Role Labeling, show-
ing that our approach can noticeably reduce training
time while yielding almost unaffected classification
accuracy, thus allowing us to handle larger data sets
at a reasonable computational cost.
The rest of the paper is structured as follows: Sec-
</bodyText>
<note confidence="0.990842">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 30–38,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.548261">
C
</figure>
<figureCaption confidence="0.9124445">
Figure 1: Esemplification of a fragment space and the
kernel product between two trees.
</figureCaption>
<bodyText confidence="0.999563571428571">
tion 2 will briefly review SVMs and Tree Kernel
functions; Section 3 will detail our proposal for the
linearization of a TK feature space; Section 4 will
review previous work on related subjects; Section 5
will describe our experiments and comment on their
results; finally, in Section 6 we will draw our con-
clusions.
</bodyText>
<sectionHeader confidence="0.954529" genericHeader="method">
2 Tree Kernel Functions
</sectionHeader>
<bodyText confidence="0.996726">
The decision function of an SVM is:
</bodyText>
<equation confidence="0.981538">
n
f(x) = w�· x�+ b = αiyiei · x� + b (1)
i=1
</equation>
<bodyText confidence="0.999866333333333">
where x� is a classifying example and w� and b are
the separating hyperplane’s gradient and its bias,
respectively. The gradient is a linear combination
of the training points xz, their labels yi and their
weights αi. These and the bias are optimized at
training time by the learning algorithm. Applying
the so-called kernel trick it is possible to replace the
scalar product with a kernel function defined over
pairs of objects:
</bodyText>
<equation confidence="0.980428">
n
f(o) = αiyik(oi, o) + b
i=1
</equation>
<bodyText confidence="0.999959966666667">
with the advantage that we do not need to provide
an explicit mapping φ(·) of our examples in a vector
space.
A Tree Kernel function is a convolution ker-
nel (Haussler, 1999) defined over pairs of trees.
Practically speaking, the kernel between two trees
evaluates the number of substructures (or fragments)
they have in common, i.e. it is a measure of their
overlap. The function can be computed recursively
in closed form, and quite efficient implementations
are available (Moschitti, 2006). Different TK func-
tions are characterized by alternative fragment defi-
nitions, e.g. (Collins and Duffy, 2002) and (Kashima
and Koyanagi, 2002). In the context of this paper
we will be focusing on the SubSet Tree (SST) ker-
nel described in (Collins and Duffy, 2002), which
relies on a fragment definition that does not allow to
break production rules (i.e. if any child of a node is
included in a fragment, then also all the other chil-
dren have to). As such, it is especially indicated for
tasks involving constituency parsed texts.
Implicitly, a TK function establishes a correspon-
dence between distinct fragments and dimensions in
some fragment space, i.e. the space of all the pos-
sible fragments. To simplify, a tree t can be repre-
sented as a vector whose attributes count the occur-
rences of each fragment within the tree. The ker-
nel between two trees is then equivalent to the scalar
product between pairs of such vectors, as exempli-
fied in Figure 1.
</bodyText>
<sectionHeader confidence="0.984176" genericHeader="method">
3 Mining the Fragment Space
</sectionHeader>
<bodyText confidence="0.999456695652174">
If we were able to efficiently mine and store in a
dictionary all the fragments encoded in a model,
we would be able to represent our objects explicitly
and use these representations to train larger models
and very quick and accurate classifiers. What we
need to devise are strategies to make this approach
convenient in terms of computational requirements,
while yielding an accuracy comparable with direct
tree kernel usage.
Our framework defines five distinct activities,
which are detailed in the following paragraphs.
Fragment Space Learning (FSL) First of all, we
can partition our training data into S smaller sets,
and use the SVM and the SST kernel to learn S mod-
els. We will use the estimated weights to drive our
feature selection process. Since the time complexity
of SVM training is approximately quadratic in the
number of examples, this way we can considerably
accelerate the process of estimating support vector
weights.
According to statistical learning theory, being
trained on smaller subsets of the available data
these models will be less robust with respect to the
</bodyText>
<figure confidence="0.994271166666667">
K(T1, T2) = (O(T1), O(T2)) = 1
A
B A
B A
O(T1) = [2, 1, 1, 1, 1, 0, 0] B
O(T2) = [0, 0, 0, 0, 1, 1, 1]
D
A
C
Fragment space
A
B A
C
D
B A
C
A
D
C
C
B A
T1
T2
A
A B A
B A B A
1 2 3 4 5 6 7
A
B A
B A
</figure>
<page confidence="0.999936">
31
</page>
<bodyText confidence="0.9100849">
minimization of the empirical risk (Vapnik, 1998).
Nonetheless, since we do not need to employ them
for classification (but just to direct our feature se-
lection process, as we will describe shortly), we can
accept to rely on sub-optimal weights. Furthermore,
research results in the field of SVM parallelization
using cascades of SVMs (Graf et al., 2004) suggest
that support vectors collected from locally learnt
models can encode many of the relevant features re-
tained by models learnt globally. Henceforth, let Ms
be the model associated with the s-th split, and Fs
the fragment space that can describe all the trees in
Ms.
Fragment Mining and Indexing (FMI) In Equa-
tion 1 it is possible to isolate the gradient w~ =
Pn i=1 αiyi ~xi, with ~xi = [x(1)
i , ... , x(N)
i ], N being
the dimensionality of the feature space. For a tree
kernel function, we can rewrite x(j) ias:
</bodyText>
<equation confidence="0.839905">
Itil
PNk=1(ti,kλ`(fk))2
</equation>
<bodyText confidence="0.998356666666667">
where: ti,j is the number of occurrences of the frag-
ment fj, associated with the j-th dimension of the
feature space, in the tree ti; λ is the kernel decay
factor; and `(fj) is the depth of the fragment.
The relevance |w(j) |of the fragment fj can be
measured as:
</bodyText>
<equation confidence="0.818508">
.(3)
</equation>
<bodyText confidence="0.99793725">
We fix a threshold L and from each model Ms
(learnt during FSL) we select the L most relevant
fragments, i.e. we build the set Fs,L = Uk{fk} so
that:
</bodyText>
<equation confidence="0.846798">
|Fs,L |= Land |w(k) |&gt; |w(i)|Vfi E F \ Fs,L .
</equation>
<bodyText confidence="0.9998526">
In order to do so, we need to harvest all the frag-
ments with a fast extraction function, store them in
a compact data structure and finally select the frag-
ments with the highest relevance. Our strategy is ex-
emplified in Figure 2. First, we represent each frag-
ment as a sequence as described in (Zaki, 2002). A
sequence contains the labels of the fragment nodes
in depth-first order. By default, each node is the
child of the previous node in the sequence. A spe-
cial symbol (T) indicates that the next node in the
</bodyText>
<figureCaption confidence="0.993161666666667">
Figure 2: Fragment indexing. Each fragment is repre-
sented as a sequence 1 and then encoded as a path in the
index 2 which keeps track of its cumulative relevance.
</figureCaption>
<bodyText confidence="0.999989243243243">
sequence should be attached after climbing one level
in the tree. For example, the tree (B (Z W)) in figure
is represented as the sequence [B, Z, T, W]. Then, we
add the elements of the sequence to a graph (which
we call an index of fragments) where each sequence
becomes a path. The nodes of the index are the la-
bels of the fragment nodes, and each arc is associ-
ated with a pair of values (d, n): d is a node identi-
fier, which is unique with respect to the source node;
n is the identifier of the arc that must be selected at
the destination node in order to follow the path as-
sociated with the sequence. Index nodes associated
with a fragment root also have a field where the cu-
mulative relevance of the fragment is stored.
As an example, the index node labeled B in fig-
ure has an associated weight of w3, thus identify-
ing the root of a fragment. Each outgoing edge
univocally identifies an indexed fragment. In this
case, the only outgoing edge is labeled with the pair
(d = 1, n = 1), meaning that we should follow it
to the next node, i.e. Z, and there select the edge la-
beled 1, as indicated by n. The edge with d = 1 in Z
is (d = 1, n = 1), so we browse to T where we se-
lect the edge (d = 1, n = −). The missing value for
n tells us that the next node, W, is the last element
of the sequence. The complete sequence is then [B,
Z, T, W], which encodes the fragment (B (Z W)).
The index implementation has been optimized for
fast insertions and has the following features: 1)
each node label is represented exactly once; 2) each
distinct sequence tail is represented exactly once.
The union of all the fragments harvested from each
model is then saved into a dictionary DL which will
be used by the next stage.
To mine the fragments, we apply to each tree in
each model the algorithm shown in Algorithm 3.1.
In this context, we call fragment expansion the pro-
</bodyText>
<figure confidence="0.9690315">
R2
B
R1
w1
1,2
weight: w1 weight: w2 weight: wg
Y
A
1,2
1,1
X Y B
Z W
3,1
Z W
R1
A B
Z W
R1, A, ↑, B, Z, ↑, W
R2, X, ↑, Y, ↑, B, Z, ↑, W
B, Z, ↑, W
2,1
1,1
1,3
B
wg
R2
w2
2
X
1,1
1
1,1 1,-
Z ↑ W
x (j) = ti,j λ`(fj) = ti,j λ`( fj) 2
)
q
|w(j) |= n ��
~X αiyix(j) ~~
�� i ~
i=1
32
Algorithm 3.1: MINE TREE(tree)
global maxdepth, maxexp
main
mined ← 0; indexed ← 0; MINE(FRAG(tree), 0)
procedure MINE(frag, depth)
if frag E indexed
then return
indexed ← indexed U {frag}
INDEX(frag)
for each node E TO EXPAND(frag)
if node E� mined
mined ← mined U {node}
then MINE(FRAG(node), 0)
</figure>
<construct confidence="0.593573">
if depth &lt; maxdepth
</construct>
<bodyText confidence="0.97455144117647">
then rfor each f ragment E EXPAND (f rag, maxexp)
l do MINE(fragment, depth + 1)
cess by which tree nodes are included in a frag-
ment. Fragment expansion is achieved via node ex-
pansions, where expanding a node means includ-
ing its direct children in the fragment. The func-
tion FRAG(n) builds the basic fragment rooted in a
given node n, i.e. the fragment consisting only of n
and its direct children. The function TO EXPAND(f)
returns the set of nodes in a fragment f that can
be expanded (i.e. internal nodes in the origin tree),
while the function EXPAND(f, maxexp) returns all
the possible expansions of a fragment f. The pa-
rameter maxexp is a limit to the number of nodes
that can be expanded at the same time when a new
fragment is generated, while maxdepth sets a limit
on the number of times that a base fragment can be
expanded. The function INDEX(f) adds the frag-
ment f to the index. To keep the notation simple,
here we assume that a fragment f contains all the
necessary information to calculate its relevance (i.e.
the weight, label and norm of the support vector αi,
yi, and IltiIl, the depth of the fragment `(f) and the
decay factor λ, see equations 2 and 3).
Performing in a different order the same node ex-
pansions on the same fragment f results in the same
fragment f�. To prevent the algorithm from entering
circular loops, we use the set indexed so that the
very same fragment in each tree cannot be explored
more than once. Similarly, the mined set is used
so that the base fragment rooted in a given node is
considered only once.
Tree Fragment Extraction (TFX) During this
phase, a data file encoding label-tree pairs (yi, ti) is
</bodyText>
<figure confidence="0.690875">
acat -1: BC +1: BC,A1
-1: A0,A2,A3,A4,A5
</figure>
<figureCaption confidence="0.68704925">
Figure 3: Examples of AST,,,, structured features.
transformed to encode label-vector pairs (yi, ~vi). To
do so, we generate the fragment space of ti, using
a variant of the mining algorithm described in Fig-
</figureCaption>
<bodyText confidence="0.981035571428571">
ure 3.1, and encode in ~vi all and only the fragments
ti,j so that ti,j E DL, i.e. we perform feature extrac-
tion based on the indexed fragments. The process is
applied to the whole training and test sets. The al-
gorithm exploits labels and production rules found
in the fragments listed in the dictionary to generate
only the fragments that may be in the dictionary. For
example, if the dictionary does not contain a frag-
ment whose root is labeled N, then if a node N is
encountered during TFX neither its base fragment
nor its expansions are generated.
Explicit Space Learning (ESL) After linearizing
the training data, we can learn a very fast model by
using all the available data and a linear kernel. The
fragment space is now explicit, as there is a mapping
between the input vectors and the fragments they en-
code.
Explicit Space Classification (ESC) After learn-
ing the linear model, we can classify our linearized
test data and evaluate the accuracy of the resulting
classifier.
</bodyText>
<sectionHeader confidence="0.8706155" genericHeader="method">
vious work
4 Pre
</sectionHeader>
<bodyText confidence="0.9999102">
A rather comprehensive overview of feature selec-
tion techniques is carried out in (Guyon and Elis-
seeff, 2003). Non-filter approaches for SVMs and
kernel machines are often concerned with polyno-
mial and Gaussian kernels, e.g. (Weston et al., 2001)
and (Neumann et al., 2005). Weston et al. (2003) use
the
norm in the SVM optimizer to stress the fea-
ture selection capabilities of the learning algorithm.
In (Kudo and Matsumoto, 2003), an extension of the
PrefixSpan algorithm (Pei et al., 2001) is used to ef-
ficiently mine the features in a low degree polyno-
mial kernel space. The authors discuss an approx-
imation of their method that allows them to handle
high degree polynomial kern
</bodyText>
<figure confidence="0.99422196969697">
`0
els.
VP
VP
bought
D-B
bought
D
NN
a
a
cat
S
(A0)
NP
VP
(A1)
⇒
NNP
NP
D
NN
Mary
bought
VB
VB-P
NP
VB-P
NP-B
⎧
⎨
⎩
do
</figure>
<page confidence="0.980238">
33
</page>
<table confidence="0.9993379">
Task Data set Neg Train Non-linearized classifiers Fl Linearized classifiers (Thr=10k)
Pos Test P R Train Test P R Fl
A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40
A1 90,636 88,455 1,206 11 89.45 88.62 89.03 376 3 89.39 88.13 88.76
A2 21,291 157,800 692 7 84.56 64.42 73.13 248 3 81.23 68.29 74.20
A3 3,481 175,610 127 2 97.67 40.00 56.76 114 3 97.56 38.10 54.79
A4 2,713 176,378 47 1 92.68 55.07 69.10 92 2 95.00 55.07 69.72
A5 69 179,022 3 0 100.00 50.00 66.67 63 2 100.00 50.00 66.67
BC 61,062 938,938 3,059 247 82.57 80.96 81.76 916 39 83.36 78.95 81.10
RM - - 2,596 27 89.37 86.00 87.65 1,090 16 88.50 85.81 87.13
</table>
<tableCaption confidence="0.97012375">
Table 1: Accuracy (P, R, Fl), training (Train) and test (Test) time of non-linearized (center) and linearized (right)
classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training
examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers
trained to recognize core PropBank roles.
</tableCaption>
<bodyText confidence="0.996291181818182">
Suzuki and Isozaki (2005) present an embedded
approach to feature selection for convolution ker-
nels based on x2-driven relevance assessment. To
our knowledge, this is the only published work
clearly focusing on feature selection for tree ker-
nel functions. In (Graf et al., 2004), an approach
to SVM parallelization is presented which is based
on a divide-et-impera strategy to reduce optimiza-
tion time. The idea of using a compact graph rep-
resentation to represent the support vectors of a TK
function is explored in (Aiolli et al., 2006), where a
Direct Acyclic Graph (DAG) is employed.
Concerning the use of kernels for NLP, inter-
esting models and results are described, for exam-
ple, in (Collins and Duffy, 2002), (Moschitti et al.,
2008), (Kudo and Matsumoto, 2003), (Cumby and
Roth, 2003), (Shen et al., 2003), (Cancedda et al.,
2003), (Culotta and Sorensen, 2004), (Daum´e III
and Marcu, 2004), (Kazama and Torisawa, 2005),
(Kudo et al., 2005), (Titov and Henderson, 2006),
(Moschitti et al., 2006), (Moschitti and Bejan, 2004)
or (Toutanova et al., 2004).
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999587760869565">
We tested our model on a Semantic Role La-
beling (SRL) benchmark, using PropBank annota-
tions (Palmer et al., 2005) and automatic Charniak
parse trees (Charniak, 2000) as provided for the
CoNLL 2005 evaluation campaign (Carreras and
M`arquez, 2005). SRL can be decomposed into
two tasks: boundary detection, where the word se-
quences that are arguments of a predicate word w
are identified, and role classification, where each ar-
gument is assigned the proper role. The former task
requires a binary Boundary Classifier (BC), whereas
the second involves a Role Multi-class Classifier
(RM).
Setup. If the constituency parse tree t of a sen-
tence s is available, we can look at all the pairs
(p, ni), where ni is any node in the tree and p is
the node dominating w, and decide whether ni is an
argument node or not, i.e. whether it exactly dom-
inates all and only the words encoding any of w’s
arguments. The objects that we classify are sub-
sets of the input parse tree that encompass both p
and ni. Namely, we use the ASTm structure defined
in (Moschitti et al., 2008), which is the minimal tree
that covers all and only the words of p and ni. In
the ASTm, p and ni are marked so that they can be
distinguished from the other nodes. An ASTm is
regarded as a positive example for BC if ni is an ar-
gument node, otherwise it is considered a negative
example. Positive BC examples can be used to train
an efficient RM: for each role r we can train a clas-
sifier whose positive examples are argument nodes
whose label is exactly r, whereas negative examples
are argument nodes labeled r&apos; 7� r. Two ASTms
extracted from an example parse tree are shown in
Figure 3: the first structure is a negative example for
BC and is not part of the data set of RM, whereas
the second is a positive instance for BC and A1.
To train BC we used PropBank sections 1 through
6, extracting ASTm structures out of the first 1 mil-
lion (p, ni) pairs from the corresponding parse trees.
As a test set we used the 149,140 instance collected
from the annotations in Section 24. There are 61,062
positive examples in the training set (i.e. 6.1%) and
8,515 in the test set (i.e. 5.7%).
For RM we considered all the argument nodes of
any of the six PropBank core roles (i.e. A0, ... ,
</bodyText>
<page confidence="0.980894">
34
</page>
<figure confidence="0.944854857142857">
BC¢ Prec BC Prec
BC¢ Rec BC Rec
BC¢ F1 BC F1
1k 2k 5k 10k 20k30k 50k 100k
Threshold (log10)
Accuracy
84
82
80
78
76
74
72
1k 2k 5k 10k 20k30k 50k 100k
Learning time (minutes)
1,200
1,000
800
600
400
200
0
929 916
Overall TFX ESL
FMI FSL
1,037
1,104
Threshold (log10)
</figure>
<figureCaption confidence="0.999589333333333">
Figure 4: Training time decomposition for the linearized
BC with respect to its main components when varying the
threshold value.
</figureCaption>
<bodyText confidence="0.97581135483871">
A5) from all the available training sections, i.e. 2
through 21, for a total of 179,091 training instances.
Similarly, we collected 5,928 test instances from the
annotations of Section 24.
In the remainder, we will mark with an E the lin-
earized classifiers, i.e. BCP and RMP will refer to
the linearized boundary and role classifiers, respec-
tively. Their traditional, vanilla SST counterparts
will be simply referred to as BC and RM.
We used 10 splits for the FMI stage and we set
maxdepth = 4 and maxexp = 5 during FMI and
TFX. We didn’t carry out an extensive validation of
these parameters. These values were selected dur-
ing the development of the software because, on a
very small development set, they resulted in a very
responsive system.
Since the main topic of this paper is the assess-
ment of the efficiency and accuracy of our lineariza-
tion technique, we did not carry out an evaluation
on the whole SRL task using the official CoNLL’05
evaluator. Indeed, producing complete annotations
requires several steps (e.g. overlap resolution, OvA
or Pairwise combination of individual role classi-
fiers) that would shade off the actual impact of the
methodology on classification.
Platform. All the experiments were run on a ma-
chine equipped with 4 Intel ® Xeon ® CPUs clocked
at 1.6 GHz and 4 GB of RAM running on a Linux
2.6.9 kernel. As a supervised learning framework
we used SVM-Light-TK 1, which extends the SVM-
Light optimizer (Joachims, 2000) with tree kernel
</bodyText>
<footnote confidence="0.985962">
1http://disi.unitn.it/˜moschitt/Tree-Kernel.htm
</footnote>
<figureCaption confidence="0.999659">
Figure 5: BCP accuracy for different thresholds.
</figureCaption>
<bodyText confidence="0.999820828571428">
support. During FSL, we learn the models using a
normalized SST kernel and the default decay factor
A = 0.4. The same parameters are used to train
the models of the non linearized classifiers. During
ESL, the classifier is trained using a linear kernel.
We did not carry out further parametrization of the
learning algorithm.
Results. The left side of Table 1 shows the distri-
bution of positive (Column Pos) and negative (Neg)
data points in each classifier’s training set. The cen-
tral group of columns lists training and test effi-
ciency and accuracy of BC and RM, i.e. the non-
linearized classifiers, along with figures for the indi-
vidual role classifiers that make up RM.
Training BC took more than two days of CPU
time and testing about 4 hours. The classifier
achieves an Fl measure of 81.76, with a good bal-
ance between precision and recall. Concerning RM,
sequential training of the 6 models took 2,596 min-
utes, while classification took 27 minutes. The slow-
est of the individual role classifiers happens to be
A1, which has an almost 1:1 ratio between posi-
tive and negative examples, i.e. they are 90,636 and
88,455 respectively.
We varied the threshold value (i.e. the number of
fragments that we mine from each model, see Sec-
tion 3) to measure its effect on the resulting classi-
fier accuracy and efficiency. In this context, we call
training time all the time necessary to obtain a lin-
earized model, i.e. the sum of FSL, FMI and TFX
time for every split, plus the time for ESL. Similarly,
we call test time the time necessary to classify a lin-
earized test set, i.e. the sum of TFX and ESC on test
data.
In Figure 4 we plot the efficiency of BCP learn-
</bodyText>
<page confidence="0.998301">
35
</page>
<bodyText confidence="0.999916">
ing with respect to different threshold values. The
Overall training time is shown alongside with par-
tial times coming from FSL (which is the same for
every threshold value and amounts to 433 minutes),
FMI, training data TFX and ESL. The plot shows
that TFX has a logarithmic behaviour, and that quite
soon becomes the main player in total training time
after FSL. For threshold values lower than 10k, ESL
time decreases as the threshold increases: too few
fragments are available and adding new ones in-
creases the probability of including relevant frag-
ments in the dictionary. After 10k, all the relevant
fragments are already there and adding more only
makes computation harder. We can see that for a
threshold value of 100k total training time amounts
to 1,104 minutes, i.e. 36% of BC. For a threshold
value of 10k, learning time further decreases to 916
minutes, i.e. less than 30%. This threshold value
was used to train the individual linearized role clas-
sifiers that make up RMP.
These considerations are backed by the trend of
classification accuracy shown in Figure 5, where the
Precision, Recall and F1 measure of BCP, evaluated
on the test set, are shown in comparison with BC.
We can see that BCP precision is almost constant,
while its recall increases as we increase the thresh-
old, reaches a maximum of 78.95% for a threshold
of 10k and then settles around 78.8%. The F1 score
is maximized for a threshold of 10k, where it mea-
sures 81.10, i.e. just 0.66 points less than BC. We
can also see that BCP is constantly more conserva-
tive than BC, i.e. it always has higher precision and
lower recall.
Table 1 compares side to side the accuracy
(columns P, R and F1), training (Train) and test
(Test) times of the different classifiers (central block
of columns) and their linearized counterparts (block
on the right). Times are measured in minutes. For
the linearized classifiers, test time is the sum of
TFX and ESC time, but the only relevant contribu-
tion comes from TFX, as the low dimensional linear
space and fast linear kernel allow us to classify test
instances very efficiently 2. Overall, BCP test time is
39 minutes, which is more than 6 times faster than
BC (i.e. 247 minutes). It should be stressed that we
</bodyText>
<footnote confidence="0.998908333333333">
2Although ESC is not shown in table, the classification of all
149k test instances with BC¢ took 5 seconds with a threshold of
1k and 17 seconds with a threshold of 100k.
</footnote>
<table confidence="0.940152666666667">
Learning parallelization
Linearized (Thr=10k)
Task Non Lin.
1 cpu 5 cpus 10 cpus
BC 3,059 916 293 215
RM 2,596 1,090 297 198
</table>
<tableCaption confidence="0.977022333333333">
Table 2: Learning time when exploiting the framework’s
parallelization capabilities. Column Non Lin. lists non-
linearized training time.
</tableCaption>
<bodyText confidence="0.999844075">
are comparing against a fast TK implementation that
is almost linear in time with respect to the number of
tree nodes (Moschitti, 2006).
Concerning RMP, we can see that the accuracy
loss is even less than with BCP, i.e. it reaches an F1
measure of 87.13 which is just 0.52 less than RM.
It is also interesting to note how the individual lin-
earized role classifiers manage to perform accurately
regardless of the distribution of examples in the data
set: for all the six classifiers the final accuracy is
in line with that of the corresponding non-linearized
classifier. In two cases, i.e. A2 and A4, the accuracy
of the linearized classifier is even higher, i.e. 74.20
vs. 73.13 and 69.72 vs. 69.10, respectively. As for
the efficiency, total training time for RMP is 37% of
RM, i.e. 1,190 vs. 2,596 minutes, while test time
is reduced to 60%, i.e. 16 vs 27 minutes. These
improvements are less evident than those measured
for boundary detection. The main reason is that
the training set for boundary classification is much
larger, i.e. 1 million vs. 179k instances: therefore,
splitting training data during FSL has a reduced im-
pact on the overall efficiency of RMP.
Parallelization. All the efficiency improvements
that have been discussed so far considered a com-
pletely sequential process. But one of the advan-
tages of our approach is that it allows us to paral-
lelize some aspect of SVM training. Indeed, every
activity (but ESL) can exploit some degree of par-
allelism: during FSL, all the models can be learnt
at the same time (for this activity, the maximum de-
gree of parallelization is conditioned by the number
of training data splits); during FMI, models can be
mined concurrently; during TFX, the data-set to be
linearized can be split arbitrarily and individual seg-
ments can be processed in parallel. Exploiting this
possibility we can drastically improve learning ef-
ficiency. As an example, in Table 2 we show how
the total learning of the BCP can be cut to as low as
215 seconds when exploiting ten CPUs and using a
</bodyText>
<page confidence="0.98658">
36
</page>
<figure confidence="0.813931">
Models
</figure>
<figureCaption confidence="0.9175085">
Figure 6: Growth of dictionary size when including frag-
ments from more splits at different threshold values.
When a low threshold is used, the contribution of indi-
vidual dictionaries tends to be more marginal.
</figureCaption>
<bodyText confidence="0.9998935">
threshold of 10k. Even running on just 5 CPUs, the
overall computational cost of BCP is less than 10%
of BC (Column Non Lin.). Similar considerations
can be drawn concerning the role multi-classifier.
Fragment space. In this section we take a look at
the fragments included in the dictionary of the BCP
classifier. During FMI, we incrementally merge the
fragments mined from each of the models learnt dur-
ing FSL. Figure 6 plots, for different threshold val-
ues, the percentage of new fragments (on the y axis)
that the i-th model (on the x axis) contributes with
respect to the number of fragments mined from each
model (i.e. the threshold value).
If we consider the curve for a threshold equal to
100k, we can see that each model after the first ap-
proximately contributes with the same number of
fragments. On the other hand, if the threshold is set
to 1k than the contribution of subsequent models is
increasingly more marginal. Eventually, less than
10% of the fragments mined from the last model are
new ones. This behaviour suggests that there is a
core set of very relevant fragments which is com-
mon across models learnt on different data, i.e. they
are relevant for the task and do not strictly depend
on the training data that we use. When we increase
the threshold value, the new fragments that we index
are more and more data specific.
The dictionary compiled with a threshold of 10k
lists 62,760 distinct fragments. 15% of the frag-
ments contain the predicate node (which generally
is the node encoding the predicate word’s POS tag),
more than one third contain the candidate argument
node and, of these, about one third are rooted in it.
This last figure strongly suggests that the internal
structure of an argument is indeed a very powerful
feature not only for role classification, as we would
expect, but also for boundary detection. About 10%
of the fragments contain both the predicate and the
argument node, while about 1% encode the Path fea-
ture traditionally used in explicit semantic role label-
ing models (Gildea and Jurafsky, 2002). About 5%
encode a sort of extended Path feature, where the ar-
gument node is represented together with its descen-
dants. Overall, about 2/3 of the fragments contain at
least some terminal symbol (i.e. words), generally a
preposition or an adverb.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999936866666667">
We presented a supervised learning framework for
Support Vector Machines that tries to combine the
power and modeling simplicity of convolution ker-
nels with the advantages of linear kernels and ex-
plicit feature representations. We tested our model
on a Semantic Role Labeling benchmark and ob-
tained very promising results in terms of accuracy
and efficiency. Indeed, our linearized classifiers
manage to be almost as accurate as non linearized
ones, while drastically reducing the time required to
train and test a model on the same amounts of data.
To our best knowledge, the main points of nov-
elty of this work are the following: 1) it addresses
the problem of feature selection for tree kernels, ex-
ploiting SVM decisions to guide the process; 2) it
provides an effective way to make the kernel space
observable; 3) it can efficiently linearize structured
data without the need for an explicit mapping; 4) it
combines feature selection and SVM parallelization.
We began investigating the fragments generated
by a TK function for SRL, and believe that study-
ing them in more depth will be useful to identify
new, relevant features for the characterization of
predicate-argument relations.
In the months to come, we plan to run a set of ex-
periments on a wider list of tasks so as to consolidate
the results we obtained so far. We will also test the
generality of the approach by testing with different
high-dimensional kernel families, such as sequence
and polynomial kernels.
</bodyText>
<figure confidence="0.999054">
1 2 3 4 5 6 7 8 9 10
Cumulative contribution (%)
100
80
60
40
20
1k 5k 10k
50k 100k
</figure>
<page confidence="0.995002">
37
</page>
<sectionHeader confidence="0.995422" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929544554455">
Fabio Aiolli, Giovanni Da San Martino, Alessandro Sper-
duti, and Alessandro Moschitti. 2006. Fast on-line
kernel learning for trees. In Proceedings of ICDM’06.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean Michel Renders. 2003. Word sequence kernels.
Journal of Machine Learning Research, 3:1059–1082.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In Proceedings of CoNLL’05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL’00.
Michael Collins and Nigel Duffy. 2002. New Rank-
ing Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In Pro-
ceedings of ACL’02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
Tree Kernels for Relation Extraction. In Proceedings
of ACL’04.
Chad Cumby and Dan Roth. 2003. Kernel Methods for
Relational Learning. In Proceedings of ICML 2003.
Hal Daum´e III and Daniel Marcu. 2004. Np bracketing
by maximum entropy tagging and SVM reranking. In
Proceedings of EMNLP’04.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245–288.
Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Dur-
danovic, and Vladimir Vapnik. 2004. Parallel support
vector machines: The cascade svm. In Neural Infor-
mation Processing Systems.
Isabelle Guyon and Andr´e Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3:1157–1182.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, Dept. of Computer Sci-
ence, University of California at Santa Cruz.
T. Joachims. 2000. Estimating the generalization per-
formance of a SVM efficiently. In Proceedings of
ICML’00.
Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for
semi-structured data. In Proceedings of ICML’02.
Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding
up training with tree kernels for node relation labeling.
In Proceedings of HLT-EMNLP’05.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL’03.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL’05.
Alessandro Moschitti and Cosmin Bejan. 2004. A se-
mantic kernel for predicate argument classification. In
CoNLL-2004, Boston, MA, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree ker-
nel joint inference. In Proceedings of CoNLL-X, New
York City.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193–224.
Alessandro Moschitti. 2006. Making tree kernels prac-
tical for natural language learning. In Proccedings of
EACL’06.
Julia Neumann, Christoph Schnorr, and Gabriele Steidl.
2005. Combined SVM-Based Feature Selection and
Classification. Machine Learning, 61(1-3):129–150.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Comput. Linguist., 31(1):71–106.
J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U.
Dayal, and M. C. Hsu. 2001. PrefixSpan Mining Se-
quential Patterns Efficiently by Prefix Projected Pat-
tern Growth. In Proceedings of ICDE’01.
Alain Rakotomamonjy. 2003. Variable selection using
SVM based criteria. Journal of Machine Learning Re-
search, 3:1357–1370.
Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP’06.
Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree
Kernels with Statistical Feature Mining. In Proceed-
ings of the 19th Annual Conference on Neural Infor-
mation Processing Systems (NIPS’05).
Ivan Titov and James Henderson. 2006. Porting statisti-
cal parsers with data-defined kernels. In Proceedings
of CoNLL-X.
Kristina Toutanova, Penka Markova, and Christopher
Manning. 2004. The Leaf Path Projection View of
Parse Trees: Exploring String Kernels for HPSG Parse
Selection. In Proceedings of EMNLP 2004.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience.
Jason Weston, Sayan Mukherjee, Olivier Chapelle, Mas-
similiano Pontil, Tomaso Poggio, and Vladimir Vap-
nik. 2001. Feature Selection for SVMs. In Proceed-
ings of NIPS’01.
Jason Weston, Andr´e Elisseeff, Bernhard Sch¨olkopf, and
Mike Tipping. 2003. Use of the zero norm with lin-
ear models and kernel methods. J. Mach. Learn. Res.,
3:1439–1461.
Mohammed J Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of KDD’02.
</reference>
<page confidence="0.999352">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238525">
<title confidence="0.999864">Efficient Linearization of Tree Kernel Functions</title>
<author confidence="0.932788">Daniele</author>
<email confidence="0.64736">FBK-Irst,HLT</email>
<author confidence="0.529582">Via di_Sommarive</author>
<author confidence="0.529582">I- Povo Italy</author>
<email confidence="0.986923">pighin@fbk.eu</email>
<author confidence="0.960007">Alessandro</author>
<affiliation confidence="0.832368">University of Trento, DISI Via di Sommarive, 14 I-38100 Povo (TN) Italy</affiliation>
<email confidence="0.998025">moschitti@disi.unitn.it</email>
<abstract confidence="0.996941176470588">The combination of Support Vector Machines with very high dimensional kernels, such as string or tree kernels, suffers from two major drawbacks: first, the implicit representation of feature spaces does not allow us to understand which features actually triggered the generalization; second, the resulting computational burden may in some cases render unfeasible to use large data sets for training. We propose an approach based on feature space reverse engineering to tackle both problems. Our experiments with Tree Kernels on a Semantic Role Labeling data set show that the proposed approach can drastically reduce the computational footprint while yielding almost unaffected accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabio Aiolli</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Sperduti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Fast on-line kernel learning for trees.</title>
<date>2006</date>
<booktitle>In Proceedings of ICDM’06.</booktitle>
<contexts>
<context position="17280" citStr="Aiolli et al., 2006" startWordPosition="3065" endWordPosition="3068">o-average of the individual classifiers trained to recognize core PropBank roles. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2-driven relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmar</context>
</contexts>
<marker>Aiolli, Martino, Sperduti, Moschitti, 2006</marker>
<rawString>Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti. 2006. Fast on-line kernel learning for trees. In Proceedings of ICDM’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
<author>Eric Gaussier</author>
<author>Cyril Goutte</author>
<author>Jean Michel Renders</author>
</authors>
<title>Word sequence kernels.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="17580" citStr="Cancedda et al., 2003" startWordPosition="3115" endWordPosition="3118">re selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predica</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel Renders. 2003. Word sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL’05.</booktitle>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling. In Proceedings of CoNLL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL’00.</booktitle>
<contexts>
<context position="17983" citStr="Charniak, 2000" startWordPosition="3180" endWordPosition="3181"> interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM). Setup. If the constituency parse tree t of a sentence s is available, we can look at all the pairs (p, ni), where ni is any node in the tree and p is the node dominating w, and de</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02.</booktitle>
<contexts>
<context position="5340" citStr="Collins and Duffy, 2002" startWordPosition="844" endWordPosition="847">= αiyik(oi, o) + b i=1 with the advantage that we do not need to provide an explicit mapping φ(·) of our examples in a vector space. A Tree Kernel function is a convolution kernel (Haussler, 1999) defined over pairs of trees. Practically speaking, the kernel between two trees evaluates the number of substructures (or fragments) they have in common, i.e. it is a measure of their overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Implicitly, a TK function establishes a correspondence between distinct fragments and dimensions in some fragment space, i.e. the space of all the possible fragments. To simplify</context>
<context position="17456" citStr="Collins and Duffy, 2002" startWordPosition="3095" endWordPosition="3098">n kernels based on x2-driven relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arque</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. In Proceedings of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency Tree Kernels for Relation Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’04.</booktitle>
<contexts>
<context position="17610" citStr="Culotta and Sorensen, 2004" startWordPosition="3119" endWordPosition="3122">nel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for Relation Extraction. In Proceedings of ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chad Cumby</author>
<author>Dan Roth</author>
</authors>
<title>Kernel Methods for Relational Learning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML</booktitle>
<contexts>
<context position="17534" citStr="Cumby and Roth, 2003" startWordPosition="3107" endWordPosition="3110">only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the </context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Chad Cumby and Dan Roth. 2003. Kernel Methods for Relational Learning. In Proceedings of ICML 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
<author>Daniel Marcu</author>
</authors>
<title>Np bracketing by maximum entropy tagging and SVM reranking.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04.</booktitle>
<marker>Daum´e, Marcu, 2004</marker>
<rawString>Hal Daum´e III and Daniel Marcu. 2004. Np bracketing by maximum entropy tagging and SVM reranking. In Proceedings of EMNLP’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--245</pages>
<contexts>
<context position="30646" citStr="Gildea and Jurafsky, 2002" startWordPosition="5399" endWordPosition="5402"> 15% of the fragments contain the predicate node (which generally is the node encoding the predicate word’s POS tag), more than one third contain the candidate argument node and, of these, about one third are rooted in it. This last figure strongly suggests that the internal structure of an argument is indeed a very powerful feature not only for role classification, as we would expect, but also for boundary detection. About 10% of the fragments contain both the predicate and the argument node, while about 1% encode the Path feature traditionally used in explicit semantic role labeling models (Gildea and Jurafsky, 2002). About 5% encode a sort of extended Path feature, where the argument node is represented together with its descendants. Overall, about 2/3 of the fragments contain at least some terminal symbol (i.e. words), generally a preposition or an adverb. 6 Conclusions We presented a supervised learning framework for Support Vector Machines that tries to combine the power and modeling simplicity of convolution kernels with the advantages of linear kernels and explicit feature representations. We tested our model on a Semantic Role Labeling benchmark and obtained very promising results in terms of accur</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans P Graf</author>
<author>Eric Cosatto</author>
<author>Leon Bottou</author>
<author>Igor Durdanovic</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Parallel support vector machines: The cascade svm.</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<contexts>
<context position="7854" citStr="Graf et al., 2004" startWordPosition="1304" endWordPosition="1307">able data these models will be less robust with respect to the K(T1, T2) = (O(T1), O(T2)) = 1 A B A B A O(T1) = [2, 1, 1, 1, 1, 0, 0] B O(T2) = [0, 0, 0, 0, 1, 1, 1] D A C Fragment space A B A C D B A C A D C C B A T1 T2 A A B A B A B A 1 2 3 4 5 6 7 A B A B A 31 minimization of the empirical risk (Vapnik, 1998). Nonetheless, since we do not need to employ them for classification (but just to direct our feature selection process, as we will describe shortly), we can accept to rely on sub-optimal weights. Furthermore, research results in the field of SVM parallelization using cascades of SVMs (Graf et al., 2004) suggest that support vectors collected from locally learnt models can encode many of the relevant features retained by models learnt globally. Henceforth, let Ms be the model associated with the s-th split, and Fs the fragment space that can describe all the trees in Ms. Fragment Mining and Indexing (FMI) In Equation 1 it is possible to isolate the gradient w~ = Pn i=1 αiyi ~xi, with ~xi = [x(1) i , ... , x(N) i ], N being the dimensionality of the feature space. For a tree kernel function, we can rewrite x(j) ias: Itil PNk=1(ti,kλ`(fk))2 where: ti,j is the number of occurrences of the fragme</context>
<context position="17020" citStr="Graf et al., 2004" startWordPosition="3021" endWordPosition="3024">Test) time of non-linearized (center) and linearized (right) classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers trained to recognize core PropBank roles. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2-driven relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e </context>
</contexts>
<marker>Graf, Cosatto, Bottou, Durdanovic, Vapnik, 2004</marker>
<rawString>Hans P. Graf, Eric Cosatto, Leon Bottou, Igor Durdanovic, and Vladimir Vapnik. 2004. Parallel support vector machines: The cascade svm. In Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Andr´e Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="15010" citStr="Guyon and Elisseeff, 2003" startWordPosition="2662" endWordPosition="2666">ring TFX neither its base fragment nor its expansions are generated. Explicit Space Learning (ESL) After linearizing the training data, we can learn a very fast model by using all the available data and a linear kernel. The fragment space is now explicit, as there is a mapping between the input vectors and the fragments they encode. Explicit Space Classification (ESC) After learning the linear model, we can classify our linearized test data and evaluate the accuracy of the resulting classifier. vious work 4 Pre A rather comprehensive overview of feature selection techniques is carried out in (Guyon and Elisseeff, 2003). Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and Gaussian kernels, e.g. (Weston et al., 2001) and (Neumann et al., 2005). Weston et al. (2003) use the norm in the SVM optimizer to stress the feature selection capabilities of the learning algorithm. In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kern `0 els. VP VP bought D-B b</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>Isabelle Guyon and Andr´e Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>Dept. of Computer Science, University of California at Santa Cruz.</institution>
<contexts>
<context position="4912" citStr="Haussler, 1999" startWordPosition="782" endWordPosition="783">sifying example and w� and b are the separating hyperplane’s gradient and its bias, respectively. The gradient is a linear combination of the training points xz, their labels yi and their weights αi. These and the bias are optimized at training time by the learning algorithm. Applying the so-called kernel trick it is possible to replace the scalar product with a kernel function defined over pairs of objects: n f(o) = αiyik(oi, o) + b i=1 with the advantage that we do not need to provide an explicit mapping φ(·) of our examples in a vector space. A Tree Kernel function is a convolution kernel (Haussler, 1999) defined over pairs of trees. Practically speaking, the kernel between two trees evaluates the number of substructures (or fragments) they have in common, i.e. it is a measure of their overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on </context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, Dept. of Computer Science, University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Estimating the generalization performance of a SVM efficiently.</title>
<date>2000</date>
<booktitle>In Proceedings of ICML’00.</booktitle>
<contexts>
<context position="21912" citStr="Joachims, 2000" startWordPosition="3893" endWordPosition="3894"> linearization technique, we did not carry out an evaluation on the whole SRL task using the official CoNLL’05 evaluator. Indeed, producing complete annotations requires several steps (e.g. overlap resolution, OvA or Pairwise combination of individual role classifiers) that would shade off the actual impact of the methodology on classification. Platform. All the experiments were run on a machine equipped with 4 Intel ® Xeon ® CPUs clocked at 1.6 GHz and 4 GB of RAM running on a Linux 2.6.9 kernel. As a supervised learning framework we used SVM-Light-TK 1, which extends the SVMLight optimizer (Joachims, 2000) with tree kernel 1http://disi.unitn.it/˜moschitt/Tree-Kernel.htm Figure 5: BCP accuracy for different thresholds. support. During FSL, we learn the models using a normalized SST kernel and the default decay factor A = 0.4. The same parameters are used to train the models of the non linearized classifiers. During ESL, the classifier is trained using a linear kernel. We did not carry out further parametrization of the learning algorithm. Results. The left side of Table 1 shows the distribution of positive (Column Pos) and negative (Neg) data points in each classifier’s training set. The central</context>
</contexts>
<marker>Joachims, 2000</marker>
<rawString>T. Joachims. 2000. Estimating the generalization performance of a SVM efficiently. In Proceedings of ICML’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hisashi Kashima</author>
<author>Teruo Koyanagi</author>
</authors>
<title>Kernels for semi-structured data.</title>
<date>2002</date>
<booktitle>In Proceedings of ICML’02.</booktitle>
<contexts>
<context position="5373" citStr="Kashima and Koyanagi, 2002" startWordPosition="849" endWordPosition="852">e advantage that we do not need to provide an explicit mapping φ(·) of our examples in a vector space. A Tree Kernel function is a convolution kernel (Haussler, 1999) defined over pairs of trees. Practically speaking, the kernel between two trees evaluates the number of substructures (or fragments) they have in common, i.e. it is a measure of their overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Implicitly, a TK function establishes a correspondence between distinct fragments and dimensions in some fragment space, i.e. the space of all the possible fragments. To simplify, a tree t can be represented as </context>
</contexts>
<marker>Kashima, Koyanagi, 2002</marker>
<rawString>Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semi-structured data. In Proceedings of ICML’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Speeding up training with tree kernels for node relation labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP’05.</booktitle>
<contexts>
<context position="17669" citStr="Kazama and Torisawa, 2005" startWordPosition="3128" endWordPosition="3131">rallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the pr</context>
</contexts>
<marker>Kazama, Torisawa, 2005</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2005. Speeding up training with tree kernels for node relation labeling. In Proceedings of HLT-EMNLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL’03.</booktitle>
<contexts>
<context position="1951" citStr="Kudo and Matsumoto, 2003" startWordPosition="285" endWordPosition="288">ly represent examples in some high dimensional kernel space, where their similarity is evaluated. Kernel functions can generate a very large number of features, which are then weighted by the SVM optimization algorithm obtaining a feature selection side-effect. Indeed, the weights encoded by the gradient of the separating hyperplane learnt by the SVM implicitly establish a ranking between features in the kernel space. This property has been exploited in feature selection models based on 30 approximations or transformations of the gradient, e.g. (Rakotomamonjy, 2003), (Weston et al., 2003) or (Kudo and Matsumoto, 2003). However, kernel based systems have two major drawbacks: first, new features may be discovered in the implicit space but they cannot be directly observed. Second, since learning is carried out in the dual space, it is not possible to use the faster SVM or perceptron algorithms optimized for linear spaces. Consequently, the processing of large data sets can be computationally very expensive, limiting the use of large amounts of data for our research or applications. We propose an approach that tries to fill in the gap between explicit and implicit feature representations by 1) selecting the mo</context>
<context position="15332" citStr="Kudo and Matsumoto, 2003" startWordPosition="2716" endWordPosition="2719">hey encode. Explicit Space Classification (ESC) After learning the linear model, we can classify our linearized test data and evaluate the accuracy of the resulting classifier. vious work 4 Pre A rather comprehensive overview of feature selection techniques is carried out in (Guyon and Elisseeff, 2003). Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and Gaussian kernels, e.g. (Weston et al., 2001) and (Neumann et al., 2005). Weston et al. (2003) use the norm in the SVM optimizer to stress the feature selection capabilities of the learning algorithm. In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kern `0 els. VP VP bought D-B bought D NN a a cat S (A0) NP VP (A1) ⇒ NNP NP D NN Mary bought VB VB-P NP VB-P NP-B ⎧ ⎨ ⎩ do 33 Task Data set Neg Train Non-linearized classifiers Fl Linearized classifiers (Thr=10k) Pos Test P R Train Test P R Fl A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40 A1 90,636 88,455 1,206 11 89.45 88.62 89.0</context>
<context position="17510" citStr="Kudo and Matsumoto, 2003" startWordPosition="3103" endWordPosition="3106"> our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: bounda</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05.</booktitle>
<contexts>
<context position="17690" citStr="Kudo et al., 2005" startWordPosition="3132" endWordPosition="3135">ich is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Cosmin Bejan</author>
</authors>
<title>A semantic kernel for predicate argument classification.</title>
<date>2004</date>
<booktitle>In CoNLL-2004,</booktitle>
<location>Boston, MA, USA.</location>
<contexts>
<context position="17774" citStr="Moschitti and Bejan, 2004" startWordPosition="3144" endWordPosition="3147"> idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role</context>
</contexts>
<marker>Moschitti, Bejan, 2004</marker>
<rawString>Alessandro Moschitti and Cosmin Bejan. 2004. A semantic kernel for predicate argument classification. In CoNLL-2004, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<location>New York City.</location>
<contexts>
<context position="17745" citStr="Moschitti et al., 2006" startWordPosition="3140" endWordPosition="3143">uce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), where</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of CoNLL-X, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="17482" citStr="Moschitti et al., 2008" startWordPosition="3099" endWordPosition="3102">n relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decom</context>
<context position="18879" citStr="Moschitti et al., 2008" startWordPosition="3339" endWordPosition="3342">signed the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM). Setup. If the constituency parse tree t of a sentence s is available, we can look at all the pairs (p, ni), where ni is any node in the tree and p is the node dominating w, and decide whether ni is an argument node or not, i.e. whether it exactly dominates all and only the words encoding any of w’s arguments. The objects that we classify are subsets of the input parse tree that encompass both p and ni. Namely, we use the ASTm structure defined in (Moschitti et al., 2008), which is the minimal tree that covers all and only the words of p and ni. In the ASTm, p and ni are marked so that they can be distinguished from the other nodes. An ASTm is regarded as a positive example for BC if ni is an argument node, otherwise it is considered a negative example. Positive BC examples can be used to train an efficient RM: for each role r we can train a classifier whose positive examples are argument nodes whose label is exactly r, whereas negative examples are argument nodes labeled r&apos; 7� r. Two ASTms extracted from an example parse tree are shown in Figure 3: the first </context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proccedings of EACL’06.</booktitle>
<contexts>
<context position="5230" citStr="Moschitti, 2006" startWordPosition="830" endWordPosition="831">s possible to replace the scalar product with a kernel function defined over pairs of objects: n f(o) = αiyik(oi, o) + b i=1 with the advantage that we do not need to provide an explicit mapping φ(·) of our examples in a vector space. A Tree Kernel function is a convolution kernel (Haussler, 1999) defined over pairs of trees. Practically speaking, the kernel between two trees evaluates the number of substructures (or fragments) they have in common, i.e. it is a measure of their overlap. The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). Different TK functions are characterized by alternative fragment definitions, e.g. (Collins and Duffy, 2002) and (Kashima and Koyanagi, 2002). In the context of this paper we will be focusing on the SubSet Tree (SST) kernel described in (Collins and Duffy, 2002), which relies on a fragment definition that does not allow to break production rules (i.e. if any child of a node is included in a fragment, then also all the other children have to). As such, it is especially indicated for tasks involving constituency parsed texts. Implicitly, a TK function establishes a correspondence between disti</context>
<context position="26482" citStr="Moschitti, 2006" startWordPosition="4685" endWordPosition="4686">i.e. 247 minutes). It should be stressed that we 2Although ESC is not shown in table, the classification of all 149k test instances with BC¢ took 5 seconds with a threshold of 1k and 17 seconds with a threshold of 100k. Learning parallelization Linearized (Thr=10k) Task Non Lin. 1 cpu 5 cpus 10 cpus BC 3,059 916 293 215 RM 2,596 1,090 297 198 Table 2: Learning time when exploiting the framework’s parallelization capabilities. Column Non Lin. lists nonlinearized training time. are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). Concerning RMP, we can see that the accuracy loss is even less than with BCP, i.e. it reaches an F1 measure of 87.13 which is just 0.52 less than RM. It is also interesting to note how the individual linearized role classifiers manage to perform accurately regardless of the distribution of examples in the data set: for all the six classifiers the final accuracy is in line with that of the corresponding non-linearized classifier. In two cases, i.e. A2 and A4, the accuracy of the linearized classifier is even higher, i.e. 74.20 vs. 73.13 and 69.72 vs. 69.10, respectively. As for the efficiency</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proccedings of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Neumann</author>
<author>Christoph Schnorr</author>
<author>Gabriele Steidl</author>
</authors>
<date>2005</date>
<booktitle>Combined SVM-Based Feature Selection and Classification. Machine Learning,</booktitle>
<pages>61--1</pages>
<contexts>
<context position="15174" citStr="Neumann et al., 2005" startWordPosition="2689" endWordPosition="2692"> using all the available data and a linear kernel. The fragment space is now explicit, as there is a mapping between the input vectors and the fragments they encode. Explicit Space Classification (ESC) After learning the linear model, we can classify our linearized test data and evaluate the accuracy of the resulting classifier. vious work 4 Pre A rather comprehensive overview of feature selection techniques is carried out in (Guyon and Elisseeff, 2003). Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and Gaussian kernels, e.g. (Weston et al., 2001) and (Neumann et al., 2005). Weston et al. (2003) use the norm in the SVM optimizer to stress the feature selection capabilities of the learning algorithm. In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kern `0 els. VP VP bought D-B bought D NN a a cat S (A0) NP VP (A1) ⇒ NNP NP D NN Mary bought VB VB-P NP VB-P NP-B ⎧ ⎨ ⎩ do 33 Task Data set Neg Train Non-linearized classifiers Fl Linearized cla</context>
</contexts>
<marker>Neumann, Schnorr, Steidl, 2005</marker>
<rawString>Julia Neumann, Christoph Schnorr, and Gabriele Steidl. 2005. Combined SVM-Based Feature Selection and Classification. Machine Learning, 61(1-3):129–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comput. Linguist.,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="17931" citStr="Palmer et al., 2005" startWordPosition="3171" endWordPosition="3174">(DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM). Setup. If the constituency parse tree t of a sentence s is available, we can look at all the pairs (p, ni), where ni is any nod</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Comput. Linguist., 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pei</author>
<author>J Han</author>
<author>Mortazavi B Asl</author>
<author>H Pinto</author>
<author>Q Chen</author>
<author>U Dayal</author>
<author>M C Hsu</author>
</authors>
<title>PrefixSpan Mining Sequential Patterns Efficiently by Prefix Projected Pattern Growth.</title>
<date>2001</date>
<booktitle>In Proceedings of ICDE’01.</booktitle>
<contexts>
<context position="15393" citStr="Pei et al., 2001" startWordPosition="2726" endWordPosition="2729">near model, we can classify our linearized test data and evaluate the accuracy of the resulting classifier. vious work 4 Pre A rather comprehensive overview of feature selection techniques is carried out in (Guyon and Elisseeff, 2003). Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and Gaussian kernels, e.g. (Weston et al., 2001) and (Neumann et al., 2005). Weston et al. (2003) use the norm in the SVM optimizer to stress the feature selection capabilities of the learning algorithm. In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kern `0 els. VP VP bought D-B bought D NN a a cat S (A0) NP VP (A1) ⇒ NNP NP D NN Mary bought VB VB-P NP VB-P NP-B ⎧ ⎨ ⎩ do 33 Task Data set Neg Train Non-linearized classifiers Fl Linearized classifiers (Thr=10k) Pos Test P R Train Test P R Fl A0 60,900 118,191 521 7 90.26 92.95 91.59 209 3 88.95 91.91 90.40 A1 90,636 88,455 1,206 11 89.45 88.62 89.03 376 3 89.39 88.13 88.76 A2 21,291 157,800 692 7 84.56 64.42</context>
</contexts>
<marker>Pei, Han, Asl, Pinto, Chen, Dayal, Hsu, 2001</marker>
<rawString>J. Pei, J. Han, Mortazavi B. Asl, H. Pinto, Q. Chen, U. Dayal, and M. C. Hsu. 2001. PrefixSpan Mining Sequential Patterns Efficiently by Prefix Projected Pattern Growth. In Proceedings of ICDE’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain Rakotomamonjy</author>
</authors>
<title>Variable selection using SVM based criteria.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1357</pages>
<contexts>
<context position="1898" citStr="Rakotomamonjy, 2003" startWordPosition="278" endWordPosition="279">ossibility of using kernel functions to implicitly represent examples in some high dimensional kernel space, where their similarity is evaluated. Kernel functions can generate a very large number of features, which are then weighted by the SVM optimization algorithm obtaining a feature selection side-effect. Indeed, the weights encoded by the gradient of the separating hyperplane learnt by the SVM implicitly establish a ranking between features in the kernel space. This property has been exploited in feature selection models based on 30 approximations or transformations of the gradient, e.g. (Rakotomamonjy, 2003), (Weston et al., 2003) or (Kudo and Matsumoto, 2003). However, kernel based systems have two major drawbacks: first, new features may be discovered in the implicit space but they cannot be directly observed. Second, since learning is carried out in the dual space, it is not possible to use the faster SVM or perceptron algorithms optimized for linear spaces. Consequently, the processing of large data sets can be computationally very expensive, limiting the use of large amounts of data for our research or applications. We propose an approach that tries to fill in the gap between explicit and im</context>
</contexts>
<marker>Rakotomamonjy, 2003</marker>
<rawString>Alain Rakotomamonjy. 2003. Variable selection using SVM based criteria. Journal of Machine Learning Research, 3:1357–1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Anoop Sarkar</author>
<author>Aravind k Joshi</author>
</authors>
<title>Using LTAG Based Features in Parse Reranking.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP’06.</booktitle>
<contexts>
<context position="17555" citStr="Shen et al., 2003" startWordPosition="3111" endWordPosition="3114">rly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that a</context>
</contexts>
<marker>Shen, Sarkar, Joshi, 2003</marker>
<rawString>Libin Shen, Anoop Sarkar, and Aravind k. Joshi. 2003. Using LTAG Based Features in Parse Reranking. In Proceedings of EMNLP’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Sequence and Tree Kernels with Statistical Feature Mining.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Annual Conference on Neural Information Processing Systems (NIPS’05).</booktitle>
<contexts>
<context position="16767" citStr="Suzuki and Isozaki (2005)" startWordPosition="2981" endWordPosition="2984">.07 69.72 A5 69 179,022 3 0 100.00 50.00 66.67 63 2 100.00 50.00 66.67 BC 61,062 938,938 3,059 247 82.57 80.96 81.76 916 39 83.36 78.95 81.10 RM - - 2,596 27 89.37 86.00 87.65 1,090 16 88.50 85.81 87.13 Table 1: Accuracy (P, R, Fl), training (Train) and test (Test) time of non-linearized (center) and linearized (right) classifiers. Times are in minutes. For each task, columns Pos and Neg list the number of positive and negative training examples, respectively. The accuracy of the role multiclassifiers is the micro-average of the individual classifiers trained to recognize core PropBank roles. Suzuki and Isozaki (2005) present an embedded approach to feature selection for convolution kernels based on x2-driven relevance assessment. To our knowledge, this is the only published work clearly focusing on feature selection for tree kernel functions. In (Graf et al., 2004), an approach to SVM parallelization is presented which is based on a divide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP</context>
</contexts>
<marker>Suzuki, Isozaki, 2005</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with Statistical Feature Mining. In Proceedings of the 19th Annual Conference on Neural Information Processing Systems (NIPS’05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Porting statistical parsers with data-defined kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X.</booktitle>
<contexts>
<context position="17719" citStr="Titov and Henderson, 2006" startWordPosition="3136" endWordPosition="3139">ide-et-impera strategy to reduce optimization time. The idea of using a compact graph representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Bound</context>
</contexts>
<marker>Titov, Henderson, 2006</marker>
<rawString>Ivan Titov and James Henderson. 2006. Porting statistical parsers with data-defined kernels. In Proceedings of CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="17802" citStr="Toutanova et al., 2004" startWordPosition="3149" endWordPosition="3152">representation to represent the support vectors of a TK function is explored in (Aiolli et al., 2006), where a Direct Acyclic Graph (DAG) is employed. Concerning the use of kernels for NLP, interesting models and results are described, for example, in (Collins and Duffy, 2002), (Moschitti et al., 2008), (Kudo and Matsumoto, 2003), (Cumby and Roth, 2003), (Shen et al., 2003), (Cancedda et al., 2003), (Culotta and Sorensen, 2004), (Daum´e III and Marcu, 2004), (Kazama and Torisawa, 2005), (Kudo et al., 2005), (Titov and Henderson, 2006), (Moschitti et al., 2006), (Moschitti and Bejan, 2004) or (Toutanova et al., 2004). 5 Experiments We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). SRL can be decomposed into two tasks: boundary detection, where the word sequences that are arguments of a predicate word w are identified, and role classification, where each argument is assigned the proper role. The former task requires a binary Boundary Classifier (BC), whereas the second involves a Role Multi-class Classifier (RM)</context>
</contexts>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Kristina Toutanova, Penka Markova, and Christopher Manning. 2004. The Leaf Path Projection View of Parse Trees: Exploring String Kernels for HPSG Parse Selection. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="7549" citStr="Vapnik, 1998" startWordPosition="1256" endWordPosition="1257">feature selection process. Since the time complexity of SVM training is approximately quadratic in the number of examples, this way we can considerably accelerate the process of estimating support vector weights. According to statistical learning theory, being trained on smaller subsets of the available data these models will be less robust with respect to the K(T1, T2) = (O(T1), O(T2)) = 1 A B A B A O(T1) = [2, 1, 1, 1, 1, 0, 0] B O(T2) = [0, 0, 0, 0, 1, 1, 1] D A C Fragment space A B A C D B A C A D C C B A T1 T2 A A B A B A B A 1 2 3 4 5 6 7 A B A B A 31 minimization of the empirical risk (Vapnik, 1998). Nonetheless, since we do not need to employ them for classification (but just to direct our feature selection process, as we will describe shortly), we can accept to rely on sub-optimal weights. Furthermore, research results in the field of SVM parallelization using cascades of SVMs (Graf et al., 2004) suggest that support vectors collected from locally learnt models can encode many of the relevant features retained by models learnt globally. Henceforth, let Ms be the model associated with the s-th split, and Fs the fragment space that can describe all the trees in Ms. Fragment Mining and In</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Sayan Mukherjee</author>
<author>Olivier Chapelle</author>
<author>Massimiliano Pontil</author>
<author>Tomaso Poggio</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Feature Selection for SVMs.</title>
<date>2001</date>
<booktitle>In Proceedings of NIPS’01.</booktitle>
<contexts>
<context position="15147" citStr="Weston et al., 2001" startWordPosition="2684" endWordPosition="2687">learn a very fast model by using all the available data and a linear kernel. The fragment space is now explicit, as there is a mapping between the input vectors and the fragments they encode. Explicit Space Classification (ESC) After learning the linear model, we can classify our linearized test data and evaluate the accuracy of the resulting classifier. vious work 4 Pre A rather comprehensive overview of feature selection techniques is carried out in (Guyon and Elisseeff, 2003). Non-filter approaches for SVMs and kernel machines are often concerned with polynomial and Gaussian kernels, e.g. (Weston et al., 2001) and (Neumann et al., 2005). Weston et al. (2003) use the norm in the SVM optimizer to stress the feature selection capabilities of the learning algorithm. In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al., 2001) is used to efficiently mine the features in a low degree polynomial kernel space. The authors discuss an approximation of their method that allows them to handle high degree polynomial kern `0 els. VP VP bought D-B bought D NN a a cat S (A0) NP VP (A1) ⇒ NNP NP D NN Mary bought VB VB-P NP VB-P NP-B ⎧ ⎨ ⎩ do 33 Task Data set Neg Train Non-linearized cl</context>
</contexts>
<marker>Weston, Mukherjee, Chapelle, Pontil, Poggio, Vapnik, 2001</marker>
<rawString>Jason Weston, Sayan Mukherjee, Olivier Chapelle, Massimiliano Pontil, Tomaso Poggio, and Vladimir Vapnik. 2001. Feature Selection for SVMs. In Proceedings of NIPS’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Andr´e Elisseeff</author>
<author>Bernhard Sch¨olkopf</author>
<author>Mike Tipping</author>
</authors>
<title>Use of the zero norm with linear models and kernel methods.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1439</pages>
<marker>Weston, Elisseeff, Sch¨olkopf, Tipping, 2003</marker>
<rawString>Jason Weston, Andr´e Elisseeff, Bernhard Sch¨olkopf, and Mike Tipping. 2003. Use of the zero norm with linear models and kernel methods. J. Mach. Learn. Res., 3:1439–1461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed J Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proceedings of KDD’02.</booktitle>
<contexts>
<context position="9171" citStr="Zaki, 2002" startWordPosition="1554" endWordPosition="1555">or; and `(fj) is the depth of the fragment. The relevance |w(j) |of the fragment fj can be measured as: .(3) We fix a threshold L and from each model Ms (learnt during FSL) we select the L most relevant fragments, i.e. we build the set Fs,L = Uk{fk} so that: |Fs,L |= Land |w(k) |&gt; |w(i)|Vfi E F \ Fs,L . In order to do so, we need to harvest all the fragments with a fast extraction function, store them in a compact data structure and finally select the fragments with the highest relevance. Our strategy is exemplified in Figure 2. First, we represent each fragment as a sequence as described in (Zaki, 2002). A sequence contains the labels of the fragment nodes in depth-first order. By default, each node is the child of the previous node in the sequence. A special symbol (T) indicates that the next node in the Figure 2: Fragment indexing. Each fragment is represented as a sequence 1 and then encoded as a path in the index 2 which keeps track of its cumulative relevance. sequence should be attached after climbing one level in the tree. For example, the tree (B (Z W)) in figure is represented as the sequence [B, Z, T, W]. Then, we add the elements of the sequence to a graph (which we call an index </context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed J Zaki. 2002. Efficiently mining frequent trees in a forest. In Proceedings of KDD’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>