<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.973535">
Close = Relevant? The Role of Context in Efficient Language Production
</title>
<author confidence="0.996094">
Ting Qian and T. Florian Jaeger
</author>
<affiliation confidence="0.997329">
Department of Brain and Cognitive Sciences
University of Rochester
</affiliation>
<address confidence="0.859512">
Rochester, NY 14627 United States
</address>
<email confidence="0.999684">
{tqian,fjaeger}@bcs.rochester.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999292">
We formally derive a mathematical model
for evaluating the effect of context rele-
vance in language production. The model
is based on the principle that distant con-
textual cues tend to gradually lose their
relevance for predicting upcoming linguis-
tic signals. We evaluate our model against
a hypothesis of efficient communication
(Genzel and Charniak’s Constant Entropy
Rate hypothesis). We show that the devel-
opment of entropy throughout discourses
is described significantly better by a model
with cue relevance decay than by previ-
ous models that do not consider context ef-
fects.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980833333333">
In this paper, we present a study on the effect
of context relevance decay on the entropy of lin-
guistic signals in natural discourses. Context rele-
vance decay refers to the phenomenon that contex-
tual cues that are distant from an upcoming event
(e.g. production of a new linguistic signal) are less
likely to be relevant to the event, as discourse con-
tents that are close to one another are likely to be
semantically related. One can also view the words
and sentences in a discourse as time steps, where
distant context becomes less relevant simply due
to normal forgetting over time (e.g. activation de-
cay in memory). The present study investigates
how this decaying property of discourse context
might affect the development of entropy of lin-
guistic signals in discourses. We first introduce
the background on efficient language production
and then propose our hypothesis.
</bodyText>
<subsectionHeader confidence="0.8560065">
1.1 Background on Efficient Language
Production
</subsectionHeader>
<bodyText confidence="0.999986232558139">
The metaphor “communication channel”, bor-
rowed from Shannon’s information theory (Shan-
non, 1948), can be conceived of as an abstract en-
tity that defines the constraints of language com-
munication (e.g. ambient noise, distortions in ar-
ticulation). For error free communication to occur,
the ensemble of messages that a speaker may utter
must be encoded in a system of signals whose en-
tropy is under the capacity of the communication
channel. Entropy of these signals, in this context,
correlates with the average number of upcoming
messages that the speaker can choose from for a
particular signal (e.g. a word to be spoken) given
preceding discourse context. In other words, if
the average number of choices given any linguis-
tic signal exceeds the channel capacity, it cannot
be guaranteed that the receiver can correctly infer
the originally intended message. Such transmis-
sion errors will reduce the efficiency of language
communication.
Keeping the entropy of linguistic signals be-
low the channel capacity alone is not efficient, for
one can devise a code where each signal corre-
sponds to a distinct message. With a unique choice
per signal, this encoding achieves an entropy of
zero at the cost of requiring a look-up table that
is too large to be possible (cf. Zipf (1935), who
makes a similar argument for meaning and form).
In fact, the most efficient code requires language
users to encode messages into signals of the en-
tropy bounded by the capacity of the channel. One
implication of this efficient encoding is that over
time, the entropy of the signals is constant. One
of the first studies to investigate such constancy
is Genzel and Charniak (2002), in which the au-
thors proposed the Constant Entropy Rate (CER)
hypothesis: in written text, the entropy per sig-
nal symbol is constant across sentence positions in
discourses. That is, if we view sentence positions
as a measure of time steps, then the entropy per
word at each step should be the same in order to
achieve efficient communication (word is selected
as the unit of signal, although it does not have to
</bodyText>
<page confidence="0.993">
45
</page>
<note confidence="0.856553666666667">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45–53,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
be case; cf. Qian and Jaeger (2009)).
</note>
<bodyText confidence="0.999865038461539">
The difficulty in testing this direct prediction is
computationally specifying the code used by hu-
man speakers to obtain a context-sensitive esti-
mate of the entropy per word. An ngram model
overestimates the entropy of upcoming messages
by relying on only the preceding n-1 words within
a sentence, while in reality the upcoming message
is also constrained by extra-sentential context that
accumulates within a discourse. The more extra-
sentential context that the ngram model ignores,
the higher estimate for entropy will be. Hence,
the CER hypothesis indirectly predicts that the
entropy of signals, as estimated by ngrams, will
increase across sentence positions. While some
studies have found the predicted positive correla-
tion between sentence position and the per-word
entropy of signals estimated by ngrams, most of
them assumed the correlation to be linear (Genzel
and Charniak, 2002; Genzel and Charniak, 2003;
Keller, 2004; Piantadosi and Gibson, 2008). How-
ever, in previous work, we found that a log-linear
regression model was a better fit for empirical data
than a simple linear regression model based on
data of 12 languages (Qian and Jaeger, under re-
view). Why this would be case remained a puzzle.
Our research question is closely related to this
indirect prediction of the Constant Entropy Rate
hypothesis. Intuitively, the number of possible
messages that a speaker can choose from for an
upcoming signal in a discourse is often restricted
by the presence of discourse context. Contex-
tual cues in the preceding discourse can make the
upcoming content more predictable and thus ef-
fectively reduces signal entropy. As previously
mentioned, however, different contextual cues, de-
pending on how long ago they were provided, have
various degrees of effectiveness in reducing sig-
nal entropy. Thus we ask the question whether
the decay of context relevance could explain the
sublinear relation between entropy and discourse
progress that has been observed in previous stud-
ies.
We formally derive two nonlinear models for
testing our Relevance Decay Hypothesis (intro-
duced next). In addition to the constant entropy as-
sumption in CER, our model assumed that the rel-
evance of early sentences in the discourse system-
atically decays as a function of discourse progress.
Our models provide the best fit to the distribution
of entropy of signals, suggesting the availability
of discourse context can affect the planning of the
rest of a discourse.
</bodyText>
<subsectionHeader confidence="0.988133">
1.2 Relevance Decay Hypothesis
</subsectionHeader>
<bodyText confidence="0.999990936170213">
We hypothesize the sublinear relation between the
entropy of signals, when estimated out of dis-
course context (hereafter, out-of-context entropy
of signals) using an ngram model, and sentence
position (Piantadosi and Gibson, 2008; Qian and
Jaeger, under review) is due to the role of dis-
course context (hereafter, context). Consider the
following example. Assume that context at the kth
sentence position comes from the 1... k − 1 sen-
tences in the past. If k is large enough, context
from the early sentences 1... i (i « k) is essen-
tially no longer relevant. Rather, the nearby k − i
sentences are contributing most of the discourse
context. As a result, the constraint on the entropy
of signals at sentence position k is mostly due to
the nearby window of k − i sentences. Then if we
look ahead to the (k + 1)th sentence position and
follow the same steps of reasoning, context at that
point also mostly comes from the nearby window
of k − i sentences (i.e. (k + 1) − (i + 1) = k − i).
Hence, for later sentence positions, the difference
in available context is minimal. Consequently,
their out-of-context entropy of signals increases
at a very small rate. On the other hand, when k
is fairly small, to the extent that the k − i win-
dow covers the entire preceding discourse, all of
the 1... k − 1 sentences are contributing relevant
context. As k increases, the number of preced-
ing sentences increases, which results in a more
significant change in relevant context, but the rel-
evance of each individual sentence decreases with
its distance to k, which results in a sublinear pat-
tern of relevant context with respect to sentence
position overall. As we will show, the relation of
out-of-context entropy of signals to sentence posi-
tion follows from the relation of relevant context
to sentence position, exhibiting a sublinear form
as well.
The problem of interest here is to specify how
quickly the relevance of a preceding sentence de-
cays as a function of its distance to a target sen-
tence position k. We experimented with two forms
of decay functions – power law decay and expo-
nential decay. It has been established that many
types of human behaviors can be well described by
the power function (Wixted and Ebbesen, 1991),
so we mainly focus on building a model under the
</bodyText>
<page confidence="0.999468">
46
</page>
<table confidence="0.999523428571429">
Language Training Data Test Data
in words in sentences in words in sentences per position
Danish 154,514 5,640 8,048 270 18
Dutch 50,309 3,255 2,105 90 6
English 597,698 23,295 31,276 1155 77
French 229,461 9,300 11,371 435 29
Italian 97,198 4,245 4,524 225 15
Mandarin Chinese 145,127 4,875 4,310 150 10
Norwegian 89,724 4,125 2,973 150 10
Portuguese 170,342 5,340 9,044 240 16
Russian 398,786 18,075 20,668 930 62
Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138
Spanish (European) 255,366 7,485 8,653 240 16
Swedish 266,348 11,535 13,369 555 37
</table>
<tableCaption confidence="0.840280666666667">
Table 1: Number of words and sentences in the training and test data for each of the twelve languages.
The last column gives the number of sentences at each sentence position (which is identical to the number
of documents contained in the corpora).
</tableCaption>
<bodyText confidence="0.999798285714286">
power law, and examine if the model under the ex-
ponential law yields any difference. Under the as-
sumptions of true entropy rate is constant across
sentences, we predict that our models will bet-
ter characterize the changes in estimated entropy
of signals than general regression models that are
blind to the role of context.
</bodyText>
<sectionHeader confidence="0.992972" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.9337">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999054740740741">
We used the Reuters Corpus Volume 1 and 2
(Lewis et al., 2004). The corpus contains about
810,000 English news articles and over 487,000
news articles in thirteen languages. Because of in-
consistent annotation, we excluded the data from
three languages, Chinese, German, and Japanese.
For Chinese, we substituted the Treebank Cor-
pus (Xue et al., 2005) for the Reuters data, leav-
ing us with twelve languages: Danish, Dutch,
English, French, Italian, Mandarin Chinese, Nor-
wegian, Portuguese, Russian, European Spanish,
Latin-American Spanish, and Swedish. In order
to estimate out-of-context entropy per word (i.e.
per signal symbol) for each sentence position, ar-
ticles were divided into a training set (95% of all
stories) for training language models and a test set
(the remaining 5%) for analysis (see Table 1 for
details). Out-of-context entropy per word was es-
timated by computing the average log probability
of sentences at that position, normalized by their
lengths in words (i.e. for an individual sentence
token s, the term to be averaged is − l ogp(s) bits per
le(word). Standard trigram language models were
used to compute these probabilities (Clarkson and
Rosenfeld, 1997). The majority of the 12 lan-
guages belong to the Indo-European family, while
Mandarin Chinese is a Sino-Tibetan language.
</bodyText>
<subsectionHeader confidence="0.999568">
2.2 Modeling Relevance Decay of Context
</subsectionHeader>
<bodyText confidence="0.999440166666667">
Formally, we define the relevance of context in the
same unit as entropy of signals – bits per word.
Let r0 denote the entropy of signals that efficiently
encode the ensemble of messages a speaker can
choose from for any sentence position, a constant
under the assumption of CER. According to Infor-
mation Theory, r0 is equivalent to the uncertainty
associated with any sentence position if context is
considered. Thus, in error free communication,
linguistic signals presented at the kth sentence po-
sition are said to have resolved the uncertainty at
k and therefore are r0-bit relevant at the kth sen-
tence position. Then, at the (k+i)th sentence posi-
tion, these linguistic signals have become context
by definition and their relevance has decayed to
some r bits. Our models start from defining the
value of r as a function of the distance between
context and a target sentence position.
</bodyText>
<subsectionHeader confidence="0.827739">
2.2.1 Power-law Decay Model
</subsectionHeader>
<bodyText confidence="0.9995734">
If the relevance of a cue q (e.g. a preceding sen-
tence), which is originally r0-bit relevant at po-
sition kQ, decays at the rate following the power
function, its remaining relevance at target sentence
position k is:
</bodyText>
<equation confidence="0.664558">
relevancepo,,,(k, q) = r0(k − kQ + 1)−-` (1)
</equation>
<bodyText confidence="0.9988235">
In Equation (1), k &gt; kQ and A is the decay rate.
This means at position k, the relevance of the cue
</bodyText>
<page confidence="0.996041">
47
</page>
<bodyText confidence="0.9998138">
from the (k−1)th sentence is r0 *2−A-bit relevant;
the relevance of the cue from the (k−2)th sentence
is r0 * 3−A-bit relevant, and so on. As a result, the
relevance of discourse-specific context at position
k is the marginalization of all cues up to qk−1:
</bodyText>
<equation confidence="0.985944">
�contextpow(k) = r0 (k − kq,a + 1)_�&apos; (2)
q%E{q1...qk−11
</equation>
<bodyText confidence="0.977793333333333">
The general trend predicted by Equation (2)
is that discourse-specific context increases more
rapidly at the beginning of a discourse and much
more slowly towards the end due to the relevance
decay of distant cues. Rewriting Equation (2) in
a closed-form formula so that a model can be fit-
ted to data is not a trivial task without knowing the
rate A, but the paradox is that A has to be estimated
from the data. As a workaround, we approximated
the value of Equation (2) by computing a definite
integral of Equation (1), where Di is a shorthand
fork − kq + 1:
</bodyText>
<equation confidence="0.990078666666667">
rpo,,,(k) = context(k) + r0 (4)
k1−A − 1
= r0 1 − A + r0
</equation>
<bodyText confidence="0.999970666666667">
Whether speakers will utilize all available con-
text as predicted by Equation (4) is another de-
bate. Here we adopt the view that speakers are
maximally efficient in that they do make use of
all available context. Thus, we make the predic-
tion that out-of-context entropy of signals, as ob-
served empirically from data, can be described by
this model. Figure 1 shows the behavior of this
function with various parameter sets.
</bodyText>
<equation confidence="0.941514666666667">
2 4 6 8 10 12 14
Model−Predicted Entropy per Word
5 6 7 8 9 10 11 12
r0 = 5.5,λ = 2
r0 = 5.5,λ = 2.2
r0 = 5,λ = 2
r0 = 5,λ = 2.2
fk
contextpo,,,(k) � J roDi−AdDi
1
= r0(k1−A − 1
1 − A ) (3)
</equation>
<subsectionHeader confidence="0.745009">
Sentence Position
</subsectionHeader>
<bodyText confidence="0.991084545454545">
Equation (3) uses an integral to approximate the
sum of a series defined as a function. The result
is usually acceptable as long as A is greater than
1 so that the series defined by Equation 1 is con-
vergent (this assumption is empirically supported;
see Figure 5). Note that Equation (3) produces
the desirable effect that upon encountering the
first sentence of a discourse, no discourse-specific
contextual cues are available to the speaker (i.e.
context(1) = 0).
Now that we know the maximum relevance of
context at sentence position k, we can predict the
amount of out-of-context entropy of signals r(k)
based on the idea of uncertainty again. There are
new linguistic signals that are r0-bit relevant in
context at any sentence position. In addition, we
now know context(k) bits of relevant context are
also available. Thus, the sum of r0 and context(k)
defines the maximum amount of out-of-context
uncertainty that can be resolved at sentence posi-
tion k. Therefore, the out-of-context entropy of
signals at k is at most:
</bodyText>
<figureCaption confidence="0.980261">
Figure 1: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is a power function.
</figureCaption>
<subsectionHeader confidence="0.641077">
2.2.2 Exponential Decay Model
</subsectionHeader>
<bodyText confidence="0.99902425">
The second model assumes the relevance of con-
text decays exponentially. Following the same no-
tations as before, the relevance of a cue q at posi-
tion k is:
</bodyText>
<equation confidence="0.923027">
relevance,p(k, q) = r0e−A(k−kq) (5)
</equation>
<bodyText confidence="0.9999612">
The major difference between the power func-
tion and the exponential one is that the relevance
of a contextual cue drops more slowly in the expo-
nential case (Anderson, 1995). The relevance of
all discourse-specific context for a speaker at k is:
</bodyText>
<equation confidence="0.985437">
context,xp(k) = r0 k−1� e−Ai (6)
i=1
</equation>
<page confidence="0.939579">
48
</page>
<bodyText confidence="0.968078">
Equation (6) is the sum of a geometric progres-
sion series. We can write Equation (6) in a closed-
form:
</bodyText>
<equation confidence="0.990303">
contextexp(k) = eλ − 1(1 − e−(k−1)λ) (7)
r0
</equation>
<bodyText confidence="0.85216">
As a result, the out-of-context entropy of signals
is:
</bodyText>
<equation confidence="0.995718">
rexp(k) = eλ − 1(1 − e−(k−1)λ) + r0 (8)
r0
</equation>
<bodyText confidence="0.8779092">
Figure 2 schematically shows the behavior of
this function. One can notice this function con-
verges against a ceiling more quickly than the
power function. Thus, this model makes a slightly
different prediction from the power law model.
</bodyText>
<figure confidence="0.50648">
Sentence Position
</figure>
<figureCaption confidence="0.926234">
Figure 2: Schematic plots of the behavior of out-
of-context entropy of signals assuming the decay
of the relevance of context is an exponential func-
tion.
</figureCaption>
<subsectionHeader confidence="0.985807">
2.3 Nonlinear Regression Analysis
</subsectionHeader>
<bodyText confidence="0.99982">
To test whether the proposed models (i.e. Equa-
tions 4 and 8) better characterize the data, we
built nonlinear regression models with document-
specific random effects, where the out-of-context
entropy of signals, rij, is regressed on sentence
position, kj. Based on the power law model, we
have
</bodyText>
<equation confidence="0.98044">
k1−β2 − 1
rij = (Q1+b1i) j1 − Q2 +(Q1+b1i)+Eij (9)
</equation>
<bodyText confidence="0.999493428571429">
where Q1 corresponds to r0, the theoretical con-
stant entropy of signals under an ideal encod-
ing. b1i represents the document-specific devia-
tions from the overall mean. Q2 corresponds to A,
the mean rate at which the relevance of a past cue
decays, which is unfortunately not considered for
random effects for the practical purpose of making
computation feasible in the current work. Finally,
Eij represents the errors independently distributed
as N(0, U2), orthogonal to document specific de-
viations.
For the exponential model, the nonlinear model
is the following (symbols have the same interpre-
tations as in Equation 9):
</bodyText>
<equation confidence="0.998894333333333">
(Q1 + b1i)
rij = eβ2 − 1 (1−e−(k;−1)β2)+(Q1+b1i)+Eij
(10)
</equation>
<bodyText confidence="0.999809142857143">
Fitting data with the above nonlinear models
requires starting estimates for fixed-effect coeffi-
cients (i.e. Q1s and Q2s). Unfortunately, there are
no principled methods for selecting these values.
We heuristically selected 6 for Q1 and 2 for Q2 as
starting values for the power law model, and 4 and
0.5 as starting values for the exponential model.
</bodyText>
<sectionHeader confidence="0.999973" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999953333333333">
We examined the quality of the models and the pa-
rameters in the models: r0, the within-context en-
tropy rate, and A, the rate of context decay.
</bodyText>
<subsectionHeader confidence="0.99876">
3.1 Model Quality Comparison
</subsectionHeader>
<bodyText confidence="0.999761578947369">
The CER hypothesis indirectly predicts that out-
of-context entropy of signals of sentence positions
(bits per word) should increase throughout a dis-
course. The two models go one step further to
predict specific sublinear increase patterns, based
on the speaker’s considerations of the relevance of
past contextual cues. We compared the quality of
models in terms of Bayesian Information Criterion
(BIC) within languages. A lower BIC score indi-
cates a better fit. As shown by Figure 3, we find
our models best explain the data in 9 out of the 12
languages, reporting lower BIC scores than both
the linear and log-linear models as reported in our
previous work (Qian and Jaeger, under review).
For Danish, English and Italian, although neither
of our models produced a better score than the log-
linear model, the relative difference is small: 0.54
on average (comparing to BIC scores on the order
of 102 to 103).
</bodyText>
<equation confidence="0.907040857142857">
2 4 6 8 10 12 14
Model−Predicted Entropy per Word
5 6 7 8 9 10 11 12
r0 = 5.5,λ = 0.6
r0 = 5.5,λ = 0.8
r0 = 5,λ = 0.6
r0 = 5,λ = 0.8
</equation>
<page confidence="0.984951">
49
</page>
<figure confidence="0.837942">
−20 −10 0 10 20
Power Law Exponential Loglinear Linear
Danish Dutch English French Italian Mandarin Norwegian Portuguese Russian E.Spanish L.Spanish Swedish
</figure>
<figureCaption confidence="0.995246333333333">
Figure 3: Our models yield superior BIC scores in most languages. The y-axis shows the differences
between BIC scores of individual models for a language and mean BIC of the models for that language
(E.Spanish = European Spanish; L.Spanish = Latin-American Spanish).
</figureCaption>
<bodyText confidence="0.9997476">
Specifically, in terms of BIC scores, the power-
law model is better than the linear model (t(11) =
−3.98, p &lt; 0.01), and the log-linear model
(t(11) = −3.10, p &lt; 0.05). The exponen-
tial model is also better than the linear model
(t(11) = −3.98, p &lt; 0.01), and the log-linear
model (t(11) = −3.18, p &lt; 0.01). The power-
law model and the exponential model are not sig-
nificantly different from each other (t(11) = 0.5,
p &gt; 0.5).
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Interpretation of Parameters
</subsectionHeader>
<bodyText confidence="0.9999301">
Constant Entropy of Signals ro. Both models
are constructed in such a way that the first param-
eter ro, in theory, corresponds to the theoretical
within-context entropy of signals of sentence po-
sitions. This parameter refers to how many bits per
word are needed to encode the ensemble of mes-
sages at a sentence position when context is taken
into account. The CER hypothesis directly pre-
dicts that this rate should be constant throughout
a discourse. Although we are unable to test this
prediction directly, it is nevertheless interesting to
compare whether these two independently devel-
oped models yield the same estimates for this pa-
rameter in each language.
Figure 4 shows encouraging results. Not only
the estimates made by the power model are well
correlated with those by the exponential model,
but also the slope of this correlation is equal to 1
(t(10) = 1.01, p &lt; 0.0001). Since there are no
reasons a priori to suspect that these two models
</bodyText>
<figure confidence="0.371845">
Estimated Within−context Entropy per Word (Exponential Model)
</figure>
<figureCaption confidence="0.990618">
Figure 4: Estimates of ro correlate between both
models with a slope of 1.
</figureCaption>
<bodyText confidence="0.999899166666667">
would give the same estimates, this is a first step to
confirming the entropy per word in sentence pro-
duction is indeed a tractable constant throughout
discourses.
Among all languages, ro has a mean of 5.0
bits in both models, and a variance of 0.46 in
the power-law model and 0.48 in the exponential
model, both remarkably small. The similarity in
ro between languages may lead one to speculate
whether the amount of uncertainty per word in dis-
courses is largely the same regardless of the actual
language used by the speakers. On the other hand,
</bodyText>
<figure confidence="0.998027947368421">
4.0 4.5 5.0 5.5 6.0 6.5
Estimated Within−context Entropy per Word (Power−law Model)
4.0 4.5 5.0 5.5 6.0 6.5
●
●
●
Danish
Dutch
English
French
Italian
Mandarin
Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish
●
</figure>
<page confidence="0.9924">
50
</page>
<bodyText confidence="0.999760888888889">
the differences in r0 may reveal the specific prop-
erties of different languages. Meanwhile, precau-
tions need to be taken in interpreting those esti-
mates given that the corpora are of different sizes,
and the ngram model is simplistic in nature.
Decay Rate A. The second parameter A corre-
sponds to the rate of relevance decay in both mod-
els. Since the base relevance r0 varies between
languages, A can be more intuitively interpreted as
to indicate the percentage of the original relevance
of a contextual cue still remains in n positions. In
the power-law model, for example, the context in-
formation from a previous sentence in Danish, on
average, is only 11.6% (2−3.10 = 0.116) as rele-
vant. Hence, the relevance of a contextual cue de-
creases rather quickly for Danish. Table 2 shows
this is in fact the general picture for all languages
we tested.
</bodyText>
<table confidence="0.997879571428572">
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 11.6 3.3 1.4
Dutch 10.4 2.8 1.1
English 0.1 0.0 0.0
French 8.5 2.0 0.7
Italian 10.2 2.7 1.0
Mandarin 7.7 1.7 0.6
Norwegian 18.9 7.1 3.6
Portuguese 5.5 1.0 0.3
Russian 12.7 3.8 1.6
E. Spanish 0.8 0.0 0.0
L. Spanish 2.7 0.3 0.1
Swedish 5.8 1.1 0.3
</table>
<tableCaption confidence="0.913474">
Table 2: In the power model, relevance of a con-
</tableCaption>
<bodyText confidence="0.991998166666667">
textual cue decays rather quickly for each lan-
guage.
The picture of A looks a little different in the
exponential model. The relevance percentage on
average is significantly higher, which confirms an
earlier point that the power function decreases
more quickly than the exponential function. Table
3 shows a summary for the 12 languages.
One may note that the decay rate varies greatly
between languages under the prediction of both
models. However, these number are only approxi-
mations since the entropy estimated by the ngram
language model is far from psychological real-
ity. Furthermore, it is unlikely that speakers of
one language would exhibit the same decay rate
of context relevance in their production, let alone
speakers of different languages, who may be sub-
ject to language-specific constraints during pro-
</bodyText>
<table confidence="0.9937415">
Language Relevance of Context in Discourse (%)
1 pos. before 2 pos. before 3 pos. before
Danish 30.1 9.1 2.7
Dutch 28.7 8.2 2.4
English 9.6 0.9 0.1
French 26.7 7.1 1.9
Italian 28.7 8.2 2.4
Mandarin 25.7 6.6 1.7
Norwegian 42.3 17.9 7.6
Portuguese 22.5 5.1 1.1
Russian 34.6 12.0 4.2
E. Spanish 14.2 2.0 0.3
L. Spanish 18.6 3.5 0.6
Swedish 23.7 5.6 1.3
</table>
<tableCaption confidence="0.8882065">
Table 3: In the exponential model, relevance of a
contextual cue decays more slowly.
</tableCaption>
<bodyText confidence="0.99850995">
duction. Therefore, the variation in estimates of
A seems reasonable.
Correlation between r0 and A. Interestingly, r0
and A are highly correlated (r2 = 0.39,p &lt; 0.05
in the power model, Figure 5; r2 = 0.47,p &lt; 0.01
in the exponential model, Figure 6): a high rel-
evance decay rate tends to be coupled with high
within-context entropy of signals. This unan-
ticipated observation is in fact compatible with
the account of efficient language production: a
high within-context entropy of signals indicates
the base relevance of a contextual cue (i.e. r0)
is high. It is then useful for its relevance to de-
cay more quickly to allow the speaker to inte-
grate context from other cues. Otherwise, the to-
tal amount of relevant context may presumably
overload working memory. However, our cur-
rent results come from only cross-linguistic sam-
ples. Cross-validation in within-language samples
is needed for confirming this hypothesis.
</bodyText>
<subsectionHeader confidence="0.99988">
3.3 The Bigger Picture
</subsectionHeader>
<bodyText confidence="0.999981846153846">
Having obtained the estimates for r0 and A, we are
now in a position to examine how out-of-context
entropy of signals increases as a function of sen-
tence positions, given the estimates of these two
parameters. As shown in Figure 7, the predictions
from both models are qualitative similar except
that 1) when the decay rate in the power-law model
is low, out-of-context entropy of signals converges
more slowly than in the exponential model (Figure
7, right panel); 2) when the decay rate in the power
model is high, it almost converges as quickly as
the exponential model, and only minor differences
exist in their predictions (Figure 7, left panel).
</bodyText>
<page confidence="0.991288">
51
</page>
<figure confidence="0.999942408602152">
●
●
●
●
4 6 8 10
●
●
Danish
Dutch
English
French
Italian
Mandarin
Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish
●
0
Out−of−context Entropy per Word in Norwegian
●
●
●
●
●
●
●
●
●
4 5 6 7 8
4 5 6 7 8
●
●
●
Out−of−context Entropy per Word in Dutch
●
●
●
●
●●
●
●
●
●
●
●
●
● ● ●
●
●
●
●
●
●
0
●
●
●
●
●
●
●
●
● ● ●
●
●
● ●
●
●
Relevance Decay Rate
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
● ●
● ●
● ●
●
●
●
●
●
●
●
●
●
●
0
●
●
● ●●
●
●
●
●
●
●
●
● ●
● ●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
●
●
●
●
●
●
●
●
●
●
4.0 4.5 5.0 5.5 6.0
●
Power:λ = 3.27
Exp:λ = 1.25
Power:λ = 2.4
● Exp:λ = 0.86
●
Within−Context Entropy per Word
2 4 6 8 12 ●2 4 6 8 12
Sentence Position ● ●
Sentence Position
</figure>
<figureCaption confidence="0.98409475">
Figure 5: The rate of relevance decay is corre-
lated with within-context entropy of signals in the
power-law model.
Figure 7: Predicted out-of-context entropy of sig-
nals by the power-law model (solid) and the expo-
nential model (dashed) in Dutch and Norwegian,
with the actual distributions plotted on the back-
ground.
</figureCaption>
<figure confidence="0.975043833333333">
Danish
Dutch
English
French
Italian
Mandarin
Norwegian
Portuguese
Russian
E. Spanish
L. Spanish
Swedish
●
●
1.0 1.5 2.0
Relevance Decay Rate
●
rexp(k)� = r0�
</figure>
<bodyText confidence="0.73738175">
e� � 1(e−(k−1)a) (12)
for the exponential model, showing the rate of in-
crease is a monotonically decreasing exponential
function. These mathematical properties indeed
match our observations in Figure 7.
●
4.5 5.0 5.5 6.0
Within−Context Entropy per Word
</bodyText>
<sectionHeader confidence="0.647727" genericHeader="method">
4 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999080947368421">
The models introduced in this paper try to answer
this question: if the relevance of a contextual cue
for predicting an upcoming linguistic signal de-
cays over the course of a discourse, how much un-
certainty (entropy) is associated with each individ-
ual sentence position? We have shown under that
models that incorporate (power law or exponen-
tial) cue relevance decay in most cases describe
the relation of out-of-context entropy of signals
to sentence position are better accounted for than
previously suggested models.
We are continuing to investigate along this line.
Specifically, we are interested in finding the role of
semantic memory in affecting the relevance decay
of context. To test that, we plan to implement a
probabilistic topic model, in which topic continu-
ity between a preceding sentence and an upcom-
ing sentence is quantitatively measured. Thus, the
decay of contextual cues can be based on the esti-
</bodyText>
<figureCaption confidence="0.982351666666667">
Figure 6: The rate of relevance decay is correlated
with within-context entropy of signals in the expo-
nential model.
</figureCaption>
<bodyText confidence="0.999913375">
Because of the nonlinearity in our models, it
is not possible to report the results in an intuitive
manner as in “an increase in sentence position cor-
responds to an increase of X bits of out-of-context
entropy per word”. Instead, we can analytically
solve for the derivative of the predicted out-of-
context entropy of signals with respect to sentence
position (Equation 4 and 8). This gives us:
</bodyText>
<equation confidence="0.820589">
rpower(k)&apos; = r0k−-\ (11)
</equation>
<bodyText confidence="0.9987255">
for the power-law model, showing the rate of in-
crease in predicted out-of-context entropy of sig-
nals is a monotonically decreasing power function,
and
</bodyText>
<page confidence="0.993502">
52
</page>
<bodyText confidence="0.9999216">
mated semantic relatedness between sentences, in
addition to the abstract notion of rate as used in
this paper.
Finally, our relevance decay model can be ap-
plied to the domain of language processing as
well. For instance, the distance between a con-
textual cue and the target word may affect how
quickly a comprehender can process the informa-
tion conveyed by the word. We plan to address
these question in future work.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996333333333">
We have presented a new approach for examin-
ing the distribution of entropy of linguistic sig-
nals in discourses, showing that not only the out-
of-context entropy of signals increases sublinearly
with sentence position, but also the sublinear trend
is better explained by our nonlinear models than
by log-linear models of previous work. Our mod-
els are built on the assumption that the relevance
of a contextual cue for predicting a linguistic sig-
nal in the future decays with its distance to the tar-
get, and predict the relation of out-of-context en-
tropy of signals to sentence position in discourses.
These results indirectly lend support to the hypoth-
esis that speakers maintain a constant entropy of
signals across sentence positions in a discourse.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99995625">
We wish to thank Meredith Brown, Alex Fine and
three anonymous reviewers for their helpful com-
ments on this paper. This work was supported by
NSF grant BCS-0845059 to TFJ.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883837837838">
John R. Anderson. 1995. Learning and Memory: An
integrated approach. John Wiley &amp; Sons.
Philip R. Clarkson and Roni Rosenfeld. 1997. Sta-
tistical language modeling using the cmu-cambridge
toolkit. In Proceedings of ESCA Eurospeech.
Dimitry Genzel and Eugene Charniak. 2002. Entropy
rate constancy in text. In ACL, pages 199–206.
Dimitry Genzel and Eugene Charniak. 2003. Variation
of entropy and parse trees of sentences as a function
of the sentence number. in. In EMNLP, pages 65–
72.
Frank Keller. 2004. The entropy rate principle as a
predictor of processing effort: An evaluation against
eye-tracking data. In EMNLP, pages 317–324.
D. D. Lewis, Y. Yang, T. Rose, and F Li. 2004. Rcv1:
A new benchmark collection for text categorization
research. J Mach Learn Res, 5:361–397.
Steve Piantadosi and Edwards Gibson. 2008. Uniform
information density in discourse: a cross-corpus
analysis of syntactic and lexical predictability. In
CUNY.
Ting Qian and T. Florian Jaeger. 2009. Evidence
for efficient language production in chinese. In
CogSci09, pages 851–856.
Ting Qian and T. Florian Jaeger. under review. En-
tropy profiles in language: A cross-linguistic inves-
tigation.
C. E. Shannon. 1948. A mathematical theory of com-
munications. Bell Labs Tech J, 27(4):623–656.
J. T. Wixted and E. B. Ebbesen. 1991. On the form of
forgetting. Psychological Science, 2:409–415.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat Lang
Eng, 11:207–238.
G. K. Zipf. 1935. Psycho-Biology of Languages.
Houghton-Mifflin.
</reference>
<page confidence="0.999341">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.667560">
<title confidence="0.999678">Close = Relevant? The Role of Context in Efficient Language Production</title>
<author confidence="0.999968">Ting Qian</author>
<author confidence="0.999968">T Florian</author>
<affiliation confidence="0.9991245">Department of Brain and Cognitive University of</affiliation>
<address confidence="0.921191">Rochester, NY 14627 United</address>
<abstract confidence="0.9823924375">We formally derive a mathematical model for evaluating the effect of context relevance in language production. The model is based on the principle that distant contextual cues tend to gradually lose their relevance for predicting upcoming linguistic signals. We evaluate our model against a hypothesis of efficient communication (Genzel and Charniak’s Constant Entropy Rate hypothesis). We show that the development of entropy throughout discourses is described significantly better by a model with cue relevance decay than by previous models that do not consider context effects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John R Anderson</author>
</authors>
<title>Learning and Memory: An integrated approach.</title>
<date>1995</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="15734" citStr="Anderson, 1995" startWordPosition="2643" endWordPosition="2644">sition k. Therefore, the out-of-context entropy of signals at k is at most: Figure 1: Schematic plots of the behavior of outof-context entropy of signals assuming the decay of the relevance of context is a power function. 2.2.2 Exponential Decay Model The second model assumes the relevance of context decays exponentially. Following the same notations as before, the relevance of a cue q at position k is: relevance,p(k, q) = r0e−A(k−kq) (5) The major difference between the power function and the exponential one is that the relevance of a contextual cue drops more slowly in the exponential case (Anderson, 1995). The relevance of all discourse-specific context for a speaker at k is: context,xp(k) = r0 k−1� e−Ai (6) i=1 48 Equation (6) is the sum of a geometric progression series. We can write Equation (6) in a closedform: contextexp(k) = eλ − 1(1 − e−(k−1)λ) (7) r0 As a result, the out-of-context entropy of signals is: rexp(k) = eλ − 1(1 − e−(k−1)λ) + r0 (8) r0 Figure 2 schematically shows the behavior of this function. One can notice this function converges against a ceiling more quickly than the power function. Thus, this model makes a slightly different prediction from the power law model. Sentenc</context>
</contexts>
<marker>Anderson, 1995</marker>
<rawString>John R. Anderson. 1995. Learning and Memory: An integrated approach. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Clarkson</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Statistical language modeling using the cmu-cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings of ESCA Eurospeech.</booktitle>
<contexts>
<context position="11165" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="1827" endWordPosition="1830">n order to estimate out-of-context entropy per word (i.e. per signal symbol) for each sentence position, articles were divided into a training set (95% of all stories) for training language models and a test set (the remaining 5%) for analysis (see Table 1 for details). Out-of-context entropy per word was estimated by computing the average log probability of sentences at that position, normalized by their lengths in words (i.e. for an individual sentence token s, the term to be averaged is − l ogp(s) bits per le(word). Standard trigram language models were used to compute these probabilities (Clarkson and Rosenfeld, 1997). The majority of the 12 languages belong to the Indo-European family, while Mandarin Chinese is a Sino-Tibetan language. 2.2 Modeling Relevance Decay of Context Formally, we define the relevance of context in the same unit as entropy of signals – bits per word. Let r0 denote the entropy of signals that efficiently encode the ensemble of messages a speaker can choose from for any sentence position, a constant under the assumption of CER. According to Information Theory, r0 is equivalent to the uncertainty associated with any sentence position if context is considered. Thus, in error free commu</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip R. Clarkson and Roni Rosenfeld. 1997. Statistical language modeling using the cmu-cambridge toolkit. In Proceedings of ESCA Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitry Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Entropy rate constancy in text.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>199--206</pages>
<contexts>
<context position="3426" citStr="Genzel and Charniak (2002)" startWordPosition="548" endWordPosition="551">r one can devise a code where each signal corresponds to a distinct message. With a unique choice per signal, this encoding achieves an entropy of zero at the cost of requiring a look-up table that is too large to be possible (cf. Zipf (1935), who makes a similar argument for meaning and form). In fact, the most efficient code requires language users to encode messages into signals of the entropy bounded by the capacity of the channel. One implication of this efficient encoding is that over time, the entropy of the signals is constant. One of the first studies to investigate such constancy is Genzel and Charniak (2002), in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per signal symbol is constant across sentence positions in discourses. That is, if we view sentence positions as a measure of time steps, then the entropy per word at each step should be the same in order to achieve efficient communication (word is selected as the unit of signal, although it does not have to 45 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45–53, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Lingu</context>
<context position="4961" citStr="Genzel and Charniak, 2002" startWordPosition="791" endWordPosition="794">ding n-1 words within a sentence, while in reality the upcoming message is also constrained by extra-sentential context that accumulates within a discourse. The more extrasentential context that the ngram model ignores, the higher estimate for entropy will be. Hence, the CER hypothesis indirectly predicts that the entropy of signals, as estimated by ngrams, will increase across sentence positions. While some studies have found the predicted positive correlation between sentence position and the per-word entropy of signals estimated by ngrams, most of them assumed the correlation to be linear (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Piantadosi and Gibson, 2008). However, in previous work, we found that a log-linear regression model was a better fit for empirical data than a simple linear regression model based on data of 12 languages (Qian and Jaeger, under review). Why this would be case remained a puzzle. Our research question is closely related to this indirect prediction of the Constant Entropy Rate hypothesis. Intuitively, the number of possible messages that a speaker can choose from for an upcoming signal in a discourse is often restricted by the presence of discourse cont</context>
</contexts>
<marker>Genzel, Charniak, 2002</marker>
<rawString>Dimitry Genzel and Eugene Charniak. 2002. Entropy rate constancy in text. In ACL, pages 199–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitry Genzel</author>
<author>Eugene Charniak</author>
</authors>
<title>Variation of entropy and parse trees of sentences as a function of the sentence number. in.</title>
<date>2003</date>
<booktitle>In EMNLP,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="4988" citStr="Genzel and Charniak, 2003" startWordPosition="795" endWordPosition="798">tence, while in reality the upcoming message is also constrained by extra-sentential context that accumulates within a discourse. The more extrasentential context that the ngram model ignores, the higher estimate for entropy will be. Hence, the CER hypothesis indirectly predicts that the entropy of signals, as estimated by ngrams, will increase across sentence positions. While some studies have found the predicted positive correlation between sentence position and the per-word entropy of signals estimated by ngrams, most of them assumed the correlation to be linear (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Piantadosi and Gibson, 2008). However, in previous work, we found that a log-linear regression model was a better fit for empirical data than a simple linear regression model based on data of 12 languages (Qian and Jaeger, under review). Why this would be case remained a puzzle. Our research question is closely related to this indirect prediction of the Constant Entropy Rate hypothesis. Intuitively, the number of possible messages that a speaker can choose from for an upcoming signal in a discourse is often restricted by the presence of discourse context. Contextual cues in the</context>
</contexts>
<marker>Genzel, Charniak, 2003</marker>
<rawString>Dimitry Genzel and Eugene Charniak. 2003. Variation of entropy and parse trees of sentences as a function of the sentence number. in. In EMNLP, pages 65– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data. In</title>
<date>2004</date>
<booktitle>EMNLP,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="5002" citStr="Keller, 2004" startWordPosition="799" endWordPosition="800"> upcoming message is also constrained by extra-sentential context that accumulates within a discourse. The more extrasentential context that the ngram model ignores, the higher estimate for entropy will be. Hence, the CER hypothesis indirectly predicts that the entropy of signals, as estimated by ngrams, will increase across sentence positions. While some studies have found the predicted positive correlation between sentence position and the per-word entropy of signals estimated by ngrams, most of them assumed the correlation to be linear (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Piantadosi and Gibson, 2008). However, in previous work, we found that a log-linear regression model was a better fit for empirical data than a simple linear regression model based on data of 12 languages (Qian and Jaeger, under review). Why this would be case remained a puzzle. Our research question is closely related to this indirect prediction of the Constant Entropy Rate hypothesis. Intuitively, the number of possible messages that a speaker can choose from for an upcoming signal in a discourse is often restricted by the presence of discourse context. Contextual cues in the preceding dis</context>
</contexts>
<marker>Keller, 2004</marker>
<rawString>Frank Keller. 2004. The entropy rate principle as a predictor of processing effort: An evaluation against eye-tracking data. In EMNLP, pages 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>J Mach Learn Res,</journal>
<pages>5--361</pages>
<contexts>
<context position="10044" citStr="Lewis et al., 2004" startWordPosition="1653" endWordPosition="1656">ces in the training and test data for each of the twelve languages. The last column gives the number of sentences at each sentence position (which is identical to the number of documents contained in the corpora). power law, and examine if the model under the exponential law yields any difference. Under the assumptions of true entropy rate is constant across sentences, we predict that our models will better characterize the changes in estimated entropy of signals than general regression models that are blind to the role of context. 2 Methods 2.1 Data We used the Reuters Corpus Volume 1 and 2 (Lewis et al., 2004). The corpus contains about 810,000 English news articles and over 487,000 news articles in thirteen languages. Because of inconsistent annotation, we excluded the data from three languages, Chinese, German, and Japanese. For Chinese, we substituted the Treebank Corpus (Xue et al., 2005) for the Reuters data, leaving us with twelve languages: Danish, Dutch, English, French, Italian, Mandarin Chinese, Norwegian, Portuguese, Russian, European Spanish, Latin-American Spanish, and Swedish. In order to estimate out-of-context entropy per word (i.e. per signal symbol) for each sentence position, art</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>D. D. Lewis, Y. Yang, T. Rose, and F Li. 2004. Rcv1: A new benchmark collection for text categorization research. J Mach Learn Res, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Piantadosi</author>
<author>Edwards Gibson</author>
</authors>
<title>Uniform information density in discourse: a cross-corpus analysis of syntactic and lexical predictability.</title>
<date>2008</date>
<booktitle>In CUNY.</booktitle>
<contexts>
<context position="5032" citStr="Piantadosi and Gibson, 2008" startWordPosition="801" endWordPosition="804">age is also constrained by extra-sentential context that accumulates within a discourse. The more extrasentential context that the ngram model ignores, the higher estimate for entropy will be. Hence, the CER hypothesis indirectly predicts that the entropy of signals, as estimated by ngrams, will increase across sentence positions. While some studies have found the predicted positive correlation between sentence position and the per-word entropy of signals estimated by ngrams, most of them assumed the correlation to be linear (Genzel and Charniak, 2002; Genzel and Charniak, 2003; Keller, 2004; Piantadosi and Gibson, 2008). However, in previous work, we found that a log-linear regression model was a better fit for empirical data than a simple linear regression model based on data of 12 languages (Qian and Jaeger, under review). Why this would be case remained a puzzle. Our research question is closely related to this indirect prediction of the Constant Entropy Rate hypothesis. Intuitively, the number of possible messages that a speaker can choose from for an upcoming signal in a discourse is often restricted by the presence of discourse context. Contextual cues in the preceding discourse can make the upcoming c</context>
<context position="6783" citStr="Piantadosi and Gibson, 2008" startWordPosition="1079" endWordPosition="1082">t). In addition to the constant entropy assumption in CER, our model assumed that the relevance of early sentences in the discourse systematically decays as a function of discourse progress. Our models provide the best fit to the distribution of entropy of signals, suggesting the availability of discourse context can affect the planning of the rest of a discourse. 1.2 Relevance Decay Hypothesis We hypothesize the sublinear relation between the entropy of signals, when estimated out of discourse context (hereafter, out-of-context entropy of signals) using an ngram model, and sentence position (Piantadosi and Gibson, 2008; Qian and Jaeger, under review) is due to the role of discourse context (hereafter, context). Consider the following example. Assume that context at the kth sentence position comes from the 1... k − 1 sentences in the past. If k is large enough, context from the early sentences 1... i (i « k) is essentially no longer relevant. Rather, the nearby k − i sentences are contributing most of the discourse context. As a result, the constraint on the entropy of signals at sentence position k is mostly due to the nearby window of k − i sentences. Then if we look ahead to the (k + 1)th sentence positio</context>
</contexts>
<marker>Piantadosi, Gibson, 2008</marker>
<rawString>Steve Piantadosi and Edwards Gibson. 2008. Uniform information density in discourse: a cross-corpus analysis of syntactic and lexical predictability. In CUNY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Qian</author>
<author>T Florian Jaeger</author>
</authors>
<title>Evidence for efficient language production in chinese.</title>
<date>2009</date>
<booktitle>In CogSci09,</booktitle>
<pages>851--856</pages>
<contexts>
<context position="4068" citStr="Qian and Jaeger (2009)" startWordPosition="654" endWordPosition="657">rs proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per signal symbol is constant across sentence positions in discourses. That is, if we view sentence positions as a measure of time steps, then the entropy per word at each step should be the same in order to achieve efficient communication (word is selected as the unit of signal, although it does not have to 45 Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45–53, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics be case; cf. Qian and Jaeger (2009)). The difficulty in testing this direct prediction is computationally specifying the code used by human speakers to obtain a context-sensitive estimate of the entropy per word. An ngram model overestimates the entropy of upcoming messages by relying on only the preceding n-1 words within a sentence, while in reality the upcoming message is also constrained by extra-sentential context that accumulates within a discourse. The more extrasentential context that the ngram model ignores, the higher estimate for entropy will be. Hence, the CER hypothesis indirectly predicts that the entropy of signa</context>
</contexts>
<marker>Qian, Jaeger, 2009</marker>
<rawString>Ting Qian and T. Florian Jaeger. 2009. Evidence for efficient language production in chinese. In CogSci09, pages 851–856.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ting Qian</author>
<author>T Florian Jaeger</author>
</authors>
<title>under review. Entropy profiles in language: A cross-linguistic investigation.</title>
<marker>Qian, Jaeger, </marker>
<rawString>Ting Qian and T. Florian Jaeger. under review. Entropy profiles in language: A cross-linguistic investigation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communications.</title>
<date>1948</date>
<journal>Bell Labs Tech J,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1862" citStr="Shannon, 1948" startWordPosition="287" endWordPosition="289">emantically related. One can also view the words and sentences in a discourse as time steps, where distant context becomes less relevant simply due to normal forgetting over time (e.g. activation decay in memory). The present study investigates how this decaying property of discourse context might affect the development of entropy of linguistic signals in discourses. We first introduce the background on efficient language production and then propose our hypothesis. 1.1 Background on Efficient Language Production The metaphor “communication channel”, borrowed from Shannon’s information theory (Shannon, 1948), can be conceived of as an abstract entity that defines the constraints of language communication (e.g. ambient noise, distortions in articulation). For error free communication to occur, the ensemble of messages that a speaker may utter must be encoded in a system of signals whose entropy is under the capacity of the communication channel. Entropy of these signals, in this context, correlates with the average number of upcoming messages that the speaker can choose from for a particular signal (e.g. a word to be spoken) given preceding discourse context. In other words, if the average number </context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. E. Shannon. 1948. A mathematical theory of communications. Bell Labs Tech J, 27(4):623–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Wixted</author>
<author>E B Ebbesen</author>
</authors>
<title>On the form of forgetting.</title>
<date>1991</date>
<journal>Psychological Science,</journal>
<pages>2--409</pages>
<contexts>
<context position="8778" citStr="Wixted and Ebbesen, 1991" startWordPosition="1437" endWordPosition="1440">ntext with respect to sentence position overall. As we will show, the relation of out-of-context entropy of signals to sentence position follows from the relation of relevant context to sentence position, exhibiting a sublinear form as well. The problem of interest here is to specify how quickly the relevance of a preceding sentence decays as a function of its distance to a target sentence position k. We experimented with two forms of decay functions – power law decay and exponential decay. It has been established that many types of human behaviors can be well described by the power function (Wixted and Ebbesen, 1991), so we mainly focus on building a model under the 46 Language Training Data Test Data in words in sentences in words in sentences per position Danish 154,514 5,640 8,048 270 18 Dutch 50,309 3,255 2,105 90 6 English 597,698 23,295 31,276 1155 77 French 229,461 9,300 11,371 435 29 Italian 97,198 4,245 4,524 225 15 Mandarin Chinese 145,127 4,875 4,310 150 10 Norwegian 89,724 4,125 2,973 150 10 Portuguese 170,342 5,340 9,044 240 16 Russian 398,786 18,075 20,668 930 62 Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138 Spanish (European) 255,366 7,485 8,653 240 16 Swedish 266,348 11,535 13</context>
</contexts>
<marker>Wixted, Ebbesen, 1991</marker>
<rawString>J. T. Wixted and E. B. Ebbesen. 1991. On the form of forgetting. Psychological Science, 2:409–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Nat Lang Eng,</title>
<date>2005</date>
<pages>11--207</pages>
<contexts>
<context position="10332" citStr="Xue et al., 2005" startWordPosition="1697" endWordPosition="1700">ference. Under the assumptions of true entropy rate is constant across sentences, we predict that our models will better characterize the changes in estimated entropy of signals than general regression models that are blind to the role of context. 2 Methods 2.1 Data We used the Reuters Corpus Volume 1 and 2 (Lewis et al., 2004). The corpus contains about 810,000 English news articles and over 487,000 news articles in thirteen languages. Because of inconsistent annotation, we excluded the data from three languages, Chinese, German, and Japanese. For Chinese, we substituted the Treebank Corpus (Xue et al., 2005) for the Reuters data, leaving us with twelve languages: Danish, Dutch, English, French, Italian, Mandarin Chinese, Norwegian, Portuguese, Russian, European Spanish, Latin-American Spanish, and Swedish. In order to estimate out-of-context entropy per word (i.e. per signal symbol) for each sentence position, articles were divided into a training set (95% of all stories) for training language models and a test set (the remaining 5%) for analysis (see Table 1 for details). Out-of-context entropy per word was estimated by computing the average log probability of sentences at that position, normali</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Nat Lang Eng, 11:207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<date>1935</date>
<journal>Psycho-Biology of Languages. Houghton-Mifflin.</journal>
<contexts>
<context position="3042" citStr="Zipf (1935)" startWordPosition="485" endWordPosition="486">r words, if the average number of choices given any linguistic signal exceeds the channel capacity, it cannot be guaranteed that the receiver can correctly infer the originally intended message. Such transmission errors will reduce the efficiency of language communication. Keeping the entropy of linguistic signals below the channel capacity alone is not efficient, for one can devise a code where each signal corresponds to a distinct message. With a unique choice per signal, this encoding achieves an entropy of zero at the cost of requiring a look-up table that is too large to be possible (cf. Zipf (1935), who makes a similar argument for meaning and form). In fact, the most efficient code requires language users to encode messages into signals of the entropy bounded by the capacity of the channel. One implication of this efficient encoding is that over time, the entropy of the signals is constant. One of the first studies to investigate such constancy is Genzel and Charniak (2002), in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per signal symbol is constant across sentence positions in discourses. That is, if we view sentence positions a</context>
</contexts>
<marker>Zipf, 1935</marker>
<rawString>G. K. Zipf. 1935. Psycho-Biology of Languages. Houghton-Mifflin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>