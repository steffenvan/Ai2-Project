<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003151">
<title confidence="0.693479">
Correcting Object-Related Misconceptions:
How Should The System Respond?&apos;
</title>
<author confidence="0.67249">
Kathleen F. McCoy
</author>
<affiliation confidence="0.9967275">
Department of Computer &amp; Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.453277">
Philadelphia, PA 19104
</address>
<sectionHeader confidence="0.885636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801090909091">
This paper describes a computational method for correcting
users&apos; misconceptions concerning the objects modelled by a
computer system. The method involves classifying object-related
misconceptions according to the knowledge-base feature involved
in the incorrect information. For each resulting class sub-types are
identified, according to the structure of the knowledge base, which
indicate what il,formation may he supporting the misconception
and therefore what information to include in the response. Such a
characterizai ion, along with a model of what the user knows,
enables the system to reason in a domain-independent way about
how best to correct the user.
</bodyText>
<sectionHeader confidence="0.997792" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.976655892857143">
A major area of Al research has been the development of
*expert systems&apos; — systems which are able to answer user&apos;s
questions concerning a particular domain. Studies identifying
desirable interaetive capabilities for such systems [Pollack et al. 821
have found that it is not sufficient simply to allow the user to ask
a question and have the system answer it. Users often want to
question the system&apos;s reasoning,to make sure certain constraints
have been taken into consideration, and so on. Thus we must
strive to provide expert systems with the ability to interact with
the user in the kind of cooperative dialogues that we see between
two human conversational partners.
Allowing such interactions between the system and a user
raises difficulties for a Natural-Language system. Since the user is
interacting with a system 3-S s/he would with a human expert, s/he
will most expect the system to brhaVr ass human expert.
Among other things, the user will expect the system to be adhering
to the cooperative principles of conversation [Grice 75, Joshi 821.
If these principles are not followed by the system, the user is likely
to become confused.
In this paper !locus on one aspect of the cooperative
behavior found between two conversational partners: responding to
recognized differences in the beliefs of the two participants. Often
when two people interact, one reveals-a belief or assumption that
is incompatible with the beliefs held by the other. Failure to
correct this disparity may not only implicitly confirm the disparate
belief. but. may even make it impossible to complete the ongoing
task. Imagine the following exchange:
U. Give me the HULL NO of all Destroyers whose
NIAST_HEAcarr is above WO.
E. All Destroyers that I know about have a
MAST_HEIGIIT between 85 and 00. Were you
thinking of the Aircraft-Carriers?
&apos;This work is partially supported by the NSF grant #MCS81-07290.
In this example, the user (U) ha .s apparently confused a Destroyer
with an Aircraft-Carrier. This confusion has caused her to
attribute a property value to Destroyers that they do not have. In
this case a correct answer by the expert (E) of &amp;quot;none• is likely to
confuse U. In order to continue the conversation with a minimal
amount of confusion, the user&apos;s incorrect belief must first be
addressed.
My primary interest is in what an expert system, aspiring to
human expert performance, should include in such responses. In
particular, I sin concerned with system responses to recognized
disparate beliefs/assumptions about objects. In the past this
problem has been left. to the tutoring or (&amp;quot;Al systems [Stevens et
al. 79, Stevens &amp; Collins 80, Brown &amp; Burton 78, SleCIII3D 821,
which attempt to correct student&apos;s misconceptions concerning a
particular domain. For the most part, their approach has been to
list a priori all misconceptions in a given domain. The futility of
this approach is emphasized in [Sleeman t321. In contrast,the
approach taken here is to classify, in a domain independent way,
object-related dc-parities :iccording to the Knowledge Base (KI3)
feature involved. A nuudwr of response strategies are associated
with each resulting class. Deciding which strategy to use for a
given misconception will be determined by analyzing a user model
and the discourse situation.
</bodyText>
<sectionHeader confidence="0.864536" genericHeader="method">
2. What Goes Into a Correction?
</sectionHeader>
<bodyText confidence="0.86752">
in this work I am making the following assumptions:
</bodyText>
<listItem confidence="0.924426">
• For the purposes of the initial correction attempt, the
system is assumed to have complet, and correct
knowledge of the domain. That is. the system will
initially perceive a disparity as a misconception on the
part of the user It will thus attempt to bring the
user&apos;s beliefs into line with its own.
• The system&apos;s KB includes the following fozturcs: an
object taxonomy, knowledge of object attributes and
their possible values, and information about possible
relationships between objects.
• The user&apos;s KII contains similar features. However,
much of the inforanal ion (content) in the system&apos;s KB
may he missing from the user&apos;s KB (e.g., the user&apos;s KB
may be sparser of coarser than the system&apos;s KB, or
various all of es ncepts may be missing Iron, the
user&apos;s KB). In addition. some inf‘trrnation in the u..er&apos;s
KB may he wrong. In this work, to say that the user&apos;s
KB is wrong means that it. is irww1.1i.itent with the
</listItem>
<bodyText confidence="0.9572445">
system &apos;.3 1&lt;13 (e.g., things may be clat•sified differently,
properties attributed differently, and so on).
</bodyText>
<page confidence="0.995674">
444
</page>
<listItem confidence="0.9352845">
• White the system may not know exactly what is
conlained in the user&apos;s KB, information about the user
can b, dei.ivcd from two sources. First, the system can
have a model of a canonical user. (Of course this
model may turn out to differ from any given user&apos;s
model.) Secondly, it can derive knowledge about what
the user knows ;Niro the ongoing diseliurse. This later
type of k tic cccledge constitutes what the system discerns
to be the mutual belies of the system and user as
defined iii Poshi 821. These two sources of information
together coast it 111 e the system&apos;s model of the user&apos;s
KB. This model itself may be incomplete and/or
incorrect with respect to the system&apos;s KB.
• A user&apos;s utterance reflects either the state of his/her
KB, or !&apos;.01111 re:tsonieg s/be has just 4011C to fill in
some mi..sing part of Coat KB, or both.
</listItem>
<bodyText confidence="0.999859326530612">
Given these assumNions, we can eonsider what should be
included in a response to an object-related disparity. If a person
exhibits what his/her C0111&amp;quot;4 rsational partner perceives as a
misconception, the very least one would expect from that partner
is to deny the false information2 - for example -
U. I thought a whale was a fish.
S. It&apos;s not.
Transcripts of •nat urally oecurring• expert systems show that
experts often include more information in their response than a
simpl, &amp;Mal. The expert may provide an alternative true
statement (e.g., •Whaies are marnmals•). S/he may offer
justification and/or simport for the correction (e.g., Whales are
nommod, because inoy breathe through lungs and feed their young
with milk.). S/he may also r..fute the faulty reasoning s/he
thought the user had done to arrive at the in (e.g.,
&apos;I laving fins and :king in the water is not enough to make a whale
a fish.). This behavior can be characterized as confirming the
correct information which may have led the user to the wrong
conclusion, but indicating why the false conclusion does not follow
by bringing in additional, overriding information.3
The problem for a computer system is to decide what kind of
information nosy he supporting a given misconception. What
things rimy he relevant? What fault). reasoning may have been
done?
I characterize object-related misconceptions in terms of the
lull 1o:fur( involved. Misclassifying an object, &apos;I thought a whale
was a fish, involves the superordinate KB feature. Giving an
object a property it doff: not have, &apos;What is the interest rate on
this stock, involve: the attribute KB feature. This
characterization is helpful in determining, in terms of the structure
of a KB, what information may be supporting a particular
misconception. Thus, it is helpful in determining what to include
in the response.
2Throughout this work I am assuming that the misconeeption is important
to the task at hand and should therefore be corrected. The responses I am
interested in generating at, the &apos;lull blown respoeses. If a misccnception is
detected which is nM important to the task at hand, it is roneeivable that
either the eliscomslition be ignored or a •trimmed• version of one of those
reqonses tic given.
3•r%t: exhibited by the v ru.. ey.perts is very similar to the &amp;quot;grain
of truth.&apos; correction found ir tumring situations as it. le ified in [Woolf &amp;
McDonald 5t. This ,trategy first identifies the grair. ,,f truth in a student&apos;s
answer and then gees on to give 0, corm, I idea or.
itt the following sections I will discuss the two classes of
object misconeeptions just mentioned: superordinate
miseonceptions and attribute misconceptions. Examples of these
classes along with correction strategies will be given. In addition,
indications of how a system might choose a particular strategy will
be investigated.
</bodyText>
<sectionHeader confidence="0.946443" genericHeader="method">
3. Superordinate Misconceptions
</sectionHeader>
<bodyText confidence="0.998586897959184">
Sitise the information that human experts include in their
response to a saperordinate misconception seems to hinge on the
expert&apos;s perception or win- the miseoneeption occurred or what
information may have been supporting the misconception, I have
sub-categorized simerordinate misconceptions according to the kind
of support they hat e. For each type (sub-category) of
supermilinate misconception, I have identified information that.
would he relevallt to the correction.
In this analysis of superordinate misconceptions, I am
assuming that the user&apos;s knowledge about the superordinate
concept is correct. &apos;The user therefore arrives at the misconception
because of his/her incomplete understanding of the object. I am
also, for the moment, ignoring misconceptions that occur because
two objects have similar names.
Given these restrictions, I found three major correction
strategies used by human experts. These correspond to three
reasons why a user might misclassify an object:
TYPE ONE - Object Shares Many Properties with Posited
Supefordinate - This may cause the user wrongly to conclude that
these shared attributes are inherited from the superordinate. This
type of misconception is illustrated by an example involving a
student and 3 teacher:4
U. I thought a whale was a fish.
E. No, it&apos;s a mammal. Although it has fins and lives in the
water, it&apos;s a mammal since it is warm blooded and
feeds its young with milk.
Notice the expert not only specifies the correct superordinate, but
also gives additional information to justify the correcjon. She
do,s this by acknowledging that there are some properties that
whales share with fish which may lead the student to conclude that
a whale is a fish. At the same time she indicates that. these
properties are not sufficient. for inclusion in the class of fish. The
whale, in fact, has other properties which define it to be a
mammal.
Thus, the strategy the expert uses when s/he perceives the
misconception to be of TYPE ONE may be characterized as: (I)
Deny the posited superordinate and indicate the correct one, (1)
State attributes (properties) that the object has in common with
Lim posited superordinate, (3) State defining attributes of the real
superordinate, thus giving evidence/justification for the correct
classification. The system may follow this strategy when the user
mod, I indicates that the User thinks the posited superordinate and
the object are similar because they share many common properties
(not held by the real superordinate).
TypE TWO - Object Shares Properties with Another Object
which is a Member of Posited Snperordinate - In this case the
!Although the analysis gis en her. snot derived through studying actual
human interactions, the examples given are simply illustrative and have not
been extracted from a real interaction.
</bodyText>
<page confidence="0.996568">
445
</page>
<bodyText confidence="0.9986714">
misclassified object and the other object* are similar because they
have some other common superordinate. The properties that they
share are not those inherited front the posited superordinate; but
those inherited from this other common superordinate. Figure
3-1 shows a representation of this situation. OBJECT and
OTIIER-OBJECT have many common properties because they
share a common superordinate (COMMON-SUPEPORDINATE).
Hence, if the user knows that OTHER-OBJECT is 3 member of
the POSITED SUPERORDINATE, s/he may wrongly conclude
that OBJECT is also a member of POSITED SUPERORDINATE.
</bodyText>
<figureCaption confidence="0.95208">
Figure 3-1: TYPE TWO Superordinate Misconception
</figureCaption>
<bodyText confidence="0.979985596774193">
For example, imagine the following exchange taking place Pt
a junior high schtad biology class (here U is a stud ent, E
teacher):
U. I thought. a tomato was a vegetable.
E. No it&apos;s a fruit. You may think it&apos;s a vegetable since
you grow tomatoes in your vegetable garden :dons
with the lettuce and green beans. However, it&apos;s a fruit
because it&apos;s really the ripened ovary of a seed plant.
Here it is important. for the student to understand about plants.
Thus, the teacher denies the posited superordinate, vegetable, and
gives the correct one, fruit. She backs this up by refuting evidence
that the student may be using to support the misconception. Iii
this case, the stud( nt !nay wrongly believe that tomatoes are
vegetables because they are like some other objects which are
vegetables. lettuce and green beans, in that all three share the
common superordinate: plants grown in vegetable garden. The
teacher acknowledges this similarity but refutes the conclusion that
tomatoes are vegetables by giving the property of tomatoes which
define them to be fruits.
The correction strategy used in this case was: (1) Deny the
classification posited by the user and indicate the correct
classification. (2) Cite the other members of the posited
superordinate that the eser may be either confusing with the
object being discussed or making a bad analogy from. (3) Give the
features which distingeish the correct. and posited superordinates
thus justifying the classification. A system may follow tHs
strategy if a structure like that in figure 3-1 is found in the user
model.
TYPE THREE - Wrong Information - The user either has
been told wrong information and has not done any reasoning to
justify it, or has misclassified the object in response to some
complex reasoning process that the system can&apos;t duplicate. In this
kind of situation, the system, just like a human expert, can only
correct the wrong information, give the corresponding true
information, and possibly give some defining features
distinguishing the posited and actual superordinates. If this
correction does not satisfy the user, it is up to him/her to continue
the interaction until the underlying misconception is &apos;bared up
(see (Jefferson 721).
The information included in this kind of response is similar
to that which McKeown&apos;s TEXT system, which answers questions
about database structure [McKeown 821, would include if the user
had asked about the difference between two entities. In her case,
the information included would depend on how similar the two
objects were according to the cistern KB, not on a model of what
the user knows or why the user might be asking the question.5
U. Is a debenture a secured bond?
S. No it&apos;s an unsecured bond - it has nothing backing it
should the issuing company default.
AND
U. Is the whiskey a missile?
S. No, it&apos;s a submarine which is an underwater vehicle
(not a destructive device).
The strategy followed in these cases can be characterized as:
(1) Deny posited superordinate and give correct one, (2) Give
additional information as needed. This -xtra information may
include defining features of the correct superordinate or
information about the highest superordinate that distinguishes the
object from the posited superordinate. This strategy may be
followed by the system when there is insufficient evidence in the
user model for concluding that either a TYPE ONE or a TYPE
TWO misconception has occurred.
</bodyText>
<sectionHeader confidence="0.926797" genericHeader="method">
4. Attribute Misconceptions
</sectionHeader>
<bodyText confidence="0.986328583333333">
A second class of misconception occurs when a person
wrongly attributes a property to an object. There are at least
three reasons why this kind of misconception may occur.
TYPE ONE - Wrong Object - The user is either confusing
the object being discussed with another object that has the
specified property, or s/he is making a bad analogy using a similar
object. In either case the F.CCOnd object should be included in the
corner ti011 SO the problem does not too itint&apos;.
In the following example the expert assumes the user is
confusing the object with a similar objeet.
U. I have my money in a money market certificate so I
can get to it right away.
E. But you can&apos;t! Your money is tied up in 3 certificate
- do you mean a money market fund?
The strategy followed in this situation can be characterized
as: (1) Deny the wrong information, (2) Give the corresponding
correct information. (3) Mention the object of confusion or possible
analogical reasoning. This strategy can be followed by a system
when there is another objeet which is *close in con, ept&apos;• to I he
object being discussed and a hieh has the property involved in the
misconception. Of course, the perception of how &amp;quot;ekke in
concept• two objects are changes with context. This may be
because some attributes are highlighted in some contexts and
hidden in others. For this reason it is anticipated that a cic,settess
</bodyText>
<footnote confidence="0.941816333333333">
5McKeown does indiente that this kind of infnrnint out would improve her
responses. The major thrust or her work was so test structure; the use of
user model could be easily integrated into her framework.
</footnote>
<table confidence="0.345887">
POSITED
SUPERORDINATE
</table>
<page confidence="0.997064">
446
</page>
<bodyText confidence="0.994821325">
measure such as that described in [Tversky 771, which takes into
account the salience of various attributes, will be useful.
TYPE TWO - Wrong Attribute - The user has confused the
attribute being discussed with another attribute. In this case the
correct attribute should be included in the response along with
additional information concerning the confused attributes (e.g.,
their similarities and differences). In the following example the
similarity of the two attributes, in this case a common function, is
mentioned in the response:
U. Where are the gills on the whale?
S. Whales don&apos;t have gills, they breathe through lungs.
The strategy followed was: (1) Deny attribute given, (2) Give
correct attribute, (3) Bring in similarities/differences of the
attributes which may have led to the confusion. A system may
follow this strategy when a similar attribute can be found.
There may be some difficulty in distinguishing between a
TYPE ONE and a TYPE TWO attribute misconception. In some
situations the user model alone will not be enough to distinguish
the two cases. The use of past immediate focus (see (Sidner 83))
looks to be promising in this case. Heuristics are currently being
worked out for determining the most likely misconception type
based on what kinds of things (e.g., sets of attributes or objects)
have been focused on in the recent past.
TYPE THREE - The user was simply given had information
or has done some complicated reasoning which can not be
duplicated by the system. Just as in the TYPE THREE
superordinate misconception, the system can only respond in a
limited way.
U. I am not working now and my husband has opened a
spousal IRA for us. I understand that if I start
working again, and want to contribute to my own IRA,
that we will have to pay a penalty on anything that
had been in our spousal account.
E. No - There is no penalty. You can split that spousal
one any way you wish. You can have 2000 in each.
Here the strategy is: (1) Deny attribute given, (2) Give correct
attribute. This strategy can be followed by the system when there
is not enough evidence in the user model to conclude that either a
TYPE ONE or a TYPE TWO attribute misconception has
occurred.
</bodyText>
<sectionHeader confidence="0.993347" genericHeader="conclusions">
5. Conclusions
</sectionHeader>
<bodyText confidence="0.979932421052632">
• In this paper I have argued that any Natural-Language
system that allows the user to engage in extended dialogues must
be prepared to handle misconceptions. Through studying various
transcripts of how people correct misconceptions, I found that they
not only correct the wrong information, but often include
additional information to convince the user of the correction
and/or refute the reasoning that may have led to the
misconception. This paper describes a framework for allowing a
computer system to mimic this behavior.
The approach taken here is first to classify object-related
misconceptions according to the KB feature involved. For each
resulting class, sub-types are identified in terms of the structure of
a KB rather than its content. The sub-types characterize the kind
of information that may support the misconception. A correction
strategy is associated with each sub-type that indicates what kind
of information to include in the response. Finally, algorithms are
being developed for identifying the type of a particular
misconception based on a user model and a model of the discourse
situation.
</bodyText>
<sectionHeader confidence="0.996779" genericHeader="acknowledgments">
6. Acknowledgements
</sectionHeader>
<bodyText confidence="0.758059">
I would like to thank Julia Hirschberg, Aravind Joshi,
Martha Pollack, and Bonnie Webber for their many helpful
comments concerning this work.
</bodyText>
<sectionHeader confidence="0.712192" genericHeader="references">
7. References
</sectionHeader>
<reference confidence="0.9996926">
[Brown &amp;r. Burton 781
Brown, J.S. and Burton, R.R. Diagnostic Models
for Procedural Bugs in Basic Mathematical Skills. Cognitive
Science 2(2)155-192, 1978.
(Grice 75) Grice, H. P. Logic and Conversation. In P. Cole
and J. L. Morgan (editor), Syntax and Semantics III: Speech
Acts, pages 41-58. Academic Press, N.Y., 1975.
[Jefferson 72) Jefferson, G. Side Sequences. In David Sudnow
(editor), Studies in Social Interaction, . Macmillan, New York,
1972.
[Joshi 821 Joshi, A. K. Mutual Beliefs in Question-Answer
Systems. In N. Smith (editor), Mutual Beliefs, . Academic Press,
N.Y., 1982.
&apos;McKeown 82) McKeown, K.. Generating Natural Language
Text in Response to Questions About Database Structure. PhD
thesis, University of Pennsylvania, May, 1982.
(Pollack et al. 82)
Pollack, M., Hirschberg, J., le Webber, B. User
Participation in the Reasoning Processes of Expert Systems. In
Proceedings of the 1982 National Conference on Artificial
Intelligence. AAAI, Pittsburgh, Pa., August, 1982.
[Sidner 831 Sidner, C. L. Focusing in the Comprehension of
Definite Anaphora. In Michael Brady and Robert Berwick (editor),
Computational Models of Discourse, pages 267-330. MIT Press,
Cambridge, Ma, 1983.
(Sleeman 82) Sleeman, D. Inferring (Mal) Rules From Pupil&apos;s
Protocols. In Proceedings of ECAI-82, pages 160-164. ECAI-82,
Orsay, France, 1982.
[Stevens &amp; Collins 80)
Stevens, A.L. and Collins, A. Multiple Conceptual
Models of a Complex System. In Richard E. Snow, Pat-Anthony
Federico and William E. Montague (editor), Aptitude, Learning,
and Instruction, pages 177-197. Erlbaum, Hillsdale, N.J., 1980.
[Stevens et al. 79)
Stevens, A., Collins, A. and Goldin, S.E.
Misconceptions in Student&apos;s Understanding. Intl. J. Man-Machine
Studies 11:145-156, 1979.
(Tversky 77) Tversky, A. Features of Similarity. Psychological
Review 84:327-352, 1977.
(Woolf &amp; McDonald 83)
Woolf, B. and McDonald, D. Human-Computer
Discourse in the Design of a PASCAL Tutor. In Ann Janda
(editor), CHP88 Conference Proceedings - Human Factors in
Computing Systems, pages 230-234. ACM SIGCHI/ HFS, Boston,
Ma., December, 1983.
</reference>
<page confidence="0.998612">
447
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.990483">
<title confidence="0.9986115">Correcting Object-Related Misconceptions: How Should The System Respond?&apos;</title>
<author confidence="0.999972">Kathleen F McCoy</author>
<affiliation confidence="0.999906">Department of Computer &amp; Information Science University of Pennsylvania</affiliation>
<address confidence="0.999626">Philadelphia, PA 19104</address>
<abstract confidence="0.99947725">This paper describes a computational method for correcting users&apos; misconceptions concerning the objects modelled by a computer system. The method involves classifying object-related misconceptions according to the knowledge-base feature involved in the incorrect information. For each resulting class sub-types are identified, according to the structure of the knowledge base, which what may he supporting the misconception and therefore what information to include in the response. Such a characterizai ion, along with a model of what the user knows, enables the system to reason in a domain-independent way about how best to correct the user.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>1978</date>
<booktitle>Diagnostic Models for Procedural Bugs in Basic Mathematical Skills. Cognitive Science</booktitle>
<pages>2--2</pages>
<marker>1978</marker>
<rawString>[Brown &amp;r. Burton 781 Brown, J.S. and Burton, R.R. Diagnostic Models for Procedural Bugs in Basic Mathematical Skills. Cognitive Science 2(2)155-192, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Logic and Conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics III: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In P. Cole and J. L. Morgan (editor),</editor>
<publisher>Academic Press,</publisher>
<location>N.Y.,</location>
<marker>Grice, 1975</marker>
<rawString>(Grice 75) Grice, H. P. Logic and Conversation. In P. Cole and J. L. Morgan (editor), Syntax and Semantics III: Speech Acts, pages 41-58. Academic Press, N.Y., 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Jefferson</author>
</authors>
<title>Side Sequences.</title>
<date>1972</date>
<booktitle>In David Sudnow (editor), Studies in Social Interaction, .</booktitle>
<publisher>Macmillan,</publisher>
<location>New York,</location>
<marker>Jefferson, 1972</marker>
<rawString>[Jefferson 72) Jefferson, G. Side Sequences. In David Sudnow (editor), Studies in Social Interaction, . Macmillan, New York, 1972.</rawString>
</citation>
<citation valid="true">
<title>Mutual Beliefs in Question-Answer Systems.</title>
<date>1982</date>
<editor>In N. Smith (editor), Mutual Beliefs, .</editor>
<publisher>Academic Press,</publisher>
<location>N.Y.,</location>
<marker>1982</marker>
<rawString>[Joshi 821 Joshi, A. K. Mutual Beliefs in Question-Answer Systems. In N. Smith (editor), Mutual Beliefs, . Academic Press, N.Y., 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Generating Natural Language Text in Response to Questions About Database Structure.</title>
<date>1982</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<marker>McKeown, 1982</marker>
<rawString>&apos;McKeown 82) McKeown, K.. Generating Natural Language Text in Response to Questions About Database Structure. PhD thesis, University of Pennsylvania, May, 1982.</rawString>
</citation>
<citation valid="true">
<title>User Participation in the Reasoning Processes of Expert Systems.</title>
<date>1982</date>
<booktitle>In Proceedings of the 1982 National Conference on Artificial Intelligence. AAAI,</booktitle>
<location>Pittsburgh, Pa.,</location>
<marker>1982</marker>
<rawString>(Pollack et al. 82) Pollack, M., Hirschberg, J., le Webber, B. User Participation in the Reasoning Processes of Expert Systems. In Proceedings of the 1982 National Conference on Artificial Intelligence. AAAI, Pittsburgh, Pa., August, 1982.</rawString>
</citation>
<citation valid="true">
<title>Focusing in the Comprehension of Definite Anaphora.</title>
<date>1983</date>
<booktitle>In Michael Brady and Robert Berwick (editor), Computational Models of Discourse,</booktitle>
<pages>267--330</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Ma,</location>
<marker>1983</marker>
<rawString>[Sidner 831 Sidner, C. L. Focusing in the Comprehension of Definite Anaphora. In Michael Brady and Robert Berwick (editor), Computational Models of Discourse, pages 267-330. MIT Press, Cambridge, Ma, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sleeman</author>
</authors>
<title>Inferring (Mal) Rules From Pupil&apos;s Protocols.</title>
<date>1982</date>
<booktitle>In Proceedings of ECAI-82,</booktitle>
<pages>160--164</pages>
<location>Orsay, France,</location>
<marker>Sleeman, 1982</marker>
<rawString>(Sleeman 82) Sleeman, D. Inferring (Mal) Rules From Pupil&apos;s Protocols. In Proceedings of ECAI-82, pages 160-164. ECAI-82, Orsay, France, 1982.</rawString>
</citation>
<citation valid="false">
<journal>Stevens &amp; Collins</journal>
<volume>80</volume>
<marker></marker>
<rawString>[Stevens &amp; Collins 80)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Stevens</author>
<author>A Collins</author>
</authors>
<title>Multiple Conceptual Models of a Complex System.</title>
<date>1980</date>
<booktitle>Aptitude, Learning, and Instruction,</booktitle>
<pages>177--197</pages>
<editor>In Richard E. Snow, Pat-Anthony Federico and William E. Montague (editor),</editor>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, N.J.,</location>
<marker>Stevens, Collins, 1980</marker>
<rawString>Stevens, A.L. and Collins, A. Multiple Conceptual Models of a Complex System. In Richard E. Snow, Pat-Anthony Federico and William E. Montague (editor), Aptitude, Learning, and Instruction, pages 177-197. Erlbaum, Hillsdale, N.J., 1980.</rawString>
</citation>
<citation valid="false">
<pages>79</pages>
<marker></marker>
<rawString>[Stevens et al. 79)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stevens</author>
<author>A Collins</author>
<author>S E Goldin</author>
</authors>
<date>1979</date>
<journal>Misconceptions in Student&apos;s Understanding. Intl. J. Man-Machine Studies</journal>
<pages>11--145</pages>
<marker>Stevens, Collins, Goldin, 1979</marker>
<rawString>Stevens, A., Collins, A. and Goldin, S.E. Misconceptions in Student&apos;s Understanding. Intl. J. Man-Machine Studies 11:145-156, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tversky</author>
</authors>
<title>Features of Similarity. Psychological Review 84:327-352,</title>
<date>1977</date>
<journal>Woolf &amp; McDonald</journal>
<volume>83</volume>
<marker>Tversky, 1977</marker>
<rawString>(Tversky 77) Tversky, A. Features of Similarity. Psychological Review 84:327-352, 1977. (Woolf &amp; McDonald 83)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Woolf</author>
<author>D McDonald</author>
</authors>
<date>1983</date>
<booktitle>Human-Computer Discourse in the Design of</booktitle>
<pages>230--234</pages>
<editor>a PASCAL Tutor. In Ann Janda (editor),</editor>
<publisher>ACM SIGCHI/ HFS,</publisher>
<location>Boston, Ma.,</location>
<marker>Woolf, McDonald, 1983</marker>
<rawString>Woolf, B. and McDonald, D. Human-Computer Discourse in the Design of a PASCAL Tutor. In Ann Janda (editor), CHP88 Conference Proceedings - Human Factors in Computing Systems, pages 230-234. ACM SIGCHI/ HFS, Boston, Ma., December, 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>