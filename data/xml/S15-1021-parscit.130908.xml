<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.9979985">
Reading Between the Lines: Overcoming Data Sparsity for Accurate
Classification of Lexical Relationships
</title>
<author confidence="0.950039">
Silvia Necs¸ulescu Sara Mendes David Jurgens
</author>
<affiliation confidence="0.938725">
Universitat Pompeu Fabra Universidade de Lisboa McGill University
</affiliation>
<address confidence="0.732738">
Barcelona, Spain Lisboa, Portugal Montreal, Canada
</address>
<email confidence="0.996881">
silvia.necsulescu@upf.edu sara.mendes@clul.ul.pt jurgens@cs.mcgill.ca
</email>
<author confidence="0.875724">
N´uria Bel
</author>
<affiliation confidence="0.849475">
Universitat Pompeu Fabra
</affiliation>
<address confidence="0.846539">
Barcelona, Spain
</address>
<email confidence="0.997767">
nuria.bel@upf.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999825882352941">
The lexical semantic relationships between
word pairs are key features for many NLP
tasks. Most approaches for automatically clas-
sifying related word pairs are hindered by data
sparsity because of their need to observe two
words co-occurring in order to detect the lexi-
cal relation holding between them. Even when
mining very large corpora, not every related
word pair co-occurs. Using novel representa-
tions based on graphs and word embeddings,
we present two systems that are able to predict
relations between words, even when these are
never found in the same sentence in a given
corpus. In two experiments, we demonstrate
superior performance of both approaches over
the state of the art, achieving significant gains
in recall.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999386066666667">
Resources containing lexical-semantic relations
such as hypernymy or meronymy have proven use-
ful in many NLP tasks. While resources such as
WordNet (Miller, 1995) contain many general rela-
tions and subsequently have seen widespread adop-
tion, developing this type of rich resource for new
languages or for new domains is prohibitively costly
and time-consuming. Therefore, automated ap-
proaches are needed and, in order to create such
a lexical-semantic database, a first step is to de-
velop accurate techniques for classifying the type of
lexical-semantic relationship between two words.
Approaches to classifying the relationship be-
tween a word pair have typically relied on the as-
sumption that contexts where word pairs co-occur
</bodyText>
<author confidence="0.432188">
Roberto Navigli
</author>
<affiliation confidence="0.478129">
Universit`a “La Sapienza”
Rome, Italy
</affiliation>
<email confidence="0.762397">
navigli@di.uniroma1.it
</email>
<bodyText confidence="0.999849029411765">
will yield information on the semantic relation (if
any) between them. Given a set of example word
pairs having some relation, relation-specific pat-
terns may be automatically acquired from the con-
texts in which these example pairs co-occur (Tur-
ney, 2008b; Mintz et al., 2009). Comparing these
relation-specific patterns with those seen with other
word pairs measures relational similarity, i.e., how
similar is the relation holding between two word
pairs. However, any classification system based on
patterns of co-occurrence is limited to only those
words co-occurring in the data considered; due to
the Zipfian distribution of words, even in a very large
corpus there are always semantically related word
pairs that do not co-occur. As a result, these pattern-
based approaches have a strict upper-bound limit
on the number of instances that they can classify.
As an alternative to requiring co-occurrence, other
works have classified the relation of a word pair us-
ing lexical similarity, i.e., the similarity of the con-
cepts themselves. Given two word pairs, (w1, w2)
and (w3, w4), if w1 is lexically similar to w3 and
w2 to w4 (i.e., are pair-wise similar) then the pairs
are said to have the same semantic relation. These
two sources of information are used as two indepen-
dent units: relational similarity is calculated using
co-occurrence information; lexical similarity is cal-
culated using distributional information (Snow et al.,
2004; S´eaghdha and Copestake, 2009; Herdadelen
and Baroni, 2009), and ultimately these scores are
combined. Experimental evidence has shown that
relational similarity cannot necessarily be revealed
through lexical similarity (Turney, 2006b; Turney,
2008a), and therefore, the issue of how to collect in-
</bodyText>
<page confidence="0.96393">
182
</page>
<note confidence="0.9533945">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999938772727273">
formation for word pairs that do not co-occur is still
an open problem.
We propose two new approaches to representing
word pairs in order to accurately classify them as
instances of lexical-semantic relations – even when
the pair members do not co-occur. The first ap-
proach creates a word pair representation based on
a graph representation of the corpus created with
dependency relations. The graph encodes the dis-
tributional behavior of each word in the pair and
consequently, patterns of co-occurrence expressing
each target relation are extracted from it as relational
information. The second approach uses word em-
beddings which have been shown to preserve linear
regularities among words and pairs of words, there-
fore, encoding lexical and relational similarities (Ba-
roni et al., 2014), a necessary property for our task.
In two experiments comparing with state-of-the-art
pattern-based and embedding-based classifiers (Tur-
ney, 2008b; Zhila et al., 2013), we demonstrate that
our approaches achieve higher accuracy with signif-
icantly increased recall.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999966569444444">
Initial approaches to the extraction of lexical-
semantic relations have relied on hand-crafted
lexico-syntactic patterns to identify instances of
semantic relations (Hearst, 1992; Widdows and
Dorow, 2002; Berland and Charniak, 1999). These
manually designed patterns are explicit construc-
tions expressing a target semantic relation such as
the pattern X is a Y for the relation of hypernymy.
However, these approaches are limited because a re-
lation may be expressed in many ways, depending
on the domain, author, and writing style, which may
not match the originally identified patterns. More-
over, the identification of high-quality patterns is
costly and time-consuming, and must be repeated
for each new relation type, domain and language.
To overcome these limitations, techniques have been
developed for the automatic acquisition of meaning-
ful patterns of co-occurrence cueing a single target
relation (Snow et al., 2004; Girju et al., 2006; Davi-
dov and Rappoport, 2006).
More recent work focuses on methods for the
classification of word pairs as instances of several
relations at once, based on their relational similarity.
This similarity is calculated using a vectorial rep-
resentation for each pair, created by relying on co-
occurrence contexts (Turney, 2008b; S´eaghdha and
Copestake, 2009; Mintz et al., 2009). These repre-
sentations are very sparse due to the scarce contexts
where the members of many word pairs co-occur.
Moreover, many semantically related word pairs do
not co-occur in corpus.
For overcoming these issues, relational similar-
ity was combined with lexical similarity calculated
based on the distributional information of words
(Cederberg and Widdows, 2003; Snow et al., 2004;
Turney, 2006a; S´eaghdha and Copestake, 2009; Her-
dadelen and Baroni, 2009). However, (Turney,
2006b; Turney, 2008a) showed that relational sim-
ilarity cannot be improved using the distributional
similarity of words. In contrast with the previous ap-
proaches that took into account lexical and relational
information as a linear combination of lexical and
relational similarity scores, the present work focuses
on introducing word pair representations that merge
and jointly represent types of information: lexical
and relational. In this way, we aim to reduce vector
sparseness and to increase the classification recall.
As a first approach, we use a graph to model the
distributional behavior of words. Other researchers
used graph-based approaches to model corpus in-
formation for the extraction of co-hyponyms (Wid-
dows and Dorow, 2002), hypernyms (Navigli and
Velardi, 2010) or synonyms (Minkov and Cohen,
2012), or for inducing word senses (Di Marco and
Navigli, 2013). Navigli and Velardi (2010) have
the most similar representation to ours, creating a
graph that models only definitional sentences. In
contrast, our objective is to create a general repre-
sentation of the whole corpus that can be used for
classifying instances of several lexical semantic re-
lations. The second approach presented in this pa-
per, relies on word embeddings to create word pair
representations. Extensive experiments have lever-
aged word embeddings to find general semantic rela-
tions (Mikolov et al., 2013a; Mikolov et al., 2013b;
Mikolov et al., 2013c; Levy and Goldberg, 2014b).
Nevertheless, only one work has applied word em-
beddings for classifying instances of a lexical se-
mantic relation, specifically the relation hyponymy-
hypernymy (Fu et al., 2014). This relation is more
complex than other semantic relations tested and
therefore, it is reflected in more than one offset, de-
</bodyText>
<page confidence="0.998612">
183
</page>
<bodyText confidence="0.99991675">
pending on the domain of each instance. The present
work uses a machine learning approach to discover
meaningful information for the semantic relations
encoded in the dimensions of the embeddings.
</bodyText>
<sectionHeader confidence="0.980759" genericHeader="method">
3 Task description
</sectionHeader>
<bodyText confidence="0.999877153846154">
The goal of this work is to classify word pairs as in-
stances of lexical-semantic relations. Given a set of
target semantic relations R = {r1, ... , rn1, and a
set of word pairs W = {(x, y)1, ... , (x, y)n1, the
task is to label each word pair (x, y)i with the rela-
tion rj E R holding between its members and out-
putting a set of tuples ((x, y)i, rj). For this task, we
propose two novel representations of word pairs (de-
scribed next), which are each used to train a classi-
fier. Following, in Section 3.1 and Section 3.2 we
describe each representation and then, in Section
3.3, we describe the common classification setup
used with both representations.
</bodyText>
<subsectionHeader confidence="0.997543">
3.1 Graph-based Representation Model
</subsectionHeader>
<bodyText confidence="0.999942541666667">
The present section introduces a novel word pair
representation model based on patterns of co-
occurrence contexts, and on a graph-based corpus
representation created with dependency relations. A
word pair is represented as a vector of features set
up with the most meaningful patterns of context and
filled in with information extracted from the graph
representation of the corpus. We refer to systems
trained with these graph-based representations as
Graph-based Classification systEm (GraCE).
The novelty of this system stands in the graph-
based representation. Its main advantage is that all
the dependency relations of a target word, extracted
from different sentences, are incident edges to its
corresponding node in the graph. Thus, words that
never co-occur in the same context in corpus, are
linked in the graph through bridging words: words
that appear in a dependency relation with each mem-
ber of the pair but in different sentences. With this
representation we address the data sparsity issue,
aiming to overcome the reported major bottleneck
of previous approaches: low recall because informa-
tion can only be gathered from co-occurrences in the
same sentence of two related words.
</bodyText>
<listItem confidence="0.962636181818182">
Word pair representations are created in three
steps:
(1). Corpus representation: the input corpus is
represented as a graph;
(2). Feature selection: the input corpus is used to
extract meaningful patterns of co-occurrence
for each semantic relation ri starting from an
initial set of examples E;
(3). Word pair representation: the information
acquired in (1) and (2) is used to create
vectorial representations of target word pairs.
</listItem>
<bodyText confidence="0.887402538461538">
Next, we present an example of how the graph repre-
sentation of the corpus addresses the sparsity prob-
lem in distributional data and formally introduce
each step of the GraCE algorithm.
Example To illustrate the benefit of acquiring in-
formation about a word pair from the graph instead
of using co-occurrence information, let us consider
that, given the sentences (S1) and (S2) below, we
want to classify the pair (chisel, tool) as an instance
of the relation of hypernymy.
(S1) The students learned how to handle screwdrivers,
hammers and other tools.
(S2) The carpenter handles the new chisel.
</bodyText>
<figureCaption confidence="0.986126">
Figure 1: Dependency multigraph built from a two
sentence corpus using GraCE. See text for details.
</figureCaption>
<bodyText confidence="0.999760266666667">
The word pair (chisel, tool) has a relation of hy-
pernymy but its members do not co-occur in the
same sentence. However, both words occur as ob-
jects of the verb to handle in different sentences,
just like other hypernym word pairs such as (ham-
mer, tool) and (screwdriver, tool) which do co-occur
in the same sentence. This shows that handle is one
of the contexts shared between these semantically
related words that provide information regarding a
possible semantic relatedness between them. Lever-
aging only the information provided by each sen-
tence, as existing pattern-based approaches do, no
evidence is acquired regarding the semantic relation
holding between chisel and tool. GraCE combines
the dependency relations seen in each sentence in
</bodyText>
<figure confidence="0.996877055555555">
carpenter
N
studentN
subj
chisel obj handleV
N hammer
obj
obj N
learnV
subj
comp
obj
toolN s crewdriverN
conj
mod
conj
otherJ
subj
</figure>
<page confidence="0.991737">
184
</page>
<bodyText confidence="0.990488212765957">
the graph shown in Figure 1. In this graph, chisel
and tool are connected by a path passing through the
bridging word handle which shows that both chisel
and tool could co-occur in a sentence as objects of
the verb to handle, although they do not in the ex-
ample two-sentence corpus.
Corpus representation The goal of the first step
is to generate a graph connecting semantically asso-
ciated words using observed dependency relations.
Formally, the corpus is represented as a graph
● = (V, E), where V is a set of POS-tagged lem-
mas in a corpus and E is the set of dependency re-
lations connecting two lemmas from V in the cor-
pus. From each parsed sentence of the corpus, a
set of dependency relations linking the words in
it is produced: D = {d1 ..., d|D|}, where d =
(wi, dep, wj) and wi, wj and dep denote POS-
tagged lemmas and a dependency relation, respec-
tively. The graph ● is created using all the depen-
dency relations from D.
The output of this step is a multigraph, where two
words are connected by the set of edges containing
all the dependency relations holding between them
in the corpus.
Feature Selection The goal of the second step
is to collect features associated with each relation
r from the parsed input corpus. Similarly to the
work of Snow et al. (2004), our features are pat-
terns of co-occurrence contexts created with depen-
dency paths. For acquiring patterns of co-occurrence
contexts for each relation r, we use the set of la-
beled examples E, assuming that all the contexts in
which a word pair (x, y)i ∈ E co-occurs provide
information about the relation r holding between
its members. All the dependency paths between x
and y up to three edges are extracted from the de-
pendency graph of each sentence where (x, y)i co-
occur.1 For example, ((hammerN, toolN), hyper)
is an instance of the relation of hypernymy. In
the dependency graph of sentence (S1), the words
hammerN (hyponym) and toolN (hypernym) are
connected by the dependency path hammerN obj
←−−
handleVobj
−−→ toolN. This path is converted into
a pattern of co-occurrence contexts by replacing the
seeds in the path with their parts of speech as fol-
</bodyText>
<footnote confidence="0.976443333333333">
1Paths with more than three edges commonly connect
semantically-unrelated portions of a sentence and therefore are
not beneficial for the purposes of relation classification.
</footnote>
<equation confidence="0.977729307692308">
XN prep such as−1 −−−−−−−−−→toolN mod
−−−→ YJ
XN obj−1
−−−−→ useV obj
−−→ YN
XN obj−1
−−−−→ useV conj
−−−→ YV
XN prep such as−1 −−−−−−−−−→ toolN −
con&amp;quot; YN
XN nn−1
−−−−→ bladeNconj
−−−→ YN
</equation>
<tableCaption confidence="0.922597">
Table 1: Examples of relation features
lows: N obj
</tableCaption>
<bodyText confidence="0.979555740740741">
←−− handleV obj
−−→ N. Table 1 illustrates
several examples of pattern of co-occurrence con-
texts.
For the word pairs vectorial representation, the
top 5000 most meaningful patterns are considered in
the final set of patterns P to form a feature space.2
In order to rank the patterns, the tf-idf score is cal-
culated for each pattern with respect to each lexi-
cal semantic relation. Here, tf − idf is defined as
maxj(log(uniq(pi,rj)+1)∗|R |), where pi is a pattern of
|Rp|
co-occurrence, uniq(pi, rj) is the number of unique
instances of the relation rj occurring in the pattern
pi and |Rp |is the number of relations rj whose ex-
ample instances are seen occurring in the pattern pi.
Each pattern is then associated with the highest tf-idf
score obtained across all relations.
Word pair representations Using the graph
model ● and the set of contextual patterns auto-
matically acquired P, each word pair (x, y) is rep-
resented as a binary distribution over each pattern
from P. Rather than using the input corpus to iden-
tify contexts of occurrence for the word pair (x, y)
and match those with the acquired patterns, GraCE
uses paths connecting x and y in ●. All the paths be-
tween x and y up to three edges are extracted from
</bodyText>
<listItem confidence="0.782239111111111">
●. These paths are then matched against the fea-
ture patterns from P and the word pair (x, y) is rep-
resented as a binary vector encoding non-zero val-
ues for all the features matching the pair’s paths
extracted from ●, and zero otherwise.3 Because
the graph contains combinations of multiple depen-
dency relations, extracted from various sentences,
paths not observed in the corpus can be found in the
graph.
</listItem>
<footnote confidence="0.606315">
2Initial experiments tested different amounts of patterns us-
ing held out data and the best results were obtained with the top
5000 patterns.
3Binary weights are used because the feature values are de-
rived observing paths in the graph, which is a generalization of
the corpus; because not all paths in the graph are observed in the
corpus, weighting based on path frequency would encounter the
same data sparsity issue that the graph is intended to overcome.
</footnote>
<figure confidence="0.8246772">
attribute
co-hyponymy
action
hypernymy
meronymy
</figure>
<page confidence="0.958857">
185
</page>
<subsectionHeader confidence="0.994289">
3.2 Word Embeddings Representations
</subsectionHeader>
<bodyText confidence="0.998321032786885">
The present section introduces two word pair repre-
sentations based on word embeddings. We refer to a
system based on embeddings as Word Embeddings
Classification systEm (WECE). An embedding is a
low-dimensional vectorial representation of a word,
where the dimensions are latent continuous features
and vector components are set to maximize the prob-
ability of the contexts in which the target word tends
to appear. Since similar words occur in similar
contexts the word embeddings learn similar vec-
tors for similar words. Moreover, the vector offset
of two word embeddings reflect the relation hold-
ing between them. For instance, Mikolov et al.
(2013c) give the example that v(king) − v(man) ≈
v(queen) − v(women), where v(x) is the embed-
ding of the word x, indicating the vectors are encod-
ing information on the words’ semantic roles.
For learning word embeddings, we used the Skip-
gram model, improved with techniques of negative
sampling and subsampling of frequent words, which
achieved the best results for detecting semantically
similar words (Mikolov et al., 2013a; Mikolov et
al., 2013b). Moreover, for a fair comparison with
the GraCE system, developed with dependency re-
lations, we also tested the results obtained with
a dependency-based Skip-gram model (Levy and
Goldberg, 2014a). Words occurring only once in
corpus are filtered out and 200-dimensional vectors
are learned.
Two embedding-based representations are consid-
ered for a relation: WECEoffset leverages the offset
of the word embeddings, while WECEconcat concate-
nates the embeddings, both described next.
WECEoffset Representation Mikolov et al.
(2013c) shows that the vectorial representation
of words provided by word embeddings captures
syntactic and semantic regularities and that each
relationship is characterized by a relation specific
vector offset. Word pairs with similar offsets
can be interpreted as word pairs with the same
semantic relation. Therefore, given a target word
pair (x, y), the vectorial representation is calcu-
lated from the difference between its vectors, i.e.,
v((x, y)) = v(x) − v(y). Note that this operation
is dependent on the order of the arguments and is
therefore potentially able to capture asymmetric
relationships.
WECEconcat Representation A novel word pair
representation is proposed to test if the information
encoded directly in the embeddings reflects the se-
mantic relation of the word pair.
A word pair is represented by concatenating the
vectorial representation of its members. Formally,
given a word pair (x, y), whose members vecto-
rial representations are v(x) = (x1, x2,... , xn),
and v(y) = (y1, y2,. .. , yn) respectively, the vec-
torial representation of (x, y) is defined as the
concatenation of v(x) and v(y): v((x, y)) =
(x1, x2, . . . , xn, y1, y2, . . . , yn) Consequently the
length of v((x, y)) is 2n, where n is the dimension
of the embedding space.
</bodyText>
<subsectionHeader confidence="0.98124">
3.3 Relation Classification
</subsectionHeader>
<bodyText confidence="0.999992538461539">
For both representations, a supervised classifier is
trained. Given a set of tuples E = ((x, y)i, ri)
of example instances for each relation ri E R, a
support vector machine (SVM) multi-class classifier
with a radial basis function kernel (Platt, 1999) is
trained using WEKA (Hall et al., 2009) to classify
each word pair based on its representation provided
by a graph-based representation model (Section 3.1)
or a word embeddings representation model (Sec-
tion 3.2) for N different lexical relations. The SVM
classifier generates a distribution over relation labels
and the highest weighted label is selected as the rela-
tion holding between the members of the word pair.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999466">
While several datasets have been created for detect-
ing semantic relations between two words in con-
text (Hendrickx et al., 2010; Segura-Bedmar et al.,
2013), in our work we focus on the classification of
word pairs as instances of lexical-semantic relations
out of context. The performance of the GraCE and
WECE systems is tested across two datasets, focus-
ing on their ability to classify instances of specific
lexical-semantic relations as well as to provide in-
sights into the systems’ generalization capabilities.
</bodyText>
<subsectionHeader confidence="0.949259">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9987515">
Corpora Many pattern-based systems increase the
size of the input corpus in an attempt to overcome
data sparsity and to achieve a better recall. There-
fore, in our experiments, we train our systems using
</bodyText>
<page confidence="0.995054">
186
</page>
<bodyText confidence="0.999553333333333">
two corpora of different sizes: the British National
Corpus (BNC), a 100 million-word corpus, and a
Wikipedia dump created from 5 million pages and
containing 1.5 billion words. The size difference al-
lows us to measure the potential impact of increased
word co-occurrence on recall. Both corpora were
initially parsed with the Stanford dependency parser
in the collapsed dependency format (Manning et al.,
2014).
Embbedings WECEoffset and WECEconcat are
implemented based on a bag-of-words (BoW)
(Mikolov et al., 2013a) and based on dependency
relations (Dep) (Levy and Goldberg, 2014a).
Evaluation We compare each system by reporting
precision (P), recall (R) and F1 measure (F).
</bodyText>
<subsectionHeader confidence="0.992993">
4.2 Comparison Systems
</subsectionHeader>
<bodyText confidence="0.999966347826087">
The two proposed models are compared with two
state-of-the-art systems and one baseline system.
PAIRCLASS The PairClass algorithm (Turney,
2008b) provides a state-of-the-art pattern-based ap-
proach for extracting and classifying the relation-
ship between word pairs and has performed well for
many relation types. Using a set of seed pairs (x, y)
for each relation, PairClass acquires a set of lexical
patterns using the template [0 to 1 words] x [0 to 3
words] y [0 to 1 words]. Using the initial set of lex-
ical patterns extracted from a corpus, additional pat-
terns are generated by optionally generalizing each
word to its part of speech. For N seed pairs, the most
frequent kN patterns are retained. We follow Tur-
ney (2008b) and set k = 20. The patterns retained
are then used as features to train an SVM classifier
over the set of possible relation types.
DSZhila &amp; DSLevy Word embeddings have previ-
ously been shown to accurately measure relational
similarity; Zhila et al. (2013) demonstrate state-of-
the-art performance on SemEval-2012 Task 2 (Jur-
gens et al., 2012) which measures word pair similar-
ity within a particular semantic relation (i.e., which
pairs are most prototypical of a semantic relation).
This approach can easily be extended to the clas-
sification setting: Given a target word pair (x, y),
the similarity is computed between (x, y) and each
word pair (x, y)i of a target relation r. The av-
erage of these similarity measurements was taken
as the final score for each relation r.4 Finally, the
word pair is classified as an instance of the rela-
tion with the highest associated score. Two types
of embeddings are used, (a) the word embeddings
produced using the method of Mikolov et al. (2011),
which was originally used in Zhila et al. (2013) and
(b) the embeddings using the method of Levy and
Goldberg (2014a), which include dependency pars-
ing information. We refer to these as DSZhila and
DSLevy, respectively. The inclusion of this sys-
tem enables comparing the performance impact of
using an SVM classifier with our embedding-based
pair representations versus classifying instances by
comparing the embeddings themselves. We note a
DS system represents a minimally-supervised sys-
tem whose features are produced in an unsupervised
way (i.e., through the embedding process) and are
therefore not necessarily tuned for the task of rela-
tion classification; however, such embeddings have
previously been shown to yield state-of-the-art per-
formance in other semantic relation tasks (Baroni et
al., 2014) and therefore the DS systems are intended
to identify potential benefits when adding feature se-
lection by means of the SVM in WECE systems.
BASELINE The purported benefit of the GraCE
model is that the graph enables identifying syntac-
tic features between pair members that are never ob-
served in the corpus, which increases the number of
instances that can be classified without sacrificing
accuracy. Therefore, to quantify the effect of the
graph, we include a baseline system, denoted BL,
that uses an identical setup to GraCE but where the
feature vector for a word pair is created only from
the dependency path features that were observed in
the corpus (as opposed to the graph). Unlike the
GraCE model which has binary weighting (due to
the graph properties), the baseline model’s feature
values correspond to the frequencies with which pat-
terns occur; following common practice, the values
are log-normalized.
</bodyText>
<subsectionHeader confidence="0.991725">
4.3 Experiment 1
</subsectionHeader>
<bodyText confidence="0.861805666666667">
Both of the proposed approaches rest on the hypoth-
esis that the graph or embeddings can enable accu-
rate pair classification, even when pairs never co-
4Additional experiments showed that using alternate ways
of measuring similarity, such as using the maximum similarity
for any instance of r, attained similar results.
</bodyText>
<page confidence="0.990056">
187
</page>
<table confidence="0.99907725">
Domain #Co-hypo #Hyper #Mero
Animals 8038 (92.4%) 3039 (97.2%) 386 (89.1%)
Plants 18972 (95.5%) 1185 (97.4%) 330 (82.4%)
Vehicles 530 (82.6%) 189 (97.9%) 455 (100%)
</table>
<tableCaption confidence="0.9843395">
Table 2: Distribution of K&amp;H dataset, with the % of
instances which occur in the corpora.
</tableCaption>
<table confidence="0.999940928571429">
P BNC F Wikipedia
R P R F
PairClass 76.9 4.6 8.7 77.0 11.7 20.4
BL 82.6 7.7 14.2 89.4 16.2 27.5
GraCE 90.7 43.8 59.0 94.0 75.5 83.7
DSZhila 31.6 15.7 21.0 32.8 22.6 26.8
DSLevy 18.7 11.4 14.2 27.7 15.6 20.0
WECEBoW 96.0 59.1 73.1 96.8 87.7 92.0
offset
WECEBoW 97.4 60.0 74.2 97.6 89.3 93.2
concat
WECEDep offset 87.9 63.1 73.5 95.4 86.1 90.5
WECEDep 93.1 64.7 76.4 96.7 88.4 92.4
concat
</table>
<tableCaption confidence="0.995894">
Table 3: Aggregated results obtained for the in-
</tableCaption>
<bodyText confidence="0.982230888888889">
domain setup with the K&amp;H dataset. Detailed results
are presented in the Appendix A.
occur in text. Therefore, in the first experiment,
we test whether the recall of classification systems
is improved when the word pair representation en-
codes information about lexical and relational sim-
ilarity. As an evaluation dataset, we expand on the
dataset of Kozareva and Hovy (2010) (K&amp;H), which
was collected from hyponym-hypernym instances
from WordNet (Miller, 1995) spanning three topi-
cal domains: animals, plants and vehicles. Because
our systems are capable of classifying instances with
more than one relation at once, we enhance this
dataset with instances of two more relation types:
co-hyponymy and meronymy. Co-hyponyms are ex-
tracted directly from the K&amp;H dataset: two words
are co-hyponyms if they have the same direct ances-
tor.5 To avoid including generic nouns, such as “mi-
grator” in the “animal” domain, only leaf nodes are
considered. The meronym instances are extracted
directly from WordNet. The final dataset excludes
multi-word expressions, which were not easily han-
dled by any of the tested systems. The total number
of instances considered in our experiments is pre-
sented in Table 2.
Results Table 3 presents the average of the results
obtained by the systems when tested in-domain in
</bodyText>
<footnote confidence="0.9768365">
5y is a direct ancestor of x if there is no other word z which
is hypernym of x and hyponym of y.
</footnote>
<bodyText confidence="0.999044041666667">
a 10-fold cross-validation setup. For the in-domain
setup, only instances from one domain are used for
training and testing.
As expected, all the systems gain recall with a
larger corpus, like Wikipedia, showing that the recall
depends on the size of that corpus when a system ac-
quires its distributional information directly from the
input corpus and thus is dependent on the word pairs
co-occurring. Indeed, in the BNC, only 19.4% of the
K&amp;H instances never co-occur, while in Wikipedia
–a corpus 15 times larger than BNC– the number
of co-occurrences rises only to 30.7%, demonstrat-
ing the challenge of classifying such pairs. There-
fore, the real upper-bound limit for these types of
systems is the amount of word pairs co-occurring in
the same sentence in the corpus. The recall achieved
by GraCE overcomes this limitation of pattern-based
systems: 40% and 78.7% of the instances that never
co-occur in BNC and in Wikipedia, respectively, are
correctly classified by GraCE. This ability causes
GraCE to improve the BL performance by 8.1 points
in precision and 36.1 points in recall on BNC and 4.6
points in precision and 59.3 in recall on Wikipedia.
Given that the BL system is constructed identically
to GraCE but without using a graph, these results
demonstrate the performance benefit of joining the
distributional information of a corpus into a graph-
based corpus representation.
Analyzing the false negatives of the GraCE clas-
sifier, we observe that even relying on a graph-
based corpus representation to extract the distribu-
tional information of a word pair, many errors are
still caused by the sparsity of their vectorial repre-
sentation. For the word pairs that do not co-occur in
the same sentence, the GraCE vector representations
of correctly-classified pairs have a median of eight
non-zero features, indicating that the graph was ben-
eficial for still providing evidence of a relationship;
in contast, incorrectly-classified pairs had a median
of only three non-zero features, suggesting that data
sparisity is still major contributor to classification er-
ror.
By combining all the distributional information
into a denser vector, WECE systems are able to im-
prove upon GraCE’s results by an average of 2.9
points in precision and 17.9 points in recall. WECE
results see an increase by 62 points in precision and
46 in recall over DSZhila which used the same em-
</bodyText>
<page confidence="0.996893">
188
</page>
<bodyText confidence="0.999966454545454">
beddings, highlighting the importance of the SVM
classifier for learning which features of the embed-
dings reflect the lexical relation. Although embed-
dings have been argued to reflect the semantic or
syntactic relations between two words (Mikolov et
al., 2013c), our results suggest that additional ma-
chine learning (as done with WECEoffset) is needed
to identify which dimensions of the embeddings ac-
curately correspond to specific relationships. Be-
tween the WECE systems, WECEconcat achieves
slightly better results on the K&amp;H dataset.
</bodyText>
<subsectionHeader confidence="0.983676">
4.4 Experiment 2
</subsectionHeader>
<bodyText confidence="0.999950285714286">
In the first experiment, the proposed systems were
compared to test the importance of having a repre-
sentation that includes information about lexical and
relational similarities for the classifier to generalize
and to gain recall. Therefore, as further validation, a
second experiment is carried out, where the systems
have to classify word pairs from a different domain
than the domains in the training set. The objective is
to assess the importance of the domain-aware train-
ing instances for the classification.
The K&amp;H dataset contains only instances from
three domains and is imbalanced between the num-
ber of instances across domains and relation types.
Therefore, our second experiment tests each method
on the BLESS dataset (Baroni and Lenci, 2011),
which spans 17 topical domains and includes five
relation types, the three in K&amp;H and (a) attributes of
concepts, a relation holding between nouns and ad-
jectives, and (b) actions performed by/to concepts a
relation holding between nouns and verbs. In total,
the BLESS dataset contains 14400 positive instances
and an equal number of negative instances. This ex-
periment measures the generalizability of each sys-
tem and tests the capabilities of the systems for
lexical-semantic relation types other than taxonomic
relations.
Domain-aware training instances To show the
importance of the domain-aware training instances,
the average results of the systems obtained for the
in-domain setup across the BLESS dataset are com-
pared with the average results obtained when the
systems are trained out-of-domain. For the out-of-
domain setup, one domain is left out from the train-
ing set and used for testing. The experiment was
repeated for each domain and the average results are
</bodyText>
<table confidence="0.999671733333333">
In-domain Out-of-domain
P R F P R F
PairClass 66.8 35.6 46.4 78.9 43.2 55.8
BL 79.5 51.6 62.6 71.7 40.0 51.4
GraCE 87.7 85.0 86.3 66.2 36.3 46.9
DSZhila 62.1 47.4 53.7 50.7 46.9 48.7
DSLevy 53.0 49.2 51.0 51.1 47.5 49.2
WECEBoW 90.0 90.9 90.4 68.0 66.9 67.5
offset
WECEBoW 89.9 91.0 90.4 83.8 57.0 67.8
concat
WECEDep 85.3 86.5 85.9 68.7 62.3 65.4
fset
WECEDep 85.9 87.0 86.5 78.2 63.8 70.3
concat
</table>
<tableCaption confidence="0.745578">
Table 4: Aggregated results obtained when systems
are tested with the BLESS dataset over BNC.
</tableCaption>
<figureCaption confidence="0.769133666666667">
Figure 2: F1 scores distribution across domains for
each proposed system and relation type over BNC
corpus.
</figureCaption>
<bodyText confidence="0.99974865">
presented in Table 4. In this experiment, the systems
are tested over the BNC corpus to show the capabil-
ities of the systems to classify out-of-domain in a
more reduced corpus.
Results When no examples from a domain are
provided, a general significant decrease in perfor-
mance is observed. The GraCE performance de-
creases 39.4 points in F1, while the WECE systems
decrease 20.55 points in average.
The results obtained show that when the instances
to be classified are less homogeneous, i.e. when
the instances belong to different domains, none of
the systems can achieve the level of performance
reported for the in-domain setup. These were the
expected results for the GraCE system due to the
lexical features that it uses and which are domain
dependent. However, the WECE systems are also
affected by this lack of domain-aware training in-
stances. WECEconcat results decrease because sim-
ilar embeddings are associated with similar words.
</bodyText>
<page confidence="0.99682">
189
</page>
<bodyText confidence="0.99989225">
When two words belong to two different topical do-
mains, their embeddings are less similar and, there-
fore, the SVM system cannot learn distinctive fea-
tures for each lexical-semantic relation.
In-domain results per relation type In this work
we are interested in creating a general approach for
the classification of any lexical semantic relation in-
stances. Figure 2 shows the box and whisker plot of
the results obtained per relation type across domains
in the in-domain setup over the BNC corpus.
Discussion The results confirm that the proposed
systems achieve satisfactory results across all the
relations, the median of the results being around
90 points in F1. The most accurate system is
WECEbow, which supports the assertion by Levy
and Goldberg (2014a) that bag-of-word embeddings
should offer superior performance to dependency-
based embeddings on task involving semantic rela-
tions. Carrying out an error analysis, the lowest re-
sults of the WECE systems are obtained in the do-
mains with the fewest training instances, making ap-
parent that word embedding systems are dependent
on the number of training instances. For these do-
mains, GraCE achieves better results.
</bodyText>
<sectionHeader confidence="0.999654" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999566631578947">
In this paper we have presented two systems for clas-
sifying the lexical-semantic relation of a word pair.
Both are designed to address the challenge of data
sparsity, i.e., classifying word pairs whose mem-
bers never co-occur, in order to improve classifica-
tion recall. The two main contributions are the word
pair vectorial representations, one based on a graph-
based corpus representation and the other one based
on word embeddings. We have demonstrated that
by including information about lexical and relational
similarity in the word pair vectorial representation,
the recall of our systems increases, overcoming the
upper-bound limit of state-of-the-art systems. Fur-
thermore, we show that our systems are able to clas-
sify target word pairs into multiple lexical seman-
tic relation types, beyond the traditional taxonomic
types. In future work, we plan to analyze the prop-
erties of the instances that can be classified with the
GraCE system but not with the WECE systems.
</bodyText>
<sectionHeader confidence="0.997715" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9931968">
The authors gratefully acknowledge the support
of the CLARA project (EU-7FP-ITN-238405), the
SKATER project (Ministerio de Economia y Com-
petitividad, TIN2012-38584-C06-05) and of the
MultiJEDI ERC Starting Grant no. 259234.
</bodyText>
<sectionHeader confidence="0.904397" genericHeader="method">
Appendix
A Full Classifier Results
</sectionHeader>
<table confidence="0.999986763157895">
P BNC F Wikipedia F
R P R
C 84.1 3.6 6.9 92.4 9.3 16.8
H 79.7 10.1 17.9 75.6 26.1 38.8
M 38.6 8.5 14.0 23.9 15.5 18.8
* 76.9 4.6 8.7 77.0 11.7 20.4
C 84.4 5.6 10.4 88.8 13.8 23.9
H 82.4 20.1 32.3 92.7 31.9 47.5
M 69.4 12.8 21.6 77.3 14.5 24.4
* 82.6 7.7 14.2 89.4 16.2 27.5
C 90.9 43.7 59.0 94.2 78.7 85.7
H 90.5 48.9 63.5 93.2 67.8 78.5
M 87.5 26.3 40.4 91.8 28.7 43.7
* 90.7 43.8 59.0 94.0 75.5 83.7
C 97.2 8.0 14.8 95.5 11.5 20.5
H 28.2 58.6 38.1 29.1 85.4 43.4
M 8.4 36.4 13.7 8.5 48.0 14.5
* 31.6 15.7 21.0 32.8 22.6 26.8
C 82.0 2.6 5.0 84.0 5.2 9.8
H 20.7 62.7 31.1 21.8 80.7 34.4
M 5.1 26.1 8.6 11.3 43.6 17.9
* 18.7 11.4 14.2 27.7 15.6 20.0
C 95.9 60.4 74.1 96.6 89.7 93.0
H 98.1 56.5 71.7 98.9 85.3 91.6
M 88.6 38.3 53.5 90.8 51.2 65.4
* 96.0 59.1 73.1 96.8 87.7 92.0
C 98.2 60.6 74.9 98.5 89.8 93.9
H 96.0 60.1 73.9 97.1 91.3 94.1
M 81.1 45.9 58.7 77.9 68.6 72.9
* 97.4 60.0 74.2 97.6 89.3 93.2
C 87.0 66.5 75.4 95.1 88.1 91.5
H 96.6 51.9 67.5 98.1 84.3 90.7
M 83.1 26.4 40.1 88.2 44.7 59.3
* 87.9 63.1 73.5 95.4 86.1 90.5
C 94.0 66.7 78.0 98.0 89.2 93.4
H 93.1 60.2 73.1 95.5 90.3 92.8
M 67.0 35.8 46.7 69.5 62.0 65.6
* 93.1 64.7 76.4 96.7 88.4 92.4
</table>
<tableCaption confidence="0.960067">
Table 5: Detailed results for each relation tested, co-
</tableCaption>
<bodyText confidence="0.874708666666667">
ordination (C), hypernymy (H) and meronymy (M),
and the aggregated results (*) obtained with K&amp;H
dataset over BNC and Wikipedia.
</bodyText>
<page confidence="0.996458">
190
</page>
<sectionHeader confidence="0.996318" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871310679612">
Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics, pages 1–10.
Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski.
2014. Don’t count, predict! a systematic compari-
son of context-counting vs. context-predicting seman-
tic vectors. In Proceedings of ACL.
Matthew Berland and Eugene Charniak. 1999. Finding
parts in very large corpora. In Proceedings of ACL,
pages 57–64.
Scott Cederberg and Dominic Widdows. 2003. Using
LSA and noun coordination information to improve
the precision and recall of automatic hyponymy ex-
traction. In Proceedings of CoNLL, pages 111–118.
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proceed-
ings of COLING-ACL, pages 297–304.
Antonio Di Marco and Roberto Navigli. 2013. Cluster-
ing and Diversifying Web Search Results with Graph-
Based Word Sense Induction. Computational Linguis-
tics, 39(3):709–754.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hier-
archies via word embeddings. In Proceedings of ACL,
volume 1.
Roxana Girju, Adriana Badulescu, and Dan I. Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations Newsletter, 11(1):10–18.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539–545.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. SemEval-2010 Task 8: Multi-Way Classi-
fication of Semantic Relations between Pairs of Nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 33–38.
Amac¸ Herdadelen and Marco Baroni. 2009. Bagpack: A
general framework to represent semantic relations. In
Proceedings of the Workshop on Geometrical Models
of Natural Language Semantics, pages 33–40.
David Jurgens, Peter D Turney, Saif M Mohammad, and
Keith J Holyoak. 2012. Semeval-2012 Task 2: Mea-
suring degrees of relational similarity. In Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 356–364.
Zornitsa Kozareva and Eduard H. Hovy. 2010. A semi-
supervised method to learn and construct taxonomies
using the web. In Proceedings of EMNLP, pages
1110–1118.
Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of ACL.
Omer Levy and Yoav Goldberg. 2014b. Linguistic regu-
larities in sparse and explicit word representations. In
Proceedings of CoNLL.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas
Burget, and Jan Cernocky. 2011. Strategies for train-
ing large scale neural network language models. In
Proceedings of the 2011 IEEE Workshop on Auto-
matic Speech Recognition and Understanding (ASRU),
pages 196–201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. Proceedings of Workshop at
ICLR.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositionality.
In Proceedings of NIPS, pages 3111–3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of HLT-NAACL,
pages 746–751.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39–41.
Einat Minkov and William W. Cohen. 2012. Graph
based similarity measures for synonym extraction
from parsed text. In Proceedings of the Workshop on
Graph-based Methods for Natural Language Process-
ing, pages 20–24.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL-CONLL,
pages 1003–1011.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym extrac-
tion. In Proceedings of ACL, pages 1318–1327.
John C. Platt. 1999. Fast training of support vector
machines using sequential minimal optimization. In
Bernhard Sch¨olkopf, Christopher J. C. Burges, and
</reference>
<page confidence="0.97885">
191
</page>
<reference confidence="0.99977903125">
Alexander J. Smola, editors, Advances in kernel meth-
ods, pages 185–208. MIT Press, Cambridge, MA,
USA.
Diarmuid O S´eaghdha and Ann Copestake. 2009. Using
lexical and relational similarity to classify semantic re-
lations. In Proceedings of EACL, pages 621–629.
Isabel Segura-Bedmar, Paloma Martınez, and Marıa
Herrero-Zazo. 2013. Semeval-2013 task 9: Extrac-
tion of drug-drug interactions from biomedical texts.
In Proceedings of the 7th International Workshop on
Semantic Evaluation.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Proceedings of NIPS.
Peter D. Turney. 2006a. Expressing implicit seman-
tic relations without supervision. In Proceedings of
COLING-ACL, pages 313–320.
Peter D Turney. 2006b. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Peter D Turney. 2008a. The latent relation mapping en-
gine: Algorithm and experiments. Journal ofArtificial
Intelligence Research (JAIR), 33:615–655.
Peter D. Turney. 2008b. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In Pro-
ceedings of COLING, pages 905–912.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In Pro-
ceedings of COLING.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining hetero-
geneous models for measuring relational similarity. In
Proceedings of HLT-NAACL, pages 1000–1009.
</reference>
<page confidence="0.998194">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374328">
<title confidence="0.9994315">Reading Between the Lines: Overcoming Data Sparsity for Classification of Lexical Relationships</title>
<author confidence="0.999258">Sara Mendes David Jurgens</author>
<affiliation confidence="0.999996">Universitat Pompeu Fabra Universidade de Lisboa McGill University</affiliation>
<address confidence="0.994747">Barcelona, Spain Lisboa, Portugal Montreal, Canada</address>
<email confidence="0.981296">silvia.necsulescu@upf.edusara.mendes@clul.ul.ptjurgens@cs.mcgill.ca</email>
<affiliation confidence="0.826486">N´uria Universitat Pompeu</affiliation>
<address confidence="0.59839">Barcelona,</address>
<email confidence="0.999877">nuria.bel@upf.edu</email>
<abstract confidence="0.998842333333334">The lexical semantic relationships between word pairs are key features for many NLP tasks. Most approaches for automatically classifying related word pairs are hindered by data sparsity because of their need to observe two words co-occurring in order to detect the lexical relation holding between them. Even when mining very large corpora, not every related word pair co-occurs. Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even when these are never found in the same sentence in a given corpus. In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving significant gains in recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="31909" citStr="Baroni and Lenci, 2011" startWordPosition="5135" endWordPosition="5138">tion about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, where the systems have to classify word pairs from a different domain than the domains in the training set. The objective is to assess the importance of the domain-aware training instances for the classification. The K&amp;H dataset contains only instances from three domains and is imbalanced between the number of instances across domains and relation types. Therefore, our second experiment tests each method on the BLESS dataset (Baroni and Lenci, 2011), which spans 17 topical domains and includes five relation types, the three in K&amp;H and (a) attributes of concepts, a relation holding between nouns and adjectives, and (b) actions performed by/to concepts a relation holding between nouns and verbs. In total, the BLESS dataset contains 14400 positive instances and an equal number of negative instances. This experiment measures the generalizability of each system and tests the capabilities of the systems for lexical-semantic relation types other than taxonomic relations. Domain-aware training instances To show the importance of the domain-aware</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4646" citStr="Baroni et al., 2014" startWordPosition="694" endWordPosition="698">nstances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target seman</context>
<context position="24883" citStr="Baroni et al., 2014" startWordPosition="3989" endWordPosition="3992">SZhila and DSLevy, respectively. The inclusion of this system enables comparing the performance impact of using an SVM classifier with our embedding-based pair representations versus classifying instances by comparing the embeddings themselves. We note a DS system represents a minimally-supervised system whose features are produced in an unsupervised way (i.e., through the embedding process) and are therefore not necessarily tuned for the task of relation classification; however, such embeddings have previously been shown to yield state-of-the-art performance in other semantic relation tasks (Baroni et al., 2014) and therefore the DS systems are intended to identify potential benefits when adding feature selection by means of the SVM in WECE systems. BASELINE The purported benefit of the GraCE model is that the graph enables identifying syntactic features between pair members that are never observed in the corpus, which increases the number of instances that can be classified without sacrificing accuracy. Therefore, to quantify the effect of the graph, we include a baseline system, denoted BL, that uses an identical setup to GraCE but where the feature vector for a word pair is created only from the d</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="5159" citStr="Berland and Charniak, 1999" startWordPosition="765" endWordPosition="768">ties among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisitio</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding parts in very large corpora. In Proceedings of ACL, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cederberg</author>
<author>Dominic Widdows</author>
</authors>
<title>Using LSA and noun coordination information to improve the precision and recall of automatic hyponymy extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="6609" citStr="Cederberg and Widdows, 2003" startWordPosition="988" endWordPosition="991">tances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and</context>
</contexts>
<marker>Cederberg, Widdows, 2003</marker>
<rawString>Scott Cederberg and Dominic Widdows. 2003. Using LSA and noun coordination information to improve the precision and recall of automatic hyponymy extraction. In Proceedings of CoNLL, pages 111–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="5901" citStr="Davidov and Rappoport, 2006" startWordPosition="880" endWordPosition="884">ttern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov and Ari Rappoport. 2006. Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words. In Proceedings of COLING-ACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and Diversifying Web Search Results with GraphBased Word Sense Induction. Computational Linguistics, 39(3):709–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Jiang Guo</author>
<author>Bing Qin</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning semantic hierarchies via word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>1</volume>
<contexts>
<context position="8367" citStr="Fu et al., 2014" startWordPosition="1258" endWordPosition="1261">jective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semantic relations. Given a set of target semantic relations R = {r1, ... , rn1, and a set of word pairs W = {(x, y)1, ... , (x, y)n1, the task is to label each word pair (x, y)i wi</context>
</contexts>
<marker>Fu, Guo, Qin, Che, Wang, Liu, 2014</marker>
<rawString>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierarchies via word embeddings. In Proceedings of ACL, volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan I Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="5871" citStr="Girju et al., 2006" startWordPosition="876" endWordPosition="879">ation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relationa</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan I. Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="20523" citStr="Hall et al., 2009" startWordPosition="3290" endWordPosition="3293">1, x2,... , xn), and v(y) = (y1, y2,. .. , yn) respectively, the vectorial representation of (x, y) is defined as the concatenation of v(x) and v(y): v((x, y)) = (x1, x2, . . . , xn, y1, y2, . . . , yn) Consequently the length of v((x, y)) is 2n, where n is the dimension of the embedding space. 3.3 Relation Classification For both representations, a supervised classifier is trained. Given a set of tuples E = ((x, y)i, ri) of example instances for each relation ri E R, a support vector machine (SVM) multi-class classifier with a radial basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our work we focus on the classification of word </context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations Newsletter, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="5105" citStr="Hearst, 1992" startWordPosition="759" endWordPosition="760"> been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techn</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>33--38</pages>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2010</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amac¸ Herdadelen</author>
<author>Marco Baroni</author>
</authors>
<title>Bagpack: A general framework to represent semantic relations.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="3470" citStr="Herdadelen and Baroni, 2009" startWordPosition="516" endWordPosition="519">ative to requiring co-occurrence, other works have classified the relation of a word pair using lexical similarity, i.e., the similarity of the concepts themselves. Given two word pairs, (w1, w2) and (w3, w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – eve</context>
<context position="6704" citStr="Herdadelen and Baroni, 2009" startWordPosition="1002" endWordPosition="1006">calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distri</context>
</contexts>
<marker>Herdadelen, Baroni, 2009</marker>
<rawString>Amac¸ Herdadelen and Marco Baroni. 2009. Bagpack: A general framework to represent semantic relations. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Peter D Turney</author>
<author>Saif M Mohammad</author>
<author>Keith J Holyoak</author>
</authors>
<title>Semeval-2012 Task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>356--364</pages>
<contexts>
<context position="23432" citStr="Jurgens et al., 2012" startWordPosition="3755" endWordPosition="3759"> to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. The patterns retained are then used as features to train an SVM classifier over the set of possible relation types. DSZhila &amp; DSLevy Word embeddings have previously been shown to accurately measure relational similarity; Zhila et al. (2013) demonstrate state-ofthe-art performance on SemEval-2012 Task 2 (Jurgens et al., 2012) which measures word pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings produced</context>
</contexts>
<marker>Jurgens, Turney, Mohammad, Holyoak, 2012</marker>
<rawString>David Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak. 2012. Semeval-2012 Task 2: Measuring degrees of relational similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 356–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard H Hovy</author>
</authors>
<title>A semisupervised method to learn and construct taxonomies using the web.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1110--1118</pages>
<contexts>
<context position="27200" citStr="Kozareva and Hovy (2010)" startWordPosition="4376" endWordPosition="4379">.4 14.2 27.7 15.6 20.0 WECEBoW 96.0 59.1 73.1 96.8 87.7 92.0 offset WECEBoW 97.4 60.0 74.2 97.6 89.3 93.2 concat WECEDep offset 87.9 63.1 73.5 95.4 86.1 90.5 WECEDep 93.1 64.7 76.4 96.7 88.4 92.4 concat Table 3: Aggregated results obtained for the indomain setup with the K&amp;H dataset. Detailed results are presented in the Appendix A. occur in text. Therefore, in the first experiment, we test whether the recall of classification systems is improved when the word pair representation encodes information about lexical and relational similarity. As an evaluation dataset, we expand on the dataset of Kozareva and Hovy (2010) (K&amp;H), which was collected from hyponym-hypernym instances from WordNet (Miller, 1995) spanning three topical domains: animals, plants and vehicles. Because our systems are capable of classifying instances with more than one relation at once, we enhance this dataset with instances of two more relation types: co-hyponymy and meronymy. Co-hyponyms are extracted directly from the K&amp;H dataset: two words are co-hyponyms if they have the same direct ancestor.5 To avoid including generic nouns, such as “migrator” in the “animal” domain, only leaf nodes are considered. The meronym instances are extra</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard H. Hovy. 2010. A semisupervised method to learn and construct taxonomies using the web. In Proceedings of EMNLP, pages 1110–1118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="8188" citStr="Levy and Goldberg, 2014" startWordPosition="1231" endWordPosition="1234">enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-se</context>
<context position="18608" citStr="Levy and Goldberg, 2014" startWordPosition="2980" endWordPosition="2983">mple that v(king) − v(man) ≈ v(queen) − v(women), where v(x) is the embedding of the word x, indicating the vectors are encoding information on the words’ semantic roles. For learning word embeddings, we used the Skipgram model, improved with techniques of negative sampling and subsampling of frequent words, which achieved the best results for detecting semantically similar words (Mikolov et al., 2013a; Mikolov et al., 2013b). Moreover, for a fair comparison with the GraCE system, developed with dependency relations, we also tested the results obtained with a dependency-based Skip-gram model (Levy and Goldberg, 2014a). Words occurring only once in corpus are filtered out and 200-dimensional vectors are learned. Two embedding-based representations are considered for a relation: WECEoffset leverages the offset of the word embeddings, while WECEconcat concatenates the embeddings, both described next. WECEoffset Representation Mikolov et al. (2013c) shows that the vectorial representation of words provided by word embeddings captures syntactic and semantic regularities and that each relationship is characterized by a relation specific vector offset. Word pairs with similar offsets can be interpreted as word </context>
<context position="22244" citStr="Levy and Goldberg, 2014" startWordPosition="3559" endWordPosition="3562">iments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns using the template [0 to 1 words] x [0 to 3 words] y [0 to 1 words]. Us</context>
<context position="24191" citStr="Levy and Goldberg (2014" startWordPosition="3888" endWordPosition="3891">on). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings produced using the method of Mikolov et al. (2011), which was originally used in Zhila et al. (2013) and (b) the embeddings using the method of Levy and Goldberg (2014a), which include dependency parsing information. We refer to these as DSZhila and DSLevy, respectively. The inclusion of this system enables comparing the performance impact of using an SVM classifier with our embedding-based pair representations versus classifying instances by comparing the embeddings themselves. We note a DS system represents a minimally-supervised system whose features are produced in an unsupervised way (i.e., through the embedding process) and are therefore not necessarily tuned for the task of relation classification; however, such embeddings have previously been shown </context>
<context position="35181" citStr="Levy and Goldberg (2014" startWordPosition="5671" endWordPosition="5674">distinctive features for each lexical-semantic relation. In-domain results per relation type In this work we are interested in creating a general approach for the classification of any lexical semantic relation instances. Figure 2 shows the box and whisker plot of the results obtained per relation type across domains in the in-domain setup over the BNC corpus. Discussion The results confirm that the proposed systems achieve satisfactory results across all the relations, the median of the results being around 90 points in F1. The most accurate system is WECEbow, which supports the assertion by Levy and Goldberg (2014a) that bag-of-word embeddings should offer superior performance to dependencybased embeddings on task involving semantic relations. Carrying out an error analysis, the lowest results of the WECE systems are obtained in the domains with the fewest training instances, making apparent that word embedding systems are dependent on the number of training instances. For these domains, GraCE achieves better results. 5 Conclusions In this paper we have presented two systems for classifying the lexical-semantic relation of a word pair. Both are designed to address the challenge of data sparsity, i.e., </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Dependencybased word embeddings. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Linguistic regularities in sparse and explicit word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="8188" citStr="Levy and Goldberg, 2014" startWordPosition="1231" endWordPosition="1234">enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-se</context>
<context position="18608" citStr="Levy and Goldberg, 2014" startWordPosition="2980" endWordPosition="2983">mple that v(king) − v(man) ≈ v(queen) − v(women), where v(x) is the embedding of the word x, indicating the vectors are encoding information on the words’ semantic roles. For learning word embeddings, we used the Skipgram model, improved with techniques of negative sampling and subsampling of frequent words, which achieved the best results for detecting semantically similar words (Mikolov et al., 2013a; Mikolov et al., 2013b). Moreover, for a fair comparison with the GraCE system, developed with dependency relations, we also tested the results obtained with a dependency-based Skip-gram model (Levy and Goldberg, 2014a). Words occurring only once in corpus are filtered out and 200-dimensional vectors are learned. Two embedding-based representations are considered for a relation: WECEoffset leverages the offset of the word embeddings, while WECEconcat concatenates the embeddings, both described next. WECEoffset Representation Mikolov et al. (2013c) shows that the vectorial representation of words provided by word embeddings captures syntactic and semantic regularities and that each relationship is characterized by a relation specific vector offset. Word pairs with similar offsets can be interpreted as word </context>
<context position="22244" citStr="Levy and Goldberg, 2014" startWordPosition="3559" endWordPosition="3562">iments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns using the template [0 to 1 words] x [0 to 3 words] y [0 to 1 words]. Us</context>
<context position="24191" citStr="Levy and Goldberg (2014" startWordPosition="3888" endWordPosition="3891">on). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings produced using the method of Mikolov et al. (2011), which was originally used in Zhila et al. (2013) and (b) the embeddings using the method of Levy and Goldberg (2014a), which include dependency parsing information. We refer to these as DSZhila and DSLevy, respectively. The inclusion of this system enables comparing the performance impact of using an SVM classifier with our embedding-based pair representations versus classifying instances by comparing the embeddings themselves. We note a DS system represents a minimally-supervised system whose features are produced in an unsupervised way (i.e., through the embedding process) and are therefore not necessarily tuned for the task of relation classification; however, such embeddings have previously been shown </context>
<context position="35181" citStr="Levy and Goldberg (2014" startWordPosition="5671" endWordPosition="5674">distinctive features for each lexical-semantic relation. In-domain results per relation type In this work we are interested in creating a general approach for the classification of any lexical semantic relation instances. Figure 2 shows the box and whisker plot of the results obtained per relation type across domains in the in-domain setup over the BNC corpus. Discussion The results confirm that the proposed systems achieve satisfactory results across all the relations, the median of the results being around 90 points in F1. The most accurate system is WECEbow, which supports the assertion by Levy and Goldberg (2014a) that bag-of-word embeddings should offer superior performance to dependencybased embeddings on task involving semantic relations. Carrying out an error analysis, the lowest results of the WECE systems are obtained in the domains with the fewest training instances, making apparent that word embedding systems are dependent on the number of training instances. For these domains, GraCE achieves better results. 5 Conclusions In this paper we have presented two systems for classifying the lexical-semantic relation of a word pair. Both are designed to address the challenge of data sparsity, i.e., </context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Linguistic regularities in sparse and explicit word representations. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="22071" citStr="Manning et al., 2014" startWordPosition="3534" endWordPosition="3537">up Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation ty</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Daniel Povey</author>
<author>Lukas Burget</author>
<author>Jan Cernocky</author>
</authors>
<title>Strategies for training large scale neural network language models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),</booktitle>
<pages>196--201</pages>
<contexts>
<context position="24074" citStr="Mikolov et al. (2011)" startWordPosition="3867" endWordPosition="3870">pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings produced using the method of Mikolov et al. (2011), which was originally used in Zhila et al. (2013) and (b) the embeddings using the method of Levy and Goldberg (2014a), which include dependency parsing information. We refer to these as DSZhila and DSLevy, respectively. The inclusion of this system enables comparing the performance impact of using an SVM classifier with our embedding-based pair representations versus classifying instances by comparing the embeddings themselves. We note a DS system represents a minimally-supervised system whose features are produced in an unsupervised way (i.e., through the embedding process) and are therefor</context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget, and Jan Cernocky. 2011. Strategies for training large scale neural network language models. In Proceedings of the 2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 196–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proceedings of Workshop at ICLR.</booktitle>
<contexts>
<context position="8116" citStr="Mikolov et al., 2013" startWordPosition="1219" endWordPosition="1222">i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description Th</context>
<context position="17970" citStr="Mikolov et al. (2013" startWordPosition="2879" endWordPosition="2882"> two word pair representations based on word embeddings. We refer to a system based on embeddings as Word Embeddings Classification systEm (WECE). An embedding is a low-dimensional vectorial representation of a word, where the dimensions are latent continuous features and vector components are set to maximize the probability of the contexts in which the target word tends to appear. Since similar words occur in similar contexts the word embeddings learn similar vectors for similar words. Moreover, the vector offset of two word embeddings reflect the relation holding between them. For instance, Mikolov et al. (2013c) give the example that v(king) − v(man) ≈ v(queen) − v(women), where v(x) is the embedding of the word x, indicating the vectors are encoding information on the words’ semantic roles. For learning word embeddings, we used the Skipgram model, improved with techniques of negative sampling and subsampling of frequent words, which achieved the best results for detecting semantically similar words (Mikolov et al., 2013a; Mikolov et al., 2013b). Moreover, for a fair comparison with the GraCE system, developed with dependency relations, we also tested the results obtained with a dependency-based Sk</context>
<context position="22177" citStr="Mikolov et al., 2013" startWordPosition="3549" endWordPosition="3552">sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns usin</context>
<context position="30859" citStr="Mikolov et al., 2013" startWordPosition="4973" endWordPosition="4976">isity is still major contributor to classification error. By combining all the distributional information into a denser vector, WECE systems are able to improve upon GraCE’s results by an average of 2.9 points in precision and 17.9 points in recall. WECE results see an increase by 62 points in precision and 46 in recall over DSZhila which used the same em188 beddings, highlighting the importance of the SVM classifier for learning which features of the embeddings reflect the lexical relation. Although embeddings have been argued to reflect the semantic or syntactic relations between two words (Mikolov et al., 2013c), our results suggest that additional machine learning (as done with WECEoffset) is needed to identify which dimensions of the embeddings accurately correspond to specific relationships. Between the WECE systems, WECEconcat achieves slightly better results on the K&amp;H dataset. 4.4 Experiment 2 In the first experiment, the proposed systems were compared to test the importance of having a representation that includes information about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, whe</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. Proceedings of Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="8116" citStr="Mikolov et al., 2013" startWordPosition="1219" endWordPosition="1222">i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description Th</context>
<context position="17970" citStr="Mikolov et al. (2013" startWordPosition="2879" endWordPosition="2882"> two word pair representations based on word embeddings. We refer to a system based on embeddings as Word Embeddings Classification systEm (WECE). An embedding is a low-dimensional vectorial representation of a word, where the dimensions are latent continuous features and vector components are set to maximize the probability of the contexts in which the target word tends to appear. Since similar words occur in similar contexts the word embeddings learn similar vectors for similar words. Moreover, the vector offset of two word embeddings reflect the relation holding between them. For instance, Mikolov et al. (2013c) give the example that v(king) − v(man) ≈ v(queen) − v(women), where v(x) is the embedding of the word x, indicating the vectors are encoding information on the words’ semantic roles. For learning word embeddings, we used the Skipgram model, improved with techniques of negative sampling and subsampling of frequent words, which achieved the best results for detecting semantically similar words (Mikolov et al., 2013a; Mikolov et al., 2013b). Moreover, for a fair comparison with the GraCE system, developed with dependency relations, we also tested the results obtained with a dependency-based Sk</context>
<context position="22177" citStr="Mikolov et al., 2013" startWordPosition="3549" endWordPosition="3552">sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns usin</context>
<context position="30859" citStr="Mikolov et al., 2013" startWordPosition="4973" endWordPosition="4976">isity is still major contributor to classification error. By combining all the distributional information into a denser vector, WECE systems are able to improve upon GraCE’s results by an average of 2.9 points in precision and 17.9 points in recall. WECE results see an increase by 62 points in precision and 46 in recall over DSZhila which used the same em188 beddings, highlighting the importance of the SVM classifier for learning which features of the embeddings reflect the lexical relation. Although embeddings have been argued to reflect the semantic or syntactic relations between two words (Mikolov et al., 2013c), our results suggest that additional machine learning (as done with WECEoffset) is needed to identify which dimensions of the embeddings accurately correspond to specific relationships. Between the WECE systems, WECEconcat achieves slightly better results on the K&amp;H dataset. 4.4 Experiment 2 In the first experiment, the proposed systems were compared to test the importance of having a representation that includes information about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, whe</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="8116" citStr="Mikolov et al., 2013" startWordPosition="1219" endWordPosition="1222">i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, de183 pending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description Th</context>
<context position="17970" citStr="Mikolov et al. (2013" startWordPosition="2879" endWordPosition="2882"> two word pair representations based on word embeddings. We refer to a system based on embeddings as Word Embeddings Classification systEm (WECE). An embedding is a low-dimensional vectorial representation of a word, where the dimensions are latent continuous features and vector components are set to maximize the probability of the contexts in which the target word tends to appear. Since similar words occur in similar contexts the word embeddings learn similar vectors for similar words. Moreover, the vector offset of two word embeddings reflect the relation holding between them. For instance, Mikolov et al. (2013c) give the example that v(king) − v(man) ≈ v(queen) − v(women), where v(x) is the embedding of the word x, indicating the vectors are encoding information on the words’ semantic roles. For learning word embeddings, we used the Skipgram model, improved with techniques of negative sampling and subsampling of frequent words, which achieved the best results for detecting semantically similar words (Mikolov et al., 2013a; Mikolov et al., 2013b). Moreover, for a fair comparison with the GraCE system, developed with dependency relations, we also tested the results obtained with a dependency-based Sk</context>
<context position="22177" citStr="Mikolov et al., 2013" startWordPosition="3549" endWordPosition="3552">sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using 186 two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns usin</context>
<context position="30859" citStr="Mikolov et al., 2013" startWordPosition="4973" endWordPosition="4976">isity is still major contributor to classification error. By combining all the distributional information into a denser vector, WECE systems are able to improve upon GraCE’s results by an average of 2.9 points in precision and 17.9 points in recall. WECE results see an increase by 62 points in precision and 46 in recall over DSZhila which used the same em188 beddings, highlighting the importance of the SVM classifier for learning which features of the embeddings reflect the lexical relation. Although embeddings have been argued to reflect the semantic or syntactic relations between two words (Mikolov et al., 2013c), our results suggest that additional machine learning (as done with WECEoffset) is needed to identify which dimensions of the embeddings accurately correspond to specific relationships. Between the WECE systems, WECEconcat achieves slightly better results on the K&amp;H dataset. 4.4 Experiment 2 In the first experiment, the proposed systems were compared to test the importance of having a representation that includes information about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, whe</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of HLT-NAACL, pages 746–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="1325" citStr="Miller, 1995" startWordPosition="186" endWordPosition="187">en them. Even when mining very large corpora, not every related word pair co-occurs. Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even when these are never found in the same sentence in a given corpus. In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving significant gains in recall. 1 Introduction Resources containing lexical-semantic relations such as hypernymy or meronymy have proven useful in many NLP tasks. While resources such as WordNet (Miller, 1995) contain many general relations and subsequently have seen widespread adoption, developing this type of rich resource for new languages or for new domains is prohibitively costly and time-consuming. Therefore, automated approaches are needed and, in order to create such a lexical-semantic database, a first step is to develop accurate techniques for classifying the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur Roberto Navigli Universit`a “La S</context>
<context position="27287" citStr="Miller, 1995" startWordPosition="4389" endWordPosition="4390">.3 93.2 concat WECEDep offset 87.9 63.1 73.5 95.4 86.1 90.5 WECEDep 93.1 64.7 76.4 96.7 88.4 92.4 concat Table 3: Aggregated results obtained for the indomain setup with the K&amp;H dataset. Detailed results are presented in the Appendix A. occur in text. Therefore, in the first experiment, we test whether the recall of classification systems is improved when the word pair representation encodes information about lexical and relational similarity. As an evaluation dataset, we expand on the dataset of Kozareva and Hovy (2010) (K&amp;H), which was collected from hyponym-hypernym instances from WordNet (Miller, 1995) spanning three topical domains: animals, plants and vehicles. Because our systems are capable of classifying instances with more than one relation at once, we enhance this dataset with instances of two more relation types: co-hyponymy and meronymy. Co-hyponyms are extracted directly from the K&amp;H dataset: two words are co-hyponyms if they have the same direct ancestor.5 To avoid including generic nouns, such as “migrator” in the “animal” domain, only leaf nodes are considered. The meronym instances are extracted directly from WordNet. The final dataset excludes multi-word expressions, which we</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>William W Cohen</author>
</authors>
<title>Graph based similarity measures for synonym extraction from parsed text.</title>
<date>2012</date>
<booktitle>In Proceedings of the Workshop on Graph-based Methods for Natural Language Processing,</booktitle>
<pages>20--24</pages>
<contexts>
<context position="7541" citStr="Minkov and Cohen, 2012" startWordPosition="1128" endWordPosition="1131">nd relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b;</context>
</contexts>
<marker>Minkov, Cohen, 2012</marker>
<rawString>Einat Minkov and William W. Cohen. 2012. Graph based similarity measures for synonym extraction from parsed text. In Proceedings of the Workshop on Graph-based Methods for Natural Language Processing, pages 20–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-CONLL,</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="2244" citStr="Mintz et al., 2009" startWordPosition="324" endWordPosition="327">a first step is to develop accurate techniques for classifying the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can classify. As an alternat</context>
<context position="6245" citStr="Mintz et al., 2009" startWordPosition="934" endWordPosition="937">ch new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of ACL-CONLL, pages 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1318--1327</pages>
<contexts>
<context position="7504" citStr="Navigli and Velardi, 2010" startWordPosition="1122" endWordPosition="1125">roaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov </context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of ACL, pages 1318–1327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization. In</title>
<date>1999</date>
<location>and</location>
<contexts>
<context position="20481" citStr="Platt, 1999" startWordPosition="3284" endWordPosition="3285">torial representations are v(x) = (x1, x2,... , xn), and v(y) = (y1, y2,. .. , yn) respectively, the vectorial representation of (x, y) is defined as the concatenation of v(x) and v(y): v((x, y)) = (x1, x2, . . . , xn, y1, y2, . . . , yn) Consequently the length of v((x, y)) is 2n, where n is the dimension of the embedding space. 3.3 Relation Classification For both representations, a supervised classifier is trained. Given a set of tuples E = ((x, y)i, ri) of example instances for each relation ri E R, a support vector machine (SVM) multi-class classifier with a radial basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our wo</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Fast training of support vector machines using sequential minimal optimization. In Bernhard Sch¨olkopf, Christopher J. C. Burges, and</rawString>
</citation>
<citation valid="false">
<booktitle>Advances in kernel methods,</booktitle>
<pages>185--208</pages>
<editor>Alexander J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker></marker>
<rawString>Alexander J. Smola, editors, Advances in kernel methods, pages 185–208. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Using lexical and relational similarity to classify semantic relations.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>621--629</pages>
<marker>S´eaghdha, Copestake, 2009</marker>
<rawString>Diarmuid O S´eaghdha and Ann Copestake. 2009. Using lexical and relational similarity to classify semantic relations. In Proceedings of EACL, pages 621–629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabel Segura-Bedmar</author>
<author>Paloma Martınez</author>
<author>Marıa Herrero-Zazo</author>
</authors>
<title>Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="21070" citStr="Segura-Bedmar et al., 2013" startWordPosition="3376" endWordPosition="3379">al basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our work we focus on the classification of word pairs as instances of lexical-semantic relations out of context. The performance of the GraCE and WECE systems is tested across two datasets, focusing on their ability to classify instances of specific lexical-semantic relations as well as to provide insights into the systems’ generalization capabilities. 4.1 Experimental Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using 186 two corpor</context>
</contexts>
<marker>Segura-Bedmar, Martınez, Herrero-Zazo, 2013</marker>
<rawString>Isabel Segura-Bedmar, Paloma Martınez, and Marıa Herrero-Zazo. 2013. Semeval-2013 task 9: Extraction of drug-drug interactions from biomedical texts. In Proceedings of the 7th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="3409" citStr="Snow et al., 2004" startWordPosition="508" endWordPosition="511"> of instances that they can classify. As an alternative to requiring co-occurrence, other works have classified the relation of a word pair using lexical similarity, i.e., the similarity of the concepts themselves. Given two word pairs, (w1, w2) and (w3, w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately c</context>
<context position="5851" citStr="Snow et al., 2004" startWordPosition="872" endWordPosition="875">target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming the</context>
<context position="13875" citStr="Snow et al. (2004)" startWordPosition="2186" endWordPosition="2189"> of the corpus, a set of dependency relations linking the words in it is produced: D = {d1 ..., d|D|}, where d = (wi, dep, wj) and wi, wj and dep denote POStagged lemmas and a dependency relation, respectively. The graph ● is created using all the dependency relations from D. The output of this step is a multigraph, where two words are connected by the set of edges containing all the dependency relations holding between them in the corpus. Feature Selection The goal of the second step is to collect features associated with each relation r from the parsed input corpus. Similarly to the work of Snow et al. (2004), our features are patterns of co-occurrence contexts created with dependency paths. For acquiring patterns of co-occurrence contexts for each relation r, we use the set of labeled examples E, assuming that all the contexts in which a word pair (x, y)i ∈ E co-occurs provide information about the relation r holding between its members. All the dependency paths between x and y up to three edges are extracted from the dependency graph of each sentence where (x, y)i cooccur.1 For example, ((hammerN, toolN), hyper) is an instance of the relation of hypernymy. In the dependency graph of sentence (S1</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="3644" citStr="Turney, 2006" startWordPosition="540" endWordPosition="541">, w2) and (w3, w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations</context>
<context position="6642" citStr="Turney, 2006" startWordPosition="996" endWordPosition="997">eir relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification r</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006a. Expressing implicit semantic relations without supervision. In Proceedings of COLING-ACL, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="3644" citStr="Turney, 2006" startWordPosition="540" endWordPosition="541">, w2) and (w3, w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations</context>
<context position="6642" citStr="Turney, 2006" startWordPosition="996" endWordPosition="997">eir relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification r</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D Turney. 2006b. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>The latent relation mapping engine: Algorithm and experiments.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research (JAIR),</journal>
<pages>33--615</pages>
<contexts>
<context position="2222" citStr="Turney, 2008" startWordPosition="321" endWordPosition="323">ntic database, a first step is to develop accurate techniques for classifying the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can cl</context>
<context position="3659" citStr="Turney, 2008" startWordPosition="542" endWordPosition="543">w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph enc</context>
<context position="6192" citStr="Turney, 2008" startWordPosition="928" endWordPosition="929">nd time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be im</context>
<context position="22507" citStr="Turney, 2008" startWordPosition="3599" endWordPosition="3600">pact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns using the template [0 to 1 words] x [0 to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. T</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D Turney. 2008a. The latent relation mapping engine: Algorithm and experiments. Journal ofArtificial Intelligence Research (JAIR), 33:615–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>A uniform approach to analogies, synonyms, antonyms, and associations.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>905--912</pages>
<contexts>
<context position="2222" citStr="Turney, 2008" startWordPosition="321" endWordPosition="323">ntic database, a first step is to develop accurate techniques for classifying the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can cl</context>
<context position="3659" citStr="Turney, 2008" startWordPosition="542" endWordPosition="543">w4), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect in182 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph enc</context>
<context position="6192" citStr="Turney, 2008" startWordPosition="928" endWordPosition="929">nd time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial representation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be im</context>
<context position="22507" citStr="Turney, 2008" startWordPosition="3599" endWordPosition="3600">pact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIRCLASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation types. Using a set of seed pairs (x, y) for each relation, PairClass acquires a set of lexical patterns using the template [0 to 1 words] x [0 to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. T</context>
</contexts>
<marker>Turney, 2008</marker>
<rawString>Peter D. Turney. 2008b. A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of COLING, pages 905–912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="5130" citStr="Widdows and Dorow, 2002" startWordPosition="761" endWordPosition="764"> preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed</context>
<context position="7465" citStr="Widdows and Dorow, 2002" startWordPosition="1116" endWordPosition="1120">ds. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to fi</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows and Beate Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alisa Zhila</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
<author>Geoffrey Zweig</author>
<author>Tomas Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>1000--1009</pages>
<contexts>
<context position="4815" citStr="Zhila et al., 2013" startWordPosition="718" endWordPosition="721"> of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, dependin</context>
<context position="23346" citStr="Zhila et al. (2013)" startWordPosition="3743" endWordPosition="3746"> PairClass acquires a set of lexical patterns using the template [0 to 1 words] x [0 to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. The patterns retained are then used as features to train an SVM classifier over the set of possible relation types. DSZhila &amp; DSLevy Word embeddings have previously been shown to accurately measure relational similarity; Zhila et al. (2013) demonstrate state-ofthe-art performance on SemEval-2012 Task 2 (Jurgens et al., 2012) which measures word pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highes</context>
</contexts>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey Zweig, and Tomas Mikolov. 2013. Combining heterogeneous models for measuring relational similarity. In Proceedings of HLT-NAACL, pages 1000–1009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>