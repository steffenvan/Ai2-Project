<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023288">
<title confidence="0.981387">
Multimodal Annotation of Conversational Data
</title>
<author confidence="0.983298666666667">
P. Blache&apos;, R. Bertrand&apos;, B. Bigi&apos;, E. Bruno&apos;, E. Cela6, R. Espesser&apos;, G. Ferré�, M. Guardiola&apos;, D. Hirst&apos;,
E.-P. Magro6, J.-C. Martine, C. Meunier&apos;, M.-A. Morel6, E. Murisasco&apos;, I Nesterenko&apos;, P. Nocera5,
B. Pallaud&apos;, L. Prévot&apos;, B. Priego-Valverde&apos;, J. Seinturier&apos;, N. Tane, M. Tellier&apos;, S. Rauzy&apos;
</author>
<listItem confidence="0.933546">
(1) LPL-CNRS-Université de Provence (2) LIMSI-CNRS-Université Paris Sud
(3) LSIS-CNRS-Université de Toulon (4) LLING-Université de Nantes
(5) LIA-Université d’Avignon (6) RFC-Université Paris 3
</listItem>
<email confidence="0.595491">
blache@lpl-aix.fr
</email>
<sectionHeader confidence="0.980603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922818181818">
We propose in this paper a broad-coverage
approach for multimodal annotation of
conversational data. Large annotation pro-
jects addressing the question of multimo-
dal annotation bring together many dif-
ferent kinds of information from different
domains, with different levels of granula-
rity. We present in this paper the first re-
sults of the OTIM project aiming at deve-
loping conventions and tools for multimo-
dal annotation.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990391304348">
We present in this paper the first results of the
OTIM1 project aiming at developing conventions
and tools for multimodal annotation. We show
here how such an approach can be applied in the
annotation of a large conversational speech cor-
pus.
Before entering into more details, let us men-
tion that our data, tools and conventions are des-
cribed and freely downlodable from our website
(http ://www.lpl-aix.fr/ otim/).
The annotation process relies on several tools
and conventions, most of them elaborated within
the framework of the project. In particular, we pro-
pose a generic transcription convention, called En-
riched Orthographic Trancription, making it pos-
sible to annotate all specific pronunciation and
speech event, facilitating signal alignment. Dif-
ferent tools have been used in order to prepare
or directly annotate the transcription : grapheme-
phoneme converter, signal alignment, syllabifica-
tion, prosodic analysis, morpho-syntactic analysis,
chunking, etc. Our ambition is to propose a large
corpus, providing rich annotations in all the dif-
</bodyText>
<footnote confidence="0.987777333333333">
1OTIM stands for Outils pour le Traitement de l’Informa-
tion Multimodale (Tools for Multimodal Annotation). This
project in funded by the French ANR agency.
</footnote>
<bodyText confidence="0.998493">
ferent linguistic domains, from prosody to gesture.
We describe in the following our first results.
</bodyText>
<sectionHeader confidence="0.993165" genericHeader="introduction">
2 Annotations
</sectionHeader>
<bodyText confidence="0.9966226">
We present in this section some of the annota-
tions of a large conversational corpus, called CID
(Corpus of Interactional Data, see (Bertrand08)),
consisting in 8 dialogues, with audio and video si-
gnal, each lasting 1 hour.
Transcription : The transcription process is
done following specific conventions derived from
that of the GARS (Blanche-Benveniste87). The
result is what we call an enriched orthographic
construction, from which two derived transcrip-
tions are generated automatically : the standard or-
thographic transcription (the list of orthographic
tokens) and a specific transcription from which
the phonetic tokens are obtained to be used by the
grapheme-phoneme converter.
From the phoneme sequence and the audio si-
gnal, the aligner outputs for each phoneme its
time localization. This aligner (Brun04) is HMM-
based, it uses a set of 10 macro-classes of vowel
(7 oral and 3 nasal), 2 semi-vowels and 15 conso-
nants. Finally, from the time aligned phoneme se-
quence plus the EOT, the orthographic tokens is
time-aligned.
Syllables : The corpus was automatically seg-
mented in syllables. Sub-syllabic constituents (on-
set, nucleus and coda) are then identified as well
as the syllable structure (V, CV, CCV, etc.). Sylla-
bic position is specified in the case of polysyllabic
words.
Prosodic phrasing : Prosodic phrasing refers
to the structuring of speech material in terms of
boundaries and groupings. Our annotation scheme
supposes the distinction between two levels of
phrasing: the level of accentual phrases (AP, (Jun,
2002)) and the higher level of intonational phrases
</bodyText>
<page confidence="0.9843">
186
</page>
<note confidence="0.392406">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 186–191,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.988954987951808">
(IP). Mean annotation time for IPs and APs was
30 minutes per minute.
Prominence : The prominence status of a syl-
lable distinguishes between accentuability (the
possibility for syllable to be prominent) and pro-
minence (at the perception level). In French the
first and last full syllables (not containing a
schwa) of a polysyllabic word can be prominent,
though this actual realization depends on spea-
kers choices. Accentuability annotation is auto-
matic while prominence annotation is manual and
perceptually based.
Tonal layer : Given a lack of consensus on the
inventory of tonal accents in French, we choose to
integrate in our annotation scheme three types of
tonal events : a/ underlying tones (for an eventual
FrenchToBI annotation) ; b/ surface tones (anno-
tated in terms of MOMel-Intsint protocol Hirst et
al 2000) ; c/ melodic contours (perceptually anno-
tated pitch movements in terms of their form and
function). The interest to have both manual and
automatic INTSINT annotations is that it allows
the study of their links.
Hand gestures : The formal model we use for
the annotation of hand gestures is adapted from
the specification files created by Kipp (2004) and
from the MUMIN coding scheme (Allwood et al.,
2005). Among the main gesture types, we anno-
tate iconics, metaphoric, deictics, beats, emblems,
butterworths or adaptors.
We used the Anvil tool (Kipp, 2004) for the ma-
nual annotations. We created a specification files
taking into account the different information types
and the addition of new values adapted to the
CID corpus description (e.g. we added a separate
track Symmetry). For each hand, the scheme has 10
tracks. We allowed the possibility of a gesture per-
taining to several semiotic types using a boolean
notation. A gesture phrase (i.e. the whole gesture)
can be decomposed into several gesture phases i.e.
the different parts of a gesture such as the prepara-
tion, the stroke (the climax of the gesture), the hold
and the retraction (when the hands return to their
rest position) (McNeill, 1992). The scheme also
enables to annotate gesture lemmas (Kipp, 2004),
the shape and orientation of the hand during the
stroke, the gesture space, and contact. We added
the three tracks to code the hand trajectory, ges-
ture velocity and gesture amplitude.
Discourse and Interaction : Our discourse an-
notation scheme relies on multidimensional fra-
meworks such as DIT++ (Bunt, 2009) and is com-
patible with the guidelines defined by the Semantic
Annotation Framework (Dialogue Act) working
group of ISO TC37/4.
Discourse units include information about their
producer, have a form (clause, fragment, dis-
fluency, non-verbal), a content and a communi-
cative function. The same span of raw data may
be covered by several discourse units playing dif-
ferent communicative functions. Two discourse
units may even have exactly the same temporal ex-
tension, due to the multifonctionality that cannot
be avoided (Bunt, 2009).
Compared to standard dialogue act annotation
frameworks, three main additions are proposed :
rhetorical function, reported speech and humor.
Our rhetorical layer is an adaptation of an exis-
ting schema developed for monologic written data
in the context of the ANNODIS project.
Disfluencies : Disfluencies are organized
around an interruption point, which can occur al-
most anywhere in the production. Disfluencies can
be prosodic (lenghtenings, silent and filled pauses,
etc.), or lexicalized. In this case, they appear as a
word or a phrase truncation, that can be comple-
ted. We distinguish three parts in a disfluency (see
(Shriberg, 1994), (Blanche-Benveniste87)) :
– Reparandum : what precedes the interruption
point. This part is mandatory in all disfluen-
cies. We indicate there the nature of the inter-
rupted unit (word or phrase), and the type of
the truncated word (lexical or grammatical) ;
</bodyText>
<listItem confidence="0.596804125">
– Break interval. It is optional, some disfluen-
cies do not bear any specific event there.
– Reparans : the part following the break, repai-
ring the reparandum. We indicate there type
of the repair (no restart, word restart, determi-
ner restart, phrase restart, etc.), and its func-
tion (continuation, repair without change, re-
pair with change, etc.).
</listItem>
<sectionHeader confidence="0.995058" genericHeader="method">
3 Quantitative information
</sectionHeader>
<bodyText confidence="0.999918166666667">
We give in this section some indication about
the state of development of the CID annotation.
Hand gestures : 75 minutes involving 6 spea-
kers have been annotated, yielding a total number
of 1477 gestures. The onset and offset of gestures
correspond to the video frames, starting from and
</bodyText>
<page confidence="0.989904">
187
</page>
<bodyText confidence="0.999527724137931">
going back to a rest position.
Face and gaze : At the present time, head move-
ments, gaze directions and facial expressions have
been coded in 15 minutes of speech yielding a to-
tal number of 1144 movements, directions and ex-
pressions, to the exclusion of gesture phases. The
onset and offset of each tag are determined in the
way as for hand gestures.
Body Posture : Our annotation scheme consi-
ders, on top of chest movements at trunk level,
attributes relevant to sitting positions (due to the
specificity of our corpus). It is based on the Pos-
ture Scoring System (Bull, 1987) and the Annota-
tion Scheme for Conversational Gestures (Kipp et
al., 2007). Our scheme covers four body parts :
arms, shoulders, trunk and legs. Seven dimensions
at arm level and six dimensions at leg level, as well
as their related reference points we take in fixing
the spatial location, are encoded.
Moreover, we added two dimensions to describe
respectively the arm posture in the sagittal plane
and the palm orientation of the forearm and the
hand. Finally, we added three dimensions for leg
posture: height, orientation and the way in which
the legs are crossed in sitting position.
We annotated postures on 15 minutes of the cor-
pus involving one pair of speakers, leading to 855
tags with respect to 15 different spatial location
dimensions of arms, shoulder, trunk and legs.
</bodyText>
<table confidence="0.999722125">
Annotation Time (min.) Units
Transcript 480 -
Hands 75 1477
Face 15 634
Gaze 15 510
Posture 15 855
R. Speech 180
Com. Function 6 229
</table>
<bodyText confidence="0.9965209">
Disfluencies At the moment, this annotation is
fully manual (we just developed a tool helping the
process in identifying disfluencies, but it has not
yet been evaluated). Annotating this phenomenon
requires 15mns for 1 minute of the corpus. The
following table illustrates the fact that disfluen-
cies are speaker-dependent in terms of quantity
and type. These figures also shows that disfluen-
cies affect lexicalized words as well as grammati-
cal ones.
</bodyText>
<table confidence="0.9980535">
Speaker_1 Speaker_1
Total number of words 1,434 1,304
Disfluent grammatical words 17 54
Disfluent lexicalized words 18 92
Truncated words 7 12
Truncated phrases 26 134
</table>
<bodyText confidence="0.995764666666667">
Transcription and phonemes The following
table recaps the main figures about the different
specific phenomena annotated in the EOT. To the
best of our knowledge, these data are the first of
this type obtained on a large corpus. This informa-
tion is still to be analyzed.
</bodyText>
<table confidence="0.9996751">
Phenomenon Number
Elision 11,058
Word truncation 1,732
Standard liaison missing 160
Unusual liaison 49
Non-standard phonetic realization 2,812
Laugh seq. 2,111
Laughing speech seq. 367
Single laugh IPU 844
Overlaps &gt; 150 ms 4,150
</table>
<bodyText confidence="0.998838866666667">
Syntax We used the stochastic parser developed
at the LPL (Blache&amp;Rauzy, 2008) to automaticaly
generate morppho-syntactic and syntactic annota-
tions. The parser has been adapted it in order to ac-
count for the specificities of speech analysis. First,
the system implements a segmentation technique,
identifying large syntactic units that can be consi-
dered as the equivalent of sentences in written
texts. This technique distinguishes between strong
and weak or soft punctuation marks. A second mo-
dification concerns the lexical frequencies used by
the parser model in order to capture phenomena
proper to conversational data.
The categories and chunks counts for the whole
corpus are summarized in the following figure:
</bodyText>
<table confidence="0.9980155">
Category Count Group Count
adverb 15123 AP 3634
adjective 4585 NP 13107
auxiliary 3057 PP 7041
determiner 9427 AdvP 15040
conjunction 9390 VPn 22925
interjection 5068 VP 1323
preposition 8693 Total 63070
pronoun 25199
noun 13419 Soft Pct 9689
verb 20436 Strong Pct 14459
Total 114397 Total 24148
</table>
<sectionHeader confidence="0.99878" genericHeader="method">
4 Evaluations
</sectionHeader>
<bodyText confidence="0.996041833333333">
Prosodic annotation : Prosodic annotation of
1 dialogue has been done by 2 experts. The
annotators worked separately using Praat. Inter-
transcriber agreement studies were done for the
annotation of higher prosodic units. First anno-
tator marked 3,159 and second annotator 2,855
</bodyText>
<page confidence="0.997247">
188
</page>
<bodyText confidence="0.999848516129032">
Intonational Phrases. Mean percentage of inter-
transcriber agreement was 91.4% and mean
kappa-statistics 0.79, which stands for a quite sub-
stantial agreement.
Gesture : We performed a measure of inter-
reliability for three independent coders for Gesture
Space. The measure is based on Cohen’s correc-
ted kappa coefficient for the validation of coding
schemes (Carletta96).
Three coders have annotated three minutes for
GestureSpace including GestureRegion and Ges-
tureCoordinates. The kappa values indicated that
the agreement is high for GestureRegion of right
hand (kappa = 0.649) and left hand (kappa =
0.674). However it is low for GestureCoordinates
of right hand (k= 0.257) and left hand (k= 0.592).
Such low agreement of GestureCoordinates might
be due to several factors. First, the number of ca-
tegorical values is important.
Second, three minutes might be limited in terms
of data to run a kappa measure. Third, GestureRe-
gion affects GestureCoordinates : if the coders di-
sagree about GestureRegion, they are likely to also
annotate GestureCoordinates in a different way.
For instance, it was decided that no coordinate
would be selected for a gesture in the center-center
region, whereas there is a coordinate value for ges-
tures occurring in other parts of the GestureRe-
gion. This means that whenever coders disagree
between the center-center or center region, the an-
notation of the coordinates cannot be congruent.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="method">
5 Information representation
</sectionHeader>
<subsectionHeader confidence="0.975873">
5.1 XML encoding
</subsectionHeader>
<bodyText confidence="0.999928952380952">
Our approach consists in first precisely define
the organization of annotations in terms of typed-
feature structures. We obtain an abstract descrip-
tion from which we automatically generate a for-
mal schema in XML. All the annotations are then
encoded following this schema.
Our XML schema, besides a basic encoding of
data following AIF, encode all information concer-
ning the organization as well as the constraints on
the structures. In the same way as TFS are used
as a tree description language in theories such as
HPSG, the XML schema generated from our TFS
representation also plays the same role with res-
pect to the XML annotation data file. On the one
hand, basic data are encoded with AIF, on the
other hand, the XML schema encode all higher
level information. Both components (basic data +
structural constraints) guarantee against informa-
tion loss that otherwise occurs when translating
from one coding format to another (for example
from Anvil to Praat).
</bodyText>
<subsectionHeader confidence="0.99946">
5.2 Querying
</subsectionHeader>
<bodyText confidence="0.999988724137931">
To ease the multimodal exploitation of the data,
our objective is to provide a set of operators dedi-
cated to concurrent querying on hierarchical an-
notation. Concurrent querying consists in que-
rying annotations belonging to two or more mo-
dalities or even in querying the relationships bet-
ween modalities. For instance, we want to be able
to express queries over gestures and intonation
contours (what kind of intonational contour does
the speaker use when he looks at the listener?).
We also want to be able to query temporal relation-
ships (in terms of anticipation, synchronization or
delay) between both gesture strokes and lexical af-
filiates.
Our proposal is to define these operators as an
extension of XQuery. From the XML encoding
and the temporal alignment of annotated data, it
will possible to express queries to find patterns and
to navigate in the structure. We also want to en-
able a user to check predicates on parts of the cor-
pus using classical criteria on values, annotations
and existing relationships (temporal or structural
ones corresponding to inclusions or overlaps bet-
ween annotations). First, we shall rely on one of
our previous proposal called MSXD (MultiStruc-
tured XML Document). It is a XML-compatible
model designed to describe and query concurrent
hierarchical structures defined over the same tex-
tual data which supports Allen’s relations.
</bodyText>
<sectionHeader confidence="0.995664" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999722583333333">
Multimodal annotation is often reduced to
the encoding of gesture, eventually accompa-
nied with another level of linguistic information
(e.g. morpho-syntax). We reported in this paper a
broad-coverage approach, aiming at encoding all
the linguistic domains into a unique framework.
We developed for this a set of conventions and
tools making it possible to bring together and align
all these different pieces of information. The result
is the CID (Corpus of Interactional Data), the first
large corpus of conversational data bearing rich
annotations on all the linguistic domains.
</bodyText>
<page confidence="0.998886">
189
</page>
<sectionHeader confidence="0.984191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998607072580645">
Allen J. (1999) Time and time again: The many way to re-
present time. International Journal of Intelligent Systems,
6(4)
Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navar-
retta, C., Paggio, P. (2005) The MUMIN Multimodal Co-
ding Scheme, NorFA yearbook 2005.
Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P.
F. Patel-Schneider (2003) The Description Logic Hand-
book: Theory, Implementation, Applications. Cambridge
University Press.
Bertrand, R., Blache, P., Espesser, R., Ferr6, G., Meunier, C.,
Priego-Valverde, B., Rauzy, S. (2008) “Le CID - Corpus
of Interactional Data - Annotation et Exploitation Multi-
modale de Parole Conversationnelle”, in revue Traitement
Automatique des Langues, 49 :3.
Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010. “Syllable
Boundaries Automatic Detection in Spontaneous Speech”,
in proceedings of LREC 2010.
Blache P. and Rauzy S. 2008. “Influence de la qualit6 de
l’6tiquetage sur le chunking : une corr6lation d6pendant de
la taille des chunks”. in proceedings of TALN 2008 (Avi-
gnon, France), pp. 290-299.
Blache P., R. Bertrand, and G. Ferr6 2009. “Creating and
Exploiting Multimodal Annotated Corpora : The ToMA
Project”. In Multimodal Corpora : From Models of Natu-
ral Interaction to Systems and Applications, Springer.
Blanche-Benveniste C. &amp; C. Jeanjean (1987) Le français
parlé. Transcription et édition, Didier Erudition.
Blanche-Benveniste C. 1987. “Syntaxe, choix du lexique et
lieux de bafouillage”, in DRLAV 36-37
Browman C. P. and L. Goldstein. 1989. “Articulatory ges-
tures as phonological units”. In Phonology 6, 201-252
Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O.
&amp; Smaili K. (2004- “Ants : Le systÃ´lme de transcription
automatique du Loria”, Actes des XXV Journées d’Etudes
sur la Parole, Fès.
E. Bruno, E. Murisasco (2006) Describing and Querying hie-
rarchical structures defined over the same textual data, in
Proceedings of the ACM Symposium on Document Engi-
neering (DocEng 2006).
Bull, P. (1987) Posture and Gesture, Pergamon Press.
Bunt H. 2009. “Multifunctionality and multidimensional
dialogue semantics.” In Proceedings of DiaHolmia’09,
SEMDIAL.
Bürki A., C. Gendrot, G. Gravier &amp; al.(2008) “Alignement
automatique et analyse phon6tique : comparaison de dif-
f6rents systèmes pour l’analyse du schwa”, in revue TAL
,49 :3
Carletta, J. (1996) “Assessing agreement on classification
tasks : The kappa statistic”, in Computational Linguistics
22.
Corlett, E. N., Wilson,John R. Manenica. I. (1986) “Influence
Parameters and Assessment Methods for Evaluating Body
Postures”, in Ergonomics of Working Postures : Models,
Methods and Cases , Proceedings of the First International
Occupational Ergonomics Symposium.
Di Cristo &amp; Hirst D. (1996) “Vers une typologie des unites in-
tonatives du français”, XXIème JEP, 219-222, 1996, Avi-
gnon, France
Di Cristo A. &amp; Di Cristo P. (2001) “Syntaix, une approche
m6trique-autosegmentale de la prosodie”, in revue Traite-
ment Automatique des Langues, 42 :1.
Dipper S., M. Goetze and S. Skopeteas (eds.) 2007. Informa-
tion Structure in Cross-Linguistic Corpora : Annotation
Guidelines, Working Papers of the SFB 632, 7 :07
FGNet Second Foresight Report (2004) Face
and Gesture Recognition Working Group.
http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rd-
fgnet-foresight-workshop.pdf
Gendner V. et al. 2003. “PEAS, the first instantiation of a
comparative framework for evaluating parsers of French”.
in Research Notes of EACL 2003 (Budapest, Hungaria).
Hawkins S. and N. Nguyen 2003. “Effects on word re-
cognition of syllable-onset cues to syllable-coda voicing”,
in Papers in Laboratory Phonology VI. Cambridge Univ.
Press.
Hirst, D., Di Cristo, A., Espesser, R. 2000. “Levels of des-
cription and levels of representation in the analysis of in-
tonation”, in Prosody : Theory and Experiment, Kluwer.
Hirst, D.J. (2005) “Form and function in the representation
of speech prosody”, in K.Hirose, D.J.Hirst &amp; Y.Sagisaka
(eds) Quantitative prosody modeling for natural speech
description and generation (Speech Communication 46 :3-
4.
Hirst, D.J. (2007) “A Praat plugin for Momel and INTSINT
with improved algorithms for modelling and coding into-
nation”, in Proceedings of the XVIth International Confe-
rence of Phonetic Sciences.
Hirst, D. (2007), Plugin Momel-Intsint. Inter-
net : http ://uk.groups.yahoo.com/group/praat-
users/files/Daniel_Hirst/plugin_momel-intsint.zip,
Boersma, Weenink, 2007.
Jun, S.-A., Fougeron, C. 2002. “Realizations of accentual
phrase in French intonation”, in Probus 14.
Kendon, A. (1980) “Gesticulation and Speech: Two Aspects
of the Porcess of Utterance”, in M.R. Key (ed.), The Re-
lationship of Verbal and Nonverbal Communication, The
Hague: Mouton.
Kita, S., Ozyurek, A. (2003) “What does cross-linguistic va-
riation in semantic coordination of speech and gesture re-
veal ? Evidence for an interface representation of spatial
thinking and speaking”, in Journal of Memory and Lan-
guage, 48.
Kipp, M. (2004). Gesture Generation by Imitation - From
Human Behavior to Computer Character Animation. Boca
Raton, Florida, Dissertation.com.
Kipp, M., Neff, M., Albrecht, I. (2007). An annotation
scheme for conversational gestures : how to economically
capture timing and form. Language Resources and Eva-
luation, 41(3).
Koiso H., Horiuchi Y., Ichikawa A. &amp; Den Y.(1998) “An ana-
lysis of turn-taking and backchannels based on prosodic
and syntactic features in Japanese map task dialogs”, in
Language and Speech, 41.
McNeill, D. (1992). Hand and Mind. What Gestures Re-
veal about Thought, Chicago : The University of Chicago
Press.
McNeill, D. (2005). Gesture and Thought, Chicago, London:
The University of Chicago Press.
Milborrow S., F. Nicolls. (2008). Locating Facial Features
with an Extended Active Shape Model. ECCV (4).
Nesterenko I. (2006) “Corpus du parler russe spontan6 : an-
notations et observations sur la distribution des frontières
prosodiques”, in revue TIPA, 25.
</reference>
<page confidence="0.974059">
190
</page>
<reference confidence="0.995593428571429">
Paroubek P. et al. 2006. “Data Annotations and Measures in
EASY the Evaluation Campaign for Parsers in French”. in
proceedings of the 5th international Conference on Lan-
guage Resources and Evaluation 2006 (Genoa, Italy), pp.
314-320.
Pierrehumbert &amp; Beckman (1988) Japanese Tone Structure.
Coll. Linguistic Inquiry Monographs, 15. Cambridge,
MA, USA: The MIT Press.
Platzer, W., Kahle W. (2004) Color Atlas and Textbook of
Human Anatomy, Thieme. Project MuDis. Technische
Universitat Munchen. http ://www9.cs.tum.edu/research
Scherer, K.R., Ekman, P. (1982) Handbook of methods in
nonverbal behavior research. Cambridge University Press.
Shriberg E. 1994. Preliminaries to a theory of speech dis-
fluencies. PhD Thesis, University of California, Berkeley
Wallhoff F., M. Ablassmeier, and G. Rigoll. (2006) “Mul-
timodal Face Detection, Head Orientation and Eye Gaze
Tracking”, in proceedings of International Conference on
Multisensor Fusion and Integration (MFI).
White, T. D., Folkens, P. A. (1991) Human Osteology. San
Diego : Academic Press, Inc.
</reference>
<page confidence="0.998471">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.138521">
<title confidence="0.999044">Multimodal Annotation of Conversational Data</title>
<author confidence="0.697248">R B E E R G M D</author>
<note confidence="0.679306">J.-C. C. M.-A. E. I P. L. B. J. N. M. S. (1) LPL-CNRS-Université de Provence (2) LIMSI-CNRS-Université Paris Sud (3) LSIS-CNRS-Université de Toulon (4) LLING-Université de Nantes (5) LIA-Université d’Avignon (6) RFC-Université Paris</note>
<email confidence="0.910991">blache@lpl-aix.fr</email>
<abstract confidence="0.9841915">We propose in this paper a broad-coverage approach for multimodal annotation of conversational data. Large annotation projects addressing the question of multimodal annotation bring together many different kinds of information from different domains, with different levels of granularity. We present in this paper the first results of the OTIM project aiming at developing conventions and tools for multimodal annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
</authors>
<title>Time and time again: The many way to represent time.</title>
<date>1999</date>
<journal>International Journal of Intelligent Systems,</journal>
<volume>6</volume>
<issue>4</issue>
<marker>Allen, 1999</marker>
<rawString>Allen J. (1999) Time and time again: The many way to represent time. International Journal of Intelligent Systems, 6(4)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allwood</author>
<author>L Cerrato</author>
<author>L Dybkjaer</author>
<author>K Jokinen</author>
<author>C Navarretta</author>
<author>P Paggio</author>
</authors>
<title>The MUMIN Multimodal Coding Scheme, NorFA yearbook</title>
<date>2005</date>
<contexts>
<context position="5302" citStr="Allwood et al., 2005" startWordPosition="812" endWordPosition="815">oose to integrate in our annotation scheme three types of tonal events : a/ underlying tones (for an eventual FrenchToBI annotation) ; b/ surface tones (annotated in terms of MOMel-Intsint protocol Hirst et al 2000) ; c/ melodic contours (perceptually annotated pitch movements in terms of their form and function). The interest to have both manual and automatic INTSINT annotations is that it allows the study of their links. Hand gestures : The formal model we use for the annotation of hand gestures is adapted from the specification files created by Kipp (2004) and from the MUMIN coding scheme (Allwood et al., 2005). Among the main gesture types, we annotate iconics, metaphoric, deictics, beats, emblems, butterworths or adaptors. We used the Anvil tool (Kipp, 2004) for the manual annotations. We created a specification files taking into account the different information types and the addition of new values adapted to the CID corpus description (e.g. we added a separate track Symmetry). For each hand, the scheme has 10 tracks. We allowed the possibility of a gesture pertaining to several semiotic types using a boolean notation. A gesture phrase (i.e. the whole gesture) can be decomposed into several gestu</context>
</contexts>
<marker>Allwood, Cerrato, Dybkjaer, Jokinen, Navarretta, Paggio, 2005</marker>
<rawString>Allwood, J., Cerrato, L., Dybkjaer, L., Jokinen, K., Navarretta, C., Paggio, P. (2005) The MUMIN Multimodal Coding Scheme, NorFA yearbook 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Baader</author>
<author>D Calvanese</author>
<author>D L McGuinness</author>
<author>D Nardi</author>
<author>P F Patel-Schneider</author>
</authors>
<title>The Description Logic Handbook: Theory, Implementation, Applications.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<marker>Baader, Calvanese, McGuinness, Nardi, Patel-Schneider, 2003</marker>
<rawString>Baader F., D. Calvanese, D. L. McGuinness, D. Nardi, P. F. Patel-Schneider (2003) The Description Logic Handbook: Theory, Implementation, Applications. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bertrand</author>
<author>P Blache</author>
<author>R Espesser</author>
<author>G Ferr6</author>
<author>C Meunier</author>
<author>B Priego-Valverde</author>
<author>S Rauzy</author>
</authors>
<date>2008</date>
<booktitle>Le CID - Corpus of Interactional Data - Annotation et Exploitation Multimodale de Parole Conversationnelle”, in revue Traitement Automatique des Langues,</booktitle>
<volume>49</volume>
<pages>3</pages>
<marker>Bertrand, Blache, Espesser, Ferr6, Meunier, Priego-Valverde, Rauzy, 2008</marker>
<rawString>Bertrand, R., Blache, P., Espesser, R., Ferr6, G., Meunier, C., Priego-Valverde, B., Rauzy, S. (2008) “Le CID - Corpus of Interactional Data - Annotation et Exploitation Multimodale de Parole Conversationnelle”, in revue Traitement Automatique des Langues, 49 :3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Meunier Bigi</author>
<author>I Nesterenko</author>
<author>R Bertrand</author>
</authors>
<title>Syllable Boundaries Automatic Detection in Spontaneous Speech”,</title>
<date>2010</date>
<booktitle>in proceedings of LREC</booktitle>
<marker>Bigi, Nesterenko, Bertrand, 2010</marker>
<rawString>Bigi, C. Meunier, I. Nesterenko, R. Bertrand 2010. “Syllable Boundaries Automatic Detection in Spontaneous Speech”, in proceedings of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blache</author>
<author>S Rauzy</author>
</authors>
<title>Influence de la qualit6 de l’6tiquetage sur le chunking : une corr6lation d6pendant de la taille des chunks”.</title>
<date>2008</date>
<booktitle>in proceedings of TALN</booktitle>
<pages>290--299</pages>
<location>Avignon,</location>
<marker>Blache, Rauzy, 2008</marker>
<rawString>Blache P. and Rauzy S. 2008. “Influence de la qualit6 de l’6tiquetage sur le chunking : une corr6lation d6pendant de la taille des chunks”. in proceedings of TALN 2008 (Avignon, France), pp. 290-299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blache</author>
<author>R Bertrand</author>
<author>G Ferr6</author>
</authors>
<title>Creating and Exploiting Multimodal Annotated Corpora : The ToMA Project”.</title>
<date>2009</date>
<booktitle>In Multimodal Corpora : From Models of Natural Interaction to Systems and Applications,</booktitle>
<publisher>Springer.</publisher>
<marker>Blache, Bertrand, Ferr6, 2009</marker>
<rawString>Blache P., R. Bertrand, and G. Ferr6 2009. “Creating and Exploiting Multimodal Annotated Corpora : The ToMA Project”. In Multimodal Corpora : From Models of Natural Interaction to Systems and Applications, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Blanche-Benveniste</author>
<author>C Jeanjean</author>
</authors>
<title>Le français parlé. Transcription et édition, Didier Erudition.</title>
<date>1987</date>
<marker>Blanche-Benveniste, Jeanjean, 1987</marker>
<rawString>Blanche-Benveniste C. &amp; C. Jeanjean (1987) Le français parlé. Transcription et édition, Didier Erudition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Blanche-Benveniste</author>
</authors>
<title>Syntaxe, choix du lexique et lieux de bafouillage”,</title>
<date>1987</date>
<booktitle>in DRLAV</booktitle>
<pages>36--37</pages>
<marker>Blanche-Benveniste, 1987</marker>
<rawString>Blanche-Benveniste C. 1987. “Syntaxe, choix du lexique et lieux de bafouillage”, in DRLAV 36-37</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Browman</author>
<author>L Goldstein</author>
</authors>
<title>Articulatory gestures as phonological units”.</title>
<date>1989</date>
<journal>In Phonology</journal>
<volume>6</volume>
<pages>201--252</pages>
<marker>Browman, Goldstein, 1989</marker>
<rawString>Browman C. P. and L. Goldstein. 1989. “Articulatory gestures as phonological units”. In Phonology 6, 201-252</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brun</author>
<author>C Cerisara</author>
<author>D Fohr</author>
<author>I Illina</author>
<author>D Langlois</author>
<author>O Mella</author>
<author>K Smaili</author>
</authors>
<title>Ants : Le systÃ´lme de transcription automatique du Loria”, Actes des XXV Journées d’Etudes sur la</title>
<date>2004</date>
<location>Parole, Fès.</location>
<marker>Brun, Cerisara, Fohr, Illina, Langlois, Mella, Smaili, 2004</marker>
<rawString>Brun A., Cerisara C., Fohr D., Illina I., Langlois D., Mella O. &amp; Smaili K. (2004- “Ants : Le systÃ´lme de transcription automatique du Loria”, Actes des XXV Journées d’Etudes sur la Parole, Fès.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bruno</author>
<author>E Murisasco</author>
</authors>
<title>Describing and Querying hierarchical structures defined over the same textual data,</title>
<date>2006</date>
<booktitle>in Proceedings of the ACM Symposium on Document Engineering (DocEng</booktitle>
<marker>Bruno, Murisasco, 2006</marker>
<rawString>E. Bruno, E. Murisasco (2006) Describing and Querying hierarchical structures defined over the same textual data, in Proceedings of the ACM Symposium on Document Engineering (DocEng 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bull</author>
</authors>
<title>Posture and Gesture,</title>
<date>1987</date>
<publisher>Pergamon Press.</publisher>
<contexts>
<context position="9152" citStr="Bull, 1987" startWordPosition="1443" endWordPosition="1444">ideo frames, starting from and 187 going back to a rest position. Face and gaze : At the present time, head movements, gaze directions and facial expressions have been coded in 15 minutes of speech yielding a total number of 1144 movements, directions and expressions, to the exclusion of gesture phases. The onset and offset of each tag are determined in the way as for hand gestures. Body Posture : Our annotation scheme considers, on top of chest movements at trunk level, attributes relevant to sitting positions (due to the specificity of our corpus). It is based on the Posture Scoring System (Bull, 1987) and the Annotation Scheme for Conversational Gestures (Kipp et al., 2007). Our scheme covers four body parts : arms, shoulders, trunk and legs. Seven dimensions at arm level and six dimensions at leg level, as well as their related reference points we take in fixing the spatial location, are encoded. Moreover, we added two dimensions to describe respectively the arm posture in the sagittal plane and the palm orientation of the forearm and the hand. Finally, we added three dimensions for leg posture: height, orientation and the way in which the legs are crossed in sitting position. We annotate</context>
</contexts>
<marker>Bull, 1987</marker>
<rawString>Bull, P. (1987) Posture and Gesture, Pergamon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
</authors>
<title>Multifunctionality and multidimensional dialogue semantics.”</title>
<date>2009</date>
<booktitle>In Proceedings of DiaHolmia’09, SEMDIAL.</booktitle>
<contexts>
<context position="6480" citStr="Bunt, 2009" startWordPosition="1006" endWordPosition="1007"> be decomposed into several gesture phases i.e. the different parts of a gesture such as the preparation, the stroke (the climax of the gesture), the hold and the retraction (when the hands return to their rest position) (McNeill, 1992). The scheme also enables to annotate gesture lemmas (Kipp, 2004), the shape and orientation of the hand during the stroke, the gesture space, and contact. We added the three tracks to code the hand trajectory, gesture velocity and gesture amplitude. Discourse and Interaction : Our discourse annotation scheme relies on multidimensional frameworks such as DIT++ (Bunt, 2009) and is compatible with the guidelines defined by the Semantic Annotation Framework (Dialogue Act) working group of ISO TC37/4. Discourse units include information about their producer, have a form (clause, fragment, disfluency, non-verbal), a content and a communicative function. The same span of raw data may be covered by several discourse units playing different communicative functions. Two discourse units may even have exactly the same temporal extension, due to the multifonctionality that cannot be avoided (Bunt, 2009). Compared to standard dialogue act annotation frameworks, three main a</context>
</contexts>
<marker>Bunt, 2009</marker>
<rawString>Bunt H. 2009. “Multifunctionality and multidimensional dialogue semantics.” In Proceedings of DiaHolmia’09, SEMDIAL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Bürki</author>
<author>C Gendrot</author>
<author>G</author>
</authors>
<title>Gravier &amp; al.(2008) “Alignement automatique et analyse phon6tique : comparaison de diff6rents systèmes pour l’analyse du schwa”, in revue</title>
<journal>TAL</journal>
<volume>49</volume>
<marker>Bürki, Gendrot, G, </marker>
<rawString>Bürki A., C. Gendrot, G. Gravier &amp; al.(2008) “Alignement automatique et analyse phon6tique : comparaison de diff6rents systèmes pour l’analyse du schwa”, in revue TAL ,49 :3</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks : The kappa statistic”,</title>
<date>1996</date>
<journal>in Computational Linguistics</journal>
<volume>22</volume>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. (1996) “Assessing agreement on classification tasks : The kappa statistic”, in Computational Linguistics 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I</author>
</authors>
<title>Influence Parameters and Assessment Methods for Evaluating Body Postures”,</title>
<date>1986</date>
<booktitle>in Ergonomics of Working Postures : Models, Methods and Cases , Proceedings of the First International Occupational Ergonomics Symposium.</booktitle>
<marker>I, 1986</marker>
<rawString>Corlett, E. N., Wilson,John R. Manenica. I. (1986) “Influence Parameters and Assessment Methods for Evaluating Body Postures”, in Ergonomics of Working Postures : Models, Methods and Cases , Proceedings of the First International Occupational Ergonomics Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Di Cristo</author>
<author>D Hirst</author>
</authors>
<title>Vers une typologie des unites intonatives du français”,</title>
<date>1996</date>
<journal>XXIème JEP,</journal>
<pages>219--222</pages>
<location>Avignon, France</location>
<marker>Di Cristo, Hirst, 1996</marker>
<rawString>Di Cristo &amp; Hirst D. (1996) “Vers une typologie des unites intonatives du français”, XXIème JEP, 219-222, 1996, Avignon, France</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Di Cristo</author>
<author>P Di Cristo</author>
</authors>
<title>Syntaix, une approche m6trique-autosegmentale de la prosodie”,</title>
<date>2001</date>
<booktitle>in revue Traitement Automatique des Langues,</booktitle>
<volume>42</volume>
<marker>Di Cristo, Di Cristo, 2001</marker>
<rawString>Di Cristo A. &amp; Di Cristo P. (2001) “Syntaix, une approche m6trique-autosegmentale de la prosodie”, in revue Traitement Automatique des Langues, 42 :1.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>Information Structure in Cross-Linguistic Corpora : Annotation Guidelines, Working Papers of the SFB 632, 7</booktitle>
<pages>07</pages>
<editor>Dipper S., M. Goetze and S. Skopeteas (eds.)</editor>
<marker>2007</marker>
<rawString>Dipper S., M. Goetze and S. Skopeteas (eds.) 2007. Information Structure in Cross-Linguistic Corpora : Annotation Guidelines, Working Papers of the SFB 632, 7 :07</rawString>
</citation>
<citation valid="true">
<authors>
<author>FGNet Second</author>
</authors>
<title>Foresight Report</title>
<date>2004</date>
<note>http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rdfgnet-foresight-workshop.pdf</note>
<marker>Second, 2004</marker>
<rawString>FGNet Second Foresight Report (2004) Face and Gesture Recognition Working Group. http ://www.mmk.ei.tum.de/ waf/fgnet-intern/3rdfgnet-foresight-workshop.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Gendner</author>
</authors>
<title>PEAS, the first instantiation of a comparative framework for evaluating parsers of French”.</title>
<date>2003</date>
<booktitle>in Research Notes of EACL</booktitle>
<location>(Budapest, Hungaria).</location>
<marker>Gendner, 2003</marker>
<rawString>Gendner V. et al. 2003. “PEAS, the first instantiation of a comparative framework for evaluating parsers of French”. in Research Notes of EACL 2003 (Budapest, Hungaria).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hawkins</author>
<author>N Nguyen</author>
</authors>
<title>Effects on word recognition of syllable-onset cues to syllable-coda voicing”,</title>
<date>2003</date>
<booktitle>in Papers in Laboratory Phonology VI.</booktitle>
<publisher>Cambridge Univ. Press.</publisher>
<marker>Hawkins, Nguyen, 2003</marker>
<rawString>Hawkins S. and N. Nguyen 2003. “Effects on word recognition of syllable-onset cues to syllable-coda voicing”, in Papers in Laboratory Phonology VI. Cambridge Univ. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hirst</author>
<author>A Di Cristo</author>
<author>R Espesser</author>
</authors>
<title>Levels of description and levels of representation in the analysis of intonation”,</title>
<date>2000</date>
<booktitle>in Prosody : Theory and Experiment,</booktitle>
<publisher>Kluwer.</publisher>
<marker>Hirst, Di Cristo, Espesser, 2000</marker>
<rawString>Hirst, D., Di Cristo, A., Espesser, R. 2000. “Levels of description and levels of representation in the analysis of intonation”, in Prosody : Theory and Experiment, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Hirst</author>
</authors>
<title>Form and function in the representation of speech prosody”, in K.Hirose, D.J.Hirst &amp; Y.Sagisaka (eds) Quantitative prosody modeling for natural speech description and generation</title>
<date>2005</date>
<journal>Speech Communication</journal>
<volume>46</volume>
<pages>3--4</pages>
<marker>Hirst, 2005</marker>
<rawString>Hirst, D.J. (2005) “Form and function in the representation of speech prosody”, in K.Hirose, D.J.Hirst &amp; Y.Sagisaka (eds) Quantitative prosody modeling for natural speech description and generation (Speech Communication 46 :3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Hirst</author>
</authors>
<title>A Praat plugin for Momel and INTSINT with improved algorithms for modelling and coding intonation”,</title>
<date>2007</date>
<booktitle>in Proceedings of the XVIth International Conference of Phonetic Sciences.</booktitle>
<marker>Hirst, 2007</marker>
<rawString>Hirst, D.J. (2007) “A Praat plugin for Momel and INTSINT with improved algorithms for modelling and coding intonation”, in Proceedings of the XVIth International Conference of Phonetic Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hirst</author>
</authors>
<title>Plugin Momel-Intsint. Internet : http ://uk.groups.yahoo.com/group/praatusers/files/Daniel_Hirst/plugin_momel-intsint.zip,</title>
<date>2007</date>
<location>Boersma, Weenink,</location>
<marker>Hirst, 2007</marker>
<rawString>Hirst, D. (2007), Plugin Momel-Intsint. Internet : http ://uk.groups.yahoo.com/group/praatusers/files/Daniel_Hirst/plugin_momel-intsint.zip, Boersma, Weenink, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-A Jun</author>
<author>C Fougeron</author>
</authors>
<title>Realizations of accentual phrase</title>
<date>2002</date>
<note>in French intonation”, in Probus 14.</note>
<marker>Jun, Fougeron, 2002</marker>
<rawString>Jun, S.-A., Fougeron, C. 2002. “Realizations of accentual phrase in French intonation”, in Probus 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Gesticulation and Speech: Two Aspects of the Porcess of Utterance”,</title>
<date>1980</date>
<booktitle>The Relationship of Verbal and Nonverbal Communication, The Hague:</booktitle>
<editor>in M.R. Key (ed.),</editor>
<publisher>Mouton.</publisher>
<marker>Kendon, 1980</marker>
<rawString>Kendon, A. (1980) “Gesticulation and Speech: Two Aspects of the Porcess of Utterance”, in M.R. Key (ed.), The Relationship of Verbal and Nonverbal Communication, The Hague: Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kita</author>
<author>A Ozyurek</author>
</authors>
<title>What does cross-linguistic variation in semantic coordination of speech and gesture reveal ? Evidence for an interface representation of spatial thinking and speaking”,</title>
<date>2003</date>
<journal>in Journal of Memory and Language,</journal>
<volume>48</volume>
<marker>Kita, Ozyurek, 2003</marker>
<rawString>Kita, S., Ozyurek, A. (2003) “What does cross-linguistic variation in semantic coordination of speech and gesture reveal ? Evidence for an interface representation of spatial thinking and speaking”, in Journal of Memory and Language, 48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Boca</title>
<date>2004</date>
<location>Raton, Florida, Dissertation.com.</location>
<contexts>
<context position="5246" citStr="Kipp (2004)" startWordPosition="804" endWordPosition="805">he inventory of tonal accents in French, we choose to integrate in our annotation scheme three types of tonal events : a/ underlying tones (for an eventual FrenchToBI annotation) ; b/ surface tones (annotated in terms of MOMel-Intsint protocol Hirst et al 2000) ; c/ melodic contours (perceptually annotated pitch movements in terms of their form and function). The interest to have both manual and automatic INTSINT annotations is that it allows the study of their links. Hand gestures : The formal model we use for the annotation of hand gestures is adapted from the specification files created by Kipp (2004) and from the MUMIN coding scheme (Allwood et al., 2005). Among the main gesture types, we annotate iconics, metaphoric, deictics, beats, emblems, butterworths or adaptors. We used the Anvil tool (Kipp, 2004) for the manual annotations. We created a specification files taking into account the different information types and the addition of new values adapted to the CID corpus description (e.g. we added a separate track Symmetry). For each hand, the scheme has 10 tracks. We allowed the possibility of a gesture pertaining to several semiotic types using a boolean notation. A gesture phrase (i.e.</context>
</contexts>
<marker>Kipp, 2004</marker>
<rawString>Kipp, M. (2004). Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Boca Raton, Florida, Dissertation.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
<author>M Neff</author>
<author>I Albrecht</author>
</authors>
<title>An annotation scheme for conversational gestures : how to economically capture timing and form.</title>
<date>2007</date>
<journal>Language Resources and Evaluation,</journal>
<volume>41</volume>
<issue>3</issue>
<contexts>
<context position="9226" citStr="Kipp et al., 2007" startWordPosition="1453" endWordPosition="1456">ace and gaze : At the present time, head movements, gaze directions and facial expressions have been coded in 15 minutes of speech yielding a total number of 1144 movements, directions and expressions, to the exclusion of gesture phases. The onset and offset of each tag are determined in the way as for hand gestures. Body Posture : Our annotation scheme considers, on top of chest movements at trunk level, attributes relevant to sitting positions (due to the specificity of our corpus). It is based on the Posture Scoring System (Bull, 1987) and the Annotation Scheme for Conversational Gestures (Kipp et al., 2007). Our scheme covers four body parts : arms, shoulders, trunk and legs. Seven dimensions at arm level and six dimensions at leg level, as well as their related reference points we take in fixing the spatial location, are encoded. Moreover, we added two dimensions to describe respectively the arm posture in the sagittal plane and the palm orientation of the forearm and the hand. Finally, we added three dimensions for leg posture: height, orientation and the way in which the legs are crossed in sitting position. We annotated postures on 15 minutes of the corpus involving one pair of speakers, lea</context>
</contexts>
<marker>Kipp, Neff, Albrecht, 2007</marker>
<rawString>Kipp, M., Neff, M., Albrecht, I. (2007). An annotation scheme for conversational gestures : how to economically capture timing and form. Language Resources and Evaluation, 41(3).</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Koiso</author>
<author>Y Horiuchi</author>
<author>A Ichikawa</author>
</authors>
<title>Den Y.(1998) “An analysis of turn-taking and backchannels based on prosodic and syntactic features in Japanese map task dialogs”,</title>
<booktitle>in Language and Speech,</booktitle>
<pages>41</pages>
<marker>Koiso, Horiuchi, Ichikawa, </marker>
<rawString>Koiso H., Horiuchi Y., Ichikawa A. &amp; Den Y.(1998) “An analysis of turn-taking and backchannels based on prosodic and syntactic features in Japanese map task dialogs”, in Language and Speech, 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and Mind. What Gestures Reveal about Thought, Chicago :</title>
<date>1992</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="6105" citStr="McNeill, 1992" startWordPosition="946" endWordPosition="947">ed a specification files taking into account the different information types and the addition of new values adapted to the CID corpus description (e.g. we added a separate track Symmetry). For each hand, the scheme has 10 tracks. We allowed the possibility of a gesture pertaining to several semiotic types using a boolean notation. A gesture phrase (i.e. the whole gesture) can be decomposed into several gesture phases i.e. the different parts of a gesture such as the preparation, the stroke (the climax of the gesture), the hold and the retraction (when the hands return to their rest position) (McNeill, 1992). The scheme also enables to annotate gesture lemmas (Kipp, 2004), the shape and orientation of the hand during the stroke, the gesture space, and contact. We added the three tracks to code the hand trajectory, gesture velocity and gesture amplitude. Discourse and Interaction : Our discourse annotation scheme relies on multidimensional frameworks such as DIT++ (Bunt, 2009) and is compatible with the guidelines defined by the Semantic Annotation Framework (Dialogue Act) working group of ISO TC37/4. Discourse units include information about their producer, have a form (clause, fragment, disfluen</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>McNeill, D. (1992). Hand and Mind. What Gestures Reveal about Thought, Chicago : The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Gesture and Thought,</title>
<date>2005</date>
<publisher>The University of Chicago Press.</publisher>
<location>Chicago, London:</location>
<marker>McNeill, 2005</marker>
<rawString>McNeill, D. (2005). Gesture and Thought, Chicago, London: The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Milborrow</author>
<author>F Nicolls</author>
</authors>
<title>Locating Facial Features with an Extended Active Shape Model.</title>
<date>2008</date>
<journal>ECCV</journal>
<volume>4</volume>
<marker>Milborrow, Nicolls, 2008</marker>
<rawString>Milborrow S., F. Nicolls. (2008). Locating Facial Features with an Extended Active Shape Model. ECCV (4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nesterenko</author>
</authors>
<title>Corpus du parler russe spontan6 : annotations et observations sur la distribution des frontières prosodiques”, in revue TIPA,</title>
<date>2006</date>
<pages>25</pages>
<marker>Nesterenko, 2006</marker>
<rawString>Nesterenko I. (2006) “Corpus du parler russe spontan6 : annotations et observations sur la distribution des frontières prosodiques”, in revue TIPA, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Paroubek</author>
</authors>
<title>Data Annotations and Measures in EASY the Evaluation Campaign for Parsers in French”.</title>
<date>2006</date>
<booktitle>in proceedings of the 5th international Conference on Language Resources and Evaluation</booktitle>
<pages>314--320</pages>
<location>Genoa,</location>
<marker>Paroubek, 2006</marker>
<rawString>Paroubek P. et al. 2006. “Data Annotations and Measures in EASY the Evaluation Campaign for Parsers in French”. in proceedings of the 5th international Conference on Language Resources and Evaluation 2006 (Genoa, Italy), pp. 314-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierrehumbert</author>
<author>Beckman</author>
</authors>
<title>Japanese Tone Structure.</title>
<date>1988</date>
<journal>Coll. Linguistic Inquiry Monographs,</journal>
<volume>15</volume>
<publisher>The MIT Press.</publisher>
<location>Cambridge, MA, USA:</location>
<marker>Pierrehumbert, Beckman, 1988</marker>
<rawString>Pierrehumbert &amp; Beckman (1988) Japanese Tone Structure. Coll. Linguistic Inquiry Monographs, 15. Cambridge, MA, USA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Platzer</author>
<author>W Kahle</author>
</authors>
<title>Color Atlas and Textbook of Human Anatomy, Thieme. Project MuDis. Technische Universitat Munchen.</title>
<date>2004</date>
<note>http ://www9.cs.tum.edu/research</note>
<marker>Platzer, Kahle, 2004</marker>
<rawString>Platzer, W., Kahle W. (2004) Color Atlas and Textbook of Human Anatomy, Thieme. Project MuDis. Technische Universitat Munchen. http ://www9.cs.tum.edu/research</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
<author>P Ekman</author>
</authors>
<title>Handbook of methods in nonverbal behavior research.</title>
<date>1982</date>
<publisher>Cambridge University Press.</publisher>
<marker>Scherer, Ekman, 1982</marker>
<rawString>Scherer, K.R., Ekman, P. (1982) Handbook of methods in nonverbal behavior research. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
</authors>
<title>Preliminaries to a theory of speech disfluencies.</title>
<date>1994</date>
<tech>PhD Thesis,</tech>
<institution>University of California, Berkeley</institution>
<contexts>
<context position="7651" citStr="Shriberg, 1994" startWordPosition="1187" endWordPosition="1188">ogue act annotation frameworks, three main additions are proposed : rhetorical function, reported speech and humor. Our rhetorical layer is an adaptation of an existing schema developed for monologic written data in the context of the ANNODIS project. Disfluencies : Disfluencies are organized around an interruption point, which can occur almost anywhere in the production. Disfluencies can be prosodic (lenghtenings, silent and filled pauses, etc.), or lexicalized. In this case, they appear as a word or a phrase truncation, that can be completed. We distinguish three parts in a disfluency (see (Shriberg, 1994), (Blanche-Benveniste87)) : – Reparandum : what precedes the interruption point. This part is mandatory in all disfluencies. We indicate there the nature of the interrupted unit (word or phrase), and the type of the truncated word (lexical or grammatical) ; – Break interval. It is optional, some disfluencies do not bear any specific event there. – Reparans : the part following the break, repairing the reparandum. We indicate there type of the repair (no restart, word restart, determiner restart, phrase restart, etc.), and its function (continuation, repair without change, repair with change, e</context>
</contexts>
<marker>Shriberg, 1994</marker>
<rawString>Shriberg E. 1994. Preliminaries to a theory of speech disfluencies. PhD Thesis, University of California, Berkeley</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wallhoff</author>
<author>M Ablassmeier</author>
<author>G Rigoll</author>
</authors>
<title>Multimodal Face Detection, Head Orientation and Eye Gaze Tracking”,</title>
<date>2006</date>
<booktitle>in proceedings of International Conference on Multisensor Fusion and Integration (MFI).</booktitle>
<marker>Wallhoff, Ablassmeier, Rigoll, 2006</marker>
<rawString>Wallhoff F., M. Ablassmeier, and G. Rigoll. (2006) “Multimodal Face Detection, Head Orientation and Eye Gaze Tracking”, in proceedings of International Conference on Multisensor Fusion and Integration (MFI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T D White</author>
<author>P A Folkens</author>
</authors>
<title>Human Osteology. San Diego :</title>
<date>1991</date>
<publisher>Academic Press, Inc.</publisher>
<marker>White, Folkens, 1991</marker>
<rawString>White, T. D., Folkens, P. A. (1991) Human Osteology. San Diego : Academic Press, Inc.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>