<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.916137">
Some Notes on Generative Capacity of Dependency Grammars
</title>
<author confidence="0.891448">
Tomasz Obrcbski *t Filip Graliriski t
</author>
<affiliation confidence="0.896112">
* Institute of Control t Department of Computational Linguistics
and Information Engineering and Artificial Intelligence,
Electric Faculty Faculty of Mathematics
Poznan University of Technology and Computer Science,
Poznan, Poland Adam Mickiewicz University
Tomasz.Obrebski@put.poznan.pl Poznan, Poland,
</affiliation>
<email confidence="0.965293">
filipg@amu.edu.pl
</email>
<bodyText confidence="0.999923372093023">
between dependency grammars and context-
free languages and some of its subclasses. Many
of the facts and observations contained in this
paper may well be known to a number of re-
searchers in the field, though have been only
fragmentarily articulated in the literature.
Taking into consideration various types of
grammatical systems we will attempt to formu-
late necessary and sufficient conditions for a de-
pendency grammar to have at least CF genera-
tive capacity. The conditions will be stated in
a way general enough to be applicable to a broad
range of grammatical systems. We will enumer-
ate the concepts which must be expressible in
a grammatical system to ensure the CF capac-
ity. Also, some cases when not all of the re-
quirements are satisfied will be analysed.
We will adopt the following general definition
of dependency grammar which expresses, at
least approximately, the essential feature shared
by the above-mentioned systems: a dependency
grammar is a formal system which, given two
finite sets of symbols, defines a correspondence
between sequences of symbols from the first set
and trees whose nodes are labeled with symbols
from the second set; tree nodes must be related
one-to-one to sequence elements.
Symbols from the first set are typically called
terminal symbols or word forms, while the ele-
ments of the second set are referred to as syn-
tactic or lexical categories. Tree arcs may or
may not be labeled with dependency types.
Only projective systems with atomic category
symbols are considered in this paper. Some of
the systems mentioned above go beyond our
general definition together with these restric-
tions (those of Maruyama&apos;s, Duchier and De-
busmann&apos;s, and Sleator and Temperley&apos;s). We
are not interested, however, in examining the
properties of these concrete systems as such, but
only in the various ways the rules/constraints
describing legal tree structures are formulated
in different frameworks.
</bodyText>
<sectionHeader confidence="0.648265" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999117846153846">
The aim of this paper is to gather and add to the
dispersed knowledge concerning the relation be-
tween dependency grammars and context-free
languages and some of its subclasses. Neces-
sary and sufficient conditions for a dependency
system to have context-free power are formu-
lated in a way general enough to be applicable to
a broad range of grammatical systems, based on
rules or constraints. Certain cases when some
of these requirements are not satisfied are also
analysed. Formal implications of the presence
of dependency types in grammatical systems are
discussed.
</bodyText>
<sectionHeader confidence="0.960551" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9840102">
The notion of dependency grammar is some-
what vague. Formal systems introduced by
Hays (1964) and Gaifman (1965), Maruyama
(1990b), Covington (1990), Carroll and Char-
niak (1992), Duchier and Debusmann (2001),
Nivre (2003), Sleator and Temperley (1991) 1 are
all called dependency grammars2, although they
are significantly different and have different for-
mal properties.
A number of results concerning descriptive
capacity of some types of dependency sys-
tems have been reported in the literature,
mostly for Gaifman-style grammars (Gaifman,
1965; Nivre, 2002) and Maruyama&apos;s grammars
(Maruyama, 1990a; White, 1995). Other kinds
of systems, in particular the constraint-based
systems other than Maruyama&apos;s one, have not
been analysed in this respect.
The goal of this paper is to gather and add to
the dispersed knowledge concerning the relation
</bodyText>
<footnote confidence="0.918703333333333">
1-The list is meant to show the variety of formal sys-
tems covered by the term &apos;dependency grammar&apos;, and is
not exhaustive in any sense.
2Sleator and Temperley&apos;s formalism is called Link
Grammar but it is usually classified as a dependency
system.
</footnote>
<bodyText confidence="0.999742052631579">
In this respect the dependency systems may
be divided into two broad classes. Into the
first class fall systems which simply enumer-
ate all possible branchings which may occur in
a tree (Gaifman, 1965; Lombardo and Lesmo,
1998; Carroll and Charniak, 1992). We will call
them rule-based systems. Into the second class –
constraint-based systems – fall formalisms where
the well-formedness conditions on acceptable
tree structures are formulated by means of con-
straints stating e.g. what arcs are allowed or
necessary in given contexts (Maruyama, 1990b;
Covington, 1990; Duchier and Debusmann,
2001; Nivre, 2003).
As a special kind of rule-based system we can
classify Link Grammar with the word descrip-
tions considered in disjunctive form (Sleator
and Temperley, 1991, 8-9), see also remarks
concerning Link Grammar in Section 6.
</bodyText>
<sectionHeader confidence="0.9579035" genericHeader="introduction">
2 Minimal constraint-based
formulation
</sectionHeader>
<bodyText confidence="0.999949384615385">
Our aim is to examine which conditions are nec-
essary and sufficient for a dependency formalism
to be capable of generating all CF languages.
First, we will introduce a simple constraint-
based formalism which has CF generative ca-
pacity and which is minimal in the sense that
it cannot be simplified without affecting its de-
scriptive power. This simple system, in our
opinion, provides good intuitions as to what has
to be expressible in a formal grammatical sys-
tem to obtain CF power. It should be made
clear, however, that the proposed formalism is
unlikely to be of any direct practical value.
</bodyText>
<equation confidence="0.763950666666667">
A Minimal Constraint-based Dependency
Grammar (MCDG) is defined as a 6-tuple
(E, C, Croot, r, OL, OR), where:
</equation>
<listItem confidence="0.97433325">
• E – terminal alphabet,
• C – set of categories,
• Coot C C – set of root categories,
• r C ExC– one-to-many relation (lexicon),
• OL: c 2cu{6} for each category A E C,
OL (A) is the set of categories allowed for
the left dependent of a node labeled with
A; a node of category A has at most one
left dependent, which is not obligatory iff
the special symbol E belongs to OL (A),
• OR: c 2cu{6} defined analogously
as OL, but for the right dependent.
</listItem>
<bodyText confidence="0.958002">
(As it is evident from the above definition,
MCDGs license only binary dependency trees.)
For example, the following grammar describes
the language anbn: E = {a, 1)}, C = {A, B},
</bodyText>
<construct confidence="0.484955">
Coot {A}, A:= {(a,A),(b,B)}, OL(A) =
OR(B) = {E}, OR(A) = {B}, OL(B) = {E,
</construct>
<bodyText confidence="0.999874875">
We are going to prove that MCDGs are
weakly equivalent to CFCs. Our approach
here is to some extent similar to the method
used by Maruyama to prove the context-free
power of his constraint dependency grammars
(Maruyama, 1990a; White, 1995).
First, let us give some auxiliary definitions:
a dependency tree is defined recursively as
</bodyText>
<equation confidence="0.972481">
(D1, • • • , DI, X, D1+1, • • •
</equation>
<bodyText confidence="0.994694666666667">
where 1, r &gt; 0, X E C is the root of the tree, D,
are dependency (sub)trees; the root of a tree D
will be denoted by r(D), i.e.
</bodyText>
<equation confidence="0.96699725">
r((Di, , Di, X, Di+1, ,Di+r)) = X,
whereas t(D) is the sequence of categories in D:
t((Di, , Di, X, Di+1, ,Di+r)) =
t(Di) ... t(D1)Xt(D1+1)... t(D1)
</equation>
<bodyText confidence="0.979606428571429">
(In MCDGs /, r E {0, 1}, as only binary trees
can be generated.)
If D is a dependency tree and G a constraint-
based dependency grammar, then G H D is to
stand for: &amp;quot;D fulfils requirements expressed by
G&amp;quot; (i.e. by OL and OR). The derivation re-
lation holds as follows: X iff 3D: G H
</bodyText>
<equation confidence="0.926693">
D A r(D) = X A t(D) = where X E C,
E C*, D is a dependency tree. Now we define
the derivation relation for sequences of words:
X iff X Y1Y2 ...Yn and
(Vi &lt; n)(w„ Y,) E L.
</equation>
<bodyText confidence="0.89996">
Given a dependency grammar G, L(G) is the
language generated by G:
L(G) ={(yEE*IX*GaAXECroot}.
</bodyText>
<construct confidence="0.9165325">
Proposition 1 For every context-free gram-
mar G which does not generate the null string
there exists a minimal constraint-based depen-
dency grammar G&apos; such that L(G&apos;) = L(C).
</construct>
<bodyText confidence="0.9353215">
Proof. We may assume that G = (V, E, P, 8)
is given in Greibach normal form, i.e. all pro-
duction rules in P = {P1,...,Pm} are of the
form:
A —&gt; 031132 • • • Bn,
where
</bodyText>
<figure confidence="0.798889">
(B bn-1
</figure>
<figureCaption confidence="0.974845">
Figure 1: A fragment of dependency tree corre-
sponding to a CFG rule
</figureCaption>
<bodyText confidence="0.967720545454546">
Let us index all the occurrences of non-
terminal symbols in the right-hand sides of the
rules: a production rule will be written as
A —&gt; agl B2 . gin ,
where Bi, B2, . . . Bn are non-terminal symbols
and i1, i2, ,i7. $ 0 are indices of the occur-
rences (index 0 will have a special meaning).
By 1(X) we denote the set of the indices of oc-
currences of X in the right-hand sides.
, weakly equivalent to G, can be con-
structed in the following way:
</bodyText>
<listItem confidence="0.974806">
• C = { XL I X —&gt; w_131 B2 . . . B!nn,i
1(X) U {0} 1,
—
• C rnot S,?„ EC},
• r { (w, XL) I w E E C},
• OL(X) = {E},
• 0L(X) = {E} if i 0 and A —&gt;
. . . B,
• 0 L(Xlio) = {17,1 I Y c C} if i 0
and A —&gt; Y&apos;q-1X&apos;q B712n ,
</listItem>
<bodyText confidence="0.987515">
j = ig_i for some q E {2, , n} (the left
dependent corresponds to the left-adjacent
symbol in the right-hand side of a CFG
rule),
</bodyText>
<listItem confidence="0.998765666666667">
• OR(XL) = /(Pk), where
/(Pk) = {E} if Pk = X —&gt; w, or
/ (Pk)= {-17i I Y,3, e C} if Pk = X —&gt;
</listItem>
<bodyText confidence="0.949845375">
w . . . nj in (the right depen-
dent corresponds to the last symbol in a
CFG rule, in which the governor is the sym-
bol in the left-hand side of the rule).
In other words, the production A —&gt;
aB11B2&apos;2 Bn&apos;n will be represented in a depen-
dency tree by a fragment shown in Figure 1.
Example: let us convert the following CFG:
</bodyText>
<listItem confidence="0.9971415">
• V = {S,A,B},
• E =
• P = —&gt; xAA,A y,A zB,B
zB , B
</listItem>
<bodyText confidence="0.541173">
After adding occurrence indices (P = IS —&gt;
</bodyText>
<listItem confidence="0.840551846153846">
x Al A2 , A —&gt; y, A —&gt; z_131 , B z B2 , B zI)
we can construct an equivalent MCD C:
• C =
• Coot = {SD,
•
(y, A), (z, (z, A), (z , B1), (z , B?)} ,
• OL(S) = OL(A) = Or(A) = OL(B) =
OL(A) = °MAD = OL(B1) = OMB?) =
{E}, °MAD = °MAD =
• OR(S) = , OR(A) = Or(A) =
0R(A) = {E}, Or(A) = O(A1) =
OR(A) = {B}, OR(M) = OR(B1) =
OR(B?) = {E,
</listItem>
<bodyText confidence="0.991088833333333">
Let G&apos; H _L D mean that either left dependent
is not obligatory for r (D) (i.e. E E OL(r(D)))
and G&apos; H D, or that this dependent is oblig-
atory and a dependency tree D fulfils the re-
quirements of G&apos; with the exception of r(D) not
being connected to a left dependent. The re-
</bodyText>
<equation confidence="0.982980333333333">
lations X L,Gi ,y and X L,G1c (where
E C*, c E E*) are defined analogously to
X &apos;y and X a respectively.
</equation>
<bodyText confidence="0.923794">
Now it is rather straighforward to prove (by
induction on the length of a) that
</bodyText>
<equation confidence="0.978173333333333">
X *G a (3w E E) (V/ E I (X))XL AIL,G1
Taking X = S, i = 0 we get S a if
(3w E E)S,()„ a, hence L(G) = L(G&apos;).
</equation>
<construct confidence="0.880173357142857">
Proposition 2 For every minimal constraint-
based dependency grammar G there exists a CF
grammar G&apos; such that L(G&apos;)= L(C).
Proof. Let G = (E,C,Croot, OL,OR). The
context-free grammar G&apos; = (C U {S} , E, P, 8)
is weakly equivalent to G where S C and
P = IS —&gt; AxB I Xe Croot, (x, X) E £,A c
0 L(X), B c OR(X)} u {X —&gt; AxBIX E
C, (x, X) c £,A E 0 L(X), B E 0 R(X)}.
Corollary 1 Every context-free grammar
which does not generate the null string can
be transformed into an equivalent grammar
in which all production rules are of the form:
X —&gt;YaZ, X —&gt;Ya, X —&gt;aY, X —&gt; a.
</construct>
<bodyText confidence="0.841774444444444">
3 Minimal requirements for
context-free power
Taking into consideration the way the MCDC
system is constructed, let us discuss in less for-
mal manner what is needed for a dependency-
based grammatical system to have at least
context-free descriptive capacity (the compo-
nents of MCDC implementing the respective
concepts are given in parentheses):
</bodyText>
<listItem confidence="0.948423571428572">
(1) lexicon expressing one-to-many (or many-
to-many) correspondence between terminal
symbols and category symbols (r),
(2) distinguished subset of category symbols al-
lowed for tree roots (Coot),
(3) at least two dependents must be allowed
(two O&apos;s),
(4) the ability to indicate the relative position
of the dependent with respect to its head
(OL vs. OR),
(5) the ability to express that a specific depen-
dent (i.e. of category belonging to a certain
class) must be present
(6) the ability to express that a specific depen-
dent (see above) cannot be repeated.
In MCDC (5) and (6) are conjointly imple-
mented by OL and OR.
(1) to (6) given above express the concepts of:
(1) lexical ambiguity, (2) &amp;quot;rootness&amp;quot;, (3) binary
branching, (4) dependency direction, (5) obliga-
toriness, (6) non-repeatability.
</listItem>
<bodyText confidence="0.997660434782609">
From the equivalence of MCDCs and CFCs
follows that the above conditions are sufficient
for a dependency system to have (at least) CF
power. We will now show that they are also
necessary.
Concepts (2), (4), (5) and (6) are needed to
generate the simple language L = {ab}. With-
out (2) or (5), ab E L implies either a E L or
b E L. Without (4), ab E L implies ba E L.
Without (6), ab E L implies either aab E L or
abb E L.
The constraints given in (5) and (6) must re-
fer to sets of categories and not to single cate-
gories. An informal argument goes as follows:
let us consider the language Oben, assuming
that a is the root, a must have c as its right
dependent, c, in turn, requires either a or b
as its left dependent.
The concepts (1) and (3) will be discussed in
the next two sections.
Finally, it can be noted that the following
concepts, frequently met in formal descriptions
of language, are not needed for CF capacity:
</bodyText>
<listItem confidence="0.946367">
• ability to express any kind of interrelation-
ship among dependents (e.g. their relative
order) other than their mutual exclusion,
• repeatable dependencies (in whatever
meaning).
4 No lexical ambiguity
</listItem>
<bodyText confidence="0.998546666666667">
Dependency systems without lexical ambigu-
ity, i.e. ones in which the lexicon is degener-
ated to a one-to-one relation between termi-
nal and category symbols or in which category
symbols do not appear at all, come as gram-
matical components of several stochastic lan-
guage models. The generative capacity of two
such systems — Carroll and Charniak&apos;s (1992)
Context-Free Dependency Crammar3 and Eis-
ner&apos;s (2000) Bilexical Dependency Crammar4 —
was analysed by Nivre (2002). He shows that
the sets of languages these systems generate
are not equal and are both proper subsets of
context-free languages (and are neither subsets
nor supersets of regular languages)5, which sup-
ports the claim (1) from Section 3.
In the following, the grammatical systems
with a one-to-one (or many-to-one) lexicon will
be called word-level grammars, whereas the sys-
tems with a one-to-many (or many-to-many)
lexicon — category-level grammars.
Two comments to Nivre&apos;s observations are in
order. Firstly, what is clear but not stated
explicitly by Nivre, the reason for the weak-
ness of these grammars is the &apos;lack&apos; of a lex-
icon — it is exclusively the one-to-one restric-
tion imposed on the lexicon that makes Carroll
and Charniak&apos;s system different from Cadman&apos;s
one, which has context-free power.
Secondly, the descriptive capacity of word-
level grammars (unlike that of category-level
ones) is very sensitive to the format in which the
rules/constraints are formulated, since transfor-
</bodyText>
<footnote confidence="0.892244636363636">
3To each terminal symbol x corresponds one nonter-
minal symbol noted Y, rules have the CFG-like form
Y —&gt; ax i 3, where a and i3 are sequences of non-terminals.
4No category symbols are introduced; for each ter-
minal symbol, possible sequences of its left and right
dependents are specified independently by two regular
expressions over terminal symbols.
31t is not difficult to show that the class of languages
generated by MCDGs with *-to-one lexicon is still differ-
ent from the two (it is actually a proper subset of their
common part).
</footnote>
<bodyText confidence="0.9997586">
mations from one format (normal form) to an-
other, similar to those which can be done for
category-level systems, are not possible, as they
usually entail multiplication of category sym-
bols, which is not feasible in word-level systems.
</bodyText>
<sectionHeader confidence="0.780895" genericHeader="method">
5 No branchings
</sectionHeader>
<bodyText confidence="0.9984835">
If we do not allow the tree structure to branch,
we obtain the power of regular grammars. This
fairly obvious fact can be shown by referring
once again to our MCDG.
After removing one 0 (e.g. OL) from the def-
inition of MCDG (with one-to-many lexicon),
we get a system equivalent to finite automata
over E6 (constructing an automaton from a one-
</bodyText>
<equation confidence="0.976459333333333">
branch MCDG: Q = C U {go}, QF = IC E C I
E E 0 R(C)} , CI E S(c, a) iff c&apos; E 0R(c) A (a, c&apos;) E
r, c&apos; E S(q0, a) if c&apos; E Coot A (a, c&apos;) E r; in the
opposite direction: C = QxQ, Coot = fqol x Q,
OR((ql, q2)) = { (q2, q3) I q3 E Q} and E E
°R((ql, q2)) iff q2 C Q).
</equation>
<bodyText confidence="0.999807333333333">
This corresponds to a Gaifman-style gram-
mar with only *(X), X(*) and X(*, Y) rules
(cf. Section 6).
To complete the puzzle, we may finally con-
sider the class of languages defined by word-
level regular grammars. This small class is de-
fined by a restricted subclass of finite automata
in which all transitions through a given sym-
bol a always go to the same destination state,
regardless of the source state (cf. the construc-
tion of an automaton from an MCDG) - thus,
states may be identified with symbols.
</bodyText>
<sectionHeader confidence="0.995581" genericHeader="method">
6 Rule-based grammars
</sectionHeader>
<bodyText confidence="0.967771032258064">
We will restrict ourselves here to two remarks
concerning possible minimal normal forms for
rule-based grammars.
In Gaifman&apos;s original formulation (Gaff-
man, 1965), the rules take the forms:
X(-171, • • • , irz,*,Yz+1, • • • ,Y) (a node of cate-
gory X may have dependents of categories
Y1,... , Yr, in this order and the position of X
is indicated by ), X(*) (a node of category X
may be a leaf), and *(X) (the root node may
be of category X).
For context-free power it is enough to have
the rules of the following five types: *(X), X(*),
X(*, Y), X(Y, ), X (Y, *, Z), since each MCDG
can easily be transformed into a Gaifman gram-
mar with this kind of rules.
6We take into consideration only languages which do
not contain the empty string.
Another normal form for Gaifman grammars,
where at most two dependents are allowed, can
be derived straightforwardly from 2-Greibach
Normal Form7, namely: *(X), X(*), X(*,Y),
X(*, Y, Z).
Let us note that Gaifman&apos;s system in the lat-
ter form can be obtained (minor details ignored)
from Link Grammar by allowing at most one left
(H connector for each word. The left connector
would serve for linking the word to its governor,
while right (±) connectors would link the word
to its dependents; connector names would play
the role equivalent to that of category symbols.
</bodyText>
<sectionHeader confidence="0.978178" genericHeader="method">
7 Maruyama&apos;s grammars
</sectionHeader>
<bodyText confidence="0.999925774193549">
In this section some properties of the gram-
matical system called Constraint Dependency
Grammar (CFG) will be discussed. This sys-
tem, introduced by Maruyama (1990a), differs
in many respects from other dependency sys-
tems and, in general, does not fall into the def-
inition of dependency grammar as adopted in
Introduction. CDG does not relate a sequence
of terminal symbols (words) to a tree, but rather
to a set of directed graphs with outdegree lim-
ited to 1. These graphs are defined by role val-
ues - roles can be seen as variables, each ter-
minal has k roles, where k is the parameter of
the grammar called degree. Each role defines
one graph. Role value is a pair composed of
a label and an index of a terminal symbol (mod-
ifiee) and can be viewed as a labeled arc pointing
from one terminal to another. The constraints
in CDGs are expressed by means of logical for-
mulae over assignments of role values to roles.
We will take into consideration only the sub-
class of CDGs restricted to systems in which
(1) roles determine the dependency relation be-
tween terminal symbols and (2) projectivity
of this relation is ensured (No-Crossover con-
straints are imposed (White, 1995, p. 12)).
It was shown (Maruyama, 1990a) that any
context-free language can be generated by
a CDG with two roles8. In the proof constraints
containing only one or two variables were used
(unary/binary constraints).
</bodyText>
<footnote confidence="0.991663666666667">
7A CFG is in 2-Greibach Normal Form if its pro-
ductions are of the following form: X —&gt; x, X —&gt; xY,
X —&gt; xY Z , where x is a terminal symbol and X, Y, Z are
non-terminal symbols. Every CFG which does not gen-
erate the null string can be transformed into an equiva-
lent grammar in 2-Greibach Normal Form (Hoperoft and
Ullman, 1979).
8CDGs can generate some non-context-free languages
as well.
</footnote>
<bodyText confidence="0.986540666666667">
Now, let us discuss why two variables and two
roles are necessary. Two variables are neces-
sary and sufficient to express non-repeatability
constraint of the type: mod(x) = mod(y) A
label(x) E K A label(y) E KA pos(x) =
pos(y), which means that no two distinct termi-
nals can be connected to their modifee through
labels both belonging to K, where K is a set of
labels.
As was demonstrated in Section 3 the neces-
sary requirements for CF power of a dependency
system include the capability of expressing both
obligatoriness and non-repeatability. One role
makes it possible to express either one or the
other type of constraint but not both of them:
for non-repeatability the role &amp;quot;arc&amp;quot; must point
from the dependent to the head (the head plays
the role of the modifee), whereas for obligatori-
ness the arc must go in the opposite direction
(the dependent is the modifee). Hence, in order
to express obligatoriness and non-repeatability
at least two CDC roles are needed (cf. the use of
the roles governor and needs in (White, 1995) or
head-role and body-role in (Maruyama, 1990a)).
Note that an MCDC can be straightforwardly
expressed by a CDC with three roles: governor,
needs-left, needs-right.
</bodyText>
<sectionHeader confidence="0.915018" genericHeader="method">
8 Dependency types
</sectionHeader>
<bodyText confidence="0.999985863636364">
We start our discussion of dependency types
with some observations regarding the use of the
label function in Maruyama&apos;s grammars. In the
examples given in (Maruyama, 1990b), the label
plays the role similar to that of a dependency
type (some of the labels appearing in the ex-
amples: DET, SUBJ, OBJ, ROOT9). On the
one hand, thus, the system may be perceived
as a word-level system with typed dependen-
cies. On the other hand, in the proof showing
the context-free power of Maruyama&apos;s grammar
(Maruyama, 1990a), the label function values
&amp;quot;store&amp;quot; nonterminal symbols — the category of
a label&apos;ed node. In this case the label func-
tion plays in fact the role of the lexicon and the
system appears more as a category-level system
with untyped dependencies.
These observations concerning the dual po-
tential functionality of label may suggest that
the function of a lexicon in a grammatical sys-
tem may be implemented with the use of depen-
dency types and vice versa.
</bodyText>
<footnote confidence="0.987214">
9ROOT is a special label used for the root node, as
a label is mandatory for all role values.
</footnote>
<bodyText confidence="0.999724066666667">
From this perspective, let us consider Caif-
man&apos;s dependency system. The transforma-
tion between this system and its typed word-
level equivalent can be performed according
to the scheme shown in Figure 2. In order
to retain equivalence, the tree context cov-
ered by the word-level rule has to be ex-
tended with the arc leading to the head node.
The rules in the typed word-level version
of Caifman&apos;s system could take the follow-
ing form: (o-, x)(7-1, , , *, 7-z+1, , Tn) , where
U, Ti,. , Tn are dependency types and x — a ter-
minal symbol (x, which is linked to its head by
a, can govern n nodes linked via dependencies
of types Ti,..., 7-n).
</bodyText>
<figure confidence="0.973193">
A
C
a
</figure>
<figureCaption confidence="0.9527795">
Figure 2: From category symbols to types and
vice versa
</figureCaption>
<bodyText confidence="0.993167">
To sum up, both dependency types and cat-
egories may be seen as providing terminal sym-
bols with additional information giving the
same syntactic potential.
</bodyText>
<sectionHeader confidence="0.975815" genericHeader="method">
9 Minimal constraint-based
formulation with types
</sectionHeader>
<bodyText confidence="0.986994068965517">
Let us consider here a simple dependency for-
malism with constraints related to dependency
types. Such an approach may be regarded as
more elegant and is more in the spirit of linguis-
tic descriptions (Mel&apos;euk, 1988; Hudson, 1990).
Thus, let us define herein a typed ver-
sion of dependency grammar as a 9-tuple
(E, C, L,T,T,91, Tobi, Tieft,Trzght, D), where E,
C and r have the same meaning as in
an MCDC, T is a set of dependency types,
Tsgl Tobl Tie f t,Trzght C T, T391 contains non-
repeatable types, Tobi — obligatory types, Tie ft —
leftward types, and Trzght — rightward types, and
finally DCCxTxC((c, T CI) E D means that
c may govern c&apos; via dependency T). If T E Tobi
then dependency T must occur for each node
with category c E C such that there exists at
least one c&apos; E C, (C, T, CI) C D. Note that no
restriction is imposed on the relative order of
dependents.
(This kind of formalism forms the basis of
the dependency system for Polish described in
(Obrebski, 2003).)
The typed dependency grammars of the form
given above have CF power since any MCDG
can be easily transformed into such a typed
grammar: for each c E C one leftward type 7-,,/
and one rightward type T,,, are introduced, now
T891= T, Tie f t ={ Te,i ICC Cr}, Tight = Te,r
</bodyText>
<figure confidence="0.73437275">
C C C Tobi ={ Te,1 IE OL(C)}U {Tc,r
E 0 R(C)} and D = {(c,T,,i,c&apos;) I c&apos; E
Tc,r,c&apos;) c&apos; C R(c)\ {M .
0 L (c) \ {E} } { (c,
</figure>
<bodyText confidence="0.992435857142857">
This form of typed grammar can be seen as its
minimal formulation.
For alternative ways of expressing the obliga-
toriness and non-repeatability (exclusion) con-
straints based on types or categories, see
(Duchier and Debusmann, 2001) and (Bes and
Blache, 1999) respectively.
</bodyText>
<sectionHeader confidence="0.994836" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999982173913043">
The minimal requirements for a dependency
grammar formalism to have context-free power
have been identified. These are: the capabil-
ity of expressing lexical ambiguity, &amp;quot;rootness&amp;quot;,
dependency direction, non-repeatability (exclu-
sivity), obligatoriness (related to types or sets
of categories), branching structure.
A formalism called Minimal Constraint-based
Dependency Grammar has been introduced.
Although this grammatical system is not likely
to have any practical value, it could be useful
for determining the descriptive power of sim-
ple dependency systems of various kinds, since
transformations between MCD Gs and such dif-
ferent systems as Maruyama&apos;s and Gaifman&apos;s
are straightforward.
Certain functional interchangeability of cat-
egories and dependency types has been shown.
Both types and categories may be seen as pro-
viding terminal symbols with additional in-
formation giving the same syntactic potential.
Types does not affect the power in case of
category-level systems.
</bodyText>
<sectionHeader confidence="0.999008" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999570707692308">
G. Bes and P. Blache. 1999. Proprieete et anal-
yse d&apos;un language. In Proc. of TALN&apos;99,
Cargese.
Glenn Carroll and Eugene Charniak. 1992.
Two experiments on learning probabilistic de-
pendency grammars from corpora. Technical
Report CS-92-16, Brown University, Depart-
ment of Computer Science, March.
Michael A. Covington. 1990. Parsing discontin-
uous constituents with dependency grammar.
Computational Linguistics, 16(4):234-236.
Denys Duchier and Ralph Debusmann. 2001.
Topological dependency trees: A constraint-
base account of linear precedence. In Proc.
of the 39th Annual Meeting of the ACL,
Toulouse.
Jason M. Eisner. 2000. Bilexical grammars
and their cubic-time parsing algorithms. In
H. Bunt and A. Nijholt, editors, Advances in
Probabilistic and Other Parsing Technologies.
Kluwer.
Haim Gaifman. 1965. Dependency systems and
phrase-structure systems. Information and
Control, 8:304-307.
David Hays. 1964. Dependency theory: A for-
malism and some observations. Language,
40(4):511-525.
Jeffrey D. Hoperoft and John E. Ullman.
1979. Introduction to Automata Theory, Lan-
guages, and Computation. Addison-Wesley.
Richard Hudson. 1990. English Word Gram-
mar. Basil Blackwell.
Vincenzo Lombardo and Leonardo Lesmo.
1998. Formal aspects and parsing issues of
dependency theory. In Proceedings of COL-
ING/ACL&apos;98, pages 787-93, Montreal.
Hiroshi Maruyama. 1990a. Constraint depen-
dency grammar. Research report RT0044,
IBM, Tokyo.
Hiroshi Maruyama. 1990b. Structural disam-
biguation with constraint propagation. In
Proceedings of the 28th ACL, pages 31-38,
Pittsburgh, PA.
Igor A. Mel&apos;euk. 1988. Dependency Syntax:
Theory and Practice. State University of New
York Press.
Joakim Nivre. 2002. Two models of stochas-
tic dependency grammar. MSI report 02118,
Vaxjii University: School of Mathematics and
System Engineering, Vaxjo.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceed-
ings of IWPT&apos;03, pages 149-160, Nancy.
Tomasz Obrebski. 2003. MTT-compatible com-
putationally efficient surface-syntactic parser.
In Proceedings of MTT&apos;03, pages 259-268,
Paris.
Daniel Sleator and Davy Temperley. 1991.
Parsing english with a link grammar. Techni-
cal Report CMU-CS-91-196, Carnegie Mellon
University, Computer Science Department.
Christopher M. White. 1995. Converting
context-free grammars to constraint depen-
dency grammars. Master&apos;s thesis, Purdue
University, August.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777434">
<title confidence="0.999935">Some Notes on Generative Capacity of Dependency Grammars</title>
<author confidence="0.997053">Tomasz Obrcbski t Filip Graliriski t</author>
<affiliation confidence="0.991482">Institute of Control t Department of Computational Linguistics and Information Engineering and Artificial Intelligence, Electric Faculty Faculty of Mathematics Poznan University of Technology and Computer Science, Poznan, Poland Adam Mickiewicz University</affiliation>
<address confidence="0.92018">Tomasz.Obrebski@put.poznan.pl Poznan, Poland,</address>
<email confidence="0.972639">filipg@amu.edu.pl</email>
<abstract confidence="0.998281175438597">between dependency grammars and contextfree languages and some of its subclasses. Many of the facts and observations contained in this paper may well be known to a number of researchers in the field, though have been only fragmentarily articulated in the literature. Taking into consideration various types of grammatical systems we will attempt to formulate necessary and sufficient conditions for a dependency grammar to have at least CF generative capacity. The conditions will be stated in a way general enough to be applicable to a broad range of grammatical systems. We will enumerate the concepts which must be expressible in a grammatical system to ensure the CF capacity. Also, some cases when not all of the requirements are satisfied will be analysed. We will adopt the following general definition of dependency grammar which expresses, at least approximately, the essential feature shared the above-mentioned systems: a a formal system which, given two finite sets of symbols, defines a correspondence between sequences of symbols from the first set and trees whose nodes are labeled with symbols from the second set; tree nodes must be related one-to-one to sequence elements. Symbols from the first set are typically called terminal symbols or word forms, while the elements of the second set are referred to as syntactic or lexical categories. Tree arcs may or may not be labeled with dependency types. Only projective systems with atomic category symbols are considered in this paper. Some of the systems mentioned above go beyond our general definition together with these restrictions (those of Maruyama&apos;s, Duchier and Debusmann&apos;s, and Sleator and Temperley&apos;s). We are not interested, however, in examining the properties of these concrete systems as such, but only in the various ways the rules/constraints describing legal tree structures are formulated in different frameworks. Abstract The aim of this paper is to gather and add to the dispersed knowledge concerning the relation between dependency grammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Bes</author>
<author>P Blache</author>
</authors>
<title>Proprieete et analyse d&apos;un language.</title>
<date>1999</date>
<booktitle>In Proc. of TALN&apos;99,</booktitle>
<location>Cargese.</location>
<contexts>
<context position="24208" citStr="Bes and Blache, 1999" startWordPosition="4364" endWordPosition="4367">rs of the form given above have CF power since any MCDG can be easily transformed into such a typed grammar: for each c E C one leftward type 7-,,/ and one rightward type T,,, are introduced, now T891= T, Tie f t ={ Te,i ICC Cr}, Tight = Te,r C C C Tobi ={ Te,1 IE OL(C)}U {Tc,r E 0 R(C)} and D = {(c,T,,i,c&apos;) I c&apos; E Tc,r,c&apos;) c&apos; C R(c)\ {M . 0 L (c) \ {E} } { (c, This form of typed grammar can be seen as its minimal formulation. For alternative ways of expressing the obligatoriness and non-repeatability (exclusion) constraints based on types or categories, see (Duchier and Debusmann, 2001) and (Bes and Blache, 1999) respectively. 10 Conclusion The minimal requirements for a dependency grammar formalism to have context-free power have been identified. These are: the capability of expressing lexical ambiguity, &amp;quot;rootness&amp;quot;, dependency direction, non-repeatability (exclusivity), obligatoriness (related to types or sets of categories), branching structure. A formalism called Minimal Constraint-based Dependency Grammar has been introduced. Although this grammatical system is not likely to have any practical value, it could be useful for determining the descriptive power of simple dependency systems of various k</context>
</contexts>
<marker>Bes, Blache, 1999</marker>
<rawString>G. Bes and P. Blache. 1999. Proprieete et analyse d&apos;un language. In Proc. of TALN&apos;99, Cargese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenn Carroll</author>
<author>Eugene Charniak</author>
</authors>
<title>Two experiments on learning probabilistic dependency grammars from corpora.</title>
<date>1992</date>
<tech>Technical Report CS-92-16,</tech>
<institution>Brown University, Department of Computer Science,</institution>
<contexts>
<context position="3111" citStr="Carroll and Charniak (1992)" startWordPosition="477" endWordPosition="481">-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is t</context>
</contexts>
<marker>Carroll, Charniak, 1992</marker>
<rawString>Glenn Carroll and Eugene Charniak. 1992. Two experiments on learning probabilistic dependency grammars from corpora. Technical Report CS-92-16, Brown University, Department of Computer Science, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>Parsing discontinuous constituents with dependency grammar.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--4</pages>
<contexts>
<context position="3082" citStr="Covington (1990)" startWordPosition="475" endWordPosition="476">ammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect</context>
<context position="4592" citStr="Covington, 1990" startWordPosition="709" endWordPosition="710">it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it cannot be simplified wi</context>
</contexts>
<marker>Covington, 1990</marker>
<rawString>Michael A. Covington. 1990. Parsing discontinuous constituents with dependency grammar. Computational Linguistics, 16(4):234-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
<author>Ralph Debusmann</author>
</authors>
<title>Topological dependency trees: A constraintbase account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of the ACL,</booktitle>
<location>Toulouse.</location>
<contexts>
<context position="3141" citStr="Duchier and Debusmann (2001)" startWordPosition="482" endWordPosition="485">ts subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the disper</context>
<context position="4621" citStr="Duchier and Debusmann, 2001" startWordPosition="711" endWordPosition="714">ssified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it cannot be simplified without affecting its descripti</context>
<context position="24181" citStr="Duchier and Debusmann, 2001" startWordPosition="4359" endWordPosition="4362">003).) The typed dependency grammars of the form given above have CF power since any MCDG can be easily transformed into such a typed grammar: for each c E C one leftward type 7-,,/ and one rightward type T,,, are introduced, now T891= T, Tie f t ={ Te,i ICC Cr}, Tight = Te,r C C C Tobi ={ Te,1 IE OL(C)}U {Tc,r E 0 R(C)} and D = {(c,T,,i,c&apos;) I c&apos; E Tc,r,c&apos;) c&apos; C R(c)\ {M . 0 L (c) \ {E} } { (c, This form of typed grammar can be seen as its minimal formulation. For alternative ways of expressing the obligatoriness and non-repeatability (exclusion) constraints based on types or categories, see (Duchier and Debusmann, 2001) and (Bes and Blache, 1999) respectively. 10 Conclusion The minimal requirements for a dependency grammar formalism to have context-free power have been identified. These are: the capability of expressing lexical ambiguity, &amp;quot;rootness&amp;quot;, dependency direction, non-repeatability (exclusivity), obligatoriness (related to types or sets of categories), branching structure. A formalism called Minimal Constraint-based Dependency Grammar has been introduced. Although this grammatical system is not likely to have any practical value, it could be useful for determining the descriptive power of simple depe</context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Denys Duchier and Ralph Debusmann. 2001. Topological dependency trees: A constraintbase account of linear precedence. In Proc. of the 39th Annual Meeting of the ACL, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Bilexical grammars and their cubic-time parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies.</booktitle>
<editor>In H. Bunt and A. Nijholt, editors,</editor>
<publisher>Kluwer.</publisher>
<marker>Eisner, 2000</marker>
<rawString>Jason M. Eisner. 2000. Bilexical grammars and their cubic-time parsing algorithms. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
</authors>
<title>Dependency systems and phrase-structure systems.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--304</pages>
<contexts>
<context position="3046" citStr="Gaifman (1965)" startWordPosition="471" endWordPosition="472">the relation between dependency grammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, ha</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Haim Gaifman. 1965. Dependency systems and phrase-structure systems. Information and Control, 8:304-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Hays</author>
</authors>
<title>Dependency theory: A formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--4</pages>
<contexts>
<context position="3027" citStr="Hays (1964)" startWordPosition="468" endWordPosition="469">edge concerning the relation between dependency grammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>David Hays. 1964. Dependency theory: A formalism and some observations. Language, 40(4):511-525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey D Hoperoft</author>
<author>John E Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="19351" citStr="Hoperoft and Ullman, 1979" startWordPosition="3497" endWordPosition="3500">vity of this relation is ensured (No-Crossover constraints are imposed (White, 1995, p. 12)). It was shown (Maruyama, 1990a) that any context-free language can be generated by a CDG with two roles8. In the proof constraints containing only one or two variables were used (unary/binary constraints). 7A CFG is in 2-Greibach Normal Form if its productions are of the following form: X —&gt; x, X —&gt; xY, X —&gt; xY Z , where x is a terminal symbol and X, Y, Z are non-terminal symbols. Every CFG which does not generate the null string can be transformed into an equivalent grammar in 2-Greibach Normal Form (Hoperoft and Ullman, 1979). 8CDGs can generate some non-context-free languages as well. Now, let us discuss why two variables and two roles are necessary. Two variables are necessary and sufficient to express non-repeatability constraint of the type: mod(x) = mod(y) A label(x) E K A label(y) E KA pos(x) = pos(y), which means that no two distinct terminals can be connected to their modifee through labels both belonging to K, where K is a set of labels. As was demonstrated in Section 3 the necessary requirements for CF power of a dependency system include the capability of expressing both obligatoriness and non-repeatabi</context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>Jeffrey D. Hoperoft and John E. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Hudson</author>
</authors>
<title>English Word Grammar.</title>
<date>1990</date>
<publisher>Basil Blackwell.</publisher>
<contexts>
<context position="22812" citStr="Hudson, 1990" startWordPosition="4096" endWordPosition="4097">rminal symbol (x, which is linked to its head by a, can govern n nodes linked via dependencies of types Ti,..., 7-n). A C a Figure 2: From category symbols to types and vice versa To sum up, both dependency types and categories may be seen as providing terminal symbols with additional information giving the same syntactic potential. 9 Minimal constraint-based formulation with types Let us consider here a simple dependency formalism with constraints related to dependency types. Such an approach may be regarded as more elegant and is more in the spirit of linguistic descriptions (Mel&apos;euk, 1988; Hudson, 1990). Thus, let us define herein a typed version of dependency grammar as a 9-tuple (E, C, L,T,T,91, Tobi, Tieft,Trzght, D), where E, C and r have the same meaning as in an MCDC, T is a set of dependency types, Tsgl Tobl Tie f t,Trzght C T, T391 contains nonrepeatable types, Tobi — obligatory types, Tie ft — leftward types, and Trzght — rightward types, and finally DCCxTxC((c, T CI) E D means that c may govern c&apos; via dependency T). If T E Tobi then dependency T must occur for each node with category c E C such that there exists at least one c&apos; E C, (C, T, CI) C D. Note that no restriction is impos</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Richard Hudson. 1990. English Word Grammar. Basil Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincenzo Lombardo</author>
<author>Leonardo Lesmo</author>
</authors>
<title>Formal aspects and parsing issues of dependency theory.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL&apos;98,</booktitle>
<pages>787--93</pages>
<location>Montreal.</location>
<contexts>
<context position="4251" citStr="Lombardo and Lesmo, 1998" startWordPosition="657" endWordPosition="660"> Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the dispersed knowledge concerning the relation 1-The list is meant to show the variety of formal systems covered by the term &apos;dependency grammar&apos;, and is not exhaustive in any sense. 2Sleator and Temperley&apos;s formalism is called Link Grammar but it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section </context>
</contexts>
<marker>Lombardo, Lesmo, 1998</marker>
<rawString>Vincenzo Lombardo and Leonardo Lesmo. 1998. Formal aspects and parsing issues of dependency theory. In Proceedings of COLING/ACL&apos;98, pages 787-93, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
</authors>
<date>1990</date>
<booktitle>Constraint dependency grammar. Research report RT0044, IBM,</booktitle>
<location>Tokyo.</location>
<contexts>
<context position="3062" citStr="Maruyama (1990" startWordPosition="473" endWordPosition="474">ween dependency grammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been anal</context>
<context position="4574" citStr="Maruyama, 1990" startWordPosition="707" endWordPosition="708">Link Grammar but it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it canno</context>
<context position="6526" citStr="Maruyama, 1990" startWordPosition="1050" endWordPosition="1051">ch is not obligatory iff the special symbol E belongs to OL (A), • OR: c 2cu{6} defined analogously as OL, but for the right dependent. (As it is evident from the above definition, MCDGs license only binary dependency trees.) For example, the following grammar describes the language anbn: E = {a, 1)}, C = {A, B}, Coot {A}, A:= {(a,A),(b,B)}, OL(A) = OR(B) = {E}, OR(A) = {B}, OL(B) = {E, We are going to prove that MCDGs are weakly equivalent to CFCs. Our approach here is to some extent similar to the method used by Maruyama to prove the context-free power of his constraint dependency grammars (Maruyama, 1990a; White, 1995). First, let us give some auxiliary definitions: a dependency tree is defined recursively as (D1, • • • , DI, X, D1+1, • • • where 1, r &gt; 0, X E C is the root of the tree, D, are dependency (sub)trees; the root of a tree D will be denoted by r(D), i.e. r((Di, , Di, X, Di+1, ,Di+r)) = X, whereas t(D) is the sequence of categories in D: t((Di, , Di, X, Di+1, ,Di+r)) = t(Di) ... t(D1)Xt(D1+1)... t(D1) (In MCDGs /, r E {0, 1}, as only binary trees can be generated.) If D is a dependency tree and G a constraintbased dependency grammar, then G H D is to stand for: &amp;quot;D fulfils requireme</context>
<context position="17799" citStr="Maruyama (1990" startWordPosition="3218" endWordPosition="3219">rmal Form7, namely: *(X), X(*), X(*,Y), X(*, Y, Z). Let us note that Gaifman&apos;s system in the latter form can be obtained (minor details ignored) from Link Grammar by allowing at most one left (H connector for each word. The left connector would serve for linking the word to its governor, while right (±) connectors would link the word to its dependents; connector names would play the role equivalent to that of category symbols. 7 Maruyama&apos;s grammars In this section some properties of the grammatical system called Constraint Dependency Grammar (CFG) will be discussed. This system, introduced by Maruyama (1990a), differs in many respects from other dependency systems and, in general, does not fall into the definition of dependency grammar as adopted in Introduction. CDG does not relate a sequence of terminal symbols (words) to a tree, but rather to a set of directed graphs with outdegree limited to 1. These graphs are defined by role values - roles can be seen as variables, each terminal has k roles, where k is the parameter of the grammar called degree. Each role defines one graph. Role value is a pair composed of a label and an index of a terminal symbol (modifiee) and can be viewed as a labeled </context>
<context position="20487" citStr="Maruyama, 1990" startWordPosition="3692" endWordPosition="3693">tem include the capability of expressing both obligatoriness and non-repeatability. One role makes it possible to express either one or the other type of constraint but not both of them: for non-repeatability the role &amp;quot;arc&amp;quot; must point from the dependent to the head (the head plays the role of the modifee), whereas for obligatoriness the arc must go in the opposite direction (the dependent is the modifee). Hence, in order to express obligatoriness and non-repeatability at least two CDC roles are needed (cf. the use of the roles governor and needs in (White, 1995) or head-role and body-role in (Maruyama, 1990a)). Note that an MCDC can be straightforwardly expressed by a CDC with three roles: governor, needs-left, needs-right. 8 Dependency types We start our discussion of dependency types with some observations regarding the use of the label function in Maruyama&apos;s grammars. In the examples given in (Maruyama, 1990b), the label plays the role similar to that of a dependency type (some of the labels appearing in the examples: DET, SUBJ, OBJ, ROOT9). On the one hand, thus, the system may be perceived as a word-level system with typed dependencies. On the other hand, in the proof showing the context-fr</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Hiroshi Maruyama. 1990a. Constraint dependency grammar. Research report RT0044, IBM, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
</authors>
<title>Structural disambiguation with constraint propagation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th ACL,</booktitle>
<pages>31--38</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="3062" citStr="Maruyama (1990" startWordPosition="473" endWordPosition="474">ween dependency grammars and context-free languages and some of its subclasses. Necessary and sufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been anal</context>
<context position="4574" citStr="Maruyama, 1990" startWordPosition="707" endWordPosition="708">Link Grammar but it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it canno</context>
<context position="6526" citStr="Maruyama, 1990" startWordPosition="1050" endWordPosition="1051">ch is not obligatory iff the special symbol E belongs to OL (A), • OR: c 2cu{6} defined analogously as OL, but for the right dependent. (As it is evident from the above definition, MCDGs license only binary dependency trees.) For example, the following grammar describes the language anbn: E = {a, 1)}, C = {A, B}, Coot {A}, A:= {(a,A),(b,B)}, OL(A) = OR(B) = {E}, OR(A) = {B}, OL(B) = {E, We are going to prove that MCDGs are weakly equivalent to CFCs. Our approach here is to some extent similar to the method used by Maruyama to prove the context-free power of his constraint dependency grammars (Maruyama, 1990a; White, 1995). First, let us give some auxiliary definitions: a dependency tree is defined recursively as (D1, • • • , DI, X, D1+1, • • • where 1, r &gt; 0, X E C is the root of the tree, D, are dependency (sub)trees; the root of a tree D will be denoted by r(D), i.e. r((Di, , Di, X, Di+1, ,Di+r)) = X, whereas t(D) is the sequence of categories in D: t((Di, , Di, X, Di+1, ,Di+r)) = t(Di) ... t(D1)Xt(D1+1)... t(D1) (In MCDGs /, r E {0, 1}, as only binary trees can be generated.) If D is a dependency tree and G a constraintbased dependency grammar, then G H D is to stand for: &amp;quot;D fulfils requireme</context>
<context position="17799" citStr="Maruyama (1990" startWordPosition="3218" endWordPosition="3219">rmal Form7, namely: *(X), X(*), X(*,Y), X(*, Y, Z). Let us note that Gaifman&apos;s system in the latter form can be obtained (minor details ignored) from Link Grammar by allowing at most one left (H connector for each word. The left connector would serve for linking the word to its governor, while right (±) connectors would link the word to its dependents; connector names would play the role equivalent to that of category symbols. 7 Maruyama&apos;s grammars In this section some properties of the grammatical system called Constraint Dependency Grammar (CFG) will be discussed. This system, introduced by Maruyama (1990a), differs in many respects from other dependency systems and, in general, does not fall into the definition of dependency grammar as adopted in Introduction. CDG does not relate a sequence of terminal symbols (words) to a tree, but rather to a set of directed graphs with outdegree limited to 1. These graphs are defined by role values - roles can be seen as variables, each terminal has k roles, where k is the parameter of the grammar called degree. Each role defines one graph. Role value is a pair composed of a label and an index of a terminal symbol (modifiee) and can be viewed as a labeled </context>
<context position="20487" citStr="Maruyama, 1990" startWordPosition="3692" endWordPosition="3693">tem include the capability of expressing both obligatoriness and non-repeatability. One role makes it possible to express either one or the other type of constraint but not both of them: for non-repeatability the role &amp;quot;arc&amp;quot; must point from the dependent to the head (the head plays the role of the modifee), whereas for obligatoriness the arc must go in the opposite direction (the dependent is the modifee). Hence, in order to express obligatoriness and non-repeatability at least two CDC roles are needed (cf. the use of the roles governor and needs in (White, 1995) or head-role and body-role in (Maruyama, 1990a)). Note that an MCDC can be straightforwardly expressed by a CDC with three roles: governor, needs-left, needs-right. 8 Dependency types We start our discussion of dependency types with some observations regarding the use of the label function in Maruyama&apos;s grammars. In the examples given in (Maruyama, 1990b), the label plays the role similar to that of a dependency type (some of the labels appearing in the examples: DET, SUBJ, OBJ, ROOT9). On the one hand, thus, the system may be perceived as a word-level system with typed dependencies. On the other hand, in the proof showing the context-fr</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>Hiroshi Maruyama. 1990b. Structural disambiguation with constraint propagation. In Proceedings of the 28th ACL, pages 31-38, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel&apos;euk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<contexts>
<context position="22797" citStr="Mel&apos;euk, 1988" startWordPosition="4094" endWordPosition="4095">es and x — a terminal symbol (x, which is linked to its head by a, can govern n nodes linked via dependencies of types Ti,..., 7-n). A C a Figure 2: From category symbols to types and vice versa To sum up, both dependency types and categories may be seen as providing terminal symbols with additional information giving the same syntactic potential. 9 Minimal constraint-based formulation with types Let us consider here a simple dependency formalism with constraints related to dependency types. Such an approach may be regarded as more elegant and is more in the spirit of linguistic descriptions (Mel&apos;euk, 1988; Hudson, 1990). Thus, let us define herein a typed version of dependency grammar as a 9-tuple (E, C, L,T,T,91, Tobi, Tieft,Trzght, D), where E, C and r have the same meaning as in an MCDC, T is a set of dependency types, Tsgl Tobl Tie f t,Trzght C T, T391 contains nonrepeatable types, Tobi — obligatory types, Tie ft — leftward types, and Trzght — rightward types, and finally DCCxTxC((c, T CI) E D means that c may govern c&apos; via dependency T). If T E Tobi then dependency T must occur for each node with category c E C such that there exists at least one c&apos; E C, (C, T, CI) C D. Note that no restr</context>
</contexts>
<marker>Mel&apos;euk, 1988</marker>
<rawString>Igor A. Mel&apos;euk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Two models of stochastic dependency grammar.</title>
<date>2002</date>
<tech>MSI report 02118,</tech>
<institution>Vaxjii University: School of Mathematics and System Engineering, Vaxjo.</institution>
<contexts>
<context position="3493" citStr="Nivre, 2002" startWordPosition="535" endWordPosition="536"> in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the dispersed knowledge concerning the relation 1-The list is meant to show the variety of formal systems covered by the term &apos;dependency grammar&apos;, and is not exhaustive in any sense. 2Sleator and Temperley&apos;s formalism is called Link Grammar but it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broa</context>
<context position="13401" citStr="Nivre (2002)" startWordPosition="2438" endWordPosition="2439"> dependents (e.g. their relative order) other than their mutual exclusion, • repeatable dependencies (in whatever meaning). 4 No lexical ambiguity Dependency systems without lexical ambiguity, i.e. ones in which the lexicon is degenerated to a one-to-one relation between terminal and category symbols or in which category symbols do not appear at all, come as grammatical components of several stochastic language models. The generative capacity of two such systems — Carroll and Charniak&apos;s (1992) Context-Free Dependency Crammar3 and Eisner&apos;s (2000) Bilexical Dependency Crammar4 — was analysed by Nivre (2002). He shows that the sets of languages these systems generate are not equal and are both proper subsets of context-free languages (and are neither subsets nor supersets of regular languages)5, which supports the claim (1) from Section 3. In the following, the grammatical systems with a one-to-one (or many-to-one) lexicon will be called word-level grammars, whereas the systems with a one-to-many (or many-to-many) lexicon — category-level grammars. Two comments to Nivre&apos;s observations are in order. Firstly, what is clear but not stated explicitly by Nivre, the reason for the weakness of these gra</context>
</contexts>
<marker>Nivre, 2002</marker>
<rawString>Joakim Nivre. 2002. Two models of stochastic dependency grammar. MSI report 02118, Vaxjii University: School of Mathematics and System Engineering, Vaxjo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT&apos;03,</booktitle>
<pages>149--160</pages>
<location>Nancy.</location>
<contexts>
<context position="3155" citStr="Nivre (2003)" startWordPosition="486" endWordPosition="487">ufficient conditions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the dispersed knowledge </context>
<context position="4635" citStr="Nivre, 2003" startWordPosition="715" endWordPosition="716">m. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simply enumerate all possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it cannot be simplified without affecting its descriptive power. This</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT&apos;03, pages 149-160, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Obrebski</author>
</authors>
<title>MTT-compatible computationally efficient surface-syntactic parser.</title>
<date>2003</date>
<booktitle>In Proceedings of MTT&apos;03,</booktitle>
<pages>259--268</pages>
<location>Paris.</location>
<contexts>
<context position="23557" citStr="Obrebski, 2003" startWordPosition="4240" endWordPosition="4241">C and r have the same meaning as in an MCDC, T is a set of dependency types, Tsgl Tobl Tie f t,Trzght C T, T391 contains nonrepeatable types, Tobi — obligatory types, Tie ft — leftward types, and Trzght — rightward types, and finally DCCxTxC((c, T CI) E D means that c may govern c&apos; via dependency T). If T E Tobi then dependency T must occur for each node with category c E C such that there exists at least one c&apos; E C, (C, T, CI) C D. Note that no restriction is imposed on the relative order of dependents. (This kind of formalism forms the basis of the dependency system for Polish described in (Obrebski, 2003).) The typed dependency grammars of the form given above have CF power since any MCDG can be easily transformed into such a typed grammar: for each c E C one leftward type 7-,,/ and one rightward type T,,, are introduced, now T891= T, Tie f t ={ Te,i ICC Cr}, Tight = Te,r C C C Tobi ={ Te,1 IE OL(C)}U {Tc,r E 0 R(C)} and D = {(c,T,,i,c&apos;) I c&apos; E Tc,r,c&apos;) c&apos; C R(c)\ {M . 0 L (c) \ {E} } { (c, This form of typed grammar can be seen as its minimal formulation. For alternative ways of expressing the obligatoriness and non-repeatability (exclusion) constraints based on types or categories, see (Duch</context>
</contexts>
<marker>Obrebski, 2003</marker>
<rawString>Tomasz Obrebski. 2003. MTT-compatible computationally efficient surface-syntactic parser. In Proceedings of MTT&apos;03, pages 259-268, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing english with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Carnegie Mellon University, Computer Science Department.</institution>
<contexts>
<context position="3185" citStr="Sleator and Temperley (1991)" startWordPosition="488" endWordPosition="491">itions for a dependency system to have context-free power are formulated in a way general enough to be applicable to a broad range of grammatical systems, based on rules or constraints. Certain cases when some of these requirements are not satisfied are also analysed. Formal implications of the presence of dependency types in grammatical systems are discussed. 1 Introduction The notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the dispersed knowledge concerning the relation 1-The </context>
<context position="4791" citStr="Sleator and Temperley, 1991" startWordPosition="738" endWordPosition="741"> possible branchings which may occur in a tree (Gaifman, 1965; Lombardo and Lesmo, 1998; Carroll and Charniak, 1992). We will call them rule-based systems. Into the second class – constraint-based systems – fall formalisms where the well-formedness conditions on acceptable tree structures are formulated by means of constraints stating e.g. what arcs are allowed or necessary in given contexts (Maruyama, 1990b; Covington, 1990; Duchier and Debusmann, 2001; Nivre, 2003). As a special kind of rule-based system we can classify Link Grammar with the word descriptions considered in disjunctive form (Sleator and Temperley, 1991, 8-9), see also remarks concerning Link Grammar in Section 6. 2 Minimal constraint-based formulation Our aim is to examine which conditions are necessary and sufficient for a dependency formalism to be capable of generating all CF languages. First, we will introduce a simple constraintbased formalism which has CF generative capacity and which is minimal in the sense that it cannot be simplified without affecting its descriptive power. This simple system, in our opinion, provides good intuitions as to what has to be expressible in a formal grammatical system to obtain CF power. It should be ma</context>
</contexts>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel Sleator and Davy Temperley. 1991. Parsing english with a link grammar. Technical Report CMU-CS-91-196, Carnegie Mellon University, Computer Science Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M White</author>
</authors>
<title>Converting context-free grammars to constraint dependency grammars. Master&apos;s thesis,</title>
<date>1995</date>
<institution>Purdue University,</institution>
<contexts>
<context position="3548" citStr="White, 1995" startWordPosition="542" endWordPosition="543">he notion of dependency grammar is somewhat vague. Formal systems introduced by Hays (1964) and Gaifman (1965), Maruyama (1990b), Covington (1990), Carroll and Charniak (1992), Duchier and Debusmann (2001), Nivre (2003), Sleator and Temperley (1991) 1 are all called dependency grammars2, although they are significantly different and have different formal properties. A number of results concerning descriptive capacity of some types of dependency systems have been reported in the literature, mostly for Gaifman-style grammars (Gaifman, 1965; Nivre, 2002) and Maruyama&apos;s grammars (Maruyama, 1990a; White, 1995). Other kinds of systems, in particular the constraint-based systems other than Maruyama&apos;s one, have not been analysed in this respect. The goal of this paper is to gather and add to the dispersed knowledge concerning the relation 1-The list is meant to show the variety of formal systems covered by the term &apos;dependency grammar&apos;, and is not exhaustive in any sense. 2Sleator and Temperley&apos;s formalism is called Link Grammar but it is usually classified as a dependency system. In this respect the dependency systems may be divided into two broad classes. Into the first class fall systems which simp</context>
<context position="6541" citStr="White, 1995" startWordPosition="1052" endWordPosition="1053">ory iff the special symbol E belongs to OL (A), • OR: c 2cu{6} defined analogously as OL, but for the right dependent. (As it is evident from the above definition, MCDGs license only binary dependency trees.) For example, the following grammar describes the language anbn: E = {a, 1)}, C = {A, B}, Coot {A}, A:= {(a,A),(b,B)}, OL(A) = OR(B) = {E}, OR(A) = {B}, OL(B) = {E, We are going to prove that MCDGs are weakly equivalent to CFCs. Our approach here is to some extent similar to the method used by Maruyama to prove the context-free power of his constraint dependency grammars (Maruyama, 1990a; White, 1995). First, let us give some auxiliary definitions: a dependency tree is defined recursively as (D1, • • • , DI, X, D1+1, • • • where 1, r &gt; 0, X E C is the root of the tree, D, are dependency (sub)trees; the root of a tree D will be denoted by r(D), i.e. r((Di, , Di, X, Di+1, ,Di+r)) = X, whereas t(D) is the sequence of categories in D: t((Di, , Di, X, Di+1, ,Di+r)) = t(Di) ... t(D1)Xt(D1+1)... t(D1) (In MCDGs /, r E {0, 1}, as only binary trees can be generated.) If D is a dependency tree and G a constraintbased dependency grammar, then G H D is to stand for: &amp;quot;D fulfils requirements expressed b</context>
<context position="18808" citStr="White, 1995" startWordPosition="3399" endWordPosition="3400"> where k is the parameter of the grammar called degree. Each role defines one graph. Role value is a pair composed of a label and an index of a terminal symbol (modifiee) and can be viewed as a labeled arc pointing from one terminal to another. The constraints in CDGs are expressed by means of logical formulae over assignments of role values to roles. We will take into consideration only the subclass of CDGs restricted to systems in which (1) roles determine the dependency relation between terminal symbols and (2) projectivity of this relation is ensured (No-Crossover constraints are imposed (White, 1995, p. 12)). It was shown (Maruyama, 1990a) that any context-free language can be generated by a CDG with two roles8. In the proof constraints containing only one or two variables were used (unary/binary constraints). 7A CFG is in 2-Greibach Normal Form if its productions are of the following form: X —&gt; x, X —&gt; xY, X —&gt; xY Z , where x is a terminal symbol and X, Y, Z are non-terminal symbols. Every CFG which does not generate the null string can be transformed into an equivalent grammar in 2-Greibach Normal Form (Hoperoft and Ullman, 1979). 8CDGs can generate some non-context-free languages as w</context>
<context position="20441" citStr="White, 1995" startWordPosition="3685" endWordPosition="3686">equirements for CF power of a dependency system include the capability of expressing both obligatoriness and non-repeatability. One role makes it possible to express either one or the other type of constraint but not both of them: for non-repeatability the role &amp;quot;arc&amp;quot; must point from the dependent to the head (the head plays the role of the modifee), whereas for obligatoriness the arc must go in the opposite direction (the dependent is the modifee). Hence, in order to express obligatoriness and non-repeatability at least two CDC roles are needed (cf. the use of the roles governor and needs in (White, 1995) or head-role and body-role in (Maruyama, 1990a)). Note that an MCDC can be straightforwardly expressed by a CDC with three roles: governor, needs-left, needs-right. 8 Dependency types We start our discussion of dependency types with some observations regarding the use of the label function in Maruyama&apos;s grammars. In the examples given in (Maruyama, 1990b), the label plays the role similar to that of a dependency type (some of the labels appearing in the examples: DET, SUBJ, OBJ, ROOT9). On the one hand, thus, the system may be perceived as a word-level system with typed dependencies. On the o</context>
</contexts>
<marker>White, 1995</marker>
<rawString>Christopher M. White. 1995. Converting context-free grammars to constraint dependency grammars. Master&apos;s thesis, Purdue University, August.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>