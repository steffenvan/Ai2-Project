<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000116">
<title confidence="0.739145">
Lexical Concept Acquisition From Collocation Map 1
</title>
<author confidence="0.918098">
Young S. Han, Young Kyoon Han, and Key-Sun Choi
</author>
<affiliation confidence="0.872589">
Computer Science Department
Korea Advanced Institute of Science and Technology
Taejon, 305-701, Korea
</affiliation>
<email confidence="0.980757">
yshan@csking.kaist.ac.kr, kschoi@csking.kaistac.kr
</email>
<sectionHeader confidence="0.959561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991375">
This paper introduces an algorithm for automatically acquiring the conceptual struc-
ture of each word from corpus. The concept of a word is defined within the proba-
bilistic framework. A variation of Belief Net named as Collocation Map is used to
compute the probabilities. The Belief Net captures the conditional independences
of words, which is obtained from the cooccurrence relations. The computation in
general Belief Nets is known to be NP-hard, so we adopted Gibbs sampling for the
approximation of the probabilities.
The use of Belief Net to model the lexical meaning is unique in that the network is
larger than expected in most other applications, and this changes the attitude toward
the use of Belief Net. The lexical concept obtained from the Collocation Map best
reflects the subdomain of language usage. The potential application of conditional
probabilities the Collocation Map provides may extend to cover very diverse areas of
language processing such as sense disambiguation, thesaurus construction, automatic
indexing, and document classification.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997270636363636">
The level of the conceptual representation of words can be very complex in certain con-
texts, but in this paper we assume rather simple structure in which a concept is a set
of weighted associated words. We propose an automatic concept acquisition framework
based on the conditional probabilities suppliedd by a network representation of lexical re-
lations. The network is in the spirit of Belief Net, but the probabilities are not necessarily
Bayesian. In fact this variation of Bayesian Net is discussed recently by (Neal, 1992). We
employed the Belief Net with non Bayesian probabilities as a base for representing the
statistical relations among concepts, and implemented the details of the computation.
Belief or Bayesian Nets have been extensively studied in the normative expert systems
(Heckerman, 1991). Experts provided the network with the Bayesian(subjective) proba-
bilities solely based on his/her technical experiences. Thus the net has been also known
as a Belief Net among a dozen other names that share all or some of the principles of
Bayesian net. The probabilistic model has been also used in the problems of integrating
various sources of evidences within sound framework (Cho, 1992). One of the powerful
features of Belief Net is that the conditional independences of the variables in the model
are naturally captured, on which we can derive a form of probabilistic inference. If we
regard the occurrence of a word as a model variable and assume the variables occur within
some conditional influences of the variables(words) that previously took place, the Belief
approach appears to be appropriate to compute some aspects of lexical relations latent in
1T1,is work was supported in part by a grant from Korea National Science Foundation as a basic
research project and by a grant from Korea Ministry of Science and Technology in project &amp;quot;an intelligent
multimedia information system platform and image signal transmission in high speed network&amp;quot;
</bodyText>
<page confidence="0.993816">
22
</page>
<bodyText confidence="0.999929923076923">
the texts. The probabilities on dependent variables are computed from the frequencies,
so the probability is now of objective nature rather than Bayesian.
The variation of Belief Net we use is identical to the sigmoid Belief Net by Neal (1992).
In ordinary Belief Nets, 2&amp;quot; probabilities for a parent variable with n children should be
specified. This certainly is a burden in our context in which the net may contain even
hundred thousands of variables with heavy interconnections. Sigmoid interpretation of
the connections as in artificial neural networks provides a solution to the problem without
damaging the power of the network. Computing a joint probability is also exponential
in an arbitrary Belief network, thus Gibbs sampling which originates from Metropolis
algorithm introduced in 50&apos;s can be used to approximate the probabilities. To speed
up the convergence of the sampling we adopted simulated annealing algorithm with the
sampling. The simulated annealing is also a descendant of metropolis algorithm, and has
been frequently used to compute an optimal state vector of a system of variables.
From the Collocation Map we can compute an arbitrary conditional probabilities of
variables. This is a very powerful utility applicable to every level of language processing.
To name a few automatic indexing, document classification, thesaurus construction, and
ambiguity resolution are promising areas. But one big problem with the model is that it
cannot be used in real time applications because the Gibbs sampling still requires an ample
amount of computation. Some applications such as automatic indexing and lexical concept
acquisition are fortunately not real time bounded tasks. We are currently undertaking
a large scale testing of the model involving one hundred thousand words, which includes
the study on the cost of sampling versus the accuracy of probability.
To reduce the computational cost in time, the multiprocessor model that is success-
fully implemented for Hopfield Network(Yoon, 1992) can be considered in the context of
sampling. Other options to make the sampling efficient should be actively pursued, and
their success is the key to the implementation of the model to the real time problems.
</bodyText>
<sectionHeader confidence="0.780636" genericHeader="method">
2 Definition of Lexical Concept
</sectionHeader>
<bodyText confidence="0.999766">
Whenever we think of a word, we are immediately reminded of some form of meaning
of the word. The reminded structure can be very diverse in size and the type of the
information that the structure delivers. Though it is not very clear at this point what the
structure is and how it is derived, we are sure that at least some type of the reminded
structure is readily converted to the verbal representation. Then the content of vebral
form must be a clue to the reminded structure. The reminded structure is commonly
referred to as the meaning of a word. Still the verbal representation can be arbitrarily
complex, yet the representation is made up of words. Thus the words in the clue to the
meaning of a word seem to be an important element of the meaning.
Now define the concept of a word as
Definition 1 The lexical concept of a word is a set of associated words that are weighted
by their associativeness.
The notion of association is rather broadly defined. A word is associated with another
word when the one word is likely to occur in the clue of the reminded structure of the other
word in some relations. The association by its definition can be seen as a probabilistic
function of two words. Some words are certainly more likely to occur in association with
a particular word. The likeliness may be deterministically explained by some formal
</bodyText>
<page confidence="0.996616">
23
</page>
<bodyText confidence="0.9851955">
theories, but we believe it is more of inductive(experimental) process. Now define the
concept a of word w as a probabilistic distribution of its associated words.
</bodyText>
<equation confidence="0.797954">
a(w) ={ (wi, pi)} , (1)
where
pi = P( wi I w) , and
&gt; T.
</equation>
<bodyText confidence="0.9999089">
Thus the set of associated words consists of those whose probability is above threshold
value T. The probabilistic distribution of words may exist independently of the influence of
relations among words though it is true that relations in fact can affect the distribution.
But in this paper we do not take other information into the model. If we do so, the
model will have the complexity and sophistication of knowledge representation. Such an
approach is exemplified by the work of Goldman and Charniak (1992).
Equation 1 can be further elaborated in several ways. It seems that the concept of
a word as in Equation 1 may not be sufficient. That is, Equation 1 is about the direct
association of a given word. Indirect association can also contribute to the meaning of a
word. Now define the expanded concept of a word as
</bodyText>
<equation confidence="0.9476475">
cr&apos;(w) = (wi, pi)) U (vi, qi)} , (2)
where
qi = P( vi I a(w)) , and
qi &gt; T .
Or,
a&apos;(w) = { (wi, pin U {o(w)} â€¢ (3)
</equation>
<bodyText confidence="0.9993465">
If the indirect association is repeated for several depths a class of words in particular
aspects can be obtained. A potential application of Equation 3 and 4 is the automatic
thesaurus construction. Subsumption relation between words may be computed by care-
fully expanding the meaning of the words. The subsumption relation, however, may not
be based on the meaning of the words, but it rather be defined in statistical nature.
The definition of lexical meaning as we defined is simple, and yet powerful in many
ways. For instance, the distance between words can be easily computed from the rep-
resentation. The probabilistic elements of the representation make the acquisition an
experimental process and base the meaning of words on more consistent foundation. The
computation of Equation 1, however, is not simple. In the next section we define Collo-
cation Map and explain the algorithm to compute the conditional probabilities from the
Map.
</bodyText>
<page confidence="0.996238">
24
</page>
<figure confidence="0.754477">
Â®
</figure>
<figureCaption confidence="0.9999745">
Figure 1: DG to DAG
Figure 2: Word Dependency in Collocation Map
</figureCaption>
<sectionHeader confidence="0.979177" genericHeader="method">
3 Collocation Map
</sectionHeader>
<bodyText confidence="0.963028818181818">
Collocation map is a kind of Belief Net or knowledge map that represents the dependencies
among words(concepts). As it does not have decision variables and utility, it is different
from influence diagram. One problem with knowledge map is that it does not allow cycles
while words can be mutually dependent. Being DAG is a big advantage of the formalism
in computing probabilistic decisions, so we cannot help but stick to it. A cyclic relation
should be broken into linear form as shown in figure 1. Considering the size of collocation
map and the connectivity of nodes in our context is huge it is not practical to maintain
all the combination of conditional probabilities for each node. For instance if a node has
n conditioning nodes there will be 2n units of probability information to be stored in the
node. We limit the scope to the direct dependencies denoted by arcs.
What follows is about the dependency between two words. In figure 2,
</bodyText>
<equation confidence="0.9999925">
P(bla) = Pi, (4)
P(cia) = P2. (5)
</equation>
<bodyText confidence="0.9999855">
P1 denotes the probability that word b occurs provided word a occurred. Once a text
is transformed into an ordered set of words, the list should be decomposed into binary
relations of words to be expressed in collocation map. Here in fact we are making an
implicit assumption that if a word physically occurs frequently around another word, the
first word is likely to occur in the reminded structure of the second word. In other words,
physical occurrence order may be a cause to the formation of associativeness among words.
</bodyText>
<equation confidence="0.548582">
Di = (a, b, c, d, e, f , â€¢ â€¢ â€¢ , z).
</equation>
<bodyText confidence="0.902675">
When Di is represented by a, b, c, â€¢ , z), the set of binary relations with window size
3(let us call this set it3) format is as follows.
</bodyText>
<page confidence="0.902757">
25
</page>
<equation confidence="0.558287">
Ds; = (ab, ac,bc,ad,bd, cd,be,ce,de,c f, â€¢ â€¢ -,).
</equation>
<bodyText confidence="0.999103333333333">
For words di and c, P(ci Idi) can be computed at least in two ways. As mentioned
earlier, we take the probability in the sense of frequency rather than belief. In the first
method,
</bodyText>
<equation confidence="0.999584666666667">
P(cildi) (cidi)
f
1(d1) (6)
</equation>
<bodyText confidence="0.9815698">
where i &lt; j.
Each node di in the map maintains two variables f(di) and f(dici), while each arc
keeps the information of P(ci Idi). From the probabilities in arcs the joint distribution
over all variables in the map can be computed, then any conditional probability can be
computed. Let S denote the state vector of the map.
</bodyText>
<equation confidence="0.974873">
= = HP(Si = sISi =Si: j &lt; i) . (7)
</equation>
<bodyText confidence="0.999985">
Computing exact conditional probability or marginal probability requires often exponen-
tial resources as the problem is know to be NF-hard. Gibb&apos;s sampling must be one of the
best solutions for computing conditional or marginal probabilities in a network such as
collocation map. It approximates the probabilities, and when optimal solutions are asked
simulated annealing can be incorporated. Not only for computing probabilities, pattern
completion and pattern classification can be done through the map using Gibb&apos;s sampling.
In Gibb&apos;s sampling, the system begins at an arbitrary state or a given S, and a free
variable is selected arbitrarily or by a selecting function, then the value of the variable
will be alternated. Once the selection is done, we may want to compute P(S =) or other
function of S. As the step is repeated, the set of S&apos;s form a sample. In choosing the next
variable, the following probability can be considered.
</bodyText>
<equation confidence="0.97981325">
P (si = I si = i)
cx P(Si = x ISi = si : j &lt; i) â€¢
H P(Si = si ISi = Z,Sk = Sk : k &lt; j,k i). (8)
.i&gt;i
</equation>
<bodyText confidence="0.972397714285714">
The probability is acquired from samples by recording frequencies, and can be up-
dated as the frequencies change. The second method is inspired by the model of (Neal
1992) which shares much similarity with Boltzmann Machine. The difference is that the
collocation map has directed arcs. The probability that a node takes a particular value is
measured by the energy difference caused by the value of the node.
P(51= silSj =S1 :j &lt; i) = cr(siE . ( 9 )
i&lt;i
</bodyText>
<page confidence="0.883381">
26
</page>
<figure confidence="0.938404">
Hidden Units
</figure>
<figureCaption confidence="0.991949">
Figure 3: Collocation Map with Hidden Units
</figureCaption>
<table confidence="0.671931857142857">
where O(t) = 1
A node takes -1 or 1 as its value.
1 + e-t â€¢
P(S= = P(S1 = silSi = si :j &lt; i) (10)
=V.&amp;quot;&apos;
. ( z_asjwii) .
i&lt;i
</table>
<bodyText confidence="0.954458">
Conditional and marginal probabilities can be approximated from Gibb&apos;s sampling. A
selection of next node to change has the following probability distribution.
</bodyText>
<equation confidence="0.992112333333333">
P(Si = xlSi = si : j 0
oc cr(x E.â€žwii) â€¢ 11 + E sktv,k))â€¢
1&gt;1 k&lt;j,k$i
</equation>
<bodyText confidence="0.999964857142857">
The acquisition of probability for each arc in the second method is more complicated
than the first one. In principle, the general patterns of variables cannot be captured
without the assistance of hidden nodes. Since in our case the pattern classification is not
an absolute requirement, we may omit the hidden nodes after careful testing. If we employ
hidden units, the collocation map may look as in figure 5 for instance.
Learning is done by changing the weights in arcs. As in (Neal, 1992), we adopt gradient
ascent algorithm that maximize log-likelihood of patterns.
</bodyText>
<equation confidence="0.990733">
L = log IT P(ci = &amp;quot;i3) =E log P(iT. = i3) . (12)
if&apos; ET /ET
= â€”sisjolâ€”siEskwik), (13)
k&lt;i
</equation>
<bodyText confidence="0.989586">
where N = ITI .
Batch learning over all the patterns is, however, unrealistic in our case considering
the size of collocation map and the gradual nature of updating. It is hard to vision
</bodyText>
<page confidence="0.990417">
27
</page>
<bodyText confidence="0.999954785714286">
that whole learning is readjusted every time a new document is to be learned. Gradual
learning(non batch) may degrade the performance of pattern classification probably by a
significant degree, but what we want to do with collocation map is not a clear cut pattern
identification up to each learning instance, but is a much more brute categorization. One
way to implement the learning is first to clamp the nodes corresponding to the input set
of binary dependencies, then run Gibb&apos;s sampling for a while. Then, add the average of
energy changes of each arc to the existing values.
So far we have discussed about computing the conditional probability from Collocation
Map. But the use of the algorithm is not limited to the acquisition of lexical concept.
The areas of the application of the Collocation Map seems to reach virtually every corner
of natural language processing and other text processing such as automatic indexing.
An indexing problem is to order the words appearing in a document by their relative
importance with respect to the document. Then the weight 0(wi) of each word is the
probability of the word conditioned by the rest of the words.
</bodyText>
<equation confidence="0.954373">
0(wi) = P( wi I ?DJ, i) â€¢ (14)
</equation>
<bodyText confidence="0.999293625">
The application of the Collocation Map in the automatic indexing is covered in detail
in Han (1993).
In the following we illustrate the function of Collocation Map by way of an example.
The Collocation Map is built from the first 12500 nouns in the textbook collection in Penn
Tree Bank. Weights are estimated using the mutual information measure. The topics of
the textbook used includes the subjects on planting where measuring by weight and length
is frequently mentioned. Consider the two probabilities as a result of the sampling on the
Collocation Map.
</bodyText>
<equation confidence="0.929164333333333">
P(depthlinch) = 0.51325,
and
P(weightlinch) = 0.19969.
</equation>
<bodyText confidence="0.999970909090909">
When the sampling was loosened, the values were 0.3075 and 0 respectively. The first
version took about two minutes, and the second one about a minute in Sun 4 workstation.
The quality of sampling can be controlled by adjusting the constant factor, the cooling
speed of temperature in simulated annealing, and the sampling density. The simple ex-
periment agrees with our intuition, and this demonstrates the potentail of Collocation
Map. It, however, should be noted that the coded information in the Map is at best local.
When the Map is applied to other areas, the values will not be very meaningful. This may
sound like a limitation of Collocation Map like approach, but can be an advantage. No
system in practice will be completely general, nor is it desirable in many cases. Figure 4
shows a dumped content of node tree in the Collocation Map, which is one of 4888 nodes
in the Map.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998157">
We have introduced a representation of lexical knowledge encoding from which an arbi-
trary conditional probability can be computed, thereby rendering an automatic acquisition
</bodyText>
<page confidence="0.994563">
28
</page>
<figure confidence="0.918416911111111">
&lt; h23 &gt; tree
f:
inch
period
mulch
plant
b:
di(36494) ctr(92)
mi(19) rooting
mi(22) straw
mi(29) pulling
mi(1) root mi(13)
mi(20) resistance mi(21)
mi(31) evaporation mi(32)
mi(34) flower mi(5)
moisture mi(28)
shrub
water
under-watering
fertilizer
tree
March
pecan
temperature
plant
fruit
mulch
blueberry
shade
planting
bank
branch
landscape
cooling
ground
inch
period
position
pocket
metal
place
grass
command
bird
building
</figure>
<equation confidence="0.996806103448276">
mi(24) c(4)
mi(26) c(3)
mi(36) c(1)
mi(59) c(3)
mi(58) c(5)
mi(54) c(2)
mi(102) c(2)
mi(106) c(1)
mi(9) c(1)
mi(107) c(1)
mi(33) c(1)
mi(123) c(1)
mi(130) c(4)
mi(155) c(1)
mi(350) c(1)
mi(172) c(1)
mi(368) c(2)
mi(586) c(1)
mi(126) c(2)
mi(133) c(1)
mi(181) c(1)
mi(596) c(1)
mi(605) c(2)
mi(612) c(1)
mi(443) c(1)
mi(381) c(1)
mi(1815) c(1)
mi(701) c(1)
mi(307) c(1)
</equation>
<figure confidence="0.974372793103448">
0.043478
0.032609
0.010870
0.032609
0.054348
0.021739
0.021739
0.010870
0.010870
0.010870
0.010870
0.010870
0.043478
0.010870
0.010870
0.010870
0.021739
0.010870
0.021739
0.010870
0.010870
0.010870
0.021739
0.010870
0.010870
0.010870
0.010870
0.010870
0.010870
</figure>
<bodyText confidence="0.921695689655172">
sprinkler
system
over-watering
hole
pound
growing
spring
February
thing
cutting
rabbiteye
ajuga
area
slope
trunk
myrtle
heating
step
root
drying
crowding
transplanting
evaporation
stake
people
triangle
breath
stone
Government
</bodyText>
<equation confidence="0.997857482758621">
mi(25) c(1)
mi(35) c(1)
mi(37) c(1)
mi(60) c(1)
mi(61) c(3)
mi(105) c(2)
mi(42) c(2)
mi(38) c(2)
mi(43) c(1)
mi(114) c(1)
mi(122) c(1)
mi(124) c(1)
mi(131) c(1)
mi(132) c(1)
mi(225) c(5)
mi(194) c(2)
mi(585) c(1)
mi(588) c(1)
mi(141) c(4)
mi(590) c(1)
mi(595) c(1)
mi(594) c(1)
mi(609) c(1)
mi(613) c(3)
mi(267) c(1)
mi(616) c(1)
mi(1334) c(1)
mi(1813) c(1)
mi(2337) c(1)
</equation>
<figure confidence="0.989777517241379">
0.010870
0.010870
0.010870
0.010870
0.032609
0.021739
0.021739
0.021739
0.010870
0.010870
0.010870
0.010870
0.010870
0.010870
0.054348
0.021739
0.010870
0.010870
0.043478
0.010870
0.010870
0.010870
0.010870
0.032609
0.010870
0.010870
0.010870
0.010870
0.010870
</figure>
<figureCaption confidence="0.93909">
Figure 4: Dumped Content of the node tree in the Collocation Map &lt; h23 &gt; indicates
the index of tree in the Map is 23. di(19) is an index to dictionary. ctr(92) says tree
occurred 92 times. mi(19) indicates the index of inch in the Map is 19. c(4) of shrub says
shrub occurred 4 times in the back list.
</figureCaption>
<page confidence="0.992484">
29
</page>
<bodyText confidence="0.999900307692308">
of lexical concept. The representation named Collocation Map is a variation of Belief Net
that uses sigmoid function in summing the conditioning evidences. The dependency is
not as strong as that of ordinary Belief Net, but is of event occurrence.
The potential power of Collocation Map can be fully appreciated when the computa-
tional overhead is further reduced. Several options to alleviate the computational burden
are also begin studied in two approaches. The one is parallel algorithm for Gibbs sampling
and the other is to localize or optimize the sampling itself. Preliminary test on the Map
built from 100 texts shows a promising outlook, and we currently having a large scale
testing on 75,000 Korean text units(two million word corpus) and Pentree Bank. The
aims of the test include the accuracy of modified sampling, sampling cost versus accuracy,
comparison with the Boltzman machine implementation of the Collocation Map, Lexical
Concept Acquisition, thesaurus construction, and sense disambiguation problems such as
in PP attachment and homonym resolution.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99994656">
[1] Baker, J. K. 1979. Trainable grammars for speech recognition. Proceedings of Spring
Conference of the Acoustical Society of America, 547-550. Boston, MA.
[2] Ackley, G.E. Hinton and T.J. Sejnowski. (1985). A Learning Algorithm for Boltzmann
machines, Cognitive Science. 9. 147-169.
[3] Cho, Sehyeong, Maida, Anthony S. (1992). &amp;quot;Using a Bayesian Framework to Identify
the Referents of Definite Descriptions.&amp;quot; AAAI Fall Symposium, Cambridge, Mas-
sachusetts.
[4] Dempster, A.P. Laird, N.M. and Rubin, D.B. (1977). Maximum likelihood from in-
complete data via the EM algorithm, J. Roy. Stat. Soc. B 39, 1-38.
[5] Gelfan, A.E. and Smith, A.F.M. (1990). Sampling-based approaches to calculating
marginal densities, J. Am. Stat. Assoc 85. 398-409.
[6] Goldman, Robert P. and Charniak Eugene. (1992). Probabilistic Text Understanding.
Statistics and Computing. 2:105-114.
[7] Han, Young S. Choi, Key-Sun. (1993). Indexing Based on Formal Relevancy of
Bayesian Document Semantics. Korea/Japan Joint Conferenceon on Expert Systems,
Seoul, Korea.
[8] Lauritzen and Spiegelhalter, D.J. (1988). Local computation with probabilities on
graphical structures and their application to expert systems. J. Roy. Stat. Soc. 50.
157-224.
[9] Metropolis, N. Rosenbluth, A. W. Teller, A.H. Teller and Teller, E. (1953). Equation
of state calculations by fast computing machines. J. Chem. Phys. 21. 1087-1092.
[10] Neal, R.M. (1992). Connectionist learning of belief network. Artificial Intelligence 56.
71-113.
[11] Pearl, J. (1988). Probabilistic Reasoning in Intelligent System: Networks of Plausible
Inference. Morgan Kaufman, San Mateo.
</reference>
<page confidence="0.975298">
30
</page>
<reference confidence="0.999931833333333">
[12] Schutze, Hinrich. (1992). Context Space, AAAI Fall Symposium Series, Cambridge,
Massachusetts.
[13] Spiegelhalter D.. and Lauritzen, S.L. . (1990). Sequential updating of conditional
probabilities on directed graphical structures. Networks 20. 579-605.
[14] Yoon, HyunSoo. (1992) &amp;quot;A Study on the Parallel Hopfield Neural Network with
Stable-State Convergence Property.&amp;quot; KAIST TR(Computer Architecture Lab).
</reference>
<page confidence="0.999913">
31
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814757">
<title confidence="0.92262">Concept Acquisition From Collocation Map 1</title>
<author confidence="0.999023">Young S Han</author>
<author confidence="0.999023">Young Kyoon Han</author>
<author confidence="0.999023">Key-Sun Choi</author>
<affiliation confidence="0.998004">Computer Science Korea Advanced Institute of Science and</affiliation>
<address confidence="0.98891">Taejon, 305-701,</address>
<email confidence="0.992221">yshan@csking.kaist.ac.kr,kschoi@csking.kaistac.kr</email>
<abstract confidence="0.9934458">This paper introduces an algorithm for automatically acquiring the conceptual structure of each word from corpus. The concept of a word is defined within the probabilistic framework. A variation of Belief Net named as Collocation Map is used to compute the probabilities. The Belief Net captures the conditional independences of words, which is obtained from the cooccurrence relations. The computation in general Belief Nets is known to be NP-hard, so we adopted Gibbs sampling for the approximation of the probabilities. The use of Belief Net to model the lexical meaning is unique in that the network is larger than expected in most other applications, and this changes the attitude toward the use of Belief Net. The lexical concept obtained from the Collocation Map best reflects the subdomain of language usage. The potential application of conditional probabilities the Collocation Map provides may extend to cover very diverse areas of language processing such as sense disambiguation, thesaurus construction, automatic indexing, and document classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>Proceedings of Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA.</location>
<marker>[1]</marker>
<rawString>Baker, J. K. 1979. Trainable grammars for speech recognition. Proceedings of Spring Conference of the Acoustical Society of America, 547-550. Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Hinton Ackley</author>
<author>T J Sejnowski</author>
</authors>
<title>A Learning Algorithm for Boltzmann machines,</title>
<date>1985</date>
<journal>Cognitive Science.</journal>
<volume>9</volume>
<pages>147--169</pages>
<marker>[2]</marker>
<rawString>Ackley, G.E. Hinton and T.J. Sejnowski. (1985). A Learning Algorithm for Boltzmann machines, Cognitive Science. 9. 147-169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sehyeong Cho</author>
<author>Anthony S Maida</author>
</authors>
<title>Using a Bayesian Framework to Identify the Referents of Definite Descriptions.&amp;quot; AAAI Fall Symposium,</title>
<date>1992</date>
<location>Cambridge, Massachusetts.</location>
<marker>[3]</marker>
<rawString>Cho, Sehyeong, Maida, Anthony S. (1992). &amp;quot;Using a Bayesian Framework to Identify the Referents of Definite Descriptions.&amp;quot; AAAI Fall Symposium, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Laird Dempster</author>
<author>N M</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm,</title>
<date>1977</date>
<journal>J. Roy. Stat. Soc. B</journal>
<volume>39</volume>
<pages>1--38</pages>
<marker>[4]</marker>
<rawString>Dempster, A.P. Laird, N.M. and Rubin, D.B. (1977). Maximum likelihood from incomplete data via the EM algorithm, J. Roy. Stat. Soc. B 39, 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Gelfan</author>
<author>A F M Smith</author>
</authors>
<title>Sampling-based approaches to calculating marginal densities,</title>
<date>1990</date>
<journal>J. Am. Stat. Assoc</journal>
<volume>85</volume>
<pages>398--409</pages>
<marker>[5]</marker>
<rawString>Gelfan, A.E. and Smith, A.F.M. (1990). Sampling-based approaches to calculating marginal densities, J. Am. Stat. Assoc 85. 398-409.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Goldman</author>
<author>Charniak Eugene</author>
</authors>
<date>1992</date>
<booktitle>Probabilistic Text Understanding. Statistics and Computing.</booktitle>
<pages>2--105</pages>
<marker>[6]</marker>
<rawString>Goldman, Robert P. and Charniak Eugene. (1992). Probabilistic Text Understanding. Statistics and Computing. 2:105-114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young S Choi Han</author>
<author>Key-Sun</author>
</authors>
<title>Indexing Based on Formal Relevancy of Bayesian Document Semantics. Korea/Japan Joint Conferenceon on Expert Systems,</title>
<date>1993</date>
<location>Seoul,</location>
<marker>[7]</marker>
<rawString>Han, Young S. Choi, Key-Sun. (1993). Indexing Based on Formal Relevancy of Bayesian Document Semantics. Korea/Japan Joint Conferenceon on Expert Systems, Seoul, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauritzen</author>
<author>D J Spiegelhalter</author>
</authors>
<title>Local computation with probabilities on graphical structures and their application to expert systems.</title>
<date>1988</date>
<journal>J. Roy. Stat. Soc.</journal>
<volume>50</volume>
<pages>157--224</pages>
<marker>[8]</marker>
<rawString>Lauritzen and Spiegelhalter, D.J. (1988). Local computation with probabilities on graphical structures and their application to expert systems. J. Roy. Stat. Soc. 50. 157-224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rosenbluth Metropolis</author>
<author>A W Teller</author>
<author>A H Teller</author>
<author>E Teller</author>
</authors>
<title>Equation of state calculations by fast computing machines.</title>
<date>1953</date>
<journal>J. Chem. Phys.</journal>
<volume>21</volume>
<pages>1087--1092</pages>
<marker>[9]</marker>
<rawString>Metropolis, N. Rosenbluth, A. W. Teller, A.H. Teller and Teller, E. (1953). Equation of state calculations by fast computing machines. J. Chem. Phys. 21. 1087-1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Neal</author>
</authors>
<title>Connectionist learning of belief network.</title>
<date>1992</date>
<journal>Artificial Intelligence</journal>
<volume>56</volume>
<pages>71--113</pages>
<marker>[10]</marker>
<rawString>Neal, R.M. (1992). Connectionist learning of belief network. Artificial Intelligence 56. 71-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent System: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo.</location>
<marker>[11]</marker>
<rawString>Pearl, J. (1988). Probabilistic Reasoning in Intelligent System: Networks of Plausible Inference. Morgan Kaufman, San Mateo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schutze</author>
</authors>
<title>Context Space, AAAI Fall Symposium Series,</title>
<date>1992</date>
<location>Cambridge, Massachusetts.</location>
<marker>[12]</marker>
<rawString>Schutze, Hinrich. (1992). Context Space, AAAI Fall Symposium Series, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Spiegelhalter</author>
<author>S L Lauritzen</author>
</authors>
<title>Sequential updating of conditional probabilities on directed graphical structures.</title>
<date>1990</date>
<journal>Networks</journal>
<volume>20</volume>
<pages>579--605</pages>
<marker>[13]</marker>
<rawString>Spiegelhalter D.. and Lauritzen, S.L. . (1990). Sequential updating of conditional probabilities on directed graphical structures. Networks 20. 579-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HyunSoo Yoon</author>
</authors>
<title>A Study on the Parallel Hopfield Neural Network with Stable-State Convergence Property.&amp;quot; KAIST TR(Computer Architecture Lab).</title>
<date>1992</date>
<marker>[14]</marker>
<rawString>Yoon, HyunSoo. (1992) &amp;quot;A Study on the Parallel Hopfield Neural Network with Stable-State Convergence Property.&amp;quot; KAIST TR(Computer Architecture Lab).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>