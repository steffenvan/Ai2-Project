<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000475">
<title confidence="0.9973495">
Seeing stars when there aren’t many stars:
Graph-based semi-supervised learning for sentiment categorization
</title>
<author confidence="0.982781">
Andrew B. Goldberg
</author>
<affiliation confidence="0.9874935">
Computer Sciences Department
University of Wisconsin-Madison
</affiliation>
<address confidence="0.883309">
Madison, W.I. 53706
</address>
<email confidence="0.998352">
goldberg@cs.wisc.edu
</email>
<author confidence="0.989481">
Xiaojin Zhu
</author>
<affiliation confidence="0.991479">
Computer Sciences Department
University of Wisconsin-Madison
</affiliation>
<address confidence="0.882997">
Madison, W.I. 53706
</address>
<email confidence="0.997745">
jerryzhu@cs.wisc.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99984875">
We present a graph-based semi-supervised
learning algorithm to address the senti-
ment analysis task of rating inference.
Given a set of documents (e.g., movie
reviews) and accompanying ratings (e.g.,
“4 stars”), the task calls for inferring nu-
merical ratings for unlabeled documents
based on the perceived sentiment ex-
pressed by their text. In particular, we
are interested in the situation where la-
beled data is scarce. We place this task
in the semi-supervised setting and demon-
strate that considering unlabeled reviews
in the learning process can improve rating-
inference performance. We do so by creat-
ing a graph on both labeled and unlabeled
data to encode certain assumptions for this
task. We then solve an optimization prob-
lem to obtain a smooth rating function
over the whole graph. When only lim-
ited labeled data is available, this method
achieves significantly better predictive ac-
curacy over other methods that ignore the
unlabeled examples during training.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816567567568">
Sentiment analysis of text documents has received
considerable attention recently (Shanahan et al.,
2005; Turney, 2002; Dave et al., 2003; Hu and
Liu, 2004; Chaovalit and Zhou, 2005). Unlike tra-
ditional text categorization based on topics, senti-
ment analysis attempts to identify the subjective sen-
timent expressed (or implied) in documents, such as
consumer product or movie reviews. In particular
Pang and Lee proposed the rating-inference problem
(2005). Rating inference is harder than binary posi-
tive / negative opinion classification. The goal is to
infer a numerical rating from reviews, for example
the number of “stars” that a critic gave to a movie.
Pang and Lee showed that supervised machine learn-
ing techniques (classification and regression) work
well for rating inference with large amounts of train-
ing data.
However, review documents often do not come
with numerical ratings. We call such documents un-
labeled data. Standard supervised machine learning
algorithms cannot learn from unlabeled data. As-
signing labels can be a slow and expensive process
because manual inspection and domain expertise are
needed. Often only a small portion of the documents
can be labeled within resource constraints, so most
documents remain unlabeled. Supervised learning
algorithms trained on small labeled sets suffer in
performance. Can one use the unlabeled reviews to
improve rating-inference? Pang and Lee (2005) sug-
gested that doing so should be useful.
We demonstrate that the answer is ‘Yes.’ Our
approach is graph-based semi-supervised learning.
Semi-supervised learning is an active research area
in machine learning. It builds better classifiers or
regressors using both labeled and unlabeled data,
under appropriate assumptions (Zhu, 2005; Seeger,
2001). This paper contains three contributions:
</bodyText>
<listItem confidence="0.5863495">
• We present a novel adaptation of graph-based
semi-supervised learning (Zhu et al., 2003)
</listItem>
<page confidence="0.990623">
45
</page>
<bodyText confidence="0.7110738">
Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52,
New York City, June 2006. c�2006 Association for Computational Linguistics
to the sentiment analysis domain, extending
past supervised learning work by Pang and
Lee (2005);
</bodyText>
<listItem confidence="0.998442714285714">
• We design a special graph which encodes
our assumptions for rating-inference problems
(section 2), and present the associated opti-
mization problem in section 3;
• We show the benefit of semi-supervised learn-
ing for rating inference with extensive experi-
mental results in section 4.
</listItem>
<sectionHeader confidence="0.787823" genericHeader="method">
2 A Graph for Sentiment Categorization
</sectionHeader>
<bodyText confidence="0.99022194">
The semi-supervised rating-inference problem is
formalized as follows. There are n review docu-
ments x1 ... xn, each represented by some standard
feature representation (e.g., word-presence vectors).
Without loss of generality, let the first l &lt; n doc-
uments be labeled with ratings y1 ... yl E C. The
remaining documents are unlabeled. In our exper-
iments, the unlabeled documents are also the test
documents, a setting known as transduction. The
set of numerical ratings are C = {c1, ... , cC}, with
c1 &lt; ... &lt; cC E R. For example, a one-star to
four-star movie rating system has C = {0, 1, 2, 3}.
We seek a function f : x H R that gives a contin-
uous rating f(x) to a document x. Classification is
done by mapping f(x) to the nearest discrete rating
in C. Note this is ordinal classification, which dif-
fers from standard multi-class classification in that
C is endowed with an order. In the following we use
‘review’ and ‘document,’ ‘rating’ and ‘label’ inter-
changeably.
We make two assumptions:
1. We are given a similarity measure wij &gt; 0
between documents xi and xj. wij should
be computable from features, so that we can
measure similarities between any documents,
including unlabeled ones. A large wij im-
plies that the two documents tend to express
the same sentiment (i.e., rating). We experi-
ment with positive-sentence percentage (PSP)
based similarity which is proposed in (Pang and
Lee, 2005), and mutual-information modulated
word-vector cosine similarity. Details can be
found in section 4.
2. Optionally, we are given numerical rating pre-
dictions yl+1, ... , yn on the unlabeled doc-
uments from a separate learner, for in-
stance E-insensitive support vector regression
(Joachims, 1999; Smola and Sch¨olkopf, 2004)
used by (Pang and Lee, 2005). This acts
as an extra knowledge source for our semi-
supervised learning framework to improve
upon. We note our framework is general and
works without the separate learner, too. (For
this to work in practice, a reliable similarity
measure is required.)
We now describe our graph for the semi-
supervised rating-inference problem. We do this
piece by piece with reference to Figure 1. Our undi-
rected graph G = (V, E) has 2n nodes V , and
weighted edges E among some of the nodes.
</bodyText>
<listItem confidence="0.999430851851852">
• Each document is a node in the graph (open cir-
cles, e.g., xi and xj). The true ratings of these
nodes f(x) are unobserved. This is true even
for the labeled documents because we allow for
noisy labels. Our goal is to infer f(x) for the
unlabeled documents.
• Each labeled document (e.g., xj) is connected
to an observed node (dark circle) whose value
is the given rating yj. The observed node is
a ‘dongle’ (Zhu et al., 2003) since it connects
only to xj. As we point out later, this serves
to pull f(xj) towards yj. The edge weight be-
tween a labeled document and its dongle is a
large number M. M represents the influence
of yj: if M —* oc then f(xj) = yj becomes a
hard constraint.
• Similarly each unlabeled document (e.g., xi) is
also connected to an observed dongle node yi,
whose value is the prediction of the separate
learner. Therefore we also require that f(xi)
is close to yi. This is a way to incorporate mul-
tiple learners in general. We set the weight be-
tween an unlabeled node and its dongle arbi-
trarily to 1 (the weights are scale-invariant oth-
erwise). As noted earlier, the separate learner
is optional: we can remove it and still carry out
graph-based semi-supervised learning.
</listItem>
<page confidence="0.99893">
46
</page>
<bodyText confidence="0.99949">
Summing over all edges in the graph, we obtain the
(un)smoothness L(f) over the whole graph. We call
L(f) the energy or loss, which should be minimized.
Let L = 1... l and U = l + 1... n be labeled
and unlabeled review indices, respectively. With the
graph in Figure 1, the loss L(f) can be written as
</bodyText>
<figure confidence="0.987028571428571">
E M(f(xi) − yi)2 + E (f(xi) − 9i)2
iEL iEU
awij(f(xi) − f(xj))2
yj
unlabeled
reviews
k’ neighbors
b wij
^
yi
1
xi
a wij
labeled
reviews
k neighbors
xj
M
E E
+ jEkNNL(i)
iEU
</figure>
<figureCaption confidence="0.966288666666667">
Figure 1: The graph for semi-supervised rating in- E E bwij(f(xi) − f(xj))2. (1)
ference. + jEk&apos;NNU(i)
iEU
</figureCaption>
<listItem confidence="0.952061571428572">
• Each unlabeled document xi is connected to
kNNL(i), its k nearest labeled documents.
Distance is measured by the given similarity
measure w. We want f(xi) to be consistent
with its similar labeled documents. The weight
between xi and xj E kNNL(i) is a • wij.
• Each unlabeled document is also connected to
k&apos;NNU(i), its k&apos; nearest unlabeled documents
(excluding itself). The weight between xi and
xj E k&apos;NNU(i) is b • wij. We also want
f(xi) to be consistent with its similar unla-
beled neighbors. We allow potentially different
numbers of neighbors (k and k&apos;), and different
weight coefficients (a and b). These parameters
</listItem>
<bodyText confidence="0.9711992">
are set by cross validation in experiments.
The last two kinds of edges are the key to semi-
supervised learning: They connect unobserved
nodes and force ratings to be smooth throughout the
graph, as we discuss in the next section.
</bodyText>
<sectionHeader confidence="0.989481" genericHeader="method">
3 Graph-Based Semi-Supervised Learning
</sectionHeader>
<bodyText confidence="0.984605">
With the graph defined, there are several algorithms
one can use to carry out semi-supervised learning
(Zhu et al., 2003; Delalleau et al., 2005; Joachims,
2003; Blum and Chawla, 2001; Belkin et al., 2005).
The basic idea is the same and is what we use in this
paper. That is, our rating function f(x) should be
smooth with respect to the graph. f(x) is not smooth
if there is an edge with large weight w between
nodes xi and xj, and the difference between f(xi)
and f(xj) is large. The (un)smoothness over the par-
ticular edge can be defined as w(f(xi) − f(xj))2.
A small loss implies that the rating of an unlabeled
review is close to its labeled peers as well as its un-
labeled peers. This is how unlabeled data can par-
ticipate in learning. The optimization problem is
minf L(f). To understand the role of the parame-
ters, we define α = ak + bk&apos; and Q = a, so that
L(f) can be written as
</bodyText>
<equation confidence="0.997377142857143">
E M(f(xi) − yi)2 + E [ (f(xi) − 9i)2
iEL iEU
+ k + Qk&apos;
α � E
jEkNNL(i) wij(f(xi) − f(xj))2
+ E Qwij(f(xi) − f(xj))2/J. (2)
jEk&apos;NNU(i)
</equation>
<bodyText confidence="0.999419333333333">
Thus Q controls the relative weight between labeled
neighbors and unlabeled neighbors; α is roughly
the relative weight given to semi-supervised (non-
dongle) edges.
We can find the closed-form solution to the opti-
mization problem. Defining an n x n matrix W,
</bodyText>
<equation confidence="0.997705">
Wij = { 0, i E L
wij, j E kNNL(i)
Qwij, j E k&apos;NNU(i). (3)
</equation>
<bodyText confidence="0.999340333333333">
Let W = max(W, WT) be a symmetrized version
of this matrix. Let D be a diagonal degree matrix
with
</bodyText>
<equation confidence="0.994013">
Dii = En Wij. (4)
j=1
</equation>
<bodyText confidence="0.987512666666667">
Note that we define a node’s degree to be the sum of
its edge weights. Let A = D − W be the combina-
torial Laplacian matrix. Let C be a diagonal dongle
</bodyText>
<page confidence="0.928149">
47
</page>
<equation confidence="0.965053142857143">
weight matrix with
Cii = t 1, i E U
r M, i E L (5)
Let f = (f(x1), ... , f(xn))T and y =
(y1,...,yl, 9l+1,..., 9 n)T. We can rewrite L(f) as
(f − y)TC(f − y) + �
k + Qk&apos;fTAf. (6)
</equation>
<bodyText confidence="0.9994145">
This is a quadratic function in f. Setting the gradient
to zero, aL(f)/af = 0 , we find the minimum loss
function
Because C has strictly positive eigenvalues, the in-
verse is well defined. All our semi-supervised learn-
ing experiments use (7) in what follows.
Before moving on to experiments, we note an
interesting connection to the supervised learning
method in (Pang and Lee, 2005), which formulates
rating inference as a metric labeling problem (Klein-
berg and Tardos, 2002). Consider a special case of
our loss function (1) when b = 0 and M —* oc. It
is easy to show for labeled nodes j E L, the opti-
mal value is the given label: f(xj) = yj. Then the
optimization problem decouples into a set of one-
dimensional problems, one for each unlabeled node
</bodyText>
<equation confidence="0.947064666666667">
i E U: Lb=0,M→oo(f(xi)) =
(f (xi) − 9 i)2 + E awij(f(xi) − yj)2. (8)
jEkNNL(i)
</equation>
<bodyText confidence="0.999960444444444">
The above problem is easy to solve. It corresponds
exactly to the supervised, non-transductive version
of metric labeling, except we use squared differ-
ence while (Pang and Lee, 2005) used absolute dif-
ference. Indeed in experiments comparing the two
(not reported here), their differences are not statis-
tically significant. From this perspective, our semi-
supervised learning method is an extension with in-
teracting terms among unlabeled data.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999910259259259">
We performed experiments using the movie re-
view documents and accompanying 4-class (C =
10, 1, 2,31) labels found in the “scale dataset v1.0”
available at http://www.cs.cornell.edu/people/pabo/
movie-review-data/ and first used in (Pang and Lee,
2005). We chose 4-class instead of 3-class labeling
because it is harder. The dataset is divided into four
author-specific corpora, containing 1770, 902, 1307,
and 1027 documents. We ran experiments individu-
ally for each author. Each document is represented
as a 10, 11 word-presence vector, normalized to sum
to 1.
We systematically vary labeled set size |L |E
10.9n, 800, 400, 200,100, 50, 25,12, 6} to observe
the effect of semi-supervised learning. |L |= 0.9n
is included to match 10-fold cross validation used
by (Pang and Lee, 2005). For each |L |we run 20
trials where we randomly split the corpus into la-
beled and test (unlabeled) sets. We ensure that all
four classes are represented in each labeled set. The
same random splits are used for all methods, allow-
ing paired t-tests for statistical significance. All re-
ported results are average test set accuracy.
We compare our graph-based semi-supervised
method with two previously studied methods: re-
gression and metric labeling as in (Pang and Lee,
2005).
</bodyText>
<subsectionHeader confidence="0.927556">
4.1 Regression
</subsectionHeader>
<bodyText confidence="0.999790285714286">
We ran linear E-insensitive support vector regression
using Joachims’ SVM&amp;quot;ght package (1999) with all
default parameters. The continuous prediction on a
test document is discretized for classification. Re-
gression results are reported under the heading ‘reg.’
Note this method does not use unlabeled data for
training.
</bodyText>
<subsectionHeader confidence="0.994974">
4.2 Metric labeling
</subsectionHeader>
<bodyText confidence="0.999592461538462">
We ran Pang and Lee’s method based on metric la-
beling, using SVM regression as the initial label
preference function. The method requires an item-
similarity function, which is equivalent to our simi-
larity measure wij. Among others, we experimented
with PSP-based similarity. For consistency with
(Pang and Lee, 2005), supervised metric labeling re-
sults with this measure are reported under ‘reg+PSP.’
Note this method does not use unlabeled data for
training either.
PSPi is defined in (Pang and Lee, 2005) as the
percentage of positive sentences in review xi. The
similarity between reviews xi, xj is the cosine angle
</bodyText>
<equation confidence="0.75445475">
(f = C +
a A Cy. (7)
k + Qk
��1
</equation>
<page confidence="0.900431">
48
</page>
<figure confidence="0.9835065">
Positive−sentence percentage (PSP) statistics
fine−grain rating
</figure>
<figureCaption confidence="0.70972375">
Figure 2: PSP for reviews expressing each fine-grain
rating. We identified positive sentences using SVM
instead of Naive Bayes, but the trend is qualitatively
the same as in (Pang and Lee, 2005).
</figureCaption>
<bodyText confidence="0.999790913043478">
between the vectors (PSPZ,1−PSPZ) and (PSPj, 1−
PSPj). Positive sentences are identified using a bi-
nary classifier trained on a separate “snippet data
set” located at the same URL as above. The snippet
data set contains 10662 short quotations taken from
movie reviews appearing on the rottentomatoes.com
Web site. Each snippet is labeled positive or neg-
ative based on the rating of the originating review.
Pang and Lee (2005) trained a Naive Bayes classi-
fier. They showed that PSP is a (noisy) measure for
comparing reviews—reviews with low ratings tend
to receive low PSP scores, and those with higher
ratings tend to get high PSP scores. Thus, two re-
views with a high PSP-based similarity are expected
to have similar ratings. For our experiments we de-
rived PSP measurements in a similar manner, but us-
ing a linear SVM classifier. We observed the same
relationship between PSP and ratings (Figure 2).
The metric labeling method has parameters
(the equivalent of k, α in our model). Pang and
Lee tuned them on a per-author basis using cross
validation but did not report the optimal parameters.
We were interested in learning a single set of
parameters for use with all authors. In addition,
since we varied labeled set size, it is convenient
to tune c = k/|L|, the fraction of labeled reviews
used as neighbors, instead of k. We then used
the same c, α for all authors at all labeled set
sizes in experiments involving PSP. Because c is
fixed, k varies directly with |L |(i.e., when less
labeled data is available, our algorithm considers
fewer nearby labeled examples). In an attempt to
reproduce the findings in (Pang and Lee, 2005),
we tuned c, α with cross validation. Tuning ranges
are c ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and α ∈
{0.01, 0.1, 0.5,1.0,1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}.
The optimal parameters we found are c = 0.2 and
α = 1.5. (In section 4.4, we discuss an alternative
similarity measure, for which we re-tuned these
parameters.)
Note that we learned a single set of shared param-
eters for all authors, whereas (Pang and Lee, 2005)
tuned k and α on a per-author basis. To demonstrate
that our implementation of metric labeling produces
comparable results, we also determined the optimal
author-specific parameters. Table 1 shows the ac-
curacy obtained over 20 trials with |L |= 0.9n for
each author, using SVM regression, reg+PSP using
shared c, α parameters, and reg+PSP using author-
specific c, α parameters (listed in parentheses). The
best result in each row of the table is highlighted in
bold. We also show in bold any results that cannot
be distinguished from the best result using a paired
t-test at the 0.05 level.
(Pang and Lee, 2005) found that their metric la-
beling method, when applied to the 4-class data we
are using, was not statistically better than regres-
sion, though they observed some improvement for
authors (c) and (d). Using author-specific parame-
ters, we obtained the same qualitative result, but the
improvement for (c) and (d) appears even less sig-
nificant in our results. Possible explanations for this
difference are the fact that we derived our PSP mea-
surements using an SVM classifier instead of an NB
classifier, and that we did not use the same range of
parameters for tuning. The optimal shared parame-
ters produced almost the same results as the optimal
author-specific parameters, and were used in subse-
quent experiments.
</bodyText>
<subsectionHeader confidence="0.998556">
4.3 Semi-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.9930724">
We used the same PSP-based similarity measure
and the same shared parameters c = 0.2, α =
1.5 from our metric labeling experiments to per-
form graph-based semi-supervised learning. The
results are reported as ‘SSL+PSP.’ SSL has three
</bodyText>
<figure confidence="0.9976955">
0.8
0.7
mean and standard deviation of PSP
0.6
0.5
0.4
0.3
0.2
0.1
00 0.2 0.4 0.6 0.8 1
Author (a)
Author (b)
Author (c)
Author (d)
</figure>
<page confidence="0.994801">
49
</page>
<table confidence="0.999522666666667">
reg+PSP reg+PSP
Author reg (shared) (specific)
0.592 0.592 0.592 (0.05, 0.01)
0.501 0.498 0.496 (0.05, 3.50)
0.592 0.589 0.593 (0.15, 1.50)
0.496 0.498 0.500 (0.05, 3.00)
</table>
<tableCaption confidence="0.961097">
Table 1: Accuracy using shared (c = 0.2, α = 1.5)
vs. author-specific parameters, with |L |= 0.9n.
</tableCaption>
<bodyText confidence="0.9625167">
additional parameters k&apos;, Q, and M. Again
we tuned k&apos;, Q with cross validation. Tuning
ranges are k&apos; E 12, 3, 5,10, 20} and Q E
10.001, 0.01, 0.1,1.0,10.01. The optimal parame-
ters are k&apos; = 5 and Q = 1.0. These were used for all
authors and for all labeled set sizes. Note that unlike
k = c|L|, which decreases as the labeled set size de-
creases, we let k&apos; remain fixed for all |L|. We set M
arbitrarily to a large number 108 to ensure that the
ratings of labeled reviews are respected.
</bodyText>
<subsectionHeader confidence="0.999735">
4.4 Alternate Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999976235294117">
In addition to using PSP as a similarity measure be-
tween reviews, we investigated several alternative
similarity measures based on the cosine of word
vectors. Among these options were the cosine be-
tween the word vectors used to train the SVM re-
gressor, and the cosine between word vectors con-
taining only words with high (top 1000 or top 5000)
mutual information values. The mutual information
is computed with respect to the positive and negative
classes in the 10662-document “snippet data set.”
Finally, we experimented with using as a similarity
measure the cosine between word vectors containing
all words, each weighted by its mutual information.
We found this measure to be the best among the op-
tions tested in pilot trial runs using the metric label-
ing algorithm. Specifically, we scaled the mutual in-
formation values such that the maximum value was
one. Then, we used these values as weights for the
corresponding words in the word vectors. For words
in the movie review data set that did not appear in
the snippet data set, we used a default weight of zero
(i.e., we excluded them. We experimented with set-
ting the default weight to one, but found this led to
inferior performance.)
We repeated the experiments described in sec-
tions 4.2 and 4.3 with the only difference being
that we used the mutual-information weighted word
vector similarity instead of PSP whenever a simi-
larity measure was required. We repeated the tun-
ing procedures described in the previous sections.
Using this new similarity measure led to the opti-
mal parameters c = 0.1, α = 1.5, k&apos; = 5, and
Q = 10.0. The results are reported under ‘reg+WV’
and ‘SSL+WV,’ respectively.
</bodyText>
<subsectionHeader confidence="0.685043">
4.5 Results
</subsectionHeader>
<bodyText confidence="0.998941605263158">
We tested the five algorithms for all four authors us-
ing each of the nine labeled set sizes. The results
are presented in table 2. Each entry in the table rep-
resents the average accuracy across 20 trials for an
author, a labeled set size, and an algorithm. The best
result in each row is highlighted in bold. Any results
on the same row that cannot be distinguished from
the best result using a paired t-test at the 0.05 level
are also bold.
The results indicate that the graph-based semi-
supervised learning algorithm based on PSP simi-
larity (SSL+PSP) achieved better performance than
all other methods in all four author corpora when
only 200, 100, 50, 25, or 12 labeled documents
were available. In 19 out of these 20 learning sce-
narios, the unlabeled set accuracy by the SSL+PSP
algorithm was significantly higher than all other
methods. While accuracy generally degraded as we
trained on less labeled data, the decrease for the SSL
approach was less severe through the mid-range la-
beled set sizes. SSL+PSP remains among the best
methods with only 6 labeled examples.
Note that the SSL algorithm appears to be quite
sensitive to the similarity measure used to form the
graph on which it is based. In the experiments where
we used mutual-information weighted word vector
similarity (reg+WV and SSL+WV), we notice that
reg+WV remained on par with reg+PSP at high la-
beled set sizes, whereas SSL+WV appears signif-
icantly worse in most of these cases. It is clear
that PSP is the more reliable similarity measure.
SSL uses the similarity measure in more ways than
the metric labeling approaches (i.e., SSL’s graph is
denser), so it is not surprising that SSL’s accuracy
would suffer more with an inferior similarity mea-
sure.
Unfortunately, our SSL approach did not do as
well with large labeled set sizes. We believe this
</bodyText>
<page confidence="0.945784">
50
</page>
<table confidence="0.999528157894737">
ILI regression PSP word vector
reg+PSP SSL+PSP reg+WV SSL+WV
Author (a) 1593 0.592 0.592 0.546 0.592 0.544
800 0.553 0.554 0.534 0.553 0.517
400 0.522 0.525 0.526 0.522 0.497
200 0.494 0.498 0.521 0.494 0.472
100 0.463 0.477 0.511 0.462 0.450
50 0.439 0.458 0.499 0.438 0.429
25 0.408 0.421 0.465 0.400 0.404
12 0.401 0.378 0.451 0.335 0.398
6 0.390 0.359 0.422 0.314 0.389
Author (b) 811 0.501 0.498 0.481 0.503 0.473
800 0.501 0.497 0.478 0.503 0.474
400 0.471 0.471 0.465 0.471 0.450
200 0.447 0.449 0.452 0.447 0.429
100 0.415 0.423 0.443 0.415 0.397
50 0.388 0.396 0.434 0.387 0.376
25 0.373 0.380 0.418 0.364 0.367
12 0.354 0.360 0.399 0.313 0.353
6 0.348 0.352 0.380 0.302 0.347
Author (c) 1176 0.592 0.589 0.566 0.594 0.514
800 0.579 0.585 0.559 0.579 0.509
400 0.550 0.556 0.544 0.551 0.491
200 0.513 0.519 0.532 0.513 0.479
100 0.484 0.495 0.521 0.484 0.466
50 0.462 0.476 0.504 0.461 0.456
25 0.459 0.472 0.484 0.439 0.454
12 0.420 0.405 0.477 0.356 0.414
6 0.320 0.382 0.366 0.334 0.322
Author (d) 924 0.496 0.498 0.495 0.499 0.490
800 0.500 0.501 0.495 0.504 0.483
400 0.474 0.478 0.486 0.477 0.463
200 0.459 0.459 0.468 0.459 0.445
100 0.444 0.445 0.460 0.444 0.437
50 0.429 0.431 0.445 0.429 0.428
25 0.411 0.411 0.425 0.400 0.409
12 0.393 0.362 0.405 0.335 0.391
6 0.393 0.357 0.403 0.312 0.393
</table>
<tableCaption confidence="0.618816333333333">
Table 2: 20-trial average unlabeled set accuracy for each author across different labeled set sizes and meth-
ods. In each row, we list in bold the best result and any results that cannot be distinguished from it with a
paired t-test at the 0.05 level.
</tableCaption>
<page confidence="0.996331">
51
</page>
<bodyText confidence="0.999859">
is due to two factors: a) the baseline SVM regres-
sor trained on a large labeled set can achieve fairly
high accuracy for this difficult task without consid-
ering pairwise relationships between examples; b)
PSP similarity is not accurate enough. Gain in vari-
ance reduction achieved by the SSL graph is offset
by its bias when labeled data is abundant.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999987647058824">
We have demonstrated the benefit of using unla-
beled data for rating inference. There are several
directions to improve the work: 1. We will inves-
tigate better document representations and similar-
ity measures based on parsing and other linguis-
tic knowledge, as well as reviews’ sentiment pat-
terns. For example, several positive sentences fol-
lowed by a few concluding negative sentences could
indicate an overall negative review, as observed in
prior work (Pang and Lee, 2005). 2. Our method
is transductive: new reviews must be added to the
graph before they can be classified. We will extend
it to the inductive learning setting based on (Sind-
hwani et al., 2005). 3. We plan to experiment with
cross-reviewer and cross-domain analysis, such as
using a model learned on movie reviews to help clas-
sify product reviews.
</bodyText>
<sectionHeader confidence="0.978174" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.998835">
We thank Bo Pang, Lillian Lee and anonymous re-
viewers for helpful comments.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999468378787879">
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2005. On manifold regularization. In Proceedings of
the Tenth International Workshop on Artificial Intelli-
gence and Statistics (AISTAT 2005).
A. Blum and S. Chawla. 2001. Learning from labeled
and unlabeled data using graph mincuts. In Proc. 18th
International Conf. on Machine Learning.
Pimwadee Chaovalit and Lina Zhou. 2005. Movie re-
view mining: a comparison between supervised and
unsupervised classification approaches. In HICSS.
IEEE Computer Society.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
WWW ’03: Proceedings of the 12th international con-
ference on World Wide Web, pages 519–528.
Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux.
2005. Efficient non-parametric function induction in
semi-supervised learning. In Proceedings of the Tenth
International Workshop on Artificial Intelligence and
Statistics (AISTAT 2005).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD ’04,
the ACMSIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177. ACM
Press.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML-03, 20th
International Conference on Machine Learning.
Jon M. Kleinberg and ´Eva Tardos. 2002. Approxima-
tion algorithms for classification problems with pair-
wise relationships: metric labeling and markov ran-
dom fields. J. ACM, 49(5):616–639.
Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Matthias Seeger. 2001. Learning with labeled and unla-
beled data. Technical report, University of Edinburgh.
James Shanahan, Yan Qu, and Janyce Wiebe, editors.
2005. Computing attitude and affect in text. Springer,
Dordrecht, The Netherlands.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive to
semi-supervised learning. In ICML05, 22nd Interna-
tional Conference on Machine Learning, Bonn, Ger-
many.
A. J. Smola and B. Sch¨olkopf. 2004. A tutorial on
support vector regression. Statistics and Computing,
14:199–222.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of ACL-02, 40th An-
nual Meeting of the Association for Computational
Linguistics, pages 417–424.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using Gaussian fields
and harmonic functions. In ICML-03, 20th Interna-
tional Conference on Machine Learning.
Xiaojin Zhu. 2005. Semi-supervised learning lit-
erature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.
</reference>
<page confidence="0.998838">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.489597">
<title confidence="0.9981765">Seeing stars when there aren’t many Graph-based semi-supervised learning for sentiment categorization</title>
<author confidence="0.99961">B Andrew</author>
<affiliation confidence="0.999353">Computer Sciences University of</affiliation>
<address confidence="0.950386">Madison, W.I.</address>
<email confidence="0.99968">goldberg@cs.wisc.edu</email>
<author confidence="0.543831">Xiaojin</author>
<affiliation confidence="0.998654">Computer Sciences University of</affiliation>
<address confidence="0.960895">Madison, W.I.</address>
<email confidence="0.999748">jerryzhu@cs.wisc.edu</email>
<abstract confidence="0.99968088">We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference. Given a set of documents (e.g., movie reviews) and accompanying ratings (e.g., “4 stars”), the task calls for inferring numerical ratings for unlabeled documents based on the perceived sentiment expressed by their text. In particular, we are interested in the situation where labeled data is scarce. We place this task in the semi-supervised setting and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>On manifold regularization.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT</booktitle>
<contexts>
<context position="8897" citStr="Belkin et al., 2005" startWordPosition="1462" endWordPosition="1465">nlabeled neighbors. We allow potentially different numbers of neighbors (k and k&apos;), and different weight coefficients (a and b). These parameters are set by cross validation in experiments. The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section. 3 Graph-Based Semi-Supervised Learning With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al., 2003; Delalleau et al., 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al., 2005). The basic idea is the same and is what we use in this paper. That is, our rating function f(x) should be smooth with respect to the graph. f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large. The (un)smoothness over the particular edge can be defined as w(f(xi) − f(xj))2. A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers. This is how unlabeled data can participate in learning. The optimization problem is minf L(f). To understand the role o</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2005</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2005. On manifold regularization. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>S Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proc. 18th International Conf. on Machine Learning.</booktitle>
<contexts>
<context position="8875" citStr="Blum and Chawla, 2001" startWordPosition="1458" endWordPosition="1461">tent with its similar unlabeled neighbors. We allow potentially different numbers of neighbors (k and k&apos;), and different weight coefficients (a and b). These parameters are set by cross validation in experiments. The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section. 3 Graph-Based Semi-Supervised Learning With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al., 2003; Delalleau et al., 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al., 2005). The basic idea is the same and is what we use in this paper. That is, our rating function f(x) should be smooth with respect to the graph. f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large. The (un)smoothness over the particular edge can be defined as w(f(xi) − f(xj))2. A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers. This is how unlabeled data can participate in learning. The optimization problem is minf L(f). To</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>A. Blum and S. Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proc. 18th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pimwadee Chaovalit</author>
<author>Lina Zhou</author>
</authors>
<title>Movie review mining: a comparison between supervised and unsupervised classification approaches. In HICSS.</title>
<date>2005</date>
<publisher>IEEE Computer Society.</publisher>
<contexts>
<context position="1518" citStr="Chaovalit and Zhou, 2005" startWordPosition="219" endWordPosition="222">cess can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie. Pang and Lee showed that supervised machine learning techniques (classification and regression) work well for rating infer</context>
</contexts>
<marker>Chaovalit, Zhou, 2005</marker>
<rawString>Pimwadee Chaovalit and Lina Zhou. 2005. Movie review mining: a comparison between supervised and unsupervised classification approaches. In HICSS. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In WWW ’03: Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="1473" citStr="Dave et al., 2003" startWordPosition="211" endWordPosition="214">unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie. Pang and Lee showed that supervised machine learning techniques (classificati</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In WWW ’03: Proceedings of the 12th international conference on World Wide Web, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Delalleau</author>
<author>Yoshua Bengio</author>
<author>Nicolas Le Roux</author>
</authors>
<title>Efficient non-parametric function induction in semi-supervised learning.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT</booktitle>
<marker>Delalleau, Bengio, Le Roux, 2005</marker>
<rawString>Olivier Delalleau, Yoshua Bengio, and Nicolas Le Roux. 2005. Efficient non-parametric function induction in semi-supervised learning. In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics (AISTAT 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of KDD ’04, the ACMSIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1491" citStr="Hu and Liu, 2004" startWordPosition="215" endWordPosition="218">n the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie. Pang and Lee showed that supervised machine learning techniques (classification and regression)</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of KDD ’04, the ACMSIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale svm learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5481" citStr="Joachims, 1999" startWordPosition="850" endWordPosition="851"> be computable from features, so that we can measure similarities between any documents, including unlabeled ones. A large wij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and Lee, 2005), and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions yl+1, ... , yn on the unlabeled documents from a separate learner, for instance E-insensitive support vector regression (Joachims, 1999; Smola and Sch¨olkopf, 2004) used by (Pang and Lee, 2005). This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is required.) We now describe our graph for the semisupervised rating-inference problem. We do this piece by piece with reference to Figure 1. Our undirected graph G = (V, E) has 2n nodes V , and weighted edges E among some of the nodes. • Each document is a node in the graph (open circles, e.g., xi an</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale svm learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Transductive learning via spectral graph partitioning.</title>
<date>2003</date>
<booktitle>In Proceedings of ICML-03, 20th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="8852" citStr="Joachims, 2003" startWordPosition="1456" endWordPosition="1457">xi) to be consistent with its similar unlabeled neighbors. We allow potentially different numbers of neighbors (k and k&apos;), and different weight coefficients (a and b). These parameters are set by cross validation in experiments. The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section. 3 Graph-Based Semi-Supervised Learning With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al., 2003; Delalleau et al., 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al., 2005). The basic idea is the same and is what we use in this paper. That is, our rating function f(x) should be smooth with respect to the graph. f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large. The (un)smoothness over the particular edge can be defined as w(f(xi) − f(xj))2. A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers. This is how unlabeled data can participate in learning. The optimization p</context>
</contexts>
<marker>Joachims, 2003</marker>
<rawString>T. Joachims. 2003. Transductive learning via spectral graph partitioning. In Proceedings of ICML-03, 20th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
<author>´Eva Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields.</title>
<date>2002</date>
<journal>J. ACM,</journal>
<volume>49</volume>
<issue>5</issue>
<contexts>
<context position="10959" citStr="Kleinberg and Tardos, 2002" startWordPosition="1869" endWordPosition="1873">1, i E U r M, i E L (5) Let f = (f(x1), ... , f(xn))T and y = (y1,...,yl, 9l+1,..., 9 n)T. We can rewrite L(f) as (f − y)TC(f − y) + � k + Qk&apos;fTAf. (6) This is a quadratic function in f. Setting the gradient to zero, aL(f)/af = 0 , we find the minimum loss function Because C has strictly positive eigenvalues, the inverse is well defined. All our semi-supervised learning experiments use (7) in what follows. Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and Lee, 2005), which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002). Consider a special case of our loss function (1) when b = 0 and M —* oc. It is easy to show for labeled nodes j E L, the optimal value is the given label: f(xj) = yj. Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node i E U: Lb=0,M→oo(f(xi)) = (f (xi) − 9 i)2 + E awij(f(xi) − yj)2. (8) jEkNNL(i) The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared difference while (Pang and Lee, 2005) used absolute difference. Indeed in experiments comparing the </context>
</contexts>
<marker>Kleinberg, Tardos, 2002</marker>
<rawString>Jon M. Kleinberg and ´Eva Tardos. 2002. Approximation algorithms for classification problems with pairwise relationships: metric labeling and markov random fields. J. ACM, 49(5):616–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2751" citStr="Pang and Lee (2005)" startWordPosition="407" endWordPosition="410">e amounts of training data. However, review documents often do not come with numerical ratings. We call such documents unlabeled data. Standard supervised machine learning algorithms cannot learn from unlabeled data. Assigning labels can be a slow and expensive process because manual inspection and domain expertise are needed. Often only a small portion of the documents can be labeled within resource constraints, so most documents remain unlabeled. Supervised learning algorithms trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is ‘Yes.’ Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001). This paper contains three contributions: • We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) 45 Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52, New York City, June 2006. c�2006 Association for Computatio</context>
<context position="5188" citStr="Pang and Lee, 2005" startWordPosition="804" endWordPosition="807">ification, which differs from standard multi-class classification in that C is endowed with an order. In the following we use ‘review’ and ‘document,’ ‘rating’ and ‘label’ interchangeably. We make two assumptions: 1. We are given a similarity measure wij &gt; 0 between documents xi and xj. wij should be computable from features, so that we can measure similarities between any documents, including unlabeled ones. A large wij implies that the two documents tend to express the same sentiment (i.e., rating). We experiment with positive-sentence percentage (PSP) based similarity which is proposed in (Pang and Lee, 2005), and mutual-information modulated word-vector cosine similarity. Details can be found in section 4. 2. Optionally, we are given numerical rating predictions yl+1, ... , yn on the unlabeled documents from a separate learner, for instance E-insensitive support vector regression (Joachims, 1999; Smola and Sch¨olkopf, 2004) used by (Pang and Lee, 2005). This acts as an extra knowledge source for our semisupervised learning framework to improve upon. We note our framework is general and works without the separate learner, too. (For this to work in practice, a reliable similarity measure is require</context>
<context position="10866" citStr="Pang and Lee, 2005" startWordPosition="1856" endWordPosition="1859">inatorial Laplacian matrix. Let C be a diagonal dongle 47 weight matrix with Cii = t 1, i E U r M, i E L (5) Let f = (f(x1), ... , f(xn))T and y = (y1,...,yl, 9l+1,..., 9 n)T. We can rewrite L(f) as (f − y)TC(f − y) + � k + Qk&apos;fTAf. (6) This is a quadratic function in f. Setting the gradient to zero, aL(f)/af = 0 , we find the minimum loss function Because C has strictly positive eigenvalues, the inverse is well defined. All our semi-supervised learning experiments use (7) in what follows. Before moving on to experiments, we note an interesting connection to the supervised learning method in (Pang and Lee, 2005), which formulates rating inference as a metric labeling problem (Kleinberg and Tardos, 2002). Consider a special case of our loss function (1) when b = 0 and M —* oc. It is easy to show for labeled nodes j E L, the optimal value is the given label: f(xj) = yj. Then the optimization problem decouples into a set of onedimensional problems, one for each unlabeled node i E U: Lb=0,M→oo(f(xi)) = (f (xi) − 9 i)2 + E awij(f(xi) − yj)2. (8) jEkNNL(i) The above problem is easy to solve. It corresponds exactly to the supervised, non-transductive version of metric labeling, except we use squared differe</context>
<context position="12554" citStr="Pang and Lee, 2005" startWordPosition="2135" endWordPosition="2138">.cs.cornell.edu/people/pabo/ movie-review-data/ and first used in (Pang and Lee, 2005). We chose 4-class instead of 3-class labeling because it is harder. The dataset is divided into four author-specific corpora, containing 1770, 902, 1307, and 1027 documents. We ran experiments individually for each author. Each document is represented as a 10, 11 word-presence vector, normalized to sum to 1. We systematically vary labeled set size |L |E 10.9n, 800, 400, 200,100, 50, 25,12, 6} to observe the effect of semi-supervised learning. |L |= 0.9n is included to match 10-fold cross validation used by (Pang and Lee, 2005). For each |L |we run 20 trials where we randomly split the corpus into labeled and test (unlabeled) sets. We ensure that all four classes are represented in each labeled set. The same random splits are used for all methods, allowing paired t-tests for statistical significance. All reported results are average test set accuracy. We compare our graph-based semi-supervised method with two previously studied methods: regression and metric labeling as in (Pang and Lee, 2005). 4.1 Regression We ran linear E-insensitive support vector regression using Joachims’ SVM&amp;quot;ght package (1999) with all defaul</context>
<context position="13889" citStr="Pang and Lee, 2005" startWordPosition="2346" endWordPosition="2349"> reported under the heading ‘reg.’ Note this method does not use unlabeled data for training. 4.2 Metric labeling We ran Pang and Lee’s method based on metric labeling, using SVM regression as the initial label preference function. The method requires an itemsimilarity function, which is equivalent to our similarity measure wij. Among others, we experimented with PSP-based similarity. For consistency with (Pang and Lee, 2005), supervised metric labeling results with this measure are reported under ‘reg+PSP.’ Note this method does not use unlabeled data for training either. PSPi is defined in (Pang and Lee, 2005) as the percentage of positive sentences in review xi. The similarity between reviews xi, xj is the cosine angle (f = C + a A Cy. (7) k + Qk ��1 48 Positive−sentence percentage (PSP) statistics fine−grain rating Figure 2: PSP for reviews expressing each fine-grain rating. We identified positive sentences using SVM instead of Naive Bayes, but the trend is qualitatively the same as in (Pang and Lee, 2005). between the vectors (PSPZ,1−PSPZ) and (PSPj, 1− PSPj). Positive sentences are identified using a binary classifier trained on a separate “snippet data set” located at the same URL as above. Th</context>
<context position="15933" citStr="Pang and Lee, 2005" startWordPosition="2696" endWordPosition="2699">author basis using cross validation but did not report the optimal parameters. We were interested in learning a single set of parameters for use with all authors. In addition, since we varied labeled set size, it is convenient to tune c = k/|L|, the fraction of labeled reviews used as neighbors, instead of k. We then used the same c, α for all authors at all labeled set sizes in experiments involving PSP. Because c is fixed, k varies directly with |L |(i.e., when less labeled data is available, our algorithm considers fewer nearby labeled examples). In an attempt to reproduce the findings in (Pang and Lee, 2005), we tuned c, α with cross validation. Tuning ranges are c ∈ {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and α ∈ {0.01, 0.1, 0.5,1.0,1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0}. The optimal parameters we found are c = 0.2 and α = 1.5. (In section 4.4, we discuss an alternative similarity measure, for which we re-tuned these parameters.) Note that we learned a single set of shared parameters for all authors, whereas (Pang and Lee, 2005) tuned k and α on a per-author basis. To demonstrate that our implementation of metric labeling produces comparable results, we also determined the optimal author-specific paramete</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Seeger</author>
</authors>
<title>Learning with labeled and unlabeled data.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="3099" citStr="Seeger, 2001" startWordPosition="459" endWordPosition="460">n of the documents can be labeled within resource constraints, so most documents remain unlabeled. Supervised learning algorithms trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is ‘Yes.’ Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001). This paper contains three contributions: • We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) 45 Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52, New York City, June 2006. c�2006 Association for Computational Linguistics to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005); • We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3; • We show the benefit of semi-supervised learning for rating inference </context>
</contexts>
<marker>Seeger, 2001</marker>
<rawString>Matthias Seeger. 2001. Learning with labeled and unlabeled data. Technical report, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shanahan</author>
<author>Yan Qu</author>
<author>Janyce Wiebe</author>
<author>editors</author>
</authors>
<title>Computing attitude and affect in text.</title>
<date>2005</date>
<publisher>Springer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<contexts>
<context position="1440" citStr="Shanahan et al., 2005" startWordPosition="205" endWordPosition="208">ing and demonstrate that considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie. Pang and Lee showed that supervised machine </context>
</contexts>
<marker>Shanahan, Qu, Wiebe, editors, 2005</marker>
<rawString>James Shanahan, Yan Qu, and Janyce Wiebe, editors. 2005. Computing attitude and affect in text. Springer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Sindhwani</author>
<author>Partha Niyogi</author>
<author>Mikhail Belkin</author>
</authors>
<title>Beyond the point cloud: from transductive to semi-supervised learning.</title>
<date>2005</date>
<booktitle>In ICML05, 22nd International Conference on Machine Learning,</booktitle>
<location>Bonn, Germany.</location>
<marker>Sindhwani, Niyogi, Belkin, 2005</marker>
<rawString>Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 2005. Beyond the point cloud: from transductive to semi-supervised learning. In ICML05, 22nd International Conference on Machine Learning, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>B Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<booktitle>Statistics and Computing,</booktitle>
<pages>14--199</pages>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>A. J. Smola and B. Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14:199–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="1454" citStr="Turney, 2002" startWordPosition="209" endWordPosition="210">t considering unlabeled reviews in the learning process can improve ratinginference performance. We do so by creating a graph on both labeled and unlabeled data to encode certain assumptions for this task. We then solve an optimization problem to obtain a smooth rating function over the whole graph. When only limited labeled data is available, this method achieves significantly better predictive accuracy over other methods that ignore the unlabeled examples during training. 1 Introduction Sentiment analysis of text documents has received considerable attention recently (Shanahan et al., 2005; Turney, 2002; Dave et al., 2003; Hu and Liu, 2004; Chaovalit and Zhou, 2005). Unlike traditional text categorization based on topics, sentiment analysis attempts to identify the subjective sentiment expressed (or implied) in documents, such as consumer product or movie reviews. In particular Pang and Lee proposed the rating-inference problem (2005). Rating inference is harder than binary positive / negative opinion classification. The goal is to infer a numerical rating from reviews, for example the number of “stars” that a critic gave to a movie. Pang and Lee showed that supervised machine learning techn</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL-02, 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
<author>John Lafferty</author>
</authors>
<title>Semi-supervised learning using Gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In ICML-03, 20th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="3232" citStr="Zhu et al., 2003" startWordPosition="476" endWordPosition="479"> trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is ‘Yes.’ Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001). This paper contains three contributions: • We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) 45 Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52, New York City, June 2006. c�2006 Association for Computational Linguistics to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005); • We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3; • We show the benefit of semi-supervised learning for rating inference with extensive experimental results in section 4. 2 A Graph for Sentiment Categorization The semi-supervised rating-inference problem</context>
<context position="6443" citStr="Zhu et al., 2003" startWordPosition="1023" endWordPosition="1026">semisupervised rating-inference problem. We do this piece by piece with reference to Figure 1. Our undirected graph G = (V, E) has 2n nodes V , and weighted edges E among some of the nodes. • Each document is a node in the graph (open circles, e.g., xi and xj). The true ratings of these nodes f(x) are unobserved. This is true even for the labeled documents because we allow for noisy labels. Our goal is to infer f(x) for the unlabeled documents. • Each labeled document (e.g., xj) is connected to an observed node (dark circle) whose value is the given rating yj. The observed node is a ‘dongle’ (Zhu et al., 2003) since it connects only to xj. As we point out later, this serves to pull f(xj) towards yj. The edge weight between a labeled document and its dongle is a large number M. M represents the influence of yj: if M —* oc then f(xj) = yj becomes a hard constraint. • Similarly each unlabeled document (e.g., xi) is also connected to an observed dongle node yi, whose value is the prediction of the separate learner. Therefore we also require that f(xi) is close to yi. This is a way to incorporate multiple learners in general. We set the weight between an unlabeled node and its dongle arbitrarily to 1 (t</context>
<context position="8812" citStr="Zhu et al., 2003" startWordPosition="1448" endWordPosition="1451"> xj E k&apos;NNU(i) is b • wij. We also want f(xi) to be consistent with its similar unlabeled neighbors. We allow potentially different numbers of neighbors (k and k&apos;), and different weight coefficients (a and b). These parameters are set by cross validation in experiments. The last two kinds of edges are the key to semisupervised learning: They connect unobserved nodes and force ratings to be smooth throughout the graph, as we discuss in the next section. 3 Graph-Based Semi-Supervised Learning With the graph defined, there are several algorithms one can use to carry out semi-supervised learning (Zhu et al., 2003; Delalleau et al., 2005; Joachims, 2003; Blum and Chawla, 2001; Belkin et al., 2005). The basic idea is the same and is what we use in this paper. That is, our rating function f(x) should be smooth with respect to the graph. f(x) is not smooth if there is an edge with large weight w between nodes xi and xj, and the difference between f(xi) and f(xj) is large. The (un)smoothness over the particular edge can be defined as w(f(xi) − f(xj))2. A small loss implies that the rating of an unlabeled review is close to its labeled peers as well as its unlabeled peers. This is how unlabeled data can par</context>
</contexts>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003. Semi-supervised learning using Gaussian fields and harmonic functions. In ICML-03, 20th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<note>http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.</note>
<contexts>
<context position="3084" citStr="Zhu, 2005" startWordPosition="457" endWordPosition="458">mall portion of the documents can be labeled within resource constraints, so most documents remain unlabeled. Supervised learning algorithms trained on small labeled sets suffer in performance. Can one use the unlabeled reviews to improve rating-inference? Pang and Lee (2005) suggested that doing so should be useful. We demonstrate that the answer is ‘Yes.’ Our approach is graph-based semi-supervised learning. Semi-supervised learning is an active research area in machine learning. It builds better classifiers or regressors using both labeled and unlabeled data, under appropriate assumptions (Zhu, 2005; Seeger, 2001). This paper contains three contributions: • We present a novel adaptation of graph-based semi-supervised learning (Zhu et al., 2003) 45 Workshop on TextGraphs, at HLT-NAACL 2006, pages 45–52, New York City, June 2006. c�2006 Association for Computational Linguistics to the sentiment analysis domain, extending past supervised learning work by Pang and Lee (2005); • We design a special graph which encodes our assumptions for rating-inference problems (section 2), and present the associated optimization problem in section 3; • We show the benefit of semi-supervised learning for ra</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Xiaojin Zhu. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison. http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>