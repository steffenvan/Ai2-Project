<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000589">
<title confidence="0.914231">
More or less supervised supersense tagging of Twitter
</title>
<author confidence="0.996686">
Anders Johannsen, Dirk Hovy, H´ector Martinez Alonso, Barbara Plank, Anders Søgaard
</author>
<affiliation confidence="0.9980645">
Center for Language Technology
University of Copenhagen, Denmark
</affiliation>
<address confidence="0.757084">
Njalsgade 140
</address>
<email confidence="0.9490065">
ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dk
plank@cst.dk, soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993827" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922230769231">
We present two Twitter datasets annotated
with coarse-grained word senses (super-
senses), as well as a series of experiments
with three learning scenarios for super-
sense tagging: weakly supervised learn-
ing, as well as unsupervised and super-
vised domain adaptation. We show that
(a) off-the-shelf tools perform poorly on
Twitter, (b) models augmented with em-
beddings learned from Twitter data per-
form much better, and (c) errors can be
reduced using type-constrained inference
with distant supervision from WordNet.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998139982758621">
Supersense tagging (SST, Ciaramita and Altun,
2006) is the task of assigning high-level ontolog-
ical classes to open-class words (here, nouns and
verbs). It is thus a coarse-grained word sense dis-
ambiguation task. The labels are based on the lexi-
cographer file names for Princeton WordNet (Fell-
baum, 1998). They include 15 senses for verbs
and 26 for nouns (see Table 1). While WordNet
also provides catch-all supersenses for adjectives
and adverbs, these are grammatically, not seman-
tically motivated, and do not provide any higher-
level abstraction (recently, however, Tsvetkov et
al. (2014) proposed a semantic taxonomy for ad-
jectives). They will not be considered in this paper.
Coarse-grained categories such as supersenses
are useful for downstream tasks such as question-
answering (QA) and open relation extraction (RE).
SST is different from NER in that it has a larger set
of labels and in the absence of strong orthographic
cues (capitalization, quotation marks, etc.). More-
over, supersenses can be applied to any of the lex-
ical parts of speech and not only proper names.
Also, while high-coverage gazetteers can be found
for named entity recognition, the lexical resources
available for SST are very limited in coverage.
Twitter is a popular micro-blogging service,
which, among other things, is used for knowledge
sharing among friends and peers. Twitter posts
(tweets) announce local events, say talks or con-
certs, present facts about pop stars or program-
ming languages, or simply express the opinions of
the author on some subject matter.
Supersense tagging is relevant for Twitter, be-
cause it can aid e.g. QA and open RE. If someone
posts a message saying that some LaTeX module
now supports “drawing trees”, it is important to
know whether the post is about drawing natural
objects such as oaks or pines, or about drawing
tree-shaped data representations.
This paper is, to the best of our knowledge, the
first work to address the problem of SST for Twit-
ter. While there exist corpora of newswire and
literary texts that are annotated with supersenses,
e.g., SEMCOR (Miller et al., 1994), no data is
available for microblogs or related domains. This
paper introduces two new data sets.
Furthermore, most, if not all, of previous work
on SST has relied on gold standard part-of-speech
(POS) tags as input. However, in a domain such
as Twitter, which has proven to be challenging
for POS tagging (Foster et al., 2011; Ritter et
al., 2011), results obtained under the assumption
of available perfect POS information are almost
meaningless for any real-life application.
In this paper, we instead use predicted POS tags
and investigate experimental settings in which one
or more of the following resources are available to
us:
</bodyText>
<listItem confidence="0.9876166">
• a large corpus of unlabeled Twitter data;
• Princeton WordNet (Fellbaum, 1998);
• SEMCOR (Miller et al., 1994); and
• a small corpus of Twitter data annotated with
supersenses.
</listItem>
<bodyText confidence="0.992372666666667">
We approach SST of Twitter using various de-
grees of supervision for both learning and domain
adaptation (here, from newswire to Twitter). In
</bodyText>
<page confidence="0.820805">
1
</page>
<note confidence="0.981212">
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1–11,
Dublin, Ireland, August 23-24 2014.
</note>
<bodyText confidence="0.998972145454546">
weakly supervised learning, only unlabeled data
and the lexical resource WordNet are available to
us. While the quality of lexical resources varies,
this is the scenario for most languages. We present
an approach to weakly supervised SST based on
type-constrained EM-trained second-order HMMs
(HMM2s) with continuous word representations.
In contrast, when using supervised learning, we
can distinguish between two degrees of supervi-
sion for domain adaptation. For some languages,
e.g., Basque, English, Swedish, sense-annotated
resources exist, but these corpora are all limited
to newswire or similar domains. In such lan-
guages, unsupervised domain adaptation (DA)
techniques can be used to exploit these resources.
The setting does not presume labeled data from
the target domain. We use discriminative mod-
els for unsupervised domain adaptation, training
on SEMCOR and testing on Twitter.
Finally, we annotated data sets for Twitter, mak-
ing supervised domain adaptation (SU) exper-
iments possible. For supervised domain adapta-
tion, we use the annotated training data sets from
both the newswire and the Twitter domain, as well
as WordNet.
For both unsupervised domain adaptation and
supervised domain adaptation, we use structured
perceptron (Collins, 2002), i.e., a discriminative
HMM model, and search-based structured predic-
tion (SEARN) (Daume et al., 2009). We aug-
ment both the EM-trained HMM2, discrimina-
tive HMMs and SEARN with type constraints and
continuous word representations. We also exper-
imented with conditional random fields (Lafferty
et al., 2001), but obtained worse or similar results
than with the other models.
Contributions In this paper, we present two
Twitter data sets with manually annotated su-
persenses, as well as a series of experiments
with these data sets. These experiments cover
existing approaches to related tasks, as well as
some new methods. In particular, we present
type-constrained extensions of discriminative
HMMs and SEARN sequence models with con-
tinuous word representations that perform well.
We show that when no in-domain labeled data
is available, type constraints improve model
performance considerably. Our best models
achieve a weighted average F1 score of 57.1 over
nouns and verbs on our main evaluation data
set, i.e., a 20% error reduction over the most
frequent sense baseline. The two annotated Twit-
ter data sets are publicly released for download
at https://github.com/coastalcph/
supersense-data-twitter.
</bodyText>
<figure confidence="0.696190714285714">
n.Tops n.object v.cognition
n.act n.person v.communication
n.animal n.phenomenon v.competition
n.artifact n.plant v.consumption
n.attribute n.possession v.contact
n.body n.process v.creation
n.cognition n.quantity v.emotion
n.communication n.relation v.motion
n.event n.shape v.perception
n.feeling n.state v.possession
n.food n.substance v.social
n.group n.time v.stative
n.location v.body v.weather
n.motive v.change
</figure>
<tableCaption confidence="0.939437">
Table 1: The 41 noun and verb supersenses in
WordNet
</tableCaption>
<bodyText confidence="0.8614455">
2 More or less supervised models
This sections covers the varying degree of super-
vision of our systems as well as the usage of type
constraints as distant supervision.
</bodyText>
<subsectionHeader confidence="0.989435">
2.1 Distant supervision
</subsectionHeader>
<bodyText confidence="0.996692722222222">
Distant supervision in these experiments was im-
plemented by only allowing a system to predict
a certain supersense for a given word if that su-
persense had either been observed in the training
data, or, for unobserved words, if the sense was
the most frequent sense in WordNet. If the word
did not appear in the training data nor in WordNet,
no filtering was applied. We refer to the distant-
supervision strategy as type constraints.
Distant supervision was implemented differ-
ently in SEARN and the HMM model. SEARN
decomposes sequential labelling into a series of
binary classifications. To constrain the labels we
simply pick the top-scoring sense for each token
from the allowed set. Structured perceptron uses
Viterbi decoding. Here we set the emission prob-
abilities for disallowed senses to negative infinity
and decode as usual.
</bodyText>
<subsectionHeader confidence="0.999465">
2.2 Weakly supervised HMMs
</subsectionHeader>
<bodyText confidence="0.98662525">
The HMM2 model is a second-order hidden
Markov model (Mari et al., 1997; Thede and
Harper, 1999) using logistic regression to estimate
emission probabilities. In addition we constrain
</bodyText>
<page confidence="0.997753">
2
</page>
<figureCaption confidence="0.9795605">
Figure 1: HMM2 with continuous word represen-
tations
</figureCaption>
<bodyText confidence="0.996637423076923">
the inference space of the HMM2 tagger using
type-level tag constraints derived from WordNet,
leading to roughly the model proposed by Li et
al. (2012), who used Wiktionary as a (part-of-
speech) tag dictionary. The basic feature model
of Li et al. (2012) is augmented with continuous
word representation features as shown in Figure 1,
and our logistic regression model thus works over
a combination of discrete and continuous variables
when estimating emission probabilities. We do 50
passes over the data as in Li et al. (2012).
We introduce two simplifications for the HMM2
model. First, we only use the most frequent senses
(k = 1) in WordNet as type constraints. The
most frequent senses seem to better direct the EM
search for a local optimum, and we see dramatic
drops in performance on held-out data when we
include more senses for the words covered by
WordNet. Second, motivated by computational
concerns, we only train and test on sequences of
(predicted) nouns and verbs, leaving out all other
word classes. Our supervised models performed
slightly worse on shortened sequences, and it is an
open question whether the HMM2 models would
perform better if we could train them on full sen-
tences.
</bodyText>
<subsectionHeader confidence="0.99944">
2.3 Structured perceptron and SEARN
</subsectionHeader>
<bodyText confidence="0.999981">
We use two approaches to supervised sequen-
tial labeling, structured perceptron (Collins, 2002)
and search-based structured prediction (SEARN)
(Daume et al., 2009). The structured perceptron
is a in-house reimplementation of Ciaramita and
Altun (2006).1 SEARN performed slightly better
than structured perceptron, so we use it as our in-
house baseline in the experiments below. In this
section, we briefly explain the two approaches.
</bodyText>
<footnote confidence="0.992059">
1https://github.com/coastalcph/
rungsted
</footnote>
<subsectionHeader confidence="0.554807">
2.3.1 Structured perceptron (HMM)
</subsectionHeader>
<bodyText confidence="0.999976777777778">
Structured perceptron learning was introduced in
Collins (2002) and is an extension of the online
perceptron learning algorithm (Rosenblatt, 1958)
with averaging (Freund and Schapire, 1999) to
structured learning problems such as sequence la-
beling.
In structured perceptron for sequential labeling,
where we learn a function from sequences of data
points x1 ... xn to sequences of labels y1 ... yn,
we begin with a random weight vector w0 initial-
ized to all zeros. This weight vector is used to
assign weights to transitions between labels, i.e.,
the discriminative counterpart of P(yi+1  |yi), and
emissions of tokens given labels, i.e., the counter-
part of P(xi  |yi). We use Viterbi decoding to de-
rive a best path yˆ through the corresponding mxn
lattice (with m the number of labels). Let the fea-
ture mapping Φ(x, y) be a function from a pair
of sequences (x, y) to all the features that fired
to make y the best path through the lattice for x.
Now the structured update for a sequence of data
points is simply α(Φ(x, y)−Φ(x, ˆy)), i.e., a fixed
positive update of features that fired to produce the
correct sequence of labels, and a fixed negative up-
date of features that fired to produce the best path
under the model. Note that if y = ˆy, no features
are updated.
</bodyText>
<sectionHeader confidence="0.408248" genericHeader="introduction">
2.3.2 SEARN
</sectionHeader>
<bodyText confidence="0.999906153846154">
SEARN is a way of decomposing structured pre-
diction problems into search and history-based
classification. In sequential labeling, we decom-
pose the sequence of m tokens into m classifica-
tion problems, conditioning our labeling of the ith
token on the history of i − 1 previous decisions.
The cost of a mislabeling at training time is de-
fined by a cost function over output structures. We
use Hamming loss rather than F1 as our cost func-
tion, and we then use stochastic gradient descent
with quantile loss as a our cost-sensitive learning
algorithm. We use a publicly available implemen-
tation.2
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999774">
We experiment with weakly supervised learning,
unsupervised domain adaptation, as well as su-
pervised domain adaptation, i.e., where our mod-
els are induced from hand-annotated newswire
and Twitter data. Note that in all our experiments,
</bodyText>
<equation confidence="0.94895375">
2http://hunch.net/˜vw/
P(t2|t1)
t1 t2
t3
P(w1|t1)
w1
w2
w3
</equation>
<page confidence="0.949472">
3
</page>
<bodyText confidence="0.999428">
we use predicted POS tags as input to the system,
in order to produce a realistic estimate of SST per-
formance.
</bodyText>
<subsectionHeader confidence="0.997934">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999881666666667">
Our experiments rely on combinations of available
resources and newly annotated Twitter data sets
made publicly available with this paper.
</bodyText>
<subsectionHeader confidence="0.975659">
3.1.1 Available resources
</subsectionHeader>
<bodyText confidence="0.999883133333333">
Princeton WordNet (Fellbaum, 1998) is the main
resource for SST. The lexicographer file names
provide the label alphabet of the task, and the tax-
onomy defined therein is used not only in the base-
lines, but also as a feature in the discriminative
models. We use the WordNet 3.0 distribution.
SEMCOR (Miller et al., 1994) is a sense-
annotated corpus composed of 80% newswire and
20% literary text, using the sense inventory from
WordNet. SEMCOR comprises 23k distinct lem-
mas in 234k instances. We use the texts which
have full annotations, leaving aside the verb-only
texts (see Section 6).
We use a distributional semantic model in order
to incorporate distributional information as fea-
tures in our system. In particular, we use the
neural-network based models from (Mikolov et
al., 2013), also referred as word embeddings. This
model makes use of skip-grams (n-grams that do
not need to be consecutive) within a word window
to calculate continuous-valued vector representa-
tions from a recurrent neural network. These dis-
tributional models have been able to outperform
state of the art in the SemEval-2012 Task 2 (Mea-
suring degrees of relational similarity). We calcu-
late the embeddings from an in-house corpus of
57m English tweets using a window size 5 and
yielding vectors of 100 dimensions.
We also use the first 20k tweets of the 57m
tweets to train our HMM2 models.
</bodyText>
<subsectionHeader confidence="0.893288">
3.1.2 Annotation
</subsectionHeader>
<bodyText confidence="0.993718090909091">
While an annotated newswire corpus and a high-
quality lexical resource already enable us to train,
we also need at least a small sample of anno-
tated tweets data to evaluate SST for Twitter. Fur-
thermore, if we want to experiment with super-
vised SST, we also need sufficient annotated Twit-
ter data to learn the distribution of sense tags.
This paper presents two data sets: (a) super-
sense annotations for the POS+NER-annotated
data set described in Ritter et al. (2011), which we
use for training, development and evaluation, us-
ing the splits proposed in Derczynski et al. (2013),
and (b) supersense annotations for a sample of 200
tweets, which we use for additional, out-of-sample
evaluation. We call these data sets RITTER-
{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-
spectively. The IN-HOUSE-EVAL dataset was
downloaded in 2013 and is a sample of tweets that
contain links to external homepages but are other-
wise unbiased. It was previously used (with part-
of-speech annotation) in (Plank et al., 2014). Both
data sets are made publicly available with this pa-
per.
Supersenses are annotated with in spans defined
by the BIO (Begin-Inside-Other) notation. To ob-
tain the Twitter data sets, we carried out an an-
notation task. We first pre-annotated all data sets
with WordNet’s most frequent senses. If the word
was not in WordNet and a noun, we assigned it the
sense n.person. All other words were labeled O.
Chains of nouns were altered to give every ele-
ment the sense of the head noun, and the BI tags
adjusted, i.e.:
</bodyText>
<subsectionHeader confidence="0.612285">
Empire/B-n.loc State/B-n.loc Building/B-n.artifact
</subsectionHeader>
<bodyText confidence="0.944470222222222">
was changed to
Empire/B-n.artifact State/I-n.artifact Building/I-
n.artifact
For the RITTER data, three paid student an-
notators worked on different subsets of the pre-
annotated data. They were asked to correct mis-
takes in both the BIO notation and the assigned
supersenses. They were free to chose from the full
label set, regardless of the pre-annotation. While
the three annotators worked on separate parts, they
overlapped on a small part of RITTER-TRAIN (841
tokens). On this subset, we computed agreement
scores and annotation difficulties. The average
raw agreement was 0.86 and Cohen’s n 0.77. The
majority of tokens received the O label by all an-
notators; this happended in 515 out of 841 cases.
Excluding these instances to evaluate the perfor-
mance on the more difficult content words, raw
agreement dropped to 0.69 and Cohen’s n to 0.69.
The IN-HOUSE-EVAL data set was annotated
by two different annotators, namely two of the au-
thors of this article. Again, for efficiency reasons
they worked on different subsets of the data, with
an overlapping portion. Their average raw agree-
ment was 0.65 and their Cohen’s n 0.62. For this
data set, we also compute F1, defined as usual as
the harmonic mean of recall and precision. To
</bodyText>
<page confidence="0.987945">
4
</page>
<bodyText confidence="0.956692833333333">
compute this, we set one of the annotators as gold
data and the other as predicted data. However,
since F1 is symmetrical, the order does not mat-
ter. The annotation F1 gives us another estimate
of annotation difficulty. We present the figures in
Table 3.
</bodyText>
<subsectionHeader confidence="0.99876">
3.2 Baselines
</subsectionHeader>
<bodyText confidence="0.999949136363637">
For most word sense disambiguation studies, pre-
dicting the most frequent sense (MFS) of a word
has been proven to be a strong baseline. Follow-
ing this, our MFS baseline simply predicts the su-
persense of the most frequent WordNet sense for
a tuple of a word and a part of speech. We use
the part of speech predicted by the LAPOS tagger
(Tsuruoka et al., 2011). Any word not in Word-
Net is labeled as noun.person, which is the most
frequent sense overall in the training data. After
tagging, we run a script to correct the BI tag pre-
fixes, as described above for the annotation ask.
We also compare to the performance of exist-
ing SST systems. In particular we use Sense-
Learner (Mihalcea and Csomai, 2005) as a base-
line, which produces estimates of the WordNet
sense for each word. For these predictions, we
retrieve the corresponding supersense. Finally,
we use a publicly available reimplementation of
Ciaramita and Altun (2006) by Michael Heilman,
which reaches comparable performance on gold-
tagged SEMCOR.3
</bodyText>
<subsectionHeader confidence="0.999628">
3.3 Model parameters
</subsectionHeader>
<bodyText confidence="0.9992664">
We use the feature model of Paaß and Reichartz
(2009) in all our models, except the weakly su-
pervised models. For the structured perceptron we
set the number of passes over the training data on
the held-out development data. The weakly super-
vised models use the default setting proposed in
Li et al. (2012). We have used the standard online
setup for SEARN, which only takes one pass over
the data.
The type of embedding is the same in all our
experiments. For a given word the embedding fea-
ture is a 100 dimensional vector, which combines
the embedding of the word with the embedding of
adjacent words. The feature combination fe for a
word wt is calculated as:
</bodyText>
<equation confidence="0.985841">
1
fe(wt) = 2(e(wt−1) + e(wt+1)) − 2e(wt),
</equation>
<footnote confidence="0.749140333333333">
3http://www.ark.cs.cmu.edu/mheilman/
questions/SupersenseTagger-10-01-12.tar.
gz
</footnote>
<bodyText confidence="0.9999725">
where the factor of two is chosen heurestically to
give more weight to the current word.
We also set a parameter k on development data
for using the k-most frequent senses inWordNet
as type constraints. Our supervised models are
trained on SEMCOR+RITTER-TRAIN or simply
RITTER-TRAIN, depending on what gave us the
best performance on the held-out data.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999971195121952">
The results are presented in Table 2. We dis-
tinguish between three settings with various de-
grees of supervision: weakly supervised, which
uses no domain annotated information, but solely
relies on embeddings trained on unlabeled Twit-
ter data; unsupervised domain adaptation (DA),
which uses SemCor for supervised training; and
supervised domain adaptation (SU), which uses
annotated Twitter data in addition to the SemCor
data for training.
In each of the two domain adaptation settings,
SEARN and HMM are evaluated with type con-
straints as distant supervision, and without for
comparison. SEARN without embeddings or dis-
tant supervision serves as an in-house baseline.
In Table 3 we present the WordNet token cov-
erage of predicted nouns and verbs in the devel-
opment and evaluation data, as well as the inter-
annotator agreement F1 scores.
All the results presented in Table 2 are
(weighted averaged) F1 measures obtained on pre-
dicted POS tags. Note that these results are con-
siderably lower than results on supersense tagging
newswire (up to 80 F1) that assume gold standard
POS tags (Ciaramita and Altun, 2006; Paaß and
Reichartz, 2009).
The re-implementation of the state-of-the-art
system improves slightly upon the most frequent
sense baseline. SenseLearner does not seem to
capture the relevant information and does not
reach baseline performance. In other words, there
is no off-the-shelf tool for supersense tagging of
Twitter that does much better than assigning the
most frequent sense to predicted nouns and verbs.
Our weakly supervised model performs worse
than the most frequent sense baseline. This is a
negative result. It is, however, well-known from
the word sense disambiguation literature that the
MFS is a very strong baseline. Moreover, the EM
learning problem is hard because of the large la-
bel set and weak distributional evidence for super-
</bodyText>
<page confidence="0.97296">
5
</page>
<table confidence="0.685585833333333">
RITTER IN-HOUSE
DEV EVAL EVAL
Wordnet noun-verb
token coverage 83.72 70.22 41.18
Inter-annotator
agreement (F1) 81.01 69.15 61.57
</table>
<tableCaption confidence="0.993062">
Table 3: Properties of dataset.
</tableCaption>
<bodyText confidence="0.9979452">
senses.
The unsupervised domain adaptation and fully
supervised systems perform considerably better
than this baseline across the board. In the unsuper-
vised domain adaptation setup, we see huge im-
provements from using type constraints as distant
supervision. In the supervised setup, we only see
significant improvements adding type constraints
for the structured perceptron (HMM), but not for
search-based structured prediction (SEARN).
For all the data sets, there is still a gap between
model performance and human inter-annotator
agreement levels (see Table 3), leaving some room
for improvements. We hope that the release of the
data sets will help further research into this.
</bodyText>
<subsectionHeader confidence="0.985677">
4.1 Coarse-grained evaluation
</subsectionHeader>
<bodyText confidence="0.999985636363636">
We also experimented with the more coarse-
grained classes proposed by Yuret and Yatbaz
(2010). Here our best model obtained an F1 score
for mental concepts (nouns) of 72.3%, and 62.6%
for physical concepts, on RITTER-DEV. The over-
all F1 score for verbs is 85.6%. The overall F1 is
75.5%. Note that this result is not directly com-
parable to the figure (72.9%) reported in Yuret
and Yatbaz (2010), since they use different data
sets, exclude verbs and make different assump-
tions, e.g., relying on gold POS tags.
</bodyText>
<sectionHeader confidence="0.99515" genericHeader="method">
5 Error analysis
</sectionHeader>
<bodyText confidence="0.99998953125">
We have seen that inter-annotator agreements on
supersense annotation are reliable at above .60
but far from perfect. The Hinton diagram in Ta-
ble 2 presents the confusion matrix between our
annotators on IN-HOUSE-EVAL.
Errors in the prediction primarily stem from
two sources: out-of-vocabulary words and incor-
rect POS tags. Figure 3 shows the distribution of
senses over the words that were not contained in
either the training data, WordNet, or the Twitter
data used to learn the embeddings. The distribu-
tion follows a power law, with the most frequent
sense being noun.person, followed by noun.group,
and noun.artifact. The first two are related to NER
categories, namely PER and ORG, and can be ex-
pected, since Twitter users frequently talk about
new actors, musicians, and bands. Nouns of com-
munication are largely related to films, but also in-
clude Twitter, Facebook, and other forms of social
media. Note that verbs occur only towards the tail
end of the distribution, i.e., there are very few un-
known verbs, even in Twitter.
Overall, our models perform best on labels with
low lexical variability, such as quantities, states
and times for nouns, as well as consumption, pos-
session and stative for verbs. This is unsurprising,
since these classes have lower out-of-vocabulary
rates.
With regards to the differences between source
(SEMCOR) and target (Twitter) domains, we ob-
serve that the distribution of supersenses is al-
ways headed by the same noun categories like
noun.person or noun.group, but the frequency of
out-of-vocabulary stative verbs plummets in the
target domain, as some semantic types are more
closed class than others. There are for instance
fewer possibilities for creating new time units
(noun.time) or stative verbs like be than people or
company names (noun.person or noun.group, re-
spectively).
The weakly supervised model HMM2 has
higher precision (57% on RITTER-DEV) than re-
call (48.7%), which means that it often predicts
words to not belong to a semantic class. This
suggests an alternative strategy, which is to train
a model on sequences of purely non-O instances.
This would force the model to only predict O on
words that do not appear in the reduced sequences.
One important source of error seems to be un-
reliable part-of-speech tagging. In particular we
predict the wrong POS for 20-35% of the verbs
across the data sets, and for 4-6.5% of the nouns.
In the SEMCOR data, for comparability, we have
wrongly predicted tags for 6-8% of the anno-
tated tokens. Nevertheless, the error propaga-
tion of wrongly predicted nouns and verbs is par-
tially compensated by our systems, since they are
trained on imperfect input, and thus it becomes
possible for the systems to predict a noun super-
sense for a verb and viceversa. In our data we have
found e.g. that the noun Thanksgiving was incor-
rectly tagged as a verb, but its supersense was cor-
rectly predicted to be noun.time, and that the verb
guess had been mistagged as noun but the system
</bodyText>
<page confidence="0.999043">
6
</page>
<table confidence="0.9563165">
Resources Results
Token-level Type-level RITTER IN-HOUSE
DEV EVAL EVAL
SemCor Twitter Embeddings Type constraints
General baselines
MFS - - - + 47.54 44.98 38.65
SENSELEARNER + - - - 14.61 26.24 22.81
HEILMAN + - - - 48.96 45.03 39.65
Weakly supervised systems
HMM2 - - - + 47.09 42.12 26.99
Unsupervised domain adaptation systems (DA)
SEARN (Baseline) + - - - 48.31 42.34 34.30
SEARN + - + - 52.45 48.30 40.22
SEARN + - + + 56.59 50.89 40.50
HMM + - + - 52.40 47.90 40.51
HMM + - + + 57.14 50.98 41.84
Supervised domain adaptation systems (SU)
SEARN (Baseline) + + - - 58.30 52.12 36.86
SEARN + + + - 63.05 57.09 42.37
SEARN + + + + 62.72 57.14 42.42
HMM + + + - 57.20 49.26 39.88
HMM + + + + 60.66 51.40 41.60
</table>
<tableCaption confidence="0.990464">
Table 2: Weighted F1 average over 41 supersenses.
</tableCaption>
<page confidence="0.998967">
7
</page>
<figureCaption confidence="0.989071">
Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.
</figureCaption>
<figure confidence="0.997811">
0.4
0.3
0.2
0.1
0
</figure>
<figureCaption confidence="0.998419">
Figure 3: Sense distribution of OOV words.
</figureCaption>
<page confidence="0.994039">
8
</page>
<bodyText confidence="0.9977995">
still predicted the correct verb.cognition as super-
sense.
</bodyText>
<sectionHeader confidence="0.999693" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999984833333334">
There has been relatively little previous work on
supersense tagging, and to the best of our knowl-
edge, all of it has been limited to English newswire
and literature (SEMCOR and SENSEVAL).
The task of supersense tagging was first intro-
duced by Ciaramita and Altun (2006), who used
a structured perceptron trained and evaluated on
SEMCOR via 5-fold cross validation. Their eval-
uation included a held-out development set on
each fold that was used to estimate the number of
epochs. They used additional training data con-
taining only verbs. More importantly, they relied
on gold standard POS tags. Their overall F1 score
on SEMCOR was 77.1. Reichartz and Paaß (Re-
ichartz and Paaß, 2008; Paaß and Reichartz, 2009)
extended this work, using a CRF model as well
as LDA topic features. They report an F1 score
of 80.2, again relying on gold standard POS fea-
tures. Our implementation follows their setup and
feature model, but we rely on predicted POS fea-
tures, not gold standard features.
Supersenses provide information similar to
higher-level distributional clusters, but more in-
terpretable, and have thus been used as high-
level features in various tasks, such as preposi-
tion sense disambiguation, noun compound inter-
pretation, and metaphor detection (Ye and Bald-
win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,
2013). Princeton WordNet only provides a fully
developed taxonomy of supersenses for verbs and
nouns, but Tsvetkov et al. (2014) have recently
proposed an extension of the taxonomy to cover
adjectives. Outside of English, supersenses have
been annotated for Arabic Wikipedia articles by
Schneider et al. (2012).
In addition, a few researchers have tried to
solve coarse-grained word sense disambiguation
problems that are very similar to supersense tag-
ging. Kohomban and Lee (2005) and Kohom-
ban and Lee (2007) also propose to use lexicogra-
pher file identifers from Princeton WordNet senses
(supersenses) and, in addition, discuss how to re-
trieve fine-grained senses from those predictions.
They evaluate their model on all-words data from
SENSEEVAL-2 and SENSEEVAL-3. They use a
classification approach rather than structured pre-
diction.
Yuret and Yatbaz (2010) present a weakly unsu-
pervised approach to this problem, still evaluating
on SENSEVAL-2 and SENSEVAL-3. They focus
only on nouns, relying on gold part-of-speech, but
also experiment with a coarse-grained mapping,
using only three high level classes.
For Twitter, we are aware of little previous work
on word sense disambiguation. Gella et al. (2014)
present lexical sample word sense disambiguation
annotation of 20 target nouns on Twitter, but no
experimental results with this data. There has also
been related work on disambiguation to Wikipedia
for Twitter (Cassidy et al., 2012).
In sum, existing work on supersense tagging
and coarse-grained word sense disambiguation for
English has to the best of our knowledge all fo-
cused on newswire and literature. Moreover, they
all rely on gold standard POS information, making
previous performance estimates rather optimistic.
</bodyText>
<sectionHeader confidence="0.999046" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968833333334">
In this paper, we present two Twitter data sets with
manually annotated supersenses, as well as a se-
ries of experiments with these data sets. The data
is publicly available for download.
In this article we have provided, to the best
of our knowledge, the first supersense tagger for
Twitter. We have shown that off-the-shelf tools
perform poorly on Twitter, and we offer two
strategies—namely distant supervision and the us-
age of embeddings as features—that can be com-
bined to improve SST for Twitter.
We propose that distant supervision imple-
mented as type constraints during decoding is a
viable method to limit the mispredictions of su-
persenses by our systems, thereby enforcing pre-
dicted senses that a word has in WordNet. This ap-
proach compensates for the size limitations of the
training data and mitigates the out-of-vocabulary
effect, but is still subject to the coverage of Word-
Net; which is far from perfect for words coming
from high-variability sources such as Twitter.
Using distributional semantics as features in
form of word embeddings also improves the pre-
diction of supersenses, because it provides seman-
tic information for words, regardless of whether
they have been observed the training data. This
method does not require a hand-created knowl-
edge base like WordNet, and is a promising tech-
nique for domain adaptation of supersense tag-
ging.
</bodyText>
<page confidence="0.997103">
9
</page>
<sectionHeader confidence="0.989943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871215686274">
Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-
biaga, and Hongzhao Huang. 2012. Analysis and
enhancement of wikification for microblogs with
context expansion. In COLING, volume 12, pages
441–456.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594–602, Sydney, Australia,
July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Hal Daume, John Langford, and Daniel Marcu. 2009.
Search-based structured prediction. Machine Learn-
ing, pages 297–325.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: overcoming sparse and noisy data. In
RANLP.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press USA.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Josef Le Roux, Joakim Nivre, Deirde Hogan, and
Josef van Genabith. 2011. From news to comments:
Resources and benchmarks for parsing the language
of Web 2.0. In IJCNLP.
Yoav Freund and Robert Schapire. 1999. Large margin
classification using the perceptron algorithm. Ma-
chine Learning, 37:277–296.
Spandana Gella, Paul Cook, and Timothy Baldwin.
2014. One sense per tweeter and other lexical se-
mantic tales of Twitter. In EACL.
Upali Kohomban and Wee Lee. 2005. Learning se-
mantic classes for word sense disambiguation. In
ACL.
Upali Kohomban and Wee Lee. 2007. Optimizing
classifier performance in word sense disambiguation
by redefining word sense classes. In IJCAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In ICML.
Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In EMNLP.
Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz
Kriouile. 1997. Automatic word recognition based
on second-order hidden Markov models. IEEE
Transactions on Speech and Audio Processing,
5(1):22–25.
Rada Mihalcea and Andras Csomai. 2005. Sense-
learner: Word sense disambiguation for all words in
unrestricted text. In Proceedings of the ACL 2005
on Interactive poster and demonstration sessions,
pages 53–56. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS.
George A. Miller, Martin Chodorow, Shari Landes,
Claudia Leacock, and Robert G. Thomas. 1994.
Using a semantic concordance for sense identifica-
tion. In Proceedings of the workshop on Human
Language Technology, pages 240–243. Association
for Computational Linguistics.
Gerhard Paaß and Frank Reichartz. 2009. Exploit-
ing semantic constraints for estimating supersenses
with CRFs. In Proc. of the Ninth SIAM Interna-
tional Conference on Data Mining, pages 485–496,
Sparks, Nevada, May.
Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014.
Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL.
Frank Reichartz and Gerhard Paaß. 2008. Estimating
Supersenses with Conditional Random Fields. In
Proceedings of ECMLPKDD.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In EMNLP.
Frank Rosenblatt. 1958. The perceptron: a probabilis-
tic model for information storage and organization
in the brain. Psychological Review, 65(6):386–408.
Nathan Schneider, Behrang Mohit, Kemal Oflazer, and
Noah A Smith. 2012. Coarse lexical semantic an-
notation with supersenses: an arabic case study. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 253–
258. Association for Computational Linguistics.
Scott Thede and Mary Harper. 1999. A second-order
hidden Markov model for part-of-speech tagging. In
ACL.
Stephen Tratz and Eduard Hovy. 2010. Isi: automatic
classification of relations between nominals using a
maximum entropy classifier. In Proceedings of the
5th International Workshop on Semantic Evaluation,
pages 222–225. Association for Computational Lin-
guistics.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi
Kazama. 2011. Learning with lookahead: can
history-based models rival globally optimized mod-
els? In CoNLL.
</reference>
<page confidence="0.949377">
10
</page>
<reference confidence="0.999465125">
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Meta4NLP 2013,
page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting english adjective senses with super-
senses. In Proc. of LREC.
Patrick Ye and Timothy Baldwin. 2007. Melb-yb:
Preposition sense disambiguation using rich seman-
tic features. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 241–244.
Association for Computational Linguistics.
Deniz Yuret and Mehmet Yatbaz. 2010. The noisy
channel model for unsupervised word sense disam-
biguation. Computational Linguistics, 36:111–127.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.328785">
<title confidence="0.996419">More or less supervised supersense tagging of Twitter</title>
<author confidence="0.990181">Anders Johannsen</author>
<author confidence="0.990181">Dirk Hovy</author>
<author confidence="0.990181">H´ector Martinez Alonso</author>
<author confidence="0.990181">Barbara Plank</author>
<author confidence="0.990181">Anders</author>
<affiliation confidence="0.999572">Center for Language University of Copenhagen,</affiliation>
<email confidence="0.709552666666667">Njalsgadeajohannsen@hum.ku.dk,dirk@cst.dk,plank@cst.dk,soegaard@hum.ku.dk</email>
<abstract confidence="0.991901357142857">We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Cassidy</author>
<author>Heng Ji</author>
<author>Lev-Arie Ratinov</author>
<author>Arkaitz Zubiaga</author>
<author>Hongzhao Huang</author>
</authors>
<title>Analysis and enhancement of wikification for microblogs with context expansion.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<volume>12</volume>
<pages>441--456</pages>
<contexts>
<context position="29054" citStr="Cassidy et al., 2012" startWordPosition="4681" endWordPosition="4684">ed prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on SENSEVAL-2 and SENSEVAL-3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. 7 Conclusion In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. The data is publicly available for download. In this article we have provided, to the best of our knowledge, the first supersense tagger for Twitter. We h</context>
</contexts>
<marker>Cassidy, Ji, Ratinov, Zubiaga, Huang, 2012</marker>
<rawString>Taylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zubiaga, and Hongzhao Huang. 2012. Analysis and enhancement of wikification for microblogs with context expansion. In COLING, volume 12, pages 441–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="891" citStr="Ciaramita and Altun, 2006" startWordPosition="118" endWordPosition="121">.dk plank@cst.dk, soegaard@hum.ku.dk Abstract We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not</context>
<context position="9755" citStr="Ciaramita and Altun (2006)" startWordPosition="1496" endWordPosition="1499">dNet. Second, motivated by computational concerns, we only train and test on sequences of (predicted) nouns and verbs, leaving out all other word classes. Our supervised models performed slightly worse on shortened sequences, and it is an open question whether the HMM2 models would perform better if we could train them on full sentences. 2.3 Structured perceptron and SEARN We use two approaches to supervised sequential labeling, structured perceptron (Collins, 2002) and search-based structured prediction (SEARN) (Daume et al., 2009). The structured perceptron is a in-house reimplementation of Ciaramita and Altun (2006).1 SEARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 1https://github.com/coastalcph/ rungsted 2.3.1 Structured perceptron (HMM) Structured perceptron learning was introduced in Collins (2002) and is an extension of the online perceptron learning algorithm (Rosenblatt, 1958) with averaging (Freund and Schapire, 1999) to structured learning problems such as sequence labeling. In structured perceptron for sequential labeling, where we learn a function from sequences of </context>
<context position="17857" citStr="Ciaramita and Altun (2006)" startWordPosition="2851" endWordPosition="2854">eech predicted by the LAPOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged SEMCOR.3 3.3 Model parameters We use the feature model of Paaß and Reichartz (2009) in all our models, except the weakly supervised models. For the structured perceptron we set the number of passes over the training data on the held-out development data. The weakly supervised models use the default setting proposed in Li et al. (2012). We have used the standard online setup for SEARN, which only takes one pass over the data. The type of embedding is the same in all our experiments. For a given word the embedding feature is</context>
<context position="20218" citStr="Ciaramita and Altun, 2006" startWordPosition="3233" endWordPosition="3236">s, SEARN and HMM are evaluated with type constraints as distant supervision, and without for comparison. SEARN without embeddings or distant supervision serves as an in-house baseline. In Table 3 we present the WordNet token coverage of predicted nouns and verbs in the development and evaluation data, as well as the interannotator agreement F1 scores. All the results presented in Table 2 are (weighted averaged) F1 measures obtained on predicted POS tags. Note that these results are considerably lower than results on supersense tagging newswire (up to 80 F1) that assume gold standard POS tags (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009). The re-implementation of the state-of-the-art system improves slightly upon the most frequent sense baseline. SenseLearner does not seem to capture the relevant information and does not reach baseline performance. In other words, there is no off-the-shelf tool for supersense tagging of Twitter that does much better than assigning the most frequent sense to predicted nouns and verbs. Our weakly supervised model performs worse than the most frequent sense baseline. This is a negative result. It is, however, well-known from the word sense disambiguation literature tha</context>
<context position="26572" citStr="Ciaramita and Altun (2006)" startWordPosition="4292" endWordPosition="4295">.05 57.09 42.37 SEARN + + + + 62.72 57.14 42.42 HMM + + + - 57.20 49.26 39.88 HMM + + + + 60.66 51.40 41.60 Table 2: Weighted F1 average over 41 supersenses. 7 Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 still predicted the correct verb.cognition as supersense. 6 Related Work There has been relatively little previous work on supersense tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (SEMCOR and SENSEVAL). The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on SEMCOR via 5-fold cross validation. Their evaluation included a held-out development set on each fold that was used to estimate the number of epochs. They used additional training data containing only verbs. More importantly, they relied on gold standard POS tags. Their overall F1 score on SEMCOR was 77.1. Reichartz and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implement</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proc. of EMNLP, pages 594–602, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="5299" citStr="Collins, 2002" startWordPosition="817" endWordPosition="818">omain adaptation (DA) techniques can be used to exploit these resources. The setting does not presume labeled data from the target domain. We use discriminative models for unsupervised domain adaptation, training on SEMCOR and testing on Twitter. Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (SEARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and SEARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. Contributions In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. These experiments cover existing approaches to related tasks, as well </context>
<context position="9599" citStr="Collins, 2002" startWordPosition="1477" endWordPosition="1478">M search for a local optimum, and we see dramatic drops in performance on held-out data when we include more senses for the words covered by WordNet. Second, motivated by computational concerns, we only train and test on sequences of (predicted) nouns and verbs, leaving out all other word classes. Our supervised models performed slightly worse on shortened sequences, and it is an open question whether the HMM2 models would perform better if we could train them on full sentences. 2.3 Structured perceptron and SEARN We use two approaches to supervised sequential labeling, structured perceptron (Collins, 2002) and search-based structured prediction (SEARN) (Daume et al., 2009). The structured perceptron is a in-house reimplementation of Ciaramita and Altun (2006).1 SEARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 1https://github.com/coastalcph/ rungsted 2.3.1 Structured perceptron (HMM) Structured perceptron learning was introduced in Collins (2002) and is an extension of the online perceptron learning algorithm (Rosenblatt, 1958) with averaging (Freund and Schapire, 199</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>John Langford</author>
<author>Daniel Marcu</author>
</authors>
<title>Search-based structured prediction.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<pages>297--325</pages>
<contexts>
<context position="5402" citStr="Daume et al., 2009" startWordPosition="830" endWordPosition="833">ume labeled data from the target domain. We use discriminative models for unsupervised domain adaptation, training on SEMCOR and testing on Twitter. Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (SEARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and SEARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. Contributions In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. These experiments cover existing approaches to related tasks, as well as some new methods. In particular, we present type-constrained extensions of discriminative HMMs and S</context>
<context position="9667" citStr="Daume et al., 2009" startWordPosition="1484" endWordPosition="1487">ormance on held-out data when we include more senses for the words covered by WordNet. Second, motivated by computational concerns, we only train and test on sequences of (predicted) nouns and verbs, leaving out all other word classes. Our supervised models performed slightly worse on shortened sequences, and it is an open question whether the HMM2 models would perform better if we could train them on full sentences. 2.3 Structured perceptron and SEARN We use two approaches to supervised sequential labeling, structured perceptron (Collins, 2002) and search-based structured prediction (SEARN) (Daume et al., 2009). The structured perceptron is a in-house reimplementation of Ciaramita and Altun (2006).1 SEARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 1https://github.com/coastalcph/ rungsted 2.3.1 Structured perceptron (HMM) Structured perceptron learning was introduced in Collins (2002) and is an extension of the online perceptron learning algorithm (Rosenblatt, 1958) with averaging (Freund and Schapire, 1999) to structured learning problems such as sequence labeling. In str</context>
</contexts>
<marker>Daume, Langford, Marcu, 2009</marker>
<rawString>Hal Daume, John Langford, and Daniel Marcu. 2009. Search-based structured prediction. Machine Learning, pages 297–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</title>
<date>2013</date>
<contexts>
<context position="14445" citStr="Derczynski et al. (2013)" startWordPosition="2276" endWordPosition="2279">eets to train our HMM2 models. 3.1.2 Annotation While an annotated newswire corpus and a highquality lexical resource already enable us to train, we also need at least a small sample of annotated tweets data to evaluate SST for Twitter. Furthermore, if we want to experiment with supervised SST, we also need sufficient annotated Twitter data to learn the distribution of sense tags. This paper presents two data sets: (a) supersense annotations for the POS+NER-annotated data set described in Ritter et al. (2011), which we use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets RITTER{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, respectively. The IN-HOUSE-EVAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data set</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: overcoming sparse and noisy data. In RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press USA.</publisher>
<contexts>
<context position="1144" citStr="Fellbaum, 1998" startWordPosition="161" endWordPosition="163">nsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the abse</context>
<context position="3656" citStr="Fellbaum, 1998" startWordPosition="571" endWordPosition="572">rthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • SEMCOR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twitter). In 1 Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1–11, Dublin, Ireland, August 23-24 2014. weakly supervised learning, only unlabeled data and the lexical resource WordNet are available to us. While the quality of lexical resources varies, this is the scenario for most languages. We present an approach to </context>
<context position="12516" citStr="Fellbaum, 1998" startWordPosition="1953" endWordPosition="1954">e experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data. Note that in all our experiments, 2http://hunch.net/˜vw/ P(t2|t1) t1 t2 t3 P(w1|t1) w1 w2 w3 3 we use predicted POS tags as input to the system, in order to produce a realistic estimate of SST performance. 3.1 Data Our experiments rely on combinations of available resources and newly annotated Twitter data sets made publicly available with this paper. 3.1.1 Available resources Princeton WordNet (Fellbaum, 1998) is the main resource for SST. The lexicographer file names provide the label alphabet of the task, and the taxonomy defined therein is used not only in the baselines, but also as a feature in the discriminative models. We use the WordNet 3.0 distribution. SEMCOR (Miller et al., 1994) is a senseannotated corpus composed of 80% newswire and 20% literary text, using the sense inventory from WordNet. SEMCOR comprises 23k distinct lemmas in 234k instances. We use the texts which have full annotations, leaving aside the verb-only texts (see Section 6). We use a distributional semantic model in orde</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Ozlem Cetinoglu</author>
<author>Joachim Wagner</author>
<author>Josef Le Roux</author>
<author>Joakim Nivre</author>
<author>Deirde Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>From news to comments: Resources and benchmarks for parsing the language of Web 2.0.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Foster, Cetinoglu, Wagner, Le Roux, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner, Josef Le Roux, Joakim Nivre, Deirde Hogan, and Josef van Genabith. 2011. From news to comments: Resources and benchmarks for parsing the language of Web 2.0. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--277</pages>
<contexts>
<context position="10201" citStr="Freund and Schapire, 1999" startWordPosition="1557" endWordPosition="1560">rceptron (Collins, 2002) and search-based structured prediction (SEARN) (Daume et al., 2009). The structured perceptron is a in-house reimplementation of Ciaramita and Altun (2006).1 SEARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 1https://github.com/coastalcph/ rungsted 2.3.1 Structured perceptron (HMM) Structured perceptron learning was introduced in Collins (2002) and is an extension of the online perceptron learning algorithm (Rosenblatt, 1958) with averaging (Freund and Schapire, 1999) to structured learning problems such as sequence labeling. In structured perceptron for sequential labeling, where we learn a function from sequences of data points x1 ... xn to sequences of labels y1 ... yn, we begin with a random weight vector w0 initialized to all zeros. This weight vector is used to assign weights to transitions between labels, i.e., the discriminative counterpart of P(yi+1 |yi), and emissions of tokens given labels, i.e., the counterpart of P(xi |yi). We use Viterbi decoding to derive a best path yˆ through the corresponding mxn lattice (with m the number of labels). Let</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37:277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spandana Gella</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>One sense per tweeter and other lexical semantic tales of Twitter.</title>
<date>2014</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="28820" citStr="Gella et al. (2014)" startWordPosition="4645" endWordPosition="4648">supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from SENSEEVAL-2 and SENSEEVAL-3. They use a classification approach rather than structured prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on SENSEVAL-2 and SENSEVAL-3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. 7 Conclusion In this paper, we present two Twitter data sets with manually </context>
</contexts>
<marker>Gella, Cook, Baldwin, 2014</marker>
<rawString>Spandana Gella, Paul Cook, and Timothy Baldwin. 2014. One sense per tweeter and other lexical semantic tales of Twitter. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Upali Kohomban</author>
<author>Wee Lee</author>
</authors>
<title>Learning semantic classes for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="28091" citStr="Kohomban and Lee (2005)" startWordPosition="4534" endWordPosition="4537">tion sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from SENSEEVAL-2 and SENSEEVAL-3. They use a classification approach rather than structured prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on SENSEVAL-2 and SENSEVAL-3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using o</context>
</contexts>
<marker>Kohomban, Lee, 2005</marker>
<rawString>Upali Kohomban and Wee Lee. 2005. Learning semantic classes for word sense disambiguation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Upali Kohomban</author>
<author>Wee Lee</author>
</authors>
<title>Optimizing classifier performance in word sense disambiguation by redefining word sense classes.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="28119" citStr="Kohomban and Lee (2007)" startWordPosition="4539" endWordPosition="4543">oun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from SENSEEVAL-2 and SENSEEVAL-3. They use a classification approach rather than structured prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on SENSEVAL-2 and SENSEVAL-3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes</context>
</contexts>
<marker>Kohomban, Lee, 2007</marker>
<rawString>Upali Kohomban and Wee Lee. 2007. Optimizing classifier performance in word sense disambiguation by redefining word sense classes. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="5605" citStr="Lafferty et al., 2001" startWordPosition="861" endWordPosition="864"> supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (SEARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and SEARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. Contributions In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. These experiments cover existing approaches to related tasks, as well as some new methods. In particular, we present type-constrained extensions of discriminative HMMs and SEARN sequence models with continuous word representations that perform well. We show that when no in-domain labeled data is available, type constraints improve model performance considerably. Our best mo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Jo˜ao Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In EMNLP.</booktitle>
<marker>Li, Grac¸a, Taskar, 2012</marker>
<rawString>Shen Li, Jo˜ao Grac¸a, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Francois Mari</author>
<author>Jean-Paul Haton</author>
<author>Abdelaziz Kriouile</author>
</authors>
<title>Automatic word recognition based on second-order hidden Markov models.</title>
<date>1997</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="8100" citStr="Mari et al., 1997" startWordPosition="1232" endWordPosition="1235">ining data nor in WordNet, no filtering was applied. We refer to the distantsupervision strategy as type constraints. Distant supervision was implemented differently in SEARN and the HMM model. SEARN decomposes sequential labelling into a series of binary classifications. To constrain the labels we simply pick the top-scoring sense for each token from the allowed set. Structured perceptron uses Viterbi decoding. Here we set the emission probabilities for disallowed senses to negative infinity and decode as usual. 2.2 Weakly supervised HMMs The HMM2 model is a second-order hidden Markov model (Mari et al., 1997; Thede and Harper, 1999) using logistic regression to estimate emission probabilities. In addition we constrain 2 Figure 1: HMM2 with continuous word representations the inference space of the HMM2 tagger using type-level tag constraints derived from WordNet, leading to roughly the model proposed by Li et al. (2012), who used Wiktionary as a (part-ofspeech) tag dictionary. The basic feature model of Li et al. (2012) is augmented with continuous word representation features as shown in Figure 1, and our logistic regression model thus works over a combination of discrete and continuous variable</context>
</contexts>
<marker>Mari, Haton, Kriouile, 1997</marker>
<rawString>Jean-Francois Mari, Jean-Paul Haton, and Abdelaziz Kriouile. 1997. Automatic word recognition based on second-order hidden Markov models. IEEE Transactions on Speech and Audio Processing, 5(1):22–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Senselearner: Word sense disambiguation for all words in unrestricted text.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>53--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17632" citStr="Mihalcea and Csomai, 2005" startWordPosition="2817" endWordPosition="2820">nse (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the LAPOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged SEMCOR.3 3.3 Model parameters We use the feature model of Paaß and Reichartz (2009) in all our models, except the weakly supervised models. For the structured perceptron we set the number of passes over the training data on the held-out development data. The weakly supervised models use the default set</context>
</contexts>
<marker>Mihalcea, Csomai, 2005</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2005. Senselearner: Word sense disambiguation for all words in unrestricted text. In Proceedings of the ACL 2005 on Interactive poster and demonstration sessions, pages 53–56. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="13268" citStr="Mikolov et al., 2013" startWordPosition="2077" endWordPosition="2080">n is used not only in the baselines, but also as a feature in the discriminative models. We use the WordNet 3.0 distribution. SEMCOR (Miller et al., 1994) is a senseannotated corpus composed of 80% newswire and 20% literary text, using the sense inventory from WordNet. SEMCOR comprises 23k distinct lemmas in 234k instances. We use the texts which have full annotations, leaving aside the verb-only texts (see Section 6). We use a distributional semantic model in order to incorporate distributional information as features in our system. In particular, we use the neural-network based models from (Mikolov et al., 2013), also referred as word embeddings. This model makes use of skip-grams (n-grams that do not need to be consecutive) within a word window to calculate continuous-valued vector representations from a recurrent neural network. These distributional models have been able to outperform state of the art in the SemEval-2012 Task 2 (Measuring degrees of relational similarity). We calculate the embeddings from an in-house corpus of 57m English tweets using a window size 5 and yielding vectors of 100 dimensions. We also use the first 20k tweets of the 57m tweets to train our HMM2 models. 3.1.2 Annotation</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Martin Chodorow</author>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Robert G Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop on Human Language Technology,</booktitle>
<pages>240--243</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2940" citStr="Miller et al., 1994" startWordPosition="453" endWordPosition="456"> or simply express the opinions of the author on some subject matter. Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE. If someone posts a message saying that some LaTeX module now supports “drawing trees”, it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., SEMCOR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the follo</context>
<context position="12801" citStr="Miller et al., 1994" startWordPosition="2002" endWordPosition="2005">|t1) w1 w2 w3 3 we use predicted POS tags as input to the system, in order to produce a realistic estimate of SST performance. 3.1 Data Our experiments rely on combinations of available resources and newly annotated Twitter data sets made publicly available with this paper. 3.1.1 Available resources Princeton WordNet (Fellbaum, 1998) is the main resource for SST. The lexicographer file names provide the label alphabet of the task, and the taxonomy defined therein is used not only in the baselines, but also as a feature in the discriminative models. We use the WordNet 3.0 distribution. SEMCOR (Miller et al., 1994) is a senseannotated corpus composed of 80% newswire and 20% literary text, using the sense inventory from WordNet. SEMCOR comprises 23k distinct lemmas in 234k instances. We use the texts which have full annotations, leaving aside the verb-only texts (see Section 6). We use a distributional semantic model in order to incorporate distributional information as features in our system. In particular, we use the neural-network based models from (Mikolov et al., 2013), also referred as word embeddings. This model makes use of skip-grams (n-grams that do not need to be consecutive) within a word win</context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>George A. Miller, Martin Chodorow, Shari Landes, Claudia Leacock, and Robert G. Thomas. 1994. Using a semantic concordance for sense identification. In Proceedings of the workshop on Human Language Technology, pages 240–243. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Paaß</author>
<author>Frank Reichartz</author>
</authors>
<title>Exploiting semantic constraints for estimating supersenses with CRFs.</title>
<date>2009</date>
<booktitle>In Proc. of the Ninth SIAM International Conference on Data Mining,</booktitle>
<pages>485--496</pages>
<location>Sparks, Nevada,</location>
<contexts>
<context position="18012" citStr="Paaß and Reichartz (2009)" startWordPosition="2875" endWordPosition="2878">he training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged SEMCOR.3 3.3 Model parameters We use the feature model of Paaß and Reichartz (2009) in all our models, except the weakly supervised models. For the structured perceptron we set the number of passes over the training data on the held-out development data. The weakly supervised models use the default setting proposed in Li et al. (2012). We have used the standard online setup for SEARN, which only takes one pass over the data. The type of embedding is the same in all our experiments. For a given word the embedding feature is a 100 dimensional vector, which combines the embedding of the word with the embedding of adjacent words. The feature combination fe for a word wt is calcu</context>
<context position="20245" citStr="Paaß and Reichartz, 2009" startWordPosition="3237" endWordPosition="3240">ted with type constraints as distant supervision, and without for comparison. SEARN without embeddings or distant supervision serves as an in-house baseline. In Table 3 we present the WordNet token coverage of predicted nouns and verbs in the development and evaluation data, as well as the interannotator agreement F1 scores. All the results presented in Table 2 are (weighted averaged) F1 measures obtained on predicted POS tags. Note that these results are considerably lower than results on supersense tagging newswire (up to 80 F1) that assume gold standard POS tags (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009). The re-implementation of the state-of-the-art system improves slightly upon the most frequent sense baseline. SenseLearner does not seem to capture the relevant information and does not reach baseline performance. In other words, there is no off-the-shelf tool for supersense tagging of Twitter that does much better than assigning the most frequent sense to predicted nouns and verbs. Our weakly supervised model performs worse than the most frequent sense baseline. This is a negative result. It is, however, well-known from the word sense disambiguation literature that the MFS is a very strong </context>
<context position="27011" citStr="Paaß and Reichartz, 2009" startWordPosition="4364" endWordPosition="4367">t of our knowledge, all of it has been limited to English newswire and literature (SEMCOR and SENSEVAL). The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on SEMCOR via 5-fold cross validation. Their evaluation included a held-out development set on each fold that was used to estimate the number of epochs. They used additional training data containing only verbs. More importantly, they relied on gold standard POS tags. Their overall F1 score on SEMCOR was 77.1. Reichartz and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2</context>
</contexts>
<marker>Paaß, Reichartz, 2009</marker>
<rawString>Gerhard Paaß and Frank Reichartz. 2009. Exploiting semantic constraints for estimating supersenses with CRFs. In Proc. of the Ninth SIAM International Conference on Data Mining, pages 485–496, Sparks, Nevada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Dirk Hovy</author>
<author>Anders Søgaard</author>
</authors>
<title>Learning part-of-speech taggers with inter-annotator agreement loss.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="14863" citStr="Plank et al., 2014" startWordPosition="2341" endWordPosition="2344">supersense annotations for the POS+NER-annotated data set described in Ritter et al. (2011), which we use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets RITTER{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, respectively. The IN-HOUSE-EVAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data sets, we carried out an annotation task. We first pre-annotated all data sets with WordNet’s most frequent senses. If the word was not in WordNet and a noun, we assigned it the sense n.person. All other words were labeled O. Chains of nouns were altered to give every element the sense of the head noun, and the BI tags adjusted, i.e.: Empire/B-n.loc State/B-n.loc Building/B-n.artifact was changed to Empire/B-n.artifact</context>
</contexts>
<marker>Plank, Hovy, Søgaard, 2014</marker>
<rawString>Barbara Plank, Dirk Hovy, and Anders Søgaard. 2014. Learning part-of-speech taggers with inter-annotator agreement loss. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Reichartz</author>
<author>Gerhard Paaß</author>
</authors>
<title>Estimating Supersenses with Conditional Random Fields.</title>
<date>2008</date>
<booktitle>In Proceedings of ECMLPKDD.</booktitle>
<contexts>
<context position="26984" citStr="Reichartz and Paaß, 2008" startWordPosition="4359" endWordPosition="4363">se tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (SEMCOR and SENSEVAL). The task of supersense tagging was first introduced by Ciaramita and Altun (2006), who used a structured perceptron trained and evaluated on SEMCOR via 5-fold cross validation. Their evaluation included a held-out development set on each fold that was used to estimate the number of epochs. They used additional training data containing only verbs. More importantly, they relied on gold standard POS tags. Their overall F1 score on SEMCOR was 77.1. Reichartz and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hov</context>
</contexts>
<marker>Reichartz, Paaß, 2008</marker>
<rawString>Frank Reichartz and Gerhard Paaß. 2008. Estimating Supersenses with Conditional Random Fields. In Proceedings of ECMLPKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="3289" citStr="Ritter et al., 2011" startWordPosition="513" endWordPosition="516">awing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., SEMCOR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • SEMCOR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twitte</context>
<context position="14335" citStr="Ritter et al. (2011)" startWordPosition="2258" endWordPosition="2261">ing a window size 5 and yielding vectors of 100 dimensions. We also use the first 20k tweets of the 57m tweets to train our HMM2 models. 3.1.2 Annotation While an annotated newswire corpus and a highquality lexical resource already enable us to train, we also need at least a small sample of annotated tweets data to evaluate SST for Twitter. Furthermore, if we want to experiment with supervised SST, we also need sufficient annotated Twitter data to learn the distribution of sense tags. This paper presents two data sets: (a) supersense annotations for the POS+NER-annotated data set described in Ritter et al. (2011), which we use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets RITTER{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, respectively. The IN-HOUSE-EVAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersense</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: an experimental study. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Rosenblatt</author>
</authors>
<title>The perceptron: a probabilistic model for information storage and organization in the brain.</title>
<date>1958</date>
<journal>Psychological Review,</journal>
<volume>65</volume>
<issue>6</issue>
<contexts>
<context position="10158" citStr="Rosenblatt, 1958" startWordPosition="1553" endWordPosition="1554">sequential labeling, structured perceptron (Collins, 2002) and search-based structured prediction (SEARN) (Daume et al., 2009). The structured perceptron is a in-house reimplementation of Ciaramita and Altun (2006).1 SEARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 1https://github.com/coastalcph/ rungsted 2.3.1 Structured perceptron (HMM) Structured perceptron learning was introduced in Collins (2002) and is an extension of the online perceptron learning algorithm (Rosenblatt, 1958) with averaging (Freund and Schapire, 1999) to structured learning problems such as sequence labeling. In structured perceptron for sequential labeling, where we learn a function from sequences of data points x1 ... xn to sequences of labels y1 ... yn, we begin with a random weight vector w0 initialized to all zeros. This weight vector is used to assign weights to transitions between labels, i.e., the discriminative counterpart of P(yi+1 |yi), and emissions of tokens given labels, i.e., the counterpart of P(xi |yi). We use Viterbi decoding to derive a best path yˆ through the corresponding mxn</context>
</contexts>
<marker>Rosenblatt, 1958</marker>
<rawString>Frank Rosenblatt. 1958. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Schneider</author>
<author>Behrang Mohit</author>
<author>Kemal Oflazer</author>
<author>Noah A Smith</author>
</authors>
<title>Coarse lexical semantic annotation with supersenses: an arabic case study.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>253--258</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27920" citStr="Schneider et al. (2012)" startWordPosition="4508" endWordPosition="4511">rovide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from SENSEEVAL-2 and SENSEEVAL-3. They use a classification approach rather than structured prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this p</context>
</contexts>
<marker>Schneider, Mohit, Oflazer, Smith, 2012</marker>
<rawString>Nathan Schneider, Behrang Mohit, Kemal Oflazer, and Noah A Smith. 2012. Coarse lexical semantic annotation with supersenses: an arabic case study. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253– 258. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Thede</author>
<author>Mary Harper</author>
</authors>
<title>A second-order hidden Markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8125" citStr="Thede and Harper, 1999" startWordPosition="1236" endWordPosition="1239">ordNet, no filtering was applied. We refer to the distantsupervision strategy as type constraints. Distant supervision was implemented differently in SEARN and the HMM model. SEARN decomposes sequential labelling into a series of binary classifications. To constrain the labels we simply pick the top-scoring sense for each token from the allowed set. Structured perceptron uses Viterbi decoding. Here we set the emission probabilities for disallowed senses to negative infinity and decode as usual. 2.2 Weakly supervised HMMs The HMM2 model is a second-order hidden Markov model (Mari et al., 1997; Thede and Harper, 1999) using logistic regression to estimate emission probabilities. In addition we constrain 2 Figure 1: HMM2 with continuous word representations the inference space of the HMM2 tagger using type-level tag constraints derived from WordNet, leading to roughly the model proposed by Li et al. (2012), who used Wiktionary as a (part-ofspeech) tag dictionary. The basic feature model of Li et al. (2012) is augmented with continuous word representation features as shown in Figure 1, and our logistic regression model thus works over a combination of discrete and continuous variables when estimating emissio</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott Thede and Mary Harper. 1999. A second-order hidden Markov model for part-of-speech tagging. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>Isi: automatic classification of relations between nominals using a maximum entropy classifier.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>222--225</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27591" citStr="Tratz and Hovy, 2010" startWordPosition="4458" endWordPosition="4461">nd Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNe</context>
</contexts>
<marker>Tratz, Hovy, 2010</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2010. Isi: automatic classification of relations between nominals using a maximum entropy classifier. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 222–225. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Learning with lookahead: can history-based models rival globally optimized models? In CoNLL.</title>
<date>2011</date>
<contexts>
<context position="17289" citStr="Tsuruoka et al., 2011" startWordPosition="2755" endWordPosition="2758">te this, we set one of the annotators as gold data and the other as predicted data. However, since F1 is symmetrical, the order does not matter. The annotation F1 gives us another estimate of annotation difficulty. We present the figures in Table 3. 3.2 Baselines For most word sense disambiguation studies, predicting the most frequent sense (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the LAPOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reach</context>
</contexts>
<marker>Tsuruoka, Miyao, Kazama, 2011</marker>
<rawString>Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Kazama. 2011. Learning with lookahead: can history-based models rival globally optimized models? In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Elena Mukomel</author>
<author>Anatole Gershman</author>
</authors>
<title>Cross-lingual metaphor detection using common semantic features.</title>
<date>2013</date>
<booktitle>Meta4NLP 2013,</booktitle>
<pages>45</pages>
<contexts>
<context position="27615" citStr="Tsvetkov et al., 2013" startWordPosition="4462" endWordPosition="4465">d Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) a</context>
</contexts>
<marker>Tsvetkov, Mukomel, Gershman, 2013</marker>
<rawString>Yulia Tsvetkov, Elena Mukomel, and Anatole Gershman. 2013. Cross-lingual metaphor detection using common semantic features. Meta4NLP 2013, page 45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Nathan Schneider</author>
<author>Dirk Hovy</author>
<author>Archna Bhatia</author>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Augmenting english adjective senses with supersenses.</title>
<date>2014</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="1431" citStr="Tsvetkov et al. (2014)" startWordPosition="204" endWordPosition="207">n from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.). Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names. Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available </context>
<context position="27738" citStr="Tsvetkov et al. (2014)" startWordPosition="4481" endWordPosition="4484">ain relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words</context>
</contexts>
<marker>Tsvetkov, Schneider, Hovy, Bhatia, Faruqui, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna Bhatia, Manaal Faruqui, and Chris Dyer. 2014. Augmenting english adjective senses with supersenses. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ye</author>
<author>Timothy Baldwin</author>
</authors>
<title>Melb-yb: Preposition sense disambiguation using rich semantic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>241--244</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="27569" citStr="Ye and Baldwin, 2007" startWordPosition="4453" endWordPosition="4457"> and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers</context>
</contexts>
<marker>Ye, Baldwin, 2007</marker>
<rawString>Patrick Ye and Timothy Baldwin. 2007. Melb-yb: Preposition sense disambiguation using rich semantic features. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 241–244. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Mehmet Yatbaz</author>
</authors>
<title>The noisy channel model for unsupervised word sense disambiguation.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<pages>36--111</pages>
<contexts>
<context position="21938" citStr="Yuret and Yatbaz (2010)" startWordPosition="3493" endWordPosition="3496"> we see huge improvements from using type constraints as distant supervision. In the supervised setup, we only see significant improvements adding type constraints for the structured perceptron (HMM), but not for search-based structured prediction (SEARN). For all the data sets, there is still a gap between model performance and human inter-annotator agreement levels (see Table 3), leaving some room for improvements. We hope that the release of the data sets will help further research into this. 4.1 Coarse-grained evaluation We also experimented with the more coarsegrained classes proposed by Yuret and Yatbaz (2010). Here our best model obtained an F1 score for mental concepts (nouns) of 72.3%, and 62.6% for physical concepts, on RITTER-DEV. The overall F1 score for verbs is 85.6%. The overall F1 is 75.5%. Note that this result is not directly comparable to the figure (72.9%) reported in Yuret and Yatbaz (2010), since they use different data sets, exclude verbs and make different assumptions, e.g., relying on gold POS tags. 5 Error analysis We have seen that inter-annotator agreements on supersense annotation are reliable at above .60 but far from perfect. The Hinton diagram in Table 2 presents the confu</context>
<context position="28471" citStr="Yuret and Yatbaz (2010)" startWordPosition="4591" endWordPosition="4594"> been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from SENSEEVAL-2 and SENSEEVAL-3. They use a classification approach rather than structured prediction. Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on SENSEVAL-2 and SENSEVAL-3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existin</context>
</contexts>
<marker>Yuret, Yatbaz, 2010</marker>
<rawString>Deniz Yuret and Mehmet Yatbaz. 2010. The noisy channel model for unsupervised word sense disambiguation. Computational Linguistics, 36:111–127.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>