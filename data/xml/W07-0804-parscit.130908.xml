<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.814384">
Arabic Cross-Document Person Name Normalization
</title>
<author confidence="0.899301">
Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan
</author>
<affiliation confidence="0.7329725">
Human Language Technologies Group
IBM Cairo Technology Development Center
</affiliation>
<address confidence="0.83211">
P.O. Box 166 El-Ahram, Giza, Egypt
</address>
<email confidence="0.986993">
{wmagdy, darwishk, emam, hanyh}@eg.ibm.com
</email>
<sectionHeader confidence="0.994549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964857142857">
This paper presents a machine learning
approach based on an SVM classifier
coupled with preprocessing rules for cross-
document named entity normalization. The
classifier uses lexical, orthographic,
phonetic, and morphological features. The
process involves disambiguating different
entities with shared name mentions and
normalizing identical entities with different
name mentions. In evaluating the quality of
the clusters, the reported approach achieves
a cluster F-measure of 0.93. The approach
is significantly better than the two baseline
approaches in which none of the entities are
normalized or entities with exact name
mentions are normalized. The two baseline
approaches achieve cluster F-measures of
0.62 and 0.74 respectively. The classifier
properly normalizes the vast majority of
entities that are misnormalized by the
baseline system.
</bodyText>
<sectionHeader confidence="0.980508" genericHeader="keywords">
1. Introduction:
</sectionHeader>
<bodyText confidence="0.999804703703704">
Much recent attention has focused on the
extraction of salient information from unstructured
text. One of the enabling technologies for
information extraction is Named Entity
Recognition (NER), which is concerned with
identifying the names of persons, organizations,
locations, expressions of times, quantities, ... etc.
(Chinchor, 1999; Maynard et al., 2001; Sekine,
2004; Joachims, 2002). The NER task is
challenging due to the ambiguity of natural
language and to the lack of uniformity in writing
styles and vocabulary used across documents
(Solorio, 2004).
Beyond NER, considerable work has focused
on the tracking and normalization of entities that
could be mentioned using different names (e.g.
George Bush, Bush) or nominals (e.g. the
president, Mr., the son) (Florian et al., 2004).
Most of the named entity tracking work has
focused on intra-document normalization with
very limited work on cross-documents
normalization.
Recognizing and tracking entities of type
“Person Name” are particularly important for
information extraction. Yet they pose interesting
challenges that require special attention. The
problems can result from:
</bodyText>
<listItem confidence="0.712585">
1. A Person’s name having many variant spellings
(especially when it is transliterated into a
foreign language). These variations are
typically limited in the same document, but are
very common across different documents from
different sources (e.g. Mahmoud Abbas =
Mahmod Abas, Mohamed El-Baradei =
Muhammad AlBaradey ... etc).
2. A person having more than one name (e.g.
Mahmoud Abbas = Abu Mazen).
3. Some names having very similar or identical
names but refer to completely different persons
(George H. W. Bush ≠ George W. Bush).
4. Single token names (e.g. Bill Clinton = Clinton
≠ Hillary Clinton).
</listItem>
<bodyText confidence="0.9996215">
This paper will focus on Arabic cross-document
normalization of named entities of type “person
name,” which would involve resolving the
aforementioned problems. As illustrated in Figure
1, the task involves normalizing a set of person
entities into a set of classes each of which is
</bodyText>
<page confidence="0.983476">
25
</page>
<note confidence="0.9222925">
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25–32,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<figure confidence="0.9056425">
E3
E5
E7
E1
E2
E4 E6
E8
E1
E4 E8
E7 E3
E5
E2
E6
Normalization
</figure>
<figureCaption confidence="0.995652">
Figure 1 Normalization Model
</figureCaption>
<bodyText confidence="0.9994919375">
formed of at least one entity. For N input entities,
the output of normalization process will be M
classes, where M ≤ N. Each class would refer to
only one person and each class would contain all
entities referring to that person.
For this work, intra-document normalization is
assumed and an entity refers to a normalized set of
name mentions and nominals referring to a single
person in a single document. Florian et al. (2004)
were kind enough to provide the authors access to
an updated version of their state-of-the-art Named
Entity Recognition and Tracking (NERT) system,
which achieves an F-measure of 0.77 for NER,
and an F-measure of 0.88 for intra-document
normalization assuming perfect NER. Although
the NERT systems is efficient for relatively short
documents, it is computational impractical for
large documents, which precludes using the NERT
system for cross-document normalization through
combining the documents into one large
document. The main challenges of this work stem
from large variations in the spelling of
transliterated foreign names and the presence of
many common Arabic names (such as
Muhammad, Abdullah, Ahmed ...etc.), which
increases the ambiguity in identifying the person
referred to by the mentioned name. Further, the
NERT system output system contains many NER
errors and intra-document normalization errors.
In this paper, cross-document normalization
system employs a two-step approach. In the first
step, preprocessing rules are used to remove errant
named entities. In the second step, a support
vector machine (SVM) classifier is used to
determine if two entities from two different
documents need to be normalized. The classifier
is trained on lexical, orthographic, phonetic, and
morphological features.
The paper is organized as follows: Section 2
provides a background on cross-document NE
normalization; Section 3 describes the
preprocessing steps and data used for training and
testing; Section 4 describes the normalization
methodology; Section 5 describes the
experimental setup; Section 6 reports and
discusses experimental results; and Section 7
concludes the paper and provides possible future
directions.
</bodyText>
<sectionHeader confidence="0.989964" genericHeader="introduction">
2. Background
</sectionHeader>
<bodyText confidence="0.999903482758621">
While considerable work has focused on named
entity normalization within a single document,
little work has focused on the challenges
associated with resolving person name references
across multiple documents. Most of the work
done in cross-document normalization focused on
the problem of determining if two instances with
the same name from different documents referring
to the same person (Fleischman and Hovy, 2004).
Fleischman and Hovy (2004) focused on
distinguishing between individuals having
identical names, but they did not extend
normalization to different names referring to the
same individual. Their task is a subtask of what is
examined in this paper. They used a large number
of features to accomplish their work, depending
mostly on language specific dictionaries and
wordnet. Some these resources are not available
for Arabic and many other languages. Mann and
Yarowsky (Mann and Yarowsky, 2003) examined
the same problem but they treated it as a clustering
task. They focused on information extraction to
build biographical profiles (date of birth, place of
birth, etc.), and they wanted to disambiguate
biographies belonging to different authors with
identical names.
Dozier and Zielund (Dozier and Zielund, 2004)
reported on cross-document person name
normalization in the legal domain. They used a
</bodyText>
<page confidence="0.992251">
26
</page>
<bodyText confidence="0.999857461538462">
finite state machine that identifies paragraphs in a
document containing the names of attorneys,
judges, or experts and a semantic parser that
extracts from the paragraphs template information
about each named individual. They relied on
reliable biographies for each individual. A
biography would typically contain a person’s first
name, middle name, last name, firm, city, state,
court, and other information. They used a
Bayesian network to match the name mentions to
the biographical records.
Bhattacharya and Getoor (Bhattacharya and
Getoor, 2006) introduced a collective decision
algorithm for author name entity resolution, where
decisions are not considered on an independent
pairwise basis. They focused on using relational
links among the references and co-author
relationships to infer collaboration groups, which
would disambiguate entity names. Such explicit
links between co-authors can be extracted directly.
However, implicit links can be useful when
looking at completely unstructured text. Other
work has extended beyond entities of type “person
name” to include the normalization of location
names (Li et al., 2002) and organizations (Ji and
Grishman. 2004).
</bodyText>
<listItem confidence="0.802822">
3. Preprocessing and the Data Set
</listItem>
<bodyText confidence="0.9955765">
For this work, a set of 7,184 person name entities
was constructed. Building new training and test
sets is warranted, because the task at hand is
sufficiently different from previously reported
tasks in the literature. The entities were
recognized from 2,931 topically related documents
(relating to the situation in the Gaza and Lebanon
during July of 2006) from different Arabic news
</bodyText>
<figureCaption confidence="0.917432">
Figure 2 Entity Description
</figureCaption>
<bodyText confidence="0.999881">
sources (obtained from searching the Arabic
version of news.google.com). The entities were
recognized and normalized (within document)
using the NERT system of Florian et al (2004).
As shown in Figure 2, each entity is composed of
a set of name mentions (one or more) and a set of
nominal mentions (zero or more).
The NERT system achieves an F-score of 0.77
with precision of 0.82 and recall of 0.73 for person
name mention and nominal recognition and an F-
score of 0.88 for tracking (assuming 100%
recognition accuracy). The produced entities may
suffer from the following:
</bodyText>
<listItem confidence="0.96851535">
1. Errant name mentions: Two name mentions
referring to two different entities are
concatenated into an errant name mention (e.g.
“Bush Blair”, “Ahmadinejad Bush”). These
types of errors stem from phrases such as “The
meeting of Bush Blair” and generally due to
lack of sufficient punctuation marks.
2. NE misrecognitions: Regular words are
recognized as person name mentions and are
embedded into person entities (e.g. Bush =
George Bush = said).
3. Errant entity tracking: name mentions of
different entities are recognized as different
mentions of the same entity (e.g. Bush =
Clinton = Ahmadinejad).
4. Lack of nominal mentions: Many entities do
not contain any nominal mentions, which
increases the entity ambiguity (especially
when there is only one name mention
composed of a single token).
</listItem>
<bodyText confidence="0.998754">
To overcome these problems, entities were
preprocessed as follows:
1. Errant name mentions such as “Bush Blair”
were automatically removed. In this step, a
dictionary of person name mentions was built
from the 2,931 documents collection from
which the entities were recognized and
normalized along with the frequency of
appearance in the collection. For each entity,
all its name mentions are checked in the
dictionary and their frequencies are compared
to each other. Any name mention with a
frequency less than 1/30 of the frequency of
the name mention with the highest frequency
is automatically removed (1/30 was picked
based on manual examination of the training
set).
</bodyText>
<page confidence="0.977434">
27
</page>
<listItem confidence="0.9812672">
2. Name mentions formed of a single token
consisting of less than 3 characters are
removed. Such names are almost always
misrecognized name entities.
3. Name entities with 10 or more different name
</listItem>
<bodyText confidence="0.948386857142857">
mentions are automatically removed. The
NERT system often produces entities that
include many different name mentions
referring to different persons as one. Such
entities are errant because they over normalize
name mentions. Persons are referred to using
a limited number of name mentions.
</bodyText>
<listItem confidence="0.9254544">
4. Nominal mentions are stemmed using a
context sensitive Arabic stemmer (Lee et al.
2003) to overcome the morphological
complexity of Arabic. For example, “c r��ر” =
“president”, “c rw.�►” = “the president”,
</listItem>
<bodyText confidence="0.73160025">
“c rw��►و” = “and the president”, “Li—Aر” = “its
presidents” ... etc are stemmed to “L41&apos;
41&apos; =
“president”.
</bodyText>
<sectionHeader confidence="0.434373" genericHeader="method">
2
</sectionHeader>
<bodyText confidence="0.998871">
Entity pairs were chosen to be included in the
training set if they match any of the following
criteria:
</bodyText>
<listItem confidence="0.98209075">
1. Both entities have one shared name mention.
2. Both entities have shared nominal mentions.
3. A name mention in one of the entities is a
substring of a name mention in the other
entity.
4. Both entities have nearly identical name
mentions (small edit distance between both
mentions).
</listItem>
<bodyText confidence="0.999984379310345">
The resulting set was composed of 19,825
pairs, which were manually judged to determine if
they should be normalized or not. These criteria
skew the selection of pairs towards more
ambiguous cases, which would be better
candidates to train the intended SVM classifier,
where the items near the boundary dividing the
hyperplane are the most important. For the
training set, 18,503 pairs were normalized, and
1,322 pairs were judged as different.
Unfortunately, the training set selection criteria
skewed the distribution of training examples
heavily in favor of positive examples. It would
interesting to examine other training sets where
the distribution of positives and negatives is
balanced or skewed in favor of negatives.
The test set was composed of 470 entities that
were manually normalized into 253 classes, of
which 304 entities were normalized to 87 classes
and 166 entities remained unnormalized (forming
single-entity classes). Using 470 entities leads to
110,215 pairwise comparisons. The test set, which
was distinct from the training set, was chosen
using the same criteria as the training set. Further,
all duplicate (identical) entities were removed
from the test set. The selection criteria insure that
the test set is skewed more towards ambiguous
cases. Randomly choosing entities would have
made the normalization too easy.
</bodyText>
<sectionHeader confidence="0.986117" genericHeader="method">
4. Normalization Methodology
</sectionHeader>
<bodyText confidence="0.99273375">
SVMLight, an SVM classifier (Joachims, 2002),
was used for classification with a linear kernel and
default parameters. The following training
features were employed:
</bodyText>
<figure confidence="0.817414">
1. The percentage of shared name mentions
between two entities calculated as:
Name Commonality =
� min J li A
&lt;common names&gt; f1 j∑f2 j
</figure>
<bodyText confidence="0.858113541666667">
where f1i is the frequency of the shared name
mention in first entity, and f2i is the frequency
of the shared name mention in the second
entity. ∑ f1i is the number of name mentions
appearing in the entity.
2. The maximum number of tokens in the shared
name mentions, i.e. if there exists more than
one shared name mention then this feature is
the number of tokens in the longest shared
name mention.
3. The percentage of shared nominal mentions
between two entities, and it is calculated as the
name commonality but for nominal mentions.
4. The smallest minimum edit distance
(Levenshtein distance with uniform weights)
between any two name mentions in both
entities (Cohen et al., 2003) and this feature is
only enabled when name commonality
between both entities equals to zero.
Cross-document entities are compared in a
pairwise manner and binary decision is taken on
whether they are the same. Therefore, the
available 7,184 entities lead to nearly 26 million
pairwise comparisons (For N entities, the number
</bodyText>
<table confidence="0.641435">
N ( N − 1) ).
of pair wise comparisons =
�
�
�
�
</table>
<page confidence="0.975968">
28
</page>
<listItem confidence="0.809390909090909">
5. Phonetic edit distance, which is similar to edit
distance except that phonetically similar
characters, namely {(ت – t, ط – T), (J – k, ق –
q),(د – d, ض – D),(ث – v, س – s, ص – S), (ذ – *,
ز – z, ظ – Z),(ج – j, غ – g),(`L – p, `L – h),(إ – &lt;,
1 – |, أ – &gt;, ا – A)1}, are normalized, vowels are
removed, and spaces between tokens are
removed.
6. The number of tokens in the pair of name
mentions that lead to the minimum edit
distance.
</listItem>
<bodyText confidence="0.999972636363636">
Some of the features might seem duplicative.
However, the edit distance and phonetic edit
distance are often necessary when names are
transliterated into Arabic and hence may have
different spellings and consequently no shared
name mentions. Conversely, given a shared name
mention between a pair of entities will lead to zero
edit distance, but the name commonality may also
be very low indicating two different persons may
have a shared name mention. For example
“Abdullah the second” and “Abdullah bin
Hussein” have the shared name mention
“Abdullah” that leads to zero edit distance, but
they are in fact two different persons. In this case,
the name commonality feature can be indicative of
the difference. Further, nominals are important in
differentiating between identical name mentions
that in fact refer to different persons (Fleischman
and Hovy, 2004). The number of tokens feature
indicates the importance of the presence of
similarity between two name mentions, as the
similarity between name mentions formed of one
token cannot be indicative for similarity when the
number of tokens is more than one.
Further, it is assumed that entities are transitive
and are not available all at once, but rather the
system has to normalize entities incrementally as
they appear. Therefore, for a given set of entity
pairs, if the classifier deems that Entityi = Entityj
and Entityj = Entityk, then Entityi is set to equal
Entityk even if the classifier indicates that Entityi ≠
Entityk, and all entities (i, j, and k) are merged into
one class.
</bodyText>
<footnote confidence="0.6156475">
1 Buckwalter transliteration scheme is used throughout
the paper
</footnote>
<sectionHeader confidence="0.983121" genericHeader="method">
5. Experimental Setup
</sectionHeader>
<bodyText confidence="0.999943875">
Two baselines were established for the
normalization process. In the first, no entities are
normalized, which produces single entity classes
(“no normalization” condition). In the second, any
two entities having two identical name mentions in
common are normalized (“surface normalization”
condition). For the rest of the experiments, focus
was given to two main issues:
</bodyText>
<listItem confidence="0.980253">
1. Determining the effect of the different features
used for classification.
2. Determining the effect of varying the number
of training examples.
</listItem>
<bodyText confidence="0.993780666666667">
To determine the effect of different features,
multiple classifiers were trained using different
features, namely:
</bodyText>
<listItem confidence="0.9990722">
• All features: all the features mentioned above
are used,
• Edit distance removed: edit distance features
(features 4, 5, and 6) are removed,
• Number of tokens per name mention removed:
</listItem>
<bodyText confidence="0.948697551724138">
the number of shared tokens and the number
of tokens leading to the least edit distance
(features 2 and 6) are removed.
To determine the effect of training examples,
the classifier was trained using all features but
with a varying number of training example pairs,
namely all 19,825 pairs, a set of randomly picked
5,000 pairs, and a set of randomly picked 2,000
pairs.
For evaluation, 470 entities in test set were
normalized into set of classes with different
thresholds for the SVM classifier. The quality of
the clusters was evaluated using purity, entropy,
and Cluster F-measure (CF-measure) in the
manner suggested by Rosell et al. (2004). For the
cluster quality measures, given cluster i (formed
using automatic normalization) and each cluster j
(reference normalization formed manually), cluster
precision (p) and recall (r) are computed as
follows:
r = , where ni number of
ij n
n j
ij
entities in cluster i, nj number of entities in cluster
j, and nij number of shared entities between cluster
i and j.
The CF-measure for an automatic cluster i
against a manually formed reference cluster j is:
</bodyText>
<equation confidence="0.8155128">
n ij
p = , and
ij
n
i
</equation>
<page confidence="0.988519">
29
</page>
<bodyText confidence="0.710906857142857">
, and the CF-measure for a
ij
reference cluster j is:
.
The final CF-measure is computed over all the
n
reference clusters as follows: CF ij CF
</bodyText>
<equation confidence="0.905940666666667">
= ∑ j .
j
n
</equation>
<bodyText confidence="0.9992878">
Purity of (ρi) of an automatically produced
cluster i is the maximum cluster precision obtained
when comparing it with all the reference clusters
as follows: ρi = maxj {pij } , and the weighted
average purity over all clusters is:
</bodyText>
<equation confidence="0.866446">
n ij
ρ = ∑where n is the total number of
i i
ni
</equation>
<bodyText confidence="0.991266666666667">
entities in the set to be normalized (470 in this
case).
As for entropy of a cluster, it is calculated as:
</bodyText>
<equation confidence="0.8104846">
Ei = −∑j pij log pij , and the average entropy
as:
n
E = ∑ i i E .
i ni
</equation>
<bodyText confidence="0.999807333333333">
The CF-measure captures both precision and
recall while purity and entropy are precision
oriented measures (Rosell et al., 2004).
</bodyText>
<sectionHeader confidence="0.997615" genericHeader="evaluation">
6. Results and Discussion
</sectionHeader>
<bodyText confidence="0.999152826086957">
Figure 3 shows the purity and CF-measure for the
two baseline conditions (no normalization, and
surface normalization) and for the normalization
system with different SVM thresholds. Since
purity is a precision measure, purity is 100% when
no normalization is done. The CF-measure is 62%
and 74% for baseline runs with no normalization
and surface normalization respectively. As can be
seen from the results, the baseline run based on
exact matching of name mentions in entities
achieves low CF-measure and low purity. Low
CF-measure values stem from the inability to
match identical entities with different name
mentions, and the low purity value stems from not
disambiguating different entities with shared name
mentions. Some notable examples where the
surface normalization baseline failed include:
1. The normalization of the different entities
referring to the Israeli soldier who is
imprisoned in Gaza with different Arabic
spellings for his name, namely “41-1 د64”
(jlEAd $lyT), “4U د64” (jlEAd $AlyT),
“c:4LI يsL-4ا” (the soldier $lyt), and so forth.
</bodyText>
<listItem confidence="0.684308166666667">
2. The separation between “W=ا ilا sc J���ا”
(King Abdullah the Second) and “ w ill ala JW
y�elا .)--” (King Abdullah ibn Abdul-Aziz)
that have a shared name mention “,اs�c JlAlا”
(King Abdullah).
3. The normalization of the different entities
</listItem>
<bodyText confidence="0.995666526315789">
representing the president of Palestinian
Authority with different name mentions,
namely “نزU gaff” (Abu Mazen) and “ د����
س���” (Mahmoud Abbas).
The proposed normalization technique
properly normalized the aforementioned examples.
Given different SVM thresholds, Figure 3 shows
that the purity of resultant classes increases as the
SVM threshold increases since the number of
normalized entities decreases as the threshold
increases. The best CF-measure of 93.1% is
obtained at a threshold of 1.4 and as show in Table
1 the corresponding purity and entropy are 97.2%
and 0.056 respectively. The results confirm the
success of the approach.
Table 1 highlights the effect of removing
different training feature and the highest CF-
measures (at different SVM thresholds) as a result.
The table shows that using all 6 features produced
the best results and the removal of the shared
names and tokens (features 2 and 6) had the most
adverse effect on normalization effectiveness. The
adverse effect is reasonable especially given that
some single token names such as “Muhammad”
and “Abdullah” are very common and matching
one of these names across entities is an insufficient
indicator that they are the same. Meanwhile, the
exclusion of edit distance features (features 4, 5,
and 6) had a lesser but significant adverse impact
on normalization effectiveness. Table 1 reports
the best results obtained using different thresholds.
Perhaps, a separate development set should be
used for ascertaining the best threshold.
Table 2 shows that decreasing the number of
training examples (all six features are used) has a
noticeable but less pronounced effect on
normalization effectiveness compared to removing
training features.
</bodyText>
<figure confidence="0.968223555555555">
CFij
rij + p
⋅
2
rij
⋅ pij
CFj =
maxi {CFij
}
</figure>
<page confidence="0.986146">
30
</page>
<tableCaption confidence="0.976708333333333">
Table 1 Quality of clusters as measured by purity (higher values are better), entropy (lower values are
better), and CF-measure (higher values are better) for different feature sets. Values are shown for max
CF-measure. Thresholds were tuned for max CF-measure for each feature configuration separately
</tableCaption>
<table confidence="0.99971125">
Training Data Purity Entropy Threshold
Maximum
CF-Measure
No Normalization 100.0% 62.6% 0.000 -
Baseline 83.4% 74.7% 0.151 -
All Features 97.2% 93.1% 0.056 1.4
Edit Distance removed 99.4% 85.5% 0.010 1.0
# of tokens/name removed 96.6% 77.8% 0.071 1.5
</table>
<tableCaption confidence="0.760362">
Figure 3 Purity and cluster F-measure versus SVM Threshold
Table 2 Effect of number of training examples on normalization effectiveness
</tableCaption>
<table confidence="0.992447833333333">
Training Data Purity Entropy Threshold
Maximum
CF-Measure
20k training pairs 97.2% 93.1% 0.056 1.4
5k training pairs 97.4% 90.5% 0.053 1.5
2k training pairs 98.5% 90.3% 0.031 1.6
</table>
<figure confidence="0.993864666666667">
100%
95%
90%
85%
80%
75%
70%
65%
60%
No
Normalization
Baseline
1.0
1.1
1.2
Normalization Evaluation
SVM Threshold
1.3
1.4
1.5
1.6
1.7
1.8
Purity
CF-Measure
1.9
2.0
</figure>
<sectionHeader confidence="0.872865" genericHeader="conclusions">
7. Conclusion:
</sectionHeader>
<bodyText confidence="0.9999749375">
This paper presented a two-step approach to cross-
document named entity normalization. In the first
step, preprocessing rules are used to remove errant
named entities. In the second step, a machine
learning approach based on an SVM classifier to
disambiguate different entities with matching
name mentions and to normalize identical entities
with different name mentions. The classifier was
trained on features that capture name mentions and
nominals overlap between entities, edit distance,
and phonetic similarity. In evaluating the quality
of the clusters, the reported approach achieved a
cluster F-measure of 0.93. The approach
outperformed that two baseline approaches in
which no normalization was done or normalization
was done when two entities had matching name
</bodyText>
<page confidence="0.999581">
31
</page>
<bodyText confidence="0.999969380952381">
mentions. The two approaches achieved cluster F-
measures of 0.62 and 0.74 respectively.
For future work, implicit links between entities
in the text can serve as the relational links that
would enable the use of entity attributes in
conjunction with relationships between entities.
An important problem that has not been
sufficiently explored is cross-lingual cross-
document normalization. This problem would
pose unique and interesting challenges. The
described approach could be generalized to
perform normalization of entities of different types
across multilingual documents. Also, the
normalization problem was treated as a
classification problem. Examining the problem as
a clustering (or alternatively an incremental
clustering) problem might prove useful. Lastly,
the effect of cross-document normalization should
be examined on applications such as information
extraction, information retrieval, and relationship
and social network visualization.
</bodyText>
<sectionHeader confidence="0.996447" genericHeader="references">
References:
</sectionHeader>
<reference confidence="0.999935121621622">
Bhattacharya I. and Getoor L. “A Latent Dirichlet
Allocation Model for Entity Resolution.” 6th SIAM
Conference on Data Mining (SDM), Bethesda, USA,
April 2006.
Chinchor N., Brown E., Ferro L., and Robinson P.
“Named Entity Recognition Task Definition.”
MITRE, 1999.
Cohen W., Ravikumar P., and Fienberg S. E. “A
Comparison of String Distance Metrics for Name-
Matching Tasks.” In Proceedings of the
International Joint Conference on Artificial
Intelligence, 2003.
Dozier C. and Zielund T. “Cross-document Co-
Reference Resolution Applications for People in the
Legal Domain.” In 42nd Annual Meeting of the
Association for Computational Linguistics,
Reference Resolution Workshop, Barcelona, Spain.
July 2004.
Fleischman M. B. and Hovy E. “Multi-Document
Person Name Resolution.” In 42nd Annual Meeting
of the Association for Computational Linguistics,
Reference Resolution Workshop, Barcelona, Spain.
July 2004.
Ji H. and Grishman R. “Applying Coreference to
Improve Name Recognition”. In 42nd Annual
Meeting of the Association for Computational
Linguistics, Reference Resolution Workshop,
Barcelona, Spain. July (2004).
Ji H. and Grishman R. &amp;quot;Improving Name Tagging by
Reference Resolution and Relation Detection.&amp;quot; ACL
2005
Joachims T. “Learning to Classify Text Using Support
Vector Machines.” Ph.D. Dissertation, Kluwer,
(2002).
Joachims T. “Optimizing Search Engines Using Click-
through Data.” Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining (KDD),
(2002).
Lee Y. S., Papineni K., Roukos S., Emam O., Hassan
H. “Language Model Based Arabic Word
Segmentation.” In ACL 2003, pp. 399-406, (2003).
Li H., Srihari R. K., Niu C., and Li W. “Location
Normalization for Information Extraction.”
Proceedings of the 19th international conference on
Computational linguistics, pp. 1-7, 2002
Li H., Srihari R. K., Niu C., and Li W. “Location
Normalization for Information Extraction.”
Proceedings of the sixth conference on applied
natural language processing, 2000. pp. 247 – 254.
Mann G. S. and Yarowsky D. “Unsupervised Personal
Name Disambiguation.” Proceedings of the seventh
conference on Natural language learning at HLT-
NAACL 2003. pp. 33-40.
Maynard D., Tablan V., Ursu C., Cunningham H., and
Wilks Y. “Named Entity Recognition from Diverse
Text Types.” Recent Advances in Natural Language
Processing Conference, (2001).
Palmer D. D. and Day D. S. “A statistical Profile of the
Named Entity Task”. Proceedings of the fifth
conference on Applied natural language processing,
pp. 190-193, (1997).
R. Florian R., Hassan H., Ittycheriah A., Jing H.,
Kambhatla N., Luo X., Nicolov N., and Roukos S.
“A Statistical Model for Multilingual Entity
Detection and Tracking.” In HLT-NAACL, 2004.
Rosell M., Kann V., and Litton J. E. “Comparing
Comparisons: Document Clustering Evaluation
Using Two Manual Classifications.” In ICON 2004
Sekine S. “Named Entity: History and Future”. Project
notes, New York University, (2004).
Solorio T. “Improvement of Named Entity Tagging by
Machine Learning.” Ph.D. thesis, National Institute
of Astrophysics, Optics and Electronics, Puebla,
Mexico, September 2005.
</reference>
<page confidence="0.9993">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341697">
<title confidence="0.946543333333333">Arabic Cross-Document Person Name Normalization Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan Human Language Technologies</title>
<affiliation confidence="0.439782">IBM Cairo Technology Development</affiliation>
<address confidence="0.892116">P.O. Box 166 El-Ahram, Giza, Egypt</address>
<email confidence="0.998728">wmagdy@eg.ibm.com</email>
<email confidence="0.998728">darwishk@eg.ibm.com</email>
<email confidence="0.998728">emam@eg.ibm.com</email>
<email confidence="0.998728">hanyh@eg.ibm.com</email>
<abstract confidence="0.999773">This paper presents a machine learning approach based on an SVM classifier coupled with preprocessing rules for crossdocument named entity normalization. The classifier uses lexical, orthographic, phonetic, and morphological features. The process involves disambiguating different entities with shared name mentions and normalizing identical entities with different name mentions. In evaluating the quality of the clusters, the reported approach achieves a cluster F-measure of 0.93. The approach is significantly better than the two baseline approaches in which none of the entities are normalized or entities with exact name mentions are normalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Bhattacharya</author>
<author>L Getoor</author>
</authors>
<title>A Latent Dirichlet Allocation Model for Entity Resolution.”</title>
<date>2006</date>
<booktitle>6th SIAM Conference on Data Mining (SDM),</booktitle>
<location>Bethesda, USA,</location>
<contexts>
<context position="7471" citStr="Bhattacharya and Getoor, 2006" startWordPosition="1111" endWordPosition="1114">n cross-document person name normalization in the legal domain. They used a 26 finite state machine that identifies paragraphs in a document containing the names of attorneys, judges, or experts and a semantic parser that extracts from the paragraphs template information about each named individual. They relied on reliable biographies for each individual. A biography would typically contain a person’s first name, middle name, last name, firm, city, state, court, and other information. They used a Bayesian network to match the name mentions to the biographical records. Bhattacharya and Getoor (Bhattacharya and Getoor, 2006) introduced a collective decision algorithm for author name entity resolution, where decisions are not considered on an independent pairwise basis. They focused on using relational links among the references and co-author relationships to infer collaboration groups, which would disambiguate entity names. Such explicit links between co-authors can be extracted directly. However, implicit links can be useful when looking at completely unstructured text. Other work has extended beyond entities of type “person name” to include the normalization of location names (Li et al., 2002) and organizations</context>
</contexts>
<marker>Bhattacharya, Getoor, 2006</marker>
<rawString>Bhattacharya I. and Getoor L. “A Latent Dirichlet Allocation Model for Entity Resolution.” 6th SIAM Conference on Data Mining (SDM), Bethesda, USA, April 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>E Brown</author>
<author>L Ferro</author>
<author>P Robinson</author>
</authors>
<title>Named Entity Recognition Task Definition.” MITRE,</title>
<date>1999</date>
<marker>Chinchor, Brown, Ferro, Robinson, 1999</marker>
<rawString>Chinchor N., Brown E., Ferro L., and Robinson P. “Named Entity Recognition Task Definition.” MITRE, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
<author>P Ravikumar</author>
<author>S E Fienberg</author>
</authors>
<title>A Comparison of String Distance Metrics for NameMatching Tasks.”</title>
<date>2003</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<contexts>
<context position="14158" citStr="Cohen et al., 2003" startWordPosition="2175" endWordPosition="2178">, and f2i is the frequency of the shared name mention in the second entity. ∑ f1i is the number of name mentions appearing in the entity. 2. The maximum number of tokens in the shared name mentions, i.e. if there exists more than one shared name mention then this feature is the number of tokens in the longest shared name mention. 3. The percentage of shared nominal mentions between two entities, and it is calculated as the name commonality but for nominal mentions. 4. The smallest minimum edit distance (Levenshtein distance with uniform weights) between any two name mentions in both entities (Cohen et al., 2003) and this feature is only enabled when name commonality between both entities equals to zero. Cross-document entities are compared in a pairwise manner and binary decision is taken on whether they are the same. Therefore, the available 7,184 entities lead to nearly 26 million pairwise comparisons (For N entities, the number N ( N − 1) ). of pair wise comparisons = � � � � 28 5. Phonetic edit distance, which is similar to edit distance except that phonetically similar characters, namely {(ت – t, ط – T), (J – k, ق – q),(د – d, ض – D),(ث – v, س – s, ص – S), (ذ – *, ز – z, ظ – Z),(ج – j, غ – g),(`</context>
</contexts>
<marker>Cohen, Ravikumar, Fienberg, 2003</marker>
<rawString>Cohen W., Ravikumar P., and Fienberg S. E. “A Comparison of String Distance Metrics for NameMatching Tasks.” In Proceedings of the International Joint Conference on Artificial Intelligence, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dozier</author>
<author>T Zielund</author>
</authors>
<title>Cross-document CoReference Resolution Applications for People in the Legal Domain.”</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6830" citStr="Dozier and Zielund, 2004" startWordPosition="1016" endWordPosition="1019">Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extraction to build biographical profiles (date of birth, place of birth, etc.), and they wanted to disambiguate biographies belonging to different authors with identical names. Dozier and Zielund (Dozier and Zielund, 2004) reported on cross-document person name normalization in the legal domain. They used a 26 finite state machine that identifies paragraphs in a document containing the names of attorneys, judges, or experts and a semantic parser that extracts from the paragraphs template information about each named individual. They relied on reliable biographies for each individual. A biography would typically contain a person’s first name, middle name, last name, firm, city, state, court, and other information. They used a Bayesian network to match the name mentions to the biographical records. Bhattacharya a</context>
</contexts>
<marker>Dozier, Zielund, 2004</marker>
<rawString>Dozier C. and Zielund T. “Cross-document CoReference Resolution Applications for People in the Legal Domain.” In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop, Barcelona, Spain. July 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Fleischman</author>
<author>E Hovy</author>
</authors>
<title>Multi-Document Person Name Resolution.”</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6015" citStr="Fleischman and Hovy, 2004" startWordPosition="892" endWordPosition="895">zation methodology; Section 5 describes the experimental setup; Section 6 reports and discusses experimental results; and Section 7 concludes the paper and provides possible future directions. 2. Background While considerable work has focused on named entity normalization within a single document, little work has focused on the challenges associated with resolving person name references across multiple documents. Most of the work done in cross-document normalization focused on the problem of determining if two instances with the same name from different documents referring to the same person (Fleischman and Hovy, 2004). Fleischman and Hovy (2004) focused on distinguishing between individuals having identical names, but they did not extend normalization to different names referring to the same individual. Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extracti</context>
<context position="15835" citStr="Fleischman and Hovy, 2004" startWordPosition="2477" endWordPosition="2480">ions. Conversely, given a shared name mention between a pair of entities will lead to zero edit distance, but the name commonality may also be very low indicating two different persons may have a shared name mention. For example “Abdullah the second” and “Abdullah bin Hussein” have the shared name mention “Abdullah” that leads to zero edit distance, but they are in fact two different persons. In this case, the name commonality feature can be indicative of the difference. Further, nominals are important in differentiating between identical name mentions that in fact refer to different persons (Fleischman and Hovy, 2004). The number of tokens feature indicates the importance of the presence of similarity between two name mentions, as the similarity between name mentions formed of one token cannot be indicative for similarity when the number of tokens is more than one. Further, it is assumed that entities are transitive and are not available all at once, but rather the system has to normalize entities incrementally as they appear. Therefore, for a given set of entity pairs, if the classifier deems that Entityi = Entityj and Entityj = Entityk, then Entityi is set to equal Entityk even if the classifier indicate</context>
</contexts>
<marker>Fleischman, Hovy, 2004</marker>
<rawString>Fleischman M. B. and Hovy E. “Multi-Document Person Name Resolution.” In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop, Barcelona, Spain. July 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Applying Coreference to Improve Name Recognition”.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop,</booktitle>
<location>Barcelona,</location>
<marker>Ji, Grishman, 2004</marker>
<rawString>Ji H. and Grishman R. “Applying Coreference to Improve Name Recognition”. In 42nd Annual Meeting of the Association for Computational Linguistics, Reference Resolution Workshop, Barcelona, Spain. July (2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Improving Name Tagging by Reference Resolution and Relation Detection.&amp;quot;</title>
<date>2005</date>
<journal>ACL</journal>
<marker>Ji, Grishman, 2005</marker>
<rawString>Ji H. and Grishman R. &amp;quot;Improving Name Tagging by Reference Resolution and Relation Detection.&amp;quot; ACL 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to Classify Text Using Support Vector Machines.” Ph.D.</title>
<date>2002</date>
<publisher>Dissertation, Kluwer,</publisher>
<contexts>
<context position="1523" citStr="Joachims, 2002" startWordPosition="207" endWordPosition="208">ormalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system. 1. Introduction: Much recent attention has focused on the extraction of salient information from unstructured text. One of the enabling technologies for information extraction is Named Entity Recognition (NER), which is concerned with identifying the names of persons, organizations, locations, expressions of times, quantities, ... etc. (Chinchor, 1999; Maynard et al., 2001; Sekine, 2004; Joachims, 2002). The NER task is challenging due to the ambiguity of natural language and to the lack of uniformity in writing styles and vocabulary used across documents (Solorio, 2004). Beyond NER, considerable work has focused on the tracking and normalization of entities that could be mentioned using different names (e.g. George Bush, Bush) or nominals (e.g. the president, Mr., the son) (Florian et al., 2004). Most of the named entity tracking work has focused on intra-document normalization with very limited work on cross-documents normalization. Recognizing and tracking entities of type “Person Name” a</context>
<context position="13213" citStr="Joachims, 2002" startWordPosition="2016" endWordPosition="2017">253 classes, of which 304 entities were normalized to 87 classes and 166 entities remained unnormalized (forming single-entity classes). Using 470 entities leads to 110,215 pairwise comparisons. The test set, which was distinct from the training set, was chosen using the same criteria as the training set. Further, all duplicate (identical) entities were removed from the test set. The selection criteria insure that the test set is skewed more towards ambiguous cases. Randomly choosing entities would have made the normalization too easy. 4. Normalization Methodology SVMLight, an SVM classifier (Joachims, 2002), was used for classification with a linear kernel and default parameters. The following training features were employed: 1. The percentage of shared name mentions between two entities calculated as: Name Commonality = � min J li A &lt;common names&gt; f1 j∑f2 j where f1i is the frequency of the shared name mention in first entity, and f2i is the frequency of the shared name mention in the second entity. ∑ f1i is the number of name mentions appearing in the entity. 2. The maximum number of tokens in the shared name mentions, i.e. if there exists more than one shared name mention then this feature is</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims T. “Learning to Classify Text Using Support Vector Machines.” Ph.D. Dissertation, Kluwer, (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing Search Engines Using Clickthrough Data.”</title>
<date>2002</date>
<booktitle>Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<contexts>
<context position="1523" citStr="Joachims, 2002" startWordPosition="207" endWordPosition="208">ormalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system. 1. Introduction: Much recent attention has focused on the extraction of salient information from unstructured text. One of the enabling technologies for information extraction is Named Entity Recognition (NER), which is concerned with identifying the names of persons, organizations, locations, expressions of times, quantities, ... etc. (Chinchor, 1999; Maynard et al., 2001; Sekine, 2004; Joachims, 2002). The NER task is challenging due to the ambiguity of natural language and to the lack of uniformity in writing styles and vocabulary used across documents (Solorio, 2004). Beyond NER, considerable work has focused on the tracking and normalization of entities that could be mentioned using different names (e.g. George Bush, Bush) or nominals (e.g. the president, Mr., the son) (Florian et al., 2004). Most of the named entity tracking work has focused on intra-document normalization with very limited work on cross-documents normalization. Recognizing and tracking entities of type “Person Name” a</context>
<context position="13213" citStr="Joachims, 2002" startWordPosition="2016" endWordPosition="2017">253 classes, of which 304 entities were normalized to 87 classes and 166 entities remained unnormalized (forming single-entity classes). Using 470 entities leads to 110,215 pairwise comparisons. The test set, which was distinct from the training set, was chosen using the same criteria as the training set. Further, all duplicate (identical) entities were removed from the test set. The selection criteria insure that the test set is skewed more towards ambiguous cases. Randomly choosing entities would have made the normalization too easy. 4. Normalization Methodology SVMLight, an SVM classifier (Joachims, 2002), was used for classification with a linear kernel and default parameters. The following training features were employed: 1. The percentage of shared name mentions between two entities calculated as: Name Commonality = � min J li A &lt;common names&gt; f1 j∑f2 j where f1i is the frequency of the shared name mention in first entity, and f2i is the frequency of the shared name mention in the second entity. ∑ f1i is the number of name mentions appearing in the entity. 2. The maximum number of tokens in the shared name mentions, i.e. if there exists more than one shared name mention then this feature is</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims T. “Optimizing Search Engines Using Clickthrough Data.” Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Lee</author>
<author>K Papineni</author>
<author>S Roukos</author>
<author>O Emam</author>
<author>H Hassan</author>
</authors>
<title>Language Model Based Arabic Word Segmentation.”</title>
<date>2003</date>
<booktitle>In ACL 2003,</booktitle>
<pages>399--406</pages>
<contexts>
<context position="11172" citStr="Lee et al. 2003" startWordPosition="1692" endWordPosition="1695"> examination of the training set). 27 2. Name mentions formed of a single token consisting of less than 3 characters are removed. Such names are almost always misrecognized name entities. 3. Name entities with 10 or more different name mentions are automatically removed. The NERT system often produces entities that include many different name mentions referring to different persons as one. Such entities are errant because they over normalize name mentions. Persons are referred to using a limited number of name mentions. 4. Nominal mentions are stemmed using a context sensitive Arabic stemmer (Lee et al. 2003) to overcome the morphological complexity of Arabic. For example, “c r��ر” = “president”, “c rw.�►” = “the president”, “c rw��►و” = “and the president”, “Li—Aر” = “its presidents” ... etc are stemmed to “L41&apos; 41&apos; = “president”. 2 Entity pairs were chosen to be included in the training set if they match any of the following criteria: 1. Both entities have one shared name mention. 2. Both entities have shared nominal mentions. 3. A name mention in one of the entities is a substring of a name mention in the other entity. 4. Both entities have nearly identical name mentions (small edit distance be</context>
</contexts>
<marker>Lee, Papineni, Roukos, Emam, Hassan, 2003</marker>
<rawString>Lee Y. S., Papineni K., Roukos S., Emam O., Hassan H. “Language Model Based Arabic Word Segmentation.” In ACL 2003, pp. 399-406, (2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>R K Srihari</author>
<author>C Niu</author>
<author>W Li</author>
</authors>
<title>Location Normalization for Information Extraction.”</title>
<date>2002</date>
<booktitle>Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="8053" citStr="Li et al., 2002" startWordPosition="1194" endWordPosition="1197">or (Bhattacharya and Getoor, 2006) introduced a collective decision algorithm for author name entity resolution, where decisions are not considered on an independent pairwise basis. They focused on using relational links among the references and co-author relationships to infer collaboration groups, which would disambiguate entity names. Such explicit links between co-authors can be extracted directly. However, implicit links can be useful when looking at completely unstructured text. Other work has extended beyond entities of type “person name” to include the normalization of location names (Li et al., 2002) and organizations (Ji and Grishman. 2004). 3. Preprocessing and the Data Set For this work, a set of 7,184 person name entities was constructed. Building new training and test sets is warranted, because the task at hand is sufficiently different from previously reported tasks in the literature. The entities were recognized from 2,931 topically related documents (relating to the situation in the Gaza and Lebanon during July of 2006) from different Arabic news Figure 2 Entity Description sources (obtained from searching the Arabic version of news.google.com). The entities were recognized and no</context>
</contexts>
<marker>Li, Srihari, Niu, Li, 2002</marker>
<rawString>Li H., Srihari R. K., Niu C., and Li W. “Location Normalization for Information Extraction.” Proceedings of the 19th international conference on Computational linguistics, pp. 1-7, 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>R K Srihari</author>
<author>C Niu</author>
<author>W Li</author>
</authors>
<title>Location Normalization for Information Extraction.”</title>
<date>2000</date>
<booktitle>Proceedings of the sixth conference on applied natural language processing,</booktitle>
<pages>247--254</pages>
<marker>Li, Srihari, Niu, Li, 2000</marker>
<rawString>Li H., Srihari R. K., Niu C., and Li W. “Location Normalization for Information Extraction.” Proceedings of the sixth conference on applied natural language processing, 2000. pp. 247 – 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G S Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Personal Name Disambiguation.”</title>
<date>2003</date>
<booktitle>Proceedings of the seventh conference on Natural language learning at HLTNAACL</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6510" citStr="Mann and Yarowsky, 2003" startWordPosition="968" endWordPosition="971">determining if two instances with the same name from different documents referring to the same person (Fleischman and Hovy, 2004). Fleischman and Hovy (2004) focused on distinguishing between individuals having identical names, but they did not extend normalization to different names referring to the same individual. Their task is a subtask of what is examined in this paper. They used a large number of features to accomplish their work, depending mostly on language specific dictionaries and wordnet. Some these resources are not available for Arabic and many other languages. Mann and Yarowsky (Mann and Yarowsky, 2003) examined the same problem but they treated it as a clustering task. They focused on information extraction to build biographical profiles (date of birth, place of birth, etc.), and they wanted to disambiguate biographies belonging to different authors with identical names. Dozier and Zielund (Dozier and Zielund, 2004) reported on cross-document person name normalization in the legal domain. They used a 26 finite state machine that identifies paragraphs in a document containing the names of attorneys, judges, or experts and a semantic parser that extracts from the paragraphs template informati</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Mann G. S. and Yarowsky D. “Unsupervised Personal Name Disambiguation.” Proceedings of the seventh conference on Natural language learning at HLTNAACL 2003. pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Maynard</author>
<author>V Tablan</author>
<author>C Ursu</author>
<author>H Cunningham</author>
<author>Y Wilks</author>
</authors>
<title>Named Entity Recognition from</title>
<date>2001</date>
<booktitle>Diverse Text Types.” Recent Advances in Natural Language Processing Conference,</booktitle>
<contexts>
<context position="1492" citStr="Maynard et al., 2001" startWordPosition="201" endWordPosition="204">ities with exact name mentions are normalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system. 1. Introduction: Much recent attention has focused on the extraction of salient information from unstructured text. One of the enabling technologies for information extraction is Named Entity Recognition (NER), which is concerned with identifying the names of persons, organizations, locations, expressions of times, quantities, ... etc. (Chinchor, 1999; Maynard et al., 2001; Sekine, 2004; Joachims, 2002). The NER task is challenging due to the ambiguity of natural language and to the lack of uniformity in writing styles and vocabulary used across documents (Solorio, 2004). Beyond NER, considerable work has focused on the tracking and normalization of entities that could be mentioned using different names (e.g. George Bush, Bush) or nominals (e.g. the president, Mr., the son) (Florian et al., 2004). Most of the named entity tracking work has focused on intra-document normalization with very limited work on cross-documents normalization. Recognizing and tracking e</context>
</contexts>
<marker>Maynard, Tablan, Ursu, Cunningham, Wilks, 2001</marker>
<rawString>Maynard D., Tablan V., Ursu C., Cunningham H., and Wilks Y. “Named Entity Recognition from Diverse Text Types.” Recent Advances in Natural Language Processing Conference, (2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
<author>D S Day</author>
</authors>
<title>A statistical Profile of the Named Entity Task”.</title>
<date>1997</date>
<booktitle>Proceedings of the fifth conference on Applied natural language processing,</booktitle>
<pages>190--193</pages>
<marker>Palmer, Day, 1997</marker>
<rawString>Palmer D. D. and Day D. S. “A statistical Profile of the Named Entity Task”. Proceedings of the fifth conference on Applied natural language processing, pp. 190-193, (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian R</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A Statistical Model for Multilingual Entity Detection and Tracking.” In</title>
<date>2004</date>
<booktitle>HLT-NAACL,</booktitle>
<marker>R, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian R., Hassan H., Ittycheriah A., Jing H., Kambhatla N., Luo X., Nicolov N., and Roukos S. “A Statistical Model for Multilingual Entity Detection and Tracking.” In HLT-NAACL, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rosell</author>
<author>V Kann</author>
<author>J E Litton</author>
</authors>
<title>Comparing Comparisons: Document Clustering Evaluation Using Two Manual Classifications.” In</title>
<date>2004</date>
<booktitle>ICON</booktitle>
<contexts>
<context position="18071" citStr="Rosell et al. (2004)" startWordPosition="2835" endWordPosition="2838">ens and the number of tokens leading to the least edit distance (features 2 and 6) are removed. To determine the effect of training examples, the classifier was trained using all features but with a varying number of training example pairs, namely all 19,825 pairs, a set of randomly picked 5,000 pairs, and a set of randomly picked 2,000 pairs. For evaluation, 470 entities in test set were normalized into set of classes with different thresholds for the SVM classifier. The quality of the clusters was evaluated using purity, entropy, and Cluster F-measure (CF-measure) in the manner suggested by Rosell et al. (2004). For the cluster quality measures, given cluster i (formed using automatic normalization) and each cluster j (reference normalization formed manually), cluster precision (p) and recall (r) are computed as follows: r = , where ni number of ij n n j ij entities in cluster i, nj number of entities in cluster j, and nij number of shared entities between cluster i and j. The CF-measure for an automatic cluster i against a manually formed reference cluster j is: n ij p = , and ij n i 29 , and the CF-measure for a ij reference cluster j is: . The final CF-measure is computed over all the n reference</context>
<context position="19300" citStr="Rosell et al., 2004" startWordPosition="3074" endWordPosition="3077"> as follows: CF ij CF = ∑ j . j n Purity of (ρi) of an automatically produced cluster i is the maximum cluster precision obtained when comparing it with all the reference clusters as follows: ρi = maxj {pij } , and the weighted average purity over all clusters is: n ij ρ = ∑where n is the total number of i i ni entities in the set to be normalized (470 in this case). As for entropy of a cluster, it is calculated as: Ei = −∑j pij log pij , and the average entropy as: n E = ∑ i i E . i ni The CF-measure captures both precision and recall while purity and entropy are precision oriented measures (Rosell et al., 2004). 6. Results and Discussion Figure 3 shows the purity and CF-measure for the two baseline conditions (no normalization, and surface normalization) and for the normalization system with different SVM thresholds. Since purity is a precision measure, purity is 100% when no normalization is done. The CF-measure is 62% and 74% for baseline runs with no normalization and surface normalization respectively. As can be seen from the results, the baseline run based on exact matching of name mentions in entities achieves low CF-measure and low purity. Low CF-measure values stem from the inability to matc</context>
</contexts>
<marker>Rosell, Kann, Litton, 2004</marker>
<rawString>Rosell M., Kann V., and Litton J. E. “Comparing Comparisons: Document Clustering Evaluation Using Two Manual Classifications.” In ICON 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>Named Entity: History and Future”. Project notes,</title>
<date>2004</date>
<location>New York University,</location>
<contexts>
<context position="1506" citStr="Sekine, 2004" startWordPosition="205" endWordPosition="206">mentions are normalized. The two baseline approaches achieve cluster F-measures of 0.62 and 0.74 respectively. The classifier properly normalizes the vast majority of entities that are misnormalized by the baseline system. 1. Introduction: Much recent attention has focused on the extraction of salient information from unstructured text. One of the enabling technologies for information extraction is Named Entity Recognition (NER), which is concerned with identifying the names of persons, organizations, locations, expressions of times, quantities, ... etc. (Chinchor, 1999; Maynard et al., 2001; Sekine, 2004; Joachims, 2002). The NER task is challenging due to the ambiguity of natural language and to the lack of uniformity in writing styles and vocabulary used across documents (Solorio, 2004). Beyond NER, considerable work has focused on the tracking and normalization of entities that could be mentioned using different names (e.g. George Bush, Bush) or nominals (e.g. the president, Mr., the son) (Florian et al., 2004). Most of the named entity tracking work has focused on intra-document normalization with very limited work on cross-documents normalization. Recognizing and tracking entities of typ</context>
</contexts>
<marker>Sekine, 2004</marker>
<rawString>Sekine S. “Named Entity: History and Future”. Project notes, New York University, (2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
</authors>
<title>Improvement of Named Entity Tagging by</title>
<date>2005</date>
<booktitle>Machine Learning.” Ph.D. thesis, National Institute of Astrophysics, Optics and Electronics,</booktitle>
<location>Puebla, Mexico,</location>
<marker>Solorio, 2005</marker>
<rawString>Solorio T. “Improvement of Named Entity Tagging by Machine Learning.” Ph.D. thesis, National Institute of Astrophysics, Optics and Electronics, Puebla, Mexico, September 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>