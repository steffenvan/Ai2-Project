<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002529">
<title confidence="0.998626">
Stacked Ensembles of Information Extractors
for Knowledge-Base Population
</title>
<author confidence="0.992695">
Nazneen Fatema Rajani* Vidhoon Viswanathan* Yinon Bentor Raymond J. Mooney
</author>
<affiliation confidence="0.9987855">
Department of Computer Science
University of Texas at Austin
</affiliation>
<address confidence="0.728705">
Austin, TX 78712, USA
</address>
<email confidence="0.999673">
{nrajani,vidhoon,yinon,mooney}@cs.utexas.edu
</email>
<sectionHeader confidence="0.994821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949722222222">
We present results on using stacking to en-
semble multiple systems for the Knowl-
edge Base Population English Slot Fill-
ing (KBP-ESF) task. In addition to us-
ing the output and confidence of each sys-
tem as input to the stacked classifier, we
also use features capturing how well the
systems agree about the provenance of
the information they extract. We demon-
strate that our stacking approach outper-
forms the best system from the 2014 KBP-
ESF competition as well as alternative en-
sembling methods employed in the 2014
KBP Slot Filler Validation task and several
other ensembling baselines. Additionally,
we demonstrate that including provenance
information further increases the perfor-
mance of stacking.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995798416666667">
Using ensembles of multiple systems is a stan-
dard approach to improving accuracy in machine
learning (Dietterich, 2000). Ensembles have been
applied to a wide variety of problems in natural
language processing, including parsing (Hender-
son and Brill, 1999), word sense disambiguation
(Pedersen, 2000), and sentiment analysis (White-
head and Yaeger, 2010). This paper presents a de-
tailed study of ensembling methods for the TAC
Knowledge Base Population (KBP) English Slot
Filling (ESF) task (Surdeanu, 2013; Surdeanu and
Ji, 2014).
We demonstrate new state-of-the-art results on
this KBP task using stacking (Wolpert, 1992),
which trains a final classifier to optimally com-
bine the results of multiple systems. We present
results for stacking all systems that competed in
both the 2013 and 2014 KBP-ESF tracks, training
∗ These authors contributed equally
on 2013 data and testing on 2014 data. The re-
sulting stacked ensemble outperforms all systems
in the 2014 competition, obtaining an F1 of 48.6%
compared to 39.5% for the best performing system
in the most recent competition.
Although the associated KBP Slot Filler Val-
idation (SFV) Track (Wang et al., 2013; Yu et
al., 2014; Sammons et al., 2014) is officially fo-
cused on improving the precision of individual ex-
isting systems by filtering their results, frequently
participants in this track also combine the results
of multiple systems and also report increased re-
call through this use of ensembling. However,
SFV participants have not employed stacking, and
we demonstrate that our stacking approach out-
performs existing published SFV ensembling sys-
tems.
KBP ESF systems must also provide prove-
nance information, i.e. each extracted slot-filler
must include a pointer to a document passage that
supports it (Surdeanu and Ji, 2014). Some SFV
systems have used this provenance information to
help filter and combine extractions (Sammons et
al., 2014). Therefore, we also explored enhancing
our stacking approach by including additional in-
put features that capture provenance information.
By including features that quantify how much the
ensembled systems agree on provenance, we fur-
ther improved our F1 score for the 2014 ESF task
to 50.1%.
The remainder of the paper is organized as fol-
lows. Section 2 provides background information
on existing KBP-ESF systems and stacking. Sec-
tion 3 provides general background on the KBP-
ESF task. Section 4 describes our stacking ap-
proach, including how provenance information is
used. Section 5 presents comprehensive exper-
iments comparing this approach to existing re-
sults and several additional baselines, demonstrat-
ing new state-of-the-art results on KBP-ESF. Sec-
tion 6 reviews prior related work on ensembling
</bodyText>
<page confidence="0.964611">
177
</page>
<note confidence="0.977503">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 177–187,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999465">
tems and build a stacked meta-classifier that learns
to combine the results from individual slot filling
systems. We detail our stacking approach for en-
sembling existing slot filling systems in Section 4.
for information extraction. Section 7 presents our
final conclusions and proposed directions for fu-
ture research.
</bodyText>
<sectionHeader confidence="0.972756" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999972956521739">
For the past few years, NIST has been conducting
the English Slot Filling (ESF) Task in the Knowl-
edge Base Population (KBP) track among various
other tasks as a part of the Text Analysis Con-
ference(TAC)(Surdeanu, 2013; Surdeanu and Ji,
2014). In the ESF task, the goal is to fill spe-
cific slots of information for a given set of query
entities (people or organizations) based on a sup-
plied text corpus. The participating systems em-
ploy a variety of techniques in different stages
of the slot filling pipeline, such as entity search,
relevant document extraction, relation modeling
and inference. In 2014, the top performing sys-
tem, DeepDive with Expert Advice from Stanford
University (Wazalwar et al., 2014), employed dis-
tant supervision (Mintz et al., 2009) and Markov
Logic Networks (Domingos et al., 2008) in their
learning and inferencing system. Another system,
RPI BLENDER (Hong et al., 2014), used a re-
stricted fuzzy matching technique in a framework
that learned event triggers and employed them to
extract relations from documents.
Given the diverse set of slot-filling systems
available, it is interesting to explore methods for
ensembling these systems. In this regard, TAC
also conducts a Slot Filler Validation (SFV) task
who goal is to improve the slot-filling performance
using the output of existing systems. The input
for this task is the set of outputs from all slot-
filling systems and the expected output is a filtered
set of slot fills. As with the ESF task, partici-
pating systems employ a variety of techniques to
perform validation. For instance, RPI BLENDER
used a Multi-dimensional Truth Finding model
(Yu et al., 2014) which is an unsupervised vali-
dation approach based on computing multidimen-
sional credibility scores. The UI CCG system
(Sammons et al., 2014) developed two different
validation systems using entailment and majority
voting.
However, stacking (Sigletos et al., 2005;
Wolpert, 1992) has not previously been employed
for ensembling KBP-ESF systems. In stacking, a
meta-classifier is learned from the output of multi-
ple underlying systems. In our work, we translate
this to the context of ensembling slot filling sys-
</bodyText>
<sectionHeader confidence="0.659216" genericHeader="method">
3 Overview of KBP Slot Filling Task
</sectionHeader>
<bodyText confidence="0.9998907">
The goal of the TAC KBP-ESF task (Surdeanu,
2013; Surdeanu and Ji, 2014) is to collect infor-
mation (fills) about specific attributes (slots) for a
set of entities (queries) from a given corpus. The
queries vary in each year of the task and can be
either a person (PER) or an organization (ORG)
entity. The slots are fixed and are listed in Ta-
ble 1 by entity type. Some slots (like per:age) are
single-valued while others (like per:children) are
list-valued i.e., they can take multiple slot fillers.
</bodyText>
<subsectionHeader confidence="0.994845">
3.1 Input and Output
</subsectionHeader>
<bodyText confidence="0.999565515151515">
The input for the task is a set of queries and the
corpus in which to look for information. The
queries are provided in an XML format containing
basic information including an ID for the query,
the name of the entity, and the type of entity (PER
or ORG). The corpus consists of documents for-
mat from discussion forums, newswire and the In-
ternet. Each document is identified by a unique
document ID.
The output for the task is a set of slot fills for
each input query. Depending on the type, each
query should have a NIL or one or more lines of
output for each of the corresponding slots. The
output line for each slot fill contains the fields
shown in Table 2. The query ID in Column 1
should match the ID of the query given as input.
The slot name (Column 2) is one of the slots listed
in Table 1 based on entity type. Run ID (Column
3) is a unique identifier for each system. Column
4 contains a NIL filler if the system could not find
any relevant slot filler. Otherwise, it contains the
relation provenance. Provenance is of the form
docid:startoffset-endoffset, where docid specifies
a source document from the corpus and the offsets
demarcate the text in this document supporting the
relation. The offsets correspond to the spans of
the candidate document that describe the relation
between the query entity and the extracted slot
filler. Column 5 contains the extracted slot filler.
Column 6 is a filler provenance that is similar in
format to relation provenance but in this case the
offset corresponds to the portion of the document
containing the extracted filler. Column 7 is a confi-
</bodyText>
<page confidence="0.996867">
178
</page>
<table confidence="0.9792425">
Person Organization
per:alternate names per:cause of death org:country of headquarters org:founded by
per:date of birth per:countries of residence org:stateorprovince of headquarters org:date dissolved
per:age per:statesorprovinces of residence org:city of headquarters org:website
per:parents per:cities of residence org:shareholders org:date founded
per:spouse per:schools attended org:top members employees org:members
per:city of birth per:city of death org:political religious affiliation org:member of
per:origin per:stateorprovince of death org:number of employees members org:subsidiaries
per:other family per:country of death org:alternate names org:parents
per:title per:employee or member of
per:religion per:stateorprovince of birth
per:children per:country of birth
per:siblings per:date of death
per:charges
</table>
<tableCaption confidence="0.997915">
Table 1: Slots for PER and ORG queries
</tableCaption>
<bodyText confidence="0.6432535">
dence score which systems can provide to indicate
their certainty in the extracted information.
</bodyText>
<subsectionHeader confidence="0.999533">
3.2 Scoring
</subsectionHeader>
<bodyText confidence="0.9999455">
The scoring for the ESF task is carried out as fol-
lows. The responses from all slot-filling systems
are pooled and a key file is generated by having
human assessors judge the correctness of these re-
sponses. In addition, LDC includes a manual key
of fillers that were determined by human judges.
Using the union of these keys as the gold standard,
precision, recall, and F1 scores are computed.
</bodyText>
<table confidence="0.808432">
Column Field Description
Column 1 Query ID
Column 2 Slot name
Column 3 Run ID
Column 4 NIL or Relation Provenance
Column 5 Slot filler
Column 6 Filler Provenance
Column 7 Confidence score
</table>
<tableCaption confidence="0.994464">
Table 2: SF Output line fields
</tableCaption>
<sectionHeader confidence="0.821098" genericHeader="method">
4 Ensembling Slot-Filling Systems
</sectionHeader>
<bodyText confidence="0.999984818181818">
Given a set of query entities and a fixed set of slots,
the goal of ensembling is to effectively combine
the output of different slot-filling systems. The in-
put to the ensembling system is the output of in-
dividual systems (in the format described in previ-
ous section) containing slot fillers and additional
information such as provenance and confidence
scores. The output of the ensembling system is
similar to the output of an individual system, but
it productively aggregates the slot fillers from dif-
ferent systems.
</bodyText>
<subsectionHeader confidence="0.996263">
4.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999845">
This section describes our ensembling approach
which trains a final binary classifier using features
that help judge the reliability and thus correctness
of individual slot fills. In a final post-processing
step, the slot fills that get classified as “correct” by
the classifier are kept while the others are set to
NIL.
</bodyText>
<subsectionHeader confidence="0.946804">
4.1.1 Stacking
</subsectionHeader>
<bodyText confidence="0.999778666666667">
Stacking is a popular ensembling method in ma-
chine learning (Wolpert, 1992) and has been suc-
cessfully used in many applications including the
top performing systems in the Netflix competition
(Sill et al., 2009). The idea is to employ multiple
learners and combine their predictions by training
a “meta-classifier” to weight and combine multi-
ple models using their confidence scores as fea-
tures. By training on a set of supervised data that
is disjoint from that used to train the individual
models, it learns how to combine their results into
an improved ensemble model. We employ a single
classifier to train and test on all slot types using an
L1-regularized SVM with a linear kernel (Fan et
al., 2008).
</bodyText>
<subsectionHeader confidence="0.767854">
4.1.2 Using Provenance
</subsectionHeader>
<bodyText confidence="0.999838636363636">
As discussed above, each system provides prove-
nance information for every non-NIL slot filler.
There are two kinds of provenance provided: the
relation provenance and the filler provenance. In
our algorithm, we only use the filler provenance
for a given slot fill. This is because of the changes
in the output formats for the ESF task from 2013 to
2014. Specifically, the 2013 specification requires
separate entity and justification provenance fields,
but the 2014 collapses these into a single relation
provenance field. An additional filler provenance
</bodyText>
<page confidence="0.993577">
179
</page>
<bodyText confidence="0.999847288888889">
field is common to both specifications. Hence,
we use the filler provenance that is common be-
tween 2013 and 2014 formats. As described ear-
lier, every provenance has a docid and startoffset-
endoffset that gives information about the docu-
ment and offset in the document from where the
slot fill has been extracted. The UI-CCG SFV sys-
tem Sammons et al. (2014) effectively used this
provenance information to help validate and filter
slot fillers. This motivated us to use provenance
in our stacking approach as additional features as
input to the meta-classifier.
We use provenance in two ways, first using
the docid information, and second using the off-
set information. We use the docids to define a
document-based provenance score in the follow-
ing way: for a given query and slot, if N sys-
tems provide answers and a maximum of n of
those systems give the same docid in their filler
provenance, then the document provenance score
for those n slot fills is n/N. Similarly, other slot
fills are given lower scores based on the fraction of
systems whose provenance document agree with
theirs. Since this provenance score is weighted
by the number of systems that refer to the same
provenance, it measures the reliability of a slot
fill based on the document from where it was ex-
tracted.
Our second provenance measure uses offsets.
The degree of overlap among the various systems’
offsets can also be a good indicator of the reliabil-
ity of the slot fill. The Jaccard similarity coeffi-
cient is a statistical measure of similarity between
sets and is thus useful in measuring the degree of
overlap among the offsets of systems. Slot fills
have variable lengths and thus the provenance off-
set ranges are variable too. A metric such as the
Jaccard coefficient captures the overlapping off-
sets along with normalizing based on the union
and thus resolving the problem with variable offset
ranges. For a given query and slot, if N systems
that attempt to fill it have the same docid in their
document provenance, then the offset provenance
(OP) score for a slot fill by a system x is calculated
as follows:
</bodyText>
<equation confidence="0.932102">
1 |offsets(i) n offsets(x)|
OP(x) = |N |&amp;quot; �|offsets(i) u offsets(x)|
i∈N,i�=x
</equation>
<bodyText confidence="0.999544466666667">
Per our definition, systems that extract slot fills
from different documents for the same query slot
have zero overlap among offsets. We note that the
offset provenance is always used along with the
document provenance and thus useful in discrim-
inating slot fills extracted from a different docu-
ment for the same query slot. Like the document
provenance score, the offset provenance score is
also a weighted feature and is a measure of relia-
bility of a slot fill based on the offsets in the docu-
ment from where it is extracted. Unlike past SFV
systems that use provenance for validation, our ap-
proach does not need access to the large corpus of
documents from where the slot fills are extracted
and is thus very computationally inexpensive.
</bodyText>
<subsectionHeader confidence="0.999457">
4.2 Eliminating Slot-Filler Aliases
</subsectionHeader>
<bodyText confidence="0.999972142857143">
When combining the output of different ESF sys-
tems, it is possible that some slot-filler entities
might overlap with each other. An ESF system
could extract a filler F1 for a slot S while another
ESF system extracts another filler F2 for the same
slot S. If the extracted fillers F1 and F2 are aliases
(i.e. different names for the same entity), the scor-
ing system for the TAC KBP SF task considers
them redundant and penalizes the precision of the
system.
In order to eliminate aliases from the output of
ensembled system, we employ a technique derived
by inverting the scheme used by the LSV ESF sys-
tem (Roth et al., 2013) for query expansion. LSV
ESF uses a Wikipedia anchor-text model (Roth
and Klakow, 2010) to generate aliases for given
query entities. By including aliases for query
names, the ESF system increase the number of
candidate sentences fetched for the query.
To eliminate filler aliases, we apply the same
technique to generate aliases for all slot fillers of
a given query and slot type. Given a slot filler,
we obtain the Wikipedia page that is most likely
linked to the filler text. Then, we obtain the anchor
texts and their respective counts from all other
Wikipedia pages that link to this page. Using these
counts, we choose top N (we use N=10 as in
LSV) and pick the corresponding anchor texts as
aliases for the given slot filler. Using the gener-
ated aliases, we then verify if any of the slot fillers
are redundant with respect to these aliases. This
scheme is not applicable to slot types whose fillers
are not entities (like date or age). Therefore, sim-
pler matching schemes are used to eliminate re-
dundancies for these slot types.
</bodyText>
<page confidence="0.977211">
180
</page>
<figure confidence="0.360516">
Common systems dataset All 2014 SFV systems dataset
</figure>
<figureCaption confidence="0.998091">
Figure 1: Precision-Recall curves for identifying the best voting performance on the two datasets
</figureCaption>
<sectionHeader confidence="0.996576" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9996984">
This section describes a comprehensive set of ex-
periments evaluating ensembling for the KBP ESF
task. Our experiments are divided into two sub-
sets based on the datasets they employ. Since
our stacking approach relies on 2013 SFV data
for training, we build a dataset of one run for ev-
ery team that participated in both the 2013 and
2014 competitions and call it the common systems
dataset. There are 10 common teams of the 17
teams that participated in ESF 2014. The other
dataset comprises of all 2014 SFV systems (in-
cluding all runs of all 17 teams that participated in
2014). There are 10 systems in the common sys-
tems dataset, while there are 65 systems in the all
2014 SFV dataset. Table 3 gives a list of the com-
mon systems for 2013 and 2014 ESF task. ESF
systems do change from year to year and it’s not a
perfect comparison, but systems generally get bet-
ter every year and thus we are probably only un-
derperforming.
</bodyText>
<figure confidence="0.827064636363636">
Common Systems
LSV
IIRG
UMass IESL
Stanford
BUPT PRIS
RPI BLENDER
CMUML
NYU
Compreno
UWashington
</figure>
<tableCaption confidence="0.998599">
Table 3: Common teams for 2013 and 2014 ESF
</tableCaption>
<subsectionHeader confidence="0.953997">
5.1 Methodology and Results
</subsectionHeader>
<bodyText confidence="0.999976228571429">
For our unsupervised ensembling baselines, we
evaluate on both the common systems dataset as
well as the entire 2014 SFV dataset. We compare
our stacking approach to three unsupervised base-
lines. The first is Union which takes the combina-
tion of values for all systems to maximize recall.
If the slot type is list-valued, it classifies all slot
fillers as correct and always includes them. If the
slot type is single-valued, if only one systems at-
tempts to answer it, then it includes that system’s
slot fill. Otherwise if multiple systems produce
a response, it only includes the slot fill with the
highest confidence value as correct and discards
the rest.
The second baseline is Voting. For this ap-
proach, we vary the threshold on the number of
systems that must agree on a slot fill from one
to all. This gradually changes the system from
the union to intersection of the slot fills, and we
identify the threshold that results in the highest
F1 score. We learn a threshold on the 2013 SFV
dataset (containing 52 systems) that results in the
best F1 score. We use this threshold for the voting
baseline on 2014 SFV dataset. As we did for the
2013 common systems dataset, we learn a thresh-
old on the 2013 common systems that results in the
best F1 score and use this threshold for the voting
baseline on 2014 common systems.
The third baseline is an “oracle threshold” ver-
sion of Voting. Since the best threshold for 2013
may not necessarily be the best threshold for 2014,
we identify the best threshold for 2014 by plot-
ting a Precision-Recall curve and finding the best
F1 score for the voting baseline on both the SFV
and common systems datasets. Figure 1 shows the
</bodyText>
<page confidence="0.997821">
181
</page>
<figureCaption confidence="0.956028">
Figure 2: Our system pipeline for evaluating supervised ensembling approaches
</figureCaption>
<table confidence="0.99986125">
Baseline Precision Recall F1
Union 0.067 0.762 0.122
Voting (threshold learned on 2013 data) 0.641 0.288 0.397
Voting (optimal threshold for 2014 data) 0.547 0.376 0.445
</table>
<tableCaption confidence="0.995269">
Table 4: Performance of baselines on all 2014 SFV dataset (65 systems)
</tableCaption>
<table confidence="0.9999631">
Approach Precision Recall F1
Union 0.176 0.647 0.277
Voting (threshold learned on 2013 data) 0.694 0.256 0.374
Best ESF system in 2014 (Stanford) 0.585 0.298 0.395
Voting (optimal threshold for 2014 data) 0.507 0.383 0.436
Stacking 0.606 0.402 0.483
Stacking + Relation 0.607 0.406 0.486
Stacking + Provenance (document) 0.499 0.486 0.492
Stacking + Provenance (document) + Relation 0.653 0.400 0.496
Stacking + Provenance (document and offset) + Relation 0.541 0.466 0.501
</table>
<tableCaption confidence="0.822573">
Table 5: Performance on the common systems dataset (10 systems) for various configurations. All
approaches except the Stanford system are our implementations.
</tableCaption>
<bodyText confidence="0.999508934782609">
Precision-Recall curve for two datasets for finding
the best possible F1 score using the voting base-
line. We find that for the common systems dataset,
a threshold of 3 (of 10) systems gives the best F1
score, while for the entire 2014 SFV dataset, a
threshold of 10 (of 65) systems gives the highest
F1. Note that this gives an upper bound on the
best results that can be achieved with voting, as-
suming an optimal threshold is chosen. Since the
upper bound can not be predicted without using
the 2014 dataset, this baseline has an unfair ad-
vantage. Table 4 shows the performance of all 3
baselines on the all 2014 SFV systems dataset.
For all our supervised ensembling approaches,
we train on the 2013 SFV data and test on the
2014 data for the common systems. We have
5 different supervised approaches. Our first ap-
proach is stacking the common systems using
their confidence scores to learn a classifier. As
discussed earlier, in stacking we train a meta-
classifier that combines the systems using their
confidence scores as features. Since the com-
mon systems dataset has 10 systems, this classifier
uses 10 features. The second approach also pro-
vides stacking with a nominal feature giving the
relation name (as listed in Table 1) for the given
slot instance. This allows the system to learn dif-
ferent evidence-combining functions for different
slot types if the classifier finds this useful. For
our third approach, we also provide the document
provenance feature described in Section 4.1. Al-
together this approach has 11 features (10 confi-
dence score + 1 document provenance score). The
fourth approach uses confidences, the document
provenance feature, and a one-hot encoding of the
relation name for the slot instance. Our final ap-
proach also includes the offset provenance (OP)
feature discussed in Section 4.1. There are alto-
gether 13 features in this approach. All our su-
pervised approaches use the Weka package (Hall
et al., 2009) for training the meta-classifier, using
an L1-regularized SVM with a linear kernel (other
classifiers gave similar results). Figure 2 shows
our system pipeline for evaluating supervised en-
sembling approaches. Table 5 gives the perfor-
mance of all our supervised approaches as well as
</bodyText>
<page confidence="0.996432">
182
</page>
<bodyText confidence="0.99954784">
our unsupervised baselines for the common sys-
tems dataset.
Analysis by Surdeanu and Ji (2014) suggests
that 2014 ESF queries are more difficult than those
for 2013. They compare two systems by running
both on 2013 and 2014 data and find there is a con-
siderable drop in the performance of both the sys-
tems. We note that they run the same exact system
on 2013 and 2014 data. Thus, in order to have a
better understanding of our results, we plot a learn-
ing curve by training on different sizes of the 2013
SFV data and using the scorer to measure the F1
score on the 2014 SFV data for the 10 common
systems. Figure 3 shows the learning curve thus
obtained. Although there are certain parts of the
dataset when the F1 score drops which we sus-
pect is due to overfitting the 2013 data, there is
still a strong correlation between the 2013 training
data size and F1 score on the 2014 dataset. Thus
we can infer that training on 2013 data is useful
even though the 2013 and 2014 data are fairly dif-
ferent. Although the queries change, the common
systems remain more-or-less the same and stack-
ing enables a meta-classifier to weigh those com-
mon systems based on their 2013 performance.
</bodyText>
<figureCaption confidence="0.734548">
Figure 3: Learning curve for training on 2013 and
testing on 2014 common systems dataset
</figureCaption>
<bodyText confidence="0.988350604166667">
To further validate our approach, we divide the
2013 SFV data based on the systems that extracted
those slot fills. Then we sort the systems, from
higher to lower, based on the number of false pos-
itives produced by them in the ensembling ap-
proach. Next we train a classifier in an incremen-
tal fashion adding one system’s slot fills for train-
ing at each step and analyzing the performance on
2014 data. This allows us to analyze the results
at the system level. Figure 4 shows the plot of
F1 score vs. the number of systems at each step.
The figure shows huge improvement in F1 score
at steps 6 and 7. At step 6 the Stanford system
is added to the pool of systems which is the best
performing ESF system in 2014 and fourth best
in 2013. At step 7, the UMass system is added
to the pool and, although the system on it own
is weak, it boosts the performance of our ensem-
bling approach. This is because the UMass system
alone contributes approximately 24% of the 2013
training data (Singh et al., 2013). Thus adding
this one system significantly improves the training
step leading to better performance. We also no-
tice that our system becomes less conservative at
this step and has higher recall. The reason for this
is that the systems from 1 to 5 had very high pre-
cision and low recall whereas from system 6 on-
wards the systems have high recall. Thus adding
the UMass system enables our meta-classifier to
have a higher recall for small decrease in precision
and thus boosting the overall F1 measure. With-
out it, the classifier produces high precision but
low recall and decreases the overall F1 score by
approximately 6 points.
Figure 4: Incrementally training on 2013 by
adding a system at each step and testing on 2014
common systems dataset
We also experimented with cross validation
within the 2014 dataset. Since we used only 2014
data for this experiment, we also included the rela-
tion provenance as discussed in Section 4.1.2. Ta-
ble 6 shows the results on 10-fold cross-validation
on 2014 data with only the filler provenance and
with both the filler and relation provenance. The
performance of using only the filler provenance is
slightly worse than training on 2013 because the
2014 SFV data has many fewer instances but uses
more systems for learning compared to the 2013
</bodyText>
<page confidence="0.997977">
183
</page>
<table confidence="0.996165">
Approach Precision Recall F1
Stacking + Filler provenance + Relation 0.606 0.415 0.493
Stacking + Filler and Relation provenance + Relation 0.609 0.434 0.506
</table>
<tableCaption confidence="0.935479">
Table 6: 10-fold Cross-Validation on 2014 SFV dataset (65 systems)
</tableCaption>
<table confidence="0.99996625">
Baseline Precision Recall F1
Union 0.054 0.877 0.101
Voting (threshold learned on 2013 data) 0.637 0.406 0.496
Voting (optimal threshold for 2014 data) 0.539 0.526 0.533
</table>
<tableCaption confidence="0.990452">
Table 7: Baseline performance on all 2014 SFV dataset (65 systems) using unofficial scorer
</tableCaption>
<table confidence="0.9999842">
Approach Precision Recall F1
Union 0.177 0.922 0.296
Voting (threshold learned on 2013 data) 0.694 0.256 0.374
Best published SFV result in 2014 (UIUC) 0.457 0.507 0.481
Voting (optimal threshold for 2014 data) 0.507 0.543 0.525
Stacking + Provenance(document) 0.498 0.688 0.578
Stacking 0.613 0.562 0.586
Stacking + Relation 0.613 0.567 0.589
Stacking + Provenance (document and offset) + Relation 0.541 0.661 0.595
Stacking + Provenance (document) + Relation 0.659 0.56 0.606
</table>
<tableCaption confidence="0.982928">
Table 8: Performance on the common systems dataset (10 systems) for various configurations using the
unofficial scorer. All approaches except the UIUC system are our implementations.
</tableCaption>
<bodyText confidence="0.989700307692308">
SFV data.
The TAC KBP official scoring key for the ESF
task includes human annotated slot fills along with
the pooled slot fills obtained by all participating
systems. However, Sammons et al. (2014) use
an unofficial scoring key in their paper that does
not include human annotated slot fills. In order
to compare to their results, we also present results
using the same unofficial key. Table 7 gives the
performance of our baseline systems on the 2014
SFV dataset using the unofficial key for scoring.
We note that our Union does not produce a recall
of 1.0 on the unofficial scorer due to our single-
valued slot selection strategy for multiple systems.
As discussed earlier for the single-valued slot, we
include the slot fill with highest confidence (which
may not necessarily be correct) and thus may not
match the unofficial scorer.
Table 8 gives the performance of all our super-
vised approaches along with the baselines on the
common systems dataset using the unofficial key
for scoring. UIUC is one of the two teams par-
ticipating in the SFV 2014 task and the only team
to report results, but they report 6 different sys-
tem configurations and we show their best perfor-
mance.
</bodyText>
<subsectionHeader confidence="0.994706">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9999181">
Our results indicate that stacking with provenance
information and relation type gives the best perfor-
mance using both the official ESF scorer as well
as the unofficial scorer that excludes the human-
generated slot fills. Our stacking approach that
uses the 10 systems common between 2013 and
2014 also outperforms the ensembling baselines
that have the advantage of using all 65 of the 2014
systems. Our stacking approach would presum-
ably perform even better if we had access to 2013
training data for all 2014 systems.
Of course, the best-performing ESF system for
2014 did not have access to the pooled slot fills
of all participating systems. Although pooling
the results has an advantage, naive pooling meth-
ods such as the ensembling baselines, in particu-
lar the voting approach, do not perform as well as
our stacked ensembles. Our best approach outper-
forms the best baseline for both the datasets by at
least 6 F1 points using both the official and unof-
</bodyText>
<page confidence="0.997299">
184
</page>
<bodyText confidence="0.998897142857143">
ficial scorer.
As expected the Union baseline has the highest
recall. Among the supervised approaches, stack-
ing with document provenance produces the high-
est precision and is significantly higher (approx-
imately 5%) than the approach that produces the
second highest precision. As discussed earlier, we
also scored our approaches on the unofficial scorer
so that we can compare our results to the UIUC
system that was the best performer in the 2014
SFV task. Our best approach beats their best sys-
tem configuration by a F1 score of 12 points. Our
stacking approach also outperforms them on pre-
cision and recall by a large margin.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999934150943396">
Our system is part of a body of work on increas-
ing the performance of relation extraction through
ensemble methods.
The use of stacked generalization for informa-
tion extraction has been demonstrated to outper-
form both majority voting and weighted voting
methods (Sigletos et al., 2005). In relation ex-
traction, a stacked classifier effectively combines
a supervised, closed-domain Conditional Ran-
dom Field-based relation extractor with an open-
domain CRF Open IE system, yielding a 10% in-
crease in precision without harming recall (Banko
et al., 2008). To our knowledge, we are the first to
apply stacking to KBP and the first to use prove-
nance as a feature in a stacking approach.
Many KBP SFV systems cast validation as
a single-document problem and apply a vari-
ety of techniques, such as rule-based consistency
checks (Angeli et al., 2013), and techniques from
the well-known Recognizing Textual Entailment
(RTE) task (Cheng et al., 2013; Sammons et al.,
2014). In contrast, the 2013 JHUAPL system ag-
gregates the results of many different extractors
using a constraint optimization framework, ex-
ploiting confidence values reported by each input
system (Wang et al., 2013). A second approach in
the UI CCG system (Sammons et al., 2014) aggre-
gates results of multiple systems by using majority
voting.
In the database, web-search, and data-mining
communities, a line of research into “truth-
finding” or “truth-discovery” methods addresses
the related problem of combining evidence for
facts from multiple sources, each with a latent
credibility (Yin et al., 2008). The RPI BLENDER
KBP system (Yu et al., 2014) casts SFV in this
framework, using a graph propagation method that
modeled the credibility of systems, sources, and
response values. However they only report scores
on the 2013 SFV data which contain less com-
plicated and easier queries compared to the 2014
data. Therefore, we cannot directly compare our
system’s performance to theirs.
Google’s Knowledge Vault system (Dong et al.,
2014) combines the output of four diverse extrac-
tion methods by building a boosted decision stump
classifier (Reyzin and Schapire, 2006). For each
proposed fact, the classifier considers both the
confidence value of each extractor and the number
of responsive documents found by the extractor.
A separate classifier is trained for each predicate,
and Platt Scaling (Platt, 1999) is used to calibrate
confidence scores.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991533333333">
This paper has presented experimental results
showing that stacking is a very promising ap-
proach to ensembling KBP systems. From our
literature survey, we observe that we are the first
to employ stacking and combine it with prove-
nance information to ensemble KBP systems. Our
stacked meta-classifier provides an F1 score of
50.1% on 2014 KBP ESF, outperforming the best
ESF and SFV systems from the 2014 competition,
and thereby achieving a new state-of-the-art for
this task. We found that provenance features in-
creased accuracy, highlighting the importance of
provenance information (even without accessing
the source corpus) in addition to confidence scores
for ensembling information extraction systems.
</bodyText>
<sectionHeader confidence="0.997089" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999345">
We thank the anonymous reviewers for their valu-
able feedback. This research was supported by
the DARPA DEFT program under AFRL grant
FA8750-13-2-0026.
</bodyText>
<sectionHeader confidence="0.998503" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990002714285714">
Gabor Angeli, Arun Chaganty, Angel Chang, Kevin
Reschke, Julie Tibshirani, Jean Y Wu, Osbert Bas-
tani, Keith Siilats, and Christopher D Manning.
2013. Stanford’s 2013 KBP system. In Proceedings
of the Sixth Text Analysis Conference (TAC2013).
Michele Banko, Oren Etzioni, and Turing Center.
2008. The tradeoffs between open and traditional
</reference>
<page confidence="0.994203">
185
</page>
<reference confidence="0.999470125">
relation extraction. In ACL08, volume 8, pages 28–
36.
Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-
Wei Chang, Zhiye Fei, Mark Sammons, John Wi-
eting, Subhro Roy, Chizheng Wang, and Dan Roth.
2013. Illinois cognitive computation group UI-CCG
TAC 2013 entity linking and slot filler validation
systems. In Proceedings of the Sixth Text Analysis
Conference (TAC2013).
T. Dietterich. 2000. Ensemble methods in machine
learning. In J. Kittler and F. Roli, editors, First
International Workshop on Multiple Classifier Sys-
tems, Lecture Notes in Computer Science, pages 1–
15. Springer-Verlag.
Pedro Domingos, Stanley Kok, Daniel Lowd, Hoifung
Poon, Matthew Richardson, and Parag Singla. 2008.
Markov logic. In Luc De Raedt, Paolo Frasconi,
Kristian Kersting, and Stephen Muggleton, editors,
Probabilistic Inductive Logic Programming, volume
4911 of Lecture Notes in Computer Science, pages
92–117. Springer.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko
Horn, Ni Lao, Kevin Murphy, Thomas Strohmann,
Shaohua Sun, and Wei Zhang. 2014. Knowl-
edge vault: A web-scale approach to probabilistic
knowledge fusion. In Proceedings of the 20th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 601–610. ACM.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871–1874.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10–
18.
John C. Henderson and Eric Brill. 1999. Exploiting
diversity in natural language processing: Combin-
ing parsers. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora (EMNLP/VLC-99), pages
187–194, College Park, MD.
Yu Hong, Xiaobin Wang, Yadong Chen, Jian Wang,
Tongtao Zhang, Jin Zheng, Dian Yu, Qi Li, Boliang
Zhang, Han Wang, et al. 2014. RPI BLENDER
TAC-KBP2014 knowledge base population system.
Proceedings of the Seventh TextAnalysis Conference
(TAC2014).
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive Bayesian classifiers for word sense
disambiguation. In Proceedings of the Meeting of
the North American Association for Computational
Linguistics, pages 63–69.
John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparisons to regularized
likelihood methods. In Peter J. Bartlett, Bernhard
Sch¨olkopf, Dale Schuurmans, and Alex J. Smola,
editors, Advances in Large Margin Classifiers, pages
61–74. MIT Press, Boston.
Lev Reyzin and Robert E Schapire. 2006. How boost-
ing the margin can also boost classifier complexity.
In Proceedings of the 23rd International Conference
on Machine Learning, pages 753–760. ACM.
Benjamin Roth and Dietrich Klakow. 2010. Cross-
language retrieval using link-based language mod-
els. In Proceedings of the 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 773–774. ACM.
Benjamin Roth, Tassilo Barth, Michael Wiegand, et al.
2013. Effective slot filling based on shallow distant
supervision methods. Proceedings of the Seventh
Text Analysis Conference (TAC2013).
Mark Sammons, Yangqiu Song, Ruichen Wang,
Gourab Kundu, et al. 2014. Overview of UI-CCG
systems for event argument extraction, entity dis-
covery and linking, and slot filler validation. Pro-
ceedings of the Seventh Text Analysis Conference
(TAC2014).
Georgios Sigletos, Georgios Paliouras, Constantine D
Spyropoulos, and Michalis Hatzopoulos. 2005.
Combining information extraction systems using
voting and stacked generalization. The Journal of
Machine Learning Research, 6:1751–1782.
Joseph Sill, G´abor Tak´acs, Lester Mackey, and David
Lin. 2009. Feature-weighted linear stacking. arXiv
preprint arXiv:0911.0460.
Sameer Singh, Limin Yao, David Belanger, Ariel Ko-
bren, Sam Anzaroot, Michael Wick, Alexandre Pas-
sos, Harshal Pandya, Jinho Choi, Brian Martin, and
Andrew McCallum. 2013. Universal schema for
slot filling and cold start: UMass IESL.
Mihai Surdeanu and Heng Ji. 2014. Overview of the
English slot filling track at the TAC2014 Knowledge
Base Population Evaluation. In Proceedings of the
Seventh Text Analysis Conference (TAC2014).
Mihai Surdeanu. 2013. Overview of the TAC2013
knowledge base population evaluation: English slot
filling and temporal slot filling. In Proceedings of
the Sixth Text Analysis Conference (TAC 2013).
I-Jeng Wang, Edwina Liu, Cash Costello, and Christine
Piatko. 2013. JHUAPL TAC-KBP2013 slot filler
validation system. In Proceedings of the Sixth Text
Analysis Conference (TAC2013).
</reference>
<page confidence="0.987524">
186
</page>
<reference confidence="0.999466913043478">
Anurag Wazalwar, Tushar Khot, Ce Zhang, Chris Re,
Jude Shavlik, and Sriraam Natarajan. 2014. TAC
KBP 2014 : English slot filling track DeepDive with
expert advice. In Proceedings of the Seventh Text
Analysis Conference (TAC2014).
Matthew Whitehead and Larry Yaeger. 2010. Senti-
ment mining using ensemble classification models.
In Tarek Sobh, editor, Innovations and Advances in
Computer Sciences and Engineering. Springer Ver-
lag, Berlin.
David H. Wolpert. 1992. Stacked generalization. Neu-
ral Networks, 5:241–259.
Xiaoxin Yin, Jiawei Han, and Philip S Yu. 2008.
Truth discovery with multiple conflicting informa-
tion providers on the web. Knowledge and Data En-
gineering, IEEE Transactions on, 20(6):796–808.
Dian Yu, Hongzhao Huang, Taylor Cassidy, Heng Ji,
Chi Wang, Shi Zhi, Jiawei Han, Clare Voss, and Ma-
lik Magdon-Ismail. 2014. The wisdom of minority:
Unsupervised slot filling validation based on multi-
dimensional truth-finding. In Proc. The 25th Inter-
national Conference on Computational Linguistics
(COLING2014).
</reference>
<page confidence="0.997843">
187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711907">
<title confidence="0.99943">Stacked Ensembles of Information for Knowledge-Base Population</title>
<author confidence="0.99912">Fatema Vidhoon Yinon Bentor Raymond J</author>
<affiliation confidence="0.9975555">Department of Computer University of Texas at</affiliation>
<address confidence="0.723833">Austin, TX 78712,</address>
<abstract confidence="0.999470421052631">We present results on using stacking to ensemble multiple systems for the Knowledge Base Population English Slot Filling (KBP-ESF) task. In addition to using the output and confidence of each system as input to the stacked classifier, we also use features capturing how well the systems agree about the provenance of the information they extract. We demonstrate that our stacking approach outperforms the best system from the 2014 KBP- ESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gabor Angeli</author>
<author>Arun Chaganty</author>
<author>Angel Chang</author>
<author>Kevin Reschke</author>
<author>Julie Tibshirani</author>
<author>Jean Y Wu</author>
<author>Osbert Bastani</author>
<author>Keith Siilats</author>
<author>Christopher D Manning</author>
</authors>
<title>KBP system.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Text Analysis Conference (TAC2013).</booktitle>
<contexts>
<context position="31471" citStr="Angeli et al., 2013" startWordPosition="5214" endWordPosition="5217">ority voting and weighted voting methods (Sigletos et al., 2005). In relation extraction, a stacked classifier effectively combines a supervised, closed-domain Conditional Random Field-based relation extractor with an opendomain CRF Open IE system, yielding a 10% increase in precision without harming recall (Banko et al., 2008). To our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recognizing Textual Entailment (RTE) task (Cheng et al., 2013; Sammons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the</context>
</contexts>
<marker>Angeli, Chaganty, Chang, Reschke, Tibshirani, Wu, Bastani, Siilats, Manning, 2013</marker>
<rawString>Gabor Angeli, Arun Chaganty, Angel Chang, Kevin Reschke, Julie Tibshirani, Jean Y Wu, Osbert Bastani, Keith Siilats, and Christopher D Manning. 2013. Stanford’s 2013 KBP system. In Proceedings of the Sixth Text Analysis Conference (TAC2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
<author>Turing Center</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In ACL08,</booktitle>
<volume>8</volume>
<pages>28--36</pages>
<contexts>
<context position="31180" citStr="Banko et al., 2008" startWordPosition="5162" endWordPosition="5165">forms them on precision and recall by a large margin. 6 Related Work Our system is part of a body of work on increasing the performance of relation extraction through ensemble methods. The use of stacked generalization for information extraction has been demonstrated to outperform both majority voting and weighted voting methods (Sigletos et al., 2005). In relation extraction, a stacked classifier effectively combines a supervised, closed-domain Conditional Random Field-based relation extractor with an opendomain CRF Open IE system, yielding a 10% increase in precision without harming recall (Banko et al., 2008). To our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recognizing Textual Entailment (RTE) task (Cheng et al., 2013; Sammons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input sys</context>
</contexts>
<marker>Banko, Etzioni, Center, 2008</marker>
<rawString>Michele Banko, Oren Etzioni, and Turing Center. 2008. The tradeoffs between open and traditional relation extraction. In ACL08, volume 8, pages 28– 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Cheng</author>
<author>Bingling Chen</author>
<author>Rajhans Samdani</author>
<author>KaiWei Chang</author>
<author>Zhiye Fei</author>
<author>Mark Sammons</author>
<author>John Wieting</author>
<author>Subhro Roy</author>
<author>Chizheng Wang</author>
<author>Dan Roth</author>
</authors>
<title>entity linking and slot filler validation systems.</title>
<date>2013</date>
<booktitle>Illinois cognitive computation group UI-CCG TAC</booktitle>
<contexts>
<context position="31569" citStr="Cheng et al., 2013" startWordPosition="5228" endWordPosition="5231">classifier effectively combines a supervised, closed-domain Conditional Random Field-based relation extractor with an opendomain CRF Open IE system, yielding a 10% increase in precision without harming recall (Banko et al., 2008). To our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recognizing Textual Entailment (RTE) task (Cheng et al., 2013; Sammons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibi</context>
</contexts>
<marker>Cheng, Chen, Samdani, Chang, Fei, Sammons, Wieting, Roy, Wang, Roth, 2013</marker>
<rawString>Xiao Cheng, Bingling Chen, Rajhans Samdani, KaiWei Chang, Zhiye Fei, Mark Sammons, John Wieting, Subhro Roy, Chizheng Wang, and Dan Roth. 2013. Illinois cognitive computation group UI-CCG TAC 2013 entity linking and slot filler validation systems. In Proceedings of the Sixth Text Analysis Conference (TAC2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Ensemble methods in machine learning.</title>
<date>2000</date>
<booktitle>First International Workshop on Multiple Classifier Systems, Lecture Notes in Computer Science,</booktitle>
<pages>1--15</pages>
<editor>In J. Kittler and F. Roli, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1119" citStr="Dietterich, 2000" startWordPosition="164" endWordPosition="165">tacked classifier, we also use features capturing how well the systems agree about the provenance of the information they extract. We demonstrate that our stacking approach outperforms the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We presen</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>T. Dietterich. 2000. Ensemble methods in machine learning. In J. Kittler and F. Roli, editors, First International Workshop on Multiple Classifier Systems, Lecture Notes in Computer Science, pages 1– 15. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Domingos</author>
<author>Stanley Kok</author>
<author>Daniel Lowd</author>
</authors>
<title>Hoifung Poon, Matthew Richardson, and Parag Singla.</title>
<date>2008</date>
<booktitle>Probabilistic Inductive Logic Programming,</booktitle>
<volume>4911</volume>
<pages>92--117</pages>
<editor>In Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="5115" citStr="Domingos et al., 2008" startWordPosition="784" endWordPosition="787">Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferencing system. Another system, RPI BLENDER (Hong et al., 2014), used a restricted fuzzy matching technique in a framework that learned event triggers and employed them to extract relations from documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the </context>
</contexts>
<marker>Domingos, Kok, Lowd, 2008</marker>
<rawString>Pedro Domingos, Stanley Kok, Daniel Lowd, Hoifung Poon, Matthew Richardson, and Parag Singla. 2008. Markov logic. In Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors, Probabilistic Inductive Logic Programming, volume 4911 of Lecture Notes in Computer Science, pages 92–117. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Dong</author>
<author>Evgeniy Gabrilovich</author>
<author>Geremy Heitz</author>
<author>Wilko Horn</author>
<author>Ni Lao</author>
<author>Kevin Murphy</author>
<author>Thomas Strohmann</author>
<author>Shaohua Sun</author>
<author>Wei Zhang</author>
</authors>
<title>Knowledge vault: A web-scale approach to probabilistic knowledge fusion.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>601--610</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="32627" citStr="Dong et al., 2014" startWordPosition="5394" endWordPosition="5397">into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibility (Yin et al., 2008). The RPI BLENDER KBP system (Yu et al., 2014) casts SFV in this framework, using a graph propagation method that modeled the credibility of systems, sources, and response values. However they only report scores on the 2013 SFV data which contain less complicated and easier queries compared to the 2014 data. Therefore, we cannot directly compare our system’s performance to theirs. Google’s Knowledge Vault system (Dong et al., 2014) combines the output of four diverse extraction methods by building a boosted decision stump classifier (Reyzin and Schapire, 2006). For each proposed fact, the classifier considers both the confidence value of each extractor and the number of responsive documents found by the extractor. A separate classifier is trained for each predicate, and Platt Scaling (Platt, 1999) is used to calibrate confidence scores. 7 Conclusion This paper has presented experimental results showing that stacking is a very promising approach to ensembling KBP systems. From our literature survey, we observe that we ar</context>
</contexts>
<marker>Dong, Gabrilovich, Heitz, Horn, Lao, Murphy, Strohmann, Sun, Zhang, 2014</marker>
<rawString>Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 601–610. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11807" citStr="Fan et al., 2008" startWordPosition="1868" endWordPosition="1871"> successfully used in many applications including the top performing systems in the Netflix competition (Sill et al., 2009). The idea is to employ multiple learners and combine their predictions by training a “meta-classifier” to weight and combine multiple models using their confidence scores as features. By training on a set of supervised data that is disjoint from that used to train the individual models, it learns how to combine their results into an improved ensemble model. We employ a single classifier to train and test on all slot types using an L1-regularized SVM with a linear kernel (Fan et al., 2008). 4.1.2 Using Provenance As discussed above, each system provides provenance information for every non-NIL slot filler. There are two kinds of provenance provided: the relation provenance and the filler provenance. In our algorithm, we only use the filler provenance for a given slot fill. This is because of the changes in the output formats for the ESF task from 2013 to 2014. Specifically, the 2013 specification requires separate entity and justification provenance fields, but the 2014 collapses these into a single relation provenance field. An additional filler provenance 179 field is common </context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD explorations newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="22842" citStr="Hall et al., 2009" startWordPosition="3746" endWordPosition="3749"> functions for different slot types if the classifier finds this useful. For our third approach, we also provide the document provenance feature described in Section 4.1. Altogether this approach has 11 features (10 confidence score + 1 document provenance score). The fourth approach uses confidences, the document provenance feature, and a one-hot encoding of the relation name for the slot instance. Our final approach also includes the offset provenance (OP) feature discussed in Section 4.1. There are altogether 13 features in this approach. All our supervised approaches use the Weka package (Hall et al., 2009) for training the meta-classifier, using an L1-regularized SVM with a linear kernel (other classifiers gave similar results). Figure 2 shows our system pipeline for evaluating supervised ensembling approaches. Table 5 gives the performance of all our supervised approaches as well as 182 our unsupervised baselines for the common systems dataset. Analysis by Surdeanu and Ji (2014) suggests that 2014 ESF queries are more difficult than those for 2013. They compare two systems by running both on 2013 and 2014 data and find there is a considerable drop in the performance of both the systems. We not</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The WEKA data mining software: an update. ACM SIGKDD explorations newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Henderson</author>
<author>Eric Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99),</booktitle>
<pages>187--194</pages>
<location>College Park, MD.</location>
<contexts>
<context position="1256" citStr="Henderson and Brill, 1999" startWordPosition="182" endWordPosition="186">t. We demonstrate that our stacking approach outperforms the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally o</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>John C. Henderson and Eric Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 187–194, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Hong</author>
<author>Xiaobin Wang</author>
<author>Yadong Chen</author>
<author>Jian Wang</author>
<author>Tongtao Zhang</author>
<author>Jin Zheng</author>
<author>Dian Yu</author>
<author>Qi Li</author>
<author>Boliang Zhang</author>
<author>Han Wang</author>
</authors>
<title>RPI BLENDER TAC-KBP2014 knowledge base population system.</title>
<date>2014</date>
<booktitle>Proceedings of the Seventh TextAnalysis Conference (TAC2014).</booktitle>
<contexts>
<context position="5205" citStr="Hong et al., 2014" startWordPosition="798" endWordPosition="801">pecific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferencing system. Another system, RPI BLENDER (Hong et al., 2014), used a restricted fuzzy matching technique in a framework that learned event triggers and employed them to extract relations from documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating syste</context>
</contexts>
<marker>Hong, Wang, Chen, Wang, Zhang, Zheng, Yu, Li, Zhang, Wang, 2014</marker>
<rawString>Yu Hong, Xiaobin Wang, Yadong Chen, Jian Wang, Tongtao Zhang, Jin Zheng, Dian Yu, Qi Li, Boliang Zhang, Han Wang, et al. 2014. RPI BLENDER TAC-KBP2014 knowledge base population system. Proceedings of the Seventh TextAnalysis Conference (TAC2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5065" citStr="Mintz et al., 2009" startWordPosition="776" endWordPosition="779">ous other tasks as a part of the Text Analysis Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferencing system. Another system, RPI BLENDER (Hong et al., 2014), used a restricted fuzzy matching technique in a framework that learned event triggers and employed them to extract relations from documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the se</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Meeting of the North American Association for Computational Linguistics,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="1300" citStr="Pedersen, 2000" startWordPosition="190" endWordPosition="191">s the best system from the 2014 KBPESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The re</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Ted Pedersen. 2000. A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation. In Proceedings of the Meeting of the North American Association for Computational Linguistics, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<editor>In Peter J. Bartlett, Bernhard Sch¨olkopf, Dale Schuurmans, and Alex J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Boston.</location>
<contexts>
<context position="33000" citStr="Platt, 1999" startWordPosition="5453" endWordPosition="5454">y report scores on the 2013 SFV data which contain less complicated and easier queries compared to the 2014 data. Therefore, we cannot directly compare our system’s performance to theirs. Google’s Knowledge Vault system (Dong et al., 2014) combines the output of four diverse extraction methods by building a boosted decision stump classifier (Reyzin and Schapire, 2006). For each proposed fact, the classifier considers both the confidence value of each extractor and the number of responsive documents found by the extractor. A separate classifier is trained for each predicate, and Platt Scaling (Platt, 1999) is used to calibrate confidence scores. 7 Conclusion This paper has presented experimental results showing that stacking is a very promising approach to ensembling KBP systems. From our literature survey, we observe that we are the first to employ stacking and combine it with provenance information to ensemble KBP systems. Our stacked meta-classifier provides an F1 score of 50.1% on 2014 KBP ESF, outperforming the best ESF and SFV systems from the 2014 competition, and thereby achieving a new state-of-the-art for this task. We found that provenance features increased accuracy, highlighting th</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Peter J. Bartlett, Bernhard Sch¨olkopf, Dale Schuurmans, and Alex J. Smola, editors, Advances in Large Margin Classifiers, pages 61–74. MIT Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Reyzin</author>
<author>Robert E Schapire</author>
</authors>
<title>How boosting the margin can also boost classifier complexity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 23rd International Conference on Machine Learning,</booktitle>
<pages>753--760</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="32758" citStr="Reyzin and Schapire, 2006" startWordPosition="5414" endWordPosition="5417">e sources, each with a latent credibility (Yin et al., 2008). The RPI BLENDER KBP system (Yu et al., 2014) casts SFV in this framework, using a graph propagation method that modeled the credibility of systems, sources, and response values. However they only report scores on the 2013 SFV data which contain less complicated and easier queries compared to the 2014 data. Therefore, we cannot directly compare our system’s performance to theirs. Google’s Knowledge Vault system (Dong et al., 2014) combines the output of four diverse extraction methods by building a boosted decision stump classifier (Reyzin and Schapire, 2006). For each proposed fact, the classifier considers both the confidence value of each extractor and the number of responsive documents found by the extractor. A separate classifier is trained for each predicate, and Platt Scaling (Platt, 1999) is used to calibrate confidence scores. 7 Conclusion This paper has presented experimental results showing that stacking is a very promising approach to ensembling KBP systems. From our literature survey, we observe that we are the first to employ stacking and combine it with provenance information to ensemble KBP systems. Our stacked meta-classifier prov</context>
</contexts>
<marker>Reyzin, Schapire, 2006</marker>
<rawString>Lev Reyzin and Robert E Schapire. 2006. How boosting the margin can also boost classifier complexity. In Proceedings of the 23rd International Conference on Machine Learning, pages 753–760. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Dietrich Klakow</author>
</authors>
<title>Crosslanguage retrieval using link-based language models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>773--774</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16051" citStr="Roth and Klakow, 2010" startWordPosition="2588" endWordPosition="2591">ntities might overlap with each other. An ESF system could extract a filler F1 for a slot S while another ESF system extracts another filler F2 for the same slot S. If the extracted fillers F1 and F2 are aliases (i.e. different names for the same entity), the scoring system for the TAC KBP SF task considers them redundant and penalizes the precision of the system. In order to eliminate aliases from the output of ensembled system, we employ a technique derived by inverting the scheme used by the LSV ESF system (Roth et al., 2013) for query expansion. LSV ESF uses a Wikipedia anchor-text model (Roth and Klakow, 2010) to generate aliases for given query entities. By including aliases for query names, the ESF system increase the number of candidate sentences fetched for the query. To eliminate filler aliases, we apply the same technique to generate aliases for all slot fillers of a given query and slot type. Given a slot filler, we obtain the Wikipedia page that is most likely linked to the filler text. Then, we obtain the anchor texts and their respective counts from all other Wikipedia pages that link to this page. Using these counts, we choose top N (we use N=10 as in LSV) and pick the corresponding anch</context>
</contexts>
<marker>Roth, Klakow, 2010</marker>
<rawString>Benjamin Roth and Dietrich Klakow. 2010. Crosslanguage retrieval using link-based language models. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 773–774. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Roth</author>
<author>Tassilo Barth</author>
<author>Michael Wiegand</author>
</authors>
<title>Effective slot filling based on shallow distant supervision methods.</title>
<date>2013</date>
<booktitle>Proceedings of the Seventh Text Analysis Conference (TAC2013).</booktitle>
<contexts>
<context position="15963" citStr="Roth et al., 2013" startWordPosition="2574" endWordPosition="2577">ombining the output of different ESF systems, it is possible that some slot-filler entities might overlap with each other. An ESF system could extract a filler F1 for a slot S while another ESF system extracts another filler F2 for the same slot S. If the extracted fillers F1 and F2 are aliases (i.e. different names for the same entity), the scoring system for the TAC KBP SF task considers them redundant and penalizes the precision of the system. In order to eliminate aliases from the output of ensembled system, we employ a technique derived by inverting the scheme used by the LSV ESF system (Roth et al., 2013) for query expansion. LSV ESF uses a Wikipedia anchor-text model (Roth and Klakow, 2010) to generate aliases for given query entities. By including aliases for query names, the ESF system increase the number of candidate sentences fetched for the query. To eliminate filler aliases, we apply the same technique to generate aliases for all slot fillers of a given query and slot type. Given a slot filler, we obtain the Wikipedia page that is most likely linked to the filler text. Then, we obtain the anchor texts and their respective counts from all other Wikipedia pages that link to this page. Usi</context>
</contexts>
<marker>Roth, Barth, Wiegand, 2013</marker>
<rawString>Benjamin Roth, Tassilo Barth, Michael Wiegand, et al. 2013. Effective slot filling based on shallow distant supervision methods. Proceedings of the Seventh Text Analysis Conference (TAC2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sammons</author>
<author>Yangqiu Song</author>
<author>Ruichen Wang</author>
<author>Gourab Kundu</author>
</authors>
<title>Overview of UI-CCG systems for event argument extraction, entity discovery and linking, and slot filler validation.</title>
<date>2014</date>
<booktitle>Proceedings of the Seventh Text Analysis Conference (TAC2014).</booktitle>
<contexts>
<context position="2201" citStr="Sammons et al., 2014" startWordPosition="334" endWordPosition="337">is KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated KBP Slot Filler Validation (SFV) Track (Wang et al., 2013; Yu et al., 2014; Sammons et al., 2014) is officially focused on improving the precision of individual existing systems by filtering their results, frequently participants in this track also combine the results of multiple systems and also report increased recall through this use of ensembling. However, SFV participants have not employed stacking, and we demonstrate that our stacking approach outperforms existing published SFV ensembling systems. KBP ESF systems must also provide provenance information, i.e. each extracted slot-filler must include a pointer to a document passage that supports it (Surdeanu and Ji, 2014). Some SFV sy</context>
<context position="6092" citStr="Sammons et al., 2014" startWordPosition="942" endWordPosition="945"> In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating systems employ a variety of techniques to perform validation. For instance, RPI BLENDER used a Multi-dimensional Truth Finding model (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surdeanu and Ji, 2014) is to collect information (fills) about specific attributes (slots) for a set of entities (queries) from a given corpus. </context>
<context position="12746" citStr="Sammons et al. (2014)" startWordPosition="2020" endWordPosition="2023">es in the output formats for the ESF task from 2013 to 2014. Specifically, the 2013 specification requires separate entity and justification provenance fields, but the 2014 collapses these into a single relation provenance field. An additional filler provenance 179 field is common to both specifications. Hence, we use the filler provenance that is common between 2013 and 2014 formats. As described earlier, every provenance has a docid and startoffsetendoffset that gives information about the document and offset in the document from where the slot fill has been extracted. The UI-CCG SFV system Sammons et al. (2014) effectively used this provenance information to help validate and filter slot fillers. This motivated us to use provenance in our stacking approach as additional features as input to the meta-classifier. We use provenance in two ways, first using the docid information, and second using the offset information. We use the docids to define a document-based provenance score in the following way: for a given query and slot, if N systems provide answers and a maximum of n of those systems give the same docid in their filler provenance, then the document provenance score for those n slot fills is n/</context>
<context position="28026" citStr="Sammons et al. (2014)" startWordPosition="4633" endWordPosition="4636">+ Provenance(document) 0.498 0.688 0.578 Stacking 0.613 0.562 0.586 Stacking + Relation 0.613 0.567 0.589 Stacking + Provenance (document and offset) + Relation 0.541 0.661 0.595 Stacking + Provenance (document) + Relation 0.659 0.56 0.606 Table 8: Performance on the common systems dataset (10 systems) for various configurations using the unofficial scorer. All approaches except the UIUC system are our implementations. SFV data. The TAC KBP official scoring key for the ESF task includes human annotated slot fills along with the pooled slot fills obtained by all participating systems. However, Sammons et al. (2014) use an unofficial scoring key in their paper that does not include human annotated slot fills. In order to compare to their results, we also present results using the same unofficial key. Table 7 gives the performance of our baseline systems on the 2014 SFV dataset using the unofficial key for scoring. We note that our Union does not produce a recall of 1.0 on the unofficial scorer due to our singlevalued slot selection strategy for multiple systems. As discussed earlier for the single-valued slot, we include the slot fill with highest confidence (which may not necessarily be correct) and thu</context>
<context position="31592" citStr="Sammons et al., 2014" startWordPosition="5232" endWordPosition="5235">ly combines a supervised, closed-domain Conditional Random Field-based relation extractor with an opendomain CRF Open IE system, yielding a 10% increase in precision without harming recall (Banko et al., 2008). To our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recognizing Textual Entailment (RTE) task (Cheng et al., 2013; Sammons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibility (Yin et al., 2008)</context>
</contexts>
<marker>Sammons, Song, Wang, Kundu, 2014</marker>
<rawString>Mark Sammons, Yangqiu Song, Ruichen Wang, Gourab Kundu, et al. 2014. Overview of UI-CCG systems for event argument extraction, entity discovery and linking, and slot filler validation. Proceedings of the Seventh Text Analysis Conference (TAC2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgios Sigletos</author>
<author>Georgios Paliouras</author>
<author>Constantine D Spyropoulos</author>
<author>Michalis Hatzopoulos</author>
</authors>
<title>Combining information extraction systems using voting and stacked generalization.</title>
<date>2005</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>6--1751</pages>
<contexts>
<context position="6214" citStr="Sigletos et al., 2005" startWordPosition="958" endWordPosition="961"> using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating systems employ a variety of techniques to perform validation. For instance, RPI BLENDER used a Multi-dimensional Truth Finding model (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surdeanu and Ji, 2014) is to collect information (fills) about specific attributes (slots) for a set of entities (queries) from a given corpus. The queries vary in each year of the task and can be either a person (PER) or an organization (ORG) entity. The slots are </context>
<context position="30915" citStr="Sigletos et al., 2005" startWordPosition="5122" endWordPosition="5125">scored our approaches on the unofficial scorer so that we can compare our results to the UIUC system that was the best performer in the 2014 SFV task. Our best approach beats their best system configuration by a F1 score of 12 points. Our stacking approach also outperforms them on precision and recall by a large margin. 6 Related Work Our system is part of a body of work on increasing the performance of relation extraction through ensemble methods. The use of stacked generalization for information extraction has been demonstrated to outperform both majority voting and weighted voting methods (Sigletos et al., 2005). In relation extraction, a stacked classifier effectively combines a supervised, closed-domain Conditional Random Field-based relation extractor with an opendomain CRF Open IE system, yielding a 10% increase in precision without harming recall (Banko et al., 2008). To our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recogni</context>
</contexts>
<marker>Sigletos, Paliouras, Spyropoulos, Hatzopoulos, 2005</marker>
<rawString>Georgios Sigletos, Georgios Paliouras, Constantine D Spyropoulos, and Michalis Hatzopoulos. 2005. Combining information extraction systems using voting and stacked generalization. The Journal of Machine Learning Research, 6:1751–1782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Sill</author>
<author>G´abor Tak´acs</author>
<author>Lester Mackey</author>
<author>David Lin</author>
</authors>
<title>Feature-weighted linear stacking. arXiv preprint arXiv:0911.0460.</title>
<date>2009</date>
<marker>Sill, Tak´acs, Mackey, Lin, 2009</marker>
<rawString>Joseph Sill, G´abor Tak´acs, Lester Mackey, and David Lin. 2009. Feature-weighted linear stacking. arXiv preprint arXiv:0911.0460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Limin Yao</author>
<author>David Belanger</author>
<author>Ariel Kobren</author>
<author>Sam Anzaroot</author>
<author>Michael Wick</author>
<author>Alexandre Passos</author>
<author>Harshal Pandya</author>
<author>Jinho Choi</author>
<author>Brian Martin</author>
<author>Andrew McCallum</author>
</authors>
<title>Universal schema for slot filling and cold start: UMass IESL.</title>
<date>2013</date>
<contexts>
<context position="25394" citStr="Singh et al., 2013" startWordPosition="4209" endWordPosition="4212">formance on 2014 data. This allows us to analyze the results at the system level. Figure 4 shows the plot of F1 score vs. the number of systems at each step. The figure shows huge improvement in F1 score at steps 6 and 7. At step 6 the Stanford system is added to the pool of systems which is the best performing ESF system in 2014 and fourth best in 2013. At step 7, the UMass system is added to the pool and, although the system on it own is weak, it boosts the performance of our ensembling approach. This is because the UMass system alone contributes approximately 24% of the 2013 training data (Singh et al., 2013). Thus adding this one system significantly improves the training step leading to better performance. We also notice that our system becomes less conservative at this step and has higher recall. The reason for this is that the systems from 1 to 5 had very high precision and low recall whereas from system 6 onwards the systems have high recall. Thus adding the UMass system enables our meta-classifier to have a higher recall for small decrease in precision and thus boosting the overall F1 measure. Without it, the classifier produces high precision but low recall and decreases the overall F1 scor</context>
</contexts>
<marker>Singh, Yao, Belanger, Kobren, Anzaroot, Wick, Passos, Pandya, Choi, Martin, McCallum, 2013</marker>
<rawString>Sameer Singh, Limin Yao, David Belanger, Ariel Kobren, Sam Anzaroot, Michael Wick, Alexandre Passos, Harshal Pandya, Jinho Choi, Brian Martin, and Andrew McCallum. 2013. Universal schema for slot filling and cold start: UMass IESL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Heng Ji</author>
</authors>
<title>Overview of the English slot filling track at the TAC2014 Knowledge Base Population Evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Seventh Text Analysis Conference (TAC2014).</booktitle>
<contexts>
<context position="1529" citStr="Surdeanu and Ji, 2014" startWordPosition="224" endWordPosition="227">cluding provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated KBP Slot Filler Validatio</context>
<context position="2788" citStr="Surdeanu and Ji, 2014" startWordPosition="424" endWordPosition="427">et al., 2014; Sammons et al., 2014) is officially focused on improving the precision of individual existing systems by filtering their results, frequently participants in this track also combine the results of multiple systems and also report increased recall through this use of ensembling. However, SFV participants have not employed stacking, and we demonstrate that our stacking approach outperforms existing published SFV ensembling systems. KBP ESF systems must also provide provenance information, i.e. each extracted slot-filler must include a pointer to a document passage that supports it (Surdeanu and Ji, 2014). Some SFV systems have used this provenance information to help filter and combine extractions (Sammons et al., 2014). Therefore, we also explored enhancing our stacking approach by including additional input features that capture provenance information. By including features that quantify how much the ensembled systems agree on provenance, we further improved our F1 score for the 2014 ESF task to 50.1%. The remainder of the paper is organized as follows. Section 2 provides background information on existing KBP-ESF systems and stacking. Section 3 provides general background on the KBPESF tas</context>
<context position="4547" citStr="Surdeanu and Ji, 2014" startWordPosition="691" endWordPosition="694">c�2015 Association for Computational Linguistics tems and build a stacked meta-classifier that learns to combine the results from individual slot filling systems. We detail our stacking approach for ensembling existing slot filling systems in Section 4. for information extraction. Section 7 presents our final conclusions and proposed directions for future research. 2 Background For the past few years, NIST has been conducting the English Slot Filling (ESF) Task in the Knowledge Base Population (KBP) track among various other tasks as a part of the Text Analysis Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferenci</context>
<context position="6570" citStr="Surdeanu and Ji, 2014" startWordPosition="1017" endWordPosition="1020">4) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surdeanu and Ji, 2014) is to collect information (fills) about specific attributes (slots) for a set of entities (queries) from a given corpus. The queries vary in each year of the task and can be either a person (PER) or an organization (ORG) entity. The slots are fixed and are listed in Table 1 by entity type. Some slots (like per:age) are single-valued while others (like per:children) are list-valued i.e., they can take multiple slot fillers. 3.1 Input and Output The input for the task is a set of queries and the corpus in which to look for information. The queries are provided in an XML format containing basic </context>
<context position="23223" citStr="Surdeanu and Ji (2014)" startWordPosition="3805" endWordPosition="3808">me for the slot instance. Our final approach also includes the offset provenance (OP) feature discussed in Section 4.1. There are altogether 13 features in this approach. All our supervised approaches use the Weka package (Hall et al., 2009) for training the meta-classifier, using an L1-regularized SVM with a linear kernel (other classifiers gave similar results). Figure 2 shows our system pipeline for evaluating supervised ensembling approaches. Table 5 gives the performance of all our supervised approaches as well as 182 our unsupervised baselines for the common systems dataset. Analysis by Surdeanu and Ji (2014) suggests that 2014 ESF queries are more difficult than those for 2013. They compare two systems by running both on 2013 and 2014 data and find there is a considerable drop in the performance of both the systems. We note that they run the same exact system on 2013 and 2014 data. Thus, in order to have a better understanding of our results, we plot a learning curve by training on different sizes of the 2013 SFV data and using the scorer to measure the F1 score on the 2014 SFV data for the 10 common systems. Figure 3 shows the learning curve thus obtained. Although there are certain parts of the</context>
</contexts>
<marker>Surdeanu, Ji, 2014</marker>
<rawString>Mihai Surdeanu and Heng Ji. 2014. Overview of the English slot filling track at the TAC2014 Knowledge Base Population Evaluation. In Proceedings of the Seventh Text Analysis Conference (TAC2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
</authors>
<title>Overview of the TAC2013 knowledge base population evaluation: English slot filling and temporal slot filling.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Text Analysis Conference (TAC</booktitle>
<contexts>
<context position="1505" citStr="Surdeanu, 2013" startWordPosition="222" endWordPosition="223">onstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated K</context>
<context position="4523" citStr="Surdeanu, 2013" startWordPosition="688" endWordPosition="690">ly 26-31, 2015. c�2015 Association for Computational Linguistics tems and build a stacked meta-classifier that learns to combine the results from individual slot filling systems. We detail our stacking approach for ensembling existing slot filling systems in Section 4. for information extraction. Section 7 presents our final conclusions and proposed directions for future research. 2 Background For the past few years, NIST has been conducting the English Slot Filling (ESF) Task in the Knowledge Base Population (KBP) track among various other tasks as a part of the Text Analysis Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in thei</context>
<context position="6546" citStr="Surdeanu, 2013" startWordPosition="1015" endWordPosition="1016"> (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surdeanu and Ji, 2014) is to collect information (fills) about specific attributes (slots) for a set of entities (queries) from a given corpus. The queries vary in each year of the task and can be either a person (PER) or an organization (ORG) entity. The slots are fixed and are listed in Table 1 by entity type. Some slots (like per:age) are single-valued while others (like per:children) are list-valued i.e., they can take multiple slot fillers. 3.1 Input and Output The input for the task is a set of queries and the corpus in which to look for information. The queries are provided in an XML </context>
</contexts>
<marker>Surdeanu, 2013</marker>
<rawString>Mihai Surdeanu. 2013. Overview of the TAC2013 knowledge base population evaluation: English slot filling and temporal slot filling. In Proceedings of the Sixth Text Analysis Conference (TAC 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I-Jeng Wang</author>
<author>Edwina Liu</author>
<author>Cash Costello</author>
<author>Christine Piatko</author>
</authors>
<title>JHUAPL TAC-KBP2013 slot filler validation system.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Text Analysis Conference (TAC2013).</booktitle>
<contexts>
<context position="2161" citStr="Wang et al., 2013" startWordPosition="326" endWordPosition="329">e new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated KBP Slot Filler Validation (SFV) Track (Wang et al., 2013; Yu et al., 2014; Sammons et al., 2014) is officially focused on improving the precision of individual existing systems by filtering their results, frequently participants in this track also combine the results of multiple systems and also report increased recall through this use of ensembling. However, SFV participants have not employed stacking, and we demonstrate that our stacking approach outperforms existing published SFV ensembling systems. KBP ESF systems must also provide provenance information, i.e. each extracted slot-filler must include a pointer to a document passage that supports</context>
<context position="31803" citStr="Wang et al., 2013" startWordPosition="5264" endWordPosition="5267">our knowledge, we are the first to apply stacking to KBP and the first to use provenance as a feature in a stacking approach. Many KBP SFV systems cast validation as a single-document problem and apply a variety of techniques, such as rule-based consistency checks (Angeli et al., 2013), and techniques from the well-known Recognizing Textual Entailment (RTE) task (Cheng et al., 2013; Sammons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibility (Yin et al., 2008). The RPI BLENDER KBP system (Yu et al., 2014) casts SFV in this framework, using a graph propagation method that modeled the credibility of systems, sources, and response values. However they only report scores</context>
</contexts>
<marker>Wang, Liu, Costello, Piatko, 2013</marker>
<rawString>I-Jeng Wang, Edwina Liu, Cash Costello, and Christine Piatko. 2013. JHUAPL TAC-KBP2013 slot filler validation system. In Proceedings of the Sixth Text Analysis Conference (TAC2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anurag Wazalwar</author>
<author>Tushar Khot</author>
<author>Ce Zhang</author>
<author>Chris Re</author>
<author>Jude Shavlik</author>
<author>Sriraam Natarajan</author>
</authors>
<title>English slot filling track DeepDive with expert advice.</title>
<date>2014</date>
<journal>TAC KBP</journal>
<booktitle>In Proceedings of the Seventh Text Analysis Conference (TAC2014).</booktitle>
<contexts>
<context position="5014" citStr="Wazalwar et al., 2014" startWordPosition="768" endWordPosition="771">n the Knowledge Base Population (KBP) track among various other tasks as a part of the Text Analysis Conference(TAC)(Surdeanu, 2013; Surdeanu and Ji, 2014). In the ESF task, the goal is to fill specific slots of information for a given set of query entities (people or organizations) based on a supplied text corpus. The participating systems employ a variety of techniques in different stages of the slot filling pipeline, such as entity search, relevant document extraction, relation modeling and inference. In 2014, the top performing system, DeepDive with Expert Advice from Stanford University (Wazalwar et al., 2014), employed distant supervision (Mintz et al., 2009) and Markov Logic Networks (Domingos et al., 2008) in their learning and inferencing system. Another system, RPI BLENDER (Hong et al., 2014), used a restricted fuzzy matching technique in a framework that learned event triggers and employed them to extract relations from documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of </context>
</contexts>
<marker>Wazalwar, Khot, Zhang, Re, Shavlik, Natarajan, 2014</marker>
<rawString>Anurag Wazalwar, Tushar Khot, Ce Zhang, Chris Re, Jude Shavlik, and Sriraam Natarajan. 2014. TAC KBP 2014 : English slot filling track DeepDive with expert advice. In Proceedings of the Seventh Text Analysis Conference (TAC2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Whitehead</author>
<author>Larry Yaeger</author>
</authors>
<title>Sentiment mining using ensemble classification models.</title>
<date>2010</date>
<booktitle>In Tarek Sobh, editor, Innovations and Advances in Computer Sciences and Engineering.</booktitle>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="1353" citStr="Whitehead and Yaeger, 2010" startWordPosition="195" endWordPosition="199">mpetition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines. Additionally, we demonstrate that including provenance information further increases the performance of stacking. 1 Introduction Using ensembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in t</context>
</contexts>
<marker>Whitehead, Yaeger, 2010</marker>
<rawString>Matthew Whitehead and Larry Yaeger. 2010. Sentiment mining using ensemble classification models. In Tarek Sobh, editor, Innovations and Advances in Computer Sciences and Engineering. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David H Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="1622" citStr="Wolpert, 1992" startWordPosition="239" endWordPosition="240">sembles of multiple systems is a standard approach to improving accuracy in machine learning (Dietterich, 2000). Ensembles have been applied to a wide variety of problems in natural language processing, including parsing (Henderson and Brill, 1999), word sense disambiguation (Pedersen, 2000), and sentiment analysis (Whitehead and Yaeger, 2010). This paper presents a detailed study of ensembling methods for the TAC Knowledge Base Population (KBP) English Slot Filling (ESF) task (Surdeanu, 2013; Surdeanu and Ji, 2014). We demonstrate new state-of-the-art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated KBP Slot Filler Validation (SFV) Track (Wang et al., 2013; Yu et al., 2014; Sammons et al., 2014) is officially focuse</context>
<context position="6230" citStr="Wolpert, 1992" startWordPosition="962" endWordPosition="963">isting systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating systems employ a variety of techniques to perform validation. For instance, RPI BLENDER used a Multi-dimensional Truth Finding model (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Surdeanu and Ji, 2014) is to collect information (fills) about specific attributes (slots) for a set of entities (queries) from a given corpus. The queries vary in each year of the task and can be either a person (PER) or an organization (ORG) entity. The slots are fixed and are li</context>
<context position="11177" citStr="Wolpert, 1992" startWordPosition="1763" endWordPosition="1764">venance and confidence scores. The output of the ensembling system is similar to the output of an individual system, but it productively aggregates the slot fillers from different systems. 4.1 Algorithm This section describes our ensembling approach which trains a final binary classifier using features that help judge the reliability and thus correctness of individual slot fills. In a final post-processing step, the slot fills that get classified as “correct” by the classifier are kept while the others are set to NIL. 4.1.1 Stacking Stacking is a popular ensembling method in machine learning (Wolpert, 1992) and has been successfully used in many applications including the top performing systems in the Netflix competition (Sill et al., 2009). The idea is to employ multiple learners and combine their predictions by training a “meta-classifier” to weight and combine multiple models using their confidence scores as features. By training on a set of supervised data that is disjoint from that used to train the individual models, it learns how to combine their results into an improved ensemble model. We employ a single classifier to train and test on all slot types using an L1-regularized SVM with a li</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David H. Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxin Yin</author>
<author>Jiawei Han</author>
<author>Philip S Yu</author>
</authors>
<title>Truth discovery with multiple conflicting information providers on the web. Knowledge and Data Engineering,</title>
<date>2008</date>
<journal>IEEE Transactions on,</journal>
<volume>20</volume>
<issue>6</issue>
<contexts>
<context position="32192" citStr="Yin et al., 2008" startWordPosition="5324" endWordPosition="5327">ons et al., 2014). In contrast, the 2013 JHUAPL system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibility (Yin et al., 2008). The RPI BLENDER KBP system (Yu et al., 2014) casts SFV in this framework, using a graph propagation method that modeled the credibility of systems, sources, and response values. However they only report scores on the 2013 SFV data which contain less complicated and easier queries compared to the 2014 data. Therefore, we cannot directly compare our system’s performance to theirs. Google’s Knowledge Vault system (Dong et al., 2014) combines the output of four diverse extraction methods by building a boosted decision stump classifier (Reyzin and Schapire, 2006). For each proposed fact, the clas</context>
</contexts>
<marker>Yin, Han, Yu, 2008</marker>
<rawString>Xiaoxin Yin, Jiawei Han, and Philip S Yu. 2008. Truth discovery with multiple conflicting information providers on the web. Knowledge and Data Engineering, IEEE Transactions on, 20(6):796–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dian Yu</author>
<author>Hongzhao Huang</author>
<author>Taylor Cassidy</author>
<author>Heng Ji</author>
<author>Chi Wang</author>
<author>Shi Zhi</author>
<author>Jiawei Han</author>
<author>Clare Voss</author>
<author>Malik Magdon-Ismail</author>
</authors>
<title>The wisdom of minority: Unsupervised slot filling validation based on multidimensional truth-finding.</title>
<date>2014</date>
<booktitle>In Proc. The 25th International Conference on Computational Linguistics (COLING2014).</booktitle>
<contexts>
<context position="2178" citStr="Yu et al., 2014" startWordPosition="330" endWordPosition="333">art results on this KBP task using stacking (Wolpert, 1992), which trains a final classifier to optimally combine the results of multiple systems. We present results for stacking all systems that competed in both the 2013 and 2014 KBP-ESF tracks, training ∗ These authors contributed equally on 2013 data and testing on 2014 data. The resulting stacked ensemble outperforms all systems in the 2014 competition, obtaining an F1 of 48.6% compared to 39.5% for the best performing system in the most recent competition. Although the associated KBP Slot Filler Validation (SFV) Track (Wang et al., 2013; Yu et al., 2014; Sammons et al., 2014) is officially focused on improving the precision of individual existing systems by filtering their results, frequently participants in this track also combine the results of multiple systems and also report increased recall through this use of ensembling. However, SFV participants have not employed stacking, and we demonstrate that our stacking approach outperforms existing published SFV ensembling systems. KBP ESF systems must also provide provenance information, i.e. each extracted slot-filler must include a pointer to a document passage that supports it (Surdeanu and</context>
<context position="5950" citStr="Yu et al., 2014" startWordPosition="920" endWordPosition="923">om documents. Given the diverse set of slot-filling systems available, it is interesting to explore methods for ensembling these systems. In this regard, TAC also conducts a Slot Filler Validation (SFV) task who goal is to improve the slot-filling performance using the output of existing systems. The input for this task is the set of outputs from all slotfilling systems and the expected output is a filtered set of slot fills. As with the ESF task, participating systems employ a variety of techniques to perform validation. For instance, RPI BLENDER used a Multi-dimensional Truth Finding model (Yu et al., 2014) which is an unsupervised validation approach based on computing multidimensional credibility scores. The UI CCG system (Sammons et al., 2014) developed two different validation systems using entailment and majority voting. However, stacking (Sigletos et al., 2005; Wolpert, 1992) has not previously been employed for ensembling KBP-ESF systems. In stacking, a meta-classifier is learned from the output of multiple underlying systems. In our work, we translate this to the context of ensembling slot filling sys3 Overview of KBP Slot Filling Task The goal of the TAC KBP-ESF task (Surdeanu, 2013; Su</context>
<context position="32238" citStr="Yu et al., 2014" startWordPosition="5333" endWordPosition="5336"> system aggregates the results of many different extractors using a constraint optimization framework, exploiting confidence values reported by each input system (Wang et al., 2013). A second approach in the UI CCG system (Sammons et al., 2014) aggregates results of multiple systems by using majority voting. In the database, web-search, and data-mining communities, a line of research into “truthfinding” or “truth-discovery” methods addresses the related problem of combining evidence for facts from multiple sources, each with a latent credibility (Yin et al., 2008). The RPI BLENDER KBP system (Yu et al., 2014) casts SFV in this framework, using a graph propagation method that modeled the credibility of systems, sources, and response values. However they only report scores on the 2013 SFV data which contain less complicated and easier queries compared to the 2014 data. Therefore, we cannot directly compare our system’s performance to theirs. Google’s Knowledge Vault system (Dong et al., 2014) combines the output of four diverse extraction methods by building a boosted decision stump classifier (Reyzin and Schapire, 2006). For each proposed fact, the classifier considers both the confidence value of </context>
</contexts>
<marker>Yu, Huang, Cassidy, Ji, Wang, Zhi, Han, Voss, Magdon-Ismail, 2014</marker>
<rawString>Dian Yu, Hongzhao Huang, Taylor Cassidy, Heng Ji, Chi Wang, Shi Zhi, Jiawei Han, Clare Voss, and Malik Magdon-Ismail. 2014. The wisdom of minority: Unsupervised slot filling validation based on multidimensional truth-finding. In Proc. The 25th International Conference on Computational Linguistics (COLING2014).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>