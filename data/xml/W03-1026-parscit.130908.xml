<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000082">
<title confidence="0.955379">
HowtogetaChineseName(Entity): Segmentation and Combination Issues
</title>
<author confidence="0.9225095">
Hongyan Jing Radu Florian Xiaoqiang Luo
Tong Zhang Abraham Ittycheriah
</author>
<affiliation confidence="0.794696">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.764">
Yorktown Heights, NY 10598
</address>
<email confidence="0.998032">
hjing,raduf,xiaoluo,tzhang,abei @us.ibm.com
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896380952381">
When building a Chinese named entity
recognition system, one must deal with
certain language-specific issues such as
whether the model should be based on
characters or words. While there is no
unique answer to this question, we discuss
in detail advantages and disadvantages of
each model, identify problems in segmen-
tation and suggest possible solutions, pre-
senting our observations, analysis, and
experimental results. The second topic
of this paper is classifier combination.
We present and describe four classifiers
for Chinese named entity recognition and
describe various methods for combining
their outputs. The results demonstrate that
classifier combination is an effective tech-
nique of improving system performance:
experiments over a large annotated corpus
of fine-grained entity types exhibit a 10%
relative reduction in F-measure error.
</bodyText>
<sectionHeader confidence="0.999334" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927282608695">
Named entity (NE) recognition has drawn much at-
tention in recent years. It was a designated task
in a number of conferences, including the Mes-
sage Understanding Conferences (MUC-6, 1995;
MUC-7, 1997), the Information Retrieval and Ex-
traction Conference (IREX, 1999), the Conferences
on Natural Language Learning (Tjong Kim Sang,
2002; Tjong Kim Sang and De Meulder, 2003),
and the recent Automatic Content Extraction Con-
ference (ACE, 2002).
A variety of algorithms have been proposed for
NE recognition. Many of these algorithms are, in
principle, language-independent. However, when
applying these algorithms to languages such as
Chinese and Japanese, we must deal with cer-
tain language-specific issues: for example, should
we build a character-based model or a word-based
model? how do word segmentation errors affect NE
recognition? how should word segmentation and NE
recognition interact with each other? Besides word
segmentation related issues, Chinese does not have
capitalization, which is a very useful feature in iden-
tifying NEs in languages such as English, Spanish,
or Dutch. How does the lack of features such as cap-
italization affect the performance?
In the first part of this paper, we discuss these
language-specific issues in Chinese NE recogni-
tion. In particular, we use a hidden Markov model
(HMM) system as an example, and discuss various
issues related to applying the HMM classifier to Chi-
nese. The HMM classifier is similar to the one de-
scribed in (Bikel et al., 1999).
In the second part of this paper, we investigate
the combination of a set of diverse NE recognition
classifiers. Four statistical classifiers are combined
in the experiments, including the above-mentioned
hidden Markov model classifier, a transformation-
based learning classifier (Brill, 1995; Florian and
Ngai, 2001), a maximum entropy classifier (Ratna-
parkhi, 1999), and a robust risk minimization classi-
fier (Zhang et al., 2002).
The remainder of this paper is organized as fol-
lows: Section 2 describes the experiment data, Sec-
tion 3 discusses specific issues related to Chinese
NE recognition, Section 4 presents the four classi-
fiers and approaches to combining these classifiers.
</bodyText>
<sectionHeader confidence="0.987563" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.964079833333333">
We used three annotated Chinese corpora in our ex-
periments.
The IBM-FBIS Corpus
The Foreign Broadcast Information Service (FBIS)
offers an extensive collection of translations and
transcriptions of open source information monitored
worldwide on diverse topics such as military af-
fairs, politics, economics, and science and technol-
ogy. The IBM-FBIS corpus consists of approxi-
mately 3,000 Chinese articles obtained from FBIS
(about 3.2 million Chinese characters in total). This
corpus was tagged by a native Chinese speaker with
32 NE categories, such as person, location, organi-
zation, country, people, date, time, percentage, car-
dinal, ordinal, product, substance, and salutation.
There are approximately 300,000 NEs in the entire
corpus, 16% of which are labeled as person, 16% as
organization, and 11% as location.
</bodyText>
<subsectionHeader confidence="0.705913">
The IBM-CT Corpus
</subsectionHeader>
<bodyText confidence="0.999866545454545">
The Chinese Treebank (Xia et al., 2000), avail-
able from Linguistic Data Consortium, consists of
a 100,000 word (approximately 160,000 characters)
corpus annotated with word segmentation, part-of-
speech tags, and syntactic bracketing. It includes
325 articles from Xinhua newswire between 1994
and 1998. The same Chinese annotator who worked
on the above IBM-FBIS data also annotated the Chi-
nese Treebank data with NE information, henceforth
the IBM-CT corpus, using the same 32 categories as
mentioned above.
</bodyText>
<subsectionHeader confidence="0.682404">
The IEER data
</subsectionHeader>
<bodyText confidence="0.99958">
The National Institute of Standard and Technol-
ogy organized the Information Extraction – Entity
Recognition (IEER) evaluation, which involves en-
tity recognition from textual information sources in
both English and Mandarin. The Mandarin training
data consists of approximately 10 hours of broad-
cast news transcripts comprised of approximately
390 stories. The test data also contains transcripts of
broadcast news1. The training data includes approx-
imately 170,000 characters and the test data includes
approximately 6,500 characters. Ten categories of
NEs were annotated, such as person, location, orga-
nization, date, duration, and measure.
</bodyText>
<footnote confidence="0.461228">
1Other types of test data were also used in IEER evaluation,
including newswire text and real automatic speech recognition
transcripts, but we did not use them in our experiments.
</footnote>
<sectionHeader confidence="0.978339" genericHeader="method">
3 Language-Specific Issues in Chinese NE
Recognition
</sectionHeader>
<bodyText confidence="0.997842571428571">
Chinese does not have delimiters between words,
so a key design issue in Chinese NE recognition
is whether to build a character-based model or a
word-based model. In this section, we use a hid-
den Markov model NE recognition system as an ex-
ample to discuss language-specific issues in Chinese
NE recognition.
</bodyText>
<subsectionHeader confidence="0.999382">
3.1 The Hidden Markov Model Classifier
</subsectionHeader>
<bodyText confidence="0.999959625">
NE recognition can be formulated as a classification
task, where the goal is to label each token with a
tag indicating whether it belongs to a specific NE
or is not part of any NE. The HMM classifier used
in our experiments follows the algorithm described
in (Bikel et al., 1999). It performs sequence clas-
sification by assigning each token either one of the
NE types or the label “O” to represent “outside any
NE”. The states in the HMM are organized into re-
gions, one region for each type of NE plus one for
“O”. Within each of the regions, a statistical lan-
guage model is used to compute the likelihood of
words occurring within that region. The transition
probabilities are smoothed by deleted interpolation,
and the decoding is performed using the Viterbi al-
gorithm.
</bodyText>
<subsectionHeader confidence="0.999441">
3.2 Character-Based, Word-Based, and
Class-Based Models
</subsectionHeader>
<bodyText confidence="0.99990415">
To build a model for identifying Chinese NEs, we
need to determine the basic unit of the model: char-
acter or word. On one hand, the word-based model
is attractive since it allows the system to inspect a
larger window of text, which may lead to more in-
formative decisions. On the other hand, a word seg-
menter is not error-prone and these errors may prop-
agate and result in errors in NE recognition.
Two systems, a character-based HMM model and
a word-based HMM model, were built for compar-
ison. The word segmenter used in our experiments
relies on dictionaries and surrounding words in lo-
cal context to determine the word boundaries. Dur-
ing training, the NE boundaries were provided to the
word segmenter; the latter is restricted to enforce
word boundaries at each entity boundary. Therefore,
at training time, the word boundaries are consistent
with the entity boundaries. At test time, however,
the segmenter could create words which do not agree
with the gold-standard entity boundaries.
</bodyText>
<table confidence="0.998901142857143">
Corpus Model Prec Rec
Character 74.36% 80.24% 77.19
IBM- Word 72.46% 75.97% 74.17
FBIS Class 72.74% 76.20% 74.43
Character 74.57% 78.01% 76.25
IEER Word 77.51% 65.22% 70.83
Class 77.21% 64.36% 70.20
</table>
<tableCaption confidence="0.999444">
Table 1: Performance of the character-based HMM
</tableCaption>
<bodyText confidence="0.98007865">
model, the word-based HMM model, and the class-
based HMM model. (The precision, recall, and P-
measure presented in this table and throughout this
paper are based on correct identification of all the
attributes of an NE, including boundary, content,
and type.)
The performance of the character-based model
and the word-based model are shown in Table 1. The
two corpora used in the evaluation, the IBM-FBIS
corpus and the IEER corpus, differ greatly in data
size and the number of NE types. The IBM-FBIS
training data consists of 3.1 million characters and
the corresponding test data has 270,000 characters.
As we can see from the table, for both corpora, the
character-based model outperforms the word-based
model, with a lead of 3 to 5.5 in F-measure. The per-
formance gap between two models is larger for the
IEER data than for the IBM-FBIS data.
We also built a class-based NE model. After word
segmentation, class tags such as number, chinese-
name, foreign-name, date, and percent are used to
replace words belonging to these classes. Whether
a word belongs to a specific class is identified by
a rule-based normalizer. The performance of the
class-based HMM model is also shown in Table 1.
For the IBM-FBIS corpus, the class-based model
outperforms the word-based model; for the IEER
corpus, the class-based model is worse than the
word-based model. In both cases, the performance
difference between the word-based model and the
class-based model is very small. The character-
based model outperforms the class-based model in
both tests.
A more careful analysis indicates that although
the word-based model performs worse than the
character-based model overall in our evaluation, it
performs better for certain NE types. For instance,
the word-based model has a better performance
for the organization category than the character-
based model in both tests. While the character-
based model has an F-measure of 65.07 (IBM-FBIS)
and 64.76 (IEER) for the organization category,
the word-based model achieves F-measure scores of
69.14 (IBM-FBIS) and 72.38 (IEER) respectively.
One reason may be that organization names tend to
contain many characters, and since the word-based
model allows the system to analyze a larger window
of text, it is more likely to make a correct guess.
We can integrate the character-based model and the
word-based model by combining the decisions from
the two models. For instance, if we use the de-
cisions of the word-based model for the organiza-
tion category, but use the decisions of the character-
based model for all the other categories, the over-
all F-measure goes up to 76.91 for the IEER data,
higher than using either the character-based or word-
based model alone. Another way to integrate the
two models is to use a hybrid model – starting with
a word-based model and backing off to character-
based model if the word is unknown.
</bodyText>
<subsectionHeader confidence="0.999851">
3.3 Granularity of Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999987596491228">
We believe that one main reason for the lower per-
formance of the word-based model is that the word
granularity defined by the word segmenter is not
suitable for the HMM model to perform the NE
recognition task. What exactly constitutes a Chinese
word has been a topic of major debate. We are in-
terested in what is the best word granularity for our
particular task.
To illustrate the word granularity problem for NE
tagging, we take person names as an example. Our
word segmenter marks a person’s name as one word,
consistent with the convention used by the Chinese
treebank and many other word segmentation sys-
tems. While this may be useful in other applications,
it is certainly not a good choice for our NE model.
Chinese names typically contain two or three char-
acters, with family name preceding first name. Only
a limited set of characters are used as family names,
while the first name can be any character(s). There-
fore, the family name is a very important and use-
ful feature in identifying an NE in the person cate-
gory. By combining the family name and the first
name into one word, this important feature is lost to
the word-based model. In our tests, the word-based
model performs much worse for the person category
than the character-based model. We believe that, for
the purpose of NE recognition, it is better to separate
the family name from the first name in word segmen-
tation, although this is not the convention used in the
Chinese treebank.
Other examples include the segmentation of
words indicating dates, countries, locations, percent-
ages, measures, and ordinals. For instance, “July
4th” is expressed by four characters “7th month 4th
day” in Chinese. The word segmenter marks the
four characters as a single word; however, the sec-
ond and the last character are actually good features
for indicating date, since the dates are usually ex-
pressed using the same structure (e.g., “March 25th”
is expressed by “3rd month 25th day” in Chinese).
For reasons similar to the above, we believe that it
is better to separate characters representing “month”
and “day”, rather than combining the four charac-
ters into one word. A similar problem can be ob-
served in English with tokens such as “61-year-old
man” if one is interested in identifying a person’s
age, in which case ’year’ and ’old’ are good features
for predication.
The above analysis suggests that a better way to
apply a word segmenter in an NE system is to first
adapt the segmenter so that the segmentation granu-
larity is more appropriate to the particular task and
model. As a guideline, characters that are good fea-
tures for identifying NEs should not be combined
with other characters into word. Additional ex-
amples include characters expressing “percent” and
characters representing monetary measures.
</bodyText>
<subsectionHeader confidence="0.956917">
3.4 The Effect of Segmentation Errors
</subsectionHeader>
<bodyText confidence="0.9999696">
Word segmentation errors can lead to mistakes in
NE recognition. Suppose an NE consists of four
characters , if the word segmen-
tation merges with a character preceding it,
then this NE cannot be correctly identified by the
word-based model since the boundary will be incor-
rect. Besides inducing NE boundary errors, incor-
rect word segmentation also leads to wrong match-
ings between training examples and testing exam-
ples, which may result in mistakes in identifying en-
tities.
We computed the upper bound for the word-based
model for the IBM-FBIS test presented in Table 1.
The upper bound of performance is computed by
dividing the total number of NEs whose bound-
aries are also recognized as boundaries by the word
segmenter by the total number of NEs in the cor-
pus, which is the precision, recall, and also the F-
measure. For the IBM-FBIS test data in Table 1,
the upper bound of the word-based model is 95.7 F-
measure.
We also did the following experiment to measure
the effect of word segmentation errors: we gave
the boundaries of NEs in the test data to the word
segmenter and forced it to mark entity boundaries
as word boundaries. This eliminates the word seg-
mentation errors that inevitably result in NE bound-
ary errors. For the IBM-FBIS data, the word-based
HMM model achieves 76.60 F-measure when the
entity boundaries in the test data are given, and the
class-based model achieves 77.77 F-measure, higher
than the 77.19 F-measure by the character-based
model in Table 1. For the IEER data, the F-measure
of the word-based model improves from 70.83 to
73.74 when the entity boundaries are given, and the
class-based model improves from 70.20 to 72.47.
This suggests that with the improvement in Chi-
nese word segmentation, the word-based model may
achieve comparable or better performance than the
character-based model.
</bodyText>
<subsectionHeader confidence="0.947254">
3.5 Lexical Features
</subsectionHeader>
<bodyText confidence="0.999918428571428">
Capitalization in English gives good evidence of
names. Our HMM classifier for English uses a set
of word-features to indicate whether a word con-
tains all capitalized letters, only digits, or capital-
ized letters and period, as described in (Bikel et al.,
1999). However, Chinese does not have capitaliza-
tion. When we applied the HMM system to Chinese,
we retained such features since Chinese text also in-
clude digits and roman words (such as in product
or company names). In an attempt to investigate the
usefulness of such features for Chinese, we removed
them from the system and observed very little dif-
ference in overall performance (0.4 difference in F-
measure).
</bodyText>
<subsectionHeader confidence="0.960364">
3.6 Sensitivity to Corpus and Training Size
Variation
</subsectionHeader>
<bodyText confidence="0.99978425">
To test the robustness of the model, we trained the
system on the 100,000 word IBM-CT data and tested
on the same IBM-FBIS data. The character-based
model achieves 61.36 F-measure and the word-
based model achieves 58.40 F-measure, compared
to 77.19 and 74.17, respectively, using the 20 times
larger IBM-FBIS training set. This represents an ap-
proximately 20% relative reduction in performance
when trained on a related yet different and consider-
ably smaller training set. We plan to investigate fur-
ther the relation between corpus type and size and
performance.
</bodyText>
<sectionHeader confidence="0.994617" genericHeader="method">
4 Classifier Combination
</sectionHeader>
<bodyText confidence="0.99960175">
This section investigates the combination of a set of
classifiers for NE recognition. We first introduce the
classifiers used in our experiments and then describe
the combination methods.
</bodyText>
<subsectionHeader confidence="0.989825">
4.1 The Classifiers
</subsectionHeader>
<bodyText confidence="0.999512666666667">
Besides the HMM classifier mentioned in the previ-
ous section, the following three classifiers were used
in the experiments.
</bodyText>
<subsubsectionHeader confidence="0.404211">
4.1.1 The Transformation-Based Learning
</subsubsectionHeader>
<bodyText confidence="0.95680475">
(fnTBL) Classifier
Transformation-based learning is an error-driven
algorithm which has two major steps: it starts by
assigning some classification to each example, and
then automatically proposing, evaluating and select-
ing the classification changes that maximally de-
crease the number of errors.
TBL has some attractive qualities that make it
suitable for the language-related tasks: it can au-
tomatically integrate heterogeneous types of knowl-
edge, without the need for explicit modeling (similar
to Snow (Dagan et al., 1997), Maximum Entropy,
decision trees, etc); it is error–driven, thus directly
minimizes the ultimate evaluation measure: the er-
ror rate. The TBL toolkit used in this experiment is
described in (Florian and Ngai, 2001).
</bodyText>
<subsubsectionHeader confidence="0.545498">
4.1.2 The Maximum Entropy Classifier
</subsubsectionHeader>
<bodyText confidence="0.980983071428571">
(MaxEnt)
The model used here is based on the maxi-
mum entropy model used for shallow parsing (Rat-
naparkhi, 1999). A sentence with NE tags is
converted into a shallow tree: tokens not in any
NE are assigned an “O” tag, while tokens within
an NE are represented as constituents whose la-
bel is the same as the NE type. For exam-
ple, the annotated sentence “I will fly to (LO-
CATION New York) (DATEREF tomorrow)” is
represented as a tree “(S I/O will/O fly/O to/O
(LOCATION New/LOCATION York/LOCATION)
(DATEREF tomorrow/DATEREF) )”. Once an NE
is represented as a shallow tree, NE recognition can
be realized by performing shallow parsing.
We use the tagging and chunking model described
in (Ratnaparkhi, 1999) for shallow parsing. In the
tagging model, the context consists of a window of
five tokens (including the token being tagged and
two tokens to its left and two tokens to its right) and
two tags to the left of the current token. Five groups
of feature templates are used: token unigram, token
bigram, token trigram, tag unigram and tag bigram
(all within the context window). In the chunking
model, the context is limited to a window of three
subtrees: the previous, current and next subtree. Un-
igram and bigram chunk (or tag) labels are used as
features.
</bodyText>
<subsectionHeader confidence="0.706069">
4.1.3 The Robust Risk Minimization (RRM)
Classifier
</subsectionHeader>
<bodyText confidence="0.999921857142857">
This system is a variant of the text chunking
system described in Zhang et al. (2002), where the
NE recognition problem is regarded as a sequential
token-based tagging problem. We denote by
(✛ ) the sequence of tokenized text,
which is the input to our system. In token-based
tagging, the goal is to assign a class-label to ev-
ery token .
In our system, this is achieved by estimating the
conditional probability for every pos-
sible class-label value , where is a feature vector
associated with token . The feature vector can
depend on previously predicted class-labels
where is the truncation
of into the interval . is a linear weight
vector and is a constant. Parameters and
can be estimated from the training data.
This classification method is based on approxi-
mately minimizing the risk function. The general-
ized Winnow method used in (Zhang et al., 2002)
implements such a robust risk minimization method.
</bodyText>
<subsectionHeader confidence="0.540599">
4.1.4 Performance of the Classifiers
</subsectionHeader>
<bodyText confidence="0.999903222222222">
We compared the performance of the four classi-
fiers by training and testing them on the same data
sets. We divided the IBM-FBIS corpus into three
subsets: 2.8 million characters for training, 330,000
characters for development testing, and 330,000
characters for testing. Table 2 shows the results of
each classifier for the development test set and the
evaluation set. The RRM and fnTBL classifiers are
the best performers for the test set, followed by Max-
Ent. The HMM classifier lags behind by around 6
,
but the dependency is typically assumed to be lo-
cal. Given such a conditional probability model,
in the decoding stage, we estimate the best possi-
ble sequence of ’s using a dynamic programming
approach.
In our system, the conditional probability model
has the following parametric form:
</bodyText>
<table confidence="0.999621375">
Development Test Test
Precision Recall F-measure Precision Recall F-measure
Baseline1 17.52% 24.92% 20.58 14.04% 20.52% 16.67
Baseline2 77.86 % 45.26% 57.24 71.06% 40.22% 51.37
HMM 71.38% 80.92% 75.85 71.73% 77.55% 74.53
MaxEnt 83.82% 78.08% 80.85 85.43% 75.22% 80.00
fnTBL 76.85% 81.55% 80.08 82.06% 80.68% 81.36
RRM 82.83% 81.06% 81.89 84.53% 78.64% 81.48
</table>
<tableCaption confidence="0.998628">
Table 2: Baselines and performance of the four classifiers.
</tableCaption>
<bodyText confidence="0.9993297">
points in F-measure from the best system. The pre-
sented results are for character-based models.
For comparison, we also computed two baselines:
one in which each character is labeled with its most
frequent label (Baseline1 in Table 2), and one in
which each entity that was seen in training data is
labeled with its most frequent classification (Base-
line2 in Table 2 - this baseline is computed using
the software provided with the CoNLL-2003 shared
task (Tjong Kim Sang and De Meulder, 2003)).
</bodyText>
<subsectionHeader confidence="0.992496">
4.2 Combination
</subsectionHeader>
<bodyText confidence="0.99990025">
The four classifiers differ in multiple dimensions,
making them good candidates for combination. We
explored various ways to combine the results from
the four classifiers.
</bodyText>
<subsectionHeader confidence="0.617398">
4.2.1 Combination algorithms
</subsectionHeader>
<bodyText confidence="0.9841693">
For the first part of the experimental setup, we
consider the following classification framework:
given the (probabilistic) output of classi-
fiers , the classifier combination problem
can be viewed a probability interpolation problem
– compute the class probability distribution condi-
tioned on the joint classifier output:
where is the ith classifier’s output, is an ob-
servable context (e.g., a word trigram) and is a
combination function. A commonly used combining
scheme is through linear interpolation of the classi-
fiers’ class probability distributions:
is an estimation of the probability that
the correct classification is , given that the output
of the classifier on context is . These param-
eters from Equation (2) can be estimated, if needed,
on development data.
Table 3 presents the combination results, for
different ways of estimating the interpolation pa-
rameters. A simple combination method is the
equal voting method (van Halteren et al., 2001;
Tjong Kim Sang et al., 2000), where the parameters
are computed as and
, being the Kronecker operator:
In other words, each of the classifiers votes with
equal weight for the class that is most likely under
its model, and the class receiving the largest num-
ber of votes wins (i.e., it is selected as the classifi-
cation output). However, this procedure may lead to
ties, where some classifications receive an identical
number of votes – one usually resorts to randomly
selecting one of the tied candidates in this case – Ta-
ble 3 presents the average results obtained by this
method, together with the variance obtained over 30
trials. To make the decision deterministically, the
weights associated with the classifiers can be cho-
sen as . In this method,
presented in Table 3 as weighted voting, better per-
forming classifiers will have a higher impact on the
final classification.
In the previously described methods, also known
as voting, each classifier gave its entire vote to one
classification – its own output. However, Equa-
tion (2) allows for classifiers to give partial credit
to alternative classifications, through the probabil-
ity . In the experiments described here,
this value is computed directly on the development
data. However, the space of possible choices for ,
and is large enough to make the estimation
The weights encode the importance given to
</bodyText>
<table confidence="0.988539">
classifier in combination, for the context , and
Precision Recall F-measure
Best Classifier 84.53 78.64 81.48
Equal weight voting 85.2 0.03 81.7 0.02 83.3 0.02
Weighted voting 86.04 80.63 83.24
Model 1 83.07 82.10 82.58
Model 2 86.3 77.55 81.69
RRM 89.01 79.61 84.05
RRM+ flags 88.96 79.84 84.18
RRM+IOB1+IOB2 88.5 80.46 84.29
Oracle 91.6 92.3 91.95
</table>
<tableCaption confidence="0.999524">
Table 3: Classifier combination results.
</tableCaption>
<bodyText confidence="0.999298666666667">
unreliable, so we use two approximations, named
Model 1 and Model 2 in Table 3:
2 and , respec-
tively. Both probability distributions are estimated
as smoothed relative frequencies on the develop-
ment data. Interestingly, both methods underper-
form the equal voting method, a fact which can be
explained by inspecting the results in Table 2: the
fnTBL method has an accuracy (computed on devel-
opment data) lower than the MaxEnt accuracy, but
it outperforms the latter on the test data. Since the
parameters are computed on the devel-
opment data, they are probably favoring the Max-
Ent method, resulting in lower performance. On the
other hand, the equal voting method does not suffer
from this problem, as its parameters are not depen-
dent on the development data. In a last set of exper-
iments, we extend the classification framework to a
larger space, in which we compute the conditional
class probability distribution conditioned on an ar-
bitrary set of features:
</bodyText>
<equation confidence="0.42328">
(3)
</equation>
<bodyText confidence="0.999942619047619">
This setup allows us to still use the classifications
of individual systems as features, but also allows for
other types of conditioning features – for instance,
one can use the output of any classifier (e.g., POS
tags, text chunk labels, etc) as features.
In the described experiments, we use the RRM
method to compute the function in Equation (3),
allowing the system to select a good performing
combination of features. At training time, the sys-
tem was fed the output of each classifier on the de-
velopment data, and, in subsequent experiments, the
system was also fed aflag stream which briefly iden-
tifies some of the tokens (numbers, romanized char-
acters, etc) and the output of each system in a differ-
ent NE encoding scheme.
In all the voting experiments, the NEs were en-
coded in an IOB1 scheme, since it seems to be the
most amenable to combination. Briefly, the IOB
general encoding scheme associates a label with
each word, corresponding to whether it begins a spe-
cific entity, continues the entity, or is outside any en-
tity. Tjong Kim Sang and Veenstra (1999) describes
in detail the IOB schemes. The final experiment also
has access to the output of systems trained in the
IOB2 encoding. The addition of each feature type
resulted in better performance, with the final result
yielding a 10% relative decrease of F-measure error
when compared with the best performing system3.
Table 3 also includes an upper-bound on the classi-
fier combination performance - the performance of
the switch oracle, which selects the correct classifi-
cation if at least one classifier outputs it.
Table 3 shows that, at least for the examined
types of combination, using a robust feature-based
classifier to compute the classification distribution
yields better performance than combining the clas-
sifications through either voting or weighted inter-
polation. The RRM-based classifier is able to in-
corporate heterogenous information from multiple
sources, obtaining a 2.8 absolute F-measure im-
provement versus the best performing classifier and
1.0 F-measure gain over the next best method.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.998759166666667">
Sun et al. (2002) proposes to use a class-based
model for Chinese NE recognition. Specifically, it
uses a character-based trigram model for the class
person, a word-based model for the class location,
and a more complicated model for the class organi-
zation. This decision is consistent with our observa-
tion that the character-based model performs better
than the word-based model for classes such as per-
son, but is worse for classes such as organization.
Sekine and Eriguchi (2000) provides an overview
of Japanese NE recognition. It presents the re-
sults of 15 systems that participated in an evalua-
tion project for Information Retrieval and Informa-
tion Extraction (IREX, 1999). Utsuro et al. (2002)
studies combining outputs of multiple Japanese NE
systems by stacking. A second stage classifier – in
this case, a decision list – is trained to combine the
outputs from first stage classifiers. This is similar
</bodyText>
<footnote confidence="0.9919355">
2 is the word associated with the context .
3Measured as .
</footnote>
<bodyText confidence="0.999842357142857">
in spirit to our application of the RRM classifier for
combining classifier outputs.
Classifier combination has been shown
to be effective in improving the perfor-
mance of NLP applications, and have been
investigated by Brill and Wu (1998) and
van Halteren et al. (2001) for part-of-speech tag-
ging, Tjong Kim Sang et al. (2000) for base noun
phrase chunking, and Florian et al. (2003a) for
word sense disambiguation. Among the investigated
techniques were voting, probability interpolation,
and classifier stacking. We also applied the classifier
combination technique discussed in this paper to
English and German (Florian et al., 2003b).
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999984">
In this paper, we discuss two topics related to Chi-
nese NE recognition: dealing with language-specific
issues such as word segmentation, and combining
multiple classifiers to enhance the system perfor-
mance. In the described experiments, the character-
based model consistently outperforms the word-
based model – one major reason for this fact is that
the segmentation granularity might not be suited for
this particular task. Combining four statistical clas-
sifiers, including a hidden Markov model classifier,
a transformation-based learning classifier, a maxi-
mum entropy classifier, and a robust risk minimiza-
tion classifier, significantly improves the system per-
formance, yielding a 10% relative reduction in F-
measure error over the best performing classifier.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999979066666667">
We would like to specially thank Fei Xia for help-
ful discussions and for providing the Chinese word
segmenter. We also thank Weijing Zhu for his as-
sistance in data preparation, and Nanda Kambhatla,
Todd Ward and Salim Roukos for their support and
suggestions.
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916
and by the Knowledge Discovery and Dissemination
Project sponsored by the National Science Founda-
tion. The views and findings contained in this ma-
terial are those of the authors and do not necessarily
reflect the position of policy of the Government and
no official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.996158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843625">
ACE. 2002. Automatic content extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what’s in a name. Machine Learning,
34(1-3):211–231.
E. Brill and J. Wu. 1998. Classifier combination for improved
lexical disambiguation. Proceedings of COLING-ACL’98,
pages 191–195, August.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543–565.
I. Dagan, Y. Karov, and D. Roth. 1997. Mistake-driven learning
in text categorization. In Proceedings ofEMNLP-1997.
R. Florian and G. Ngai, 2001. Fast Transformation-
Based Learning Toolkit. Johns Hopkins University,
http://nlp.cs.jhu.edu/˜rflorian/fntbl/documentation.html.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky. 2003a.
Combining classifiers for word sense disambiguation. Jour-
nal ofNatural Language Engineering, 8(4).
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003b. Named
entity recognition through classifier combination. In Pro-
ceedings of CoNLL-2003. Edmonton, Canada.
IREX. 1999. Information retrieval and extraction exercise.
http://nlp.cs.nyu.edu/irex/index-e.html.
MUC-6. 1995. The sixth message understanding conference.
http://www.cs.nyu.edu/cs/faculty/grishman/muc6.html.
MUC-7. 1997. The seventh message understanding con-
ference. http://www.itl.nist.gov/iad/894.02/related projects/
muc/proceedings/muc 7 toc.html.
A. Ratnaparkhi. 1999. Learning to parse natural language with
maximum entropy models. Machine Learning, 34:151–178.
S. Sekine and Y. Eriguchi. 2000. Japanese named entity ex-
traction evaluation - analysis of results. In Proceedings of
COLING-2000, Saarbr¨uecken, Germany.
J. Sun, J. Gao, L. Zhang, M. Zhou, and C. Huang. 2002. Chi-
nese named entity identification using class-based language
model. In Proceedings of COLING-2002, Taiwan.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent named
entity recognition. In Proceedings of CoNLL-2003.
E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text
chunks. In Proceedings of EACL’99.
E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling,
Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Ap-
plying system combination to base noun phrase identifica-
tion. In Proceedings of COLING 2000, pages 857–863.
E. F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002
shared task: Language-independent named entity recogni-
tion. In Proceedings of CoNLL-2002. Taiwan.
T. Utsuro, M. Sassano, and K. Uchimoto. 2002. Combining out-
puts of multiple Japanese named entity chunkers by stacking.
In Proceedings ofEMNLP-2002, Pennsylvania.
H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improv-
ing accuracy in word class tagging through the combination
fo machine learning systems. Computational Linguistics,
27(2):199–230.
F. Xia, M. Palmer, N. Xue, M..E. Okurowski, J. Kovarik,
S. Huang, T. Kroch, and M. Marcus. 2000. Developing
guidelines and ensuring consistency for Chinese text annota-
tion. In Proceedings of the 2nd International Conference on
Language Resources and Evaluation, Athens, Greece.
T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text chunking
based on a generalization of Winnow. Journal of Machine
Learning Research, 2:615–637.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.748027">
<title confidence="0.999978">HowtogetaChineseName(Entity): Segmentation and Combination Issues</title>
<author confidence="0.9084165">Hongyan Jing Radu Florian Xiaoqiang Luo Tong Zhang Abraham Ittycheriah</author>
<affiliation confidence="0.998369">IBM T.J. Watson Research</affiliation>
<address confidence="0.970749">Yorktown Heights, NY 10598</address>
<email confidence="0.983356">hjing,raduf,xiaoluo,tzhang,abei@us.ibm.com</email>
<abstract confidence="0.998138318181818">When building a Chinese named entity recognition system, one must deal with certain language-specific issues such as whether the model should be based on characters or words. While there is no unique answer to this question, we discuss in detail advantages and disadvantages of each model, identify problems in segmentation and suggest possible solutions, presenting our observations, analysis, and experimental results. The second topic of this paper is classifier combination. We present and describe four classifiers for Chinese named entity recognition and describe various methods for combining their outputs. The results demonstrate that classifier combination is an effective technique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>Automatic content extraction.</title>
<date>2002</date>
<note>http://www.ldc.upenn.edu/Projects/ACE/.</note>
<contexts>
<context position="1553" citStr="ACE, 2002" startWordPosition="219" endWordPosition="220">f improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error. 1 Introduction Named entity (NE) recognition has drawn much attention in recent years. It was a designated task in a number of conferences, including the Message Understanding Conferences (MUC-6, 1995; MUC-7, 1997), the Information Retrieval and Extraction Conference (IREX, 1999), the Conferences on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), and the recent Automatic Content Extraction Conference (ACE, 2002). A variety of algorithms have been proposed for NE recognition. Many of these algorithms are, in principle, language-independent. However, when applying these algorithms to languages such as Chinese and Japanese, we must deal with certain language-specific issues: for example, should we build a character-based model or a word-based model? how do word segmentation errors affect NE recognition? how should word segmentation and NE recognition interact with each other? Besides word segmentation related issues, Chinese does not have capitalization, which is a very useful feature in identifying NEs</context>
</contexts>
<marker>ACE, 2002</marker>
<rawString>ACE. 2002. Automatic content extraction. http://www.ldc.upenn.edu/Projects/ACE/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>R L Schwartz</author>
<author>R M Weischedel</author>
</authors>
<title>An algorithm that learns what’s in a name.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2606" citStr="Bikel et al., 1999" startWordPosition="385" endWordPosition="388">ecognition interact with each other? Besides word segmentation related issues, Chinese does not have capitalization, which is a very useful feature in identifying NEs in languages such as English, Spanish, or Dutch. How does the lack of features such as capitalization affect the performance? In the first part of this paper, we discuss these language-specific issues in Chinese NE recognition. In particular, we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese. The HMM classifier is similar to the one described in (Bikel et al., 1999). In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers. Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformationbased learning classifier (Brill, 1995; Florian and Ngai, 2001), a maximum entropy classifier (Ratnaparkhi, 1999), and a robust risk minimization classifier (Zhang et al., 2002). The remainder of this paper is organized as follows: Section 2 describes the experiment data, Section 3 discusses specific issues related to Chinese NE recognition, S</context>
<context position="6158" citStr="Bikel et al., 1999" startWordPosition="930" endWordPosition="933"> does not have delimiters between words, so a key design issue in Chinese NE recognition is whether to build a character-based model or a word-based model. In this section, we use a hidden Markov model NE recognition system as an example to discuss language-specific issues in Chinese NE recognition. 3.1 The Hidden Markov Model Classifier NE recognition can be formulated as a classification task, where the goal is to label each token with a tag indicating whether it belongs to a specific NE or is not part of any NE. The HMM classifier used in our experiments follows the algorithm described in (Bikel et al., 1999). It performs sequence classification by assigning each token either one of the NE types or the label “O” to represent “outside any NE”. The states in the HMM are organized into regions, one region for each type of NE plus one for “O”. Within each of the regions, a statistical language model is used to compute the likelihood of words occurring within that region. The transition probabilities are smoothed by deleted interpolation, and the decoding is performed using the Viterbi algorithm. 3.2 Character-Based, Word-Based, and Class-Based Models To build a model for identifying Chinese NEs, we ne</context>
<context position="15763" citStr="Bikel et al., 1999" startWordPosition="2532" endWordPosition="2535">ER data, the F-measure of the word-based model improves from 70.83 to 73.74 when the entity boundaries are given, and the class-based model improves from 70.20 to 72.47. This suggests that with the improvement in Chinese word segmentation, the word-based model may achieve comparable or better performance than the character-based model. 3.5 Lexical Features Capitalization in English gives good evidence of names. Our HMM classifier for English uses a set of word-features to indicate whether a word contains all capitalized letters, only digits, or capitalized letters and period, as described in (Bikel et al., 1999). However, Chinese does not have capitalization. When we applied the HMM system to Chinese, we retained such features since Chinese text also include digits and roman words (such as in product or company names). In an attempt to investigate the usefulness of such features for Chinese, we removed them from the system and observed very little difference in overall performance (0.4 difference in Fmeasure). 3.6 Sensitivity to Corpus and Training Size Variation To test the robustness of the model, we trained the system on the 100,000 word IBM-CT data and tested on the same IBM-FBIS data. The charac</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An algorithm that learns what’s in a name. Machine Learning, 34(1-3):211–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL’98,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="29120" citStr="Brill and Wu (1998)" startWordPosition="4700" endWordPosition="4703"> an evaluation project for Information Retrieval and Information Extraction (IREX, 1999). Utsuro et al. (2002) studies combining outputs of multiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim Sang et al. (2000) for base noun phrase chunking, and Florian et al. (2003a) for word sense disambiguation. Among the investigated techniques were voting, probability interpolation, and classifier stacking. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al., 2003b). 6 Conclusion In this paper, we discuss two topics related to Chinese NE recognition: dealing with language-specific issues such as word segmentation, and combining multiple classifiers to enhance the</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. Proceedings of COLING-ACL’98, pages 191–195, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="2899" citStr="Brill, 1995" startWordPosition="428" endWordPosition="429">irst part of this paper, we discuss these language-specific issues in Chinese NE recognition. In particular, we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese. The HMM classifier is similar to the one described in (Bikel et al., 1999). In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers. Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformationbased learning classifier (Brill, 1995; Florian and Ngai, 2001), a maximum entropy classifier (Ratnaparkhi, 1999), and a robust risk minimization classifier (Zhang et al., 2002). The remainder of this paper is organized as follows: Section 2 describes the experiment data, Section 3 discusses specific issues related to Chinese NE recognition, Section 4 presents the four classifiers and approaches to combining these classifiers. 2 Data We used three annotated Chinese corpora in our experiments. The IBM-FBIS Corpus The Foreign Broadcast Information Service (FBIS) offers an extensive collection of translations and transcriptions of op</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>Y Karov</author>
<author>D Roth</author>
</authors>
<title>Mistake-driven learning in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings ofEMNLP-1997.</booktitle>
<contexts>
<context position="17709" citStr="Dagan et al., 1997" startWordPosition="2832" endWordPosition="2835">wing three classifiers were used in the experiments. 4.1.1 The Transformation-Based Learning (fnTBL) Classifier Transformation-based learning is an error-driven algorithm which has two major steps: it starts by assigning some classification to each example, and then automatically proposing, evaluating and selecting the classification changes that maximally decrease the number of errors. TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling (similar to Snow (Dagan et al., 1997), Maximum Entropy, decision trees, etc); it is error–driven, thus directly minimizes the ultimate evaluation measure: the error rate. The TBL toolkit used in this experiment is described in (Florian and Ngai, 2001). 4.1.2 The Maximum Entropy Classifier (MaxEnt) The model used here is based on the maximum entropy model used for shallow parsing (Ratnaparkhi, 1999). A sentence with NE tags is converted into a shallow tree: tokens not in any NE are assigned an “O” tag, while tokens within an NE are represented as constituents whose label is the same as the NE type. For example, the annotated sente</context>
</contexts>
<marker>Dagan, Karov, Roth, 1997</marker>
<rawString>I. Dagan, Y. Karov, and D. Roth. 1997. Mistake-driven learning in text categorization. In Proceedings ofEMNLP-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>G Ngai</author>
</authors>
<date>2001</date>
<institution>Fast TransformationBased Learning Toolkit. Johns Hopkins University,</institution>
<note>http://nlp.cs.jhu.edu/˜rflorian/fntbl/documentation.html.</note>
<contexts>
<context position="2924" citStr="Florian and Ngai, 2001" startWordPosition="430" endWordPosition="433">this paper, we discuss these language-specific issues in Chinese NE recognition. In particular, we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese. The HMM classifier is similar to the one described in (Bikel et al., 1999). In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers. Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformationbased learning classifier (Brill, 1995; Florian and Ngai, 2001), a maximum entropy classifier (Ratnaparkhi, 1999), and a robust risk minimization classifier (Zhang et al., 2002). The remainder of this paper is organized as follows: Section 2 describes the experiment data, Section 3 discusses specific issues related to Chinese NE recognition, Section 4 presents the four classifiers and approaches to combining these classifiers. 2 Data We used three annotated Chinese corpora in our experiments. The IBM-FBIS Corpus The Foreign Broadcast Information Service (FBIS) offers an extensive collection of translations and transcriptions of open source information mon</context>
<context position="17923" citStr="Florian and Ngai, 2001" startWordPosition="2865" endWordPosition="2868"> assigning some classification to each example, and then automatically proposing, evaluating and selecting the classification changes that maximally decrease the number of errors. TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling (similar to Snow (Dagan et al., 1997), Maximum Entropy, decision trees, etc); it is error–driven, thus directly minimizes the ultimate evaluation measure: the error rate. The TBL toolkit used in this experiment is described in (Florian and Ngai, 2001). 4.1.2 The Maximum Entropy Classifier (MaxEnt) The model used here is based on the maximum entropy model used for shallow parsing (Ratnaparkhi, 1999). A sentence with NE tags is converted into a shallow tree: tokens not in any NE are assigned an “O” tag, while tokens within an NE are represented as constituents whose label is the same as the NE type. For example, the annotated sentence “I will fly to (LOCATION New York) (DATEREF tomorrow)” is represented as a tree “(S I/O will/O fly/O to/O (LOCATION New/LOCATION York/LOCATION) (DATEREF tomorrow/DATEREF) )”. Once an NE is represented as a shal</context>
</contexts>
<marker>Florian, Ngai, 2001</marker>
<rawString>R. Florian and G. Ngai, 2001. Fast TransformationBased Learning Toolkit. Johns Hopkins University, http://nlp.cs.jhu.edu/˜rflorian/fntbl/documentation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>S Cucerzan</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2003</date>
<journal>Journal ofNatural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="29264" citStr="Florian et al. (2003" startWordPosition="4726" endWordPosition="4729">tiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim Sang et al. (2000) for base noun phrase chunking, and Florian et al. (2003a) for word sense disambiguation. Among the investigated techniques were voting, probability interpolation, and classifier stacking. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al., 2003b). 6 Conclusion In this paper, we discuss two topics related to Chinese NE recognition: dealing with language-specific issues such as word segmentation, and combining multiple classifiers to enhance the system performance. In the described experiments, the characterbased model consistently outperforms the wordbased model – one major reason for </context>
</contexts>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2003</marker>
<rawString>R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky. 2003a. Combining classifiers for word sense disambiguation. Journal ofNatural Language Engineering, 8(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>T Zhang</author>
</authors>
<title>Named entity recognition through classifier combination.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="29264" citStr="Florian et al. (2003" startWordPosition="4726" endWordPosition="4729">tiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim Sang et al. (2000) for base noun phrase chunking, and Florian et al. (2003a) for word sense disambiguation. Among the investigated techniques were voting, probability interpolation, and classifier stacking. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al., 2003b). 6 Conclusion In this paper, we discuss two topics related to Chinese NE recognition: dealing with language-specific issues such as word segmentation, and combining multiple classifiers to enhance the system performance. In the described experiments, the characterbased model consistently outperforms the wordbased model – one major reason for </context>
</contexts>
<marker>Florian, Ittycheriah, Jing, Zhang, 2003</marker>
<rawString>R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003b. Named entity recognition through classifier combination. In Proceedings of CoNLL-2003. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IREX</author>
</authors>
<title>Information retrieval and extraction exercise.</title>
<date>1999</date>
<note>http://nlp.cs.nyu.edu/irex/index-e.html.</note>
<contexts>
<context position="1379" citStr="IREX, 1999" startWordPosition="192" endWordPosition="193"> for Chinese named entity recognition and describe various methods for combining their outputs. The results demonstrate that classifier combination is an effective technique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error. 1 Introduction Named entity (NE) recognition has drawn much attention in recent years. It was a designated task in a number of conferences, including the Message Understanding Conferences (MUC-6, 1995; MUC-7, 1997), the Information Retrieval and Extraction Conference (IREX, 1999), the Conferences on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), and the recent Automatic Content Extraction Conference (ACE, 2002). A variety of algorithms have been proposed for NE recognition. Many of these algorithms are, in principle, language-independent. However, when applying these algorithms to languages such as Chinese and Japanese, we must deal with certain language-specific issues: for example, should we build a character-based model or a word-based model? how do word segmentation errors affect NE recognition? how should word segmentation </context>
<context position="28589" citStr="IREX, 1999" startWordPosition="4613" endWordPosition="4614">NE recognition. Specifically, it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organization. This decision is consistent with our observation that the character-based model performs better than the word-based model for classes such as person, but is worse for classes such as organization. Sekine and Eriguchi (2000) provides an overview of Japanese NE recognition. It presents the results of 15 systems that participated in an evaluation project for Information Retrieval and Information Extraction (IREX, 1999). Utsuro et al. (2002) studies combining outputs of multiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim</context>
</contexts>
<marker>IREX, 1999</marker>
<rawString>IREX. 1999. Information retrieval and extraction exercise. http://nlp.cs.nyu.edu/irex/index-e.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<title>The sixth message understanding conference.</title>
<date>1995</date>
<note>http://www.cs.nyu.edu/cs/faculty/grishman/muc6.html.</note>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. The sixth message understanding conference. http://www.cs.nyu.edu/cs/faculty/grishman/muc6.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<title>The seventh message understanding conference.</title>
<date>1997</date>
<note>http://www.itl.nist.gov/iad/894.02/related projects/ muc/proceedings/muc 7 toc.html.</note>
<marker>MUC-7, 1997</marker>
<rawString>MUC-7. 1997. The seventh message understanding conference. http://www.itl.nist.gov/iad/894.02/related projects/ muc/proceedings/muc 7 toc.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--151</pages>
<contexts>
<context position="2974" citStr="Ratnaparkhi, 1999" startWordPosition="438" endWordPosition="440">n Chinese NE recognition. In particular, we use a hidden Markov model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese. The HMM classifier is similar to the one described in (Bikel et al., 1999). In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers. Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformationbased learning classifier (Brill, 1995; Florian and Ngai, 2001), a maximum entropy classifier (Ratnaparkhi, 1999), and a robust risk minimization classifier (Zhang et al., 2002). The remainder of this paper is organized as follows: Section 2 describes the experiment data, Section 3 discusses specific issues related to Chinese NE recognition, Section 4 presents the four classifiers and approaches to combining these classifiers. 2 Data We used three annotated Chinese corpora in our experiments. The IBM-FBIS Corpus The Foreign Broadcast Information Service (FBIS) offers an extensive collection of translations and transcriptions of open source information monitored worldwide on diverse topics such as militar</context>
<context position="18073" citStr="Ratnaparkhi, 1999" startWordPosition="2891" endWordPosition="2893">se the number of errors. TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling (similar to Snow (Dagan et al., 1997), Maximum Entropy, decision trees, etc); it is error–driven, thus directly minimizes the ultimate evaluation measure: the error rate. The TBL toolkit used in this experiment is described in (Florian and Ngai, 2001). 4.1.2 The Maximum Entropy Classifier (MaxEnt) The model used here is based on the maximum entropy model used for shallow parsing (Ratnaparkhi, 1999). A sentence with NE tags is converted into a shallow tree: tokens not in any NE are assigned an “O” tag, while tokens within an NE are represented as constituents whose label is the same as the NE type. For example, the annotated sentence “I will fly to (LOCATION New York) (DATEREF tomorrow)” is represented as a tree “(S I/O will/O fly/O to/O (LOCATION New/LOCATION York/LOCATION) (DATEREF tomorrow/DATEREF) )”. Once an NE is represented as a shallow tree, NE recognition can be realized by performing shallow parsing. We use the tagging and chunking model described in (Ratnaparkhi, 1999) for sha</context>
</contexts>
<marker>Ratnaparkhi, 1999</marker>
<rawString>A. Ratnaparkhi. 1999. Learning to parse natural language with maximum entropy models. Machine Learning, 34:151–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>Y Eriguchi</author>
</authors>
<title>Japanese named entity extraction evaluation - analysis of results.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<location>Saarbr¨uecken, Germany.</location>
<contexts>
<context position="28393" citStr="Sekine and Eriguchi (2000)" startWordPosition="4580" endWordPosition="4583">ning a 2.8 absolute F-measure improvement versus the best performing classifier and 1.0 F-measure gain over the next best method. 5 Related Work Sun et al. (2002) proposes to use a class-based model for Chinese NE recognition. Specifically, it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organization. This decision is consistent with our observation that the character-based model performs better than the word-based model for classes such as person, but is worse for classes such as organization. Sekine and Eriguchi (2000) provides an overview of Japanese NE recognition. It presents the results of 15 systems that participated in an evaluation project for Information Retrieval and Information Extraction (IREX, 1999). Utsuro et al. (2002) studies combining outputs of multiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has</context>
</contexts>
<marker>Sekine, Eriguchi, 2000</marker>
<rawString>S. Sekine and Y. Eriguchi. 2000. Japanese named entity extraction evaluation - analysis of results. In Proceedings of COLING-2000, Saarbr¨uecken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sun</author>
<author>J Gao</author>
<author>L Zhang</author>
<author>M Zhou</author>
<author>C Huang</author>
</authors>
<title>Chinese named entity identification using class-based language model.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING-2002,</booktitle>
<contexts>
<context position="27929" citStr="Sun et al. (2002)" startWordPosition="4506" endWordPosition="4509">itch oracle, which selects the correct classification if at least one classifier outputs it. Table 3 shows that, at least for the examined types of combination, using a robust feature-based classifier to compute the classification distribution yields better performance than combining the classifications through either voting or weighted interpolation. The RRM-based classifier is able to incorporate heterogenous information from multiple sources, obtaining a 2.8 absolute F-measure improvement versus the best performing classifier and 1.0 F-measure gain over the next best method. 5 Related Work Sun et al. (2002) proposes to use a class-based model for Chinese NE recognition. Specifically, it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organization. This decision is consistent with our observation that the character-based model performs better than the word-based model for classes such as person, but is worse for classes such as organization. Sekine and Eriguchi (2000) provides an overview of Japanese NE recognition. It presents the results of 15 systems that participated in an evaluation project for I</context>
</contexts>
<marker>Sun, Gao, Zhang, Zhou, Huang, 2002</marker>
<rawString>J. Sun, J. Gao, L. Zhang, M. Zhou, and C. Huang. 2002. Chinese named entity identification using class-based language model. In Proceedings of COLING-2002, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. F. Tjong Kim Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>J Veenstra</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proceedings of EACL’99.</booktitle>
<contexts>
<context position="26888" citStr="Sang and Veenstra (1999)" startWordPosition="4345" endWordPosition="4348"> system was fed the output of each classifier on the development data, and, in subsequent experiments, the system was also fed aflag stream which briefly identifies some of the tokens (numbers, romanized characters, etc) and the output of each system in a different NE encoding scheme. In all the voting experiments, the NEs were encoded in an IOB1 scheme, since it seems to be the most amenable to combination. Briefly, the IOB general encoding scheme associates a label with each word, corresponding to whether it begins a specific entity, continues the entity, or is outside any entity. Tjong Kim Sang and Veenstra (1999) describes in detail the IOB schemes. The final experiment also has access to the output of systems trained in the IOB2 encoding. The addition of each feature type resulted in better performance, with the final result yielding a 10% relative decrease of F-measure error when compared with the best performing system3. Table 3 also includes an upper-bound on the classifier combination performance - the performance of the switch oracle, which selects the correct classification if at least one classifier outputs it. Table 3 shows that, at least for the examined types of combination, using a robust </context>
</contexts>
<marker>Sang, Veenstra, 1999</marker>
<rawString>E. F. Tjong Kim Sang and J. Veenstra. 1999. Representing text chunks. In Proceedings of EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>W Daelemans</author>
<author>H Dejean</author>
<author>R Koeling</author>
<author>Y Krymolowsky</author>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>857--863</pages>
<contexts>
<context position="23130" citStr="Sang et al., 2000" startWordPosition="3714" endWordPosition="3717">ervable context (e.g., a word trigram) and is a combination function. A commonly used combining scheme is through linear interpolation of the classifiers’ class probability distributions: is an estimation of the probability that the correct classification is , given that the output of the classifier on context is . These parameters from Equation (2) can be estimated, if needed, on development data. Table 3 presents the combination results, for different ways of estimating the interpolation parameters. A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as and , being the Kronecker operator: In other words, each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins (i.e., it is selected as the classification output). However, this procedure may lead to ties, where some classifications receive an identical number of votes – one usually resorts to randomly selecting one of the tied candidates in this case – Table 3 presents the average results obtained by this method, together with the variance obtained over 30 tr</context>
<context position="29208" citStr="Sang et al. (2000)" startWordPosition="4716" endWordPosition="4719"> Utsuro et al. (2002) studies combining outputs of multiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim Sang et al. (2000) for base noun phrase chunking, and Florian et al. (2003a) for word sense disambiguation. Among the investigated techniques were voting, probability interpolation, and classifier stacking. We also applied the classifier combination technique discussed in this paper to English and German (Florian et al., 2003b). 6 Conclusion In this paper, we discuss two topics related to Chinese NE recognition: dealing with language-specific issues such as word segmentation, and combining multiple classifiers to enhance the system performance. In the described experiments, the characterbased model consistently</context>
</contexts>
<marker>Sang, Daelemans, Dejean, Koeling, Krymolowsky, Punyakanok, Roth, 2000</marker>
<rawString>E. F. Tjong Kim Sang, W. Daelemans, H. Dejean, R. Koeling, Y. Krymolowsky, V. Punyakanok, and D. Roth. 2000. Applying system combination to base noun phrase identification. In Proceedings of COLING 2000, pages 857–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002.</booktitle>
<contexts>
<context position="1447" citStr="Sang, 2002" startWordPosition="202" endWordPosition="203">r combining their outputs. The results demonstrate that classifier combination is an effective technique of improving system performance: experiments over a large annotated corpus of fine-grained entity types exhibit a 10% relative reduction in F-measure error. 1 Introduction Named entity (NE) recognition has drawn much attention in recent years. It was a designated task in a number of conferences, including the Message Understanding Conferences (MUC-6, 1995; MUC-7, 1997), the Information Retrieval and Extraction Conference (IREX, 1999), the Conferences on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), and the recent Automatic Content Extraction Conference (ACE, 2002). A variety of algorithms have been proposed for NE recognition. Many of these algorithms are, in principle, language-independent. However, when applying these algorithms to languages such as Chinese and Japanese, we must deal with certain language-specific issues: for example, should we build a character-based model or a word-based model? how do word segmentation errors affect NE recognition? how should word segmentation and NE recognition interact with each other? Besides word segmentati</context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>E. F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition. In Proceedings of CoNLL-2002. Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>M Sassano</author>
<author>K Uchimoto</author>
</authors>
<title>Combining outputs of multiple Japanese named entity chunkers by stacking.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP-2002,</booktitle>
<location>Pennsylvania.</location>
<contexts>
<context position="28611" citStr="Utsuro et al. (2002)" startWordPosition="4615" endWordPosition="4618">n. Specifically, it uses a character-based trigram model for the class person, a word-based model for the class location, and a more complicated model for the class organization. This decision is consistent with our observation that the character-based model performs better than the word-based model for classes such as person, but is worse for classes such as organization. Sekine and Eriguchi (2000) provides an overview of Japanese NE recognition. It presents the results of 15 systems that participated in an evaluation project for Information Retrieval and Information Extraction (IREX, 1999). Utsuro et al. (2002) studies combining outputs of multiple Japanese NE systems by stacking. A second stage classifier – in this case, a decision list – is trained to combine the outputs from first stage classifiers. This is similar 2 is the word associated with the context . 3Measured as . in spirit to our application of the RRM classifier for combining classifier outputs. Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al. (2001) for part-of-speech tagging, Tjong Kim Sang et al. (2000) fo</context>
</contexts>
<marker>Utsuro, Sassano, Uchimoto, 2002</marker>
<rawString>T. Utsuro, M. Sassano, and K. Uchimoto. 2002. Combining outputs of multiple Japanese named entity chunkers by stacking. In Proceedings ofEMNLP-2002, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through the combination fo machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improving accuracy in word class tagging through the combination fo machine learning systems. Computational Linguistics, 27(2):199–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M Palmer</author>
<author>N Xue</author>
<author>M E Okurowski</author>
<author>J Kovarik</author>
<author>S Huang</author>
<author>T Kroch</author>
<author>M Marcus</author>
</authors>
<title>Developing guidelines and ensuring consistency for Chinese text annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="4174" citStr="Xia et al., 2000" startWordPosition="622" endWordPosition="625">s such as military affairs, politics, economics, and science and technology. The IBM-FBIS corpus consists of approximately 3,000 Chinese articles obtained from FBIS (about 3.2 million Chinese characters in total). This corpus was tagged by a native Chinese speaker with 32 NE categories, such as person, location, organization, country, people, date, time, percentage, cardinal, ordinal, product, substance, and salutation. There are approximately 300,000 NEs in the entire corpus, 16% of which are labeled as person, 16% as organization, and 11% as location. The IBM-CT Corpus The Chinese Treebank (Xia et al., 2000), available from Linguistic Data Consortium, consists of a 100,000 word (approximately 160,000 characters) corpus annotated with word segmentation, part-ofspeech tags, and syntactic bracketing. It includes 325 articles from Xinhua newswire between 1994 and 1998. The same Chinese annotator who worked on the above IBM-FBIS data also annotated the Chinese Treebank data with NE information, henceforth the IBM-CT corpus, using the same 32 categories as mentioned above. The IEER data The National Institute of Standard and Technology organized the Information Extraction – Entity Recognition (IEER) ev</context>
</contexts>
<marker>Xia, Palmer, Xue, Okurowski, Kovarik, Huang, Kroch, Marcus, 2000</marker>
<rawString>F. Xia, M. Palmer, N. Xue, M..E. Okurowski, J. Kovarik, S. Huang, T. Kroch, and M. Marcus. 2000. Developing guidelines and ensuring consistency for Chinese text annotation. In Proceedings of the 2nd International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Zhang</author>
<author>F Damerau</author>
<author>D E Johnson</author>
</authors>
<title>Text chunking based on a generalization of Winnow.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--615</pages>
<contexts>
<context position="3038" citStr="Zhang et al., 2002" startWordPosition="448" endWordPosition="451"> model (HMM) system as an example, and discuss various issues related to applying the HMM classifier to Chinese. The HMM classifier is similar to the one described in (Bikel et al., 1999). In the second part of this paper, we investigate the combination of a set of diverse NE recognition classifiers. Four statistical classifiers are combined in the experiments, including the above-mentioned hidden Markov model classifier, a transformationbased learning classifier (Brill, 1995; Florian and Ngai, 2001), a maximum entropy classifier (Ratnaparkhi, 1999), and a robust risk minimization classifier (Zhang et al., 2002). The remainder of this paper is organized as follows: Section 2 describes the experiment data, Section 3 discusses specific issues related to Chinese NE recognition, Section 4 presents the four classifiers and approaches to combining these classifiers. 2 Data We used three annotated Chinese corpora in our experiments. The IBM-FBIS Corpus The Foreign Broadcast Information Service (FBIS) offers an extensive collection of translations and transcriptions of open source information monitored worldwide on diverse topics such as military affairs, politics, economics, and science and technology. The </context>
<context position="19358" citStr="Zhang et al. (2002)" startWordPosition="3112" endWordPosition="3115">ow of five tokens (including the token being tagged and two tokens to its left and two tokens to its right) and two tags to the left of the current token. Five groups of feature templates are used: token unigram, token bigram, token trigram, tag unigram and tag bigram (all within the context window). In the chunking model, the context is limited to a window of three subtrees: the previous, current and next subtree. Unigram and bigram chunk (or tag) labels are used as features. 4.1.3 The Robust Risk Minimization (RRM) Classifier This system is a variant of the text chunking system described in Zhang et al. (2002), where the NE recognition problem is regarded as a sequential token-based tagging problem. We denote by (✛ ) the sequence of tokenized text, which is the input to our system. In token-based tagging, the goal is to assign a class-label to every token . In our system, this is achieved by estimating the conditional probability for every possible class-label value , where is a feature vector associated with token . The feature vector can depend on previously predicted class-labels where is the truncation of into the interval . is a linear weight vector and is a constant. Parameters and can be est</context>
</contexts>
<marker>Zhang, Damerau, Johnson, 2002</marker>
<rawString>T. Zhang, F. Damerau, and D. E. Johnson. 2002. Text chunking based on a generalization of Winnow. Journal of Machine Learning Research, 2:615–637.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>