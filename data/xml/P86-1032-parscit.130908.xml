<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.8802565">
A MODEL OF PLAN INFERENCE THAT DISTINGUISHES
BETWEEN THE BELIEFS OF ACTORS AND OBSERVERS
</title>
<note confidence="0.38234575">
Martha E. Pollack
Artificial Intelligence Center
and
Center for the Study of Language and Information
</note>
<sectionHeader confidence="0.5514815" genericHeader="abstract">
SRI International
333 Ravenswood Avenue
Menlo Park, CA 94025
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9998071">
Existing models of plan inference (PI) in conversation have as-
sumed that the agent whose plan is being inferred (the actor)
and the agent drawing the inference (the observer) have iden-
tical beliefs about actions in the domain. I argue that this as-
sumption often results in failure of both the PI process and the
communicative process that PI is meant to support. In par-
ticular, it precludes the principled generation of appropriate
responses to queries that arise from invalid plans. I describe
a model of PI that abandons this assumption. It rests on an
analysis of plans as mental phenomena. Judgements that a
plan is invalid are associated with particular discrepancies be-
tween the beliefs that the observer ascribes to the actor when
the former believes that the latter has some plan, and the be-
liefs that the observer herself holds. I show that the content
of an appropriate response to a query is affected by the types
of any such discrepancies of belief judged to be present in the
plan inferred to underlie that query. The PI model described
here has been implemented in SPIRIT, a small demonstration
system that answers questions about the domain of computer
mail.
</bodyText>
<sectionHeader confidence="0.998081" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.971517640625">
The importance of plan inference (PI) in models of conversa-
tion has been widely noted in the computational-linguistics lit-
erature. Incorporating PI capabilities into systems that answer
users&apos; questions has enabled such systems to handle indirect
speech acts 1131, supply more information than is actually re-
quested in a query 121, provide helpful information in response
to a yes/no query answered in the negative [2], disambiguate
requests 1171, resolve certain forms of intersentential ellipsis
16,111, and handle such discourse phenomena as clarification
subdialogues [I1], and correction or &apos;debugging&amp;quot; subdialogues
The research reported in this paper has been made possible in part by
an IBM Graduate Fellowship, in part by a gift from the Systems Develop-
ment Foundation, and in part by support from the Defense Advanced Re-
search Projects Agency under Contract N00039-84-K-0078 with the Space
and Naval Warfare Command. The views and conclusions contained in
this document are those of the author and should not be interpreted as
representative of the official policies, either expressed or implied, of the
Defense Advanced Research Projects Agency or the United States Gov-
ernment.
I am grateful to Barbara Grosz, James Allen, Phil Cohen, Amy Lansky,
Candy Sidner and Bonnie Webber for their comments on an earlier draft.
[16,11].
The PI process in each of these systems, however, has as-
sumed that the agent whose plan is being inferred (to whom
I shall refer as the actor), and the agent drawing the infer-
ence (to whom I shall refer as the observer), have identical
beliefs about the actions in the domain. Thus, Allen&apos;s model,
which was one of the earliest accounts of PI in conversationl
and inspired a great deal of the work done subsequently, in-
cludes, as a typical PI rule, the following: &amp;quot;SBAW(P)
SBAW(ACT) if P is a precondition of ACT&amp;quot; [2, page 120].
This rule can be glossed as &amp;quot;if the system (observer) believes
that an agent (actor) wants some proposition P to be true,
then the system may draw the inference that the agent wants
to perform some action ACT of which P is a precondition.&amp;quot;
Note that it is left unstated precisely who it is—the observer
or the actor—that believes that P is a precondition of ACT.
If we take this to be a belief of the observer, it is not clear
that the latter will infer the actor&apos;s plan; on the other hand, if
we consider it to be a belief of the actor, it is unclear how the
observer comes to have direct access to it. In practice, there
is only a single set of operators relating preconditions and ac-
tions in Allen&apos;s system; the belief in question is regarded as
being both the actor&apos;s and the observer&apos;s.
In many situations, an assumption that the re.,1wt beliefs
of the actor are identical with those of the observer results
in failure not only of the PI process, but also of the commu-
nicative process that PI is meant to support. In particular, it
precludes the principled generation of appropriate responses
to queries that arise from invalid plans. In this paper, I report
on a model of PI in conversation that distinguishes between
the beliefs of the actor and those of the observer. The model
rests on an analysis of plans as mental phenomena: &amp;quot;having a
plan&apos; is analyzed as having a particular configuration of i.
liefs and intentions. Judgements that a plan is invalid are
associated with particular discrepancies between the beliefs
that the observer ascribes to the actor when the former be-
lieves that the latter has some plan, and the beliefs observer
herself holds. I give an account of different types of plan in-
validities, and show how this account provides an explanation
for certain regularities that are observable in cooperative re-
sponses to questions. The PI model described here has been
implemented in SPIRIT, a small demonstration system that
answers questions about the domain of computer mail. More
</bodyText>
<note confidence="0.428734">
&apos;Allen&apos;s article 121 summarizes his dissertation research Pl.
</note>
<page confidence="0.994194">
207
</page>
<bodyText confidence="0.941744">
extensive discussion of both the PI model and SPIRIT can be
found in my dissertation [14].
</bodyText>
<sectionHeader confidence="0.864999" genericHeader="method">
PLANS AS MENTAL PHENOMENA
</sectionHeader>
<bodyText confidence="0.999503771428572">
We can distinguish between two views of plans. As Bratman
[5, page 271] has observed, there is an ambiguity in speaking
of an agent&apos;s plan: &amp;quot;On the one hand, [this] could mean an
appropriate abstract structure—some sort of partial function
from circumstances to actions, perhaps. On the other hand,
[it] could mean an appropriate state of mind, one naturally
describable in terms of such structures.&amp;quot; We might call the
former sense the data-structure view of plans, and the latter
the mental phenomenon view of plans. Work in plan synthe-
sis (e.g., Fikes and Nilsson [8], Sacerdoti [15], Wilkins [18],
and Pednault [12]), has taken the data-structure view, con-
sidering plans to be structures encoding aggregates of actions
that, when performed in circumstances satisfying some speci-
fied preconditions, achieve some specified results. For the pur-
poses of PI, however, it is much more useful to adopt a mental
phenomenon view and consider plans to be particular configu-
rations of beliefs and intentions that some agent has. After all,
inferring another agent&apos;s plan means figuring out what actions
he &amp;quot;has in mind,&amp;quot; and he may well be wrong about the effects
of those intended actions.
Consider, for example, the plan I have to find out how Kathy
is feeling. Believing that Kathy is at the hospital, I plan to do
this by finding out the phone number of the hospital, calling
there, asking to be connected to Kathy&apos;s room, and finally
saying &amp;quot;How are you doing?&amp;quot; If, unbeknownst to me, Kathy
has already been discharged, then executing my plan will not
lead to my goal of finding out how she is feeling. For me to
have a plan to do # that consists of doing some collection
of actions H, it is not necessary that the performance of II
actually lead to the performance of #. What is necessary is
that I believe that its performance will do so. This insight is at
the core of a view of plans as mental phenomena; in this view
a plan &amp;quot;exists&amp;quot;—i.e., gains its status as a plan—by virtue of
the beliefs, as well as the intentions, of the person whose plan
it is.
</bodyText>
<listItem confidence="0.822030416666667">
Further consideration of our common-sense conceptions of
what it means to have a plan leads to the following analysis
[14, Chap. 3]2:
(P0) An agent G has a plan to do #, that consists in doing
some set of acts II, provided that
1. G believes that he can execute each act in II.
2. G believes that executing the acts in H will entail
the performance of #.
3. G believes that each act in H plays a role in his plan.
(See discussion below.)
4. G intends to execute each act in II.
5. G intends to execute H as a way of doing #.
</listItem>
<footnote confidence="0.9836985">
zAlthough this definition ignores some important issues of commitment
over time, as discussed by Bratman [4] and Cohen and Levesque 171, it is
sufficient to support the PI process needed for many question-answering
situations. This is because, in such situations, unexpected changes in
the world that would force a reconsideration of the actor&apos;s intentions can
usually be safely ignored.
</footnote>
<sectionHeader confidence="0.883026" genericHeader="method">
6. G intends each act in H to play a role in his plan.
</sectionHeader>
<bodyText confidence="0.991585024390244">
The notion of an act playing a role in a plan is defined in
terms of two relationships over acts: generation, in the sense
defined by Goldman [9], and enablement. Roughly, one act
generates another if, by performing the first, the agent also
does the second; thus, saying to Kathy &amp;quot;How are you doing?&amp;quot;
may generate asking her how she is feeling. Or, to take an
example from the computer-mail domain, typing DEL . at
the prompt for a computer mail system may generate deleting
the current message, which may in turn generate cleaning out
one&apos;s mail file. In contrast, one act enables the generation of a
second by a third if the first brings about circumstances that
are necessary for the generation. Thus, typing HEADER 15
may enable the generation of deleting the fifteenth message by
typing DEL ., because it makes message 15 be the current
message, to which `.&apos; refers.3 The difference between gener-
ation and enablement consists largely in the fact that, when
an act a generates an act #, the agent need only do a, and 11
will automatically be done also. However, when a enables the
generation of some / by /3, the agent needs to do something
more than just a to have done either # or In this paper,
I consider only the inference of a restricted subset of plans,
which I shall call simple plans. An agent has a simple plan if
and only if he believes that all the acts in that plan play a role
in it by generating another act; i.e., if it includes no acts that
he believes are related to one another by enablement.
It is important to distinguish between types of actions (act-
types), such as typing DEL ., and actions themselves, such
as my typing DEL . right now. Actions or acts—I will use
the two terms interchangeably—can be thought of as triples
of act-type, agent, and time. Generation is a relation over
actions, not over act-types. Not every case of an agent typing
DEL . will result in the agent deleting the current message;
for example, my typing it just now did not, because I was not
typing it to a computer mail system. Similarly, executability—
the relation expressed in Clause (1) of (PO) as &amp;quot;can execute&amp;quot;—
applies to actions, and the objects of an agent&apos;s intentions are,
in this model, also actions.
Using the representation language specified in my thesis [14],
which builds upon Allen&apos;s interval-based temporal logic [3], the
conditions on G&apos;s having a simple plan to do # can be encoded
as follows:
</bodyText>
<equation confidence="0.9094582">
(P1) SIMPLE-PLAN(G,a„,(ai,..., t1)■-•
(i) BEL(G,EXEC(ai,G,t2),4), for i = 1,...,n A
(ii) BEL(G,GEN(a1,a41,G,t2),t1), for i = 1,... ,n-1 A
(iii) INT(G,04,12, t1), for i = 1,... , n A
(iv) INT(G,by(ai,ai+i), 12,4), for i = 1,...,n-1
</equation>
<bodyText confidence="0.994398571428571">
The left-hand side of (P1) denotes that the agent G has, at
time 4, a simple plan to do a„, consisting of doing the set of
acts (al, , n_iat t2. Note that all these are simultaneous
acts; this is a consequence of the restriction to simple plans.
The right-hand side of (P1) corresponds directly to (PO), ex-
cept that, in keeping with the restriction to simple plans, spe-
cific assertions about each act generating another replace the
</bodyText>
<footnote confidence="0.93885225">
8Enablement here thus differs from the usual binary relation in which
one action enables another. Since this paper does not further consider
plans with enabling actions, the advantages of the alternative definition
will not be discussed.
</footnote>
<page confidence="0.995985">
208
</page>
<bodyText confidence="0.999862083333333">
more general statement regarding the fact that each act plays
a role in the plan. The relation BEL(G,P,t) should be taken
to mean that agent G believes proposition P throughout time
interval t; INT(G,a, t2, ti) means that at time t1 G intends
to do a at t2. The relation EXEC(a,G,t) is true if and only
if the act of G doing a at t is executable, and the relation
GEN(a, G , t) is true if and only if the act of G doing a at
t generates the act of G doing fl at t. The function by maps
two act-type terms into a third act-type term: if an agent G
intends to do by(a„ 8), then G intends to do the complex act
fl-by-a, i.e., he intends to do a in order to do f/. Further dis-
cussion of these relations and functions can be found in Pollack
114, Chap. 4].
Clause (i) of (131) captures clause (1) of (P0).4 Clause (ii) of
(131) captures both clauses (2) and (3) of (P0): when I takes
the value n-1, clause (ii) of (P1) captures the requirement,
stated in clause (2) of (P0), that G believes his acts will entail
his goal; when i takes values between 1 and n-2, it captures
the requirement of clause (3) of (PO), that G believes each of
his acts plays a role in his plan. Similarly, clause (iii) of (P1)
captures clause (4) of (PO), and clause (iv) of (P1) captures
clauses (5) and (6) of (PO).
(P1) can be used to state what it means for an actor to have
an invalid simple plan: G has an invalid simple plan if and
only if he has the configuration of beliefs and intentions listed
in (P1), where one or more of those beliefs is incorrect, and,
consequently, one or more of the intentions is unrealizable. The
correctness of the actor&apos;s beliefs thus determines the validity
of his plan: if all the beliefs that are part of his plan are
correct, then all the intentions in it are realizable, and the
plan is valid. Validity in this absolute sense, however, is not of
primary concern in modeling plan inference in conversation.
What is important here is rather the observer&apos;s judgement
of whether the actor&apos;s plan is valid. It is to the analysis of
such invalidity judgements, and their effect on the question.
answering process, that we now turn.
</bodyText>
<sectionHeader confidence="0.9855695" genericHeader="method">
PLAN INFERENCE IN
QUESTION-ANSWERING
</sectionHeader>
<bodyText confidence="0.986103666666667">
Models of the question-answering process often include a claim
that the respondent (R) must infer the plans of the questioner
(Q). So R is the observer, and Q the actor. Building on the
analysis of plans as mental phenomena, we can say that, if R.
believes that she has inferred Q&apos;s plan, there is some set of be-
liefs and intentions satisfying (P1) that R believes Q has (or is
at least likely to have). Then there are particular discrepancies
that may arise between the beliefs that R ascribes to Q when
she believes he has some plan, and the beliefs that R herself
holds. Specifically, R may not herself believe one or more of
the beliefs, corresponding to Clauses (i) and (ii) of (P1), that
she ascribes to Q. We can associate such discrepancies with
&apos;In fact, it captures more: to encode Clause (i) of (P0), the parameter
I in Clause (i) of (P1) need only vary between 1 and n-1. However, given
the relationship between EXEC and GEN specified in Pollack 1141, namely
</bodyText>
<equation confidence="0.350454">
EX EC(ot,G,t) A GEN(cr,#,G,t) EX EC($,G,t)
</equation>
<bodyText confidence="0.950849695652174">
the instance of Clause (i) of (P1) with i=n is a consequence of the instance
of Clause (i) with i=n-1 and the instance of Clause (ii) with i=n-1. A
similar argument can be made about Clause (iii).
R&apos;s judgement that the plan she has inferred is invalid.5 The
type of any invalidities, defined in terms of the clauses of (P1)
that contain the discrepant beliefs, can be shown to influence
the content of a cooperative response. However, they do not
fully determine it: the plan inferred to underlie a query, along
with any invalidities it is judged to have, are but two factors
affecting the response-generation process, the most significant
others being factors of relevance and salience.
I will illustrate the effect of invalidity judgements on re-
sponse content with a query of the form &amp;quot;I want to perform
an act of fi, so I need to find out how to perform an act of a,&apos;
in which the goal is explicit, as in example (1) below&apos;:
(1) &amp;quot;I want to prevent Tom from reading my mail file. How
can I set the permissions on it to faculty-read only?&amp;quot;
In questions in which no goal is mentioned explicitly, analysis
depends upon inferring a plan leading to a goal that is rea-
sonable in the domain situation. Let us assume that, given
query (1), R has inferred that Q has the simple plan that con-
sists only in setting the permissions to faculty-read only, and
thereby directly preventing Tom from reading the file, i.e.:
</bodyText>
<equation confidence="0.743574">
(2)BEL(R,SIMPLE-PLAN(Q, prevent(mmfile,read,tom),
Iset-permissions(mmfile,read,facult y)],
g2, t1),
t1)
</equation>
<bodyText confidence="0.9999855">
Later in this paper, I will describe the process by which R can
come to have this belief. Bear in mind that, by (P1), (2) can
be expanded into a set of beliefs that R has about Q&apos;s beliefs
and intentions.
The first potential discrepancy is that R may believe to be
false some belief, corresponding to Clause (i) of (P1), that,
by virtue of (2), she ascribes to Q. In such a case, I will say
that she believes that some action in the inferred plan is an-
executable. Examples of responses in which R conveys this
information are (3) (in which R believes that at least one in-
tended act is tmexecutable) and (4) (in which R believes that
at least two intended acts are unexecutable):
</bodyText>
<listItem confidence="0.954032">
(3) &apos;There is no way for you to set the permissions on a file to
faculty-read only. What you can do is move it into a password-
protected subdirectory; that will prevent Tom from reading
it.&amp;quot;
(4) &apos;There is no way for you to set the permissions on a file
to faculty-read only, nor is there any way for you to prevent
Toni from reading it.&apos;
</listItem>
<bodyText confidence="0.9996649375">
&apos;This assumes that R always believes that her own beliefs are complete
and correct. Such an assumption is not an unreasonable one for question.
answering systems to make. More general conversational systems must
abandon this assumption, sometimes updating their own beliefs upon de-
tecting a discrepancy.
The analysis below is related to that provided by Joshi, Webber, and
Weischedel 1101. There are significant differences in my approach, how-
ever, which involve (i) a different structural analysis, which applies unez-
sestability to actions rather than plans and introduces incoherence (this
latter notion I define in the next section); (ii) a claim that the types of
invalldities (e.g., formedness, executability of the queried action, and ex-
ecutability of a goal action) are independent of one another; and (iii) a
claim that recognition of any invalidities, while necessary for determining
what information to include in an appropriate response, is not in itself
sufficient for this purpose. Also, Joshi et al. do not consider the question
of how invalid plans can be inferred.
</bodyText>
<page confidence="0.994007">
209
</page>
<bodyText confidence="0.9688685">
The discrepancy resulting in (3) is represented in (5); the dis-
crepancy in (4) is represented in (5) plus (6):
</bodyText>
<figure confidence="0.944338933333333">
(5) BEL( R,BEL(Q,EXEC(set-permissions(mmfile,read,facult y),
qtz),
t1),
ti)
A
BEL( R,-,EXEC(set-permissions(mmfile,read,facult y),
Q42),
t1)
(6)BEL(R,BEL(Q,EXEC(prevent(mmfile,read,tom),
qt2),
t1)
A
BEL(R,-,EXEC(prevent(mmfile,read,tom),
Q42),
ti)
</figure>
<bodyText confidence="0.9976055">
The second potential discrepancy is that R may believe false
some belief corresponding to Clause (ii) of (P1) that, by virtue
of (2), she ascribes to Q. I will then say that she believes the
plan to be ill-formed. In this case, her response may convey
that the intended acts in the plan will not fit together as ex-
pected, as in (7), which might be uttered if R believes it to be
mutually believed by R and Q that Tom is the system man-
ager:
(7) &amp;quot;Well, the command is SET PROTECTION = (Fac-
ulty:Read), but that won&apos;t keep Tom out: file permissions
don&apos;t apply to the system manager.&amp;quot;
The discrepancy resulting in (7) is (8):
</bodyText>
<equation confidence="0.759655111111111">
(8)BEL( R,BEL(Q,G EN (set-permissions(mmfile,read,facult y),
prevent(mmfile,read,tom),
Q,t2),
ti)
A
BEL(R,-.G EN (set-permissions(mmfile,read,facult y),
prevent(mmfile,read,tom),
Q,t2),
t1)
</equation>
<bodyText confidence="0.980303333333333">
Alternatively, there may be some combination of these dis-
crepancies between R&apos;s own beliefs and those that R attributes
to Q, as reflected in a response such as (9):
</bodyText>
<listItem confidence="0.961927333333333">
(9) &amp;quot;There is no way for you to set the permissions to faculty-
read only; and even if you could, it wouldn&apos;t keep Tom out:
file permissions don&apos;t apply to the system manager.&apos;
</listItem>
<bodyText confidence="0.998994166666667">
The discrepancies encoded in (5) and (8) together might result
in (9).
Of course, it is also possible that no discrepancy exists at
all, in which case I will say that R believes that Q&apos;s plan is
valid. A response such as (10) can be modeled as arising from
an inferred plan that R believes valid:
</bodyText>
<listItem confidence="0.929868">
(10) &amp;quot;Type SET PROTECTION = (Faculty:Read).&amp;quot;
</listItem>
<bodyText confidence="0.99996725">
Of the eight possible combinations of formedness, exe-
cutability of the queried act and executability of the goal act,
seven are possible: the only logically incompatible combina-
tion is a well-formed plan with an executable queried act, but
unexecutable goal act. This range of invalidities accounts for a
great deal of the information conveyed in naturally occurring
dialogues. But there is an important regularity that the PI
model does not yet explain.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="method">
A PROBLEM FOR PLAN
INFERENCE
</sectionHeader>
<bodyText confidence="0.966754444444444">
In all of the preceding cases, R has intuitively &amp;quot;made sense&amp;quot; of
Q&apos;s query, by determining some underlying plan whose com-
ponents she understands, though she may also believe that the
plan is flawed. For instance in (7), R has determined that Q
may mistakenly believe that, when one sets the permissions on
a file to allow a particular access to a particular group, no one
who is not a member of that group can gain access to the file.
This (incorrect) belief explains why Q believes that setting the
permissions will prevent Tom from reading the file.
There are also cases in which R may not even be able to
&amp;quot;make sense&amp;quot; of Q&apos;s query. As a somewhat whimsical example,
imagine Q saying:
(11) &amp;quot;I want to talk to Kathy, so I need to find out how to
stand on my head.&amp;quot;
In many contexts, a perfectly reasonable response to this query
is &amp;quot;Huh?&amp;quot;. Q&apos;s query is incoherent: R cannot understand why
Q believes that finding out how to stand on his head (or stand-
ing on his head) will lead to talking with Kathy. One can, of
course, construct scenarios in which Q&apos;s query makes perfect
sense: Kathy might, for example, be currently hanging by her
feet in gravity boots. The point here is not to imagine such
circumstances in which Q&apos;s query would be coherent, but in-
stead to realize that there are many circumstances in which it
would not.
The judgement that a query is incoherent is not the same as
a judgement that the plan inferred to underlie it is ill-formed.
To see this, contrast example (11) with the following:
</bodyText>
<listItem confidence="0.781283">
(12) want to talk to Kathy. Do you know the phone number
at the hospital?&apos;
</listItem>
<bodyText confidence="0.999580705882353">
Here, if R believes that Kathy has already been discharged
from the hospital, she may judge the plan she infers to underlie
Q&apos;s query to be ill-formed, and may inform him that calling
the hospital will not lead to talking to Kathy. She can even
inform him why the plan is ill-formed, namely, because Kathy
is no longer at the hospital. This differs from (11), in which R
cannot inform Q of the reason his plan is invalid, because she
cannot, on an intuitive level, even determine what his plan is.
Unfortunately, the model as developed so far does not dis-
tinguish between incoherence and ill-formedness. The reason
is that, given a reasonable account of semantic interpretation,
it is transparent from the query in (11) that Q intends to
talk to Kathy, intends to find out how to stand on his head,
and intends his doing the latter to play a role in his plan to
do the former and that he also believes that he can talk to
Kathy, believes that he can find out how to stand on his head,
and believes that his doing the latter will play a role in his
</bodyText>
<page confidence="0.995822">
210
</page>
<bodyText confidence="0.99986325">
plan to do the former.&apos; But these beliefs and intentions are
precisely what are required to have a plan according to (PO).
Consequently, after hearing (11), R can, in fact, infer a plan
underlying Q&apos;s query, namely the obvious one: to find out how
to stand on his head (or to stand on his head) in order to talk
to Kathy. Then, since R does not herself believe that the for-
mer act will lead to the latter, on the analysis so far given, we
would regard R as judging Q&apos;s plan to be ill-formed. But this
is not the desired analysis: the model should instead capture
the fact that R cannot make sense of Q&apos;s query here—that it
is incoherent.
Let us return to the set of examples about setting the per-
missions on a file, discussed in the previous section. In her se-
mantic interpretation of the query in (1), R may come to have
a number of beliefs about Q&apos;s beliefs and intentions. Specifi-
cally, all of the following may be true:
</bodyText>
<equation confidence="0.992429809523809">
(13) BEL(R,BEL(Q,EXEC(set-permissions(mmfile,read,facult y),
Q42),
JO,
ti)
(14)BEL(R,BEL(Q,EXEC(prevent(mmfile,read,tom),
Q42),
t1)
(15) BEL( R, B EL( Q ,G EN(set-permissions(mmfile,read,facult y),
prevent(mmfile,read,tom),
Q42),
ti),
ti )
(16)BEL(R,INT(Q,set-permissions(mmfile,read,facult y),
t1)
(17)BEL(R,INT(Q,prevent(mmfile,read,tom),
t241),
t1)
(18) BEL( R,INT(Q,by(set-permissions(mmfile,read,facult y),
prevent(mmfile,read,tom)),
t2,t1),
t1)
</equation>
<bodyText confidence="0.99901912">
Together, (13)-(18) are sufficient for R&apos;s believing that Q has
the simple plan as expressed in (2). This much is not surpris-
ing. In effect Q has stated in his query what his plan is—to
prevent Tom from reading the file by setting the permission on
it to faculty-read only—so, of course, R should be able to infer
just that. And if R further believes that the system manager
can override file permissions and that Tom is the system man-
ager, but also that Q does not know the former fact, R will
judge that Q&apos;s plan is ill-formed, and may provide a response
such as that in (7). There is a discrepancy here between the
belief R ascribes to Q in satisfaction of Clause (ii) of (Fl)—
namely, that expressed in (15)—and R&apos;s own beliefs about the
domain.
But what if It, instead of believing that it is mutually be-
lieved by Q and R that Tom is the system manager, believes
that they mutually believe that he is a faculty member? In
this case, (13)-(18) may still be true. However we do not want
to say that this case is indistinguishable from the previous one.
&apos;Actually, the requirement that Q have these beliefs may be slightly
too strong; see Pollack 114, Chap. 31 for discussion.
In the previous case, R understood the source of Q&apos;s erroneous
belief: she realized that Q did not know that the system man-
ager could override file protections, and therefore thought that,
by setting permissions to restrict access to a group that Tom is
not a member of, he could prevent Tom from reading the file.
In contrast, in the current case, R cannot really understand
Q&apos;s plan: she cannot determine why Q believes that he will
prevent Tom from reading the file by setting the permissions
on it to faculty-read only, given that Q believes that Tom is a
faculty member. This current case is like the case in (11): Q&apos;s
query is incoherent to R.
To capture the difference between ill-formedness and inco-
herence, I will claim that, when an agent R is asked a question
by an actor Q, R needs to attempt to ascribe to Q more than
just a set of beliefs and intentions satisfying (P1). Specifi-
cally, for each belief satisfying Clause (ii) of (P1), R must also
ascribe to Q another belief that explains the former in a cer-
tain specifiable way. The beliefs that satisfy Clause (ii) are
beliefs about the relation between two particular actions: for
instance, the plan underlying query (12) includes Q&apos;s belief
that his action of calling the hospital at t2 will generate his
action of establishing a communication channel to Kathy at
t2. This belief can be explained by a belief Q has about the
relation between the act-types &amp;quot;calling a location&amp;quot; and &amp;quot;estab-
lishing a communication channel to an agent.&amp;quot; Q may believe
that acts of the former type generate acts of the latter type
provided that the agent to whom the communication channel
is to be established is at the location to be called. Such a belief
can be encoded using the predicate CG EN, which can be read
&apos;conditionally generates,&apos; as follows:
</bodyText>
<equation confidence="0.534864">
(19)BEL(Q, CGEN(cal4X),establish-channel(Y),at(X,Y)), ti)
</equation>
<bodyText confidence="0.998011740740741">
The relation CGEN(a, C) is true if and only if acts of type ct
performed when condition C holds will generate acts of type P.
Thus, the sentence CG EN(a, 8, C) can be seen as one possible
interpretation of a hierarchical planning operator with header
)9, preconditions C, and body a. Conditional generation is a
relation between two act-types and a set of conditions; gener-
ation, which is a relation between two actions, can be defined
in terms of conditional generation.
In reasoning about (12), R can attribute to Q the belief ex-
pressed in (19), combined with a belief that Kathy will be at
the hospital at time t2. Together, these beliefs explain Q&apos;s be-
lief that, by calling the hospital at t2, he will establish a com-
munication channel to Kathy. Similarly, in reasoning about
query (1) in the case in which R does not believe that Q knows
that Tom is a faculty member, R can ascribe to Q the beliefs
that, by setting the permissions on a file to restrict access to a
particular group, one denies access to everyone who is neither
a member of that group nor the system manager, as expressed
in (20):
(20)BEL(R,BEL(Q,CGEN(set-permissions(X,P,Y),
prevent(X,P,Z),
-■member(Z,Y)),
tt)
She can also ascribe to Q the belief that Tom is not a mem-
ber of the faculty, (or more precisely, that Tom will not be a
member of the faculty at the intended performance time t2),
i.e.,
</bodyText>
<page confidence="0.995425">
211
</page>
<bodyText confidence="0.997363133333333">
(21)BEL(R,BEL(Q, HOLDS(-anember(tom,faculty),t2)41),t1)
The conjunction of these two beliefs explains Q&apos;s further belief,
expressed in (15), that, by setting the permissions to faculty-
read only at t2, he can prevent Tom from reading the file.
In contrast, in example (11), R has no basis for ascribing to
Q beliefs that will explain why he thinks that standing on his
head will lead to talking with Kathy. And, in the version of
example (1) in which R believes that Q believes that Tom is a
faculty member, R has no basis for ascribing to Q a belief that
explains Q&apos;s belief that setting the permissions to faculty-read
only will prevent Tom from reading the file.
Explanatory beliefs are incorporated in the PI model by the
introduction of explanatory plans, or eplans. Saying that an
agent R believes that another agent Q has some eplan is short-
hand for describing a set of beliefs possessed by R, specifically:
</bodyText>
<equation confidence="0.910879">
(P2) (R,EPLAN(Q,a,[at, • • • , an--11,[Pi,• • •
t2,ti),ti)
4-4
</equation>
<bodyText confidence="0.7036034">
BEL(R,BEL(Q,EXEC(ai,Q42)41)41),
for i = 1,...,n A
(ii) BEL(R,BEL(Q,GEN(ai, ai+1,Q42)41),t1),
for i = 1,...,n-1 A
(iii) BEL(R,INT(Q,ai, t2It1),t1),
for i = 1,... , n A
(iv) BEL(R,INT(Q,by(cti, t2, ti),ti),
for i = 1,... ,n-1 A
(v) BEL( R,BEL(Q,pi, ti),ti ),
where each pi is
</bodyText>
<listItem confidence="0.652688">
CGEN(ai, Ci) A HOLDS(Ci, t2)
</listItem>
<bodyText confidence="0.9991448">
I claim that the PI process underlying cooperative question-
answering can be modeled as an attempt to infer an eplan,
i.e., to form a set of beliefs about the questioner&apos;s beliefs and
intentions that satisfies (P2). Thus the next question to ask
is: how can R come to have such a set of beliefs?
</bodyText>
<sectionHeader confidence="0.993501" genericHeader="method">
THE INFERENCE PROCESS
</sectionHeader>
<bodyText confidence="0.998948022222222">
In the complete PI model, the inference of an eplan is a two-
stage process. First, R infers beliefs and intentions that Q
plausibly has. Then when she has found some set of these
that is large enough to account for Q&apos;s query, their epistemic
status can be upgraded, from beliefs and intentions that R be-
lieves Q plausibly has, to beliefs and intentions that R will, for
the purposes of forming her response, consider Q actually to
have. Within this paper, however, I will blur the distinction
between attitudes that R believes Q plausibly has and atti-
tudes that R believes Q indeed has; in consequence I will also
omit discussion of the second stage of the PI process.
A set of plan inference rules encodes the principles by which
an inferring agent R can reason from some set of beliefs and
intentions—call this the antecedent eplan—that she thinks Q
has, to some further set of beliefs and intentions—call this the
consequent eplan—that she also thinks he has. The beliefs and
intentions that the antecedent eplan comprises are a proper
subset of those that the consequent eplan comprises. To reason
from antecedent eplan to consequent eplan, R must attribute
some explanatory belief to Q on the basis of something other
than just Q&apos;s query. In more detail, if part of R&apos;s belief that
Q has the antecedent eplan is a belief that Q intends to do
some act a, and R has reason to believe that Q believes that
act-type a conditionally generates act-type -y under condition
C, then R can infer that Q intends to do a in order to do 1,
believing as well that C will hold at performance time. R can
also reason in the other direction: if part of her belief that Q
has some plausible eplan is a belief that Q intends to do some
act a and R has reason to believe that Q believes that act-type
ry conditionally generates act-type a under condition C, then
R can infer that Q intends to do 1 in order to do a, believing
that C will hold at performance time.
The plan inference rules encode the pattern of reasoning ex-
pressed in the last two sentences. Different plan inference rules
encode the different bases upon which R may decide that Q
may believe that a conditional generation relation holds be-
tween some a, an act of which is intended as part of the an-
tecedent eplan, and some -y. This ascription of beliefs, as well
as the ascription of intentions, is a nonmonotonic process. For
arbitrary proposition P, R will only decide that Q may believe
that P if R has no reason to believe Q believes that
In the most straightforward case, R will ascribe to Q a be-
lief about a conditional generation relation that she herself
believes true. This reasoning can be encoded in the represen-
tation language in rule (PI1):
</bodyText>
<equation confidence="0.853115">
(PI/l) BEL(R,EPLAN(Q,an,frki, • • • , tt.-11,IPI, • • •
t2, 1040
A
BEL(R,CGEN(an, -y, C),t&apos;)
■••■
</equation>
<bodyText confidence="0.858092071428571">
BEL(R,EPLAN(Q,1,1ni, • ,(14101, • • • ,On142, t1)41)
where pn mit CGEN(cs„, C) A HOLDS(C, ix)
This rule says that, if R&apos;s belief that Q has some eplan includes
a belief that Q intends to do an act an, and R also believes that
act-type an conditionally generates some -y under condition C,
then R can (nonmonotonically) infer that Q has the additional
intention of doing an in order to do -y—i.e., that he intends to
do by(a., -y). Q&apos;s having this intention depends upon his also
having the supporting belief that an conditionally generates -I
under some condition C, and the further belief that this C will
hold at performance time. A rule symmetric to (P11) is also
needed since It can not only reason about what acts might be
generated by an act that she already believes Q intends, but
also about what acts might generate such an act.
Consider R&apos;s use of (PI1) in attempting to infer the plan
underlying query (1).8 R herself has a particular belief about
the relation between the act-types &amp;quot;setting the permissions on
a file&apos; and &amp;quot;preventing someone access to the file,&amp;quot; a belief we
can encode as follows:
(22)BEL(R,CGEN(set-permissions(X,P,Y),
prevent(X,P,Z),
-nmember(Z,Y) A -,system-mgr(Z)),
ti)
From query (1), Ft can directly attribute to Q two trivial
eplans:
°I have simplified somewhat in the following account for presentational
purposes. A step-by-step account of this inference process is given in
Pollack 114, Chap. 61.
</bodyText>
<page confidence="0.985366">
212
</page>
<equation confidence="0.9607978">
(23)BEL(R,EPLAN(Q,set-permissions(mmfile,readfacult y),
i,t2, tt),
ti)
(24)BEL(R,EPLAN(Q,prevent(mmfile,read,tom),[ )42, t1),
ti)
</equation>
<bodyText confidence="0.996843">
The belief in (23) is justified by the fact that (13) satisfies
Clause (i) of (P2), (16) satisfies Clause (iv) of (P2), and
Clauses (ii), (iii), and (v) are vacuously satisfied. An anal-
ogous argument applies to (24).
Now, if R applies (PI1), she will attribute to Q exactly the
same belief as she herself has, as expressed in (22), along with
a belief that the condition C specified there will hold at t2.
That is, as part of her belief that a particular eplan underlies
(1), R will have the following belief:
</bodyText>
<figure confidence="0.537566857142857">
(25)BEL(R,BEL(Q,CGEN(set-permissions(X,P,Y),
prevent(X,P,Z),
-,member(Z,Y) A -,system-mrg(Z))
A
HOLDS(-,member(tom,faculty)
A -,system-mgr(tom), tz),
ti)
</figure>
<bodyText confidence="0.999690789473684">
The belief that R attributes to Q, as expressed in (25), is
an explanatory belief supporting (15). Note that it is not the
same explanatory belief that was expressed in (20) and (21). In
(25), the discrepancy between R&apos;s beliefs and R&apos;s beliefs about
Q&apos;s beliefs is about whether Tom is the system manager. This
discrepancy may result in a response like (26), which conveys
different information than does (7) about the source of the
judged ill-formedness.
(26) &amp;quot;Well, the command is SET PROTECTION = (Fac-
ulty:Read), but that won&apos;t keep Tom out: he&apos;s the system
manager.&amp;quot;
(PIO (and its symmetric partner) are not sufficient to model
the inference of the eplan that results in (7). This is because, in
using (PI1), R is restricted to ascribing to Q the same beliefs
about the relation between domain act-types as she herself
has.9 The eplan that results in (7) includes a belief that R
attributes to Q involving a relation between act-types that R
believes false, specifically, the CGEN relation in (20). What
is needed to derive this is a rule such as (PI2):
</bodyText>
<equation confidence="0.6661132">
(PI2) BEL(R,EPLAN(Q,o„,[ni, • • • an-thiPt, • • •
t2, ti),ti)
A
BEL(R,CGEN(an, -1,Ci A ... A C,„,),t1)
BEL(R,EPLAN(Q,7,fai, • • • , • • • ,Oni,ez,ti),t1)
</equation>
<bodyText confidence="0.987281911764706">
where pn = CGEN(an,l, Ci A • A Ci+i A ...AC,n)A
HOLDS(Ci A ... A A Ci+i A . • • A t2)
°Hence, existing PI systems that equate R&apos;s and Q&apos;s beliefs about
actions could, in principle, have handled examples such as (26) which
f t :!11-e only the use of (PII), although they have not done so. Further,
whili they could have handled the particular type of invalidity that can be
inferred using (PII), without an analysis of the general problem of invalid
plans and their effects on cooperative responses, these systems would need
to treat this as a special case in which a variant response is required.
What (PI2) expresses is that R may ascribe to Q a belief about
a relation between act-types that is a slight variation of one
she herself has. What (PI2) asserts is that, if there is some
CGEN relation that R believes true, she may attribute to Q
a belief in a similar CGEN relation that is stronger, in that it
is missing one of the required conditions. If R uses (PI2) in
attempting to infer the plan that underlies query (1), she may
decide that Q&apos;s belief about the conditions under which setting
the permissions on a file prevents someone from accessing the
file do not include the person&apos;s not being the system manager.
This can result in R attributing to Q the explanatory belief in
(20) and (21), which, in turn, may result in a response such as
that in (7).
Of course, both the kind of discrepancy that may be in-
troduced by (PI1) and the kind that is always introduced by
(PI2) may be present simultaneously, resulting in a response
like (27):
(27) &apos;Well, the command is SET PROTECTION = (Fac-
ulty:Read), but that won&apos;t keep Tom out: he&apos;s the system
manager, and file permissions don&apos;t apply to the system man-
ager.&amp;quot;
(PI2) represents just one kind of variation of her own beliefs
that R may consider attributing to Q. Additional PI rules
encode other variations and can also be used to encode any
typical misconceptions that R may attribute to Q.
</bodyText>
<sectionHeader confidence="0.997967" genericHeader="method">
IMPLEMENTATION
</sectionHeader>
<bodyText confidence="0.9999803125">
The inference process described in this paper has been imple-
mented in SPIRIT, a System for Plan Inference that Reasons
about Invalidities Too. SPIRIT infers and evaluates the plans
underlying questions asked by users about the domain of com-
puter mail. It also uses the result of its inference and eval-
uation to generate simulated cooperative responses. SPIRIT
is implemented in C-Prolog, and has run on several differ-
ent machines, including a Sun Workstation, a Vax 11-750,
and a DEC-20. SPIRIT is a demonstration system, imple-
mented to demonstrate the PI model developed in this work;
consequently only a few key examples, which are sufficient to
demonstrate SPIRIT&apos;s capabilities, have been implemented.
Of course, SPIRIT&apos;s knowledge base could be expanded in a
straightforward manner. SPIRIT has no mechanisms for com-
puting relevance or salience and, consequently, always pro-
duces as complete an answer as possible.
</bodyText>
<sectionHeader confidence="0.998669" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.99971975">
In this paper I demonstrated that modeling cooperative con-
versation, in particular cooperative question-answering, re-
quires a model of plan inference that distinguishes between
the beliefs of actors and those of observers. I reported on such
a model, which rests on an analysis of plans as mental phenom-
ena. Under this analysis there can be discrepancies between an
agent&apos;s own beliefs and the beliefs that she ascribes to an actor
when she thinks he has some plan. Such discrepancies were as-
sociated with the observer&apos;s judgement that the actor&apos;s plan is
invalid. Then the types of any invalidities judged to be present
in a plan inferred to underlie a query were shown to affect the
content of a cooperative response. I further suggested that, to
</bodyText>
<page confidence="0.996152">
213
</page>
<bodyText confidence="0.999938555555556">
guarantee a cooperative response, the observer must attempt
to ascribe to the questioner more than just a set of beliefs and
intentions sufficient to believe that he has some plan: she must
also attempt to ascribe to him beliefs that explain those beliefs
and intentions. The eplan construct was introduced to capture
this requirement. Finally, I described the process of inferring
eplans—that is, of ascribing to another agent beliefs and in-
tentions that explain his query and can influence a response
to it.
</bodyText>
<sectionHeader confidence="0.999777" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999934333333334">
[1] James F. Allen. A Plan Based Approach to Speech Act
Recognition. Technical Report TR 121/79, University of
Toronto, 1979.
[2] James F. Allen. Recognizing intentions from natural
language utterances. In Michael Brady and Robert C.
Berwick, editors, Computational Models of Discourse,
pages 107-166, MIT Press, Cambridge, Mass., 1983.
[3] James F. Allen. Towards a general theory of action and
time. Artificial Intelligence, 23(2):123-154, 1984.
[4] Michael Bratman. Intention, Plans and Practical Reason.
Harvard University Press, Cambridge, Ma., forthcoming.
[5] Michael Bratman. Taking plans seriously. Social Theory
and Practice, 9:271-287, 1983.
161 M. Sandra Carberry. Pragmatic Modeling in Information
System Interfaces. PhD thesis, University of Delaware,
1985.
[7] Philip R. Cohen and Hector J. Levesque. Speech acts and
rationality. In Proceedings of the 28rd Conference of the
Association for Computational Linguistics, pages 49-59,
Stanford, Ca., 1985.
[8] R. E. Fikes and Nils J. Nilsson. Strips: a new approach
to the application of theorem proving to problem solving.
Artificial Intelligence, 2:189-208, 1971.
[9] Alvin I. Goldman. A Theory of Human Action. Prentice-
HA, Englewood Cliffs, N.J., 1970.
[10] Aravind K. Joshi, Bonnie Webber, and Ralph Weischedel.
Living up to expectations: computing expert responses.
In Proceedings of the Fourth National Conference on Ar-
tificial Intelligence, pages 169-175, Austin, Tx., 1984.
111] Diane Litman. Plan Recognition and Discourse Analy-
sis: An Integrated Approach for Understanding Dialogues.
PhD thesis, University of Rochester, 1985.
[12] Edwin P.D. Pednault. Preliminary Report on a Theory of
Plan Synthesis. Technical Report 358, SRI International,
1985.
[13] C. Raymond Perrault and James F. Allen. A plan-based
analysis of indirect speech acts. American Journal of
Computational Linguistics, 6:167-182, 1980.
[14] Martha E. Pollack. Inferring Domain Plans in Question-
Answering. PhD thesis, University of Pennsylvania, 1986.
[15] Earl D. Sacerdoti. A Structure for Plans and Behavior.
American Elsevier, New York, 1977.
[16] Candace L. Sidner. Plan parsing for intended response
recognition in discourse. Computational Intelligence,
1(1), 1985.
[17] Candace L. Sidner. What the speaker means: the recogni-
tion of speakers&apos; plans in discourse. International Journal
of Computers and Mathematics, 9:71-82, 1983.
118] David E. Wilkins. Domain-independent planning: rep-
resentation and plan generation. Artificial Intelligence,
22:269-301, 1984.
</reference>
<page confidence="0.998953">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941274">
<title confidence="0.995678">A MODEL OF PLAN INFERENCE THAT DISTINGUISHES BETWEEN THE BELIEFS OF ACTORS AND OBSERVERS</title>
<author confidence="0.999999">Martha E Pollack</author>
<affiliation confidence="0.997585">Artificial Intelligence Center and Center for the Study of Language and Information SRI International</affiliation>
<address confidence="0.9984285">333 Ravenswood Avenue Menlo Park, CA 94025</address>
<abstract confidence="0.998010714285714">Existing models of plan inference (PI) in conversation have assumed that the agent whose plan is being inferred (the actor) and the agent drawing the inference (the observer) have identical beliefs about actions in the domain. I argue that this assumption often results in failure of both the PI process and the communicative process that PI is meant to support. In particular, it precludes the principled generation of appropriate responses to queries that arise from invalid plans. I describe a model of PI that abandons this assumption. It rests on an of plans as mental phenomena. Judgements a is invalid are with particular discrepancies between the beliefs that the observer ascribes to the actor when the former believes that the latter has some plan, and the beliefs that the observer herself holds. I show that the content an appropriate response to a query is affected the types of any such discrepancies of belief judged to be present in the plan inferred to underlie that query. The PI model described here has been implemented in SPIRIT, a small demonstration that answers questions about the of computer mail.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>A Plan Based Approach to Speech Act Recognition.</title>
<date>1979</date>
<tech>Technical Report TR 121/79,</tech>
<institution>University of Toronto,</institution>
<marker>[1]</marker>
<rawString>James F. Allen. A Plan Based Approach to Speech Act Recognition. Technical Report TR 121/79, University of Toronto, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Recognizing intentions from natural language utterances.</title>
<date>1983</date>
<booktitle>Computational Models of Discourse,</booktitle>
<pages>107--166</pages>
<editor>In Michael Brady and Robert C. Berwick, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.,</location>
<contexts>
<context position="1866" citStr="[2]" startWordPosition="304" endWordPosition="304"> to underlie that query. The PI model described here has been implemented in SPIRIT, a small demonstration system that answers questions about the domain of computer mail. INTRODUCTION The importance of plan inference (PI) in models of conversation has been widely noted in the computational-linguistics literature. Incorporating PI capabilities into systems that answer users&apos; questions has enabled such systems to handle indirect speech acts 1131, supply more information than is actually requested in a query 121, provide helpful information in response to a yes/no query answered in the negative [2], disambiguate requests 1171, resolve certain forms of intersentential ellipsis 16,111, and handle such discourse phenomena as clarification subdialogues [I1], and correction or &apos;debugging&amp;quot; subdialogues The research reported in this paper has been made possible in part by an IBM Graduate Fellowship, in part by a gift from the Systems Development Foundation, and in part by support from the Defense Advanced Research Projects Agency under Contract N00039-84-K-0078 with the Space and Naval Warfare Command. The views and conclusions contained in this document are those of the author and should not </context>
</contexts>
<marker>[2]</marker>
<rawString>James F. Allen. Recognizing intentions from natural language utterances. In Michael Brady and Robert C. Berwick, editors, Computational Models of Discourse, pages 107-166, MIT Press, Cambridge, Mass., 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Towards a general theory of action and time.</title>
<date>1984</date>
<journal>Artificial Intelligence,</journal>
<pages>23--2</pages>
<contexts>
<context position="10869" citStr="[3]" startWordPosition="1878" endWordPosition="1878">of as triples of act-type, agent, and time. Generation is a relation over actions, not over act-types. Not every case of an agent typing DEL . will result in the agent deleting the current message; for example, my typing it just now did not, because I was not typing it to a computer mail system. Similarly, executability— the relation expressed in Clause (1) of (PO) as &amp;quot;can execute&amp;quot;— applies to actions, and the objects of an agent&apos;s intentions are, in this model, also actions. Using the representation language specified in my thesis [14], which builds upon Allen&apos;s interval-based temporal logic [3], the conditions on G&apos;s having a simple plan to do # can be encoded as follows: (P1) SIMPLE-PLAN(G,a„,(ai,..., t1)■-• (i) BEL(G,EXEC(ai,G,t2),4), for i = 1,...,n A (ii) BEL(G,GEN(a1,a41,G,t2),t1), for i = 1,... ,n-1 A (iii) INT(G,04,12, t1), for i = 1,... , n A (iv) INT(G,by(ai,ai+i), 12,4), for i = 1,...,n-1 The left-hand side of (P1) denotes that the agent G has, at time 4, a simple plan to do a„, consisting of doing the set of acts (al, , n_iat t2. Note that all these are simultaneous acts; this is a consequence of the restriction to simple plans. The right-hand side of (P1) corresponds dir</context>
</contexts>
<marker>[3]</marker>
<rawString>James F. Allen. Towards a general theory of action and time. Artificial Intelligence, 23(2):123-154, 1984.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michael Bratman</author>
</authors>
<title>Intention, Plans and Practical Reason.</title>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Ma., forthcoming.</location>
<contexts>
<context position="8193" citStr="[4]" startWordPosition="1404" endWordPosition="1404">ommon-sense conceptions of what it means to have a plan leads to the following analysis [14, Chap. 3]2: (P0) An agent G has a plan to do #, that consists in doing some set of acts II, provided that 1. G believes that he can execute each act in II. 2. G believes that executing the acts in H will entail the performance of #. 3. G believes that each act in H plays a role in his plan. (See discussion below.) 4. G intends to execute each act in II. 5. G intends to execute H as a way of doing #. zAlthough this definition ignores some important issues of commitment over time, as discussed by Bratman [4] and Cohen and Levesque 171, it is sufficient to support the PI process needed for many question-answering situations. This is because, in such situations, unexpected changes in the world that would force a reconsideration of the actor&apos;s intentions can usually be safely ignored. 6. G intends each act in H to play a role in his plan. The notion of an act playing a role in a plan is defined in terms of two relationships over acts: generation, in the sense defined by Goldman [9], and enablement. Roughly, one act generates another if, by performing the first, the agent also does the second; thus, </context>
</contexts>
<marker>[4]</marker>
<rawString>Michael Bratman. Intention, Plans and Practical Reason. Harvard University Press, Cambridge, Ma., forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bratman</author>
</authors>
<title>Taking plans seriously. Social Theory and Practice,</title>
<date>1983</date>
<tech>PhD thesis,</tech>
<pages>9--271</pages>
<institution>University of Delaware,</institution>
<marker>[5]</marker>
<rawString>Michael Bratman. Taking plans seriously. Social Theory and Practice, 9:271-287, 1983. 161 M. Sandra Carberry. Pragmatic Modeling in Information System Interfaces. PhD thesis, University of Delaware, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Speech acts and rationality.</title>
<date>1985</date>
<booktitle>In Proceedings of the 28rd Conference of the Association for Computational Linguistics,</booktitle>
<pages>49--59</pages>
<location>Stanford, Ca.,</location>
<marker>[7]</marker>
<rawString>Philip R. Cohen and Hector J. Levesque. Speech acts and rationality. In Proceedings of the 28rd Conference of the Association for Computational Linguistics, pages 49-59, Stanford, Ca., 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Fikes</author>
<author>Nils J Nilsson</author>
</authors>
<title>Strips: a new approach to the application of theorem proving to problem solving.</title>
<date>1971</date>
<journal>Artificial Intelligence,</journal>
<pages>2--189</pages>
<contexts>
<context position="6076" citStr="[8]" startWordPosition="1017" endWordPosition="1017">n [14]. PLANS AS MENTAL PHENOMENA We can distinguish between two views of plans. As Bratman [5, page 271] has observed, there is an ambiguity in speaking of an agent&apos;s plan: &amp;quot;On the one hand, [this] could mean an appropriate abstract structure—some sort of partial function from circumstances to actions, perhaps. On the other hand, [it] could mean an appropriate state of mind, one naturally describable in terms of such structures.&amp;quot; We might call the former sense the data-structure view of plans, and the latter the mental phenomenon view of plans. Work in plan synthesis (e.g., Fikes and Nilsson [8], Sacerdoti [15], Wilkins [18], and Pednault [12]), has taken the data-structure view, considering plans to be structures encoding aggregates of actions that, when performed in circumstances satisfying some specified preconditions, achieve some specified results. For the purposes of PI, however, it is much more useful to adopt a mental phenomenon view and consider plans to be particular configurations of beliefs and intentions that some agent has. After all, inferring another agent&apos;s plan means figuring out what actions he &amp;quot;has in mind,&amp;quot; and he may well be wrong about the effects of those inte</context>
</contexts>
<marker>[8]</marker>
<rawString>R. E. Fikes and Nils J. Nilsson. Strips: a new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2:189-208, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvin I Goldman</author>
</authors>
<title>A Theory of Human Action.</title>
<date>1970</date>
<publisher>PrenticeHA,</publisher>
<location>Englewood Cliffs, N.J.,</location>
<contexts>
<context position="8673" citStr="[9]" startWordPosition="1489" endWordPosition="1489">ay of doing #. zAlthough this definition ignores some important issues of commitment over time, as discussed by Bratman [4] and Cohen and Levesque 171, it is sufficient to support the PI process needed for many question-answering situations. This is because, in such situations, unexpected changes in the world that would force a reconsideration of the actor&apos;s intentions can usually be safely ignored. 6. G intends each act in H to play a role in his plan. The notion of an act playing a role in a plan is defined in terms of two relationships over acts: generation, in the sense defined by Goldman [9], and enablement. Roughly, one act generates another if, by performing the first, the agent also does the second; thus, saying to Kathy &amp;quot;How are you doing?&amp;quot; may generate asking her how she is feeling. Or, to take an example from the computer-mail domain, typing DEL . at the prompt for a computer mail system may generate deleting the current message, which may in turn generate cleaning out one&apos;s mail file. In contrast, one act enables the generation of a second by a third if the first brings about circumstances that are necessary for the generation. Thus, typing HEADER 15 may enable the generat</context>
</contexts>
<marker>[9]</marker>
<rawString>Alvin I. Goldman. A Theory of Human Action. PrenticeHA, Englewood Cliffs, N.J., 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Bonnie Webber</author>
<author>Ralph Weischedel</author>
</authors>
<title>Living up to expectations: computing expert responses.</title>
<date>1984</date>
<booktitle>In Proceedings of the Fourth National Conference on Artificial Intelligence,</booktitle>
<tech>PhD thesis,</tech>
<pages>169--175</pages>
<institution>University of Rochester,</institution>
<location>Austin, Tx.,</location>
<marker>[10]</marker>
<rawString>Aravind K. Joshi, Bonnie Webber, and Ralph Weischedel. Living up to expectations: computing expert responses. In Proceedings of the Fourth National Conference on Artificial Intelligence, pages 169-175, Austin, Tx., 1984. 111] Diane Litman. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues. PhD thesis, University of Rochester, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edwin P D Pednault</author>
</authors>
<title>Preliminary Report on a Theory of Plan Synthesis.</title>
<date>1985</date>
<tech>Technical Report 358,</tech>
<publisher>SRI International,</publisher>
<contexts>
<context position="6125" citStr="[12]" startWordPosition="1024" endWordPosition="1024">uish between two views of plans. As Bratman [5, page 271] has observed, there is an ambiguity in speaking of an agent&apos;s plan: &amp;quot;On the one hand, [this] could mean an appropriate abstract structure—some sort of partial function from circumstances to actions, perhaps. On the other hand, [it] could mean an appropriate state of mind, one naturally describable in terms of such structures.&amp;quot; We might call the former sense the data-structure view of plans, and the latter the mental phenomenon view of plans. Work in plan synthesis (e.g., Fikes and Nilsson [8], Sacerdoti [15], Wilkins [18], and Pednault [12]), has taken the data-structure view, considering plans to be structures encoding aggregates of actions that, when performed in circumstances satisfying some specified preconditions, achieve some specified results. For the purposes of PI, however, it is much more useful to adopt a mental phenomenon view and consider plans to be particular configurations of beliefs and intentions that some agent has. After all, inferring another agent&apos;s plan means figuring out what actions he &amp;quot;has in mind,&amp;quot; and he may well be wrong about the effects of those intended actions. Consider, for example, the plan I h</context>
</contexts>
<marker>[12]</marker>
<rawString>Edwin P.D. Pednault. Preliminary Report on a Theory of Plan Synthesis. Technical Report 358, SRI International, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>James F Allen</author>
</authors>
<title>A plan-based analysis of indirect speech acts.</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>6--167</pages>
<marker>[13]</marker>
<rawString>C. Raymond Perrault and James F. Allen. A plan-based analysis of indirect speech acts. American Journal of Computational Linguistics, 6:167-182, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>Inferring Domain Plans in QuestionAnswering.</title>
<date>1986</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="5479" citStr="[14]" startWordPosition="918" endWordPosition="918">e actor when the former believes that the latter has some plan, and the beliefs observer herself holds. I give an account of different types of plan invalidities, and show how this account provides an explanation for certain regularities that are observable in cooperative responses to questions. The PI model described here has been implemented in SPIRIT, a small demonstration system that answers questions about the domain of computer mail. More &apos;Allen&apos;s article 121 summarizes his dissertation research Pl. 207 extensive discussion of both the PI model and SPIRIT can be found in my dissertation [14]. PLANS AS MENTAL PHENOMENA We can distinguish between two views of plans. As Bratman [5, page 271] has observed, there is an ambiguity in speaking of an agent&apos;s plan: &amp;quot;On the one hand, [this] could mean an appropriate abstract structure—some sort of partial function from circumstances to actions, perhaps. On the other hand, [it] could mean an appropriate state of mind, one naturally describable in terms of such structures.&amp;quot; We might call the former sense the data-structure view of plans, and the latter the mental phenomenon view of plans. Work in plan synthesis (e.g., Fikes and Nilsson [8], S</context>
<context position="10808" citStr="[14]" startWordPosition="1870" endWordPosition="1870"> acts—I will use the two terms interchangeably—can be thought of as triples of act-type, agent, and time. Generation is a relation over actions, not over act-types. Not every case of an agent typing DEL . will result in the agent deleting the current message; for example, my typing it just now did not, because I was not typing it to a computer mail system. Similarly, executability— the relation expressed in Clause (1) of (PO) as &amp;quot;can execute&amp;quot;— applies to actions, and the objects of an agent&apos;s intentions are, in this model, also actions. Using the representation language specified in my thesis [14], which builds upon Allen&apos;s interval-based temporal logic [3], the conditions on G&apos;s having a simple plan to do # can be encoded as follows: (P1) SIMPLE-PLAN(G,a„,(ai,..., t1)■-• (i) BEL(G,EXEC(ai,G,t2),4), for i = 1,...,n A (ii) BEL(G,GEN(a1,a41,G,t2),t1), for i = 1,... ,n-1 A (iii) INT(G,04,12, t1), for i = 1,... , n A (iv) INT(G,by(ai,ai+i), 12,4), for i = 1,...,n-1 The left-hand side of (P1) denotes that the agent G has, at time 4, a simple plan to do a„, consisting of doing the set of acts (al, , n_iat t2. Note that all these are simultaneous acts; this is a consequence of the restriction</context>
</contexts>
<marker>[14]</marker>
<rawString>Martha E. Pollack. Inferring Domain Plans in QuestionAnswering. PhD thesis, University of Pennsylvania, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Earl D Sacerdoti</author>
</authors>
<title>A Structure for Plans and Behavior.</title>
<date>1977</date>
<publisher>American Elsevier,</publisher>
<location>New York,</location>
<contexts>
<context position="6092" citStr="[15]" startWordPosition="1019" endWordPosition="1019">S MENTAL PHENOMENA We can distinguish between two views of plans. As Bratman [5, page 271] has observed, there is an ambiguity in speaking of an agent&apos;s plan: &amp;quot;On the one hand, [this] could mean an appropriate abstract structure—some sort of partial function from circumstances to actions, perhaps. On the other hand, [it] could mean an appropriate state of mind, one naturally describable in terms of such structures.&amp;quot; We might call the former sense the data-structure view of plans, and the latter the mental phenomenon view of plans. Work in plan synthesis (e.g., Fikes and Nilsson [8], Sacerdoti [15], Wilkins [18], and Pednault [12]), has taken the data-structure view, considering plans to be structures encoding aggregates of actions that, when performed in circumstances satisfying some specified preconditions, achieve some specified results. For the purposes of PI, however, it is much more useful to adopt a mental phenomenon view and consider plans to be particular configurations of beliefs and intentions that some agent has. After all, inferring another agent&apos;s plan means figuring out what actions he &amp;quot;has in mind,&amp;quot; and he may well be wrong about the effects of those intended actions. Co</context>
</contexts>
<marker>[15]</marker>
<rawString>Earl D. Sacerdoti. A Structure for Plans and Behavior. American Elsevier, New York, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Plan parsing for intended response recognition in discourse.</title>
<date>1985</date>
<journal>Computational Intelligence,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2783" citStr="[16,11]" startWordPosition="445" endWordPosition="445">y a gift from the Systems Development Foundation, and in part by support from the Defense Advanced Research Projects Agency under Contract N00039-84-K-0078 with the Space and Naval Warfare Command. The views and conclusions contained in this document are those of the author and should not be interpreted as representative of the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government. I am grateful to Barbara Grosz, James Allen, Phil Cohen, Amy Lansky, Candy Sidner and Bonnie Webber for their comments on an earlier draft. [16,11]. The PI process in each of these systems, however, has assumed that the agent whose plan is being inferred (to whom I shall refer as the actor), and the agent drawing the inference (to whom I shall refer as the observer), have identical beliefs about the actions in the domain. Thus, Allen&apos;s model, which was one of the earliest accounts of PI in conversationl and inspired a great deal of the work done subsequently, includes, as a typical PI rule, the following: &amp;quot;SBAW(P) SBAW(ACT) if P is a precondition of ACT&amp;quot; [2, page 120]. This rule can be glossed as &amp;quot;if the system (observer) believes that a</context>
</contexts>
<marker>[16]</marker>
<rawString>Candace L. Sidner. Plan parsing for intended response recognition in discourse. Computational Intelligence, 1(1), 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>What the speaker means: the recognition of speakers&apos; plans in discourse.</title>
<date>1983</date>
<journal>International Journal of Computers and Mathematics,</journal>
<pages>9--71</pages>
<marker>[17]</marker>
<rawString>Candace L. Sidner. What the speaker means: the recognition of speakers&apos; plans in discourse. International Journal of Computers and Mathematics, 9:71-82, 1983. 118] David E. Wilkins. Domain-independent planning: representation and plan generation. Artificial Intelligence, 22:269-301, 1984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>