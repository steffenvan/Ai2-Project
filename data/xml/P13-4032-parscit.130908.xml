<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000059">
<title confidence="0.969474">
VSEM: An open library for visual semantics representation
</title>
<author confidence="0.998781">
Elia Bruni
</author>
<affiliation confidence="0.998693">
University of Trento
</affiliation>
<email confidence="0.977635">
elia.bruni@unitn.it
</email>
<author confidence="0.993498">
Jasper Uijlings
</author>
<affiliation confidence="0.997808">
University of Trento
</affiliation>
<email confidence="0.991892">
jrr@disi.unitn.it
</email>
<author confidence="0.972195">
Ulisse Bordignon
</author>
<affiliation confidence="0.993354">
University of Trento
</affiliation>
<email confidence="0.965479">
ulisse.bordignon@unitn.it
</email>
<author confidence="0.99143">
Irina Sergienya
</author>
<affiliation confidence="0.997312">
University of Trento
</affiliation>
<email confidence="0.991321">
irina.sergienya@unitn.it
</email>
<author confidence="0.996992">
Adam Liska
</author>
<affiliation confidence="0.998148">
University of Trento
</affiliation>
<email confidence="0.973592">
adam.liska@unitn.it
</email>
<sectionHeader confidence="0.993306" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999741909090909">
VSEM is an open library for visual se-
mantics. Starting from a collection of
tagged images, it is possible to auto-
matically construct an image-based rep-
resentation of concepts by using off-the-
shelf VSEM functionalities. VSEM is en-
tirely written in MATLAB and its object-
oriented design allows a large flexibility
and reusability. The software is accompa-
nied by a website with supporting docu-
mentation and examples.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996975">
In the last years we have witnessed great progress
in the area of automated image analysis. Important
advances, such as the introduction of local features
for a robust description of the image content (see
Mikolajczyk et al. (2005) for a systematic review)
and the bag-of-visual-words method (BoVW)1 for
a standard representation across multiple images
(Sivic and Zisserman, 2003), have contributed to
make image analysis ubiquitous, with applications
ranging from robotics to biology, from medicine to
photography.
Two facts have played a key role in the rapid ad-
vance of these ideas. First, the introduction of very
well defined challenges which have been attracting
also a wide community of “outsiders&amp;quot; specialized
in a variety of disciplines (e.g., machine learning,
neural networks, graphical models and natural lan-
guage processing). Second, the sharing of effec-
tive, well documented implementations of cutting
edge image analysis algorithms, such as OpenCV2
</bodyText>
<footnote confidence="0.616645">
1Bag-of-visual-words model is a popular technique for
image classification inspired by the traditional bag-of-words
model in Information Retrieval. It represents an image with
discrete image-describing features. Visual words are iden-
tified by clustering a large corpus of lower-level continuous
features.
2http://opencv.org/
</footnote>
<bodyText confidence="0.993230378378379">
and VLFeat.3
A comparable story can be told about automatic
text analysis. The last decades have seen a long
series of successes in the processing of large text
corpora in order to extract more or less structured
semantic knowledge. In particular, under the as-
sumption that meaning can be captured by patterns
of co-occurrences of words, distributional seman-
tic models such as Latent Semantic Analysis (Lan-
dauer and Dumais, 1997) or Topic Models (Blei
et al., 2003) have been shown to be very effective
both in general semantic tasks such as approximat-
ing human intuitions about meaning, as well as in
more application-driven tasks such as information
retrieval, word disambiguation and query expan-
sion (Turney and Pantel, 2010). And also in the
case of automated text analysis, a wide range of
method implementations are at the disposal of the
scientific community.4
Nowadays, given the parallel success of the two
disciplines, there is growing interest in making
the visual and textual channels interact for mutual
benefit. If we look at the image analysis commu-
nity, we discover a well established tradition of
studies that exploit both channels of information.
For example, there is a relatively extended amount
of literature about enhancing the performance on
visual tasks such as object recognition or image re-
trieval by replacing a purely image-based pipeline
with hybrid methods augmented with textual in-
formation (Barnard et al., 2003; Farhadi et al.,
2009; Berg et al., 2010; Kulkarni et al., 2011).
Unfortunately, the same cannot be said of the
exploitation of image analysis from within the text
community. Despite the huge potential that au-
tomatically induced visual features could repre-
sent as a new source of perceptually grounded
</bodyText>
<footnote confidence="0.99679175">
3http://www.vlfeat.org/
4See for example the annotated list of corpus-based
computational linguistics resources at http://www-nlp.
stanford.edu/links/statnlp.html.
</footnote>
<page confidence="0.926194">
187
</page>
<bodyText confidence="0.945614727272727">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
semantic knowledge,5 image-enhanced models of
semantics developed so far (Feng and Lapata,
2010; Bruni et al., 2011; Leong and Mihalcea,
2011; Bergsma and Goebel, 2011; Bruni et al.,
2012a; Bruni et al., 2012b) have only scratched
this great potential and are still considered as
proof-of-concept studies only.
One possible reason of this delay with respect to
the image analysis community might be ascribed
to the high entry barriers that NLP researchers
adopting image analysis methods have to face. Al-
though many of the image analysis toolkits are
open source and well documented, they mainly ad-
dress users within the same community and there-
fore their use is not as intuitive for others. The
final goal of libraries such VLFeat and OpenCV
is the representation and classification of images.
Therefore, they naturally lack of a series of com-
plementary functionalities that are necessary to
bring the visual representation to the level of se-
mantic concepts.6
To fill the gap we just described, we present
hereby VSEM,7 a novel toolkit which allows the
extraction of image-based representations of con-
cepts in an easy fashion. VSEM is equipped with
state-of-the-art algorithms, from low-level feature
detection and description up to the BoVW repre-
sentation of images, together with a set of new rou-
tines necessary to move from an image-wise to a
concept-wise representation of image content. In
a nutshell, VSEM extracts visual information in a
way that resembles how it is done for automatic
text analysis. Thanks to BoVW, the image con-
tent is indeed discretized and visual units some-
how comparable to words in text are produced (the
visual words). In this way, from a corpus of im-
ages annotated with a set of concepts, it is pos-
sible to derive semantic vectors of co-occurrence
counts of concepts and visual words akin to the
representations of words in terms of textual collo-
cates in standard distributional semantics. Impor-
5In recent years, a conspicuous literature of studies has
surfaced, wherein demonstration was made of how text based
models are not sufficiently good at capturing the environment
we acquire language from. This is due to the fact that they
are lacking of perceptual information (Andrews et al., 2009;
Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and
Jones, 2011).
6The authors of the aforementioned studies usually refer
to words instead of concepts. We chose to call them concepts
to account for the both theoretical and practical differences
standing between a word and the perceptual information it
brings along, which we define its concept.
</bodyText>
<footnote confidence="0.814525">
7http://clic.cimec.unitn.it/vsem/
</footnote>
<bodyText confidence="0.999954818181818">
tantly, the obtained visual semantic vectors can be
easily combined with more traditional text-based
vectors to arrive at a multimodal representation of
meaning (see e.g. (Bruni et al., 2011)). It has
been shown that the resulting multimodal models
perform better than text-only models in semantic
tasks such as approximating semantic similarity
and relatedness ((Feng and Lapata, 2010; Bruni et
al., 2012b)).
VSEM functionalities concerning image anal-
ysis is based on VLFeat (Vedaldi and Fulkerson,
2010). This guarantees that the image analysis un-
derpinnings of the library are well maintained and
state-of-the-art.
The rest of the paper is organized as follows.
In Section 2 we introduce the procedure to obtain
an image-based representation of a concept. Sec-
tion 3 describes the VSEM architecture. Section
4 shows how to install and run VSEM through
an example that uses the Pascal VOC data set.
Section 5 concludes summarizing the material and
discussing further directions.
</bodyText>
<sectionHeader confidence="0.935315" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999996111111111">
As shown by Feng and Lapata (2010), Bruni et
al. (2011) and Leong and Mihalcea (2011), it is
possible to construct an image-based representa-
tion of a set of target concepts by starting from a
collection of images depicting those concepts, en-
coding the image contents into low-level features
(e.g., SIFT) and scaling up to a higher level rep-
resentation, based on the well-established BoVW
method to represent images. In addition, as shown
by Bruni et al. (2012b), better representations can
be extracted if the object depicting the concept is
first localized in the image.
More in detail, the pipeline encapsulating the
whole process mentioned above takes as input a
collection of images together with their associated
tags and optionally object location annotations. Its
output is a set of concept representation vectors
for individual tags. The following steps are in-
volved: (i) extraction of local image features, (ii)
visual vocabulary construction, (iii) encoding the
local features in a BoVW histogram, (iv) including
spatial information with spatial binning, (v) aggre-
gation of visual words on a per-concept basis in
order to obtain the co-occurrence counts for each
concept and (vi) transforming the counts into asso-
ciation scores and/or reducing the dimensionality
of the data. A brief description of the individual
</bodyText>
<page confidence="0.995608">
188
</page>
<figureCaption confidence="0.825645">
Figure 1: An example of a visual vocabulary cre-
ation pipeline. From a set of images, a larger set
of features are extracted and clustered, forming the
visual vocabulary.
</figureCaption>
<bodyText confidence="0.995670974025974">
steps follows.
Local features Local features are designed to
find local image structures in a repeatable fash-
ion and to represent them in robust ways that are
invariant to typical image transformations, such
as translation, rotation, scaling, and affine defor-
mation. Local features constitute the basis of
approaches developed to automatically recognize
specific objects (Grauman and Leibe, 2011). The
most popular local feature extraction method is the
Scale Invariant Feature Transform (SIFT), intro-
duced by Lowe (2004). VSEM uses the VLFeat
implementation of SIFT.
Visual vocabulary To obtain a BoVW repre-
sentation of the image content, a large set of lo-
cal features extracted from a large corpus of im-
ages are clustered. In this way the local fea-
ture space is divided into informative regions (vi-
sual words) and the collection of the obtained vi-
sual words is called visual vocabulary. k-means
is the most commonly used clustering algorithm
(Grauman and Leibe, 2011). In the special case
of Fisher encoding (see below), the clustering of
the features is performed with a Gaussian mixture
model (GMM), see Perronnin et al. (2010). Fig-
ure 1 exemplifies a visual vocabulary construction
pipeline. VSEM contains both the k-means and
the GMM implementations.
Encoding The encoding step maps the local fea-
tures extracted from an image to the correspond-
ing visual words of the previously created vocab-
ulary. The most common encoding strategy is
called hard quantization, which assigns each fea-
ture to the nearest visual word’s centroid (in Eu-
clidean distance). Recently, more effective encod-
ing methods have been introduced, among which
the Fisher encoding (Perronnin et al., 2010) has
been shown to outperform all the others (Chatfield
et al., 2011). VSEM uses both the hard quantiza-
tion and the Fisher encoding.
Spatial binning A consolidated way of intro-
ducing spatial information in BoVW is the use of
spatial histograms (Lazebnik et al., 2006). The
main idea is to divide the image into several (spa-
tial) regions, compute the encoding for each region
and stack the resulting histograms. This technique
is referred to as spatial binning and it is imple-
mented in VSEM. Figure 2 exemplifies the BoVW
pipeline for a single image, involving local fea-
tures extraction, encoding and spatial binning.
Figure 2: An example of a BoVW representation
pipeline for an image. Figure inspired by Chatfield
et al. (2011). Each feature extracted from the tar-
get image is assigned to the corresponding visual
word(s). Then, spatial binning is performed.
Moreover, the input of spatial binning can be
further refined by introducing localization. Three
different types of localization are typically used:
global, object, and surrounding. Global extracts
visual information from the whole image and it is
also the default option when the localization in-
formation is missing. Object extracts visual infor-
mation from the object location only and the sur-
rounding extracts visual information from outside
the object location. Localization itself can either
be done by humans (or ground truth annotation)
but also by existing localization methods (Uijlings
et al., 2013).
For localization, VSEM uses annotated object
locations (in the format of bounding boxes) of the
target object.
Aggregation Since each concept is represented
by multiple images, an aggregation function for
pooling the visual word occurrences across images
has to be defined. As far as we know, the sum
function has been the only function utilized so far.
An example for the aggregation step is sketched in
</bodyText>
<figure confidence="0.579401">
feature extraction
feature extraction
encoding
spatial binning
</figure>
<page confidence="0.908922">
189
</page>
<figureCaption confidence="0.954184857142857">
Figure 3: An example of a concept representa-
tion pipeline for cat. First, several images depict-
ing a cat are represented as vectors of visual word
counts and, second, the vectors are aggregated into
one single concept vector.
figure 3. VSEM offers an implementation of the
sum function.
</figureCaption>
<bodyText confidence="0.993018454545455">
Transformations Once the concept-
representing visual vectors are built, two types
of transformation can be performed over them to
refine their raw visual word counts: association
scores and dimensionality reduction. So far,
the vectors that we have obtained represent co-
occurrence counts of visual words with concepts.
The goal of association scores is to distinguish
interesting co-occurrences from those that are due
to chance. In order to do this, VSEM implements
two versions of mutual information (pointwise
and local), see Evert (2005).
On the other hand, dimensionality reduction
leads to matrices that are smaller and easier to
work with. Moreover, some techniques are able
to smooth the matrices and uncover latent dimen-
sions. Common dimensionality reduction methods
are singular value decomposition (Manning et al.,
2008), non-negative matrix factorization (Lee and
Seung, 2001) and neural networks (Hinton and
Salakhutdinov, 2006). VSEM implements the sin-
gular value decomposition method.
</bodyText>
<sectionHeader confidence="0.990004" genericHeader="method">
3 Framework design
</sectionHeader>
<bodyText confidence="0.9999522">
VSEM offers a friendly implementation of the
pipeline described in Section 2. The framework is
organized into five parts, which correspond to an
equal number of MATLAB packages and it is writ-
ten in object-oriented programming to encourage
</bodyText>
<listItem confidence="0.837000565217392">
reusability. A description of the packages follows.
• datasets This package contains the code
that manages the image data sets. We al-
ready provide a generic wrapper for sev-
eral possible dataset formats (VsemDataset
). Therefore, to use a new image data set
two solutions are possible: either write a
new class which extends GenericDataset or
use directly VsemDataset after having rear-
ranged the new data as described in help
VsemDataset.
• vision This package contains the code for
extracting the bag-of-visual-words represen-
tation of images. In the majority of cases,
it can be used as a “black box” by the user.
Nevertheless, if the user wants to add new
functionalities such as new features or encod-
ings, this is possible by simply extending the
corresponding generic classes and the class
VsemHistogramExtractor.
• concepts This is the package that deals
with the construction of the image-based rep-
resentation of concepts. concepts is the
</listItem>
<bodyText confidence="0.664042333333333">
most important package of VSEM. It ap-
plies the image analysis methods to obtain the
BoVW representation of the image data and
then aggregates visual word counts concept-
wise. The main class of this package is
ConceptSpace, which takes care of storing
concepts names and vectors and provides
managing and transformation utilities as its
methods.
</bodyText>
<listItem confidence="0.985873">
• benchmarks VSEM offers a benchmarking
suite to assess the quality of the visual con-
cept representations. For example, it can be
used to find the optimal parametrization of
the visual pipeline.
• helpers This package contains supporting
classes. There is a general helpers with
functionalities shared across packages and
several package specific helpers.
</listItem>
<sectionHeader confidence="0.848461" genericHeader="method">
4 Getting started
</sectionHeader>
<bodyText confidence="0.9970808">
Installation VSEM can be easily installed by
running the file vsemSetup.m. Moreover, pascal-
DatasetSetup.m can be run to download and place
the popular dataset, integrating it in the current
pipeline.
</bodyText>
<table confidence="0.547610142857143">
cat
=
+
+
+
ai
aggregation
</table>
<page confidence="0.994337">
190
</page>
<bodyText confidence="0.772997434782609">
Documentation All the MATLAB commands
of VSEM are self documented (e.g. help vsem)
and an HTML version of the MATLAB command
documentation is available from the VSEM web-
site.
The Pascal VOC demo The Pascal VOC demo
provides a comprehensive example of the work-
ings of VSEM. From the demo file pascalVQDemo
.mmultiple configurations are accessible. Addi-
tional settings are available and documented for
each function, class or package in the toolbox (see
Documentation).
Running the demo file executes the following
lines of code and returns as output ConceptSpace,
which contains the visual concept representations
for the Pascal data set.
% Create a matlab structure with the
% whole set of images in the Pascal
% dataset along with their annotation
dataset = datasets.VsemDataset(
configuration.imagesPath,’
annotationFolder’,configuration.
annotationPath);
% Initiate the class that handles
% the extraction of visual features.
featureExtractor = vision.features.
PhowFeatureExtractor();
% Create the visual vocabulary
vocabulary = KmeansVocabulary.
trainVocabulary(dataset,
featureExtractor);
% Calculate semantic vectors
conceptSpace = conceptExtractor.
extractConcepts(dataset,
histogramExtractor);
% Compute pointwise mutual
% information
conceptSpace = conceptSpace.reweight();
% Conclude the demo, computing
% the similarity of correlation
% measures of the 190 possible
% pair of concepts from the Pascal
% dataset against a gold standard
[correlationScore, p-value] =
similarityBenchmark.computeBenchmark
(conceptSpace,similarityExtractor);
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999981769230769">
We have introduced VSEM, an open library for vi-
sual semantics. With VSEM it is possible to ex-
tract visual semantic information from tagged im-
ages and arrange such information into concept
representations according to the tenets of distri-
butional semantics, as applied to images instead
of text. To analyze images, it uses state-of-the-art
techniques such as the SIFT features and the bag-
of-visual-words with spatial pyramid and Fisher
encoding. In the future, we would like to add
automatic localization strategies, new aggregation
functions and a completely new package for fusing
image- and text-based representations.
</bodyText>
<sectionHeader confidence="0.998526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999420976744186">
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463–498.
Kobus Barnard, Pinar Duygulu, David Forsyth, Nando
de Freitas, David Blei, and Michael Jordan. 2003.
Matching words and pictures. Journal of Machine
Learning Research, 3:1107–1135.
Marco Baroni and Alessandro Lenci. 2008. Concepts
and properties in word spaces. Italian Journal of
Linguistics, 20(1):55–88.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222–254.
Tamara Berg, Alexander Berg, and Jonathan Shih.
2010. Automatic attribute discovery and characteri-
zation from noisy Web data. In ECCV, pages 663–
676, Crete, Greece.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
Proceedings of RANLP, pages 399–405, Hissar, Bul-
garia.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proceedings of the EMNLP GEMS Workshop, pages
22–32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136–
145, Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proceedings of
ACM Multimedia, pages 1219–1228, Nara, Japan.
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and
Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding meth-
ods. In Proceedings of BMVC, Dundee, UK.
</reference>
<page confidence="0.991158">
191
</page>
<reference confidence="0.999849767123288">
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Ali Farhadi, Ian Endres, Derek Hoiem, and David
Forsyth. 2009. Describing objects by their at-
tributes. In Proceedings of CVPR, pages 1778–
1785, Miami Beach, FL.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings
of HLT-NAACL, pages 91–99, Los Angeles, CA.
Kristen Grauman and Bastian Leibe. 2011. Visual Ob-
ject Recognition. Morgan &amp; Claypool, San Fran-
cisco.
Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Re-
ducing the dimensionality of data with neural net-
works. Science, 313(5786):504 – 507.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In Proceedings of
CVPR, Colorado Springs, MSA.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211–
240.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories.
In Proceedings of CVPR, pages 2169–2178, Wash-
ington, DC.
Daniel D. Lee and H. Sebastian Seung. 2001. Algo-
rithms for non-negative matrix factorization. In In
NIPS, pages 556–562. MIT Press.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403–1407.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2), November.
Chris Manning, Prabhakar Raghavan, and Hinrich
Schütze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge,
UK.
K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisser-
man, J. Matas, F. Schaffalitzky, T. Kadir, and L. V.
Gool. 2005. A Comparison of Affine Region De-
tectors. International Journal of Computer Vision,
65(1).
Florent Perronnin, Jorge Sanchez, and Thomas
Mensink. 2010. Improving the fisher kernel for
large-scale image classification. In Proceedings of
ECCV, pages 143–156, Berlin, Heidelberg.
Brian Riordan and Michael Jones. 2011. Redundancy
in perceptual and linguistic experience: Comparing
feature-based and distributional models of semantic
representation. Topics in Cognitive Science, 3(2):1–
43.
Josef Sivic and Andrew Zisserman. 2003. Video
Google: A text retrieval approach to object match-
ing in videos. In Proceedings of ICCV, pages 1470–
1477, Nice, France.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and
A.W.M. Smeulders. 2013. Selective search for ob-
ject recognition. IJCV.
Andrea Vedaldi and Brian Fulkerson. 2010. Vlfeat
– an open and portable library of computer vision
algorithms. In Proceedings of ACM Multimedia,
pages 1469–1472, Firenze, Italy.
</reference>
<page confidence="0.998196">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.316573">
<title confidence="0.999905">VSEM: An open library for visual semantics representation</title>
<author confidence="0.998835">Elia</author>
<affiliation confidence="0.999707">University of Trento</affiliation>
<email confidence="0.995404">elia.bruni@unitn.it</email>
<author confidence="0.826856">Jasper</author>
<affiliation confidence="0.999086">University of Trento</affiliation>
<email confidence="0.98058">jrr@disi.unitn.it</email>
<affiliation confidence="0.92265">Ulisse University of Trento</affiliation>
<email confidence="0.975079">ulisse.bordignon@unitn.it</email>
<affiliation confidence="0.8216335">Irina University of Trento</affiliation>
<email confidence="0.987233">irina.sergienya@unitn.it</email>
<author confidence="0.798516">Adam</author>
<affiliation confidence="0.998801">University of Trento</affiliation>
<email confidence="0.992666">adam.liska@unitn.it</email>
<abstract confidence="0.995875083333333">VSEM is an open library for visual semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability. The software is accompanied by a website with supporting documentation and examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Andrews</author>
<author>Gabriella Vigliocco</author>
<author>David Vinson</author>
</authors>
<title>Integrating experiential and distributional data to learn semantic representations.</title>
<date>2009</date>
<journal>Psychological Review,</journal>
<volume>116</volume>
<issue>3</issue>
<contexts>
<context position="6396" citStr="Andrews et al., 2009" startWordPosition="974" endWordPosition="977">rds in text are produced (the visual words). In this way, from a corpus of images annotated with a set of concepts, it is possible to derive semantic vectors of co-occurrence counts of concepts and visual words akin to the representations of words in terms of textual collocates in standard distributional semantics. Impor5In recent years, a conspicuous literature of studies has surfaced, wherein demonstration was made of how text based models are not sufficiently good at capturing the environment we acquire language from. This is due to the fact that they are lacking of perceptual information (Andrews et al., 2009; Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and Jones, 2011). 6The authors of the aforementioned studies usually refer to words instead of concepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown </context>
</contexts>
<marker>Andrews, Vigliocco, Vinson, 2009</marker>
<rawString>Mark Andrews, Gabriella Vigliocco, and David Vinson. 2009. Integrating experiential and distributional data to learn semantic representations. Psychological Review, 116(3):463–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Pinar Duygulu</author>
<author>David Forsyth</author>
<author>Nando de Freitas</author>
<author>David Blei</author>
<author>Michael Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, Forsyth, de Freitas, Blei, Jordan, 2003</marker>
<rawString>Kobus Barnard, Pinar Duygulu, David Forsyth, Nando de Freitas, David Blei, and Michael Jordan. 2003. Matching words and pictures. Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Concepts and properties in word spaces.</title>
<date>2008</date>
<journal>Italian Journal of Linguistics,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="6441" citStr="Baroni and Lenci, 2008" startWordPosition="982" endWordPosition="985">. In this way, from a corpus of images annotated with a set of concepts, it is possible to derive semantic vectors of co-occurrence counts of concepts and visual words akin to the representations of words in terms of textual collocates in standard distributional semantics. Impor5In recent years, a conspicuous literature of studies has surfaced, wherein demonstration was made of how text based models are not sufficiently good at capturing the environment we acquire language from. This is due to the fact that they are lacking of perceptual information (Andrews et al., 2009; Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and Jones, 2011). 6The authors of the aforementioned studies usually refer to words instead of concepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform </context>
</contexts>
<marker>Baroni, Lenci, 2008</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2008. Concepts and properties in word spaces. Italian Journal of Linguistics, 20(1):55–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Eduard Barbu</author>
<author>Brian Murphy</author>
<author>Massimo Poesio</author>
</authors>
<title>Strudel: A distributional semantic model based on properties and types.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="6417" citStr="Baroni et al., 2010" startWordPosition="978" endWordPosition="981">ed (the visual words). In this way, from a corpus of images annotated with a set of concepts, it is possible to derive semantic vectors of co-occurrence counts of concepts and visual words akin to the representations of words in terms of textual collocates in standard distributional semantics. Impor5In recent years, a conspicuous literature of studies has surfaced, wherein demonstration was made of how text based models are not sufficiently good at capturing the environment we acquire language from. This is due to the fact that they are lacking of perceptual information (Andrews et al., 2009; Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and Jones, 2011). 6The authors of the aforementioned studies usually refer to words instead of concepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting mu</context>
</contexts>
<marker>Baroni, Barbu, Murphy, Poesio, 2010</marker>
<rawString>Marco Baroni, Eduard Barbu, Brian Murphy, and Massimo Poesio. 2010. Strudel: A distributional semantic model based on properties and types. Cognitive Science, 34(2):222–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Berg</author>
<author>Alexander Berg</author>
<author>Jonathan Shih</author>
</authors>
<title>Automatic attribute discovery and characterization from noisy Web data. In</title>
<date>2010</date>
<booktitle>ECCV,</booktitle>
<pages>663--676</pages>
<location>Crete, Greece.</location>
<contexts>
<context position="3553" citStr="Berg et al., 2010" startWordPosition="529" endWordPosition="532"> Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If we look at the image analysis community, we discover a well established tradition of studies that exploit both channels of information. For example, there is a relatively extended amount of literature about enhancing the performance on visual tasks such as object recognition or image retrieval by replacing a purely image-based pipeline with hybrid methods augmented with textual information (Barnard et al., 2003; Farhadi et al., 2009; Berg et al., 2010; Kulkarni et al., 2011). Unfortunately, the same cannot be said of the exploitation of image analysis from within the text community. Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computati</context>
</contexts>
<marker>Berg, Berg, Shih, 2010</marker>
<rawString>Tamara Berg, Alexander Berg, and Jonathan Shih. 2010. Automatic attribute discovery and characterization from noisy Web data. In ECCV, pages 663– 676, Crete, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In Proceedings of RANLP,</booktitle>
<pages>399--405</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="4337" citStr="Bergsma and Goebel, 2011" startWordPosition="635" endWordPosition="638">l that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The final goal of libraries such VLFeat and OpenCV is the representation an</context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In Proceedings of RANLP, pages 399–405, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="2534" citStr="Blei et al., 2003" startWordPosition="367" endWordPosition="370">ith discrete image-describing features. Visual words are identified by clustering a large corpus of lower-level continuous features. 2http://opencv.org/ and VLFeat.3 A comparable story can be told about automatic text analysis. The last decades have seen a long series of successes in the processing of large text corpora in order to extract more or less structured semantic knowledge. In particular, under the assumption that meaning can be captured by patterns of co-occurrences of words, distributional semantic models such as Latent Semantic Analysis (Landauer and Dumais, 1997) or Topic Models (Blei et al., 2003) have been shown to be very effective both in general semantic tasks such as approximating human intuitions about meaning, as well as in more application-driven tasks such as information retrieval, word disambiguation and query expansion (Turney and Pantel, 2010). And also in the case of automated text analysis, a wide range of method implementations are at the disposal of the scientific community.4 Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If we look at the image analysis commun</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Giang Binh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Distributional semantics from text and images.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP GEMS Workshop,</booktitle>
<pages>22--32</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="4285" citStr="Bruni et al., 2011" startWordPosition="627" endWordPosition="630"> the text community. Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The final goal of libra</context>
<context position="6975" citStr="Bruni et al., 2011" startWordPosition="1060" endWordPosition="1063">eptual information (Andrews et al., 2009; Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and Jones, 2011). 6The authors of the aforementioned studies usually refer to words instead of concepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness ((Feng and Lapata, 2010; Bruni et al., 2012b)). VSEM functionalities concerning image analysis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM arc</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2011</marker>
<rawString>Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011. Distributional semantics from text and images. In Proceedings of the EMNLP GEMS Workshop, pages 22–32, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>136--145</pages>
<location>Jeju Island,</location>
<contexts>
<context position="4357" citStr="Bruni et al., 2012" startWordPosition="639" endWordPosition="642">ed visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The final goal of libraries such VLFeat and OpenCV is the representation and classification of </context>
<context position="7189" citStr="Bruni et al., 2012" startWordPosition="1092" endWordPosition="1095">l them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness ((Feng and Lapata, 2010; Bruni et al., 2012b)). VSEM functionalities concerning image analysis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM architecture. Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set. Section 5 concludes summarizing the material and discussing further directions. 2 Background As shown by </context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012a. Distributional semantics in Technicolor. In Proceedings of ACL, pages 136– 145, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Jasper Uijlings</author>
<author>Marco Baroni</author>
<author>Nicu Sebe</author>
</authors>
<title>Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning.</title>
<date>2012</date>
<booktitle>In Proceedings of ACM Multimedia,</booktitle>
<pages>1219--1228</pages>
<location>Nara, Japan.</location>
<contexts>
<context position="4357" citStr="Bruni et al., 2012" startWordPosition="639" endWordPosition="642">ed visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The final goal of libraries such VLFeat and OpenCV is the representation and classification of </context>
<context position="7189" citStr="Bruni et al., 2012" startWordPosition="1092" endWordPosition="1095">l them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness ((Feng and Lapata, 2010; Bruni et al., 2012b)). VSEM functionalities concerning image analysis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM architecture. Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set. Section 5 concludes summarizing the material and discussing further directions. 2 Background As shown by </context>
</contexts>
<marker>Bruni, Uijlings, Baroni, Sebe, 2012</marker>
<rawString>Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu Sebe. 2012b. Distributional semantics with eyes: Using image analysis to improve computational representations of word meaning. In Proceedings of ACM Multimedia, pages 1219–1228, Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Chatfield</author>
<author>Victor Lempitsky</author>
<author>Andrea Vedaldi</author>
<author>Andrew Zisserman</author>
</authors>
<title>The devil is in the details: an evaluation of recent feature encoding methods.</title>
<date>2011</date>
<booktitle>In Proceedings of BMVC,</booktitle>
<location>Dundee, UK.</location>
<contexts>
<context position="11021" citStr="Chatfield et al., 2011" startWordPosition="1701" endWordPosition="1704">. (2010). Figure 1 exemplifies a visual vocabulary construction pipeline. VSEM contains both the k-means and the GMM implementations. Encoding The encoding step maps the local features extracted from an image to the corresponding visual words of the previously created vocabulary. The most common encoding strategy is called hard quantization, which assigns each feature to the nearest visual word’s centroid (in Euclidean distance). Recently, more effective encoding methods have been introduced, among which the Fisher encoding (Perronnin et al., 2010) has been shown to outperform all the others (Chatfield et al., 2011). VSEM uses both the hard quantization and the Fisher encoding. Spatial binning A consolidated way of introducing spatial information in BoVW is the use of spatial histograms (Lazebnik et al., 2006). The main idea is to divide the image into several (spatial) regions, compute the encoding for each region and stack the resulting histograms. This technique is referred to as spatial binning and it is implemented in VSEM. Figure 2 exemplifies the BoVW pipeline for a single image, involving local features extraction, encoding and spatial binning. Figure 2: An example of a BoVW representation pipeli</context>
</contexts>
<marker>Chatfield, Lempitsky, Vedaldi, Zisserman, 2011</marker>
<rawString>Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, and Andrew Zisserman. 2011. The devil is in the details: an evaluation of recent feature encoding methods. In Proceedings of BMVC, Dundee, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences. Dissertation,</title>
<date>2005</date>
<institution>Stuttgart University.</institution>
<contexts>
<context position="13721" citStr="Evert (2005)" startWordPosition="2128" endWordPosition="2129">pt vector. figure 3. VSEM offers an implementation of the sum function. Transformations Once the conceptrepresenting visual vectors are built, two types of transformation can be performed over them to refine their raw visual word counts: association scores and dimensionality reduction. So far, the vectors that we have obtained represent cooccurrence counts of visual words with concepts. The goal of association scores is to distinguish interesting co-occurrences from those that are due to chance. In order to do this, VSEM implements two versions of mutual information (pointwise and local), see Evert (2005). On the other hand, dimensionality reduction leads to matrices that are smaller and easier to work with. Moreover, some techniques are able to smooth the matrices and uncover latent dimensions. Common dimensionality reduction methods are singular value decomposition (Manning et al., 2008), non-negative matrix factorization (Lee and Seung, 2001) and neural networks (Hinton and Salakhutdinov, 2006). VSEM implements the singular value decomposition method. 3 Framework design VSEM offers a friendly implementation of the pipeline described in Section 2. The framework is organized into five parts, </context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Cooccurrences. Dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Ian Endres</author>
<author>Derek Hoiem</author>
<author>David Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>1778--1785</pages>
<location>Miami Beach, FL.</location>
<contexts>
<context position="3534" citStr="Farhadi et al., 2009" startWordPosition="525" endWordPosition="528">scientific community.4 Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If we look at the image analysis community, we discover a well established tradition of studies that exploit both channels of information. For example, there is a relatively extended amount of literature about enhancing the performance on visual tasks such as object recognition or image retrieval by replacing a purely image-based pipeline with hybrid methods augmented with textual information (Barnard et al., 2003; Farhadi et al., 2009; Berg et al., 2010; Kulkarni et al., 2011). Unfortunately, the same cannot be said of the exploitation of image analysis from within the text community. Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Associ</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. 2009. Describing objects by their attributes. In Proceedings of CVPR, pages 1778– 1785, Miami Beach, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>91--99</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="4265" citStr="Feng and Lapata, 2010" startWordPosition="623" endWordPosition="626">ge analysis from within the text community. Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The</context>
<context position="7169" citStr="Feng and Lapata, 2010" startWordPosition="1088" endWordPosition="1091">ncepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness ((Feng and Lapata, 2010; Bruni et al., 2012b)). VSEM functionalities concerning image analysis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM architecture. Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set. Section 5 concludes summarizing the material and discussing further directions. 2 Bac</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In Proceedings of HLT-NAACL, pages 91–99, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristen Grauman</author>
<author>Bastian Leibe</author>
</authors>
<title>Visual Object Recognition.</title>
<date>2011</date>
<publisher>Morgan &amp; Claypool,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="9671" citStr="Grauman and Leibe, 2011" startWordPosition="1481" endWordPosition="1484">ty of the data. A brief description of the individual 188 Figure 1: An example of a visual vocabulary creation pipeline. From a set of images, a larger set of features are extracted and clustered, forming the visual vocabulary. steps follows. Local features Local features are designed to find local image structures in a repeatable fashion and to represent them in robust ways that are invariant to typical image transformations, such as translation, rotation, scaling, and affine deformation. Local features constitute the basis of approaches developed to automatically recognize specific objects (Grauman and Leibe, 2011). The most popular local feature extraction method is the Scale Invariant Feature Transform (SIFT), introduced by Lowe (2004). VSEM uses the VLFeat implementation of SIFT. Visual vocabulary To obtain a BoVW representation of the image content, a large set of local features extracted from a large corpus of images are clustered. In this way the local feature space is divided into informative regions (visual words) and the collection of the obtained visual words is called visual vocabulary. k-means is the most commonly used clustering algorithm (Grauman and Leibe, 2011). In the special case of Fi</context>
</contexts>
<marker>Grauman, Leibe, 2011</marker>
<rawString>Kristen Grauman and Bastian Leibe. 2011. Visual Object Recognition. Morgan &amp; Claypool, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Hinton</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Reducing the dimensionality of data with neural networks.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>313</volume>
<issue>5786</issue>
<pages>507</pages>
<contexts>
<context position="14121" citStr="Hinton and Salakhutdinov, 2006" startWordPosition="2183" endWordPosition="2186">oncepts. The goal of association scores is to distinguish interesting co-occurrences from those that are due to chance. In order to do this, VSEM implements two versions of mutual information (pointwise and local), see Evert (2005). On the other hand, dimensionality reduction leads to matrices that are smaller and easier to work with. Moreover, some techniques are able to smooth the matrices and uncover latent dimensions. Common dimensionality reduction methods are singular value decomposition (Manning et al., 2008), non-negative matrix factorization (Lee and Seung, 2001) and neural networks (Hinton and Salakhutdinov, 2006). VSEM implements the singular value decomposition method. 3 Framework design VSEM offers a friendly implementation of the pipeline described in Section 2. The framework is organized into five parts, which correspond to an equal number of MATLAB packages and it is written in object-oriented programming to encourage reusability. A description of the packages follows. • datasets This package contains the code that manages the image data sets. We already provide a generic wrapper for several possible dataset formats (VsemDataset ). Therefore, to use a new image data set two solutions are possible</context>
</contexts>
<marker>Hinton, Salakhutdinov, 2006</marker>
<rawString>Geoffrey Hinton and Ruslan Salakhutdinov. 2006. Reducing the dimensionality of data with neural networks. Science, 313(5786):504 – 507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<location>Colorado Springs, MSA.</location>
<contexts>
<context position="3577" citStr="Kulkarni et al., 2011" startWordPosition="533" endWordPosition="536">e parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If we look at the image analysis community, we discover a well established tradition of studies that exploit both channels of information. For example, there is a relatively extended amount of literature about enhancing the performance on visual tasks such as object recognition or image retrieval by replacing a purely image-based pipeline with hybrid methods augmented with textual information (Barnard et al., 2003; Farhadi et al., 2009; Berg et al., 2010; Kulkarni et al., 2011). Unfortunately, the same cannot be said of the exploitation of image analysis from within the text community. Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semanti</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In Proceedings of CVPR, Colorado Springs, MSA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="2498" citStr="Landauer and Dumais, 1997" startWordPosition="359" endWordPosition="363">ormation Retrieval. It represents an image with discrete image-describing features. Visual words are identified by clustering a large corpus of lower-level continuous features. 2http://opencv.org/ and VLFeat.3 A comparable story can be told about automatic text analysis. The last decades have seen a long series of successes in the processing of large text corpora in order to extract more or less structured semantic knowledge. In particular, under the assumption that meaning can be captured by patterns of co-occurrences of words, distributional semantic models such as Latent Semantic Analysis (Landauer and Dumais, 1997) or Topic Models (Blei et al., 2003) have been shown to be very effective both in general semantic tasks such as approximating human intuitions about meaning, as well as in more application-driven tasks such as information retrieval, word disambiguation and query expansion (Turney and Pantel, 2010). And also in the case of automated text analysis, a wide range of method implementations are at the disposal of the scientific community.4 Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Lazebnik</author>
<author>Cordelia Schmid</author>
<author>Jean Ponce</author>
</authors>
<title>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories.</title>
<date>2006</date>
<booktitle>In Proceedings of CVPR,</booktitle>
<pages>2169--2178</pages>
<location>Washington, DC.</location>
<contexts>
<context position="11219" citStr="Lazebnik et al., 2006" startWordPosition="1734" endWordPosition="1737"> an image to the corresponding visual words of the previously created vocabulary. The most common encoding strategy is called hard quantization, which assigns each feature to the nearest visual word’s centroid (in Euclidean distance). Recently, more effective encoding methods have been introduced, among which the Fisher encoding (Perronnin et al., 2010) has been shown to outperform all the others (Chatfield et al., 2011). VSEM uses both the hard quantization and the Fisher encoding. Spatial binning A consolidated way of introducing spatial information in BoVW is the use of spatial histograms (Lazebnik et al., 2006). The main idea is to divide the image into several (spatial) regions, compute the encoding for each region and stack the resulting histograms. This technique is referred to as spatial binning and it is implemented in VSEM. Figure 2 exemplifies the BoVW pipeline for a single image, involving local features extraction, encoding and spatial binning. Figure 2: An example of a BoVW representation pipeline for an image. Figure inspired by Chatfield et al. (2011). Each feature extracted from the target image is assigned to the corresponding visual word(s). Then, spatial binning is performed. Moreove</context>
</contexts>
<marker>Lazebnik, Schmid, Ponce, 2006</marker>
<rawString>Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Proceedings of CVPR, pages 2169–2178, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization. In</title>
<date>2001</date>
<booktitle>In NIPS,</booktitle>
<pages>556--562</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14068" citStr="Lee and Seung, 2001" startWordPosition="2176" endWordPosition="2179">cooccurrence counts of visual words with concepts. The goal of association scores is to distinguish interesting co-occurrences from those that are due to chance. In order to do this, VSEM implements two versions of mutual information (pointwise and local), see Evert (2005). On the other hand, dimensionality reduction leads to matrices that are smaller and easier to work with. Moreover, some techniques are able to smooth the matrices and uncover latent dimensions. Common dimensionality reduction methods are singular value decomposition (Manning et al., 2008), non-negative matrix factorization (Lee and Seung, 2001) and neural networks (Hinton and Salakhutdinov, 2006). VSEM implements the singular value decomposition method. 3 Framework design VSEM offers a friendly implementation of the pipeline described in Section 2. The framework is organized into five parts, which correspond to an equal number of MATLAB packages and it is written in object-oriented programming to encourage reusability. A description of the packages follows. • datasets This package contains the code that manages the image data sets. We already provide a generic wrapper for several possible dataset formats (VsemDataset ). Therefore, t</context>
</contexts>
<marker>Lee, Seung, 2001</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2001. Algorithms for non-negative matrix factorization. In In NIPS, pages 556–562. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="4311" citStr="Leong and Mihalcea, 2011" startWordPosition="631" endWordPosition="634"> Despite the huge potential that automatically induced visual features could represent as a new source of perceptually grounded 3http://www.vlfeat.org/ 4See for example the annotated list of corpus-based computational linguistics resources at http://www-nlp. stanford.edu/links/statnlp.html. 187 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 187–192, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics semantic knowledge,5 image-enhanced models of semantics developed so far (Feng and Lapata, 2010; Bruni et al., 2011; Leong and Mihalcea, 2011; Bergsma and Goebel, 2011; Bruni et al., 2012a; Bruni et al., 2012b) have only scratched this great potential and are still considered as proof-of-concept studies only. One possible reason of this delay with respect to the image analysis community might be ascribed to the high entry barriers that NLP researchers adopting image analysis methods have to face. Although many of the image analysis toolkits are open source and well documented, they mainly address users within the same community and therefore their use is not as intuitive for others. The final goal of libraries such VLFeat and OpenC</context>
<context position="7862" citStr="Leong and Mihalcea (2011)" startWordPosition="1200" endWordPosition="1203">sis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM architecture. Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set. Section 5 concludes summarizing the material and discussing further directions. 2 Background As shown by Feng and Lapata (2010), Bruni et al. (2011) and Leong and Mihalcea (2011), it is possible to construct an image-based representation of a set of target concepts by starting from a collection of images depicting those concepts, encoding the image contents into low-level features (e.g., SIFT) and scaling up to a higher level representation, based on the well-established BoVW method to represent images. In addition, as shown by Bruni et al. (2012b), better representations can be extracted if the object depicting the concept is first localized in the image. More in detail, the pipeline encapsulating the whole process mentioned above takes as input a collection of image</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In Proceedings of IJCNLP, pages 1403–1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="9796" citStr="Lowe (2004)" startWordPosition="1502" endWordPosition="1503">ges, a larger set of features are extracted and clustered, forming the visual vocabulary. steps follows. Local features Local features are designed to find local image structures in a repeatable fashion and to represent them in robust ways that are invariant to typical image transformations, such as translation, rotation, scaling, and affine deformation. Local features constitute the basis of approaches developed to automatically recognize specific objects (Grauman and Leibe, 2011). The most popular local feature extraction method is the Scale Invariant Feature Transform (SIFT), introduced by Lowe (2004). VSEM uses the VLFeat implementation of SIFT. Visual vocabulary To obtain a BoVW representation of the image content, a large set of local features extracted from a large corpus of images are clustered. In this way the local feature space is divided into informative regions (visual words) and the collection of the obtained visual words is called visual vocabulary. k-means is the most commonly used clustering algorithm (Grauman and Leibe, 2011). In the special case of Fisher encoding (see below), the clustering of the features is performed with a Gaussian mixture model (GMM), see Perronnin et </context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Schütze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="14011" citStr="Manning et al., 2008" startWordPosition="2169" endWordPosition="2172">tion. So far, the vectors that we have obtained represent cooccurrence counts of visual words with concepts. The goal of association scores is to distinguish interesting co-occurrences from those that are due to chance. In order to do this, VSEM implements two versions of mutual information (pointwise and local), see Evert (2005). On the other hand, dimensionality reduction leads to matrices that are smaller and easier to work with. Moreover, some techniques are able to smooth the matrices and uncover latent dimensions. Common dimensionality reduction methods are singular value decomposition (Manning et al., 2008), non-negative matrix factorization (Lee and Seung, 2001) and neural networks (Hinton and Salakhutdinov, 2006). VSEM implements the singular value decomposition method. 3 Framework design VSEM offers a friendly implementation of the pipeline described in Section 2. The framework is organized into five parts, which correspond to an equal number of MATLAB packages and it is written in object-oriented programming to encourage reusability. A description of the packages follows. • datasets This package contains the code that manages the image data sets. We already provide a generic wrapper for seve</context>
</contexts>
<marker>Manning, Raghavan, Schütze, 2008</marker>
<rawString>Chris Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mikolajczyk</author>
<author>T Tuytelaars</author>
<author>C Schmid</author>
<author>A Zisserman</author>
<author>J Matas</author>
<author>F Schaffalitzky</author>
<author>T Kadir</author>
<author>L V Gool</author>
</authors>
<title>A Comparison of Affine Region Detectors.</title>
<date>2005</date>
<journal>International Journal of Computer Vision,</journal>
<volume>65</volume>
<issue>1</issue>
<contexts>
<context position="1011" citStr="Mikolajczyk et al. (2005)" startWordPosition="142" endWordPosition="145"> semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability. The software is accompanied by a website with supporting documentation and examples. 1 Introduction In the last years we have witnessed great progress in the area of automated image analysis. Important advances, such as the introduction of local features for a robust description of the image content (see Mikolajczyk et al. (2005) for a systematic review) and the bag-of-visual-words method (BoVW)1 for a standard representation across multiple images (Sivic and Zisserman, 2003), have contributed to make image analysis ubiquitous, with applications ranging from robotics to biology, from medicine to photography. Two facts have played a key role in the rapid advance of these ideas. First, the introduction of very well defined challenges which have been attracting also a wide community of “outsiders&amp;quot; specialized in a variety of disciplines (e.g., machine learning, neural networks, graphical models and natural language proce</context>
</contexts>
<marker>Mikolajczyk, Tuytelaars, Schmid, Zisserman, Matas, Schaffalitzky, Kadir, Gool, 2005</marker>
<rawString>K. Mikolajczyk, T. Tuytelaars, C. Schmid, A. Zisserman, J. Matas, F. Schaffalitzky, T. Kadir, and L. V. Gool. 2005. A Comparison of Affine Region Detectors. International Journal of Computer Vision, 65(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florent Perronnin</author>
<author>Jorge Sanchez</author>
<author>Thomas Mensink</author>
</authors>
<title>Improving the fisher kernel for large-scale image classification.</title>
<date>2010</date>
<booktitle>In Proceedings of ECCV,</booktitle>
<pages>143--156</pages>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="10406" citStr="Perronnin et al. (2010)" startWordPosition="1604" endWordPosition="1607">y Lowe (2004). VSEM uses the VLFeat implementation of SIFT. Visual vocabulary To obtain a BoVW representation of the image content, a large set of local features extracted from a large corpus of images are clustered. In this way the local feature space is divided into informative regions (visual words) and the collection of the obtained visual words is called visual vocabulary. k-means is the most commonly used clustering algorithm (Grauman and Leibe, 2011). In the special case of Fisher encoding (see below), the clustering of the features is performed with a Gaussian mixture model (GMM), see Perronnin et al. (2010). Figure 1 exemplifies a visual vocabulary construction pipeline. VSEM contains both the k-means and the GMM implementations. Encoding The encoding step maps the local features extracted from an image to the corresponding visual words of the previously created vocabulary. The most common encoding strategy is called hard quantization, which assigns each feature to the nearest visual word’s centroid (in Euclidean distance). Recently, more effective encoding methods have been introduced, among which the Fisher encoding (Perronnin et al., 2010) has been shown to outperform all the others (Chatfiel</context>
</contexts>
<marker>Perronnin, Sanchez, Mensink, 2010</marker>
<rawString>Florent Perronnin, Jorge Sanchez, and Thomas Mensink. 2010. Improving the fisher kernel for large-scale image classification. In Proceedings of ECCV, pages 143–156, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Riordan</author>
<author>Michael Jones</author>
</authors>
<title>Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation.</title>
<date>2011</date>
<journal>Topics in Cognitive Science,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>43</pages>
<contexts>
<context position="6467" citStr="Riordan and Jones, 2011" startWordPosition="986" endWordPosition="989">rpus of images annotated with a set of concepts, it is possible to derive semantic vectors of co-occurrence counts of concepts and visual words akin to the representations of words in terms of textual collocates in standard distributional semantics. Impor5In recent years, a conspicuous literature of studies has surfaced, wherein demonstration was made of how text based models are not sufficiently good at capturing the environment we acquire language from. This is due to the fact that they are lacking of perceptual information (Andrews et al., 2009; Baroni et al., 2010; Baroni and Lenci, 2008; Riordan and Jones, 2011). 6The authors of the aforementioned studies usually refer to words instead of concepts. We chose to call them concepts to account for the both theoretical and practical differences standing between a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only mode</context>
</contexts>
<marker>Riordan, Jones, 2011</marker>
<rawString>Brian Riordan and Michael Jones. 2011. Redundancy in perceptual and linguistic experience: Comparing feature-based and distributional models of semantic representation. Topics in Cognitive Science, 3(2):1– 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video Google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1470--1477</pages>
<location>Nice, France.</location>
<contexts>
<context position="1160" citStr="Sivic and Zisserman, 2003" startWordPosition="162" endWordPosition="165">g off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability. The software is accompanied by a website with supporting documentation and examples. 1 Introduction In the last years we have witnessed great progress in the area of automated image analysis. Important advances, such as the introduction of local features for a robust description of the image content (see Mikolajczyk et al. (2005) for a systematic review) and the bag-of-visual-words method (BoVW)1 for a standard representation across multiple images (Sivic and Zisserman, 2003), have contributed to make image analysis ubiquitous, with applications ranging from robotics to biology, from medicine to photography. Two facts have played a key role in the rapid advance of these ideas. First, the introduction of very well defined challenges which have been attracting also a wide community of “outsiders&amp;quot; specialized in a variety of disciplines (e.g., machine learning, neural networks, graphical models and natural language processing). Second, the sharing of effective, well documented implementations of cutting edge image analysis algorithms, such as OpenCV2 1Bag-of-visual-w</context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video Google: A text retrieval approach to object matching in videos. In Proceedings of ICCV, pages 1470– 1477, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="2797" citStr="Turney and Pantel, 2010" startWordPosition="408" endWordPosition="411">long series of successes in the processing of large text corpora in order to extract more or less structured semantic knowledge. In particular, under the assumption that meaning can be captured by patterns of co-occurrences of words, distributional semantic models such as Latent Semantic Analysis (Landauer and Dumais, 1997) or Topic Models (Blei et al., 2003) have been shown to be very effective both in general semantic tasks such as approximating human intuitions about meaning, as well as in more application-driven tasks such as information retrieval, word disambiguation and query expansion (Turney and Pantel, 2010). And also in the case of automated text analysis, a wide range of method implementations are at the disposal of the scientific community.4 Nowadays, given the parallel success of the two disciplines, there is growing interest in making the visual and textual channels interact for mutual benefit. If we look at the image analysis community, we discover a well established tradition of studies that exploit both channels of information. For example, there is a relatively extended amount of literature about enhancing the performance on visual tasks such as object recognition or image retrieval by r</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R R Uijlings</author>
<author>K E A van de Sande</author>
<author>T Gevers</author>
<author>A W M Smeulders</author>
</authors>
<title>Selective search for object recognition.</title>
<date>2013</date>
<publisher>IJCV.</publisher>
<marker>Uijlings, van de Sande, Gevers, Smeulders, 2013</marker>
<rawString>J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and A.W.M. Smeulders. 2013. Selective search for object recognition. IJCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Vedaldi</author>
<author>Brian Fulkerson</author>
</authors>
<title>Vlfeat – an open and portable library of computer vision algorithms.</title>
<date>2010</date>
<booktitle>In Proceedings of ACM Multimedia,</booktitle>
<pages>1469--1472</pages>
<location>Firenze, Italy.</location>
<contexts>
<context position="7289" citStr="Vedaldi and Fulkerson, 2010" startWordPosition="1106" endWordPosition="1109">ween a word and the perceptual information it brings along, which we define its concept. 7http://clic.cimec.unitn.it/vsem/ tantly, the obtained visual semantic vectors can be easily combined with more traditional text-based vectors to arrive at a multimodal representation of meaning (see e.g. (Bruni et al., 2011)). It has been shown that the resulting multimodal models perform better than text-only models in semantic tasks such as approximating semantic similarity and relatedness ((Feng and Lapata, 2010; Bruni et al., 2012b)). VSEM functionalities concerning image analysis is based on VLFeat (Vedaldi and Fulkerson, 2010). This guarantees that the image analysis underpinnings of the library are well maintained and state-of-the-art. The rest of the paper is organized as follows. In Section 2 we introduce the procedure to obtain an image-based representation of a concept. Section 3 describes the VSEM architecture. Section 4 shows how to install and run VSEM through an example that uses the Pascal VOC data set. Section 5 concludes summarizing the material and discussing further directions. 2 Background As shown by Feng and Lapata (2010), Bruni et al. (2011) and Leong and Mihalcea (2011), it is possible to constru</context>
</contexts>
<marker>Vedaldi, Fulkerson, 2010</marker>
<rawString>Andrea Vedaldi and Brian Fulkerson. 2010. Vlfeat – an open and portable library of computer vision algorithms. In Proceedings of ACM Multimedia, pages 1469–1472, Firenze, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>