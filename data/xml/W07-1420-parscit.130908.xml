<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000650">
<title confidence="0.996741">
Textual Entailment Through Extended Lexical Overlap and
Lexico-Semantic Matching
</title>
<author confidence="0.99603">
Rod Adams, Gabriel Nicolae, Cristina Nicolae and Sanda Harabagiu
</author>
<affiliation confidence="0.98606">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.605428">
Richardson, Texas
</address>
<email confidence="0.985874">
{rod, gabriel, cristina, sanda}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979333333333">
This paper presents two systems for textual
entailment, both employing decision trees
as a supervised learning algorithm. The
first one is based primarily on the con-
cept of lexical overlap, considering a bag of
words similarity overlap measure to form a
mapping of terms in the hypothesis to the
source text. The second system is a lexico-
semantic matching between the text and the
hypothesis that attempts an alignment be-
tween chunks in the hypothesis and chunks
in the text, and a representation of the text
and hypothesis as two dependency graphs.
Their performances are compared and their
positive and negative aspects are analyzed.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993018181818182">
Textual entailment is the task of taking a pair of pas-
sages, referred to as the text and the hypothesis, and
labeling whether or not the hypothesis (H) can be
fully inferred from the text (T), as is illustrated in
Pair 1. In Pair 1, the knowledge that an attorney rep-
resenting someone’s interests entails that they work
for that person.
Pair 1 (RTE2 IE 58)
T: “A force majeure is an act of God,” said attorney Phil
Wittmann, who represents the New Orleans Saints and owner
Tom Benson’s local interests.
</bodyText>
<subsectionHeader confidence="0.678305">
H: Phil Wittmann works for Tom Benson.
</subsectionHeader>
<bodyText confidence="0.9419625">
The Third PASCAL Recognizing Textual Entail-
ment Challenge&apos; follows the experience of the sec-
</bodyText>
<footnote confidence="0.299121">
lhttp://www.pascal-network.org/Challenges/RTE3/
</footnote>
<bodyText confidence="0.999579617647059">
ond challenge (Bar-Haim et al., 2006), whose main
task was to automatically detect if a hypothesis H
is entailed by a text T. To increase the “reality” of
the task, the text-hypothesis examples were taken
from outputs of actual systems that solved appli-
cations like Question Answering (QA), Informa-
tion Retrieval (IR), Information Extraction (IE) and
Summarization (SUM).
In the challenge, there are two corpora, each con-
sisting of 800 annotated pairs of texts and hypothe-
ses. Pairs are annotated as to whether there exists
a positive entailment between them and from which
application domain each example came from. In-
stances are distributed evenly among the four tasks
in both corpora, as are the positive and negative ex-
amples. One corpus was designated for development
and training, while the other was reserved for test-
ing.
In the Second PASCAL RTE Challenge (Bar-
Haim et al., 2006), one of the best performing sub-
missions was (Adams, 2006), which focused on
strict lexical methods so that the system could re-
main relatively simple and be easily applied to var-
ious entailment applications. However, this simple
approach did not take into account details like the
syntactic structure, the coreference or the semantic
relations between words, all necessary for a deeper
understanding of natural language text. Thus, a new
system, based on the same decision tree learning al-
gorithm, was designed in an attempt to gain perfor-
mance by adding alignment and dependency rela-
tions information. The two systems will be com-
pared and their advantages and disadvantages dis-
cussed.
</bodyText>
<page confidence="0.991968">
119
</page>
<bodyText confidence="0.906134142857143">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 119–124,
Prague, June 2007. c�2007 Association for Computational Linguistics
This paper is organized as follows: The first sys-
tem is discussed in Section 2, followed by the sec-
ond system in Section 3. The experimental results
are presented in Section 4, and the paper concludes
in Section 5.
</bodyText>
<sectionHeader confidence="0.598163" genericHeader="method">
2 Textual entailment through extended
lexical overlap
</sectionHeader>
<bodyText confidence="0.999748181818182">
The first system (Adams, 2006) follows a four step
framework. The first step is a tokenization process
that applies to the content words of the text and
hypothesis. The second step is building a “token
map” of how the individual tokens in the hypoth-
esis are tied to those in the text, as explained in
Section 2.1. Thirdly, several features, as described
in Section 2.2, are extracted from the token map.
Finally, the extracted features are fed into Weka’s
(Witten and Frank, 2005) J48 decision tree for train-
ing and evaluation.
</bodyText>
<subsectionHeader confidence="0.996931">
2.1 The token map
</subsectionHeader>
<bodyText confidence="0.9990734">
Central to this system is the concept of the token
map. This map is inspired by (Glickman et al.,
2005)’s use of the most probable lexical entailment
for each hypothesis pair, but has been modified in
how each pair is evaluated, and that the mapping
is stored for additional extraction of features. The
complete mapping is a list of (Hi, Tj) mappings,
where Hi represents the ith token in the hypothesis,
and Tj is similarly the jth token in the text. Each
mapping has an associated similarity score. There is
one mapping per token in the hypothesis. Text to-
kens are allowed to appear in multiple mappings.
The mappings are created by considering each hy-
pothesis token and comparing it to each token in the
text and keeping the one with the highest similarity
score.
Similarity scores A similarity score ranging from
0.0 to 1.0 is computed for any two tokens via a com-
bination of two scores. This score can be thought of
as the probability that the text token implies the hy-
pothesis one, even though the methods used to pro-
duce it were not strictly probabilistic in nature.
The first score is derived from the cost of a Word-
Net (Fellbaum, 1998) path. The WordNet paths
between two tokens are built with the method re-
ported in (Hirst and St-Onge, 1998), and designated
as SimWN(Hi, Tj). Exact word matches are al-
ways given a score of 1.0, words that are morpho-
logically related or that share a common sense are
0.9 and other paths give lower scores down to 0.0.
This method of obtaining a path makes use of three
groups of WordNet relations: Up (e.g. hypernym,
member meronym), Down (e.g. hyponym, cause)
and Horizontal (e.g. nominalization, derivation).
The path can only follow certain combinations of
these groupings, and assigns penalties for each link
in the path, as well as for changing from one direc-
tion group to another.
The secondary scoring routine is the lexical en-
tailment probability, lep(u, v), from (Glickman et
al., 2005). This probability is estimated by taking
the page counts returned from the AltaVista2 search
engine for a combined u and v search term, and di-
viding by the count for just the v term. This can be
precisely expressed as:
</bodyText>
<equation confidence="0.9830035">
AVCount(Hi &amp;Tj)
SimAV (Hi, Tj) = AVCount(Tj)
</equation>
<bodyText confidence="0.99982275">
The two scores are combined such that the sec-
ondary score can take up whatever slack the domi-
nant score leaves available. The exact combination
is:
</bodyText>
<equation confidence="0.974131">
Sim(Hi,Tj) = SimWN(Hh,Tt)
+ α · (1 − SimWN(Hh, Tt)) · SimAV (Hh, Tt)
</equation>
<bodyText confidence="0.999870666666667">
where α is a tuned constant (α E [0,1]). Empirical
analysis found the best results with very low values
of α3. This particular combination was chosen over
a strict linear combination, so as to more strongly re-
late to SimWN when it’s values are high, but allow
SimAV to play a larger role when SimWN is low.
</bodyText>
<subsectionHeader confidence="0.999341">
2.2 Feature extraction
</subsectionHeader>
<bodyText confidence="0.99948">
The following three features were constructed from
the token map for use in the training of the decision
tree, and producing entailment predictions.
Baseline score This score is the product of the
similarities of the mapped pairs, and is an extension
of (Glickman et al., 2005)’s notion of P(H|T). This
</bodyText>
<footnote confidence="0.991839">
2http://www.av.com
3The results reported here used α = 0.1
</footnote>
<page confidence="0.965637">
120
</page>
<figure confidence="0.498926444444445">
identity 1.0 coreference 0.8
synonymy 0.8 antonymy -0.8
hypernymy 0.5 hyponymy -0.5
meronymy 0.4 holonymy -0.4
entailment 0.6 entailed by -0.6
cause 0.6 caused by -0.6
is the base feature of entailment.
5coreBASE = ri Sim(HZ, TJ)
(Hi,Tj)EMap
</figure>
<bodyText confidence="0.999542866666667">
One notable characteristic of this feature is that
the overall score can be no higher than the lowest
score of any single mapping. The failure to locate a
strong similarity for even one token will produce a
very low base score.
Unmapped negations A token is considered un-
mapped if it does not appear in any pair of the token
map, or if the score associated with that mapping is
zero. A token is considered a negation if it is in a set
list of terms such as no or not. Both the text and
the hypothesis are searched for unmapped negations,
and total count of them is kept, with the objective of
determining whether there is an odd or even num-
ber of them. A (possibly) modified, or flipped, score
feature is generated:
</bodyText>
<equation confidence="0.9957888">
n = # of negations found.
�
5coreNEG 5coreBASE if n is even,
=
1 − 5coreBASE if n is odd.
</equation>
<bodyText confidence="0.99974925">
Task The task domain used for evaluating entail-
ment (i.e. IE, IR, QA or SUM) was also used as a
feature to allow different thresholds among the do-
mains.
</bodyText>
<sectionHeader confidence="0.9211865" genericHeader="method">
3 Textual entailment through
lexico-semantic matching
</sectionHeader>
<bodyText confidence="0.999775538461539">
This second system obtains the probability of entail-
ment between a text and a hypothesis from a su-
pervised learning algorithm that incorporates lexi-
cal and semantic information extracted from Word-
Net and PropBank. To generate learning examples,
the system computes features that are based upon
the alignment between chunks from the text and the
hypothesis. In the preliminary stage, each instance
pair of text and hypothesis is processed by a chunker.
The resulting chunks can be simple tokens or com-
pound words that exist in WordNet, e.g., pick up.
They constitute the lexical units in the next stages of
the algorithm.
</bodyText>
<tableCaption confidence="0.971545">
Table 2: Alignment relations and their scores.
</tableCaption>
<subsectionHeader confidence="0.997744">
3.1 Alignment
</subsectionHeader>
<bodyText confidence="0.999947684210526">
Once all the chunks have been identified, the sys-
tem searches for alignment candidates between the
chunks of the hypothesis and those of the text. The
search pairs all the chunks of the hypothesis, in turn,
with all the text chunks, and for each pair it ex-
tracts all the relations between the two nodes. Stop
words and auxiliary verbs are discarded, and only
two chunks with the same part of speech are com-
pared (a noun must be transformed into a verb to
compare it with another verb). The alignments ob-
tained in this manner constitute a one-to-many map-
ping between the chunks of the hypothesis and the
chunks of the text.
The following relations are identified: (a) iden-
tity (between the original spellings, lowercase forms
or stems), (b) coreference and (c) WordNet relations
(synonymy, antonymy, hypernymy, meronymy, en-
tailment and causation). Each of these relations is
attached to a score between -1 and 1, which is hand-
crafted by trial and error on the development set (Ta-
ble 2).
The score is positive if the relation from the text
word to the hypothesis word is compatible with an
entailment, e.g., identity, coreference, synonymy,
hypernymy, meronymy, entailment and causation,
and negative in the opposite case, e.g., antonymy,
hyponymy, holonymy, reverse entailment and re-
verse causation. This is a way of quantifying in-
tuitions like: “The cat ate the cake” entails “The
animal ate the cake”. To identify these relations,
no word sense disambiguation is performed; instead,
all senses from WordNet are considered. Negations
present in text or hypothesis influence the sign of
the score; for instance, if a negated noun is aligned
with a positive noun through a negative link like
antonymy, the two negations cancel each other and
the score of the relation will be positive. The score
of an alignment is the sum of the scores of all the
</bodyText>
<page confidence="0.994176">
121
</page>
<figureCaption confidence="0.999418823529412">
Figure 1: The dependency graphs and alignment candidates for Pair 2 (RTE3 SUM 633).
Category Feature name Feature description
alignment (score) totaligscore the total alignment score (sum of all scores)
totminaligscore the total alignment score when considering only the minimum scored relation
totmaxaligscore for any two chunks aligned
the total alignment score when considering only the maximum scored relation
for any two chunks aligned
alignment (count) allaligs the number of chunks aligned considering all alignments
posaligs the number of chunks aligned considering only positive alignments
negaligs the number of chunks aligned considering only negative alignments
minposaligs the number of alignments that have the minimum of their scores positive
maxposaligs the number of alignments that have the maximum of their scores positive
minnegaligs the number of alignments that have the minimum their scores negative
maxnegaligs the number of alignments that have the maximum of their scores negative
dependency edgelabels the pair of labels of non matching edges
match the number of relations that match when comparing the two edges
nonmatch the number of relations that don’t match when comparing the two edges
</figureCaption>
<tableCaption confidence="0.818271">
Table 1: Features for lexico-semantic matching.
</tableCaption>
<figure confidence="0.998511689655172">
policeman
Swedes
police sting operation
det
amod
num
num
det amod
A Belgian posing
partmod
a
Three
Three
Belgian
Swedes
were
prep−as
dealer
art
det
prep_in
an Brussels
nsubj
arrested
prep_in
arrested
dobj
auxpass
nsubjpass
</figure>
<bodyText confidence="0.9670735">
relations between the two words, and if the sum is
positive, the alignment is considered positive.
</bodyText>
<subsectionHeader confidence="0.999239">
3.2 Dependency graphs
</subsectionHeader>
<bodyText confidence="0.999988045454545">
The system then creates two dependency graphs, one
for the text and one for the hypothesis. The de-
pendency graphs are directed graphs with chunks as
nodes, interconnected by edges according to the re-
lations between them, which are represented as edge
labels. The tool used is the dependency parser de-
veloped by (de Marneffe et al., 2006), which as-
signs some of 48 grammatical relations to each pair
of words within a sentence. Further information
is added from the predicate-argument structures in
PropBank, e.g., a node can be the ARG0 of another
node, which is a predicate.
Because the text can have more than one sentence,
the dependency graphs for each of the sentences are
combined into a larger one. This is done by col-
lapsing together nodes (chunks) that are coreferent,
identical or in an nn relation (as given by the parser).
The relations between the original nodes and the rest
of the nodes in the text (dependency links) and nodes
in the hypothesis (alignment links) are all inherited
by the new node. Again, each edge can have multi-
ple relations as labels.
</bodyText>
<subsectionHeader confidence="0.993631">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.998948583333333">
With the alignment candidates and dependency
graphs obtained in the previous steps, the system
computes the values of the feature set. The features
used are of two kinds (Table 1):
(a) The alignment features are based on the scores
and counts of the candidate alignments. All the
scores are represented as real numbers between -1
and 1, normalized by the number of concepts in the
hypothesis.
(b) The dependency features consider each posi-
tively scored aligned pair with each of the other pos-
itively scored aligned pairs, and compare the set of
</bodyText>
<page confidence="0.978366">
122
</page>
<figure confidence="0.999451652173913">
prep_in nn
elected drew
auxpass
nsubj
prep_after prep_in prt
prep_as
nsubjpass
dobj
secretary death he/Natta 1969 up report
was
det partmod
poss
1984 party Berlinguer the proposing
dobj prep_from
expulsion party
succeeded
nsubj
dobj
prep_of
det det
the the group Manifesto
Berlinguer Natta det
the
</figure>
<figureCaption confidence="0.999978">
Figure 2: The dependency graphs and alignment candidates for Pair 3 (RTE3 IE 19).
</figureCaption>
<bodyText confidence="0.999917">
relations between the two nodes in the text with the
set of relations between the two nodes in the hypoth-
esis. This comparison is performed on the depen-
dency graphs, on the edges that immediately connect
the two text chunks and the two hypothesis chunks,
respectively. They have numerical values between 0
and 1, normalized by the square of the total number
of aligned chunks.
</bodyText>
<subsectionHeader confidence="0.981614">
3.4 Examples
</subsectionHeader>
<bodyText confidence="0.951135">
Pair 2 (RTE3 SUM 633)
</bodyText>
<subsectionHeader confidence="0.5686515">
T: A Belgian policeman posing as an art dealer in Brussels ar-
rested three Swedes.
</subsectionHeader>
<bodyText confidence="0.973551452380952">
H: Three Swedes were arrested in a Belgian police sting opera-
tion.
Figure 1 illustrates the dependency graphs and align-
ment candidates extracted for the instance in Pair 2.
There is no merging of graphs necessary here, be-
cause the text is made up of a single sentence. The
vertical line in the center divides the graph corre-
sponding to the text from the one corresponding to
the hypothesis. The dependency relations in the two
graphs are represented as labels of full lines, while
the alignment candidate pairs are joined by dotted
lines. As can be observed, the alignment was done
based on identity of spelling, e.g., Swedes-Swedes,
and stem, e.g., policeman-police. For the sake of
simplicity, the predicate-argument relations have not
been included in the drawing. This is a case of a pos-
itive instance, and the dependency and alignment re-
lations strongly support the entailment.
Pair 3 (RTE3 IE 19)
T: In 1969, he drew up the report proposing the expulsion from
the party of the Manifesto group. In 1984, after Berlinguer’s
death, Natta was elected as party secretary.
H: Berlinguer succeeded Natta.
Figure 2 contains an example of a negative in-
stance (Pair 3) that cannot be solved through the
simple analysis of alignment and dependency rela-
tions. The graphs corresponding to the two sen-
tences of the text have been merged into a single
graph because of the coreference between the pro-
noun he in the first sentence and the proper name
Natta in the second one. This merging has enriched
the overall information about relations, but the algo-
rithm does not take advantage of this. To correctly
solve this problem of entailment, one needs addi-
tional information delivered by a temporal relations
system. The chain of edges between Berlinguer and
Natta in the text graph expresses the fact that the
event of Natta’s election happened after Berlinguer’s
death. Since the hypothesis states that Berlinguer
succeeded Natta, the entailment is obviously false.
The system presented in this section will almost cer-
tainly solve this kind of instance incorrectly.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999260857142857">
The experimental results are summarized in Ta-
bles 3 and 4. The first table presents the accu-
racy scores obtained by running the two systems
through 10-fold crossvalidation on incremental RTE
datasets. The first system, based on extended lexical
overlap (ELO), almost consistently outperforms the
second system, lexico-semantic matching (LSM),
</bodyText>
<page confidence="0.994979">
123
</page>
<table confidence="0.999911142857143">
Evaluation set ELO LSM ELO+LSM
J48 J48 J48 JRip
RTE3Dev 66.38 63.63 65.50 67.50
+RTE2Dev 64.38 59.19 61.56 62.50
+RTE1Dev 62.11 56.67 60.36 59.62
+RTE2Test 61.04 57.77 61.51 61.20
+RTE1Test 60.07 56.57 59.04 60.42
</table>
<tableCaption confidence="0.8858655">
Table 3: Accuracy for the two systems on various
datasets.
</tableCaption>
<table confidence="0.987153">
Task IE IR QA SUM All
Accuracy 53.50 73.50 80.00 61.00 67.00
</table>
<tableCaption confidence="0.9905665">
Table 4: Accuracy by task for the Extended Lexical
Overlap system tested on the RTE3Test corpus.
</tableCaption>
<bodyText confidence="0.99689768">
and the combination of the two. The only case
when the combination gives the best score is on the
RTE3 development set, using the rule-based classi-
fier JRip. It can be observed from the table that the
more data is added to the evaluation set, the poorer
the results are. This can be explained by the fact that
each RTE dataset covers a specific kind of instances.
Because of this variety in the data, the results ob-
tained on the whole collection of RTE datasets avail-
able are more representative than the results reported
on each set, because they express the way the sys-
tems would perform in real-life natural language
processing as opposed to an academic setup.
Since the ELO system was clearly the better of
the two, it was the one submitted to the Third PAS-
CAL Challenge evaluation. Table 4 contains the
scores obtained by the system on the RTE3 testing
set. The overall accuracy is 67%, which represents
an increase from the score the system achieved at the
Second PASCAL Challenge (62.8%). The task with
the highest performance was Question Answering,
while the task that ranked the lowest was Informa-
tion Extraction. This is understandable, since IE in-
volves a very deep understanding of the text, which
the ELO system is not designed to do.
</bodyText>
<sectionHeader confidence="0.999787" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999957789473684">
This paper has presented two different approaches of
solving textual entailment: one based on extended
lexical overlap and the other on lexico-semantic
matching. The experiments have shown that the first
approach, while simpler in concept, yields a greater
performance when applied on the PASCAL RTE3
development set. At first glance, it seems puzzling
that a simple approach has outperformed one that
takes advantage of a deeper analysis of the text.
However, ELO system treats the text naively, as a
bag of words, and does not rely on any preprocess-
ing application. The LSM system, while attempting
an understanding of the text, uses three other sys-
tems that are not perfect: the coreference resolver,
the dependency parser and the semantic parser. The
performance of the LSM system is limited by the
performance of the tools it uses. It will be of interest
to evaluate this system again once they increase in
accuracy.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99967008">
Rod Adams. 2006. Textual entailment through extended
lexical overlap. In The Second PASCAL Recognising
Textual Entailment Challenge (RTE-2).
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In PASCAL RTE Challenge.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. The MIT Press.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
Web based probabilistic textual entailment. In PAS-
CAL RTE Challenge.
Graeme Hirst and David St-Onge. 1998. Lexical chains
as representations of context for the detection and cor-
rection of malapropisms. In Christiane Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305–332. The MIT Press.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, 2nd edition.
</reference>
<page confidence="0.998311">
124
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493167">
<title confidence="0.997869">Textual Entailment Through Extended Lexical Overlap Lexico-Semantic Matching</title>
<author confidence="0.992299">Rod Adams</author>
<author confidence="0.992299">Gabriel Nicolae</author>
<author confidence="0.992299">Cristina Nicolae</author>
<author confidence="0.992299">Sanda</author>
<affiliation confidence="0.990672">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.5806">Richardson,</address>
<email confidence="0.845678">gabriel,cristina,</email>
<abstract confidence="0.99813525">This paper presents two systems for textual entailment, both employing decision trees as a supervised learning algorithm. The first one is based primarily on the concept of lexical overlap, considering a bag of words similarity overlap measure to form a mapping of terms in the hypothesis to the source text. The second system is a lexicosemantic matching between the text and the hypothesis that attempts an alignment between chunks in the hypothesis and chunks in the text, and a representation of the text and hypothesis as two dependency graphs. Their performances are compared and their positive and negative aspects are analyzed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rod Adams</author>
</authors>
<title>Textual entailment through extended lexical overlap.</title>
<date>2006</date>
<booktitle>In The Second PASCAL Recognising Textual Entailment Challenge (RTE-2).</booktitle>
<contexts>
<context position="2574" citStr="Adams, 2006" startWordPosition="408" endWordPosition="409">xtraction (IE) and Summarization (SUM). In the challenge, there are two corpora, each consisting of 800 annotated pairs of texts and hypotheses. Pairs are annotated as to whether there exists a positive entailment between them and from which application domain each example came from. Instances are distributed evenly among the four tasks in both corpora, as are the positive and negative examples. One corpus was designated for development and training, while the other was reserved for testing. In the Second PASCAL RTE Challenge (BarHaim et al., 2006), one of the best performing submissions was (Adams, 2006), which focused on strict lexical methods so that the system could remain relatively simple and be easily applied to various entailment applications. However, this simple approach did not take into account details like the syntactic structure, the coreference or the semantic relations between words, all necessary for a deeper understanding of natural language text. Thus, a new system, based on the same decision tree learning algorithm, was designed in an attempt to gain performance by adding alignment and dependency relations information. The two systems will be compared and their advantages a</context>
</contexts>
<marker>Adams, 2006</marker>
<rawString>Rod Adams. 2006. Textual entailment through extended lexical overlap. In The Second PASCAL Recognising Textual Entailment Challenge (RTE-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="1667" citStr="Bar-Haim et al., 2006" startWordPosition="256" endWordPosition="259">thesis, and labeling whether or not the hypothesis (H) can be fully inferred from the text (T), as is illustrated in Pair 1. In Pair 1, the knowledge that an attorney representing someone’s interests entails that they work for that person. Pair 1 (RTE2 IE 58) T: “A force majeure is an act of God,” said attorney Phil Wittmann, who represents the New Orleans Saints and owner Tom Benson’s local interests. H: Phil Wittmann works for Tom Benson. The Third PASCAL Recognizing Textual Entailment Challenge&apos; follows the experience of the seclhttp://www.pascal-network.org/Challenges/RTE3/ ond challenge (Bar-Haim et al., 2006), whose main task was to automatically detect if a hypothesis H is entailed by a text T. To increase the “reality” of the task, the text-hypothesis examples were taken from outputs of actual systems that solved applications like Question Answering (QA), Information Retrieval (IR), Information Extraction (IE) and Summarization (SUM). In the challenge, there are two corpora, each consisting of 800 annotated pairs of texts and hypotheses. Pairs are annotated as to whether there exists a positive entailment between them and from which application domain each example came from. Instances are distri</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web based probabilistic textual entailment.</title>
<date>2005</date>
<booktitle>In PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="4275" citStr="Glickman et al., 2005" startWordPosition="687" endWordPosition="690">06) follows a four step framework. The first step is a tokenization process that applies to the content words of the text and hypothesis. The second step is building a “token map” of how the individual tokens in the hypothesis are tied to those in the text, as explained in Section 2.1. Thirdly, several features, as described in Section 2.2, are extracted from the token map. Finally, the extracted features are fed into Weka’s (Witten and Frank, 2005) J48 decision tree for training and evaluation. 2.1 The token map Central to this system is the concept of the token map. This map is inspired by (Glickman et al., 2005)’s use of the most probable lexical entailment for each hypothesis pair, but has been modified in how each pair is evaluated, and that the mapping is stored for additional extraction of features. The complete mapping is a list of (Hi, Tj) mappings, where Hi represents the ith token in the hypothesis, and Tj is similarly the jth token in the text. Each mapping has an associated similarity score. There is one mapping per token in the hypothesis. Text tokens are allowed to appear in multiple mappings. The mappings are created by considering each hypothesis token and comparing it to each token in </context>
<context position="6110" citStr="Glickman et al., 2005" startWordPosition="1006" endWordPosition="1009">given a score of 1.0, words that are morphologically related or that share a common sense are 0.9 and other paths give lower scores down to 0.0. This method of obtaining a path makes use of three groups of WordNet relations: Up (e.g. hypernym, member meronym), Down (e.g. hyponym, cause) and Horizontal (e.g. nominalization, derivation). The path can only follow certain combinations of these groupings, and assigns penalties for each link in the path, as well as for changing from one direction group to another. The secondary scoring routine is the lexical entailment probability, lep(u, v), from (Glickman et al., 2005). This probability is estimated by taking the page counts returned from the AltaVista2 search engine for a combined u and v search term, and dividing by the count for just the v term. This can be precisely expressed as: AVCount(Hi &amp;Tj) SimAV (Hi, Tj) = AVCount(Tj) The two scores are combined such that the secondary score can take up whatever slack the dominant score leaves available. The exact combination is: Sim(Hi,Tj) = SimWN(Hh,Tt) + α · (1 − SimWN(Hh, Tt)) · SimAV (Hh, Tt) where α is a tuned constant (α E [0,1]). Empirical analysis found the best results with very low values of α3. This pa</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan, and Moshe Koppel. 2005. Web based probabilistic textual entailment. In PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representations of context for the detection and correction of malapropisms.</title>
<date>1998</date>
<booktitle>WordNet: An electronic lexical database,</booktitle>
<pages>305--332</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5423" citStr="Hirst and St-Onge, 1998" startWordPosition="891" endWordPosition="894">ated by considering each hypothesis token and comparing it to each token in the text and keeping the one with the highest similarity score. Similarity scores A similarity score ranging from 0.0 to 1.0 is computed for any two tokens via a combination of two scores. This score can be thought of as the probability that the text token implies the hypothesis one, even though the methods used to produce it were not strictly probabilistic in nature. The first score is derived from the cost of a WordNet (Fellbaum, 1998) path. The WordNet paths between two tokens are built with the method reported in (Hirst and St-Onge, 1998), and designated as SimWN(Hi, Tj). Exact word matches are always given a score of 1.0, words that are morphologically related or that share a common sense are 0.9 and other paths give lower scores down to 0.0. This method of obtaining a path makes use of three groups of WordNet relations: Up (e.g. hypernym, member meronym), Down (e.g. hyponym, cause) and Horizontal (e.g. nominalization, derivation). The path can only follow certain combinations of these groupings, and assigns penalties for each link in the path, as well as for changing from one direction group to another. The secondary scoring</context>
</contexts>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Graeme Hirst and David St-Onge. 1998. Lexical chains as representations of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An electronic lexical database, pages 305–332. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="4106" citStr="Witten and Frank, 2005" startWordPosition="655" endWordPosition="658">e experimental results are presented in Section 4, and the paper concludes in Section 5. 2 Textual entailment through extended lexical overlap The first system (Adams, 2006) follows a four step framework. The first step is a tokenization process that applies to the content words of the text and hypothesis. The second step is building a “token map” of how the individual tokens in the hypothesis are tied to those in the text, as explained in Section 2.1. Thirdly, several features, as described in Section 2.2, are extracted from the token map. Finally, the extracted features are fed into Weka’s (Witten and Frank, 2005) J48 decision tree for training and evaluation. 2.1 The token map Central to this system is the concept of the token map. This map is inspired by (Glickman et al., 2005)’s use of the most probable lexical entailment for each hypothesis pair, but has been modified in how each pair is evaluated, and that the mapping is stored for additional extraction of features. The complete mapping is a list of (Hi, Tj) mappings, where Hi represents the ith token in the hypothesis, and Tj is similarly the jth token in the text. Each mapping has an associated similarity score. There is one mapping per token in</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>