<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001213">
<title confidence="0.983287">
UTTime: Temporal Relation Classification using Deep Syntactic Features
</title>
<author confidence="0.990666">
Natsuda Laokulrat
</author>
<affiliation confidence="0.994916">
The University of Tokyo
</affiliation>
<address confidence="0.720204">
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
</address>
<email confidence="0.991729">
natsuda@logos.t.u-tokyo.ac.jp
</email>
<author confidence="0.993731">
Yoshimasa Tsuruoka
</author>
<affiliation confidence="0.999436">
The University of Tokyo
</affiliation>
<address confidence="0.7687705">
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
</address>
<email confidence="0.998658">
tsuruoka@logos.t.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.993904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999533071428571">
In this paper, we present a system, UTTime,
which we submitted to TempEval-3 for Task
C: Annotating temporal relations. The sys-
tem uses logistic regression classifiers and ex-
ploits features extracted from a deep syntactic
parser, including paths between event words in
phrase structure trees and their path lengths,
and paths between event words in predicate-
argument structures and their subgraphs. UT-
Time achieved an F1 score of 34.9 based
on the graphed-based evaluation for Task C
(ranked 2 d) and 56.45 for Task C-relation-
only (ranked 1st) in the TempEval-3 evalua-
tion.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948222222222">
Temporal annotation is the task of identifying tem-
poral relationships between pairs of temporal enti-
ties, namely temporal expressions and events, within
a piece of text. The temporal relationships are im-
portant to support other NLP applications such as
textual entailment, document summarization, and
question answering. The temporal annotation task
consists of several subtasks, including temporal ex-
pression extraction, event extraction, and temporal
link identification and relation classification.
In TempEval-3, there are three subtasks of the
temporal annotation process offered, i.e., Task A:
Temporal expression extraction and normalization,
Task B: Event extraction, and Task C: Annotating
temporal relations. This paper presents a system
to handle Task C. Based on the annotated data pro-
vided, this subtask requires identifying pairs of tem-
poral entities and classifying the pairs into one of the
</bodyText>
<page confidence="0.984947">
88
</page>
<author confidence="0.330649">
Makoto Miwa
</author>
<affiliation confidence="0.707104">
The University of Manchester
</affiliation>
<address confidence="0.941574">
131 Princess Street,
Manchester, M1 7DN, UK
</address>
<email confidence="0.979715">
makoto.miwa@manchester.ac.uk
</email>
<author confidence="0.878067">
Takashi Chikayama
</author>
<affiliation confidence="0.666177666666667">
The University of Tokyo
3-7-1 Hongo, Bunkyo-ku,
Tokyo, Japan
</affiliation>
<email confidence="0.951032">
chikayama@logos.t.u-tokyo.ac.jp
</email>
<bodyText confidence="0.989359733333333">
14 relation types according to TimeML (Pustejovsky
et al., 2005), i.e., BEFORE, AFTER, IMMEDIATELY BE-
FORE, IMMEDIATELY AFTER, INCLUDES, IS INCLUDED,
DURING, DURING INVERSE, SIMULTANEOUS, IDENTITY,
BEGINS, BEGUN BY, END, and ENDED BY.
The motivation behind our work is to utilize syn-
tactic and semantic relationships between a pair of
temporal entities in the temporal relation classifica-
tion task, since we believe that these relationships
convey the temporal relation. In addition to general
features, which are easily extracted from sentences
(e.g., part of speech tags, lemmas, synnonyms), we
use features extracted using a deep syntactic parser.
The features from the deep parser can be divided into
two groups: features from phrase structure trees and
features from predicate-argument structures. These
features are only applicable in the case that the tem-
poral entities appear in the same sentence, so we use
only the general features for inter-sentence relations.
Predicate-argument structure expresses semantic
relations between words. This information can be
extracted from a deep syntactic parser. Features
from predicate-argument structures can capture im-
portant temporal information (e.g., prepositions of
time) from sentences effectively.
The remaining part of this paper is organized as
follows. We explain our approach in detail in Sec-
tion 2 and then show the evaluation and results in
Section 3. Finally, we conclude with directions for
future work in Section 4.
</bodyText>
<sectionHeader confidence="0.98795" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.99532625">
Our system, UTTime, is based on a supervised ma-
chine learning approach. UTTime performs two
tasks; TLINK identification and classification. In
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 88–92, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
other words, UTTime identifies pairs of temporal en-
tities and classifies these pairs into temporal relation
types.
</bodyText>
<subsectionHeader confidence="0.927733">
2.1 TLINK identification
</subsectionHeader>
<bodyText confidence="0.999460125">
A pair of temporal entities that have a temporal rela-
tion is called a TLINK. The system first determines
which pairs of temporal entities are linked by using
a ruled-based approach as a baseline approach.
All the TempEval-3’s possible pairs of temporal
entities are extracted by a set of simple rules; pairs
of temporal entities that satisfy one of the following
rules are considered as TLINKs.
</bodyText>
<listItem confidence="0.9999574">
• Event and document creation time
• Events in the same sentence
• Event and temporal expression in the same sen-
tence
• Events in consecutive sentences
</listItem>
<subsectionHeader confidence="0.769359">
2.2 TLINK classification
</subsectionHeader>
<bodyText confidence="0.997820222222222">
Each TLINK is classified into a temporal relation
type. We use a machine learning approach for the
temporal relation classification. Two L2-regularized
logistic regression classifiers, LIBLINEAR (Fan et
al., 2008), are used; one for event-event TLINKs,
and another one for event-time TLINKs. In addition
to general features at different linguistic levels, fea-
tures extracted by a deep syntactic parser are used.
The general features we employed are:
</bodyText>
<listItem confidence="0.995641">
• Event and timex attributes
</listItem>
<bodyText confidence="0.998368">
All attributes associated with events (class,
tense, aspect, modality, and polarity) and
temporal expressions (type, value, func-
tionInDocument, and temporalFunction) are
used. For event-event TLINKs, we also use
tense/class/aspect match, tense/class/aspect bi-
grams as features (Chambers et al., 2007).
</bodyText>
<listItem confidence="0.946345666666667">
• Morphosyntactic information
Words, part of speech tags, lemmas within a
window before/after event words are extracted
using Stanford coreNLP (Stanford NLP Group,
2012).
• Lexical semantic information
</listItem>
<figureCaption confidence="0.998442">
Figure 1: Phrase structure tree
</figureCaption>
<bodyText confidence="0.998392">
Synonyms of event word tokens from WordNet
lexical database (Fellbaum, 1998) are used as
features.
</bodyText>
<listItem confidence="0.955182">
• Event-Event information
</listItem>
<bodyText confidence="0.999506153846154">
For event-event TLINKs, we use
same sentence feature to differentiate pairs
of events in the same sentence from pairs of
events from different sentences (Chambers et
al., 2007).
In the case that temporal entities of a particu-
lar TLINK are in the same sentence, we extract
two new types of sentence-level semantic informa-
tion from a deep syntactic parser. We use the Enju
parser (Miyao and Tsujii, 2008). It analyzes syn-
tactic/semantic structures of sentences and provides
phrase structures and predicate-argument structures.
The features we extract from the deep parser are
</bodyText>
<listItem confidence="0.50361">
• Paths between event words in the phrase struc-
ture tree, and up(T)/down(j) lengths of paths.
</listItem>
<bodyText confidence="0.96482275">
We use 3-grams of paths as features instead of
full paths since these are too sparse. An ex-
ample is shown in Figure 1. In this case, the
path between the event words, estimates and
worth, is VBZT, VXT, VPT, VPT, VP, PPI, PXI, INI.
The 3-grams of the path are, therefore, {VBZT-
VXT-VPT, VXT-VPT-VPT, VPT-VPT-VP, VPT-VP-PPI,
VP-PPI-PXI, PPI-PX-I-INIj. The up/down path
</bodyText>
<page confidence="0.999004">
89
</page>
<figureCaption confidence="0.961732">
Figure 2: Predicate argument structure
</figureCaption>
<table confidence="0.502048">
lengths are 4 (VBZ↑, VX↑, VP↑, VP↑) and 3 (PP↓,
PX↓, IN↓) respectively.
</table>
<listItem confidence="0.979173">
• Paths between event words in predicate-
argument structure, and their subgraphs.
</listItem>
<bodyText confidence="0.904737">
For the previous example, we can express the
relations in predicate-argument structure repre-
sentation as
</bodyText>
<listItem confidence="0.9588915">
– verb arg12: estimate (she, properties)
– prep arg12: worth (estimate, dollars)
</listItem>
<bodyText confidence="0.994492">
In this case, the path between the event words,
estimates and worth, is ←prep arg12:arg1. That
is, the type of the predicate worth is prep arg12
and it has estimate as the first argument (arg1).
The path from estimate to worth is in reverse
direction (+–).
The next example sentence, John saw mary be-
fore the meeting, gives an idea of a more com-
plex predicate-argument structure as shown
in Figure 2. The path between the event
words, saw and meeting is ←prep arg12:arg1,
prep arg12:arg2.
We use (v, e, v) and (e, v, e) tuples of the
edges and vertices on the path as features.
For example, in Figure 2, the (v,e,v) tuples
are (see, ←prep arg12:arg1, before) and (be-
fore, prep arg12:arg2, meeting). In the same
way, the (e,v,e) tuple is (←prep arg12:arg1,
before, prep arg12:arg2). The subgraphs
of (v, e, v) and (e, v, e) tuples are also
used, including (see, ←prep arg12:arg1,
</bodyText>
<construct confidence="0.9983104">
*), (*, ←prep arg12:arg1, before), (*,
←prep arg12:arg1, *), (*, prep arg12:arg2,
meeting), (before, prep arg12:arg2, *), (*,
prep arg12:arg2, *), (*, before, prep arg12:arg2),
(←prep arg12:arg1, before, *), (*, before, *).
</construct>
<bodyText confidence="0.9943704">
From the above example, the features from pred-
icate argument structure can properly capture the
preposition before. It can also capture a preposi-
tion from a compound sentence such as John met
Mary before he went back home. The path between
the event words met and went are (←conj arg12:arg1,
conj arg12:arg2) and the (v, e, v) and (e, v, e)
tuples are (met, ←conj arg12:arg1, before), (before,
conj arg12:arg2, went), and (←prep arg12:arg1, be-
fore, prep arg12:arg2).
</bodyText>
<subsectionHeader confidence="0.99975">
2.3 Hybrid approach
</subsectionHeader>
<bodyText confidence="0.999941090909091">
The rule-based approach described in Section 2.1
produces many unreasonable and excessive links.
We thus use a machine learning approach to filter
out those unreasonable links by training the model
in Section 2.2 with an additional relation type, UN-
KNOWN, for links that satisfy the rules in Section
2.1 but do not appear in the training data.
In this way, for Task C, we first extract all the links
that satisfy the rules and classify the relation types of
those links. After classifying temporal relations, we
remove the links that are classified as UNKNOWN.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999993714285714">
The scores are calculated by the graph-based eval-
uation metric proposed by UzZaman and Allen
(2011). We trained the models with TimeBank and
AQUAINT corpora. We also trained our models on
the training set with inverse relations. The perfor-
mance analysis is based on 10-fold cross validation
on the development data.
</bodyText>
<subsectionHeader confidence="0.999555">
3.1 Task C
</subsectionHeader>
<bodyText confidence="0.999952571428571">
In Task C, a system has to identify appropriate tem-
poral links and to classify each link into one tempo-
ral relation type. For Task C evaluation, we compare
the results of the models trained with and without the
features from the deep parser. The results are shown
in Table 1. The rule-based approach gives a very low
precision.
</bodyText>
<subsectionHeader confidence="0.999089">
3.2 Task C-relation-only
</subsectionHeader>
<bodyText confidence="0.991590285714286">
Task C-relation-only provides a system with all the
appropriate temporal links and only needs the sys-
tem to classify the relation types. Since our goal is to
exploit the features from the deep parser, in Task C-
relation-only, we measured the contribution of those
features to temporal relation classification in Table
2.
</bodyText>
<page confidence="0.990264">
90
</page>
<table confidence="0.979626">
Features F1 P R
gen. (rule) 22.51 14.32 52.58
gen. + ph. + pas. (rule) 22.61 14.30 54.01
gen. + ph. + pas. (hyb.) 33.52 36.23 31.19
gen. + ph. + pas. (hyb. + inv.) 39.53 37.56 41.70
</table>
<tableCaption confidence="0.99757575">
Table 1: Result of Task C. (rule: rule-based approach,
hyb.: hybrid approach, gen.: general features, ph.:phrase
structure tree features, pas.:predicate-argument structure
features, and inv.: Inverse relations are used for training.)
</tableCaption>
<table confidence="0.999502">
Features F1 P R
gen. 64.42 64.59 64.25
gen. + ph. 65.24 65.42 65.06
gen. + pas. 66.40 66.55 66.25
gen. + ph. + pas. 66.39 66.55 66.23
gen. + ph. + pas. (inv.) 65.30 65.39 65.20
</table>
<tableCaption confidence="0.99954">
Table 2: Result of Task C-relation-only. (gen.:
</tableCaption>
<bodyText confidence="0.902728583333333">
general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
The predicate-argument-structure features con-
tributed to the improvement more than those of
phrase structures in both precision and recall. The
reason is probably that the features from phrase
structures that we used did not imply a temporal re-
lation of events in the sentence. For instance, the
sentence “John saw Mary before the meeting” gives ex-
actly the same path as of the sentence “John saw Mary
after the meeting”.
</bodyText>
<subsectionHeader confidence="0.958628">
3.3 Results on test data
</subsectionHeader>
<bodyText confidence="0.9999545">
Tables 3 and 4 show the results on the test data,
which were manually annotated and provided by the
TempEval-3 organizer. We also show the scores of
the other systems in the tables. For the evaluation
on the test data, we used the models trained with
general features, phrase structure tree features, and
predicate-argument structure features.
UTTime-5 ranked 2nd best in Task C. Interest-
ingly, training the models with inverse relations im-
proved the system only when using the hybrid ap-
proach. This means that the inverse relations did not
improve the temporal classification but helped the
system filter out unreasonable links (UNKNOWN)
in the hybrid approach. As expected, the ruled-based
approach got a very high recall score at the expense
of precision. UTTime-1, although it achieved the F1
</bodyText>
<table confidence="0.999865">
Approach F1 P R
rule (UTTime-1) 24.65 15.18 65.64
rule + inv (UTTime-3) 24.28 15.1 61.99
hyb. (UTTime-4) 28.81 37.41 23.43
hyb. + inv. (UTTime-5) 34.9 35.94 33.92
cleartk 36.26 37.32 35.25
NavyTime 31.06 35.48 27.62
JU-CSE 26.41 21.04 35.47
KUL-KULTaskC 24.83 23.35 26.52
</table>
<tableCaption confidence="0.897256">
Table 3: Result of Tack C on test data. (rule: rule-based
approach, hyb.: hybrid approach, and inv.: Inverse rela-
tions are used for training.)
</tableCaption>
<table confidence="0.998666666666667">
Approach F1 P R
gen. + ph. + pas. (UTTime-1) 56.45 55.58 57.35
gen. + ph. + pas. (UTTime-2) 54.26 53.2 55.36
gen. + ph. + pas. (inv.) (UTTime-3) 54.7 53.85 55.58
NavyTime 46.83 46.59 47.07
JU-CSE 34.77 35.07 34.48
</table>
<tableCaption confidence="0.9556295">
Table 4: Result of Task C-relation-only on test data.
(gen.: general features, ph.:phrase structure tree features,
pas.:predicate-argument structure features, and inv.: In-
verse relations are used for training.)
</tableCaption>
<bodyText confidence="0.999492">
score of only 24.65, got the highest recall among all
the systems.
For Task C-relation-only, we achieved the highest
F1 score, precision, and recall. UTTime-2 basically
had the same models as that of UTTime-1, but we
put different weights for each relation type. The re-
sults show that using the weights did not improve
the score in graph-based evaluation.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999980214285714">
The system, UTTime, identifying temporal links and
classifying temporal relation, is proposed. The links
were identified based on the rule-based approach
and then some links were filtered out by a classi-
fier. The filtering helped improve the system consid-
erably. For the relation classification task, the fea-
tures extracted from phrase structures and predicate-
argument structures were proposed, and the features
improved the classification in precision, recall, and
F-score.
In future work, we hope to improve the classifica-
tion performance by constructing timegraphs (Miller
and Schubert, 1999), so that the system can use in-
formation from neighbor TLINKs as features.
</bodyText>
<page confidence="0.998384">
91
</page>
<sectionHeader confidence="0.995862" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835608695652">
James Pustejovsky, Robert Ingria, Roser Sauri, Jos´e
Casta˜no, Jessica Littman, Rob Gaizauskas, Andrea
Setzer, Graham Katz, Inderjeet Mani 2005. The spec-
ification language TimeML. The Language of Time: A
reader, pages 545–557
Stanford Natural Language Processing Group. 2012.
Stanford CoreNLP.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification.
Nathanael Chambers, Shan Wang and Dan Jurafsky.
2007. Classifying Temporal Relations between
Events. In ACL 2007, pages 173–176.
Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest
Models for Probabilistic HPSG Parsing. In Computa-
tional Linguistics. 34(1). pages 35–80, MIT Press.
Naushad UzZaman and James F. Allen. 2011. Temporal
Evaluation. In ACL 2011, pages 351–356.
Stephanie A. Miller and Lenhart K. Schubert. 1999.
Time Revisited. In Computational Intelligence 6,
pages 108–118.
</reference>
<page confidence="0.99599">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.274589">
<title confidence="0.999448">UTTime: Temporal Relation Classification using Deep Syntactic Features</title>
<author confidence="0.923731">Natsuda</author>
<affiliation confidence="0.996985">The University of</affiliation>
<address confidence="0.921626">3-7-1 Hongo, Tokyo, Japan</address>
<email confidence="0.968232">natsuda@logos.t.u-tokyo.ac.jp</email>
<author confidence="0.65299">Yoshimasa</author>
<affiliation confidence="0.996408">The University of</affiliation>
<address confidence="0.9218675">3-7-1 Hongo, Tokyo, Japan</address>
<email confidence="0.986179">tsuruoka@logos.t.u-tokyo.ac.jp</email>
<abstract confidence="0.962028266666667">In this paper, we present a system, UTTime, which we submitted to TempEval-3 for Task C: Annotating temporal relations. The system uses logistic regression classifiers and exploits features extracted from a deep syntactic parser, including paths between event words in phrase structure trees and their path lengths, and paths between event words in predicateargument structures and their subgraphs. UT- Time achieved an F1 score of 34.9 based on the graphed-based evaluation for Task C and 56.45 for Task C-relation- (ranked in the TempEval-3 evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Robert Ingria</author>
<author>Roser Sauri</author>
<author>Jos´e Casta˜no</author>
<author>Jessica Littman</author>
<author>Rob Gaizauskas</author>
</authors>
<title>The specification language TimeML. The Language of Time: A reader,</title>
<date>2005</date>
<pages>545--557</pages>
<location>Andrea Setzer, Graham Katz, Inderjeet Mani</location>
<marker>Pustejovsky, Ingria, Sauri, Casta˜no, Littman, Gaizauskas, 2005</marker>
<rawString>James Pustejovsky, Robert Ingria, Roser Sauri, Jos´e Casta˜no, Jessica Littman, Rob Gaizauskas, Andrea Setzer, Graham Katz, Inderjeet Mani 2005. The specification language TimeML. The Language of Time: A reader, pages 545–557</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanford</author>
</authors>
<title>Natural Language Processing Group.</title>
<date>2012</date>
<institution>Stanford CoreNLP.</institution>
<marker>Stanford, 2012</marker>
<rawString>Stanford Natural Language Processing Group. 2012. Stanford CoreNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="5709" citStr="Fellbaum, 1998" startWordPosition="830" endWordPosition="831">es All attributes associated with events (class, tense, aspect, modality, and polarity) and temporal expressions (type, value, functionInDocument, and temporalFunction) are used. For event-event TLINKs, we also use tense/class/aspect match, tense/class/aspect bigrams as features (Chambers et al., 2007). • Morphosyntactic information Words, part of speech tags, lemmas within a window before/after event words are extracted using Stanford coreNLP (Stanford NLP Group, 2012). • Lexical semantic information Figure 1: Phrase structure tree Synonyms of event word tokens from WordNet lexical database (Fellbaum, 1998) are used as features. • Event-Event information For event-event TLINKs, we use same sentence feature to differentiate pairs of events in the same sentence from pairs of events from different sentences (Chambers et al., 2007). In the case that temporal entities of a particular TLINK are in the same sentence, we extract two new types of sentence-level semantic information from a deep syntactic parser. We use the Enju parser (Miyao and Tsujii, 2008). It analyzes syntactic/semantic structures of sentences and provides phrase structures and predicate-argument structures. The features we extract fr</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<contexts>
<context position="4831" citStr="Fan et al., 2008" startWordPosition="703" endWordPosition="706">sed approach as a baseline approach. All the TempEval-3’s possible pairs of temporal entities are extracted by a set of simple rules; pairs of temporal entities that satisfy one of the following rules are considered as TLINKs. • Event and document creation time • Events in the same sentence • Event and temporal expression in the same sentence • Events in consecutive sentences 2.2 TLINK classification Each TLINK is classified into a temporal relation type. We use a machine learning approach for the temporal relation classification. Two L2-regularized logistic regression classifiers, LIBLINEAR (Fan et al., 2008), are used; one for event-event TLINKs, and another one for event-time TLINKs. In addition to general features at different linguistic levels, features extracted by a deep syntactic parser are used. The general features we employed are: • Event and timex attributes All attributes associated with events (class, tense, aspect, modality, and polarity) and temporal expressions (type, value, functionInDocument, and temporalFunction) are used. For event-event TLINKs, we also use tense/class/aspect match, tense/class/aspect bigrams as features (Chambers et al., 2007). • Morphosyntactic information Wo</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Shan Wang</author>
<author>Dan Jurafsky</author>
</authors>
<title>Classifying Temporal Relations between Events. In ACL</title>
<date>2007</date>
<pages>173--176</pages>
<contexts>
<context position="5397" citStr="Chambers et al., 2007" startWordPosition="784" endWordPosition="787">tic regression classifiers, LIBLINEAR (Fan et al., 2008), are used; one for event-event TLINKs, and another one for event-time TLINKs. In addition to general features at different linguistic levels, features extracted by a deep syntactic parser are used. The general features we employed are: • Event and timex attributes All attributes associated with events (class, tense, aspect, modality, and polarity) and temporal expressions (type, value, functionInDocument, and temporalFunction) are used. For event-event TLINKs, we also use tense/class/aspect match, tense/class/aspect bigrams as features (Chambers et al., 2007). • Morphosyntactic information Words, part of speech tags, lemmas within a window before/after event words are extracted using Stanford coreNLP (Stanford NLP Group, 2012). • Lexical semantic information Figure 1: Phrase structure tree Synonyms of event word tokens from WordNet lexical database (Fellbaum, 1998) are used as features. • Event-Event information For event-event TLINKs, we use same sentence feature to differentiate pairs of events in the same sentence from pairs of events from different sentences (Chambers et al., 2007). In the case that temporal entities of a particular TLINK are </context>
</contexts>
<marker>Chambers, Wang, Jurafsky, 2007</marker>
<rawString>Nathanael Chambers, Shan Wang and Dan Jurafsky. 2007. Classifying Temporal Relations between Events. In ACL 2007, pages 173–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Feature Forest Models for Probabilistic HPSG Parsing.</title>
<date>2008</date>
<journal>In Computational Linguistics.</journal>
<volume>34</volume>
<issue>1</issue>
<pages>35--80</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6160" citStr="Miyao and Tsujii, 2008" startWordPosition="903" endWordPosition="906">eNLP (Stanford NLP Group, 2012). • Lexical semantic information Figure 1: Phrase structure tree Synonyms of event word tokens from WordNet lexical database (Fellbaum, 1998) are used as features. • Event-Event information For event-event TLINKs, we use same sentence feature to differentiate pairs of events in the same sentence from pairs of events from different sentences (Chambers et al., 2007). In the case that temporal entities of a particular TLINK are in the same sentence, we extract two new types of sentence-level semantic information from a deep syntactic parser. We use the Enju parser (Miyao and Tsujii, 2008). It analyzes syntactic/semantic structures of sentences and provides phrase structures and predicate-argument structures. The features we extract from the deep parser are • Paths between event words in the phrase structure tree, and up(T)/down(j) lengths of paths. We use 3-grams of paths as features instead of full paths since these are too sparse. An example is shown in Figure 1. In this case, the path between the event words, estimates and worth, is VBZT, VXT, VPT, VPT, VP, PPI, PXI, INI. The 3-grams of the path are, therefore, {VBZTVXT-VPT, VXT-VPT-VPT, VPT-VPT-VP, VPT-VP-PPI, VP-PPI-PXI, </context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Yusuke Miyao and Jun’ichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. In Computational Linguistics. 34(1). pages 35–80, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James F Allen</author>
</authors>
<title>Temporal Evaluation.</title>
<date>2011</date>
<booktitle>In ACL 2011,</booktitle>
<pages>351--356</pages>
<contexts>
<context position="9436" citStr="UzZaman and Allen (2011)" startWordPosition="1443" endWordPosition="1446">produces many unreasonable and excessive links. We thus use a machine learning approach to filter out those unreasonable links by training the model in Section 2.2 with an additional relation type, UNKNOWN, for links that satisfy the rules in Section 2.1 but do not appear in the training data. In this way, for Task C, we first extract all the links that satisfy the rules and classify the relation types of those links. After classifying temporal relations, we remove the links that are classified as UNKNOWN. 3 Evaluation The scores are calculated by the graph-based evaluation metric proposed by UzZaman and Allen (2011). We trained the models with TimeBank and AQUAINT corpora. We also trained our models on the training set with inverse relations. The performance analysis is based on 10-fold cross validation on the development data. 3.1 Task C In Task C, a system has to identify appropriate temporal links and to classify each link into one temporal relation type. For Task C evaluation, we compare the results of the models trained with and without the features from the deep parser. The results are shown in Table 1. The rule-based approach gives a very low precision. 3.2 Task C-relation-only Task C-relation-onl</context>
</contexts>
<marker>UzZaman, Allen, 2011</marker>
<rawString>Naushad UzZaman and James F. Allen. 2011. Temporal Evaluation. In ACL 2011, pages 351–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie A Miller</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Time Revisited.</title>
<date>1999</date>
<journal>In Computational Intelligence</journal>
<volume>6</volume>
<pages>108--118</pages>
<marker>Miller, Schubert, 1999</marker>
<rawString>Stephanie A. Miller and Lenhart K. Schubert. 1999. Time Revisited. In Computational Intelligence 6, pages 108–118.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>