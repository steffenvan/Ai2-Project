<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003618">
<note confidence="0.537790333333333">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.960089">
The University of Maryland SENSEVAL-3 System Descriptions
</title>
<author confidence="0.997905">
Clara Cabezas, Indrajit Bhattacharya, and Philip Resnik
</author>
<affiliation confidence="0.999702">
University of Maryland, College Park, MD 20742 USA
</affiliation>
<email confidence="0.997669">
clarac@umiacs.umd.edu, indrajit@cs.umd.edu, resnik@umd.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929818181818">
For SENSEVAL-3, the University of Maryland
(UMD) team focused on two primary issues: the
portability of sense disambigation across languages,
and the exploitation of real-world bilingual text as a
resource for unsupervised sense tagging. We vali-
dated the portability of our supervised disambigua-
tion approach by applying it in seven tasks (En-
glish, Basque, Catalan, Chinese, Romanian, Span-
ish, and “multilingual” lexical samples), and we ex-
perimented with a new unsupervised algorithm for
sense modeling using parallel corpora.
</bodyText>
<sectionHeader confidence="0.9592215" genericHeader="keywords">
1 Supervised Sense Tagging for Lexical
Samples
</sectionHeader>
<subsectionHeader confidence="0.998304">
1.1 Tagging Framework
</subsectionHeader>
<bodyText confidence="0.986113416666667">
For the English, Basque, Catalan, Chinese, Roma-
nian, Spanish, and “multilingual” lexical samples,
we employed the UMD-SST system developed for
SENSEVAL-2 (Cabezas et al., 2001); we refer the
reader to that paper for a detailed system descrip-
tion. Briefly, UMD-SST takes a supervised learning
approach, treating each word in a task’s vocabulary
as an independent problem of classification into that
word’s sense inventory. Each training and test item
is represented as a weighted feature vector, with di-
mensions corresponding to properties of the con-
text. As in SENSEVAL-2, our system supported the
following kinds of features:
Local context. For each = 1, 2, and 3, and for
each word in the vocabulary, there is a fea-
ture representing the presence of word
at a distance of words to the left of the word
being disambigated; there is a corresponding
set of features for the local context to
the right of the word.
Wide context. Each word in the training set
vocabulary has a corresponding feature indi-
cating its presence. For SENSEVAL-3, wide
context features were taken from the entire
training or test instance. In other settings, one
might make further distinctions, e.g. between
words in the same paragraph and words in the
document.
We also experimented with the following additional
kinds of features for English:
Grammatical context. We use a syntactic de-
pendency parser (Lin, 1998) to produce, for
each word to be disambiguated, features iden-
tifying relevant syntactic relationships in the
sentence where it occurs. For example, in the
sentence The U.S. government announced a
new visa waiver policy, the word government
would have syntactic features like DET:THE,
MOD:U.S., and SUBJ-OF:ANNOUNCED.
Expanded context. In information retrieval,
we and other researchers have found that it
can be useful to expand the representation of a
document to include informative words from
similar documents (Levow et al., 2001). In
a similar spirit, we create a set of expanded-
context features by (a) treating the
WSD context as a bag of words, (b) issuing it
as a query to a standard information retrieval
system that has indexed a large collection
of documents, and (c) including the non-
stopword vocabulary of the top documents
returned. So, for example, in a context
containing the sentence The U.S. government
announced a new visa waiver policy, the query
might retrieve news articles like “US to Ex-
tend Fingerprinting to Europeans, Japanese”
(Bloomberg.com, April 2, 2004), leading to
the addition of features like EXT:EUROPEAN,
EXT:JAPANESE, EXT:FINGERPRINTING
EXT:VISITORS, EXT:TOURISM, and so forth.
</bodyText>
<table confidence="0.99855525">
Lexical Sample Coarse (prec/rec) Fine (prec/rec)
UMD-SST 0.643/0.643 0.568/0.568
UMD-SST-gram 0.600/0.600 0.576/0.576
UMD-SST-docexp 0.541/0.542 0.516/0.491
</table>
<tableCaption confidence="0.9735325">
Table 1: UMD-SST variations on the SENSEVAL-2
English lexical sample task
</tableCaption>
<bodyText confidence="0.99890776">
As described by Cabezas et al. (2001), we have
adopted the framework of support vector machines
(SVMs) in order to perform supervised classifica-
tion. Because we used a version of SVM learn-
ing designed for binary classification tasks, rather
than the multi-way classification needed for disam-
biguating among senses, we constructed a family
of SVM classifiers for each word — one for each
of the word’s senses. All positive training ex-
amples for a sense of were treated as negative
training examples for all the other senses , .
Table 1 shows the performance of our approach
on the English lexical sample task from the previ-
ous SENSEVAL exercise (SENSEVAL-2), includ-
ing the basic system (UMD-SST), the basic sys-
tem with grammatical features added (UMD-SST-
gram), and the basic system with document expan-
sion features added (UMD-SST-docexp). (We have
not done a run with both sets of features added.) The
results showed a possible potential benefit for us-
ing grammatical features, in the fine-grained scor-
ing. However, we deemed the benefit too small to
rely upon, and submitted our official SENSEVAL-3
runs using UMD-SST without the grammatical or
document-expansion features.
</bodyText>
<table confidence="0.99977575">
Lexical Sample Precision (%) Recall (%)
Basque 65.6 58.7
Catalan 81.5 80.3
Chinese 51.3 51.2
English 66.0 66.0
Romanian 70.7 70.7
Spanish 82.5 82.5
Multilingual 58.8 58.8
</table>
<tableCaption confidence="0.704532">
Table 2: UMD-SST results (fine-grained) on
SENSEVAL-3 lexical sample tasks
</tableCaption>
<table confidence="0.99839275">
Lexical Sample Coarse (prec/rec) Fine (prec/rec)
UMD-SST 0.709/0.709 0.660/0.660
UMD-SST-gram 0.703/0.703 0.655/0.655
UMD-SST-docexp 0.691/0.680 0.637/0.627
</table>
<tableCaption confidence="0.809704">
Table 3: UMD-SST variations on the SENSEVAL-3
English lexical sample task
</tableCaption>
<bodyText confidence="0.999307769230769">
SENSEVAL-3 performance on the lexical sample
runs in which we participated, using fine-grained
scores.
In unofficial runs, we also experimented with
the grammatical and document-expansion features.
Table 3 shows the results, which indicate that on this
task the additional features did not help and may
have hurt performance slightly. Although we have
not yet reached any firm conclusions, we conjecture
that value potentially added by these features may
have been offset by the expansion in the size of the
feature space; in future work we plan to explore fea-
ture selection and alternative learning frameworks.
</bodyText>
<subsectionHeader confidence="0.995679">
1.2 SENSEVAL-3 Lexical Sample Tasks
</subsectionHeader>
<bodyText confidence="0.996597">
For SENSEVAL-3, the modularity of our system
made it very easy to participate in the many lex-
ical sample tasks, including the multilingual lexi-
cal sample, where the “sense inventory” consisted
of vocabulary items from a second language.&apos; In-
deed, we participated in several tasks without hav-
ing anyone on the team who could read the lan-
guage. (Whether or not this was a good idea remains
to be seen.) For Basque, Catalan, and Spanish, we
used the lemmatized word forms provided by the
task organizers; for the other languages, including
English, we used only simple tokenization.
Table 2 shows the UMD-SST system’s official
&apos;Data format problems prevented us from participating in
the Italian lexical sample task.
</bodyText>
<sectionHeader confidence="0.9652815" genericHeader="method">
2 Unsupervised Sense Tagging using
Bilingual Text
</sectionHeader>
<subsectionHeader confidence="0.98995">
2.1 Probabilistic Sense Model
</subsectionHeader>
<bodyText confidence="0.9945594625">
For the past several years, the University of Mary-
land group has been exploring unsupervised ap-
proaches to word sense disambiguation that take ad-
vantage of parallel corpora (Diab and Resnik, 2002;
Diab, 2003). Recently, Bhattacharya et al. (2004)
(in a UMD/Montreal collaboration) have developed
a variation on this bilingual approach that is in-
spired by the central insight of Diab’s work, but re-
casts it in a probabilistic framework. A generative
model, it is a variant of the graphical model of Ben-
gio and Kermorvant (2003), which groups seman-
tically related words from the two languages into
“senses”; translations are generated by probabilis-
tically choosing a sense and then words from the
sense.
Briefly, the model of Bhattacharya et al. uses
probabilistic analysis and independence assump-
tions: it assumes that senses and words have cer-
tain occurrence probabilities and that the choice of
the word can be made independently once the sense
has been decided. Here interaction between dif-
ferent words arising from the same sense comes
into play, even if the words are not related through
translations, and this interdependence of the senses
through common words plays a role in sense disam-
biguation.
The model takes as its starting point the idea of a
“translation pair” — a pair of words and that are
aligned in two sentences (here “English” and “non-
English”) that are translations of each other. For
example, in the English-Spanish sentence pair Me
gusta la ciudad/I like the city, one would find the
translation pairs , ,,
and .2 Those familiar with statistical
machine translation (MT) models will note that a
translation pair is equivalent to a link in a word-level
alignment, and in fact we obtain translation pairs
from sentence-aligned parallel text by training a sta-
tistical MT model (using GIZA++, (Och and Ney,
2003)) and using the word-level alignments that re-
sult.
The probabilistic sense model makes the assump-
tion that the English word and the non-English
word in a translation pair share the same precise
sense, or, in other words, that the set of sense labels
for the words in the two languages is the same and
may be collapsed into one set of senses that is re-
sponsible for both English and non-English words.
Thus the one latent variable in the model is the
sense label generating both words, represented by
variables and . The model also makes the
assumption that words in both languages are con-
ditionally independent given the sense label. The
generative parameters for the model are the prior
probability of each sense and the conditional
probabilities and of each word
and in the two languages given the sense. The
generation of a translation pair by this model may
be viewed as a two-step process that first selects
a sense according to the priors on the senses, and
then selects a word from each language using the
2Speakers of both Spanish and English will observe that
translation pairs may well include words that are not exactly
translations of each other.
conditional probabilities for that sense. This may
be imagined as a factoring of the joint distribution:
.
Given WordNet as a sense inventory, the set
of English senses is as defined in the WordNet
database. Since the model assumes the sense labels
for the two languages are the same, it must use the
same WordNet labels for the non-English words as
well. Rather than considering all possible words in
the non-English vocabulary for each WordNet sense
, we permit an association between non-English
word and WordNet sense if is the trans-
lation of any English word in ’s synonym set.
We use the popular EM algorithm (Dempster et al.,
1977) to estimate the model’s parameters from a set
of translation pairs derived from a parallel corpus.
</bodyText>
<sectionHeader confidence="0.976745" genericHeader="method">
3 Using the Model for WSD
</sectionHeader>
<bodyText confidence="0.9392785">
Like Diab’s system, the sense model of Bhat-
tacharya et al. requires a parallel corpus in order
to estimate its parameters, and as currently imple-
mented it can only assign sense tags to words in par-
allel text. In order to perform WSD experiments on
English test items, therefore, two steps are neces-
sary: obtaining translation pairs from an English-F
parallel corpus in order to create a model, and trans-
lating test items from English into F in order to ob-
tain word-level alignments that can be used as the
basis for disambiguation.
In order to accomplish these steps, Bhattacharya
et al. (2004) used the pseudo-translation ap-
proach of Diab and Resnik (2002): they created
the model using an English-Spanish parallel corpus
constructed by using Systran to translate a large col-
lection of English text, and they obtained parallel
Spanish text for the test items in the same fashion.
On the nouns (only) in the SENSEVAL-2 English
all-words task, they obtained precision and recall of
0.624 and 0.616, respectively, improving on Diab’s
precision and recall of 0.618 and 0.572.3
Our goal for SENSEVAL-3 was to investigate the
use of human-translated text, rather than pseudo-
translated text, in creating the model. To that end,
we used three sources of sentence-aligned parallel
text:
Spanish-English: A set of 107,222 sentence
pairs sampled from modern Bible translations,
3Their paper includes a refinement of the probabilistic
model that improves performance further, to precision and re-
call of 0.672 and 0.651.
</bodyText>
<table confidence="0.9987265">
Language pair Precision (%) Recall (%)
English-Chinese 0.445 0.445
English-Spanish 0.444 0.444
English-French 0.445 0.445
</table>
<tableCaption confidence="0.695202333333333">
Table 4: Unsupervised probabilistic model results
(fine-grained) on the SENSEVAL-2 English all-
words task
United Nations Proceedings, and newswire
translations from FBIS (the Foreign Broadcast
Information Service).
</tableCaption>
<bodyText confidence="0.9922585">
French-English: a set of 1,008,591 sentence
pairs from the Europarl corpus (Koehn, 2003)
Chinese-English: a set of 440,223 sentence
pairs from FBIS.
In order to tag new test sentences, we used ma-
chine translation from English test items into each
of Spanish, French, and Chinese. We used Systran
for Spanish and French, and for Chinese we used
an implementation of the alignment template frame-
work for statistical MT (Kumar and Byrne, 2003).
Once having obtained the translations for test sen-
tences, we used GIZA++ to create word-level align-
ments within which translation pairs could be iden-
tified. We used the probabilistic model only for
WSD of nouns, where nouns were identified using
an automatic part-of-speech tagger. For other parts
of speech, we used the first-listed WordNet sense.
Time limitations prevented us from completing
SENSEVAL-3 runs in time for this writing. Ta-
ble 4 shows the performance of the system on the
SENSEVAL-2 English all-words task. This perfor-
mance level places the approach in the middle group
of performers on this task at the time of SENSEVAL-
2 (with scores roughly in the range of 0.44-0.46), as
contrasted to the top group (with scores in the range
of 0.56-0.69). In the near future, we hope to repeat
the experiment with the SENSEVAL-3 English all-
words and lexical sample test data, and also to ex-
plore evidence combination from multiple language
pairs.
</bodyText>
<sectionHeader confidence="0.999737" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99997">
Our precision and recall in the SENSEVAL-3 lexi-
cal sample tasks was in all cases very close to the
median reported by the task organizer, thus demon-
strating an ability to obtain credible performance
with a simple, robust approach. Additionally, we
explored the use of additional features, and we ex-
perimented with applying a new unsupervised prob-
abilistic model using human-translated rather than
pseudo-translated parallel text, with equivocal re-
sults for the various extensions beyond the basic
system. In the future we plan to focus our attention
more heavily on the learning paradigm and prob-
abilistic modeling, with the particular aim of more
effectively exploiting local and document-level con-
text for both sense disambiguation and lexical selec-
tion.
</bodyText>
<sectionHeader confidence="0.996932" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999118">
The work described in this paper was supported
in part by ONR MURI Contract FCPO.810548265,
National Science Foundation grant EIA0130422,
and Department of Defense contract RD-02-5700.
The authors gratefully acknowledge the assistance
of Mona Diab, Aaron Elkiss, Adam Lopez, and
Grazia Russo-Lassner in data preparation details,
and the kind help of Shankar Kumar and Yonggang
Deng in using their framework for statistical ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.992259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.91084214893617">
Yoshua Bengio and Christopher Kermorvant. 2003.
Extracting hidden sense probabilities from bi-
texts. Technical report, TR 1231, Departement
d’informatique et recherche operationnelle, Uni-
versite de Montreal.
Indrajit Bhattacharya, Lise Getoor, and Yoshua
Bengio. 2004. Unsupervised sense disambigua-
tion using bilingual probabilistic models. In
Meeting of the Association for Computational
Linguistics.
Clara Cabezas, Philip Resnik, and Jessica Stevens.
2001. Supervised sense tagging using support
vector machines. In Proceedings of the Sec-
ond International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
Toulouse, France, July.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via
the EM algorithm. Journal of the Royal Statis-
tical Society, B 39:1–38.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using paral-
lel corpora. In 40th Anniversary Meeting of the
Association for Computational Linguistics (ACL-
02), Philadelphia, July.
Mona Diab. 2003. Word Sense Disambiguation
within a Multilingual Framework. Ph.D. thesis,
University of Maryland.
Philipp Koehn. 2003. Europarl: A multilingual cor-
pus for evaluation of machine translation. unpub-
lished manuscript.
S. Kumar and W. Byrne. 2003. A weighted finite
state transducer implementation of the alignment
template model for statistical machine transla-
tion. In Proceedings of HLT-NAACL, Edmonton,
Canada, May.
Gina Levow, Douglas Oard, and Philip Resnik.
2001. Rapidly retargetable interactive translin-
gual retrieval. In Human Language Technology
Conference (HLT-2001), San Diego, CA, March.
Dekang Lin. 1998. Dependency-based evaluation
of MINIPAR. In Workshop on the Evaluation of
Parsing Systems, Granada, Spain, May.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19–51.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.8072315">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.5046385">Association for Computational Linguistics The University of Maryland SENSEVAL-3 System Descriptions</title>
<author confidence="0.692084">Clara Cabezas</author>
<author confidence="0.692084">Indrajit Bhattacharya</author>
<author confidence="0.692084">Philip</author>
<affiliation confidence="0.810568">University of Maryland, College Park, MD 20742 USA</affiliation>
<email confidence="0.999685">clarac@umiacs.umd.edu,indrajit@cs.umd.edu,resnik@umd.edu</email>
<abstract confidence="0.990760594427245">the University of Maryland (UMD) team focused on two primary issues: the portability of sense disambigation across languages, and the exploitation of real-world bilingual text as a resource for unsupervised sense tagging. We validated the portability of our supervised disambiguation approach by applying it in seven tasks (English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples), and we experimented with a new unsupervised algorithm for sense modeling using parallel corpora. 1 Supervised Sense Tagging for Lexical Samples 1.1 Tagging Framework For the English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples, we employed the UMD-SST system developed for (Cabezas et al., 2001); we refer the reader to that paper for a detailed system description. Briefly, UMD-SST takes a supervised learning approach, treating each word in a task’s vocabulary as an independent problem of classification into that word’s sense inventory. Each training and test item is represented as a weighted feature vector, with dimensions corresponding to properties of the con- As in our system supported the following kinds of features: Local context. For each = 1, 2, and 3, and for word in the vocabulary, there is a feature representing the presence of at a distance of words to the left of the word being disambigated; there is a corresponding set of features for the local context the right of the word. Wide context. Each word in the training set vocabulary has a corresponding feature indiits presence. For wide context features were taken from the entire training or test instance. In other settings, one might make further distinctions, e.g. between words in the same paragraph and words in the document. We also experimented with the following additional kinds of features for English: Grammatical context. We use a syntactic dependency parser (Lin, 1998) to produce, for each word to be disambiguated, features identifying relevant syntactic relationships in the sentence where it occurs. For example, in the U.S. government announced a visa waiver the word have syntactic features like and Expanded context. In information retrieval, we and other researchers have found that it can be useful to expand the representation of a document to include informative words from similar documents (Levow et al., 2001). In similar spirit, we create a set of expandedcontext features by (a) treating WSD context as a bag of words, (b) issuing it as a query to a standard information retrieval system that has indexed a large collection of documents, and (c) including the nonstopword vocabulary of the top documents returned. So, for example, in a context the sentence U.S. government a new visa waiver the query might retrieve news articles like “US to Extend Fingerprinting to Europeans, Japanese” (Bloomberg.com, April 2, 2004), leading to addition of features like and so forth. Lexical Sample Coarse (prec/rec) Fine (prec/rec) UMD-SST 0.643/0.643 0.568/0.568 UMD-SST-gram 0.600/0.600 0.576/0.576 UMD-SST-docexp 0.541/0.542 0.516/0.491 1: UMD-SST variations on the English lexical sample task As described by Cabezas et al. (2001), we have adopted the framework of support vector machines (SVMs) in order to perform supervised classification. Because we used a version of SVM learning designed for binary classification tasks, rather than the multi-way classification needed for disambiguating among senses, we constructed a family of SVM classifiers for each word — one for each the word’s senses. All positive training amples for a sense of were treated as negative training examples for all the other senses , . Table 1 shows the performance of our approach on the English lexical sample task from the previous SENSEVAL exercise (SENSEVAL-2), including the basic system (UMD-SST), the basic system with grammatical features added (UMD-SSTgram), and the basic system with document expansion features added (UMD-SST-docexp). (We have not done a run with both sets of features added.) The results showed a possible potential benefit for using grammatical features, in the fine-grained scoring. However, we deemed the benefit too small to upon, and submitted our official runs using UMD-SST without the grammatical or document-expansion features. Lexical Sample Precision (%) Recall (%) Basque 65.6 58.7 Catalan 81.5 80.3 Chinese 51.3 51.2 English 66.0 66.0 Romanian 70.7 70.7 Spanish 82.5 82.5 Multilingual 58.8 58.8 Table 2: UMD-SST results (fine-grained) on SENSEVAL-3 lexical sample tasks Lexical Sample Coarse (prec/rec) Fine (prec/rec) UMD-SST 0.709/0.709 0.660/0.660 UMD-SST-gram 0.703/0.703 0.655/0.655 UMD-SST-docexp 0.691/0.680 0.637/0.627 3: UMD-SST variations on the English lexical sample task performance on the lexical sample runs in which we participated, using fine-grained scores. In unofficial runs, we also experimented with the grammatical and document-expansion features. Table 3 shows the results, which indicate that on this task the additional features did not help and may have hurt performance slightly. Although we have not yet reached any firm conclusions, we conjecture that value potentially added by these features may have been offset by the expansion in the size of the feature space; in future work we plan to explore feature selection and alternative learning frameworks. 1.2 SENSEVAL-3 Lexical Sample Tasks For SENSEVAL-3, the modularity of our system made it very easy to participate in the many lexical sample tasks, including the multilingual lexical sample, where the “sense inventory” consisted vocabulary items from a second Indeed, we participated in several tasks without having anyone on the team who could read the language. (Whether or not this was a good idea remains to be seen.) For Basque, Catalan, and Spanish, we used the lemmatized word forms provided by the task organizers; for the other languages, including English, we used only simple tokenization. Table 2 shows the UMD-SST system’s official format problems prevented us from participating in the Italian lexical sample task. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assumptions: it assumes that senses and words have certain occurrence probabilities and that the choice of the word can be made independently once the sense has been decided. Here interaction between different words arising from the same sense comes into play, even if the words are not related through translations, and this interdependence of the senses through common words plays a role in sense disambiguation. The model takes as its starting point the idea of a “translation pair” — a pair of words and that are aligned in two sentences (here “English” and “non- English”) that are translations of each other. For in the English-Spanish sentence pair la like the one would find the translation pairs , ,, Those familiar with machine translation (MT) models will note that a translation pair is equivalent to a link in a word-level alignment, and in fact we obtain translation pairs sentence-aligned parallel text by training a statistical MT model (using GIZA++, (Och and Ney, 2003)) and using the word-level alignments that result. probabilistic sense model makes the assumption that the English word and the non-English word in a translation pair share the same precise sense, or, in other words, that the set of sense labels for the words in the two languages is the same and may be collapsed into one set of senses that is responsible for both English and non-English words. Thus the one latent variable in the model is the sense label generating both words, represented by variables and . The model also makes assumption that words in both languages are conditionally independent given the sense label. The generative parameters for the model are the prior probability of each sense and the conditional probabilities and of each word and in the two languages given the sense. generation of a translation pair by this model may be viewed as a two-step process that first selects a sense according to the priors on the senses, and then selects a word from each language using the of both Spanish and English will observe that translation pairs may well include words that are not exactly translations of each other. conditional probabilities for that sense. This may be imagined as a factoring of the joint distribution: . Given WordNet as a sense inventory, the set of English senses is as defined in the WordNet database. Since the model assumes the sense labels for the two languages are the same, it must use the same WordNet labels for the non-English words as well. Rather than considering all possible words in the non-English vocabulary for each WordNet sense , we permit an association between non-English and WordNet sense if is the translation of any English word in ’s synonym We use the popular EM algorithm (Dempster et al., 1977) to estimate the model’s parameters from a set of translation pairs derived from a parallel corpus. 3 Using the Model for WSD Like Diab’s system, the sense model of Bhattacharya et al. requires a parallel corpus in order to estimate its parameters, and as currently implemented it can only assign sense tags to words in parallel text. In order to perform WSD experiments on English test items, therefore, two steps are necessary: obtaining translation pairs from an English-F parallel corpus in order to create a model, and translating test items from English into F in order to obtain word-level alignments that can be used as the basis for disambiguation. In order to accomplish these steps, Bhattacharya et al. (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. the nouns (only) in the English all-words task, they obtained precision and recall of 0.624 and 0.616, respectively, improving on Diab’s and recall of 0.618 and goal for was to investigate the use of human-translated text, rather than pseudotranslated text, in creating the model. To that end, we used three sources of sentence-aligned parallel text: Spanish-English: A set of 107,222 sentence pairs sampled from modern Bible translations, paper includes a refinement of the probabilistic model that improves performance further, to precision and recall of 0.672 and 0.651. Language pair Precision (%) Recall (%) English-Chinese 0.445 0.445 English-Spanish 0.444 0.444 English-French 0.445 0.445 Table 4: Unsupervised probabilistic model results on the English allwords task United Nations Proceedings, and newswire translations from FBIS (the Foreign Broadcast Information Service). French-English: a set of 1,008,591 sentence pairs from the Europarl corpus (Koehn, 2003) Chinese-English: a set of 440,223 sentence pairs from FBIS. In order to tag new test sentences, we used machine translation from English test items into each of Spanish, French, and Chinese. We used Systran for Spanish and French, and for Chinese we used an implementation of the alignment template framework for statistical MT (Kumar and Byrne, 2003). Once having obtained the translations for test sentences, we used GIZA++ to create word-level alignments within which translation pairs could be iden- We used the probabilistic model for of where nouns were identified using an automatic part-of-speech tagger. For other parts of speech, we used the first-listed WordNet sense. Time limitations prevented us from completing runs in time for this writing. Table 4 shows the performance of the system on the English all-words task. This performance level places the approach in the middle group performers on this task at the time of 2 (with scores roughly in the range of 0.44-0.46), as contrasted to the top group (with scores in the range of 0.56-0.69). In the near future, we hope to repeat experiment with the English allwords and lexical sample test data, and also to explore evidence combination from multiple language pairs. 4 Conclusions precision and recall in the lexical sample tasks was in all cases very close to the median reported by the task organizer, thus demonstrating an ability to obtain credible performance with a simple, robust approach. Additionally, we explored the use of additional features, and we experimented with applying a new unsupervised probabilistic model using human-translated rather than pseudo-translated parallel text, with equivocal results for the various extensions beyond the basic system. In the future we plan to focus our attention more heavily on the learning paradigm and probabilistic modeling, with the particular aim of more effectively exploiting local and document-level context for both sense disambiguation and lexical selection.</abstract>
<note confidence="0.700819">Acknowledgements The work described in this paper was supported in part by ONR MURI Contract FCPO.810548265, National Science Foundation grant EIA0130422, and Department of Defense contract RD-02-5700. The authors gratefully acknowledge the assistance</note>
<abstract confidence="0.875270444444444">of Mona Diab, Aaron Elkiss, Adam Lopez, and Grazia Russo-Lassner in data preparation details, and the kind help of Shankar Kumar and Yonggang Deng in using their framework for statistical machine translation. References Yoshua Bengio and Christopher Kermorvant. 2003. Extracting hidden sense probabilities from bitexts. Technical report, TR 1231, Departement d’informatique et recherche operationnelle, Universite de Montreal. Indrajit Bhattacharya, Lise Getoor, and Yoshua Bengio. 2004. Unsupervised sense disambiguation using bilingual probabilistic models. In Meeting of the Association for Computational Clara Cabezas, Philip Resnik, and Jessica Stevens. 2001. Supervised sense tagging using support machines. In of the Sec-</abstract>
<affiliation confidence="0.412826">ond International Workshop on Evaluating Word Disambiguation Systems</affiliation>
<address confidence="0.364156">Toulouse, France, July.</address>
<note confidence="0.992474">A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.</note>
<title confidence="0.801676">Maximum likelihood from incomplete data via</title>
<author confidence="0.867849">of the Royal Statis-</author>
<note confidence="0.56248025">B 39:1–38. Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using paralcorpora. In Anniversary Meeting of the</note>
<affiliation confidence="0.585631">Association for Computational Linguistics (ACL-</affiliation>
<address confidence="0.5247445">Philadelphia, July. Diab. 2003. Sense Disambiguation</address>
<affiliation confidence="0.8331475">a Multilingual Ph.D. thesis, University of Maryland.</affiliation>
<abstract confidence="0.899822666666667">Philipp Koehn. 2003. Europarl: A multilingual corpus for evaluation of machine translation. unpublished manuscript. S. Kumar and W. Byrne. 2003. A weighted finite state transducer implementation of the alignment template model for statistical machine transla-</abstract>
<affiliation confidence="0.952028">In of Edmonton,</affiliation>
<address confidence="0.561893333333333">Canada, May. Gina Levow, Douglas Oard, and Philip Resnik. 2001. Rapidly retargetable interactive translin-</address>
<affiliation confidence="0.755806">retrieval. In Language Technology</affiliation>
<address confidence="0.887079">San Diego, CA, March.</address>
<note confidence="0.437841">Dekang Lin. 1998. Dependency-based evaluation MINIPAR. In on the Evaluation of Granada, Spain, May. Franz Josef Och and Hermann Ney. 2003. A</note>
<abstract confidence="0.926502">systematic comparison of various statistical models.</abstract>
<intro confidence="0.613874">29(1):19–51.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Christopher Kermorvant</author>
</authors>
<title>Extracting hidden sense probabilities from bitexts.</title>
<date>2003</date>
<tech>Technical report, TR 1231,</tech>
<institution>Departement</institution>
<contexts>
<context position="7456" citStr="Bengio and Kermorvant (2003)" startWordPosition="1139" endWordPosition="1143"> lexical sample task. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assumptions: it assumes that senses and words have certain occurrence probabilities and that the choice of the word can be made independently once the sense has been decided. Here interaction between different words arising from the same sense comes into play, even if the words are not related through translations, and this interdepend</context>
</contexts>
<marker>Bengio, Kermorvant, 2003</marker>
<rawString>Yoshua Bengio and Christopher Kermorvant. 2003. Extracting hidden sense probabilities from bitexts. Technical report, TR 1231, Departement d’informatique et recherche operationnelle, Universite de Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Indrajit Bhattacharya</author>
<author>Lise Getoor</author>
<author>Yoshua Bengio</author>
</authors>
<title>Unsupervised sense disambiguation using bilingual probabilistic models.</title>
<date>2004</date>
<booktitle>In Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7176" citStr="Bhattacharya et al. (2004)" startWordPosition="1092" endWordPosition="1095">atalan, and Spanish, we used the lemmatized word forms provided by the task organizers; for the other languages, including English, we used only simple tokenization. Table 2 shows the UMD-SST system’s official &apos;Data format problems prevented us from participating in the Italian lexical sample task. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assumptions: it assumes that senses and words have certai</context>
<context position="11278" citStr="Bhattacharya et al. (2004)" startWordPosition="1791" endWordPosition="1794">rpus. 3 Using the Model for WSD Like Diab’s system, the sense model of Bhattacharya et al. requires a parallel corpus in order to estimate its parameters, and as currently implemented it can only assign sense tags to words in parallel text. In order to perform WSD experiments on English test items, therefore, two steps are necessary: obtaining translation pairs from an English-F parallel corpus in order to create a model, and translating test items from English into F in order to obtain word-level alignments that can be used as the basis for disambiguation. In order to accomplish these steps, Bhattacharya et al. (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. On the nouns (only) in the SENSEVAL-2 English all-words task, they obtained precision and recall of 0.624 and 0.616, respectively, improving on Diab’s precision and recall of 0.618 and 0.572.3 Our goal for SENSEVAL-3 was to investigate the use of human-translated text, rather than pseudotranslated text, in creat</context>
</contexts>
<marker>Bhattacharya, Getoor, Bengio, 2004</marker>
<rawString>Indrajit Bhattacharya, Lise Getoor, and Yoshua Bengio. 2004. Unsupervised sense disambiguation using bilingual probabilistic models. In Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clara Cabezas</author>
<author>Philip Resnik</author>
<author>Jessica Stevens</author>
</authors>
<title>Supervised sense tagging using support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="1181" citStr="Cabezas et al., 2001" startWordPosition="158" endWordPosition="161">ploitation of real-world bilingual text as a resource for unsupervised sense tagging. We validated the portability of our supervised disambiguation approach by applying it in seven tasks (English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples), and we experimented with a new unsupervised algorithm for sense modeling using parallel corpora. 1 Supervised Sense Tagging for Lexical Samples 1.1 Tagging Framework For the English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples, we employed the UMD-SST system developed for SENSEVAL-2 (Cabezas et al., 2001); we refer the reader to that paper for a detailed system description. Briefly, UMD-SST takes a supervised learning approach, treating each word in a task’s vocabulary as an independent problem of classification into that word’s sense inventory. Each training and test item is represented as a weighted feature vector, with dimensions corresponding to properties of the context. As in SENSEVAL-2, our system supported the following kinds of features: Local context. For each = 1, 2, and 3, and for each word in the vocabulary, there is a feature representing the presence of word at a distance of wor</context>
<context position="3875" citStr="Cabezas et al. (2001)" startWordPosition="579" endWordPosition="582">, in a context containing the sentence The U.S. government announced a new visa waiver policy, the query might retrieve news articles like “US to Extend Fingerprinting to Europeans, Japanese” (Bloomberg.com, April 2, 2004), leading to the addition of features like EXT:EUROPEAN, EXT:JAPANESE, EXT:FINGERPRINTING EXT:VISITORS, EXT:TOURISM, and so forth. Lexical Sample Coarse (prec/rec) Fine (prec/rec) UMD-SST 0.643/0.643 0.568/0.568 UMD-SST-gram 0.600/0.600 0.576/0.576 UMD-SST-docexp 0.541/0.542 0.516/0.491 Table 1: UMD-SST variations on the SENSEVAL-2 English lexical sample task As described by Cabezas et al. (2001), we have adopted the framework of support vector machines (SVMs) in order to perform supervised classification. Because we used a version of SVM learning designed for binary classification tasks, rather than the multi-way classification needed for disambiguating among senses, we constructed a family of SVM classifiers for each word — one for each of the word’s senses. All positive training examples for a sense of were treated as negative training examples for all the other senses , . Table 1 shows the performance of our approach on the English lexical sample task from the previous SENSEVAL ex</context>
</contexts>
<marker>Cabezas, Resnik, Stevens, 2001</marker>
<rawString>Clara Cabezas, Philip Resnik, and Jessica Stevens. 2001. Supervised sense tagging using support vector machines. In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B</journal>
<pages>39--1</pages>
<contexts>
<context position="10558" citStr="Dempster et al., 1977" startWordPosition="1666" endWordPosition="1669">abilities for that sense. This may be imagined as a factoring of the joint distribution: . Given WordNet as a sense inventory, the set of English senses is as defined in the WordNet database. Since the model assumes the sense labels for the two languages are the same, it must use the same WordNet labels for the non-English words as well. Rather than considering all possible words in the non-English vocabulary for each WordNet sense , we permit an association between non-English word and WordNet sense if is the translation of any English word in ’s synonym set. We use the popular EM algorithm (Dempster et al., 1977) to estimate the model’s parameters from a set of translation pairs derived from a parallel corpus. 3 Using the Model for WSD Like Diab’s system, the sense model of Bhattacharya et al. requires a parallel corpus in order to estimate its parameters, and as currently implemented it can only assign sense tags to words in parallel text. In order to perform WSD experiments on English test items, therefore, two steps are necessary: obtaining translation pairs from an English-F parallel corpus in order to create a model, and translating test items from English into F in order to obtain word-level ali</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
<author>Philip Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In 40th Anniversary Meeting of the Association for Computational Linguistics (ACL02),</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="7125" citStr="Diab and Resnik, 2002" startWordPosition="1085" endWordPosition="1088">a good idea remains to be seen.) For Basque, Catalan, and Spanish, we used the lemmatized word forms provided by the task organizers; for the other languages, including English, we used only simple tokenization. Table 2 shows the UMD-SST system’s official &apos;Data format problems prevented us from participating in the Italian lexical sample task. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assump</context>
<context position="11341" citStr="Diab and Resnik (2002)" startWordPosition="1801" endWordPosition="1804"> of Bhattacharya et al. requires a parallel corpus in order to estimate its parameters, and as currently implemented it can only assign sense tags to words in parallel text. In order to perform WSD experiments on English test items, therefore, two steps are necessary: obtaining translation pairs from an English-F parallel corpus in order to create a model, and translating test items from English into F in order to obtain word-level alignments that can be used as the basis for disambiguation. In order to accomplish these steps, Bhattacharya et al. (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. On the nouns (only) in the SENSEVAL-2 English all-words task, they obtained precision and recall of 0.624 and 0.616, respectively, improving on Diab’s precision and recall of 0.618 and 0.572.3 Our goal for SENSEVAL-3 was to investigate the use of human-translated text, rather than pseudotranslated text, in creating the model. To that end, we used three sources of sentence-a</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>Mona Diab and Philip Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In 40th Anniversary Meeting of the Association for Computational Linguistics (ACL02), Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona Diab</author>
</authors>
<title>Word Sense Disambiguation within a Multilingual Framework.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Maryland.</institution>
<contexts>
<context position="7138" citStr="Diab, 2003" startWordPosition="1089" endWordPosition="1090">be seen.) For Basque, Catalan, and Spanish, we used the lemmatized word forms provided by the task organizers; for the other languages, including English, we used only simple tokenization. Table 2 shows the UMD-SST system’s official &apos;Data format problems prevented us from participating in the Italian lexical sample task. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assumptions: it ass</context>
</contexts>
<marker>Diab, 2003</marker>
<rawString>Mona Diab. 2003. Word Sense Disambiguation within a Multilingual Framework. Ph.D. thesis, University of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A multilingual corpus for evaluation of machine translation.</title>
<date>2003</date>
<note>unpublished manuscript.</note>
<contexts>
<context position="12617" citStr="Koehn, 2003" startWordPosition="1990" endWordPosition="1991"> pairs sampled from modern Bible translations, 3Their paper includes a refinement of the probabilistic model that improves performance further, to precision and recall of 0.672 and 0.651. Language pair Precision (%) Recall (%) English-Chinese 0.445 0.445 English-Spanish 0.444 0.444 English-French 0.445 0.445 Table 4: Unsupervised probabilistic model results (fine-grained) on the SENSEVAL-2 English allwords task United Nations Proceedings, and newswire translations from FBIS (the Foreign Broadcast Information Service). French-English: a set of 1,008,591 sentence pairs from the Europarl corpus (Koehn, 2003) Chinese-English: a set of 440,223 sentence pairs from FBIS. In order to tag new test sentences, we used machine translation from English test items into each of Spanish, French, and Chinese. We used Systran for Spanish and French, and for Chinese we used an implementation of the alignment template framework for statistical MT (Kumar and Byrne, 2003). Once having obtained the translations for test sentences, we used GIZA++ to create word-level alignments within which translation pairs could be identified. We used the probabilistic model only for WSD of nouns, where nouns were identified using </context>
</contexts>
<marker>Koehn, 2003</marker>
<rawString>Philipp Koehn. 2003. Europarl: A multilingual corpus for evaluation of machine translation. unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>A weighted finite state transducer implementation of the alignment template model for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="12969" citStr="Kumar and Byrne, 2003" startWordPosition="2047" endWordPosition="2050">tic model results (fine-grained) on the SENSEVAL-2 English allwords task United Nations Proceedings, and newswire translations from FBIS (the Foreign Broadcast Information Service). French-English: a set of 1,008,591 sentence pairs from the Europarl corpus (Koehn, 2003) Chinese-English: a set of 440,223 sentence pairs from FBIS. In order to tag new test sentences, we used machine translation from English test items into each of Spanish, French, and Chinese. We used Systran for Spanish and French, and for Chinese we used an implementation of the alignment template framework for statistical MT (Kumar and Byrne, 2003). Once having obtained the translations for test sentences, we used GIZA++ to create word-level alignments within which translation pairs could be identified. We used the probabilistic model only for WSD of nouns, where nouns were identified using an automatic part-of-speech tagger. For other parts of speech, we used the first-listed WordNet sense. Time limitations prevented us from completing SENSEVAL-3 runs in time for this writing. Table 4 shows the performance of the system on the SENSEVAL-2 English all-words task. This performance level places the approach in the middle group of performer</context>
</contexts>
<marker>Kumar, Byrne, 2003</marker>
<rawString>S. Kumar and W. Byrne. 2003. A weighted finite state transducer implementation of the alignment template model for statistical machine translation. In Proceedings of HLT-NAACL, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina Levow</author>
<author>Douglas Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Rapidly retargetable interactive translingual retrieval.</title>
<date>2001</date>
<booktitle>In Human Language Technology Conference (HLT-2001),</booktitle>
<location>San Diego, CA,</location>
<contexts>
<context position="2926" citStr="Levow et al., 2001" startWordPosition="439" endWordPosition="442">es for English: Grammatical context. We use a syntactic dependency parser (Lin, 1998) to produce, for each word to be disambiguated, features identifying relevant syntactic relationships in the sentence where it occurs. For example, in the sentence The U.S. government announced a new visa waiver policy, the word government would have syntactic features like DET:THE, MOD:U.S., and SUBJ-OF:ANNOUNCED. Expanded context. In information retrieval, we and other researchers have found that it can be useful to expand the representation of a document to include informative words from similar documents (Levow et al., 2001). In a similar spirit, we create a set of expandedcontext features by (a) treating the WSD context as a bag of words, (b) issuing it as a query to a standard information retrieval system that has indexed a large collection of documents, and (c) including the nonstopword vocabulary of the top documents returned. So, for example, in a context containing the sentence The U.S. government announced a new visa waiver policy, the query might retrieve news articles like “US to Extend Fingerprinting to Europeans, Japanese” (Bloomberg.com, April 2, 2004), leading to the addition of features like EXT:EUR</context>
</contexts>
<marker>Levow, Oard, Resnik, 2001</marker>
<rawString>Gina Levow, Douglas Oard, and Philip Resnik. 2001. Rapidly retargetable interactive translingual retrieval. In Human Language Technology Conference (HLT-2001), San Diego, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="2392" citStr="Lin, 1998" startWordPosition="361" endWordPosition="362">s to the left of the word being disambigated; there is a corresponding set of features for the local context to the right of the word. Wide context. Each word in the training set vocabulary has a corresponding feature indicating its presence. For SENSEVAL-3, wide context features were taken from the entire training or test instance. In other settings, one might make further distinctions, e.g. between words in the same paragraph and words in the document. We also experimented with the following additional kinds of features for English: Grammatical context. We use a syntactic dependency parser (Lin, 1998) to produce, for each word to be disambiguated, features identifying relevant syntactic relationships in the sentence where it occurs. For example, in the sentence The U.S. government announced a new visa waiver policy, the word government would have syntactic features like DET:THE, MOD:U.S., and SUBJ-OF:ANNOUNCED. Expanded context. In information retrieval, we and other researchers have found that it can be useful to expand the representation of a document to include informative words from similar documents (Levow et al., 2001). In a similar spirit, we create a set of expandedcontext features</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8761" citStr="Och and Ney, 2003" startWordPosition="1355" endWordPosition="1358"> takes as its starting point the idea of a “translation pair” — a pair of words and that are aligned in two sentences (here “English” and “nonEnglish”) that are translations of each other. For example, in the English-Spanish sentence pair Me gusta la ciudad/I like the city, one would find the translation pairs , ,, and .2 Those familiar with statistical machine translation (MT) models will note that a translation pair is equivalent to a link in a word-level alignment, and in fact we obtain translation pairs from sentence-aligned parallel text by training a statistical MT model (using GIZA++, (Och and Ney, 2003)) and using the word-level alignments that result. The probabilistic sense model makes the assumption that the English word and the non-English word in a translation pair share the same precise sense, or, in other words, that the set of sense labels for the words in the two languages is the same and may be collapsed into one set of senses that is responsible for both English and non-English words. Thus the one latent variable in the model is the sense label generating both words, represented by variables and . The model also makes the assumption that words in both languages are conditionally i</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>