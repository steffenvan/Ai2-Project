<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000221">
<title confidence="0.9982705">
Building Japanese Textual Entailment Specialized Data Sets
for Inference of Basic Sentence Relations
</title>
<author confidence="0.985541">
Kimi Kaneko t Yusuke Miyao t Daisuke Bekki t
</author>
<affiliation confidence="0.994227">
t Ochanomizu University, Tokyo, Japan
t National Institute of Informatics, Tokyo, Japan
</affiliation>
<email confidence="0.9764775">
t{kaneko.kimi  |bekki}@is.ocha.ac.jp
tyusuke@nii.ac.jp
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762545454545">
This paper proposes a methodology for
generating specialized Japanese data sets
for textual entailment, which consists of
pairs decomposed into basic sentence rela-
tions. We experimented with our method-
ology over a number of pairs taken from
the RITE-2 data set. We compared
our methodology with existing studies
in terms of agreement, frequencies and
times, and we evaluated its validity by in-
vestigating recognition accuracy.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.961337428571429">
In recognizing textual entailment (RTE), auto-
mated systems assess whether a human reader
would consider that, given a snippet of text t1 and
some unspecified (but restricted) world knowl-
edge, a second snippet of text t2 is true. An ex-
ample is given below.
Ex. 1) Example of a sentence pair for RTE
</bodyText>
<listItem confidence="0.999900333333333">
• Label: Y
• t1: Shakespeare wrote Hamlet and Macbeth.
• t2: Shakespeare is the author of Hamlet.
</listItem>
<bodyText confidence="0.996867571428571">
“Label” on line 1 shows whether textual entail-
ment (TE) holds between t1 and t2. The pair is
labeled ‘Y’ if the pair exhibits TE and ‘N’ other-
wise.
It is difficult for computers to make such as-
sessments because pairs have multiple interrelated
basic sentence relations (BSRs, for detailed in-
formation on BSRs, see section 3). Recognizing
each BSRs in pairs exactly is difficult for com-
puters. Therefore, we should generate special-
ized data sets consisting of t1-t2 pairs decomposed
into BSRs and a methodology for generating such
data sets since such data and methodologies for
Japanese are unavailable at present.
This paper proposes a methodology for gener-
ating specialized Japanese data sets for TE that
consist of monothematic t1-t2 pairs (i.e., pairs in
which only one BSR relevant to the entailment
relation is highlighted and isolated). In addition,
we compare our methodology with existing stud-
ies and analyze its validity.
</bodyText>
<sectionHeader confidence="0.965941" genericHeader="method">
2 Existing Studies
</sectionHeader>
<bodyText confidence="0.999790382352941">
Sammons et al.(2010) point out that it is necessary
to establish a methodology for decomposing pairs
into chains of BSRs, and that establishing such
methodology will enable understanding of how
other existing studies can be combined to solve
problems in natural language processing and iden-
tification of currently unsolvable problems. Sam-
mons et al. experimented with their methodology
over the RTE-5 data set and showed that the recog-
nition accuracy of a system trained with their spe-
cialized data set was higher than that of the system
trained with the original data set. In addition, Ben-
tivogli et al.(2010) proposed a methodology for
classifying more details than was possible in the
study by Sammons et al..
However, these studies were based on only En-
glish data sets. In this regard, the word-order
rules and the grammar of many languages (such
as Japanese) are different from those of English.
We thus cannot assess the validity of methodolo-
gies for any Japanese data set because each lan-
guage has different usages. Therefore, it is neces-
sary to assess the validity of such methodologies
with specialized Japanese data sets.
Kotani et al. (2008) generated specialized
Japanese data sets for RTE that were designed
such that each pair included only one BSR. How-
ever, in that approach the data set is generated ar-
tificially, and BSRs between pairs of real world
texts cannot be analyzed.
We develop our methodology by generating
specialized data sets from a collection of pairs
from RITE-21 binary class (BC) subtask data sets
containing sentences from Wikipedia. RITE-2 is
</bodyText>
<page confidence="0.979214">
273
</page>
<bodyText confidence="0.892773692307692">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273–277,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
an evaluation-based workshop focusing on RTE.
Four subtasks are available in RITE-2, one of
which is the BC subtask whereby systems assess
whether there is TE between t1 and t2. The rea-
son why we apply our methodology to part of the
RITE-2 BC subtask data set is that we can con-
sider the validity of the methodology in view of
the recognition accuracy by using the data sets
generated in RITE-2 tasks, and that we can an-
alyze BSRs in real texts by using sentence pairs
extracted from Wikipedia.
</bodyText>
<sectionHeader confidence="0.997796" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9653895">
In this study, we extended and refined the method-
ology defined in Bentivogli et al.(2010) and devel-
oped a methodology for generating Japanese data
sets broken down into BSRs and non-BSRs as de-
fined below.
Basic sentence relations (BSRs):
</bodyText>
<listItem confidence="0.894318833333333">
- Lexical: Synonymy, Hypernymy, Entailment,
Meronymy;
- Phrasal: Synonymy, Hypernymy, Entailment,
Meronymy, Nominalization, Corference;
- Syntactic: Scrambling, Case alteration, Modi-
fier, Transparent head, Clause, List, Apposi-
tion, Relative clause;
- Reasoning: Temporal, Spatial, Quantity, Im-
plicit relation, Inference;
Non-basic sentence relations (non-BSRs):
- Disagreement: Lexical, Phrasal, Modal, Mod-
ifier, Temporal, Spatial, Quantity;
</listItem>
<bodyText confidence="0.974195724137931">
Mainly, we used relations defined in Bentivogli
et al.(2010) and divided Synonymy, Hypernymy,
Entailment and Meronymy into Lexical and
Phrasal. The differences between our study and
Bentivogli et al.(2010) areas follows. Demonymy
and Statements in Bentivogli et al.(2010) were
not considered in our study because they were
not necessary for Japanese data sets. In addi-
tion, Scrambling, Entailment, Disagreement:
temporal, Disagreement: spatial and Disagree-
ment: quantity were newly added in our study.
Scrambling is a rule for changing the order of
phrases and clauses. Entailment is a rule whereby
the latter sentence is true whenever the former is
true (e.g., “divorce” → “marry”). Entailment is a
rule different from Synonymy, Hypernymy and
Meronymy.
The rules for decomposition are schematized as
follows:
lhttp://www.cl.ecei.tohoku.ac.jp/rite2/doku.php
- Break down pairs into BSRs in order to bring
t1 close to t2 gradually, as the interpretation
of the converted sentence becomes wider
- Label each pair of BSRs or non-BSRs
such that each pair is decomposed to ensure
that there are not multiple BSRs
An example is shown below, where the underlined
parts represent the revised points.
t1: シェイクスピアは ハムレット や マクベスを 書いた。
</bodyText>
<subsectionHeader confidence="0.588547">
Shakespearenom Hamlet com Macbethacc writepast
</subsectionHeader>
<bodyText confidence="0.51155375">
‘Shakespeare wrote Hamlet and Macbeth.’
[List] シェイクスピアは ハムレットを 書いた。
Shakespearenom Hamletacc writepast
‘Shakespeare wrote Hamlet.’
</bodyText>
<equation confidence="0.493543">
t2:[Synonymy] シェイクスピアは ハムレットの 作者 である。
</equation>
<bodyText confidence="0.653566">
:phrasal Shakespearenom Hamletgen authorcomp becop
‘Shakespeare is the author of Hamlet.’
</bodyText>
<tableCaption confidence="0.988963">
Table 1: Example of a pair with TE
</tableCaption>
<bodyText confidence="0.6568295">
An example of a pair without TE is shown below.
t1: ブルガリアは ユーラシア大陸に ある。
Bulgarianom Eurasia.continentdat becop
‘Bulgaria is on the Eurasian continent.’
[Entailment] ブルガリアは 大陸国家 である。
: phrasal Bulgarianom continental.statecomp becop
‘Bulgaria is a continental state.’
t2:[Disagreement] ブルガリアは 島国 である。
:lexical Bulgarianom island.countrycomp becop
‘Bulgaria is an island country.’
</bodyText>
<tableCaption confidence="0.974528">
Table 2: Example of a pair without TE (Part 1)
</tableCaption>
<bodyText confidence="0.9931764">
To facilitate TE assessments like Table 3, non-
BSR labels were used in decomposing pairs. In
addition, we allowed labels to be used several
times when some BSRs in a pair are related to ‘N’
assessments.
</bodyText>
<table confidence="0.902308222222222">
t1: ブルガリアは ユーラシア大陸に ある。
Bulgarianom Eurasia.continentdat becop
‘Bulgaria is on the Eurasian continent.’
[Disagreement] ブルガリアは ユーラシア大陸に ない。
:modal Bulgarianom Eurasia.continentdat becop_neg
‘Bulgaria is not on the Eurasian continent.’
t2:[Synonymy] ブルガリアは ヨーロッパに 属さない。
:lexical Bulgarianom Europedat belongcop_neg
‘Bulgaria does not belong to Europe.’
</table>
<tableCaption confidence="0.999894">
Table 3: Example of a pair without TE (Part 2)
</tableCaption>
<bodyText confidence="0.999970857142857">
As mentioned above, the idea here is to decom-
pose pairs in order to bring t1 closer to t2, the
latter of which in principle has a wider semantic
scope. We prohibited the conversion of t2 because
it was possible to decompose the pairs such that
they could be true even if there was no TE. Never-
theless, since it is sometimes easier to convert t2,
</bodyText>
<page confidence="0.988846">
274
</page>
<bodyText confidence="0.999644">
we allowed the conversion of t2 in only the case
that t1 contradicted t2 and the scope of t2 did not
overlap with that of t1 even if t2 was converted and
TE would be unchanged. An example in case that
we allowed to convert t2 is shown below. Bold-
faced types in Table 4 shows that it becomes easy
to compare t1 with t2 by converting to t2.
</bodyText>
<table confidence="0.910732142857143">
t1: トムは 今日、朝食を 食べなかった。
Tomnom today breakfastacc eatpast−neg
‘Tom didn’t eat breakfast today.’
[Scrambling] 今日、 トムは 朝食を 食べなかった。
today Tomnom breakfastacc eatpast−neg
‘Today, Tom didn’t eat breakfast.’
t2: 今朝、 トムは パンを 食べた。
this.morning Tomnom breadacc eatpast
‘This morning, Tom ate bread and salad.’
[Entailment] 今日、 トムは 朝食を 食べた。
:phrasal today Tomnom breakfastacc eatpast
‘Today, Tom ate breakfast.’
[Disagreement] 今日、トムは朝食を食べた。
:modal ‘Today, Tom ate breakfast.’
</table>
<tableCaption confidence="0.999649">
Table 4: Example of conversion of t2
</tableCaption>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999733">
4.1 Comparison with Existing Studies
</subsectionHeader>
<bodyText confidence="0.96718585">
We applied our methodology to 173 pairs from the
RITE-2 BC subtask data set. The pairs were de-
composed by one annotator, and the decomposed
pairs were assigned labels by two annotators. Dur-
ing labeling, we used the labels presented in Sec-
tion 3 and “unknown” in cases where pairs could
not be labeled. Our methodology was developed
based on 112 pairs, and by using the other 61 pairs,
we evaluated the inter-annotator agreement as well
as the frequencies and times of decomposition.
The agreement for 241 monothematic pairs gen-
erated from 61 pairs amounted to 0.83 and was
computed as follows. The kappa coefficient for
them amounted 0.81.
Agreement = “Agreed&amp;quot; labels/Total 2
Bentivogli et al. (2010) reported an agreement
rate of 0.78, although they computed the agree-
ment by using the Dice coefficient (Dice, 1945),
and therefore the results are not directly compara-
ble to ours. Nevertheless, the close values suggest
</bodyText>
<footnote confidence="0.8055115">
2Because the “Agreed” pairs were clear to be classi-
fied as “Agreed”, where “Total” is the number of pairs la-
beled “Agreed” subtracted from the number of labeled pairs.
“Agreed” labels is the number of pairs labeled “Agreed” sub-
tract from the number of pairs with the same label assigned
by the two annotators.
</footnote>
<bodyText confidence="0.990942">
that our methodology is comparable to that in Ben-
tivogli’s study in terms of agreement.
Table 5 shows the distribution of monothematic
pairs with respect to original Y/N pairs.
</bodyText>
<table confidence="0.999309">
Original pairs Monothematic pairs
Y N Total
Y (32) 116 – 116
N (29) 96 29 125
Total (61) 212 29 241
</table>
<tableCaption confidence="0.9027855">
Table 5: Distribution of monothematic pairs with
respect to original Y/N pairs
</tableCaption>
<bodyText confidence="0.997806461538461">
When the methodology was applied to 61 pairs,
a total of 241 and an average of 3.95 monothe-
matic pairs were derived. The average was slightly
greater than the 2.98 reported in (Bentivogli et al.,
2010). For pairs originally labeled ‘Y’ and ‘N’, an
average of 3.62 and 3.31 monothematic pairs were
derived, respectively. Both average values were
slightly higher than the values of 3.03 and 2.80 re-
ported in (Bentivogli et al., 2010). On the basis of
the small differences between the average values
in our study and those in (Bentivogli et al., 2010),
we are justified in saying that our methodology is
valid.
Table 6 3 shows the distribution of BSRs in t1-
t2 pairs in an existing study and the present study.
We can see from Table 6 that Corference was seen
more frequently in Bentivogli’s study than in our
study, while Entailment and Scrambling were
seen more frequently in our study. This demon-
strates that differences between languages are rele-
vant to the distribution and classification of BSRs.
An average of 5 and 4 original pairs were de-
composed per hour in our study and Bentivogli’s
study, respectively. This indicates that the com-
plexity of our methodology is not much different
from that in Bentivogli et al.(2010).
</bodyText>
<subsectionHeader confidence="0.999489">
4.2 Evaluation of Accuracy in BSR
</subsectionHeader>
<bodyText confidence="0.999659333333333">
In the RITE-2 formal run4, 15 teams used our spe-
cialized data set for the evaluation of their systems.
Table 7 shows the average of F1 scores5 for each
BSR.
Scrambling and Modifier yielded high scores
(close to 90%). The score of List was also
</bodyText>
<footnote confidence="0.991809428571429">
3Because “lexical” and “phrasal” are classified together
in Bentivogli et al.(2010), they are not shown separately in
Table 6.
4In RITE-2, data generated by our methodology were re-
leased as “unit test data”.
5The traditional Fl score is the harmonic mean of preci-
sion and recall.
</footnote>
<page confidence="0.988369">
275
</page>
<table confidence="0.998296233333333">
BSR Monothematic pairs
Bentivogli et al. Present study
Total Y N Total Y N
Synonymy 25 22 3 45 45 0
Hypernymy 5 3 2 5 5 0
Entailment - - - 44 44 0
Meronymy 7 4 3 1 1 0
Nominalization 9 9 0 1 1 0
Corference 49 48 1 3 3 0
Scrambling - - - 15 15 0
Case alteration 7 5 2 7 7 0
Modifier 25 15 10 42 42 0
Transparent head 6 6 0 1 1 0
Clause 5 4 1 14 14 0
List 1 1 0 3 3 0
Apposition 3 2 1 1 1 0
Relative clause 1 1 0 8 8 0
Temporal 2 1 1 1 1 0
Spatial 1 1 0 1 1 0
Quantity 6 0 6 0 0 0
Implicit relation 7 7 0 18 18 0
Inference 40 26 14 2 2 0
Disagreement: lexical/phrasal 3 0 3 27 0 27
Disagreement: modal 1 0 1 1 0 1
Disagreement: temporal - - - 1 0 1
Disagreement: spatial - - - 0 0 0
Disagreement: quantity - - - 0 0 0
Demonymy 1 1 0 - - -
Statements 1 1 0 - - -
total 205 157 48 241 212 29
</table>
<tableCaption confidence="0.972059666666667">
Table 6: Distribution of BSRs in t1-t2 pairs in an
existing study and in the present study using our
methodology
</tableCaption>
<table confidence="0.997756423076923">
BSR F1(%) Monothematic Miss
Pairs
Scrambling 89.6 15 4
Modifier 88.8 42 0
List 88.6 3 0
Temporal 85.7 1 1
Relative clause 85.4 8 2
Clause 85.0 14 2
Hypernymy:lexical 85.0 5 1
Disagreement: phrasal 80.1 25 0
Case alteration 79.9 7 2
Synonymy: lexical 79.7 9 6
Transparent head 78.6 1 2
Implicit relation 75.7 18 2
Synonymy: phrasal 73.6 36 9
Corference 70.9 3 1
Entailment: phrasal 70.2 44 7
Disagreement: lexical 69.0 2 0
Meronymy: lexical 64.3 1 1
Nominalization 64.3 1 0
Apposition 50.0 1 1
Spatial 50.0 1 1
Inference 40.5 2 2
Disagreement: modal 35.7 1 0
Disagreement: temporal 28.6 1 1
Total - 241 41
</table>
<tableCaption confidence="0.9261745">
Table 7: Average F1 scores in BSR and frequen-
cies of misclassifications by annotators
</tableCaption>
<bodyText confidence="0.985612038461538">
nearly 90%, although the data sets included only
3 instances. These scores were high because
pairs with these BSRs are easily recognized in
terms of syntactic structure. By contrast, Dis-
agreement: temporal, Disagreement: modal,
Inference, Spatial and Apposition yielded low
scores (less than 50%). The scores of Disagree-
ment: lexical, Nominalization and Disagree-
ment: Meronymy were about 50-70%. BSRs
that yielded scores of less than 70% occurred less
than 3 times, and those that yielded scores of not
more than 70% occurred 3 times or more, except
for Temporal and Transparent head. Therefore,
the frequencies of BSRs are related to F1 scores,
and we should consider how to build systems that
recognize infrequent BSRs accurately. In addi-
tion, F1 scores in Synonymy: phrasal and En-
tailment: phrasal are low, although these are la-
beled frequently. This is one possible direction of
future work.
Table 7 also shows the number of pairs in BSR
to which the two annotators assigned different la-
bels. For example, one annotator labeled t2 [Ap-
position] while the other labeled t2 [Spatial] in
the following pair:
Ex. 2) Example of a pair for RTE
</bodyText>
<listItem confidence="0.999768">
• t1: Tokyo, the capital of Japan, is in Asia.
• t2: The capital of Japan is in Asia.
</listItem>
<bodyText confidence="0.999930222222222">
We can see from Table 7 that the F1 scores for
BSRs, which are often assessed as different by dif-
ferent people, are generally low, except for several
labels, such as Synonymy: lexical and Scram-
bling. For this reason, we can conjecture that
cases in which computers experience difficulty de-
termining the correct labels are correlated with
cases in which humans also experience such dif-
ficulty.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999982142857143">
This paper presented a methodology for generat-
ing Japanese data sets broken down into BSRs
and Non-BSRs, and we conducted experiments in
which we applied our methodology to 61 pairs
extracted from the RITE-2 BC subtask data set.
We compared our method with that of Bentivogli
et al.(2010) in terms of agreement as well as
frequencies and times of decomposition, and we
obtained similar results. This demonstrated that
our methodology is as feasible as Bentivogli et
al.(2010) and that differences between languages
emerge only as the different sets of labels and the
different distributions of BSRs. In addition, 241
monothematic pairs were recognized by comput-
ers, and we showed that both the frequencies of
BSRs and the rate of misclassification by humans
are relevant to F1 scores.
Decomposition patterns were not empirically
compared in the present study and will be investi-
gated in future work. We will also develop an RTE
inference system by using our specialized data set.
</bodyText>
<page confidence="0.996821">
276
</page>
<sectionHeader confidence="0.995895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890111111111">
Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,
Leggio, M. L., Magnini,B. 2010. Building Textual
Entailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to In-
ference. In Proceedings of LREC 2010, Valletta,
Malta.
Dagan, I, Glickman, O., Magnini, B. 2005. Recog-
nizing Textual Entailment Challenge. In Proc. of
the First PASCAL Challenges Workshop on RTE.
Southampton, U.K.
Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.
Building Textual Entailment Japanese Data Sets and
Recognizing Reasoning Relations Based on Syn-
onymy Acquired Automatically. In Proceedings of
the 14th Annual Meeting of the Association for Nat-
ural Language Processing, Tokyo, Japan.
Magnini, B., Cabrio, E. 2009. Combining Special-
izedd Entailment Engines. In Proceedings of LTC
’09. Poznan, Poland.
Dice, L. R. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297-
302.
Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.
2010. ”Ask not what textual entailment can do for
you...”. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, pp. 1199-1208.
</reference>
<page confidence="0.997268">
277
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.771141">
<title confidence="0.998393">Building Japanese Textual Entailment Specialized Data for Inference of Basic Sentence Relations</title>
<author confidence="0.977761">Kaneko tYusuke Miyao tDaisuke Bekki</author>
<affiliation confidence="0.9851">University, Tokyo, Institute of Informatics, Tokyo,</affiliation>
<email confidence="0.837444">|</email>
<abstract confidence="0.997879416666667">This paper proposes a methodology generating specialized Japanese data sets for textual entailment, which consists decomposed into basic sentence rela- We experimented with our ology over a number of pairs taken from the RITE-2 data set. We our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>E Cabrio</author>
<author>I Dagan</author>
<author>D Giampiccolo</author>
<author>M L Leggio</author>
<author>B Magnini</author>
</authors>
<title>Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="9574" citStr="Bentivogli et al. (2010)" startWordPosition="1505" endWordPosition="1508"> The pairs were decomposed by one annotator, and the decomposed pairs were assigned labels by two annotators. During labeling, we used the labels presented in Section 3 and “unknown” in cases where pairs could not be labeled. Our methodology was developed based on 112 pairs, and by using the other 61 pairs, we evaluated the inter-annotator agreement as well as the frequencies and times of decomposition. The agreement for 241 monothematic pairs generated from 61 pairs amounted to 0.83 and was computed as follows. The kappa coefficient for them amounted 0.81. Agreement = “Agreed&amp;quot; labels/Total 2 Bentivogli et al. (2010) reported an agreement rate of 0.78, although they computed the agreement by using the Dice coefficient (Dice, 1945), and therefore the results are not directly comparable to ours. Nevertheless, the close values suggest 2Because the “Agreed” pairs were clear to be classified as “Agreed”, where “Total” is the number of pairs labeled “Agreed” subtracted from the number of labeled pairs. “Agreed” labels is the number of pairs labeled “Agreed” subtract from the number of pairs with the same label assigned by the two annotators. that our methodology is comparable to that in Bentivogli’s study in te</context>
<context position="10891" citStr="Bentivogli et al., 2010" startWordPosition="1731" endWordPosition="1734">al Y/N pairs. Original pairs Monothematic pairs Y N Total Y (32) 116 – 116 N (29) 96 29 125 Total (61) 212 29 241 Table 5: Distribution of monothematic pairs with respect to original Y/N pairs When the methodology was applied to 61 pairs, a total of 241 and an average of 3.95 monothematic pairs were derived. The average was slightly greater than the 2.98 reported in (Bentivogli et al., 2010). For pairs originally labeled ‘Y’ and ‘N’, an average of 3.62 and 3.31 monothematic pairs were derived, respectively. Both average values were slightly higher than the values of 3.03 and 2.80 reported in (Bentivogli et al., 2010). On the basis of the small differences between the average values in our study and those in (Bentivogli et al., 2010), we are justified in saying that our methodology is valid. Table 6 3 shows the distribution of BSRs in t1- t2 pairs in an existing study and the present study. We can see from Table 6 that Corference was seen more frequently in Bentivogli’s study than in our study, while Entailment and Scrambling were seen more frequently in our study. This demonstrates that differences between languages are relevant to the distribution and classification of BSRs. An average of 5 and 4 origina</context>
</contexts>
<marker>Bentivogli, Cabrio, Dagan, Giampiccolo, Leggio, Magnini, 2010</marker>
<rawString>Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D., Leggio, M. L., Magnini,B. 2010. Building Textual Entailment Specialized Data Sets: a Methodology for Isolating Linguistic Phenomena Relevant to Inference. In Proceedings of LREC 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>Recognizing Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proc. of the First PASCAL Challenges Workshop on RTE.</booktitle>
<location>Southampton, U.K.</location>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, I, Glickman, O., Magnini, B. 2005. Recognizing Textual Entailment Challenge. In Proc. of the First PASCAL Challenges Workshop on RTE. Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kotani</author>
<author>T Shibata</author>
<author>T Nakata</author>
<author>S Kurohashi</author>
</authors>
<title>Building Textual Entailment Japanese Data Sets and Recognizing Reasoning Relations Based on Synonymy Acquired Automatically.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th Annual Meeting of the Association for Natural Language Processing,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="3238" citStr="Kotani et al. (2008)" startWordPosition="517" endWordPosition="520">ystem trained with the original data set. In addition, Bentivogli et al.(2010) proposed a methodology for classifying more details than was possible in the study by Sammons et al.. However, these studies were based on only English data sets. In this regard, the word-order rules and the grammar of many languages (such as Japanese) are different from those of English. We thus cannot assess the validity of methodologies for any Japanese data set because each language has different usages. Therefore, it is necessary to assess the validity of such methodologies with specialized Japanese data sets. Kotani et al. (2008) generated specialized Japanese data sets for RTE that were designed such that each pair included only one BSR. However, in that approach the data set is generated artificially, and BSRs between pairs of real world texts cannot be analyzed. We develop our methodology by generating specialized data sets from a collection of pairs from RITE-21 binary class (BC) subtask data sets containing sentences from Wikipedia. RITE-2 is 273 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273–277, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computat</context>
</contexts>
<marker>Kotani, Shibata, Nakata, Kurohashi, 2008</marker>
<rawString>Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008. Building Textual Entailment Japanese Data Sets and Recognizing Reasoning Relations Based on Synonymy Acquired Automatically. In Proceedings of the 14th Annual Meeting of the Association for Natural Language Processing, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>E Cabrio</author>
</authors>
<title>Combining Specializedd Entailment Engines.</title>
<date>2009</date>
<booktitle>In Proceedings of LTC ’09.</booktitle>
<location>Poznan, Poland.</location>
<marker>Magnini, Cabrio, 2009</marker>
<rawString>Magnini, B., Cabrio, E. 2009. Combining Specializedd Entailment Engines. In Proceedings of LTC ’09. Poznan, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<pages>26--3</pages>
<contexts>
<context position="9690" citStr="Dice, 1945" startWordPosition="1526" endWordPosition="1527"> used the labels presented in Section 3 and “unknown” in cases where pairs could not be labeled. Our methodology was developed based on 112 pairs, and by using the other 61 pairs, we evaluated the inter-annotator agreement as well as the frequencies and times of decomposition. The agreement for 241 monothematic pairs generated from 61 pairs amounted to 0.83 and was computed as follows. The kappa coefficient for them amounted 0.81. Agreement = “Agreed&amp;quot; labels/Total 2 Bentivogli et al. (2010) reported an agreement rate of 0.78, although they computed the agreement by using the Dice coefficient (Dice, 1945), and therefore the results are not directly comparable to ours. Nevertheless, the close values suggest 2Because the “Agreed” pairs were clear to be classified as “Agreed”, where “Total” is the number of pairs labeled “Agreed” subtracted from the number of labeled pairs. “Agreed” labels is the number of pairs labeled “Agreed” subtract from the number of pairs with the same label assigned by the two annotators. that our methodology is comparable to that in Bentivogli’s study in terms of agreement. Table 5 shows the distribution of monothematic pairs with respect to original Y/N pairs. Original </context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>Dice, L. R. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297-302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Sammons</author>
<author>V G Vinod Vydiswaran</author>
<author>Dan Roth</author>
</authors>
<title>Ask not what textual entailment can do for you...”.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1199--1208</pages>
<location>Uppsala,</location>
<marker>Sammons, Vydiswaran, Roth, 2010</marker>
<rawString>Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth. 2010. ”Ask not what textual entailment can do for you...”. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, pp. 1199-1208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>